<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:33:47Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|63001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1106</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1106</id><created>2014-07-03</created><authors><author><keyname>K.</keyname><forenames>Arti M.</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Manav R.</forenames></author></authors><title>Performance Analysis of Two-Way AF MIMO Relaying of OSTBCs with
  Imperfect Channel Gains</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the relaying of orthogonal space time block codes
(OSTBCs) in a two-way amplify-and-forward (AF) multiple-input multiple-output
(MIMO) relay system with estimated channel state information (CSI). A simple
four phase protocol is used for training and OSTBC data transmission. Decoding
of OSTBC data at a user terminal is performed by replacing the exact CSI by the
estimated CSI, in a maximum likelihood decoder. Tight approximations for the
moment generating function (m.g.f.) of the received signal-to-noise ratio at a
user is derived under Rayleigh fading by ignoring the higher order noise terms.
Analytical average error performance of the considered cooperative scheme is
derived by using the m.g.f. expression. Moreover, the analytical diversity
order of the considered scheme is also obtained for certain system
configurations. It is shown by simulations and analysis that the channel
estimation does not affect the diversity order of the OSTBC based two-way AF
MIMO relay system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1109</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1109</id><created>2014-07-03</created><updated>2015-01-29</updated><authors><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Vukobratovic</keyname><forenames>Dejan</forenames></author><author><keyname>Crnojevic</keyname><forenames>Vladimir</forenames></author></authors><title>Cooperative Slotted Aloha for Multi-Base Station Systems</title><categories>cs.IT math.IT</categories><comments>extended version of a paper submitted for journal publication;
  revised Nov 6, 2014, and Jan 24, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework to study slotted Aloha with cooperative base
stations. Assuming a geographic-proximity communication model, we propose
several decoding algorithmswith different degrees of base stations' cooperation
(non-cooperative, spatial, temporal, and spatio-temporal). With spatial
cooperation, neighboring base stations inform each other whenever they collect
a user within their coverage overlap; temporal cooperation corresponds to
(temporal) successive interference cancellation done locally at each station.
We analyze the four decoding algorithms and establish several fundamental
results. With all algorithms, the peak throughput (average number of decoded
users per slot, across all base stations) increases linearly with the number of
base stations. Further, temporal and spatio-temporal cooperations exhibit a
threshold behavior with respect to the normalized load (number of users per
station, per slot). There exists a positive load $G^\star$, such that, below
$G^\star$, the decoding probability is asymptotically maximal possible, equal
the probability that a user is heard by at least one base station; with
non-cooperative decoding and spatial cooperation, we show that $G^\star$ is
zero. Finally, with spatio-temporal cooperation, we optimize the degree
distribution according to which users transmit their packet replicas; the
optimum is in general very different from the corresponding optimal
distribution of the single-base station system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1112</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1112</id><created>2014-07-03</created><authors><author><keyname>Bhatnagar</keyname><forenames>Manav R.</forenames></author></authors><title>On the Capacity of Decode-and-Forward Relaying over Rician Fading
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we derive the probability density function (PDF) and
cumulative distribution function (CDF) of the minimum of two non-central
Chi-square random variables with two degrees of freedom in terms of power
series. With the help of the derived PDF and CDF, we obtain the exact ergodic
capacity of the following adaptive protocols in a decode-and-forward (DF)
cooperative system over dissimilar Rician fading channels: (i) constant power
with optimal rate adaptation; (ii) optimal simultaneous power and rate
adaptation; (iii) channel inversion with fixed rate. By using the analytical
expressions of the capacity, it is observed that the optimal power and rate
adaptation provides better capacity than the optimal rate adaptation with
constant power from low to moderate signal-to-noise ratio values over
dissimilar Rician fading channels. Despite low complexity, the channel
inversion based adaptive transmission is shown to suffer from significant loss
in capacity as compared to the other adaptive transmission based techniques
over DF Rician channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1116</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1116</id><created>2014-07-03</created><authors><author><keyname>Berry</keyname><forenames>Jonathan W.</forenames></author><author><keyname>Fostvedt</keyname><forenames>Luke A.</forenames></author><author><keyname>Nordman</keyname><forenames>Daniel J.</forenames></author><author><keyname>Phillips</keyname><forenames>Cynthia A.</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Wilson</keyname><forenames>Alyson G.</forenames></author></authors><title>Why do simple algorithms for triangle enumeration work in the real
  world?</title><categories>cs.SI cs.DM cs.DS physics.soc-ph</categories><comments>Conference version in Innovations of Theoretical Computer Science
  (ITCS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Listing all triangles is a fundamental graph operation. Triangles can have
important interpretations in real-world graphs, especially social and other
interaction networks. Despite the lack of provably efficient (linear, or
slightly super-linear) worst-case algorithms for this problem, practitioners
run simple, efficient heuristics to find all triangles in graphs with millions
of vertices. How are these heuristics exploiting the structure of these special
graphs to provide major speedups in running time?
  We study one of the most prevalent algorithms used by practitioners. A
trivial algorithm enumerates all paths of length $2$, and checks if each such
path is incident to a triangle. A good heuristic is to enumerate only those
paths of length $2$ where the middle vertex has the lowest degree. It is easily
implemented and is empirically known to give remarkable speedups over the
trivial algorithm.
  We study the behavior of this algorithm over graphs with heavy-tailed degree
distributions, a defining feature of real-world graphs. The erased
configuration model (ECM) efficiently generates a graph with asymptotically
(almost) any desired degree sequence. We show that the expected running time of
this algorithm over the distribution of graphs created by the ECM is controlled
by the $\ell_{4/3}$-norm of the degree sequence. Norms of the degree sequence
are a measure of the heaviness of the tail, and it is precisely this feature
that allows non-trivial speedups of simple triangle enumeration algorithms. As
a corollary of our main theorem, we prove expected linear-time performance for
degree sequences following a power law with exponent $\alpha \geq 7/3$, and
non-trivial speedup whenever $\alpha \in (2,3)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1120</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1120</id><created>2014-07-04</created><updated>2014-11-16</updated><authors><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author></authors><title>From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for
  SPD Matrices</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing images and videos with Symmetric Positive Definite (SPD)
matrices and considering the Riemannian geometry of the resulting space has
proven beneficial for many recognition tasks. Unfortunately, computation on the
Riemannian manifold of SPD matrices --especially of high-dimensional ones--
comes at a high cost that limits the applicability of existing techniques. In
this paper we introduce an approach that lets us handle high-dimensional SPD
matrices by constructing a lower-dimensional, more discriminative SPD manifold.
To this end, we model the mapping from the high-dimensional SPD manifold to the
low-dimensional one with an orthonormal projection. In particular, we search
for a projection that yields a low-dimensional manifold with maximum
discriminative power encoded via an affinity-weighted similarity measure based
on metrics on the manifold. Learning can then be expressed as an optimization
problem on a Grassmann manifold. Our evaluation on several classification tasks
shows that our approach leads to a significant accuracy gain over
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1121</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1121</id><created>2014-07-04</created><authors><author><keyname>Ma</keyname><forenames>Qiang</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Sandler</keyname><forenames>Mark</forenames></author></authors><title>Frugal Streaming for Estimating Quantiles:One (or two) memory suffices</title><categories>cs.DB cs.DS</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern applications require processing streams of data for estimating
statistical quantities such as quantiles with small amount of memory. In many
such applications, in fact, one needs to compute such statistical quantities
for each of a large number of groups, which additionally restricts the amount
of memory available for the stream for any particular group. We address this
challenge and introduce frugal streaming, that is algorithms that work with
tiny -- typically, sub-streaming -- amount of memory per group.
  We design a frugal algorithm that uses only one unit of memory per group to
compute a quantile for each group. For stochastic streams where data items are
drawn from a distribution independently, we analyze and show that the algorithm
finds an approximation to the quantile rapidly and remains stably close to it.
We also propose an extension of this algorithm that uses two units of memory
per group. We show with extensive experiments with real world data from HTTP
trace and Twitter that our frugal algorithms are comparable to existing
streaming algorithms for estimating any quantile, but these existing algorithms
use far more space per group and are unrealistic in frugal applications;
further, the two memory frugal algorithm converges significantly faster than
the one memory algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1123</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1123</id><created>2014-07-04</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author></authors><title>Expanding the Family of Grassmannian Kernels: An Embedding Perspective</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling videos and image-sets as linear subspaces has proven beneficial for
many visual recognition tasks. However, it also incurs challenges arising from
the fact that linear subspaces do not obey Euclidean geometry, but lie on a
special type of Riemannian manifolds known as Grassmannian. To leverage the
techniques developed for Euclidean spaces (e.g, support vector machines) with
subspaces, several recent studies have proposed to embed the Grassmannian into
a Hilbert space by making use of a positive definite kernel. Unfortunately,
only two Grassmannian kernels are known, none of which -as we will show- is
universal, which limits their ability to approximate a target function
arbitrarily well. Here, we introduce several positive definite Grassmannian
kernels, including universal ones, and demonstrate their superiority over
previously-known kernels in various tasks, such as classification, clustering,
sparse coding and hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1133</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1133</id><created>2014-07-04</created><authors><author><keyname>Arora</keyname><forenames>Palvi</forenames></author><author><keyname>Bhalla</keyname><forenames>Tarun</forenames></author></authors><title>A Synonym Based Approach of Data Mining in Search Engine Optimization</title><categories>cs.IR</categories><comments>5 pages, 2figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>Palvi Arora , Tarun Bhalla.&quot;A Synonym Based Approach of Data
  Mining in Search Engine Optimization&quot;. International Journal of Computer
  Trends and Technology (IJCTT) V12(4):201-205, June 2014. ISSN:2231-2803.
  www.ijcttjournal.org</journal-ref><doi>10.14445/22312803/IJCTT-V12P140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In todays era with the rapid growth of information on the web, makes users
turn to search engines as a replacement of traditional media. This makes
sorting of particular information through billions of webpages and displaying
the relevant data makes the task tough for the search engine. Remedy for this
is SEO i.e. having a website optimized in such a way that it will display the
relevant webpages based on ranking. This is the main reason that makes search
engine optimization a prominent position in online world. This paper present a
synonym based data mining approach for SEO that makes the task of improving the
ranking of the website much easier way and user will get answer to their query
easily through any of search engine available in market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1140</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1140</id><created>2014-07-04</created><updated>2014-09-16</updated><authors><author><keyname>P&#x105;k</keyname><forenames>Karol</forenames><affiliation>University of Bialystok, Institute of Computer Science, Bialystok, Poland</affiliation></author></authors><title>Improving legibility of natural deduction proofs is not trivial</title><categories>cs.LO</categories><comments>33 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  18, 2014) lmcs:850</journal-ref><doi>10.2168/LMCS-10(3:23)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In formal proof checking environments such as Mizar it is not merely the
validity of mathematical formulas that is evaluated in the process of adoption
to the body of accepted formalizations, but also the readability of the proofs
that witness validity. As in case of computer programs, such proof scripts may
sometimes be more and sometimes be less readable. To better understand the
notion of readability of formal proofs, and to assess and improve their
readability, we propose in this paper a method of improving proof readability
based on Behaghel's First Law of sentence structure. Our method maximizes the
number of local references to the directly preceding statement in a proof
linearisation. It is shown that our optimization method is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1151</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1151</id><created>2014-07-04</created><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wu</keyname><forenames>Jianxin</forenames></author></authors><title>Optimizing Ranking Measures for Compact Binary Code Learning</title><categories>cs.LG cs.CV</categories><comments>Appearing in Proc. European Conference on Computer Vision 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hashing has proven a valuable tool for large-scale information retrieval.
Despite much success, existing hashing methods optimize over simple objectives
such as the reconstruction error or graph Laplacian related loss functions,
instead of the performance evaluation criteria of interest---multivariate
performance measures such as the AUC and NDCG. Here we present a general
framework (termed StructHash) that allows one to directly optimize multivariate
performance measures. The resulting optimization problem can involve
exponentially or infinitely many variables and constraints, which is more
challenging than standard structured output learning. To solve the StructHash
optimization problem, we use a combination of column generation and
cutting-plane techniques. We demonstrate the generality of StructHash by
applying it to ranking prediction and image retrieval, and show that it
outperforms a few state-of-the-art hashing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1164</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1164</id><created>2014-07-04</created><authors><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Uthaicharoenpong</keyname><forenames>Tawit</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Danwei</forenames></author></authors><title>Throughput Optimal Distributed Traffic Signal Control</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1205.5938</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed algorithm for controlling traffic signals, allowing
constraints such as periodic switching sequences of phases and minimum and
maximum green time to be incorporated. Our algorithm is adapted from
backpressure routing, which has been mainly applied to communication and power
networks. We formally prove that our algorithm ensures global optimality as it
leads to maximum network throughput even though the controller is constructed
and implemented in a completely distributed manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1165</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1165</id><created>2014-07-04</created><authors><author><keyname>Bordea</keyname><forenames>Prashant</forenames></author><author><keyname>Varpeb</keyname><forenames>Amarsinh</forenames></author><author><keyname>Manzac</keyname><forenames>Ramesh</forenames></author><author><keyname>Yannawara</keyname><forenames>Pravin</forenames></author></authors><title>Recognition of Isolated Words using Zernike and MFCC features for Audio
  Visual Speech Recognition</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic Speech Recognition (ASR) by machine is an attractive research topic
in signal processing domain and has attracted many researchers to contribute in
this area. In recent year, there have been many advances in automatic speech
reading system with the inclusion of audio and visual speech features to
recognize words under noisy conditions. The objective of audio-visual speech
recognition system is to improve recognition accuracy. In this paper we
computed visual features using Zernike moments and audio feature using Mel
Frequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of
Independent Standard Words) dataset which contains collection of isolated set
of city names of 10 speakers. The visual features were normalized and dimension
of features set was reduced by Principal Component Analysis (PCA) in order to
recognize the isolated word utterance on PCA space.The performance of
recognition of isolated words based on visual only and audio only features
results in 63.88 and 100 respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1166</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1166</id><created>2014-07-04</created><authors><author><keyname>Bhatnagar</keyname><forenames>Manav R.</forenames></author></authors><title>On the Capacity of CSI Based Transmission Link Selection in
  Decode-and-Forward Cooperative System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of best transmission link selection in a
decode-and-forward (DF) cooperative system from capacity point of view. The
transmission link can be a cooperative (via a relay) or direct link between the
source and destination nodes. In a two-hop DF system with multiple relays and a
direct link in between the source and destination, the transmission link
selection can be performed based on full or partial channel state information
(CSI) of all links involved in cooperation. We derive analytical ergodic
capacity of full and partial CSI based path selection schemes in the DF
cooperative system. Further, the full and partial CSI based link selection
schemes are compared with help of these expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1167</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1167</id><created>2014-07-04</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Davide</forenames></author><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Proietti</keyname><forenames>Guido</forenames></author></authors><title>Specializations and Generalizations of the Stackelberg Minimum Spanning
  Tree Game</title><categories>cs.GT cs.DS</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let be given a graph $G=(V,E)$ whose edge set is partitioned into a set $R$
of \emph{red} edges and a set $B$ of \emph{blue} edges, and assume that red
edges are weighted and form a spanning tree of $G$. Then, the \emph{Stackelberg
Minimum Spanning Tree} (\stack) problem is that of pricing (i.e., weighting)
the blue edges in such a way that the total weight of the blue edges selected
in a minimum spanning tree of the resulting graph is maximized. \stack \ is
known to be \apx-hard already when the number of distinct red weights is 2. In
this paper we analyze some meaningful specializations and generalizations of
\stack, which shed some more light on the computational complexity of the
problem. More precisely, we first show that if $G$ is restricted to be
\emph{complete}, then the following holds: (i) if there are only 2 distinct red
weights, then the problem can be solved optimally (this contrasts with the
corresponding \apx-hardness of the general problem); (ii) otherwise, the
problem can be approximated within $7/4 + \epsilon$, for any $\epsilon &gt; 0$.
Afterwards, we define a natural extension of \stack, namely that in which blue
edges have a non-negative \emph{activation cost} associated, and it is given a
global \emph{activation budget} that must not be exceeded when pricing blue
edges. Here, after showing that the very same approximation ratio as that of
the original problem can be achieved, we prove that if the spanning tree of red
edges can be rooted so as that any root-leaf path contains at most $h$ edges,
then the problem admits a $(2h+\epsilon)$-approximation algorithm, for any
$\epsilon &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1170</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1170</id><created>2014-07-04</created><authors><author><keyname>Wang</keyname><forenames>Zhijian</forenames></author><author><keyname>Xu</keyname><forenames>Bin</forenames></author></authors><title>Incentive and stability in the Rock-Paper-Scissors game: an experimental
  investigation</title><categories>physics.soc-ph cs.GT</categories><comments>19 pages, 14 figures, Keywords: experimental economics, conditional
  response, best response, win-stay-lose-shift, evolutionary game theory,
  behavior economics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a two-person Rock-Paper-Scissors (RPS) game, if we set a loss worth
nothing and a tie worth 1, and the payoff of winning (the incentive a) as a
variable, this game is called as generalized RPS game. The generalized RPS game
is a representative mathematical model to illustrate the game dynamics,
appearing widely in textbook. However, how actual motions in these games depend
on the incentive has never been reported quantitatively. Using the data from 7
games with different incentives, including 84 groups of 6 subjects playing the
game in 300-round, with random-pair tournaments and local information recorded,
we find that, both on social and individual level, the actual motions are
changing continuously with the incentive. More expressively, some
representative findings are, (1) in social collective strategy transit views,
the forward transition vector field is more and more centripetal as the
stability of the system increasing; (2) In the individual behavior of strategy
transit view, there exists a phase transformation as the stability of the
systems increasing, and the phase transformation point being near the standard
RPS; (3) Conditional response behaviors are structurally changing accompanied
by the controlled incentive. As a whole, the best response behavior increases
and the win-stay lose-shift (WSLS) behavior declines with the incentive.
Further, the outcome of win, tie, and lose influence the best response behavior
and WSLS behavior. Both as the best response behavior, the win-stay behavior
declines with the incentive while the lose-left-shift behavior increase with
the incentive. And both as the WSLS behavior, the lose-left-shift behavior
increase with the incentive, but the lose-right-shift behaviors declines with
the incentive. We hope to learn which one in tens of learning models can
interpret the empirical observation above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1176</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1176</id><created>2014-07-04</created><authors><author><keyname>Llinares</keyname><forenames>Felipe</forenames></author><author><keyname>Sugiyama</keyname><forenames>Mahito</forenames></author><author><keyname>Borgwardt</keyname><forenames>Karsten M.</forenames></author></authors><title>Identifying Higher-order Combinations of Binary Features</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding statistically significant interactions between binary variables is
computationally and statistically challenging in high-dimensional settings, due
to the combinatorial explosion in the number of hypotheses. Terada et al.
recently showed how to elegantly address this multiple testing problem by
excluding non-testable hypotheses. Still, it remains unclear how their approach
scales to large datasets.
  We here proposed strategies to speed up the approach by Terada et al. and
evaluate them thoroughly in 11 real-world benchmark datasets. We observe that
one approach, incremental search with early stopping, is orders of magnitude
faster than the current state-of-the-art approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1199</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1199</id><created>2014-07-04</created><authors><author><keyname>Soul&#xe9;</keyname><forenames>Robert</forenames></author><author><keyname>Basu</keyname><forenames>Shrutarshi</forenames></author><author><keyname>Marandi</keyname><forenames>Parisa Jalili</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Sirer</keyname><forenames>Emin G&#xfc;n</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author></authors><title>Merlin: A Language for Provisioning Network Resources</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Merlin, a new framework for managing resources in
software-defined networks. With Merlin, administrators express high-level
policies using programs in a declarative language. The language includes
logical predicates to identify sets of packets, regular expressions to encode
forwarding paths, and arithmetic formulas to specify bandwidth constraints. The
Merlin compiler uses a combination of advanced techniques to translate these
policies into code that can be executed on network elements including a
constraint solver that allocates bandwidth using parameterizable heuristics. To
facilitate dynamic adaptation, Merlin provides mechanisms for delegating
control of sub-policies and for verifying that modifications made to
sub-policies do not violate global constraints. Experiments demonstrate the
expressiveness and scalability of Merlin on real-world topologies and
applications. Overall, Merlin simplifies network administration by providing
high-level abstractions for specifying network policies and scalable
infrastructure for enforcing them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1201</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1201</id><created>2014-07-04</created><authors><author><keyname>P&#x142;o&#x144;ski</keyname><forenames>Piotr</forenames></author><author><keyname>Zaremba</keyname><forenames>Krzysztof</forenames></author></authors><title>Improving Performance of Self-Organising Maps with Distance Metric
  Learning Method</title><categories>cs.LG cs.NE</categories><comments>9 pages, 2 figures</comments><journal-ref>Artificial Intelligence and Soft Computing, Lecture Notes in
  Computer Science Volume 7267, 2012, pp 169-177</journal-ref><doi>10.1007/978-3-642-29347-4_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-Organising Maps (SOM) are Artificial Neural Networks used in Pattern
Recognition tasks. Their major advantage over other architectures is human
readability of a model. However, they often gain poorer accuracy. Mostly used
metric in SOM is the Euclidean distance, which is not the best approach to some
problems. In this paper, we study an impact of the metric change on the SOM's
performance in classification problems. In order to change the metric of the
SOM we applied a distance metric learning method, so-called 'Large Margin
Nearest Neighbour'. It computes the Mahalanobis matrix, which assures small
distance between nearest neighbour points from the same class and separation of
points belonging to different classes by large margin. Results are presented on
several real data sets, containing for example recognition of written digits,
spoken letters or faces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1208</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1208</id><created>2014-07-04</created><authors><author><keyname>Bojanowski</keyname><forenames>Piotr</forenames></author><author><keyname>Lajugie</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Laptev</keyname><forenames>Ivan</forenames></author><author><keyname>Ponce</keyname><forenames>Jean</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author><author><keyname>Sivic</keyname><forenames>Josef</forenames></author></authors><title>Weakly Supervised Action Labeling in Videos Under Ordering Constraints</title><categories>cs.CV cs.LG</categories><comments>17 pages, completed version of a ECCV2014 conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are given a set of video clips, each one annotated with an {\em ordered}
list of actions, such as &quot;walk&quot; then &quot;sit&quot; then &quot;answer phone&quot; extracted from,
for example, the associated text script. We seek to temporally localize the
individual actions in each clip as well as to learn a discriminative classifier
for each action. We formulate the problem as a weakly supervised temporal
assignment with ordering constraints. Each video clip is divided into small
time intervals and each time interval of each video clip is assigned one action
label, while respecting the order in which the action labels appear in the
given annotations. We show that the action label assignment can be determined
together with learning a classifier for each action in a discriminative manner.
We evaluate the proposed model on a new and challenging dataset of 937 video
clips with a total of 787720 frames containing sequences of 16 different
actions from 69 Hollywood movies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1209</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1209</id><created>2014-07-04</created><updated>2015-05-27</updated><authors><author><keyname>Corr&#xea;a</keyname><forenames>Ricardo C.</forenames></author><author><keyname>Michelon</keyname><forenames>Philippe</forenames></author><author><keyname>Cun</keyname><forenames>Bertrand Le</forenames></author><author><keyname>Mautor</keyname><forenames>Thierry</forenames></author><author><keyname>Donne</keyname><forenames>Diego Delle</forenames></author></authors><title>A Bit-Parallel Russian Dolls Search for a Maximum Cardinality Clique in
  a Graph</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the clique of maximum cardinality in an arbitrary graph is an NP-Hard
problem that has many applications, which has motivated studies to solve it
exactly despite its difficulty. The great majority of algorithms proposed in
the literature are based on the Branch and Bound method. In this paper, we
propose an exact algorithm for the maximum clique problem based on the Russian
Dolls Search method. When compared to Branch and Bound, the main difference of
the Russian Dolls method is that the nodes of its search tree correspond to
decision subproblems, instead of the optimization subproblems of the Branch and
Bound method. In comparison to a first implementation of this Russian Dolls
method from the literature, several improvements are presented. Some of them
are adaptations of techniques already employed successfully in Branch and Bound
algorithms, like the use of approximate coloring for pruning purposes and
bit-parallel operations. Two different coloring heuristics are tested: the
standard greedy and the greedy with recoloring. Other improvements are directly
related to the Russian Dolls scheme: the adoption of recursive calls where each
subproblem (doll) is solved itself via the same principles than the Russian
Dolls Search and the application of an elimination rule allowing not to
generate a significant number of dolls. Results of computational experiments
show that the algorithm outperforms the best exact combinatorial algorithms in
the literature for the great majority of the dense graphs tested, being more
than twice faster in several cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1232</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1232</id><created>2014-06-19</created><authors><author><keyname>Dertli</keyname><forenames>Abdullah</forenames></author><author><keyname>Cengellenmis</keyname><forenames>Yasemin</forenames></author><author><keyname>Eren</keyname><forenames>Senol</forenames></author></authors><title>On Quantum Codes Obtained From Cyclic Codes Over F_2+vF_2+v^2F_2</title><categories>cs.IT math.IT math.RA quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new Gray map which is both an isometry and a weight preserving map from
R=F_2+vF_2+v^2F_2 to (F_2)^3 is defined. A construction for quantum error
correcting codes from cyclic codes over finite ring R=F_2+vF_2+v^2F_2, v^3=v is
given. The parameters of quantum codes which are obtained from cyclic codes
over R are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1237</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1237</id><created>2014-07-04</created><authors><author><keyname>Kumar</keyname><forenames>Vinit</forenames></author><author><keyname>Agarwal</keyname><forenames>Ajay</forenames></author></authors><title>HT-Paxos: High Throughput State-Machine Replication Protocol for Large
  Clustered Data Centers</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paxos is a prominent theory of state machine replication. Recent data
intensive Systems those implement state machine replication generally require
high throughput. Earlier versions of Paxos as few of them are classical Paxos,
fast Paxos and generalized Paxos have a major focus on fault tolerance and
latency but lacking in terms of throughput and scalability. A major reason for
this is the heavyweight leader. Through offloading the leader, we can further
increase throughput of the system. Ring Paxos, Multi Ring Paxos and S-Paxos are
few prominent attempts in this direction for clustered data centers. In this
paper, we are proposing HT-Paxos, a variant of Paxos that one is the best
suitable for any large clustered data center. HT-Paxos further offloads the
leader very significantly and hence increases the throughput and scalability of
the system. While at the same time, among high throughput state-machine
replication protocols, HT-Paxos provides reasonably low latency and response
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1239</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1239</id><created>2014-07-04</created><updated>2015-01-26</updated><authors><author><keyname>Liu</keyname><forenames>Shuhao</forenames></author><author><keyname>Bai</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Cai</keyname><forenames>Zhiping</forenames></author></authors><title>RepNet: Cutting Tail Latency in Data Center Networks with Flow
  Replication</title><categories>cs.NI cs.DC cs.SY</categories><acm-class>C.2.1; C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data center networks need to provide low latency, especially at the tail, as
demanded by many interactive applications. To improve tail latency, existing
approaches require modifications to switch hardware and/or end-host operating
systems, making them difficult to be deployed. We present the design,
implementation, and evaluation of RepNet, an application layer transport that
can be deployed today. RepNet exploits the fact that only a few paths among
many are congested at any moment in the network, and applies simple flow
replication to mice flows to opportunistically use the less congested path.
RepNet has two designs for flow replication: (1) RepSYN, which only replicates
SYN packets and uses the first connection that finishes TCP handshaking for
data transmission, and (2) RepFlow which replicates the entire mice flow. We
implement RepNet on {\tt node.js}, one of the most commonly used platforms for
networked interactive applications. {\tt node}'s single threaded event-loop and
non-blocking I/O make flow replication highly efficient. Performance evaluation
on a real network testbed and in Mininet reveals that RepNet is able to reduce
the tail latency of mice flows, as well as application completion times, by
more than 50\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1245</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1245</id><created>2014-07-04</created><authors><author><keyname>Schill</keyname><forenames>Mischael</forenames></author><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Dynamic Checking of Safe Concurrent Memory Access using Shared Ownership</title><categories>cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In shared-memory concurrent programming, shared resources can be protected
using synchronization mechanisms such as monitors or channels. The connection
between these mechanisms and the resources they protect is, however, only given
implicitly; this makes it difficult both for programmers to apply the
mechanisms correctly and for compilers to check that resources are properly
protected. This paper presents a mechanism to automatically check that shared
memory is accessed properly, using a methodology called shared ownership. In
contrast to traditional ownership, shared ownership offers more flexibility by
permitting multiple owners of a resource. On the basis of this methodology, we
define an abstract model of resource access that provides operations to manage
data dependencies, as well as sharing and transfer of access privileges. The
model is rigorously defined using a formal semantics, and shown to be free from
data races. This property can be used to detect unsafe memory accesses when
simulating the model together with the execution of a program. The
expressiveness and efficiency of the approach is demonstrated on a variety of
programs using common synchronization mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1255</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1255</id><created>2014-07-04</created><updated>2015-01-14</updated><authors><author><keyname>Lokhov</keyname><forenames>Andrey Y.</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Dynamic message-passing equations for models with unidirectional
  dynamics</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>Final version</comments><journal-ref>Phys. Rev. E 91, 012811 (2015)</journal-ref><doi>10.1103/PhysRevE.91.012811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding and quantifying the dynamics of disordered out-of-equilibrium
models is an important problem in many branches of science. Using the dynamic
cavity method on time trajectories, we construct a general procedure for
deriving the dynamic message-passing equations for a large class of models with
unidirectional dynamics, which includes the zero-temperature random field Ising
model, the susceptible-infected-recovered model, and rumor spreading models. We
show that unidirectionality of the dynamics is the key ingredient that makes
the problem solvable. These equations are applicable to single instances of the
corresponding problems with arbitrary initial conditions, and are
asymptotically exact for problems defined on locally tree-like graphs. When
applied to real-world networks, they generically provide a good analytic
approximation of the real dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1257</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1257</id><created>2014-07-04</created><authors><author><keyname>Pandiyavathi</keyname><forenames>T.</forenames></author></authors><title>Usage of Optimal Restructuring Plan in Detection of Code Smells</title><categories>cs.SE</categories><comments>5 pages Published with International Journal of Computer Trends and
  Technology</comments><journal-ref>International Journal of Computer Trends and Technology volume 12
  number 4 Jun 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To remain useful for their users, software systems need to continuously
enhance and extend their functionality. Nevertheless, in many object-oriented
applications, features are not represented explicitly. The lack of
modularization is known to make application features difficult to locate, to
comprehend and to modify in isolation from one another. In our work, we
implement restructuring using Featureous plug-in where we can change the
classes between packages. In the previous literature works, lot of complexities
arises while using the tool due to dependencies. Changes to the code without
knowing the root cause of the problem leads to further production of new
errors. So, we aim in finding the restructuring candidates which have to be
rearranged, thereby applying changes to the code on those parts using the tool
helps in ordered arrangement of the source code. Applying modularization to the
restructuring candidates will lead to decrease in the human effort as well as
tool effort in restructuring. Unwanted evolution of new errors will be
eliminated
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1267</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1267</id><created>2014-07-04</created><authors><author><keyname>Fu</keyname><forenames>Qiang</forenames></author><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Calibration of Multiple Fish-Eye Cameras Using a Wand</title><categories>cs.CV</categories><comments>23 pages, 9 figures, submitted to IET Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fish-eye cameras are becoming increasingly popular in computer vision, but
their use for 3D measurement is limited partly due to the lack of an accurate,
efficient and user-friendly calibration procedure. For such a purpose, we
propose a method to calibrate the intrinsic and extrinsic parameters (including
radial distortion parameters) of two/multiple fish-eye cameras simultaneously
by using a wand under general motions. Thanks to the generic camera model used,
the proposed calibration method is also suitable for two/multiple conventional
cameras and mixed cameras (e.g. two conventional cameras and a fish-eye
camera). Simulation and real experiments demonstrate the effectiveness of the
proposed method. Moreover, we develop the camera calibration toolbox, which is
available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1270</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1270</id><created>2014-07-04</created><updated>2015-05-19</updated><authors><author><keyname>Burger</keyname><forenames>Reinhold</forenames></author><author><keyname>Heinle</keyname><forenames>Albert</forenames></author></authors><title>A New Primitive for a Diffie-Hellman-like Key Exchange Protocol Based on
  Multivariate Ore Polynomials</title><categories>cs.CR cs.SC math.RA</categories><msc-class>94A60, 68P25, 47N99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new primitive for a key exchange protocol based on
multivariate non-commutative polynomial rings, analogous to the classic
Diffie-Hellman method. Our technique extends the proposed scheme of Boucher et
al. from 2010. Their method was broken by Dubois and Kammerer in 2011, who
exploited the Euclidean domain structure of the chosen ring. However, our
proposal is immune against such attacks, without losing the advantages of
non-commutative polynomial rings as outlined by Boucher et al. Moreover, our
extension is not restricted to any particular ring, but is designed to allow
users to readily choose from a large class of rings when applying the protocol.
Our primitive can also be applied to other cryptographic paradigms. In
particular, we develop a three-pass protocol, a public key cryptosystem, a
digital signature scheme and a zero-knowledge proof protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1276</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1276</id><created>2014-07-04</created><authors><author><keyname>Abdo</keyname><forenames>Hosam</forenames></author><author><keyname>Dimitrov</keyname><forenames>Darko</forenames></author></authors><title>Non-regular graphs with minimal total irregularity</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\it total irregularity} of a simple undirected graph $G$ is defined as
${\rm irr}_t(G) =$ $\frac{1}{2}\sum_{u,v \in V(G)}$ $\left| d_G(u)-d_G(v)
\right|$, where $d_G(u)$ denotes the degree of a vertex $u \in V(G)$.
Obviously, ${\rm irr}_t(G)=0$ if and only if $G$ is regular. Here, we
characterize the non-regular graphs with minimal total irregularity and thereby
resolve the recent conjecture by Zhu, You and Yang~\cite{zyy-mtig-2014} about
the lower bound on the minimal total irregularity of non-regular connected
graphs. We show that the conjectured lower bound of $2n-4$ is attained only if
non-regular connected graphs of even order are considered, while the sharp
lower bound of $n-1$ is attained by graphs of odd order. We also characterize
the non-regular graphs with the second and the third smallest total
irregularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1285</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1285</id><created>2014-07-04</created><authors><author><keyname>Satyanarayana</keyname><forenames>J V</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A G</forenames></author></authors><title>Compressed EEG Acquisition with Limited Channels using Estimated Signal
  Correlation</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nearby scalp channels in multi-channel EEG data exhibit high correlation. A
question that naturally arises is whether it is required to record signals from
all the electrodes in a group of closely spaced electrodes in a typical
measurement setup. One could save on the number of channels that are recorded,
if it were possible to reconstruct the omitted channels to the accuracy needed
for identifying the relevant information (say, spectral content in the signal),
required to carry out a preliminary diagnosis. We address this problem from a
compressed sensing perspective and propose a measurement and reconstruction
scheme. Working with publicly available EEG database, we put our scheme to
experiment and illustrate that if it is only a matter of estimating the
frequency content of the signal in various EEG bands, then all the channels
need not be recorded. We have achieved an average error below 15% between the
original and reconstructed signals with respect to estimation of the spectral
content in the delta, theta and alpha bands. We have demonstrated that channels
in the 10-10 system of electrode placement can be estimated, with an error less
than 10% using recordings on the sparser 10-20 system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1289</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1289</id><created>2014-07-04</created><updated>2015-04-15</updated><authors><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Musco</keyname><forenames>Cameron</forenames></author><author><keyname>Musco</keyname><forenames>Christopher</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Single Pass Spectral Sparsification in Dynamic Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first single pass algorithm for computing spectral sparsifiers
of graphs in the dynamic semi-streaming model. Given a single pass over a
stream containing insertions and deletions of edges to a graph G, our algorithm
maintains a randomized linear sketch of the incidence matrix of G into
dimension O((1/epsilon^2) n polylog(n)). Using this sketch, at any point, the
algorithm can output a (1 +/- epsilon) spectral sparsifier for G with high
probability.
  While O((1/epsilon^2) n polylog(n)) space algorithms are known for computing
&quot;cut sparsifiers&quot; in dynamic streams [AGM12b, GKP12] and spectral sparsifiers
in &quot;insertion-only&quot; streams [KL11], prior to our work, the best known single
pass algorithm for maintaining spectral sparsifiers in dynamic streams required
sketches of dimension Omega((1/epsilon^2) n^(5/3)) [AGM14].
  To achieve our result, we show that, using a coarse sparsifier of G and a
linear sketch of G's incidence matrix, it is possible to sample edges by
effective resistance, obtaining a spectral sparsifier of arbitrary precision.
Sampling from the sketch requires a novel application of ell_2/ell_2 sparse
recovery, a natural extension of the ell_0 methods used for cut sparsifiers in
[AGM12b]. Recent work of [MP12] on row sampling for matrix approximation gives
a recursive approach for obtaining the required coarse sparsifiers.
  Under certain restrictions, our approach also extends to the problem of
maintaining a spectral approximation for a general matrix A^T A given a stream
of updates to rows in A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1291</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1291</id><created>2014-07-04</created><updated>2014-09-17</updated><authors><author><keyname>Dimitrov</keyname><forenames>Stoyan</forenames></author><author><keyname>Lguensat</keyname><forenames>Redouane</forenames></author></authors><title>Reinforcement Learning Based Algorithm for the Maximization of EV
  Charging Station Revenue</title><categories>cs.CE cs.LG math.OC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an online reinforcement learning based application which
increases the revenue of one particular electric vehicles (EV) station,
connected to a renewable source of energy. Moreover, the proposed application
adapts to changes in the trends of the station's average number of customers
and their types. Most of the parameters in the model are simulated
stochastically and the algorithm used is a Q-learning algorithm. A computer
simulation was implemented which demonstrates and confirms the utility of the
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1303</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1303</id><created>2014-07-04</created><updated>2016-01-06</updated><authors><author><keyname>Asante-Asamani</keyname><forenames>E. O.</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>A Cylindrical Basis Function for Solving Partial Differential Equations
  on Manifolds</title><categories>math.NA cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical solutions of partial differential equations (PDEs) on manifolds
continues to generate a lot of interest among scientists in the natural and
applied sciences. On the other hand, recent developments of 3D scanning and
computer vision technologies have produced a large number of 3D surface models
represented as point clouds. Herein, we develop a simple and efficient method
for solving PDEs on closed surfaces represented as point clouds. By projecting
the radial vector of standard radial basis function(RBF) kernels onto the local
tangent plane, we are able to produce a representation of functions that
permits the replacement of surface differential operators with their Cartesian
equivalent. We demonstrate, numerically, the efficiency of the method in
discretizing the Laplace Beltrami operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1307</identifier>
 <datestamp>2014-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1307</id><created>2014-07-04</created><authors><author><keyname>Guan</keyname><forenames>Yang</forenames></author><author><keyname>Xiao</keyname><forenames>Yao</forenames></author><author><keyname>Feng</keyname><forenames>Hao</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author><author><keyname>Cimini</keyname><forenames>Leonard J.</forenames><suffix>Jr</suffix></author></authors><title>MobiCacher: Mobility-Aware Content Caching in Small-Cell Networks</title><categories>cs.NI</categories><comments>Accepted by Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small-cell networks have been proposed to meet the demand of ever growing
mobile data traffic. One of the prominent challenges faced by small-cell
networks is the lack of sufficient backhaul capacity to connect small-cell base
stations (small-BSs) to the core network. We exploit the effective application
layer semantics of both spatial and temporal locality to reduce the backhaul
traffic. Specifically, small-BSs are equipped with storage facility to cache
contents requested by users. As the {\em cache hit ratio} increases, most of
the users' requests can be satisfied locally without incurring traffic over the
backhaul. To make informed caching decisions, the mobility patterns of users
must be carefully considered as users might frequently migrate from one small
cell to another. We study the issue of mobility-aware content caching, which is
formulated into an optimization problem with the objective to maximize the
caching utility. As the problem is NP-complete, we develop a polynomial-time
heuristic solution termed {\em MobiCacher} with bounded approximation ratio. We
also conduct trace-based simulations to evaluate the performance of {\em
MobiCacher}, which show that {\em MobiCacher} yields better caching utility
than existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1328</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1328</id><created>2014-07-04</created><authors><author><keyname>Singh</keyname><forenames>Chanpreet</forenames></author><author><keyname>Singh</keyname><forenames>Kanwaldeep</forenames></author><author><keyname>Manrao</keyname><forenames>Parth</forenames></author><author><keyname>Kapoor</keyname><forenames>Rashi</forenames></author><author><keyname>Shukla</keyname><forenames>Sagar</forenames></author><author><keyname>Patel</keyname><forenames>Shivam</forenames></author><author><keyname>Preet</keyname><forenames>Simar</forenames></author><author><keyname>Alungh</keyname><forenames>Suman</forenames></author></authors><title>Toward Software Measurement and Quality Analysis of MARF and GIPSY Case
  Studies, a Team 8 SOEN6611-S14 Project Report</title><categories>cs.SE</categories><comments>49 Pages, 48 Figures, 30 Tables</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurement is an important criterion to improve the performance of a
product. This paper presents a comparative study involving measurements between
two frameworks MARF and GIPSY. Initially it establishes a thorough
understanding of these frameworks and their applications. MARF comprises of a
number of algorithms for voice and speech processing etc. GIPSY on the contrary
provides a multi lingual platform for developing compiler components. These
frameworks are meant to provide an open source environment for the programmers
or users and implement them in applications. Several metrics are used for
object-oriented design quality assessment. We use these metrics to evaluate the
code quality of both MARF and GIPSY. We describe how tools can be used to
analyze these metric values and categorize the quality of the code as excellent
or worse. Based on these values we interpret the results in terms of quality
attributes achieved. Quantitative and qualitative analysis of metric values is
made in this regard to elaborate the impact of design parameters on the quality
of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1338</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1338</id><created>2014-07-04</created><updated>2015-11-19</updated><authors><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Extremal Mechanisms for Local Differential Privacy</title><categories>cs.IT math.IT</categories><comments>52 pages, 10 figures in JMLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local differential privacy has recently surfaced as a strong measure of
privacy in contexts where personal information remains private even from data
analysts. Working in a setting where both the data providers and data analysts
want to maximize the utility of statistical analyses performed on the released
data, we study the fundamental trade-off between local differential privacy and
utility. This trade-off is formulated as a constrained optimization problem:
maximize utility subject to local differential privacy constraints. We
introduce a combinatorial family of extremal privatization mechanisms, which we
call staircase mechanisms, and show that it contains the optimal privatization
mechanisms for a broad class of information theoretic utilities such as mutual
information and $f$-divergences. We further prove that for any utility function
and any privacy level, solving the privacy-utility maximization problem is
equivalent to solving a finite-dimensional linear program, the outcome of which
is the optimal staircase mechanism. However, solving this linear program can be
computationally expensive since it has a number of variables that is
exponential in the size of the alphabet the data lives in. To account for this,
we show that two simple privatization mechanisms, the binary and randomized
response mechanisms, are universally optimal in the low and high privacy
regimes, and well approximate the intermediate regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1339</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1339</id><created>2014-07-04</created><authors><author><keyname>Kulkarni</keyname><forenames>Tejas D.</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash K.</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Inverse Graphics with Probabilistic CAD Models</title><categories>cs.CV cs.AI stat.ML</categories><comments>For correspondence, contact tejask@mit.edu</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recently, multiple formulations of vision problems as probabilistic
inversions of generative models based on computer graphics have been proposed.
However, applications to 3D perception from natural images have focused on
low-dimensional latent scenes, due to challenges in both modeling and
inference. Accounting for the enormous variability in 3D object shape and 2D
appearance via realistic generative models seems intractable, as does inverting
even simple versions of the many-to-many computations that link 3D scenes to 2D
images. This paper proposes and evaluates an approach that addresses key
aspects of both these challenges. We show that it is possible to solve
challenging, real-world 3D vision problems by approximate inference in
generative models for images based on rendering the outputs of probabilistic
CAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D
meshes corresponding to plausible objects and apply affine transformations to
place them in a scene. Image likelihoods are based on similarity in a feature
space based on standard mid-level image representations from the vision
literature. Our inference algorithm integrates single-site and locally blocked
Metropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative
data-driven proposals learned from training data generated from our models. We
apply this approach to 3D human pose estimation and object shape reconstruction
from single images, achieving quantitative and qualitative performance
improvements over state-of-the-art baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1352</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1352</id><created>2014-07-04</created><authors><author><keyname>Zhao</keyname><forenames>Deli</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Homophilic Clustering by Locally Asymmetric Geometry</title><categories>cs.CV</categories><comments>10 pages, 24 figures</comments><msc-class>68T10, 62H30, 91C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is indispensable for data analysis in many scientific disciplines.
Detecting clusters from heavy noise remains challenging, particularly for
high-dimensional sparse data. Based on graph-theoretic framework, the present
paper proposes a novel algorithm to address this issue. The locally asymmetric
geometries of neighborhoods between data points result in a directed similarity
graph to model the structural connectivity of data points. Performing
similarity propagation on this directed graph simply by its adjacency matrix
powers leads to an interesting discovery, in the sense that if the in-degrees
are ordered by the corresponding sorted out-degrees, they will be
self-organized to be homophilic layers according to the different distributions
of cluster densities, which is dubbed the Homophilic In-degree figure (the HI
figure). With the HI figure, we can easily single out all cores of clusters,
identify the boundary between cluster and noise, and visualize the intrinsic
structures of clusters. Based on the in-degree homophily, we also develop a
simple efficient algorithm of linear space complexity to cluster noisy data.
Extensive experiments on toy and real-world scientific data validate the
effectiveness of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1355</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1355</id><created>2014-07-04</created><updated>2015-04-14</updated><authors><author><keyname>Nguyen</keyname><forenames>Hung D.</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Voltage Multistability and Pulse Emergency Control for Distribution
  System with Power Flow Reversal</title><categories>math.DS cs.SY</categories><comments>13 pages, 22 figures. IEEE Transactions on Smart Grid 2015, in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High levels of penetration of distributed generation and aggressive reactive
power compensation may result in the reversal of power flows in future
distribution grids. The voltage stability of these operating conditions may be
very different from the more traditional power consumption regime. This paper
focused on demonstration of multistability phenomenon in radial distribution
systems with reversed power flow, where multiple stable equilibria co-exist at
the given set of parameters. The system may experience transitions between
different equilibria after being subjected to disturbances such as short-term
losses of distributed generation or transient faults. Convergence to an
undesirable equilibrium places the system in an emergency or \textit{in
extremis} state. Traditional emergency control schemes are not capable of
restoring the system if it gets entrapped in one of the low voltage equilibria.
Moreover, undervoltage load shedding may have a reverse action on the system
and can induce voltage collapse. We propose a novel pulse emergency control
strategy that restores the system to the normal state without any interruption
of power delivery. The results are validated with dynamic simulations of IEEE
$13$-bus feeder performed with SystemModeler software. The dynamic models can
be also used for characterization of the solution branches via a novel approach
so-called the admittance homotopy power flow method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1360</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1360</id><created>2014-07-05</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author></authors><title>Differential Dual-Hop Relaying under User Mobility</title><categories>cs.IT math.IT</categories><comments>to appear in IET Communications Journal, 2014. arXiv admin note:
  substantial text overlap with arXiv:1403.5330</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies dual-hop amplify-and-forward relaying system employing
differential encoding and decoding over time-varying Rayleigh fading channels.
First, the convectional &quot;two-symbol&quot; differential detection (CDD) is
theoretically analysed in terms of the bit-error-rate (BER). The obtained
analysis clearly shows that performance of two-symbol differential detection
severely degrades in fast-fading channels and reaches an irreducible error
floor at high signal-to-noise ratio region. To overcome the error floor
experienced with fast-fading, a practical suboptimal &quot;multiple-symbol&quot;
detection (MSD) is designed and its performance is theoretically analysed. The
analysis of CDD and MSD are verified and illustrated with simulation results
under different fading scenarios. Specifically, the obtained results show that
the proposed MSD can significantly improve the system performance in
fast-fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1383</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1383</id><created>2014-07-05</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mahmoud H.</forenames></author><author><keyname>Tawfik</keyname><forenames>Hazim</forenames></author></authors><title>Opportunistic Beamforming using Dumb Basis Patterns in Multiple Access
  Cognitive Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate multiuser diversity in interference-limited
Multiple Access (MAC) underlay cognitive channels with Line-of-Sight
interference (LoS) from the secondary to the primary network. It is shown that
for $N$ secondary users, and assuming Rician interference channels, the
secondary sum capacity scales like
$\log\left(\frac{K^{2}+K}{\mathcal{W}\left(\frac{K e^{K}}{N}\right)}\right)$,
where $K$ is the $K$-factor of the Rician channels, and $\mathcal{W}(.)$ is the
Lambert W function. Thus, LoS interference hinders the achievable multiuser
diversity gain experienced in Rayleigh channels, where the sum capacity grows
like $\log(N)$. To overcome this problem, we propose the usage of single radio
Electronically Steerable Parasitic Array Radiator (ESPAR) antennas at the
secondary mobile terminals. Using ESPAR antennas, we induce artificial
fluctuations in the interference channels to restore the $\log(N)$ growth rate
by assigning random weights to orthogonal {\it basis patterns}. We term this
technique as {\it Random Aerial Beamforming} (RAB). While LoS interference is
originally a source of capacity hindrance, we show that using RAB, it can
actually be exploited to improve multiuser interference diversity by boosting
the {\it effective number of users} with minimal hardware complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1386</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1386</id><created>2014-07-05</created><updated>2015-06-08</updated><authors><author><keyname>Hampson</keyname><forenames>Christopher</forenames></author><author><keyname>Kurucz</keyname><forenames>Agi</forenames></author></authors><title>Undecidable propositional bimodal logics and one-variable first-order
  linear temporal logics with counting</title><categories>cs.LO</categories><acm-class>F.4.1; I.2.4</acm-class><journal-ref>ACM TOCL vol. 16(3) (2015), 27:1-27:36</journal-ref><doi>10.1145/2757285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First-order temporal logics are notorious for their bad computational
behaviour. It is known that even the two-variable monadic fragment is highly
undecidable over various linear timelines, and over branching time even
one-variable fragments might be undecidable. However, there have been several
attempts on finding well-behaved fragments of first-order temporal logics and
related temporal description logics, mostly either by restricting the available
quantifier patterns, or considering sub-Boolean languages. Here we analyse
seemingly `mild' extensions of decidable one-variable fragments with counting
capabilities, interpreted in models with constant, decreasing, and expanding
first-order domains. We show that over most classes of linear orders these
logics are (sometimes highly) undecidable, even without constant and function
symbols, and with the sole temporal operator `eventually'.
  We establish connections with bimodal logics over 2D product structures
having linear and `difference' (inequality) component relations, and prove our
results in this bimodal setting. We show a general result saying that
satisfiability over many classes of bimodal models with commuting linear and
difference relations is undecidable. As a by-product, we also obtain new
examples of finitely axiomatisable but Kripke incomplete bimodal logics. Our
results generalise similar lower bounds on bimodal logics over products of two
linear relations, and our proof methods are quite different from the proofs of
these results. Unlike previous proofs that first `diagonally encode' an
infinite grid, and then use reductions of tiling or Turing machine problems,
here we make direct use of the grid-like structure of product frames and obtain
undecidability by reductions of counter (Minsky) machine problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1395</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1395</id><created>2014-07-05</created><updated>2014-07-09</updated><authors><author><keyname>Akbari</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Vakili</keyname><forenames>Vahid Tabataba</forenames></author></authors><title>CB-REFIM: A Practical Coordinated Beamforming in Multicell Networks</title><categories>cs.IT math.IT</categories><comments>20 pages, 8 figures, to appear in IET Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance of multicell systems is inevitably limited by interference and
available resources. Although intercell interference can be mitigated by Base
Station (BS) Coordination, the demand on inter-BS information exchange and
computational complexity grows rapidly with the number of cells, subcarriers,
and users. On the other hand, some of the existing coordination beamforming
methods need computation of pseudo-inverse or generalized eigenvector of a
matrix, which are practically difficult to implement in a real system. To
handle these issues, we propose a novel linear beamforming across a set of
coordinated cells only with limiting backhaul signalling. Resource allocation
(i.e. precoding and power control) is formulated as an optimization problem
with objective function of signal-to-interference-plus-noise ratios (SINRs) in
order to maximize the instantaneous weighted sum-rate subject to power
constraints. Although the primal problem is nonconvex and difficult to be
optimally solved, an iterative algorithm is presented based on the
Karush-Kuhn-Tucker (KKT) condition. To have a practical solution with low
computational complexity and signalling overhead, we present CB-REFIM
(coordination beamforming-reference based interference management) and show the
recently proposed REFIM algorithm can be interpreted as a special case of
CB-REFIM. We evaluate CB-REFIM through extensive simulation and observe that
the proposed strategies achieve close-to-optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1399</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1399</id><created>2014-07-05</created><authors><author><keyname>Shang</keyname><forenames>Fanhua</forenames></author><author><keyname>Liu</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author></authors><title>Generalized Higher-Order Tensor Decomposition via Parallel ADMM</title><categories>cs.NA cs.LG</categories><comments>9 pages, 5 figures, AAAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order tensors are becoming prevalent in many scientific areas such as
computer vision, social network analysis, data mining and neuroscience.
Traditional tensor decomposition approaches face three major challenges: model
selecting, gross corruptions and computational efficiency. To address these
problems, we first propose a parallel trace norm regularized tensor
decomposition method, and formulate it as a convex optimization problem. This
method does not require the rank of each mode to be specified beforehand, and
can automatically determine the number of factors in each mode through our
optimization scheme. By considering the low-rank structure of the observed
tensor, we analyze the equivalent relationship of the trace norm between a
low-rank tensor and its core tensor. Then, we cast a non-convex tensor
decomposition model into a weighted combination of multiple much smaller-scale
matrix trace norm minimization. Finally, we develop two parallel alternating
direction methods of multipliers (ADMM) to solve our problems. Experimental
results verify that our regularized formulation is effective, and our methods
are robust to noise or outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1402</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1402</id><created>2014-07-05</created><authors><author><keyname>Wang</keyname><forenames>Sinong</forenames></author><author><keyname>Tian</keyname><forenames>Xiaohua</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author></authors><title>Exploiting the Unexploited of Coded Caching for Wireless Content
  Distribution: Detailed Theoretical Proofs</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies show that the coded caching technique can facilitate the
wireless content distribution by mitigating the wireless traffic rate during
the peak-traffic time, where the contents are partially prefetched to the local
cache of mobile devices during the off-peak time. The remaining contents are
then jointly coded and delivered in multicast, when many content requests are
initiated in the peak-traffic time. The requested contents can be recovered
from the local-prefetched and multicast data with requesters experiencing less
congestions. However, the benefit of the coded caching scheme is still under
estimated, where the potential gain by appropriate caching distribution is
under exploited. In this paper, we propose a theoretical model to minimize the
average wireless traffic rate required in the coded caching, for which the
optimized caching distribution is derived with the content popularity
distribution taken into account. In order to improve the computational
efficiency for determining the appropriate caching distribution, we transform
the objective function from the average wireless traffic rate into the average
size of un-prefetched contents. We theoretically show the order optimality of
the derived results from both the primal model and the relaxed one. Simulation
results show that the coded caching performance can be further improved with
the derived caching distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1403</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1403</id><created>2014-07-05</created><updated>2014-10-23</updated><authors><author><keyname>Manyem</keyname><forenames>Prabhu</forenames></author></authors><title>Decision versions of optimization problems: cardinality constraint
  (lower bound) as a CNF Horn formula for Maximum Matching</title><categories>cs.LO</categories><msc-class>90C99, 68Q19, 68Q15, 68Q17, 03C13</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a formula for the lower bound in the form of $|F| \ge K$, in such
a way that the decision version of unweighted non-bipartite matching can be
solved in polynomial time. ~The parameter $K$ can vary from instance to
instance. We assume that the domains, the set of vertices and the set of edges,
are ordered. To our knowledge, no polynomially solvable satisfiability
expression has been developed for this problem so far, or for that matter, for
any decision problem derived from optimization. Hence for such problems, this
opens up a new approach to solving them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1408</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1408</id><created>2014-07-05</created><updated>2015-02-20</updated><authors><author><keyname>Hescott</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author></authors><title>The Complexity of Reasoning with FODD and GFODD</title><categories>cs.AI cs.CC cs.LO</categories><comments>A short version of this paper appears in AAAI 2014. Version 2
  includes a reorganization and some expanded proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work introduced Generalized First Order Decision Diagrams (GFODD) as a
knowledge representation that is useful in mechanizing decision theoretic
planning in relational domains. GFODDs generalize function-free first order
logic and include numerical values and numerical generalizations of existential
and universal quantification. Previous work presented heuristic inference
algorithms for GFODDs and implemented these heuristics in systems for decision
theoretic planning. In this paper, we study the complexity of the computational
problems addressed by such implementations. In particular, we study the
evaluation problem, the satisfiability problem, and the equivalence problem for
GFODDs under the assumption that the size of the intended model is given with
the problem, a restriction that guarantees decidability. Our results provide a
complete characterization placing these problems within the polynomial
hierarchy. The same characterization applies to the corresponding restriction
of problems in first order logic, giving an interesting new avenue for
efficient inference when the number of objects is bounded. Our results show
that for $\Sigma_k$ formulas, and for corresponding GFODDs, evaluation and
satisfiability are $\Sigma_k^p$ complete, and equivalence is $\Pi_{k+1}^p$
complete. For $\Pi_k$ formulas evaluation is $\Pi_k^p$ complete, satisfiability
is one level higher and is $\Sigma_{k+1}^p$ complete, and equivalence is
$\Pi_{k+1}^p$ complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1422</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1422</id><created>2014-07-05</created><authors><author><keyname>Axenovich</keyname><forenames>Maria</forenames></author><author><keyname>Cherubini</keyname><forenames>Enrica</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Spectrum of mixed bi-uniform hypergraphs</title><categories>math.CO cs.DM</categories><comments>9 pages, 5 figures</comments><msc-class>05C15, 05C65</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mixed hypergraph is a triple $H=(V,\mathcal{C},\mathcal{D})$, where $V$ is
a set of vertices, $\mathcal{C}$ and $\mathcal{D}$ are sets of hyperedges. A
vertex-coloring of $H$ is proper if $C$-edges are not totally multicolored and
$D$-edges are not monochromatic. The feasible set $S(H)$ of $H$ is the set of
all integers, $s$, such that $H$ has a proper coloring with $s$ colors.
  Bujt\'as and Tuza [Graphs and Combinatorics 24 (2008), 1--12] gave a
characterization of feasible sets for mixed hypergraphs with all $C$- and
$D$-edges of the same size $r$, $r\geq 3$.
  In this note, we give a short proof of a complete characterization of all
possible feasible sets for mixed hypergraphs with all $C$-edges of size $\ell$
and all $D$-edges of size $m$, where $\ell, m \geq 2$. Moreover, we show that
for every sequence $(r(s))_{s=\ell}^n$, $n \geq \ell$, of natural numbers there
exists such a hypergraph with exactly $r(s)$ proper colorings using $s$ colors,
$s = \ell,\ldots,n$, and no proper coloring with more than $n$ colors. Choosing
$\ell = m=r$ this answers a question of Bujt\'as and Tuza, and generalizes
their result with a shorter proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1423</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1423</id><created>2014-07-05</created><updated>2014-08-11</updated><authors><author><keyname>Hu</keyname><forenames>Lansheng</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Xu</keyname><forenames>Jing</forenames></author></authors><title>Simultaneous Wireless Information and Power Transfer with Co-Channel
  Interference</title><categories>cs.IT math.IT</categories><comments>Some figures appear in the wrong places. We need to reformat the
  manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous wireless information and power transfer (SWIPT) is an appealing
solution to balance the energy distribution in wireless networks and improve
the energy-efficiency of the entire network. In this paper, we study the
optimal policies for SWIPT over the flat-fading channel subject to the
co-channel interference. The SWIPT receiver using dynamic power splitting (DPS)
is expected to achieve the maximum information rate given an average harvested
energy constraint. In the case that channel state information at the
transmitter (CSIT) is not available, the optimal power splitting is derived.
For the case with CSIT, the joint optimal policy of the transmit power and the
power splitting are also provided. Through simulations, we show the optimal
rate-energy (R-E) trade-offs of our proposed policies. The case with CSIT
achieves better R-E trade-off than the case with no CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1424</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1424</id><created>2014-07-05</created><authors><author><keyname>Baligh</keyname><forenames>H.</forenames></author><author><keyname>Hong</keyname><forenames>M.</forenames></author><author><keyname>Liao</keyname><forenames>W. -C.</forenames></author><author><keyname>Luo</keyname><forenames>Z. -Q.</forenames></author><author><keyname>Razaviyayn</keyname><forenames>M.</forenames></author><author><keyname>Sanjabi</keyname><forenames>M.</forenames></author><author><keyname>Sun</keyname><forenames>R.</forenames></author></authors><title>Cross Layer Provision of Future Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To cope with the growing demand for wireless data and to extend service
coverage, future 5G networks will increasingly rely on the use of low powered
nodes to support massive connectivity in diverse set of applications and
services [1]. To this end, virtualized and mass-scale cloud architectures are
proposed as promising technologies for 5G in which all the nodes are connected
via a backhaul network and managed centrally by such cloud centers. The
significant computing power made available by the cloud technologies has
enabled the implementation of sophisticated signal processing algorithms,
especially by way of parallel processing, for both interference management and
network provision. The latter two are among the major signal processing tasks
for 5G due to increased level of frequency sharing, node density, interference
and network congestion. This article outlines several theoretical and practical
aspects of joint interference management and network provisioning for future 5G
networks. A cross-layer optimization framework is proposed for joint user
admission, user-base station association, power control, user grouping,
transceiver design as well as routing and flow control. We show that many of
these cross-layer tasks can be treated in a unified way and implemented in a
parallel manner using an efficient algorithmic framework called WMMSE (Weighted
MMSE). Some recent developments in this area are highlighted and future
research directions are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1425</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1425</id><created>2014-07-05</created><updated>2015-01-27</updated><authors><author><keyname>Zhang</keyname><forenames>Junhao</forenames></author><author><keyname>Chen</keyname><forenames>Tongfei</forenames></author><author><keyname>Hu</keyname><forenames>Junfeng</forenames></author></authors><title>On the relationship between Gaussian stochastic blockmodels and label
  propagation algorithms</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 17 figures</comments><journal-ref>J. Stat. Mech. (2015) P03009</journal-ref><doi>10.1088/1742-5468/2015/03/P03009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of community detection receives great attention in recent years.
Many methods have been proposed to discover communities in networks. In this
paper, we propose a Gaussian stochastic blockmodel that uses Gaussian
distributions to fit weight of edges in networks for non-overlapping community
detection. The maximum likelihood estimation of this model has the same
objective function as general label propagation with node preference. The node
preference of a specific vertex turns out to be a value proportional to the
intra-community eigenvector centrality (the corresponding entry in principal
eigenvector of the adjacency matrix of the subgraph inside that vertex's
community) under maximum likelihood estimation. Additionally, the maximum
likelihood estimation of a constrained version of our model is highly related
to another extension of label propagation algorithm, namely, the label
propagation algorithm under constraint. Experiments show that the proposed
Gaussian stochastic blockmodel performs well on various benchmark networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1428</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1428</id><created>2014-07-05</created><updated>2014-10-06</updated><authors><author><keyname>Miller</keyname><forenames>Avery</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Fast Rendezvous with Advice</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two mobile agents, starting from different nodes of an $n$-node network at
possibly different times, have to meet at the same node. This problem is known
as rendezvous. Agents move in synchronous rounds using a deterministic
algorithm. In each round, an agent decides to either remain idle or to move to
one of the adjacent nodes. Each agent has a distinct integer label from the set
$\{1,...,L\}$, which it can use in the execution of the algorithm, but it does
not know the label of the other agent.
  If $D$ is the distance between the initial positions of the agents, then
$\Omega(D)$ is an obvious lower bound on the time of rendezvous. However, if
each agent has no initial knowledge other than its label, time $O(D)$ is
usually impossible to achieve. We study the minimum amount of information that
has to be available a priori to the agents to achieve rendezvous in optimal
time $\Theta(D)$. This information is provided to the agents at the start by an
oracle knowing the entire instance of the problem, i.e., the network, the
starting positions of the agents, their wake-up rounds, and both of their
labels. The oracle helps the agents by providing them with the same binary
string called advice, which can be used by the agents during their navigation.
The length of this string is called the size of advice. Our goal is to find the
smallest size of advice which enables the agents to meet in time $\Theta(D)$.
We show that this optimal size of advice is $\Theta(D\log(n/D)+\log\log L)$.
The upper bound is proved by constructing an advice string of this size, and
providing a natural rendezvous algorithm using this advice that works in time
$\Theta(D)$ for all networks. The matching lower bound, which is the main
contribution of this paper, is proved by exhibiting classes of networks for
which it is impossible to achieve rendezvous in time $\Theta(D)$ with smaller
advice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1429</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1429</id><created>2014-07-05</created><authors><author><keyname>Nassiry</keyname><forenames>Mohammad</forenames></author><author><keyname>Mukhtar</keyname><forenames>Muriati</forenames></author></authors><title>Business types classification via e-commerce stage model in oil industry
  in Iran</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the strategies and plans for e-commerce development are different for
different industries and since the oil industry is one of the most important
industries in Iran, the scope of this research is thus confined to that of the
oil industry in Iran. The main aim of this study is to identify and classify
the different features of e-commerce development stages and features based on
the different business types present in companies in the oil industry in Iran.
In order to achieve both of these objectives a questionnaire was developed and
administered online. The questionnaire was distributed to forty representatives
working in different companies. The collected data was classified and sorted
and the priority e-commerce features was classified and displayed as triangles
for each business type. Furthermore, the experts were asked to indicate the
features which they implemented in their companies in order to know the most
used features in each stage. The results of this study give an insight to the
practice of e-commerce for Iranian oil companies and can be used to strategize
future directions for the industry in terms of e- commerce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1442</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1442</id><created>2014-07-05</created><authors><author><keyname>Yang</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Multiple-Candidate Successive Interference Cancellation with
  Widely-Linear Processing for MAI and Jamming Suppression in DS-CDMA Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>8 figures, 4 Tables, IET Signal Processing, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a widely-linear (WL) receiver structure for
multiple access interference (MAI) and {jamming signal (JS)} suppression in
direct-sequence code-division multiple-access (DS-CDMA) systems. A vector space
projection (VSP) scheme is also considered to cancel the {JS} before detecting
the desired signals. We develop a novel multiple-candidate successive
interference cancellation (MC-SIC) scheme which processes two consecutive user
symbols at one time to process the unreliable estimates and a number of
selected points serve as the feedback candidates for interference cancellation,
which is effective for alleviating the effect of error propagation in the SIC
algorithm. Widely-linear signal processing is then used to enhance the
performance of the receiver in non-circular modulation scheme. By bringing
together the techniques mentioned above, a novel interference suppression
scheme is proposed which combines the widely-linear multiple-candidate SIC
(WL-MC-SIC) minimum mean-squared error (MMSE) algorithm with the VSP scheme to
suppress MAI and {JS} simultaneously. Simulations for binary phase shift keying
(BPSK) modulation scenarios show that the proposed structure achieves a better
MAI suppression performance compared with previously reported SIC MMSE
receivers at lower complexity and a superior {JS} suppression performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1443</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1443</id><created>2014-07-05</created><authors><author><keyname>Hajas</keyname><forenames>Peter</forenames></author><author><keyname>Gutierrez</keyname><forenames>Louis</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Mukkai S.</forenames></author></authors><title>Analysis of Yelp Reviews</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>24 pages, 20 figures and 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era of Big Data and Social Computing, the role of customer reviews and
ratings can be instrumental in predicting the success and sustainability of
businesses. In this paper, we show that, despite the apparent subjectivity of
user ratings, there are also external, or objective factors which help to
determine the outcome of a business's reviews. The current model for social
business review sites, such as Yelp, allows data (reviews, ratings) to be
compiled concurrently, which introduces a bias to participants (Yelp Users).
Our work examines Yelp Reviews for businesses in and around college towns. We
demonstrate that an Observer Effect causes data to behave cyclically: rising
and falling as momentum (quantified in user ratings) shifts for businesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1450</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1450</id><created>2014-07-05</created><authors><author><keyname>Yang</keyname><forenames>Ning</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Wang</keyname><forenames>Fengjiao</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>When and Where: Predicting Human Movements Based on Social
  Spatial-Temporal Events</title><categories>cs.SI stat.AP</categories><journal-ref>In Proceedings of 2014 SIAM International Conference on Data
  Mining (SDM 2014), 2014. 515-523</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting both the time and the location of human movements is valuable but
challenging for a variety of applications. To address this problem, we propose
an approach considering both the periodicity and the sociality of human
movements. We first define a new concept, Social Spatial-Temporal Event (SSTE),
to represent social interactions among people. For the time prediction, we
characterise the temporal dynamics of SSTEs with an ARMA (AutoRegressive Moving
Average) model. To dynamically capture the SSTE kinetics, we propose a Kalman
Filter based learning algorithm to learn and incrementally update the ARMA
model as a new observation becomes available. For the location prediction, we
propose a ranking model where the periodicity and the sociality of human
movements are simultaneously taken into consideration for improving the
prediction accuracy. Extensive experiments conducted on real data sets validate
our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1454</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1454</id><created>2014-07-05</created><updated>2014-08-11</updated><authors><author><keyname>Yan</keyname><forenames>Jing</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Gao</keyname><forenames>Zhenzhen</forenames></author></authors><title>Distributed Relay Selection Protocols for Simultaneous Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>The figures appear in the wrong places. We need to reformat the
  manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harvesting energy from the radio-frequency (RF) signal is an exciting
solution to replenish energy in energy-constrained wireless networks. In this
paper, an amplify-and-forward (AF) based wireless relay network is considered,
where the relay nodes need to harvest energy from the source's RF signal to
forward information to the destination. To improve the performance of
information transmission, we propose two distributed relay selection protocols,
Maximum Harvested Energy (MHE) protocol and Maximum Signal-to-Noise Ratio
(MSNR) protocol. Then, we derive the outage probabilities of the system with
our proposed relay selection protocols and prove that the proposed selection
protocols indeed can improve the system performances and the MSNR protocol
outperforms the MHE protocol. Simulation results verify the analysis and
theorems. In addition, the effects of key system parameters are also
investigated via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1460</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1460</id><created>2014-07-06</created><authors><author><keyname>Macktoobian</keyname><forenames>Matin</forenames></author></authors><title>Bi-directioal Motion Detection: A Neural Intelligent Model For
  Perception of Cognitive Robots</title><categories>cs.RO</categories><comments>6 pages, 8 figures, 2nd Basic &amp; Clinical Neuroscience Congress BCNC
  2013, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new neuronal circuit, based on the spiking neuronal network
model, is proposed in order to detect the movement direction of dynamic objects
wandering around cognitive robots. Capability of our new approach in
bi-directional movement detection is beholden to its symmetric configuration of
the proposed circuit. With due attention to magnificence of handling of
blocking problems in neuronal networks such as epilepsy, mounting both
excitatory and inhibitory stimuli has been taken into account. Investigations
upon applied implementation of aforementioned strategy on PIONEER cognitive
robot reveals that the strategy leads to alleviation of potential level in the
sensory networks. Furthermore, investigation on intrinsic delay of the circuit
reveals not only the noticeable switching rate which could be acquired but the
high-efficient coupling of the circuit with the other high-speed ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1461</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1461</id><created>2014-07-06</created><authors><author><keyname>Macktoobian</keyname><forenames>Matin</forenames></author></authors><title>Curved Trajectory Detection : A Novel Neurocognitive Perception Approach
  for Autonomous Smart Robots</title><categories>cs.RO</categories><comments>6 pages, 10 figures, 2nd basic &amp; Clinical Neuroscience Congress BCNC
  2013, Tehran, Iran</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Braitenberg vehicles could be mentioned as the seminal elements for cognitive
studies in robotics fields especially neurorobotics to invent more smart
robots. Motion detection of dynamic objects could be taken as one of the most
inspiring abilities into account which can lead to evolve more intelligent
Braitenberg vehicles. In this paper, a new neuronal circuit is established in
order to detect curved movements of the objects wandering around Braitenberg
vehicles. Modular structure of the novel circuit provides the opportunity to
expand the model into huge sensory-biosystems. Furthermore, robust performance
of the circuit against epileptic seizures is beholden to simultaneous
utilization of excitatory and inhibitory stimuli in the circuit construction.
Also, straight movements, as special case of curved movements could be tracked.
PIONEER, with due attention to its suitable neurosensors, is used as a
Braitenberg vehicle for empirical evaluations. Simulated results and practical
experiments are applied to this vehicle in order to verify new achievements of
the curved trajectory detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1465</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1465</id><created>2014-07-06</created><authors><author><keyname>Mahajan</keyname><forenames>Sonam</forenames></author><author><keyname>Singh</keyname><forenames>Maninder</forenames></author></authors><title>Analysis of RSA algorithm using GPU programming</title><categories>cs.CR</categories><comments>14 pages, Journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern-day computer security relies heavily on cryptography as a means to
protect the data that we have become increasingly reliant on. The main research
in computer security domain is how to enhance the speed of RSA algorithm. The
computing capability of Graphic Processing Unit as a co-processor of the CPU
can leverage massive-parallelism. This paper presents a novel algorithm for
calculating modulo value that can process large power of numbers which
otherwise are not supported by built-in data types. First the traditional
algorithm is studied. Secondly, the parallelized RSA algorithm is designed
using CUDA framework. Thirdly, the designed algorithm is realized for small
prime numbers and large prime number . As a result the main fundamental problem
of RSA algorithm such as speed and use of poor or small prime numbers that has
led to significant security holes, despite the RSA algorithm's mathematical
soundness can be alleviated by this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1466</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1466</id><created>2014-07-06</created><authors><author><keyname>Syed</keyname><forenames>Umair Atique</forenames></author><author><keyname>Muniandy</keyname><forenames>Uma Kandan</forenames></author></authors><title>The Smart Shower</title><categories>cs.CY</categories><comments>2 Pages, 3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart shower is an intelligent device that saves the water during the
shower. It uses the indicator lamps that inform the user of the amount of the
water. Like the traffic signal it has three sets of lamps, green, yellow and
red, each indicating the amount of time spent. This device brain is the Siemens
Logo PLC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1471</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1471</id><created>2014-07-06</created><authors><author><keyname>Kant</keyname><forenames>Shashi</forenames></author><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author><author><keyname>Priyanto</keyname><forenames>Basuki E.</forenames></author></authors><title>A Robust Low-Complexity MIMO Detector for Rank 4 LTE/LTE-A Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in PIMRC-2014, Washington DC, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with MIMO detection for rank 4 LTE systems. The paper
revolves around a previously known detector [1, by Inkyu Lee, TCOM'2010] which
we shall refer to as RCSMLD
(Reduced-Constellation-Size-Maximum-Likelihood-Detector). However, a direct
application of the scheme in [1, by Inkyu Lee, TCOM'2010] to LTE/LTE-A rank 4
test cases results in unsatisfactory performance. The first contribution of the
paper is to introduce several modifications that can jointly be applied to the
basic RCSMLD scheme which, taken together, result in excellent performance. Our
second contribution is the development of a highly efficient hardware structure
for RCSMLD that allows for an implementation with very few multiplications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1474</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1474</id><created>2014-07-06</created><authors><author><keyname>Bakhtiyari</keyname><forenames>Kaveh</forenames></author><author><keyname>Husain</keyname><forenames>Hafizah</forenames></author></authors><title>Fuzzy Model on Human Emotions Recognition</title><categories>cs.AI cs.HC</categories><comments>12th WSEAS International Conference on Applications of Computer
  Engineering (ACE '13), Cambridge, MA, USA, 30 Jan. - 1 Feb. 2013 ISBN:
  978-1-61804-156-2, Pages 77-82</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a fuzzy model for multi-level human emotions recognition
by computer systems through keyboard keystrokes, mouse and touchscreen
interactions. This model can also be used to detect the other possible emotions
at the time of recognition. Accuracy measurements of human emotions by the
fuzzy model are discussed through two methods; the first is accuracy analysis
and the second is false positive rate analysis. This fuzzy model detects more
emotions, but on the other hand, for some of emotions, a lower accuracy was
obtained with the comparison with the non-fuzzy human emotions detection
methods. This system was trained and tested by Support Vector Machine (SVM) to
recognize the users' emotions. Overall, this model represents a closer
similarity between human brain detection of emotions and computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1477</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1477</id><created>2014-07-06</created><authors><author><keyname>Szabo</keyname><forenames>Jozsef</forenames></author></authors><title>Noiseless coding theorem proved by induction for finite stationary
  memoryless information sources</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noiseless coding theorem for finite stationary memoryless information sources
is proved by using induction on the number of source symbols and the inequality
of geometric and harmonic means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1480</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1480</id><created>2014-07-06</created><authors><author><keyname>Huang</keyname><forenames>Shenwei</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Narrowing the Complexity Gap for Colouring ($C_s$,$P_t$)-Free Graphs</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a positive integer $k$ and graph $G=(V,E)$, a $k$-colouring of $G$ is a
mapping $c: V\rightarrow\{1,2,\ldots,k\}$ such that $c(u)\neq c(v)$ whenever
$uv\in E$. The $k$-Colouring problem is to decide, for a given $G$, whether a
$k$-colouring of $G$ exists. The $k$-Precolouring Extension problem is to
decide, for a given $G=(V,E)$, whether a colouring of a subset of $V$ can be
extended to a $k$-colouring of $G$. A $k$-list assignment of a graph is an
allocation of a list -a subset of $\{1,\ldots,k\}$- to each vertex, and the
List $k$-Colouring problem is to decide, for a given $G$, whether $G$ has a
$k$-colouring in which each vertex is coloured with a colour from its list. We
continued the study of the computational complexity of these three decision
problems when restricted to graphs that contain neither a cycle on $s$ vertices
nor a path on $t$ vertices as induced subgraphs (for fixed positive integers
$s$ and~$t$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1482</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1482</id><created>2014-07-06</created><updated>2016-02-15</updated><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author><author><keyname>Song</keyname><forenames>Jian</forenames></author></authors><title>A Survey on the Computational Complexity of Colouring Graphs with
  Forbidden Subgraphs</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a positive integer $k$, a $k$-colouring of a graph $G=(V,E)$ is a mapping
$c: V\rightarrow\{1,2,...,k\}$ such that $c(u)\neq c(v)$ whenever $uv\in E$.
The Colouring problem is to decide, for a given $G$ and $k$, whether a
$k$-colouring of $G$ exists. If $k$ is fixed (that is, it is not part of the
input), we have the decision problem $k$-Colouring instead. We survey known
results on the computational complexity of Colouring and $k$-Colouring for
graph classes that are characterized by one or two forbidden induced subgraphs.
We also consider a number of variants: for example, where the problem is to
extend a partial colouring, or where lists of permissible colours are given for
each vertex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1484</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1484</id><created>2014-07-06</created><authors><author><keyname>Kefayati</keyname><forenames>Mahdi</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author></authors><title>Optimal Policies for Simultaneous Energy Consumption and Ancillary
  Service Provision for Flexible Loads under Stochastic Prices and No Capacity
  Reservation Constraint</title><categories>math.OC cs.SY</categories><comments>submitted to International Journal of Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexible loads, i.e. the loads whose power trajectory is not bound to a
specific one, constitute a sizable portion of current and future electric
demand. This flexibility can be used to improve the performance of the grid,
should the right incentives be in place.
  In this paper, we consider the optimal decision making problem faced by a
flexible load, demanding a certain amount of energy over its availability
period, subject to rate constraints. The load is also capable of providing
Ancillary Services (AS) by decreasing or increasing its consumption in response
to signals from the Independent System Operator. Under arbitrarily distributed
and correlated Markovian energy and AS prices, we obtain the optimal policy for
minimizing expected total cost, which includes cost of energy and benefits from
AS provision, assuming no capacity reservation requirement for AS provision. We
also prove that the optimal policy has a multi-threshold form and can be
computed, stored and operated efficiently. We further study the effectiveness
of our proposed optimal policy and its impact on the grid.
  We show that, while optimal simultaneous consumption and AS provision under
real-time stochastic prices is achievable with acceptable computational burden,
the impact of adopting such real-time pricing schemes on the network might not
be as good as suggested by the majority of the existing literature. In fact, we
show that such price responsive loads are likely to induce peak-to-average
ratios much more than what is observed in the current distribution networks and
adversely affect the grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1487</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1487</id><created>2014-07-06</created><authors><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Precoder Index Modulation</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.6543</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Index modulation, where information bits are conveyed through antenna indices
(spatial modulation) and subcarrier indices (subcarrier index modulation) in
addition to information bits conveyed through conventional modulation symbols,
is getting increased research attention. In this paper, we introduce {\em
precoder index modulation}, where information bits are conveyed through the
choice of a precoder matrix at the transmitter from a set of pre-determined
pseudo-random phase precoder (PRPP) matrices. Combining precoder index
modulation (PIM) and spatial modulation (SM), we introduce a PIM-SM scheme
which conveys information bits through both antenna index as well as precoder
index. Spectral efficiency (in bits per channel use) and bit error performance
of these index modulation schemes are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1490</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1490</id><created>2014-07-06</created><authors><author><keyname>Li</keyname><forenames>Jianguo</forenames></author><author><keyname>Chen</keyname><forenames>Yurong</forenames></author></authors><title>Large-scale Supervised Hierarchical Feature Learning for Face
  Recognition</title><categories>cs.CV</categories><comments>8 pages; 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes a novel face recognition algorithm based on large-scale
supervised hierarchical feature learning. The approach consists of two parts:
hierarchical feature learning and large-scale model learning. The hierarchical
feature learning searches feature in three levels of granularity in a
supervised way. First, face images are modeled by receptive field theory, and
the representation is an image with many channels of Gaussian receptive maps.
We activate a few most distinguish channels by supervised learning. Second, the
face image is further represented by patches of picked channels, and we search
from the over-complete patch pool to activate only those most discriminant
patches. Third, the feature descriptor of each patch is further projected to
lower dimension subspace with discriminant subspace analysis.
  Learned feature of activated patches are concatenated to get a full face
representation.A linear classifier is learned to separate face pairs from same
subjects and different subjects. As the number of face pairs are extremely
large, we introduce ADMM (alternative direction method of multipliers) to train
the linear classifier on a computing cluster. Experiments show that more
training samples will bring notable accuracy improvement.
  We conduct experiments on FRGC and LFW. Results show that the proposed
approach outperforms existing algorithms under the same protocol notably.
Besides, the proposed approach is small in memory footprint, and low in
computing cost, which makes it suitable for embedded applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1492</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1492</id><created>2014-07-06</created><authors><author><keyname>Son</keyname><forenames>Hyukmin</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Joint Beamforming Design for Multi-User Wireless Information and Power
  Transfer</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a joint beamforming algorithm for a multiuser
wireless information and power transfer (MU-WIPT) system that is compatible
with the conventional multiuser multiple input multiple output (MU-MIMO)
system. The proposed joint beamforming vectors are initialized using the well
established MU-MIMO zero-forcing beamforming (ZFBF) and are further updated to
maximize the total harvested energy of energy harvesting (EH) users and
guarantee the signal to interference plus noise ratio (SINR) constraints of the
co-scheduled information decoding (ID) users. When ID and EH users are
simultaneously served by joint beamforming vectors, the harvested energy can be
increased at the cost of an SINR loss for ID users. To characterize the SINR
loss, the target SINR ratio,u, is introduced as the target SINR (i.e., SINR
constraint) normalized by the received SINR achievable with ZFBF. Based on that
ratio, the sum rate and harvested energy obtained from the proposed algorithm
are analyzed under perfect/imperfect channel state information at the
transmitter (CSIT). Through simulations and numerical results, we validate the
derived analyses and demonstrate the EH and ID performance compared to both
state of the art and conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1497</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1497</id><created>2014-07-06</created><updated>2016-03-01</updated><authors><author><keyname>Keshtkarjahromi</keyname><forenames>Yasaman</forenames></author><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author><author><keyname>Khokhar</keyname><forenames>Ashfaq</forenames></author></authors><title>Content-Aware Network Coding over Device-to-Device Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a scenario of broadcasting a common content to a group of
cooperating mobile devices that are within proximity of each other. Devices in
this group may receive partial content from the source due to packet losses
over wireless broadcast links. We further consider that packet losses are
different for different devices. The remaining missing content at each device
can then be recovered, thanks to cooperation among the devices by exploiting
device-to-device (D2D) connections. In this context, the minimum amount of time
that can guarantee a complete acquisition of the common content at every device
is referred to as the &quot;completion time&quot;. It has been shown that instantly
decodable network coding (IDNC) reduces the completion time as compared to no
network coding in this scenario. Yet, for applications such as video streaming,
not all packets have the same importance and not all devices are interested in
the same quality of content. This problem is even more interesting when
additional, but realistic constraints, such as strict deadline, bandwidth, or
limited energy are added in the problem formulation. We assert that direct
application of IDNC in such a scenario yields poor performance in terms of
content quality and completion time. In this paper, we propose a novel Content
and Loss-Aware IDNC scheme that improves content quality and network coding
opportunities jointly by taking into account importance of each packet towards
the desired quality of service (QoS) as well as the channel losses over D2D
links. Our proposed Content and Loss-Aware IDNC (i) maximizes the quality under
the completion time constraint, and (ii) minimizes the completion time under
the quality constraint. We demonstrate the benefits of Content and Loss-Aware
IDNC through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1502</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1502</id><created>2014-07-06</created><authors><author><keyname>Feyzmahdavian</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Charalambous</keyname><forenames>Themistoklis</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Sub-homogeneous positive monotone systems are insensitive to
  heterogeneous time-varying delays</title><categories>cs.SY</categories><comments>Submitted to the 21st International Symposium on Mathematical Theory
  of Networks and Systems (MTNS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a sub-homogeneous positive monotone system with bounded
heterogeneous time-varying delays is globally asymptotically stable if and only
if the corresponding delay-free system is globally asymptotically stable. The
proof is based on an extension of a delay-independent stability result for
monotone systems under constant delays by Smith to systems with bounded
heterogeneous time-varying delays. Under the additional assumption of
positivity and sub-homogeneous vector fields, we establish the aforementioned
delay insensitivity property and derive a novel test for global asymptotic
stability. If the system has a unique equilibrium point in the positive
orthant, we prove that our stability test is necessary and sufficient.
Specialized to positive linear systems, our results extend and sharpen existing
results from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1507</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1507</id><created>2014-07-06</created><authors><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author><author><keyname>Kokot</keyname><forenames>Marek</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Debudaj-Grabysz</keyname><forenames>Agnieszka</forenames></author></authors><title>KMC 2: Fast and resource-frugal $k$-mer counting</title><categories>cs.DS cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Building the histogram of occurrences of every $k$-symbol long
substring of nucleotide data is a standard step in many bioinformatics
applications, known under the name of $k$-mer counting. Its applications
include developing de Bruijn graph genome assemblers, fast multiple sequence
alignment and repeat detection. The tremendous amounts of NGS data require fast
algorithms for $k$-mer counting, preferably using moderate amounts of memory.
  Results: We present a novel method for $k$-mer counting, on large datasets at
least twice faster than the strongest competitors (Jellyfish~2, KMC~1), using
about 12\,GB (or less) of RAM memory. Our disk-based method bears some
resemblance to MSPKmerCounter, yet replacing the original minimizers with
signatures (a carefully selected subset of all minimizers) and using $(k,
x)$-mers allows to significantly reduce the I/O, and a highly parallel overall
architecture allows to achieve unprecedented processing speeds. For example,
KMC~2 allows to count the 28-mers of a human reads collection with 44-fold
coverage (106\,GB of compressed size) in about 20 minutes, on a 6-core Intel i7
PC with an SSD.
  Availability: KMC~2 is freely available at http://sun.aei.polsl.pl/kmc.
  Contact: sebastian.deorowicz@polsl.pl
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1508</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1508</id><created>2014-07-06</created><authors><author><keyname>Silva</keyname><forenames>Jose Mairton B. da</forenames><suffix>Jr.</suffix></author><author><keyname>Fodor</keyname><forenames>Gabor</forenames></author><author><keyname>Maciel</keyname><forenames>Tarcisio F.</forenames></author></authors><title>Performance Analysis of Network-Assisted Two-Hop D2D Communications</title><categories>cs.NI</categories><comments>6 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-assisted single-hop device-to-device (D2D) communication can increase
the spectral and energy efficiency of cellular networks by taking advantage of
the proximity, reuse, and hop gains when radio resources are properly managed
between the cellular and D2D layers. In this paper we argue that D2D technology
can be used to further increase the spectral and energy efficiency if the key
D2D radio resource management algorithms are suitably extended to support
network assisted multi-hop D2D communications. Specifically, we propose a
novel, distributed utility maximizing D2D power control (PC) scheme that is
able to balance spectral and energy efficiency while taking into account mode
selection and resource allocation constraints that are important in the
integrated cellular-D2D environment. Our analysis and numerical results
indicate that multi-hop D2D communications combined with the proposed PC scheme
can be useful not only for harvesting the potential gains previously identified
in the literature, but also for extending the coverage of cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1513</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1513</id><created>2014-07-06</created><authors><author><keyname>de la Higuera</keyname><forenames>Colin</forenames></author><author><keyname>Scicluna</keyname><forenames>James</forenames></author><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames></author></authors><title>On the Computation of Distances for Probabilistic Context-Free Grammars</title><categories>cs.FL</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic context-free grammars (PCFGs) are used to define distributions
over strings, and are powerful modelling tools in a number of areas, including
natural language processing, software engineering, model checking,
bio-informatics, and pattern recognition. A common important question is that
of comparing the distributions generated or modelled by these grammars: this is
done through checking language equivalence and computing distances. Two PCFGs
are language equivalent if every string has identical probability with both
grammars. This also means that the distance (whichever norm is used) is null.
It is known that the language equivalence problem is interreducible with that
of multiple ambiguity for context-free grammars, a long-standing open question.
In this work, we prove that computing distances corresponds to solving
undecidable questions: this is the case for the L1, L2 norm, the variation
distance and the Kullback-Leibler divergence. Two more results are less
negative: 1. The most probable string can be computed, and, 2. The Chebyshev
distance (where the distance between two distributions is the maximum
difference of probabilities over all strings) is interreducible with the
language equivalence problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1514</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1514</id><created>2014-07-06</created><updated>2015-03-21</updated><authors><author><keyname>Krishnan</keyname><forenames>Nikhil</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>A Universal Parallel Two-Pass MDL Context Tree Compression Algorithm</title><categories>cs.IT math.IT</categories><comments>Accepted to Journal of Selected Topics in Signal Processing special
  issue on Signal Processing for Big Data (expected publication date June
  2015). 10 pages double column, 6 figures, and 2 tables. arXiv admin note:
  substantial text overlap with arXiv:1405.6322. Version: Mar 2015: Corrected a
  typo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing problems that handle large amounts of data necessitate the use of
lossless data compression for efficient storage and transmission. We present a
novel lossless universal data compression algorithm that uses parallel
computational units to increase the throughput. The length-$N$ input sequence
is partitioned into $B$ blocks. Processing each block independently of the
other blocks can accelerate the computation by a factor of $B$, but degrades
the compression quality. Instead, our approach is to first estimate the minimum
description length (MDL) context tree source underlying the entire input, and
then encode each of the $B$ blocks in parallel based on the MDL source. With
this two-pass approach, the compression loss incurred by using more parallel
units is insignificant. Our algorithm is work-efficient, i.e., its
computational complexity is $O(N/B)$. Its redundancy is approximately
$B\log(N/B)$ bits above Rissanen's lower bound on universal compression
performance, with respect to any context tree source whose maximal depth is at
most $\log(N/B)$. We improve the compression by using different quantizers for
states of the context tree based on the number of symbols corresponding to
those states. Numerical results from a prototype implementation suggest that
our algorithm offers a better trade-off between compression and throughput than
competing universal data compression algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1520</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1520</id><created>2014-07-06</created><authors><author><keyname>Singh</keyname><forenames>Vineet Kumar</forenames></author><author><keyname>Dutta</keyname><forenames>Maitreyee</forenames></author></authors><title>Analyzing Cryptographic Algorithms for Secure Cloud Network</title><categories>cs.DC cs.CR</categories><comments>9 Pages, 8 Figures, IJASCSE ISSN No.-2278-7917, Vol 3, Issue 6, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pay as per usage concept of Cloud computing has brought revolutionary changes
in the information technology world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1521</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1521</id><created>2014-07-06</created><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Costello</keyname><forenames>Kevin</forenames></author><author><keyname>Gasieniec</keyname><forenames>Leszek</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author></authors><title>Information Gathering in Ad-Hoc Radio Networks with Tree Topology</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of information gathering in ad-hoc radio networks
without collision detection, focussing on the case when the network forms a
tree, with edges directed towards the root. Initially, each node has a piece of
information that we refer to as a rumor. Our goal is to design protocols that
deliver all rumors to the root of the tree as quickly as possible. The protocol
must complete this task within its allotted time even though the actual tree
topology is unknown when the computation starts. In the deterministic case,
assuming that the nodes are labeled with small integers, we give an O(n)-time
protocol that uses unbounded messages, and an O(n log n)-time protocol using
bounded messages, where any message can include only one rumor. We also
consider fire-and-forward protocols, in which a node can only transmit its own
rumor or the rumor received in the previous step. We give a deterministic
fire-and- forward protocol with running time O(n^1.5), and we show that it is
asymptotically optimal. We then study randomized algorithms where the nodes are
not labelled. In this model, we give an O(n log n)-time protocol and we prove
that this bound is asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1524</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1524</id><created>2014-07-06</created><updated>2014-09-10</updated><authors><author><keyname>Liu</keyname><forenames>Bing</forenames></author><author><keyname>Kong</keyname><forenames>Soonho</forenames></author><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Zuliani</keyname><forenames>Paolo</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund M.</forenames></author></authors><title>Parameter Synthesis for Cardiac Cell Hybrid Models Using Delta-Decisions</title><categories>cs.LO cs.CE q-bio.QM q-bio.TO</categories><comments>12th Conference on Computational Methods in Systems Biology (CMSB
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in systems biology is to identify parameter values such
that a biological model satisfies some behavioral constraints (\eg, time
series). In this paper we focus on parameter synthesis for hybrid
(continuous/discrete) models, as many biological systems can possess multiple
operational modes with specific continuous dynamics in each mode. These
biological systems are naturally modeled as hybrid automata, most often with
nonlinear continuous dynamics. However, hybrid automata are notoriously hard to
analyze --- even simple reachability for hybrid systems with linear
differential dynamics is an undecidable problem. In this paper we present a
parameter synthesis framework based on $\delta$-complete decision procedures
that sidesteps undecidability. We demonstrate our method on two highly
nonlinear hybrid models of the cardiac cell action potential. The results show
that our parameter synthesis framework is convenient and efficient, and it
enabled us to select a suitable model to study and identify crucial parameter
ranges related to cardiac disorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1525</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1525</id><created>2014-07-06</created><authors><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Xia</keyname><forenames>Ge</forenames></author></authors><title>Flip Distance is in FPT time $O(n+ k \cdot c^k)$</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let ${\cal T}$ be a triangulation of a set ${\cal P}$ of $n$ points in the
plane, and let $e$ be an edge shared by two triangles in ${\cal T}$ such that
the quadrilateral $Q$ formed by these two triangles is convex. A {\em flip} of
$e$ is the operation of replacing $e$ by the other diagonal of $Q$ to obtain a
new triangulation of ${\cal P}$ from ${\cal T}$. The {\em flip distance}
between two triangulations of ${\cal P}$ is the minimum number of flips needed
to transform one triangulation into the other. The Flip Distance problem asks
if the flip distance between two given triangulations of ${\cal P}$ is $k$, for
some given $k \in N$. It is a fundamental and a challenging problem whose
complexity for the case of triangulations of a convex polygon remains open for
over 25 years.
  In this paper we present an algorithm for the Flip Distance problem that runs
in time $O(n + k \cdot c^{k})$, for $c=392$, which implies that the problem is
fixed-parameter tractable. The NP-hardness reduction for the Flip Distance
problem given by Lubiw and Pathak can be used to show that, unless the
exponential-time hypothesis (ETH) fails, the Flip Distance problem cannot be
solved in time $O^*(2^{o(k)})$. Therefore, one cannot expect an asymptotic
improvement in the exponent of the running time of the presented algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1531</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1531</id><created>2014-07-06</created><updated>2015-04-14</updated><authors><author><keyname>Valkonen</keyname><forenames>Tuomo</forenames></author></authors><title>The jump set under geometric regularisation. Part 1: Basic technique and
  first-order denoising</title><categories>math.FA cs.CV</categories><msc-class>26B30, 49Q20, 65J20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $u \in \mbox{BV}(\Omega)$ solve the total variation denoising problem
with $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model.
Simul. 6 (2008), 879--894] have shown the containment $\mathcal{H}^{m-1}(J_u
\setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proof
unfortunately depends heavily on the co-area formula, as do many results in
this area, and as such is not directly extensible to higher-order,
curvature-based, and other advanced geometric regularisers, such as total
generalised variation (TGV) and Euler's elastica. These have received increased
attention in recent times due to their better practical regularisation
properties compared to conventional total variation or wavelets. We prove
analogous jump set containment properties for a general class of regularisers.
We do this with novel Lipschitz transformation techniques, and do not require
the co-area formula. In the present Part 1 we demonstrate the general technique
on first-order regularisers, while in Part 2 we will extend it to higher-order
regularisers. In particular, we concentrate in this part on TV and, as a
novelty, Huber-regularised TV. We also demonstrate that the technique would
apply to non-convex TV models as well as the Perona-Malik anisotropic
diffusion, if these approaches were well-posed to begin with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1537</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1537</id><created>2014-07-06</created><updated>2015-01-02</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author></authors><title>Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent</title><categories>cs.DS cs.LG cs.NA math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First-order methods play a central role in large-scale convex optimization.
Even though many variations exist, each suited to a particular problem form,
almost all such methods fundamentally rely on two types of algorithmic steps
and two corresponding types of analysis: gradient-descent steps, which yield
primal progress, and mirror-descent steps, which yield dual progress. In this
paper, we observe that the performances of these two types of step are
complementary, so that faster algorithms can be designed by linearly coupling
the two steps.
  In particular, we obtain a simple accelerated gradient method for the class
of smooth convex optimization problems. The first such method was proposed by
Nesterov back to 1983, but to the best of our knowledge, the proof of the fast
convergence of accelerated gradient methods has not found a clear
interpretation and is still regarded by many as crucially relying on &quot;algebraic
tricks&quot;. We apply our novel insights to construct a new accelerated gradient
method as a natural linear coupling of gradient descent and mirror descent and
to write its proof of convergence as a simple combination of the convergence
analyses of the two underlying descent steps.
  We believe that the complementary view and the linear coupling technique in
this paper will prove very useful in the design of first-order methods as it
allows us to design fast algorithms in a conceptually easier way. For instance,
our technique greatly facilitates the recent breakthroughs in solving packing
and covering linear programs [AO14, AO15].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1538</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1538</id><created>2014-07-06</created><authors><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Wu</keyname><forenames>Zhaoming</forenames></author><author><keyname>Li</keyname><forenames>Li-Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Ruofei</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Wu</keyname><forenames>Hang</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author></authors><title>Large-Scale Multi-Label Learning with Incomplete Label Assignments</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-label learning deals with the classification problems where each
instance can be assigned with multiple labels simultaneously. Conventional
multi-label learning approaches mainly focus on exploiting label correlations.
It is usually assumed, explicitly or implicitly, that the label sets for
training instances are fully labeled without any missing labels. However, in
many real-world multi-label datasets, the label assignments for training
instances can be incomplete. Some ground-truth labels can be missed by the
labeler from the label set. This problem is especially typical when the number
instances is very large, and the labeling cost is very high, which makes it
almost impossible to get a fully labeled training set. In this paper, we study
the problem of large-scale multi-label learning with incomplete label
assignments. We propose an approach, called MPU, based upon positive and
unlabeled stochastic gradient descent and stacked models. Unlike prior works,
our method can effectively and efficiently consider missing labels and label
correlations simultaneously, and is very scalable, that has linear time
complexities over the size of the data. Extensive experiments on two real-world
multi-label datasets show that our MPU model consistently outperform other
commonly-used baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1539</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1539</id><created>2014-07-06</created><authors><author><keyname>L&#xfc;ke</keyname><forenames>Thomas</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>A Framework for Specific Term Recommendation Systems</title><categories>cs.IR</categories><comments>2 pages, 1 figure, SIGIR 13, July 28-August 1, 2013, Dublin, Ireland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the IRSA framework that enables the automatic
creation of search term suggestion or recommendation systems (TS). Such TS are
used to operationalize interactive query expansion and help users in refining
their information need in the query formulation phase. Our recent research has
shown TS to be more effective when specific to a certain domain. The presented
technical framework allows owners of Digital Libraries to create their own
specific TS constructed via OAI-harvested metadata with very little effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1540</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1540</id><created>2014-07-06</created><authors><author><keyname>Kern</keyname><forenames>Dagmar</forenames></author><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Establishing an Online Access Panel for Interactive Information
  Retrieval Research</title><categories>cs.IR cs.DL</categories><comments>2 pages, 1 figure, 2014 IEEE/ACM Joint Conference on Digital
  Libraries (JCDL), London, 8th-12th September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an online access panel to support the evaluation process of
Interactive Information Retrieval (IIR) systems - called IIRpanel. By
maintaining an online access panel with users of IIR systems we assume that the
recurring effort to recruit participants for web-based as well as for lab
studies can be minimized. We target on using the online access panel not only
for our own development processes but to open it for other interested
researchers in the field of IIR. In this paper we present the concept of
IIRpanel as well as first implementation details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1543</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1543</id><created>2014-07-06</created><updated>2014-11-07</updated><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Kelner</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Dictionary Learning and Tensor Decomposition via the Sum-of-Squares
  Method</title><categories>cs.DS cs.LG stat.ML</categories><acm-class>F.2.1; F.2.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new approach to the dictionary learning (also known as &quot;sparse
coding&quot;) problem of recovering an unknown $n\times m$ matrix $A$ (for $m \geq
n$) from examples of the form \[ y = Ax + e, \] where $x$ is a random vector in
$\mathbb R^m$ with at most $\tau m$ nonzero coordinates, and $e$ is a random
noise vector in $\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,
our algorithm recovers every column of $A$ within arbitrarily good constant
accuracy in time $m^{O(\log m/\log(\tau^{-1}))}$, in particular achieving
polynomial time if $\tau = m^{-\delta}$ for any $\delta&gt;0$, and time $m^{O(\log
m)}$ if $\tau$ is (a sufficiently small) constant. Prior algorithms with
comparable assumptions on the distribution required the vector $x$ to be much
sparser---at most $\sqrt{n}$ nonzero coordinates---and there were intrinsic
barriers preventing these algorithms from applying for denser $x$.
  We achieve this by designing an algorithm for noisy tensor decomposition that
can recover, under quite general conditions, an approximate rank-one
decomposition of a tensor $T$, given access to a tensor $T'$ that is
$\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our
knowledge, this is the first algorithm for tensor decomposition that works in
the constant spectral-norm noise regime, where there is no guarantee that the
local optima of $T$ and $T'$ have similar structures.
  Our algorithm is based on a novel approach to using and analyzing the Sum of
Squares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and
it can be viewed as an indication of the utility of this very general and
powerful tool for unsupervised learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1545</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1545</id><created>2014-07-06</created><authors><author><keyname>Southern</keyname><forenames>Mary</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>A Lambda Prolog Based Animation of Twelf Specifications</title><categories>cs.PL</categories><comments>15 pages, accepted for presentation at the International Colloquium
  on Implementation of Constraint and Logic Programming Systems (CICLOPS) in
  Vienna</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specifications in the Twelf system are based on a logic programming
interpretation of the Edinburgh Logical Framework or LF. We consider an
approach to animating such specifications using a Lambda Prolog implementation.
This approach is based on a lossy translation of the dependently typed LF
expressions into the simply typed lambda calculus (STLC) terms of Lambda Prolog
and a subsequent encoding of lost dependency information in predicates that are
defined by suitable clauses. To use this idea in an implementation of logic
programming a la Twelf, it is also necessary to translate the results found for
Lambda Prolog queries back into LF expressions. We describe such an inverse
translation and show that it has the necessary properties to facilitate an
emulation of Twelf behavior through our translation of LF specifications into
Lambda Prolog programs. A characteristic of Twelf is that it permits queries to
consist of types which have unspecified parts represented by meta-variables for
which values are to be found through computation. We show that this capability
can be supported within our translation based approach to animating Twelf
specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1546</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1546</id><created>2014-07-06</created><updated>2014-10-07</updated><authors><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Differentially Private Multi-party Computation: Optimality of
  Non-Interactive Randomized Response</title><categories>cs.CR</categories><comments>21 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of interactive function computation by multiple parties
possessing a single bit each in a differential privacy setting (i.e., there
remains an uncertainty in any specific party's bit even when given the
transcript of the interactions and all the other parties' bits). Each party is
interested in computing a function, which could differ from party to party, and
there could be a central observer interested in computing a separate function.
Performance at each party and the central observer is measured via the accuracy
of the function computed. We allow for an arbitrary cost function to measure
the distortion between the true and the computed function value. Our main
result is the exact optimality of a simple non-interactive protocol: each party
randomizes (sufficiently) and publishes its own bit. In other words,
non-interactive randomized response is exactly optimal. Each party and the
central observer then separately compute their respective function to maximize
the appropriate notion of their accuracy measure. The optimality is very
general: it holds for all types of functions, heterogeneous privacy conditions
on the parties, all types of cost metrics, and both average and worst-case
(over the inputs) measures of accuracy. Finally, the optimality result is
simultaneous, in terms of maximizing accuracy at each of the parties and the
central observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1549</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1549</id><created>2014-07-06</created><authors><author><keyname>Sarkar</keyname><forenames>S.</forenames></author><author><keyname>Chawla</keyname><forenames>S.</forenames></author><author><keyname>Weng</keyname><forenames>H.</forenames></author></authors><title>Resilience of human brain functional coactivation networks under
  thresholding</title><categories>cs.SI physics.soc-ph q-bio.NC</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have demonstrated the existence of community structure and
rich club nodes, (i.e., highly interconnected, high degree hub nodes), in human
brain functional networks. The cognitive relevance of the detected modules and
hubs has also been demonstrated, for both task based and default mode networks,
suggesting that the brain self-organizes into patterns of co-activated sets of
regions for performing specific tasks or in resting state. In this paper, we
report studies on the resilience or robustness of this modular structure: under
systematic erosion of connectivity in the network under thresholding, how
resilient is the modularity and hub structure? The results show that the
network shows show strong resilience properties, with the modularity and hub
structure maintaining itself over a large range of connection strengths. Then,
at a certain critical threshold that falls very close to 0, the connectivity,
the modularity, and hub structure suddenly break down, showing a phase
transition like property. Additionally, the spatial and topological
organization of erosion of connectivity at all levels was found to be
homogenous rather than heterogenous; i.e., no &quot;structural holes&quot; of any
significant sizes were found, and no gradual increases in numbers of components
were detected. Any loss of connectivity is homogenously spread out across the
network. The results suggest that human task-based functional brain networks
are very resilient, where the whole network structure fails only when
connectivity is almost fully removed from the network. The findings may help
further the understanding of dynamics of and relationships between structural
and functional brain networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1556</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1556</id><created>2014-07-06</created><authors><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Dong</keyname><forenames>Mianxiong</forenames></author><author><keyname>Ota</keyname><forenames>Kaoru</forenames></author><author><keyname>Wu</keyname><forenames>Jun</forenames></author><author><keyname>Sato</keyname><forenames>Takuro</forenames></author></authors><title>Energy Efficiency and Spectral Efficiency Tradeoff in Device-to-Device
  (D2D) Communications</title><categories>cs.GT cs.IT math.IT</categories><comments>8 pages, 6 figures, long version paper of IEEE Wireless
  Communications Letters, accepted for publication. arXiv admin note: text
  overlap with arXiv:1405.1963</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we investigate the tradeoff between energy efficiency (EE)
and spectral efficiency (SE) in device-to-device (D2D) communications
underlaying cellular networks with uplink channel reuse. The resource
allocation problem is modeled as a noncooperative game, in which each user
equipment (UE) is self-interested and wants to maximize its own EE. Given the
SE requirement and maximum transmission power constraints, a distributed
energy-efficient resource allocation algorithm is proposed by exploiting the
properties of the nonlinear fractional programming. The relationships between
the EE and SE tradeoff of the proposed algorithm and system parameters are
analyzed and verified through computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1569</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1569</id><created>2014-07-06</created><updated>2015-06-15</updated><authors><author><keyname>Fitch</keyname><forenames>Katherine E.</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>Joint Centrality Distinguishes Optimal Leaders in Noisy Networks</title><categories>math.OC cs.SY</categories><comments>Conditionally accepted to IEEE TCNS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of a network of agents tasked with tracking an
external unknown signal in the presence of stochastic disturbances and under
the condition that only a limited subset of agents, known as leaders, can
measure the signal directly. We investigate the optimal leader selection
problem for a prescribed maximum number of leaders, where the optimal leader
set minimizes total system error defined as steady-state variance about the
external signal. In contrast to previously established greedy algorithms for
optimal leader selection, our results rely on an expression of total system
error in terms of properties of the underlying network graph. We demonstrate
that the performance of any given set of leaders depends on their influence as
determined by a new graph measure of centrality of a set. We define the $joint
\; centrality$ of a set of nodes in a network graph such that a leader set with
maximal joint centrality is an optimal leader set. In the case of a single
leader, we prove that the optimal leader is the node with maximal information
centrality. In the case of multiple leaders, we show that the nodes in the
optimal leader set balance high information centrality with a coverage of the
graph. For special cases of graphs, we solve explicitly for optimal leader
sets. We illustrate with examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1571</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1571</id><created>2014-07-06</created><updated>2015-03-14</updated><authors><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Private Multiplicative Weights Beyond Linear Queries</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of fundamental data analyses in machine learning, such as
linear and logistic regression, require minimizing a convex function defined by
the data. Since the data may contain sensitive information about individuals,
and these analyses can leak that sensitive information, it is important to be
able to solve convex minimization in a privacy-preserving way.
  A series of recent results show how to accurately solve a single convex
minimization problem in a differentially private manner. However, the same data
is often analyzed repeatedly, and little is known about solving multiple convex
minimization problems with differential privacy. For simpler data analyses,
such as linear queries, there are remarkable differentially private algorithms
such as the private multiplicative weights mechanism (Hardt and Rothblum, FOCS
2010) that accurately answer exponentially many distinct queries. In this work,
we extend these results to the case of convex minimization and show how to give
accurate and differentially private solutions to *exponentially many* convex
minimization problems on a sensitive dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1572</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1572</id><created>2014-07-06</created><authors><author><keyname>Ambikasaran</keyname><forenames>Sivaram</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>The Inverse Fast Multipole Method</title><categories>math.NA cs.NA</categories><comments>25 pages, 28 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a new fast direct solver for linear systems arising
out of wide range of applications, integral equations, multivariate statistics,
radial basis interpolation, etc., to name a few. \emph{The highlight of this
new fast direct solver is that the solver scales linearly in the number of
unknowns in all dimensions.} The solver, termed as Inverse Fast Multipole
Method (abbreviated as IFMM), works on the same data-structure as the Fast
Multipole Method (abbreviated as FMM). More generally, the solver can be
immediately extended to the class of hierarchical matrices, denoted as
$\mathcal{H}^2$ matrices with strong admissibility criteria (weak low-rank
structure), i.e., \emph{the interaction between neighboring cluster of
particles is full-rank whereas the interaction between particles corresponding
to well-separated clusters can be efficiently represented as a low-rank
matrix}. The algorithm departs from existing approaches in the fact that
throughout the algorithm the interaction corresponding to neighboring clusters
are always treated as full-rank interactions. Our approach relies on two major
ideas: (i) The $N \times N$ matrix arising out of FMM (from now on termed as
FMM matrix) can be represented as an extended sparser matrix of size $M \times
M$, where $M \approx 3N$. (ii) While solving the larger extended sparser
matrix, \emph{the fill-in's that arise in the matrix blocks corresponding to
well-separated clusters are hierarchically compressed}. The ordering of the
equations and the unknowns in the extended sparser matrix is strongly related
to the local and multipole coefficients in the FMM~\cite{greengard1987fast} and
\emph{the order of elimination is different from the usual nested dissection
approach}. Numerical benchmarks on $2$D manifold confirm the linear scaling of
the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1576</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1576</id><created>2014-07-07</created><authors><author><keyname>Rassaei</keyname><forenames>Farshad</forenames></author><author><keyname>Soh</keyname><forenames>Wee-Seng</forenames></author><author><keyname>Chua</keyname><forenames>Kee-Chaing</forenames></author></authors><title>A Statistical Modelling and Analysis of PHEVs' Power Demand in Smart
  Grids</title><categories>cs.CE stat.AP</categories><comments>6 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric vehicles (EVs) and particularly plug-in hybrid electric vehicles
(PHEVs) are foreseen to become popular in the near future. Not only are they
much more environmentally friendly than conventional internal combustion engine
(ICE) vehicles, their fuel can also be catered from diverse energy sources and
resources. However, they add significant load on the power grid as they become
widespread. The characteristics of this extra load follow the patterns of
people's driving behaviours. In particular, random parameters such as arrival
time and driven distance of the vehicles determine their expected demand
profile from the power grid. In this paper, we first present a model for
uncoordinated charging power demand of PHEVs based on a stochastic process and
accordingly we characterize the EV's expected daily power demand profile. Next,
we adopt different distributions for the EV's charging time following some
available empirical research data in the literature. Simulation results show
that the EV's expected daily power demand profiles obtained under the uniform,
Gaussian with positive support and Rician distributions for charging time are
identical when the first and second order statistics of these distributions are
the same. This gives us useful insights into the long-term planning for
upgrading power systems' infrastructure to accommodate PHEVs. In addition, the
results from this modelling can be incorporated into designing demand response
(DR) algorithms and evaluating the available DR techniques more accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1584</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1584</id><created>2014-07-07</created><authors><author><keyname>Hosseini</keyname><forenames>Hadi</forenames></author><author><keyname>Hoey</keyname><forenames>Jesse</forenames></author><author><keyname>Cohen</keyname><forenames>Robin</forenames></author></authors><title>A Coordinated MDP Approach to Multi-Agent Planning for Resource
  Allocation, with Applications to Healthcare</title><categories>cs.AI cs.MA</categories><comments>6 pages</comments><msc-class>68T37, 68T42</msc-class><acm-class>I.2.11; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a novel approach to scalable multiagent resource
allocation in dynamic settings. We propose an approximate solution in which
each resource consumer is represented by an independent MDP-based agent that
models expected utility using an average model of its expected access to
resources given only limited information about all other agents. A global
auction-based mechanism is proposed for allocations based on expected regret.
We assume truthful bidding and a cooperative coordination mechanism, as we are
considering healthcare scenarios. We illustrate the performance of our
coordinated MDP approach against a Monte-Carlo based planning algorithm
intended for large-scale applications, as well as other approaches suitable for
allocating medical resources. The evaluations show that the global utility
value across all consumer agents is closer to optimal when using our algorithms
under certain time constraints, with low computational cost. As such, we offer
a promising approach for addressing complex resource allocation problems that
arise in healthcare settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1591</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1591</id><created>2014-07-07</created><updated>2015-08-25</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author></authors><title>Consistency Thresholds for the Planted Bisection Model</title><categories>math.PR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The planted bisection model is a random graph model in which the nodes are
divided into two equal-sized communities and then edges are added randomly in a
way that depends on the community membership. We establish necessary and
sufficient conditions for the asymptotic recoverability of the planted
bisection in this model. When the bisection is asymptotically recoverable, we
give an efficient algorithm that successfully recovers it. We also show that
the planted bisection is recoverable asymptotically if and only if with high
probability every node belongs to the same community as the majority of its
neighbors.
  Our algorithm for finding the planted bisection runs in time almost linear in
the number of edges. It has three stages: spectral clustering to compute an
initial guess, a &quot;replica&quot; stage to get almost every vertex correct, and then
some simple local moves to finish the job. An independent work by Abbe,
Bandeira, and Hall establishes similar (slightly weaker) results but only in
the case of logarithmic average degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1593</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1593</id><created>2014-07-07</created><updated>2015-06-24</updated><authors><author><keyname>Batselier</keyname><forenames>Kim</forenames></author><author><keyname>Liu</keyname><forenames>Haotian</forenames></author><author><keyname>Wong</keyname><forenames>Ngai</forenames></author></authors><title>A Constructive Algorithm for Decomposing a Tensor into a Finite Sum of
  Orthonormal Rank-1 Terms</title><categories>math.NA cs.NA</categories><comments>Added subsection on orthogonal complement tensors. Added constructive
  proof of maximal CP-rank of a 2x2x2 tensor. Added perturbation of singular
  values result. Added conversion of the TTr1 decomposition to the Tucker
  decomposition. Added example that demonstrates how the rank behaves when
  subtracting rank-1 terms. Added example with exponential decaying singular
  values</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a constructive algorithm that decomposes an arbitrary real tensor
into a finite sum of orthonormal rank-1 outer products. The algorithm, named
TTr1SVD, works by converting the tensor into a tensor-train rank-1 (TTr1)
series via the singular value decomposition (SVD). TTr1SVD naturally
generalizes the SVD to the tensor regime with properties such as uniqueness for
a fixed order of indices, orthogonal rank-1 outer product terms, and easy
truncation error quantification. Using an outer product column table it also
allows, for the first time, a complete characterization of all tensors
orthogonal with the original tensor. Incidentally, this leads to a strikingly
simple constructive proof showing that the maximum rank of a real $2 \times 2
\times 2$ tensor over the real field is 3. We also derive a conversion of the
TTr1 decomposition into a Tucker decomposition with a sparse core tensor.
Numerical examples illustrate each of the favorable properties of the TTr1
decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1598</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1598</id><created>2014-07-07</created><updated>2014-12-08</updated><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal M.</forenames><affiliation>GREYC</affiliation></author></authors><title>Low Complexity Regularization of Linear Inverse Problems</title><categories>math.OC cs.IT math.IT stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse problems and regularization theory is a central theme in contemporary
signal processing, where the goal is to reconstruct an unknown signal from
partial indirect, and possibly noisy, measurements of it. A now standard method
for recovering the unknown signal is to solve a convex optimization problem
that enforces some prior knowledge about its structure. This has proved
efficient in many problems routinely encountered in imaging sciences,
statistics and machine learning. This chapter delivers a review of recent
advances in the field where the regularization prior promotes solutions
conforming to some notion of simplicity/low-complexity. These priors encompass
as popular examples sparsity and group sparsity (to capture the compressibility
of natural signals and images), total variation and analysis sparsity (to
promote piecewise regularity), and low-rank (as natural extension of sparsity
to matrix-valued data). Our aim is to provide a unified treatment of all these
regularizations under a single umbrella, namely the theory of partial
smoothness. This framework is very general and accommodates all low-complexity
regularizers just mentioned, as well as many others. Partial smoothness turns
out to be the canonical way to encode low-dimensional models that can be linear
spaces or more general smooth manifolds. This review is intended to serve as a
one stop shop toward the understanding of the theoretical properties of the
so-regularized solutions. It covers a large spectrum including: (i) recovery
guarantees and stability to noise, both in terms of $\ell^2$-stability and
model (manifold) identification; (ii) sensitivity analysis to perturbations of
the parameters involved (in particular the observations), with applications to
unbiased risk estimation ; (iii) convergence properties of the forward-backward
proximal splitting scheme, that is particularly well suited to solve the
corresponding large-scale regularized optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1601</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1601</id><created>2014-07-07</created><updated>2015-07-28</updated><authors><author><keyname>Bitar</keyname><forenames>Eilyan</forenames></author><author><keyname>Xu</keyname><forenames>Yunjian</forenames></author></authors><title>Deadline Differentiated Pricing of Deferrable Electric Loads</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large fraction of the total electric load is comprised of end-use devices
whose demand for energy is inherently deferrable in time. Of interest is the
potential to leverage on such latent flexibility in demand to absorb
variability in power supplied from intermittent renewable generation. The
challenge, however, lies in designing incentives to reliably induce the desired
response in demand. With an eye to electric vehicle charging, we propose a
novel forward market for differentiated electric power services, where
consumers consent to deferred service of pre-specified loads in exchange for a
reduced per-unit price for energy. The longer a consumer is willing to defer,
the larger the reduction in price. The proposed forward contract provides a
guarantee on the aggregate quantity of energy to be delivered by a
consumer-specified deadline. Under the earliest-deadline-first (EDF) scheduling
policy, which is shown to be optimal for the supplier, we explicitly
characterize a non-discriminatory, deadline-differentiated pricing scheme that
yields an efficient competitive equilibrium between the supplier and consumers.
We further show that this efficient pricing scheme, in combination with EDF
scheduling, is incentive compatible (IC) in that every consumer would like to
reveal her true deadline to the supplier, regardless of the actions taken by
other consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1605</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1605</id><created>2014-07-07</created><authors><author><keyname>Lecuit</keyname><forenames>&#xc9;meline</forenames><affiliation>LLL</affiliation></author><author><keyname>Maurel</keyname><forenames>Denis</forenames><affiliation>LI</affiliation></author><author><keyname>Vitas</keyname><forenames>Dusko</forenames></author></authors><title>Les noms propres se traduisent-ils ? \'Etude d'un corpus multilingue</title><categories>cs.CL</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Corpus 10 (2011) 201-218</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of the translation of proper names. We
introduce our hypothesis according to which proper names can be translated more
often than most people seem to think. Then, we describe the construction of a
parallel multilingual corpus used to illustrate our point. We eventually
evaluate both the advantages and limits of this corpus in our study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1610</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1610</id><created>2014-07-07</created><updated>2014-09-22</updated><authors><author><keyname>Agrawal</keyname><forenames>Pulkit</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Analyzing the Performance of Multilayer Neural Networks for Object
  Recognition</title><categories>cs.CV cs.NE</categories><comments>Published in European Conference on Computer Vision 2014 (ECCV-2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last two years, convolutional neural networks (CNNs) have achieved an
impressive suite of results on standard recognition datasets and tasks.
CNN-based features seem poised to quickly replace engineered representations,
such as SIFT and HOG. However, compared to SIFT and HOG, we understand much
less about the nature of the features learned by large CNNs. In this paper, we
experimentally probe several aspects of CNN feature learning in an attempt to
help practitioners gain useful, evidence-backed intuitions about how to apply
CNNs to computer vision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1629</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1629</id><created>2014-07-07</created><authors><author><keyname>Dehghan</keyname><forenames>Mostafa</forenames></author><author><keyname>Seetharam</keyname><forenames>Anand</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author><author><keyname>Salonidis</keyname><forenames>Theodoros</forenames></author><author><keyname>Kurose</keyname><forenames>Jim</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Optimal Caching and Routing in Hybrid Networks</title><categories>cs.NI</categories><comments>submitted to Milcom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid networks consisting of MANET nodes and cellular infrastructure have
been recently proposed to improve the performance of military networks. Prior
work has demonstrated the benefits of in-network content caching in a wired,
Internet context. We investigate the problem of developing optimal routing and
caching policies in a hybrid network supporting in-network caching with the
goal of minimizing overall content-access delay. Here, needed content may
always be accessed at a back-end server via the cellular infrastructure;
alternatively, content may also be accessed via cache-equipped &quot;cluster&quot; nodes
within the MANET. To access content, MANET nodes must thus decide whether to
route to in-MANET cluster nodes or to back-end servers via the cellular
infrastructure; the in-MANET cluster nodes must additionally decide which
content to cache. We model the cellular path as either i) a
congestion-insensitive fixed-delay path or ii) a congestion-sensitive path
modeled as an M/M/1 queue. We demonstrate that under the assumption of
stationary, independent requests, it is optimal to adopt static caching (i.e.,
to keep a cache's content fixed over time) based on content popularity. We also
show that it is optimal to route to in-MANET caches for content cached there,
but to route requests for remaining content via the cellular infrastructure for
the congestion-insensitive case and to split traffic between the in-MANET
caches and cellular infrastructure for the congestion-sensitive case. We
develop a simple distributed algorithm for the joint routing/caching problem
and demonstrate its efficacy via simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1637</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1637</id><created>2014-07-07</created><authors><author><keyname>Gagarin</keyname><forenames>Andrei</forenames></author><author><keyname>Zverovich</keyname><forenames>Vadim</forenames></author></authors><title>Bounds and algorithms for limited packings in graphs</title><categories>cs.DM math.CO</categories><comments>extended abstract, 4 pages, Proc. 9th International Colloquium on
  Graph Theory and Combinatorics, ICGT 2014, Grenoble, France, June 30-July 4,
  2014, paper no.27</comments><msc-class>05C70, 05C85, 68W20, 68R10, 90B15, 90B10, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider (closed neighbourhood) packings and their generalization in
graphs called limited packings. A vertex set X in a graph G is a k-limited
packing if for any vertex $v\in V(G)$, $\left|N[v] \cap X\right| \le k$, where
$N[v]$ is the closed neighbourhood of $v$. The k-limited packing number
$L_k(G)$ is the largest size of a k-limited packing in G. Limited packing
problems can be considered as secure facility location problems in networks. We
develop probabilistic and greedy approaches to limited packings in graphs,
providing lower bounds for the k-limited packing number, and randomized and
greedy algorithms to find k-limited packings satisfying the bounds. Some upper
bounds for $L_k(G)$ are given as well. The problem of finding a maximum size
k-limited packing is known to be NP-complete even in split or bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1640</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1640</id><created>2014-07-07</created><authors><author><keyname>Gao</keyname><forenames>Bin</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>WordRep: A Benchmark for Research on Learning Word Representations</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  WordRep is a benchmark collection for the research on learning distributed
word representations (or word embeddings), released by Microsoft Research. In
this paper, we describe the details of the WordRep collection and show how to
use it in different types of machine learning research related to word
embedding. Specifically, we describe how the evaluation tasks in WordRep are
selected, how the data are sampled, and how the evaluation tool is built. We
then compare several state-of-the-art word representations on WordRep, report
their evaluation performance, and make discussions on the results. After that,
we discuss new potential research topics that can be supported by WordRep, in
addition to algorithm comparison. We hope that this paper can help people gain
deeper understanding of WordRep, and enable more interesting research on
learning distributed word representations and related topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1647</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1647</id><created>2014-07-07</created><authors><author><keyname>Kehagias</keyname><forenames>Athanasios</forenames></author><author><keyname>Konstantinidis</keyname><forenames>Georgios</forenames></author></authors><title>Cops and Robbers, Game Theory and Zermelo's Early Results</title><categories>cs.DM cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a game theoretic framework for the game of cops and robbers (CR).
Within this framework we study certain assumptions which underlie the concepts
of optimal strategies and capture time. We also point out a connection of these
concepts to early work by Zermelo and D. Konig. Finally, we discuss the
relationship between CR and related pursuit games to reachability games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1660</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1660</id><created>2014-07-07</created><authors><author><keyname>Mardani</keyname><forenames>Morteza</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Estimating Traffic and Anomaly Maps via Network Tomography</title><categories>cs.NI</categories><comments>16 pages, 9 Figures, submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mapping origin-destination (OD) network traffic is pivotal for network
management and proactive security tasks. However, lack of sufficient flow-level
measurements as well as potential anomalies pose major challenges towards this
goal. Leveraging the spatiotemporal correlation of nominal traffic, and the
sparse nature of anomalies, this paper brings forth a novel framework to map
out nominal and anomalous traffic, which treats jointly important network
monitoring tasks including traffic estimation, anomaly detection, and traffic
interpolation. To this end, a convex program is first formulated with nuclear
and $\ell_1$-norm regularization to effect sparsity and low rank for the
nominal and anomalous traffic with only the link counts and a {\it small}
subset of OD-flow counts. Analysis and simulations confirm that the proposed
estimator can {\em exactly} recover sufficiently low-dimensional nominal
traffic and sporadic anomalies so long as the routing paths are sufficiently
&quot;spread-out&quot; across the network, and an adequate amount of flow counts are
randomly sampled. The results offer valuable insights about data acquisition
strategies and network scenaria giving rise to accurate traffic estimation. For
practical networks where the aforementioned conditions are possibly violated,
the inherent spatiotemporal traffic patterns are taken into account by adopting
a Bayesian approach along with a bilinear characterization of the nuclear and
$\ell_1$ norms. The resultant nonconvex program involves quadratic regularizers
with correlation matrices, learned systematically from (cyclo)stationary
historical data. Alternating-minimization based algorithms with provable
convergence are also developed to procure the estimates. Insightful tests with
synthetic and real Internet data corroborate the effectiveness of the novel
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1664</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1664</id><created>2014-07-07</created><updated>2014-12-01</updated><authors><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Darst</keyname><forenames>Richard K.</forenames></author><author><keyname>Iacovacci</keyname><forenames>Jacopo</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Triadic closure as a basic generating mechanism of communities in
  complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 14 figures. Published version</comments><journal-ref>Phys. Rev. E 90, 042806 (2014)</journal-ref><doi>10.1103/PhysRevE.90.042806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the complex social, technological and biological networks have a
significant community structure. Therefore the community structure of complex
networks has to be considered as a universal property, together with the much
explored small-world and scale-free properties of these networks. Despite the
large interest in characterizing the community structures of real networks, not
enough attention has been devoted to the detection of universal mechanisms able
to spontaneously generate networks with communities. Triadic closure is a
natural mechanism to make new connections, especially in social networks. Here
we show that models of network growth based on simple triadic closure naturally
lead to the emergence of community structure, together with fat-tailed
distributions of node degree, high clustering coefficients. Communities emerge
from the initial stochastic heterogeneity in the concentration of links,
followed by a cycle of growth and fragmentation. Communities are the more
pronounced, the sparser the graph, and disappear for high values of link
density and randomness in the attachment procedure. By introducing a
fitness-based link attractivity for the nodes, we find a novel phase
transition, where communities disappear for high heterogeneity of the fitness
distribution, but a new mesoscopic organization of the nodes emerges, with
groups of nodes being shared between just a few superhubs, which attract most
of the links of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1667</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1667</id><created>2014-07-07</created><updated>2014-07-09</updated><authors><author><keyname>Nain</keyname><forenames>Sumit</forenames><affiliation>Rice University</affiliation></author><author><keyname>Lustig</keyname><forenames>Yoad</forenames><affiliation>Rice University</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y</forenames><affiliation>Rice University</affiliation></author></authors><title>Synthesis from Probabilistic Components</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 30,
  2014) lmcs:1181</journal-ref><doi>10.2168/LMCS-10(2:17)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis is the automatic construction of a system from its specification.
In classical synthesis algorithms, it is always assumed that the system is
&quot;constructed from scratch&quot; rather than composed from reusable components. This,
of course, rarely happens in real life, where almost every non-trivial
commercial software system relies heavily on using libraries of reusable
components. Furthermore, other contexts, such as web-service orchestration, can
be modeled as synthesis of a system from a library of components. Recently,
Lustig and Vardi introduced dataflow and control-flow synthesis from libraries
of reusable components. They proved that dataflow synthesis is undecidable,
while control-flow synthesis is decidable. In this work, we consider the
problem of control-flow synthesis from libraries of probabilistic components .
We show that this more general problem is also decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1670</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1670</id><created>2014-07-07</created><authors><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Trotignon</keyname><forenames>Nicolas</forenames></author></authors><title>Equistarable graphs and counterexamples to three conjectures on
  equistable graphs</title><categories>math.CO cs.DM</categories><msc-class>05C22, 05C50, 05C69, 05C76</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Equistable graphs are graphs admitting positive weights on vertices such that
a subset of vertices is a maximal stable set if and only if it is of total
weight $1$. In $1994$, Mahadev et al.~introduced a subclass of equistable
graphs, called strongly equistable graphs, as graphs such that for every $c \le
1$ and every non-empty subset $T$ of vertices that is not a maximal stable set,
there exist positive vertex weights such that every maximal stable set is of
total weight $1$ and the total weight of $T$ does not equal $c$. Mahadev et al.
conjectured that every equistable graph is strongly equistable. General
partition graphs are the intersection graphs of set systems over a finite
ground set $U$ such that every maximal stable set of the graph corresponds to a
partition of $U$. In $2009$, Orlin proved that every general partition graph is
equistable, and conjectured that the converse holds as well.
  Orlin's conjecture, if true, would imply the conjecture due to Mahadev,
Peled, and Sun. An intermediate conjecture, one that would follow from Orlin's
conjecture and would imply the conjecture by Mahadev, Peled, and Sun, was posed
by Miklavi\v{c} and Milani\v{c} in $2011$, and states that every equistable
graph has a clique intersecting all maximal stable sets. The above conjectures
have been verified for several graph classes. We introduce the notion of
equistarable graphs and based on it construct counterexamples to all three
conjectures within the class of complements of line graphs of triangle-free
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1687</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1687</id><created>2014-07-07</created><updated>2014-09-05</updated><authors><author><keyname>Cui</keyname><forenames>Qing</forenames></author><author><keyname>Gao</keyname><forenames>Bin</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author><author><keyname>Qiu</keyname><forenames>Siyu</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>KNET: A General Framework for Learning Word Embedding using
  Morphological Knowledge</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Neural network techniques are widely applied to obtain high-quality
distributed representations of words, i.e., word embeddings, to address text
mining, information retrieval, and natural language processing tasks. Recently,
efficient methods have been proposed to learn word embeddings from context that
captures both semantic and syntactic relationships between words. However, it
is challenging to handle unseen words or rare words with insufficient context.
In this paper, inspired by the study on word recognition process in cognitive
psychology, we propose to take advantage of seemingly less obvious but
essentially important morphological knowledge to address these challenges. In
particular, we introduce a novel neural network architecture called KNET that
leverages both contextual information and morphological word similarity built
based on morphological knowledge to learn word embeddings. Meanwhile, the
learning architecture is also able to refine the pre-defined morphological
knowledge and obtain more accurate word similarity. Experiments on an
analogical reasoning task and a word similarity task both demonstrate that the
proposed KNET framework can greatly enhance the effectiveness of word
embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1689</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1689</id><created>2014-07-07</created><updated>2015-01-15</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Anup</forenames></author><author><keyname>Issac</keyname><forenames>Davis</forenames></author><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Sampling in Space Restricted Settings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space efficient algorithms play a central role in dealing with large amount
of data. In such settings, one would like to analyse the large data using small
amount of &quot;working space&quot;. One of the key steps in many algorithms for
analysing large data is to maintain a (or a small number) random sample from
the data points. In this paper, we consider two space restricted settings --
(i) streaming model, where data arrives over time and one can use only a small
amount of storage, and (ii) query model, where we can structure the data in low
space and answer sampling queries. In this paper, we prove the following
results in above two settings:
  - In the streaming setting, we would like to maintain a random sample from
the elements seen so far. We prove that one can maintain a random sample using
$O(\log n)$ random bits and $O(\log n)$ space, where $n$ is the number of
elements seen so far. We can extend this to the case when elements have weights
as well.
  - In the query model, there are $n$ elements with weights $w_1, ..., w_n$
(which are $w$-bit integers) and one would like to sample a random element with
probability proportional to its weight. Bringmann and Larsen (STOC 2013) showed
how to sample such an element using $nw +1 $ space (whereas, the information
theoretic lower bound is $n w$). We consider the approximate sampling problem,
where we are given an error parameter $\varepsilon$, and the sampling
probability of an element can be off by an $\varepsilon$ factor. We give
matching upper and lower bounds for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1697</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1697</id><created>2014-07-07</created><updated>2014-07-23</updated><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Martin</keyname><forenames>Clyde F.</forenames></author></authors><title>L1 Control Theoretic Smoothing Splines</title><categories>cs.IT cs.SY math.IT math.OC stat.CO</categories><comments>Accepted for publication in IEEE Signal Processing Letters. 4 pages
  (twocolumn), 5 figures</comments><doi>10.1109/LSP.2014.2337017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose control theoretic smoothing splines with L1
optimality for reducing the number of parameters that describes the fitted
curve as well as removing outlier data. A control theoretic spline is a
smoothing spline that is generated as an output of a given linear dynamical
system. Conventional design requires exactly the same number of base functions
as given data, and the result is not robust against outliers. To solve these
problems, we propose to use L1 optimality, that is, we use the L1 norm for the
regularization term and/or the empirical risk term. The optimization is
described by a convex optimization, which can be efficiently solved via a
numerical optimization software. A numerical example shows the effectiveness of
the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1706</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1706</id><created>2014-07-07</created><authors><author><keyname>Cray</keyname><forenames>Henri Perret du</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author></authors><title>Improved FPT algorithms for weighted independent set in bull-free graphs</title><categories>cs.DS cs.DM</categories><comments>15 pages</comments><msc-class>68R10, 05C85</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very recently, Thomass\'e, Trotignon and Vuskovic [WG 2014] have given an FPT
algorithm for Weighted Independent Set in bull-free graphs parameterized by the
weight of the solution, running in time $2^{O(k^5)} \cdot n^9$. In this article
we improve this running time to $2^{O(k^2)} \cdot n^7$. As a byproduct, we also
improve the previous Turing-kernel for this problem from $O(k^5)$ to $O(k^2)$.
Furthermore, for the subclass of bull-free graphs without holes of length at
most $2p-1$ for $p \geq 3$, we speed up the running time to $2^{O(k \cdot
k^{\frac{1}{p-1}})} \cdot n^7$. As $p$ grows, this running time is
asymptotically tight in terms of $k$, since we prove that for each integer $p
\geq 3$, Weighted Independent Set cannot be solved in time $2^{o(k)} \cdot
n^{O(1)}$ in the class of $\{bull,C_4,\ldots,C_{2p-1}\}$-free graphs unless the
ETH fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1723</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1723</id><created>2014-07-07</created><authors><author><keyname>M&#xf6;llenhoff</keyname><forenames>Thomas</forenames></author><author><keyname>Strekalovskiy</keyname><forenames>Evgeny</forenames></author><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings</title><categories>math.NA cs.CV cs.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the analysis of a recent reformulation of the
primal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischof
and Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], which
allows to apply it to nonconvex regularizers as first proposed for truncated
quadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, it
investigates variational problems for which the energy to be minimized can be
written as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is a
linear operator. We study the method and prove convergence in the case where
the nonconvexity of $F$ is compensated by the strong convexity of the $G$. The
convergence proof yields an interesting requirement for the choice of algorithm
parameters, which we show to not only be sufficient, but necessary.
Additionally, we show boundedness of the iterates under much weaker conditions.
Finally, we demonstrate effectiveness and convergence of the algorithm beyond
the theoretical guarantees in several numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1737</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1737</id><created>2014-07-07</created><authors><author><keyname>Darzi</keyname><forenames>Touseef Yousuf</forenames></author><author><keyname>Zabi</keyname><forenames>Aminuddin</forenames></author><author><keyname>M</keyname><forenames>Pallavi</forenames></author></authors><title>An Approach for Controlling Faults in Wireless Sensor Networks Using
  Clustering</title><categories>cs.NI</categories><comments>7 pages, 11 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT). arXiv admin note: text overlap
  with arXiv:1209.4751 by other authors without attribution</comments><journal-ref>IJETT, Vol.12, No.6, pp.286-292, Jun 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V12P256</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Fault control and tolerance in wireless sensor network is a challenging
problem because of limited energy, bandwidth, and computational complexity.
While facing numerous threats these severely resource constrained nodes are
responsible for data collection, data processing, localization, time
synchronization aggregation and data forwarding. One of the effective
approaches to control and tolerate these threats is through clustering. In this
paper we present a new method called Efficient Fault Control Mechanism for
fault controlling in wireless sensor networks based on clustering and
cluster-head selection. Simulation results show Efficient Fault Control
Mechanism has better performance over state of art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1746</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1746</id><created>2014-07-07</created><updated>2016-02-10</updated><authors><author><keyname>Kurpisz</keyname><forenames>Adam</forenames></author><author><keyname>Lepp&#xe4;nen</keyname><forenames>Samuli</forenames></author><author><keyname>Mastrolilli</keyname><forenames>Monaldo</forenames></author></authors><title>Sum-of-squares hierarchy lower bounds for symmetric formulations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method for proving Sum-of-Squares (SoS)/ Lasserre hierarchy
lower bounds when the initial problem formulation exhibits a high degree of
symmetry. Our main technical theorem allows us to reduce the study of the
positive semidefiniteness to the analysis of &quot;well-behaved&quot; univariate
polynomial inequalities.
  We illustrate the technique on two problems, one unconstrained and the other
with constraints. More precisely, we give a short elementary proof of
Grigoriev/Laurent lower bound for finding the integer cut polytope of the
complete graph. We also show that the SoS hierarchy requires a non-constant
number of rounds to improve the initial integrality gap of 2 for the
Min-Knapsack linear program strengthened with cover inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1756</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1756</id><created>2014-07-07</created><updated>2014-09-25</updated><authors><author><keyname>Liu</keyname><forenames>Xin-Ji</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author><author><keyname>Dai</keyname><forenames>Tao</forenames></author></authors><title>Deterministic Construction of Binary Measurement Matrices with Various
  Sizes</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general framework to deterministically construct binary
measurement matrices for compressed sensing. The proposed matrices are composed
of (circulant) permutation submatrix blocks and zero submatrix blocks, thus
making their hardware realization convenient and easy. Firstly, using the
famous Johnson bound for binary constant weight codes, we derive a new lower
bound for the coherence of binary matrices with uniform column weights.
Afterwards, a large class of binary base matrices with coherence asymptotically
achieving this new bound are presented. Finally, by choosing proper rows and
columns from these base matrices, we construct the desired measurement matrices
with various sizes and they show empirically comparable performance to that of
the corresponding Gaussian matrices
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1768</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1768</id><created>2014-07-07</created><authors><author><keyname>Zerenner</keyname><forenames>Tanja</forenames></author><author><keyname>Venema</keyname><forenames>Victor</forenames></author><author><keyname>Friederichs</keyname><forenames>Petra</forenames></author><author><keyname>Simmer</keyname><forenames>Clemens</forenames></author></authors><title>Downscaling near-surface atmospheric fields with multi-objective Genetic
  Programming</title><categories>physics.ao-ph cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coupling of models for the different components of the
Soil-Vegetation-Atmosphere-System is required to investigate component
interactions and feedback processes. However, the component models for
atmosphere, land-surface and subsurface are usually operated at different
resolutions in space and time owing to the dominant processes. The
computationally often more expensive atmospheric models, for instance, are
typically employed at a coarser resolution than land-surface and subsurface
models. Thus up- and downscaling procedures are required at the interface
between the atmospheric model and the land-surface/subsurface models. We apply
multi-objective Genetic Programming (GP) to a training data set of
high-resolution atmospheric model runs to learn equations or short programs
that reconstruct the fine-scale fields (e.g., 400 m resolution) of the
near-surface atmospheric state variables from the coarse atmospheric model
output (e.g., 2.8 km resolution). Like artificial neural networks, GP can
flexibly incorporate multivariate and nonlinear relations, but offers the
advantage that the solutions are human readable and thus can be checked for
physical consistency. Using the Strength Pareto Approach for multi-objective
fitness assignment allows us to consider multiple characteristics of the
fine-scale fields during the learning procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1772</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1772</id><created>2014-07-07</created><authors><author><keyname>Wang</keyname><forenames>Senzhang</forenames></author><author><keyname>Xie</keyname><forenames>Sihong</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoming</forenames></author><author><keyname>Li</keyname><forenames>Zhoujun</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Shu</keyname><forenames>Xinyu</forenames></author></authors><title>Future Influence Ranking of Scientific Literature</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>9 pages, Proceedings of the 2014 SIAM International Conference on
  Data Mining</comments><doi>10.1137/1.9781611973440.86</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers or students entering a emerging research area are particularly
interested in what newly published papers will be most cited and which young
researchers will become influential in the future, so that they can catch the
most recent advances and find valuable research directions. However, predicting
the future importance of scientific articles and authors is challenging due to
the dynamic nature of literature networks and evolving research topics.
Different from most previous studies aiming to rank the current importance of
literatures and authors, we focus on \emph{ranking the future popularity of new
publications and young researchers} by proposing a unified ranking model to
combine various available information. Specifically, we first propose to
extract two kinds of text features, words and words co-occurrence to
characterize innovative papers and authors. Then, instead of using static and
un-weighted graphs, we construct time-aware weighted graphs to distinguish the
various importance of links established at different time. Finally, by
leveraging both the constructed text features and graphs, we propose a mutual
reinforcement ranking framework called \emph{MRFRank} to rank the future
importance of papers and authors simultaneously. Experimental results on the
ArnetMiner dataset show that the proposed approach significantly outperforms
the baselines on the metric \emph{recommendation intensity}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1779</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1779</id><created>2014-07-07</created><updated>2014-07-26</updated><authors><author><keyname>Bul&#xed;n</keyname><forenames>Jakub</forenames></author></authors><title>On the complexity of $\mathbb H$-coloring for special oriented trees</title><categories>math.CO cs.CC cs.DM</categories><comments>19 pages, minor revisions, corrected a mistake in the definition
  above Corollary 5.10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a fixed digraph $\mathbb H$, the $\mathbb H$-coloring problem is the
problem of deciding whether a given input digraph $\mathbb G$ admits a
homomorphism to $\mathbb H$. The CSP dichotomy conjecture of Feder and Vardi is
equivalent to proving that, for any $\mathbb H$, the $\mathbb H$-coloring
problem is in in P or NP-complete. We confirm this dichotomy for a certain
class of oriented trees, which we call special trees (generalizing earlier
results on special triads and polyads). Moreover, we prove that every tractable
special oriented tree has bounded width, i.e., the corresponding $\mathbb
H$-coloring problem is solvable by local consistency checking. Our proof relies
on recent algebraic tools, namely characterization of congruence
meet-semidistributivity via pointing operations and absorption theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1785</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1785</id><created>2014-07-07</created><updated>2014-10-30</updated><authors><author><keyname>Zhang</keyname><forenames>Zemin</forenames></author><author><keyname>Ely</keyname><forenames>Gregory</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Hao</keyname><forenames>Ning</forenames></author><author><keyname>Kilmer</keyname><forenames>Misha</forenames></author></authors><title>Novel methods for multilinear data completion and de-noising based on
  tensor-SVD</title><categories>cs.CV</categories><comments>8 pages, 8 figures. It is accepted as CVPR 2014 oral presentation.
  arXiv admin note: substantial text overlap with arXiv:1307.0805</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose novel methods for completion (from limited samples)
and de-noising of multilinear (tensor) data and as an application consider 3-D
and 4- D (color) video data completion and de-noising. We exploit the recently
proposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, the
notion of multilinear rank and a related tensor nuclear norm was proposed in
[11] to characterize informational and structural complexity of multilinear
data. We first show that videos with linear camera motion can be represented
more efficiently using t-SVD compared to the approaches based on vectorizing or
flattening of the tensors. Since efficiency in representation implies
efficiency in recovery, we outline a tensor nuclear norm penalized algorithm
for video completion from missing entries. Application of the proposed
algorithm for video recovery from missing entries is shown to yield a superior
performance over existing methods. We also consider the problem of tensor
robust Principal Component Analysis (PCA) for de-noising 3-D video data from
sparse random corruptions. We show superior performance of our method compared
to the matrix robust PCA adapted to this setting as proposed in [4].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1786</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1786</id><created>2014-07-07</created><updated>2015-07-17</updated><authors><author><keyname>Noh</keyname><forenames>Song</forenames></author><author><keyname>Zoltowski</keyname><forenames>Michael D.</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author></authors><title>Training Sequence Design for Feedback Assisted Hybrid Beamforming in
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 9 figures, replaced with revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of large-scale antenna systems in future commercial wireless
communications is an emerging technology that uses an excess of transmit
antennas to realize high spectral efficiency. Achieving potential gains with
large-scale antenna arrays in practice hinges on sufficient channel estimation
accuracy. Much prior work focuses on TDD based networks, relying on reciprocity
between the uplink and downlink channels. However, most currently deployed
commercial wireless systems are FDD based, making it difficult to exploit
channel reciprocity. In massive MIMO FDD systems, the problem of channel
estimation becomes even more challenging due to the attendant substantial
training resources and feedback requirements which scale with the number of
antennas. In this paper, we consider the problem of training sequence design
that employs a set of training signals and its mapping to the training periods.
We focus on reduced-dimension training sequence designs, along with transmit
precoder designs, aimed at reducing both hardware complexity and power
consumption. The resulting designs are extended to hybrid analog-digital
beamforming systems, which employ a limited number of active RF chains for
transmit precoding, by applying the Toeplitz distribution theorem to
large-scale linear antenna systems. A practical guideline for training sequence
parameter selection is presented along with performance analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1807</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1807</id><created>2014-07-04</created><authors><author><keyname>Shatnawi</keyname><forenames>Raed</forenames></author><author><keyname>Althebyan</keyname><forenames>Qutaibah</forenames></author><author><keyname>Ghalib</keyname><forenames>Baraq</forenames></author><author><keyname>Al-Maolegi</keyname><forenames>Mohammed</forenames></author></authors><title>Building A Smart Academic Advising System Using Association Rule Mining</title><categories>cs.DB</categories><comments>5 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In an academic environment, student advising is considered a paramount
activity for both advisors and student to improve the academic performance of
students. In universities of large numbers of students, advising is a
time-consuming activity that may take a considerable effort of advisors and
university administration in guiding students to complete their registration
successfully and efficiently. Current systems are traditional and depend
greatly on the effort of the advisor to find the best selection of courses to
improve students performance. There is a need for a smart system that can
advise a large number of students every semester. In this paper, we propose a
smart system that uses association rule mining to help both students and
advisors in selecting and prioritizing courses. The system helps students to
improve their performance by suggesting courses that meet their current needs
and at the same time improve their academic performance. The system uses
association rule mining to find associations between courses that have been
registered by students in many previous semesters. The system successfully
generates a list of association rules that guide a particular student to select
courses registered by similar students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1808</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1808</id><created>2014-07-07</created><authors><author><keyname>Hariharan</keyname><forenames>Bharath</forenames></author><author><keyname>Arbel&#xe1;ez</keyname><forenames>Pablo</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Simultaneous Detection and Segmentation</title><categories>cs.CV</categories><comments>To appear in the European Conference on Computer Vision (ECCV), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to detect all instances of a category in an image and, for each
instance, mark the pixels that belong to it. We call this task Simultaneous
Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS
requires a segmentation and not just a box. Unlike classical semantic
segmentation, we require individual object instances. We build on recent work
that uses convolutional neural networks to classify category-independent region
proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We
then use category-specific, top- down figure-ground predictions to refine our
bottom-up proposals. We show a 7 point boost (16% relative) over our baselines
on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic
segmentation, and state-of-the-art performance in object detection. Finally, we
provide diagnostic tools that unpack performance and provide directions for
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1809</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1809</id><created>2014-07-07</created><authors><author><keyname>Abuelenin</keyname><forenames>Sherif M.</forenames></author></authors><title>Decomposed Interval Type-2 Fuzzy Systems with Application to Inverted
  Pendulum</title><categories>cs.SY</categories><comments>5 pages, 10 figures, presented in ICET 2014 (2nd International
  Conference on Engineering and Technology,Cairo, Egypt, Apr 19-20, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces the idea of decomposition of interval Type-2 fuzzy
logic system into two parallel type-1 fuzzy systems. This decomposition avoids
the problems associated with type-reduction techniques normally needed in
type-2 fuzzy systems. Next, we compare the performance of a decomposed type-2
controller to the performance of a type-1 controller in stabilizing an inverted
pendulum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1830</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1830</id><created>2014-07-06</created><authors><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Rost</keyname><forenames>Peter</forenames></author></authors><title>The Role of Computational Outage in Dense Cloud-Based Centralized Radio
  Access Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>7 pages, 10 figures, IEEE Global Telecommunication Conference
  (GLOBECOM), 2014, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centralized radio access network architectures consolidate the baseband
operation towards a cloud-based platform, thereby allowing for efficient
utilization of computing assets, effective inter-cell coordination, and
exploitation of global channel state information. This paper considers the
interplay between computational efficiency and data throughput that is
fundamental to centralized RAN. It introduces the concept of computational
outage in mobile networks, and applies it to the analysis of complexity
constrained dense centralized RAN networks. The framework is applied to
single-cell and multi-cell scenarios using parameters drawn from the LTE
standard. It is found that in computationally limited networks, the effective
throughput can be improved by using a computationally aware policy for
selecting the modulation and coding scheme, which sacrifices spectral
efficiency in order to reduce the computational outage probability. When
signals of multiple base stations are processed centrally, a computational
diversity benefit emerges, and the benefit grows with increasing user density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1853</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1853</id><created>2014-07-07</created><updated>2014-07-25</updated><authors><author><keyname>Farczadi</keyname><forenames>Linda</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>K&#xf6;nemann</keyname><forenames>Jochen</forenames></author></authors><title>Stable marriage with general preferences</title><categories>cs.GT</categories><comments>This is an extended version of a paper to appear at the The 7th
  International Symposium on Algorithmic Game Theory (SAGT 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalization of the classical stable marriage problem. In our
model, the preferences on one side of the partition are given in terms of
arbitrary binary relations, which need not be transitive nor acyclic. This
generalization is practically well-motivated, and as we show, encompasses the
well studied hard variant of stable marriage where preferences are allowed to
have ties and to be incomplete. As a result, we prove that deciding the
existence of a stable matching in our model is NP-complete. Complementing this
negative result we present a polynomial-time algorithm for the above decision
problem in a significant class of instances where the preferences are
asymmetric. We also present a linear programming formulation whose feasibility
fully characterizes the existence of stable matchings in this special case.
Finally, we use our model to study a long standing open problem regarding the
existence of cyclic 3D stable matchings. In particular, we prove that the
problem of deciding whether a fixed 2D perfect matching can be extended to a 3D
stable matching is NP-complete, showing this way that a natural attempt to
resolve the existence (or not) of 3D stable matchings is bound to fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1873</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1873</id><created>2014-07-04</created><authors><author><keyname>Bodini</keyname><forenames>Olivier</forenames></author><author><keyname>Genitrini</keyname><forenames>Antoine</forenames></author><author><keyname>Peschanski</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>A Quantitative Study of Pure Parallel Processes</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the interleaving -- or pure merge -- operator that
most often characterizes parallelism in concurrency theory. This operator is a
principal cause of the so-called combinatorial explosion that makes very hard -
at least from the point of view of computational complexity - the analysis of
process behaviours e.g. by model-checking. The originality of our approach is
to study this combinatorial explosion phenomenon on average, relying on
advanced analytic combinatorics techniques. We study various measures that
contribute to a better understanding of the process behaviours represented as
plane rooted trees: the number of runs (corresponding to the width of the
trees), the expected total size of the trees as well as their overall shape.
Two practical outcomes of our quantitative study are also presented: (1) a
linear-time algorithm to compute the probability of a concurrent run prefix,
and (2) an efficient algorithm for uniform random sampling of concurrent runs.
These provide interesting responses to the combinatorial explosion problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1875</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1875</id><created>2014-07-03</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed G.</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Hasna</keyname><forenames>Mazen O.</forenames></author></authors><title>A Novel Spectrally-Efficient Scheme for Physical Layer Network Coding</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel three-time-slot transmission scheme
combined with an efficient embedded linear channel equalization (ELCE)
technique for the Physical layer Network Coding (PNC). Our transmission scheme,
we achieve about 33% increase in the spectral efficiency over the conventional
two-time-slot scheme while maintaining the same end-toend BER performance.We
derive an exact expression for the endto- end BER of the proposed
three-time-slot transmission scheme combined with the proposed ELCE technique
for BPSK transmission. Numerical results demonstrate that the exact expression
for the end-to-end BER is consistent with the BER simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1885</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1885</id><created>2014-06-29</created><updated>2014-09-27</updated><authors><author><keyname>Schutz</keyname><forenames>Antony</forenames></author><author><keyname>Ferrari</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Mary</keyname><forenames>David</forenames></author><author><keyname>Soulez</keyname><forenames>F&#xe9;rr&#xe9;ol</forenames></author><author><keyname>Thi&#xe9;baut</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Vannier</keyname><forenames>Martin</forenames></author></authors><title>PAINTER: a spatio-spectral image reconstruction algorithm for optical
  interferometry</title><categories>astro-ph.IM cs.CV</categories><comments>12 pages, 10 figures,
  http://www.opticsinfobase.org/submit/review/copyright_permissions.cfm</comments><doi>10.1364/JOSAA.31.002334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astronomical optical interferometers sample the Fourier transform of the
intensity distribution of a source at the observation wavelength. Because of
rapid perturbations caused by atmospheric turbulence, the phases of the complex
Fourier samples (visibilities) cannot be directly exploited. Consequently,
specific image reconstruction methods have been devised in the last few
decades. Modern polychromatic optical interferometric instruments are now
paving the way to multiwavelength imaging. This paper is devoted to the
derivation of a spatio-spectral (3D) image reconstruction algorithm, coined
PAINTER (Polychromatic opticAl INTErferometric Reconstruction software). The
algorithm relies on an iterative process, which alternates estimation of
polychromatic images and of complex visibilities. The complex visibilities are
not only estimated from squared moduli and closure phases, but also
differential phases, which helps to better constrain the polychromatic
reconstruction. Simulations on synthetic data illustrate the efficiency of the
algorithm and in particular the relevance of injecting a differential phases
model in the reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1889</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1889</id><created>2014-07-07</created><updated>2014-10-12</updated><authors><author><keyname>Chonev</keyname><forenames>Ventsislav</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>The Polyhedron-Hitting Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider polyhedral versions of Kannan and Lipton's Orbit Problem (STOC
'80 and JACM '86)---determining whether a target polyhedron V may be reached
from a starting point x under repeated applications of a linear transformation
A in an ambient vector space Q^m. In the context of program verification, very
similar reachability questions were also considered and left open by Lee and
Yannakakis in (STOC '92). We present what amounts to a complete
characterisation of the decidability landscape for the Polyhedron-Hitting
Problem, expressed as a function of the dimension m of the ambient space,
together with the dimension of the polyhedral target V: more precisely, for
each pair of dimensions, we either establish decidability, or show hardness for
longstanding number-theoretic open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1890</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1890</id><created>2014-07-07</created><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Mitchell</keyname><forenames>Logan</forenames></author><author><keyname>Giraud-Carrier</keyname><forenames>Christophe</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>Recommending Learning Algorithms and Their Associated Hyperparameters</title><categories>cs.LG stat.ML</categories><comments>Short paper--2 pages, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of machine learning on a given task dependson, among other
things, which learning algorithm is selected and its associated
hyperparameters. Selecting an appropriate learning algorithm and setting its
hyperparameters for a given data set can be a challenging task, especially for
users who are not experts in machine learning. Previous work has examined using
meta-features to predict which learning algorithm and hyperparameters should be
used. However, choosing a set of meta-features that are predictive of algorithm
performance is difficult. Here, we propose to apply collaborative filtering
techniques to learning algorithm and hyperparameter selection, and find that
doing so avoids determining which meta-features to use and outperforms
traditional meta-learning approaches in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1891</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1891</id><created>2014-07-07</created><updated>2014-10-13</updated><authors><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Pinto</keyname><forenames>Jo&#xe3;o Sousa</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>On Termination of Integer Linear Loops</title><categories>cs.CC</categories><comments>Accepted to SODA15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in program verification concerns the termination of
simple linear loops of the form x := u ; while Bx &gt;= b do {x := Ax + a} where x
is a vector of variables, u, a, and c are integer vectors, and A and B are
integer matrices. Assuming the matrix A is diagonalisable, we give a decision
procedure for the problem of whether, for all initial integer vectors u, such a
loop terminates. The correctness of our algorithm relies on sophisticated tools
from algebraic and analytic number theory, Diophantine geometry, and real
algebraic geometry. To the best of our knowledge, this is the first substantial
advance on a 10-year-old open problem of Tiwari (2004) and Braverman (2006).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1905</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1905</id><created>2014-07-07</created><authors><author><keyname>Chen</keyname><forenames>Bocong</forenames></author><author><keyname>Dinh</keyname><forenames>Hai Q.</forenames></author><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author></authors><title>Polyadic Constacyclic Codes</title><categories>cs.IT math.IT math.NT</categories><comments>We provide complete solutions on two basic questions on polyadic
  constacyclic cdes, and construct some optimal codes from the polyadic
  constacyclic cdes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any given positive integer $m$, a necessary and sufficient condition for
the existence of Type I $m$-adic constacyclic codes is given. Further, for any
given integer $s$, a necessary and sufficient condition for $s$ to be a
multiplier of a Type I polyadic constacyclic code is given. As an application,
some optimal codes from Type I polyadic constacyclic codes, including
generalized Reed-Solomon codes and alternant MDS codes, are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1910</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1910</id><created>2014-07-07</created><authors><author><keyname>Pettie</keyname><forenames>Seth</forenames></author></authors><title>Sensitivity Analysis of Minimum Spanning Trees in Sub-Inverse-Ackermann
  Time</title><categories>cs.DS</categories><comments>Extended abstract appeared in ISAAC 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic algorithm for computing the sensitivity of a
minimum spanning tree (MST) or shortest path tree in $O(m\log\alpha(m,n))$
time, where $\alpha$ is the inverse-Ackermann function. This improves upon a
long standing bound of $O(m\alpha(m,n))$ established by Tarjan. Our algorithms
are based on an efficient split-findmin data structure, which maintains a
collection of sequences of weighted elements that may be split into smaller
subsequences. As far as we are aware, our split-findmin algorithm is the first
with superlinear but sub-inverse-Ackermann complexity. We also give a reduction
from MST sensitivity to the MST problem itself. Together with the randomized
linear time MST algorithm of Karger, Klein, and Tarjan, this gives another
randomized linear time MST sensitivity algoritm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1922</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1922</id><created>2014-07-07</created><authors><author><keyname>Papadopoulos</keyname><forenames>Athanasios</forenames></author><author><keyname>Czap</keyname><forenames>Laszlo</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author></authors><title>Secret message capacity of a line network</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of information theoretically secure communication
in a line network with erasure channels and state feedback. We consider a
spectrum of cases for the private randomness that intermediate nodes can
generate, ranging from having intermediate nodes generate unlimited private
randomness, to having intermediate nodes generate no private randomness, and
all cases in between. We characterize the secret message capacity when either
only one of the channels is eavesdropped or all of the channels are
eavesdropped, and we develop polynomial time algorithms that achieve these
capacities. We also give an outer bound for the case where an arbitrary number
of channels is eavesdropped. Our work is the first to characterize the secrecy
capacity of a network of arbitrary size, with imperfect channels and feedback.
As a side result, we derive the secret key and secret message capacity of an
one-hop network, when the source has limited randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1923</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1923</id><created>2014-07-07</created><authors><author><keyname>Fox-Epstein</keyname><forenames>Eli</forenames></author><author><keyname>Uehara</keyname><forenames>Ryuhei</forenames></author></authors><title>The Convex Configurations of &quot;Sei Shonagon Chie no Ita&quot; and Other
  Dissection Puzzles</title><categories>cs.CG math.HO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tangram and Sei Shonagon Chie no Ita are popular dissection puzzles
consisting of seven pieces. Each puzzle can be formed by identifying edges from
sixteen identical right isosceles triangles. It is known that the tangram can
form 13 convex polygons. We show that Sei Shonagon Chie no Ita can form 16
convex polygons, propose a new puzzle that can form 19, no 7 piece puzzle can
form 20, and 11 pieces are necessary and sufficient to form all 20 polygons
formable by 16 identical isosceles right triangles. Finally, we examine the
number of convex polygons formable by different quantities of these triangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1925</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1925</id><created>2014-07-07</created><updated>2014-11-06</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author></authors><title>Using Optimization to Break the Epsilon Barrier: A Faster and Simpler
  Width-Independent Algorithm for Solving Positive Linear Programs in Parallel</title><categories>cs.DS cs.DC cs.NA math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of nearly-linear-time algorithms for approximately
solving positive linear programs. Both the parallel and the sequential
deterministic versions of these algorithms require
$\tilde{O}(\varepsilon^{-4})$ iterations, a dependence that has not been
improved since the introduction of these methods in 1993 by Luby and Nisan.
Moreover, previous algorithms and their analyses rely on update steps and
convergence arguments that are combinatorial in nature, and do not seem to
arise naturally from an optimization viewpoint. In this paper, we leverage
insights from optimization theory to construct a novel algorithm that breaks
the longstanding $\tilde{O}(\varepsilon^{-4})$ barrier. Our algorithm has a
simple analysis and a clear motivation. Our work introduces a number of novel
techniques, such as the combined application of gradient descent and mirror
descent, and a truncated, smoothed version of the standard multiplicative
weight update, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1930</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1930</id><created>2014-07-07</created><authors><author><keyname>Hayes</keyname><forenames>Thomas P.</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Lower Bounds on the Critical Density in the Hard Disk Model via
  Optimized Metrics</title><categories>cs.CC cond-mat.stat-mech</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a new lower bound on the critical density $\rho_c$ of the hard disk
model, i.e., the density below which it is possible to efficiently sample
random configurations of $n$ non-overlapping disks in a unit torus. We use a
classic Markov chain which moves one disk at a time, but with an improved path
coupling analysis. Our main tool is an optimized metric on neighboring pairs of
configurations, i.e., configurations that differ in the position of a single
disk: we define a metric that depends on the difference in these positions, and
which approaches zero continuously as they coincide. This improves the previous
lower bound $\rho_c \ge 1/8$ to $\rho_c \ge 0.154$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1931</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1931</id><created>2014-07-07</created><authors><author><keyname>Venkatakrishnan</keyname><forenames>Shaileshh Bojja</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Deterministic Near-Optimal P2P Streaming</title><categories>cs.NI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider streaming over a peer-to-peer network with homogeneous nodes in
which a single source broadcasts a data stream to all the users in the system.
Peers are allowed to enter or leave the system (adversarially) arbitrarily.
Previous approaches for streaming in this setting have either used randomized
distribution graphs or structured trees with randomized maintenance algorithms.
Randomized graphs handle peer churn well but have poor connectivity guarantees,
while structured trees have good connectivity but have proven hard to maintain
under peer churn. We improve upon both approaches by presenting a novel
distribution structure with a deterministic and distributed algorithm for
maintenance under peer churn; our result is inspired by a recent work proposing
deterministic algorithms for rumor spreading in graphs. A key innovation in our
approach is in having redundant links in the distribution structure. While this
leads to a reduction in the maximum streaming rate possible, we show that for
the amount of redundancy used, the delay guarantee of the proposed algorithm is
near optimal. We introduce a tolerance parameter that captures the worst-case
transient streaming rate received by the peers during churn events and
characterize the fundamental tradeoff between rate, delay and tolerance. A
natural generalization of the deterministic algorithm achieves this tradeoff
near optimally. Finally, the proposed deterministic algorithm is robust enough
to handle various generalizations: ability to deal with heterogeneous node
capacities of the peers and more complicated streaming patterns where multiple
source transmissions are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1933</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1933</id><created>2014-07-07</created><authors><author><keyname>Saulwick</keyname><forenames>Adam</forenames></author></authors><title>Lexpresso: a Controlled Natural Language</title><categories>cs.CL cs.AI</categories><comments>12 pages, 2 figures, 4th Workshop on Controlled Natural Language 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an overview of `Lexpresso', a Controlled Natural Language
developed at the Defence Science &amp; Technology Organisation as a bidirectional
natural language interface to a high-level information fusion system. The paper
describes Lexpresso's main features including lexical coverage, expressiveness
and range of linguistic syntactic and semantic structures. It also touches on
its tight integration with a formal semantic formalism and tentatively
classifies it against the PENS system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1935</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1935</id><created>2014-07-07</created><updated>2015-11-05</updated><authors><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>K. B.</forenames></author></authors><title>Fundamental Limits of Caching: Improved Bounds For Small Buffer Users</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the peak rate of the caching problem is investigated, under the
scenario that the users are with small buffer sizes and the number of users is
no less than the amount of files in the server. A novel coded caching strategy
is proposed for such a scenario, leading to a lower peak rate compared to
recent results in the literature. Furthermore, it is verified that our peak
rates coincides with the cut-set bound analytically in an information-theoretic
view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1944</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1944</id><created>2014-07-07</created><updated>2014-10-21</updated><authors><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Zhu</keyname><forenames>Junan</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author></authors><title>Compressed Sensing via Universal Denoising and Approximate Message
  Passing</title><categories>cs.IT math.IT</categories><comments>Appeared at 52'd annual Allerton Conference on Communication,
  Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study compressed sensing (CS) signal reconstruction problems where an
input signal is measured via matrix multiplication under additive white
Gaussian noise. Our signals are assumed to be stationary and ergodic, but the
input statistics are unknown; the goal is to provide reconstruction algorithms
that are universal to the input statistics. We present a novel algorithm that
combines: (i) the approximate message passing (AMP) CS reconstruction
framework, which converts the matrix channel recovery problem into scalar
channel denoising; (ii) a universal denoising scheme based on context
quantization, which partitions the stationary ergodic signal denoising into
independent and identically distributed (i.i.d.) subsequence denoising; and
(iii) a density estimation approach that approximates the probability
distribution of an i.i.d. sequence by fitting a Gaussian mixture (GM) model. In
addition to the algorithmic framework, we provide three contributions: (i)
numerical results showing that state evolution holds for non-separable Bayesian
sliding-window denoisers; (ii) a universal denoiser that does not require the
input signal to be bounded; and (iii) we modify the GM learning algorithm, and
extend it to an i.i.d. denoiser. Our universal CS recovery algorithm compares
favorably with existing reconstruction algorithms in terms of both
reconstruction quality and runtime, despite not knowing the input statistics of
the stationary ergodic signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1952</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1952</id><created>2014-07-08</created><authors><author><keyname>Lin</keyname><forenames>Shang-Wei</forenames><affiliation>Universit&#xe9; Paris 13, Sorbonne Paris Cit&#xe9;, LIPN, CNRS, UMR 7030, F-93430, Villetaneuse, France</affiliation></author><author><keyname>Petrucci</keyname><forenames>Laure</forenames><affiliation>Universit&#xe9; Paris 13, Sorbonne Paris Cit&#xe9;, LIPN, CNRS, UMR 7030, F-93430, Villetaneuse, France</affiliation></author></authors><title>Proceedings 2nd French Singaporean Workshop on Formal Methods and
  Applications</title><categories>cs.LO cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.1; D.2.2; D.2.4; D.2.10; D.2.11; F.1.1; F.1.2; F.3.1; F.3.2</acm-class><journal-ref>EPTCS 156, 2014</journal-ref><doi>10.4204/EPTCS.156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 2nd French Singaporean Workshop
on Formal Methods and Applications (FSFMA'14). The workshop was held in
Singapore on May 13th, 2014, as a satellite event of the 19th International
Symposium on Formal Methods (FM'14).
  FSFMA aims at sharing research interests and launching collaborations in the
area of formal methods and their applications. The scientific subject of the
workshop covers (but is not limited to) areas such as formal specification,
model checking, verification, program analysis/transformation, software
engineering, and applications in major areas of computer science, including
aeronautics and aerospace. The workshop brings together researchers and
industry R&amp;D experts from France, Singapore and other countries together to
exchange their knowledge, discuss their research findings, and explore
potential collaborations.
  This volume contains eight contributions: four invited talks and four regular
papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1957</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1957</id><created>2014-07-08</created><authors><author><keyname>Bristow</keyname><forenames>Hilton</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Regression-Based Image Alignment for General Object Categories</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient-descent methods have exhibited fast and reliable performance for
image alignment in the facial domain, but have largely been ignored by the
broader vision community. They require the image function be smooth and
(numerically) differentiable -- properties that hold for pixel-based
representations obeying natural image statistics, but not for more general
classes of non-linear feature transforms. We show that transforms such as Dense
SIFT can be incorporated into a Lucas Kanade alignment framework by predicting
descent directions via regression. This enables robust matching of instances
from general object categories whilst maintaining desirable properties of Lucas
Kanade such as the capacity to handle high-dimensional warp parametrizations
and a fast rate of convergence. We present alignment results on a number of
objects from ImageNet, and an extension of the method to unsupervised joint
alignment of objects from a corpus of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1963</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1963</id><created>2014-07-08</created><authors><author><keyname>Paraiso</keyname><forenames>Fawaz</forenames><affiliation>LIFL, INRIA Lille - Nord Europe</affiliation></author><author><keyname>Merle</keyname><forenames>Philippe</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Seinturier</keyname><forenames>Lionel</forenames><affiliation>LIFL, INRIA Lille - Nord Europe, IUF</affiliation></author></authors><title>soCloud: A service-oriented component-based PaaS for managing
  portability, provisioning, elasticity, and high availability across multiple
  clouds</title><categories>cs.SE cs.DC</categories><proxy>ccsd</proxy><doi>10.1007/s00607-014-0421-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-cloud computing is a promising paradigm to support very large scale
world wide distributed applications. Multi-cloud computing is the usage of
multiple, independent cloud environments, which assumed no priori agreement
between cloud providers or third party. However, multi-cloud computing has to
face several key challenges such as portability, provisioning, elasticity, and
high availability. Developers will not only have to deploy applications to a
specific cloud, but will also have to consider application portability from one
cloud to another, and to deploy distributed applications spanning multiple
clouds. This article presents soCloud a service-oriented component-based
Platform as a Service (PaaS) for managing portability, elasticity,
provisioning, and high availability across multiple clouds. soCloud is based on
the OASIS Service Component Architecture (SCA) standard in order to address
portability. soCloud provides services for managing provisioning, elasticity,
and high availability across multiple clouds. soCloud has been deployed and
evaluated on top of ten existing cloud providers: Windows Azure, DELL KACE,
Amazon EC2, CloudBees, OpenShift, dotCloud, Jelastic, Heroku, Appfog, and an
Eucalyptus private cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1966</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1966</id><created>2014-07-08</created><updated>2015-05-21</updated><authors><author><keyname>Marchan</keyname><forenames>Luz Elimar</forenames><affiliation>LAGA</affiliation></author><author><keyname>Ordaz</keyname><forenames>Oscar</forenames><affiliation>LAGA</affiliation></author><author><keyname>Santos</keyname><forenames>Irene</forenames><affiliation>LAGA</affiliation></author><author><keyname>Schmid</keyname><forenames>Wolfgang</forenames><affiliation>LAGA</affiliation></author></authors><title>Multi-wise and constrained fully weighted Davenport constants and
  interactions with coding theory</title><categories>math.NT cs.IT math.CO math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two families of weighted zero-sum constants for finite abelian
groups. For a finite abelian group $( G , + )$, a set of weights $W \subset
\mathbb{Z}$, and an integral parameter $m$, the $m$-wise Davenport constant
with weights $W$ is the smallest integer $n$ such that each sequence over $G$
of length $n$ has at least $m$ disjoint zero-subsums with weights $W$. And, for
an integral parameter $d$, the $d$-constrained Davenport constant with weights
$W$ is the smallest $n$ such that each sequence over $G$ of length $n$ has a
zero-subsum with weights $W$ of size at most $d$. First, we establish a link
between these two types of constants and several basic and general results on
them. Then, for elementary $p$-groups, establishing a link between our
constants and the parameters of linear codes as well as the cardinality of cap
sets in certain projective spaces, we obtain various explicit results on the
values of these constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1972</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1972</id><created>2014-07-08</created><authors><author><keyname>Rajaram</keyname><forenames>S.</forenames></author><author><keyname>Karuppiah</keyname><forenames>A. Babu</forenames></author><author><keyname>Kumar</keyname><forenames>K. Vinoth</forenames></author></authors><title>Secure Routing Path Using Trust Values for Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>10 pages, 4 figures, International Journal on Cryptography and
  Information Security (IJCIS)</comments><journal-ref>http://airccse.org/journal/ijcis/current2014.html</journal-ref><doi>10.5121/ijcis.2014.4203</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Traditional cryptography-based security mechanisms such as authentication and
authorization are not effective against insider attacks like wormhole,
sinkhole, selective forwarding attacks, etc., Trust based approaches have been
widely used to counter insider attacks in wireless sensor networks. It provides
a quantitative way to evaluate the trustworthiness of sensor nodes. An
untrustworthy node can wreak considerable damage and adversely affect the
quality and reliability of data. Therefore, analysing the trust level of a node
is important. In this paper we focused about indirect trust mechanism, in which
each node monitors the forwarding behavior of its neighbors in order to detect
any node that behaves selfishly and does not forward the packets it receives.
For this, we used a link state routing protocol based indirect trusts which
forms the shortest route and finds the best trustworthy route among them by
comparing the values of all the calculated route trusts as for each route
present in the network. And finally, we compare our work with similar routing
protocols and show its advantages over them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1974</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1974</id><created>2014-07-08</created><updated>2015-05-18</updated><authors><author><keyname>Zhang</keyname><forenames>Jianjia</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhou</keyname><forenames>Luping</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author></authors><title>Learning Discriminative Stein Kernel for SPD Matrices and Its
  Applications</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stein kernel has recently shown promising performance on classifying images
represented by symmetric positive definite (SPD) matrices. It evaluates the
similarity between two SPD matrices through their eigenvalues. In this paper,
we argue that directly using the original eigenvalues may be problematic
because: i) Eigenvalue estimation becomes biased when the number of samples is
inadequate, which may lead to unreliable kernel evaluation; ii) More
importantly, eigenvalues only reflect the property of an individual SPD matrix.
They are not necessarily optimal for computing Stein kernel when the goal is to
discriminate different sets of SPD matrices. To address the two issues in one
shot, we propose a discriminative Stein kernel, in which an extra parameter
vector is defined to adjust the eigenvalues of the input SPD matrices. The
optimal parameter values are sought by optimizing a proxy of classification
performance. To show the generality of the proposed method, three different
kernel learning criteria that are commonly used in the literature are employed
respectively as a proxy. A comprehensive experimental study is conducted on a
variety of image classification tasks to compare our proposed discriminative
Stein kernel with the original Stein kernel and other commonly used methods for
evaluating the similarity between SPD matrices. The experimental results
demonstrate that, the discriminative Stein kernel can attain greater
discrimination and better align with classification tasks by altering the
eigenvalues. This makes it produce higher classification performance than the
original Stein kernel and other commonly used methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1976</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1976</id><created>2014-07-08</created><authors><author><keyname>Phani</keyname><forenames>Shanta</forenames></author><author><keyname>Lahiri</keyname><forenames>Shibamouli</forenames></author><author><keyname>Biswas</keyname><forenames>Arindam</forenames></author></authors><title>Inter-Rater Agreement Study on Readability Assessment in Bengali</title><categories>cs.CL</categories><comments>6 pages, 4 tables, Accepted in ICCONAC, 2014</comments><journal-ref>International Journal on Natural Language Computing (IJNLC), 3(3),
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An inter-rater agreement study is performed for readability assessment in
Bengali. A 1-7 rating scale was used to indicate different levels of
readability. We obtained moderate to fair agreement among seven independent
annotators on 30 text passages written by four eminent Bengali authors. As a by
product of our study, we obtained a readability-annotated ground truth dataset
in Bengali. .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1982</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1982</id><created>2014-07-08</created><updated>2014-08-01</updated><authors><author><keyname>Elsaadani</keyname><forenames>Mohamed</forenames></author></authors><title>Influence of ICTs on workforce productivity in Egyptian industrial
  organizations</title><categories>cs.CY</categories><comments>This paper has been withdrawn by the author. major analysis errors</comments><journal-ref>International Journal of Advanced Information Technology (IJAIT)
  Vol. 4, No. 3, June 2014</journal-ref><doi>10.5121/ijait.2014.4301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Present study aims to investigate the influence of ICTs dimensions:
Information Technology, Management Information System, Office automation,
Intranet and Internet on workforce productivity for a group of industrial
organizations in Alexandria, Egypt. The study findings revealed that the
specified dimensions of ICTs positively affect workforce productivity of
industrial organizations in Alexandria, Egypt
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1993</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1993</id><created>2014-07-08</created><authors><author><keyname>Teytaud</keyname><forenames>Fabien</forenames></author><author><keyname>Fonlupt</keyname><forenames>Cyril</forenames></author></authors><title>A Critical Reassessment of Evolutionary Algorithms on the cryptanalysis
  of the simplified data encryption standard algorithm</title><categories>cs.CR cs.NE</categories><comments>12 pages; International Journal on Cryptography and Information
  Security 4 (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the cryptanalysis of the simplified data encryption
standard algorithm using meta-heuristics and in particular genetic algorithms.
The classic fitness function when using such an algorithm is to compare n-gram
statistics of a the decrypted message with those of the target message. We show
that using such a function is irrelevant in case of Genetic Algorithm, simply
because there is no correlation between the distance to the real key (the
optimum) and the value of the fitness, in other words, there is no hidden
gradient. In order to emphasize this assumption we experimentally show that a
genetic algorithm perform worse than a random search on the cryptanalysis of
the simplified data encryption standard algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.1996</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.1996</id><created>2014-07-08</created><updated>2014-07-18</updated><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author></authors><title>Reachability in succinct one-counter games</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the reachability problem on transition systems corresponding to
succinct one-counter machines, that is, machines where the counter is
incremented or decremented by a value given in binary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2002</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2002</id><created>2014-07-08</created><updated>2016-02-29</updated><authors><author><keyname>Walk</keyname><forenames>Simon</forenames></author><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author><author><keyname>Tudorache</keyname><forenames>Tania</forenames></author><author><keyname>Musen</keyname><forenames>Mark A.</forenames></author><author><keyname>Noy</keyname><forenames>Natalya F.</forenames></author></authors><title>Discovering Beaten Paths in Collaborative Ontology-Engineering Projects
  using Markov Chains</title><categories>cs.SI cs.AI cs.DL physics.data-an</categories><comments>Published in the Journal of Biomedical Informatics</comments><doi>10.1016/j.jbi.2014.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biomedical taxonomies, thesauri and ontologies in the form of the
International Classification of Diseases (ICD) as a taxonomy or the National
Cancer Institute Thesaurus as an OWL-based ontology, play a critical role in
acquiring, representing and processing information about human health. With
increasing adoption and relevance, biomedical ontologies have also
significantly increased in size. For example, the 11th revision of the ICD,
which is currently under active development by the WHO contains nearly 50,000
classes representing a vast variety of different diseases and causes of death.
This evolution in terms of size was accompanied by an evolution in the way
ontologies are engineered. Because no single individual has the expertise to
develop such large-scale ontologies, ontology-engineering projects have evolved
from small-scale efforts involving just a few domain experts to large-scale
projects that require effective collaboration between dozens or even hundreds
of experts, practitioners and other stakeholders. Understanding how these
stakeholders collaborate will enable us to improve editing environments that
support such collaborations. We uncover how large ontology-engineering
projects, such as the ICD in its 11th revision, unfold by analyzing usage logs
of five different biomedical ontology-engineering projects of varying sizes and
scopes using Markov chains. We discover intriguing interaction patterns (e.g.,
which properties users subsequently change) that suggest that large
collaborative ontology-engineering projects are governed by a few general
principles that determine and drive development. From our analysis, we identify
commonalities and differences between different projects that have implications
for project managers, ontology editors, developers and contributors working on
collaborative ontology-engineering projects and tools in the biomedical domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2006</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2006</id><created>2014-07-08</created><authors><author><keyname>Mryglod</keyname><forenames>Olesya</forenames></author><author><keyname>Fuchs</keyname><forenames>Benedikt</forenames></author><author><keyname>Szell</keyname><forenames>Michael</forenames></author><author><keyname>Holovatch</keyname><forenames>Yurij</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Interevent time distributions of human multi-level activity in a virtual
  world</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages</comments><journal-ref>Physica A 419, 681-690 (2014)</journal-ref><doi>10.1016/j.physa.2014.09.056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying human behaviour in virtual environments provides extraordinary
opportunities for a quantitative analysis of social phenomena with levels of
accuracy that approach those of the natural sciences. In this paper we use
records of player activities in the massive multiplayer online game Pardus over
1,238 consecutive days, and analyze dynamical features of sequences of actions
of players. We build on previous work were temporal structures of human actions
of the same type were quantified, and extend provide an empirical understanding
of human actions of different types. This study of multi-level human activity
can be seen as a dynamic counterpart of static multiplex network analysis. We
show that the interevent time distributions of actions in the Pardus universe
follow highly non-trivial distribution functions, from which we extract
action-type specific characteristic &quot;decay constants&quot;. We discuss
characteristic features of interevent time distributions, including periodic
patterns on different time scales, bursty dynamics, and various functional
forms on different time scales. We comment on gender differences of players in
emotional actions, and find that while male and female act similarly when
performing some positive actions, females are slightly faster for negative
actions. We also observe effects on the age of players: more experienced
players are generally faster in making decisions about engaging and terminating
in enmity and friendship, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2019</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2019</id><created>2014-07-08</created><authors><author><keyname>Baruah</keyname><forenames>Kalyanee Kanchan</forenames></author><author><keyname>Das</keyname><forenames>Pranjal</forenames></author><author><keyname>Hannan</keyname><forenames>Abdul</forenames></author><author><keyname>Sarma</keyname><forenames>Shikhar Kr.</forenames></author></authors><title>Assamese-English Bilingual Machine Translation</title><categories>cs.CL</categories><comments>In the proceedings of International Conference of Natural Language
  Processing and Cognitive Computing (ICONACC)-2014, pp. 227-231</comments><journal-ref>International Journal on Natural Language Computing (IJNLC) Vol.
  3, No.3, June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine translation is the process of translating text from one language to
another. In this paper, Statistical Machine Translation is done on Assamese and
English language by taking their respective parallel corpus. A statistical
phrase based translation toolkit Moses is used here. To develop the language
model and to align the words we used two another tools IRSTLM, GIZA
respectively. BLEU score is used to check our translation system performance,
how good it is. A difference in BLEU scores is obtained while translating
sentences from Assamese to English and vice-versa. Since Indian languages are
morphologically very rich hence translation is relatively harder from English
to Assamese resulting in a low BLEU score. A statistical transliteration system
is also introduced with our translation system to deal basically with proper
nouns, OOV (out of vocabulary) words which are not present in our corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2025</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2025</id><created>2014-07-08</created><authors><author><keyname>Alsahlany</keyname><forenames>Ali M.</forenames></author></authors><title>Performance Analysis of VOIP Traffic Over Integrating Wireless LAN and
  WAN Using Different Codecs</title><categories>cs.NI</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN)Vol. 6,
  No. 3, June 2014</journal-ref><doi>10.5121/ijwmn.2014.6306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simulation model is presented to analyze and evaluate the performance of
VoIP based integrated wireless LAN/WAN with taking into account various voice
encoding schemes. The network model was simulated using OPNET Modeler software.
Different parameters that indicate the QoS like MOS, jitter, end to end delay,
traffic send and traffic received are calculated and analyzed in Wireless
LAN/WAN scenarios. Depending on this evaluation, Selection codecs G.729A
consider the best choice for VoIP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2027</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2027</id><created>2014-07-08</created><authors><author><keyname>Mataracioglu</keyname><forenames>Tolga</forenames></author><author><keyname>Yildirim</keyname><forenames>Sevgi Ozkan</forenames></author></authors><title>Obstructions of Turkish Public Organizations Getting ISO/IEC 27001
  Certified</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper; a comparison has been made among the Articles contained in the
ISO/IEC 27001 Standard and the Articles of the Civil Servants Law No 657, which
should essentially be complied with by the personnel employed within the bodies
of public institutions in Turkey; and efforts have been made in order to
emphasize the consistent Articles; and in addition, the matters, which should
be paid attention by the public institutions indenting to obtain the ISO/IEC
27001 certificate for the Articles of the Civil Servants Law No 657 which are
not consistent with the ISO/IEC 27001 certification process, have been
mentioned. Furthermore, solution offers have been presented in order to ensure
that the mentioned Articles become consistent with the ISO/IEC 27001
certification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2029</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2029</id><created>2014-07-08</created><authors><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Caviglione</keyname><forenames>Luca</forenames></author><author><keyname>Meier</keyname><forenames>Michael</forenames></author></authors><title>Hidden and Uncontrolled - On the Emergence of Network Steganographic
  Threats</title><categories>cs.CR</categories><comments>11 pages</comments><journal-ref>ISSE 2014 Securing Electronic Business Processes: Highlights of
  the Information Security Solutions Europe 2014 Conference, N. Pohlmann, H.
  Reimer, W. Schneider (Editors), pp. 1-11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network steganography is the art of hiding secret information within innocent
network transmissions. Recent findings indicate that novel malware is
increasingly using network steganography. Similarly, other malicious activities
can profit from network steganography, such as data leakage or the exchange of
pedophile data. This paper provides an introduction to network steganography
and highlights its potential application for harmful purposes. We discuss the
issues related to countering network steganography in practice and provide an
outlook on further research directions and problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2032</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2032</id><created>2014-07-08</created><authors><author><keyname>Yu</keyname><forenames>Long</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>A class of $p$-ary cyclic codes and their weight enumerators</title><categories>cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $m$, $k$ be positive integers such that $\frac{m}{\gcd(m,k)}\geq 3$, $p$
be an odd prime and $\pi $ be a primitive element of $\mathbb{F}_{p^m}$. Let
$h_1(x)$ and $h_2(x)$ be the minimal polynomials of $-\pi^{-1}$ and
$\pi^{-\frac{p^k+1}{2}}$ over $\mathbb{F}_p$, respectively. In the case of odd
$\frac{m}{\gcd(m,k)}$, when $k$ is even, $\gcd(m,k)$ is odd or when
$\frac{k}{\gcd(m,k)}$ is odd, Zhou et~al. in \cite{zhou} obtained the weight
distribution of a class of cyclic codes $\mathcal{C}$ over $\mathbb{F}_p$ with
parity-check polynomial $h_1(x)h_2(x)$. In this paper, we further investigate
this class of cyclic codes $\mathcal{C}$ over $\mathbb{F}_p$ in the rest case
of odd $\frac{m}{\gcd(m,k)}$ and the case of even $\frac{m}{\gcd(m,k)}$.
Moreover, we determine the weight distribution of cyclic codes $\mathcal{C}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2033</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2033</id><created>2014-07-08</created><updated>2015-02-22</updated><authors><author><keyname>Shachnai</keyname><forenames>Hadas</forenames></author><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>A Multivariate Framework for Weighted FPT Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel multivariate approach for solving weighted parameterized
problems. In our model, given an instance of size $n$ of a minimization
(maximization) problem, and a parameter $W \geq 1$, we seek a solution of
weight at most (or at least) $W$. We use our general framework to obtain
efficient algorithms for such fundamental graph problems as Vertex Cover,
3-Hitting Set, Edge Dominating Set and Max Internal Out-Branching. The best
known algorithms for these problems admit running times of the form $c^W
n^{O(1)}$, for some constant $c&gt;1$. We improve these running times to $c^s
n^{O(1)}$, where $s\leq W$ is the minimum size of a solution of weight at most
(at least) $W$. If no such solution exists, $s=\min\{W,m\}$, where $m$ is the
maximum size of a solution. Clearly, $s$ can be substantially smaller than $W$.
In particular, the running times of our algorithms are (almost) the same as the
best known $O^*$ running times for the unweighted variants. Thus, we solve the
weighted versions of
  * Vertex Cover in $1.381^s n^{O(1)}$ time and $n^{O(1)}$ space.
  * 3-Hitting Set in $2.168^s n^{O(1)}$ time and $n^{O(1)}$ space.
  * Edge Dominating Set in $2.315^s n^{O(1)}$ time and $n^{O(1)}$ space.
  * Max Internal Out-Branching in $6.855^s n^{O(1)}$ time and space.
  We further show that Weighted Vertex Cover and Weighted Edge Dominating Set
admit fast algorithms whose running times are of the form $c^t n^{O(1)}$, where
$t \leq s$ is the minimum size of a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2034</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2034</id><created>2014-07-08</created><updated>2014-10-12</updated><authors><author><keyname>Conforti</keyname><forenames>Michele</forenames></author><author><keyname>Del Pia</keyname><forenames>Alberto</forenames></author><author><keyname>Di Summa</keyname><forenames>Marco</forenames></author><author><keyname>Faenza</keyname><forenames>Yuri</forenames></author></authors><title>Reverse Split Rank</title><categories>math.OC cs.DM</categories><comments>31 pages, 9 figures</comments><msc-class>90C10</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reverse split rank of an integral polyhedron P is defined as the supremum
of the split ranks of all rational polyhedra whose integer hull is P. Already
in R^3 there exist polyhedra with infinite reverse split rank. We give a
geometric characterization of the integral polyhedra in R^n with infinite
reverse split rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2036</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2036</id><created>2014-07-08</created><authors><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Limouzy</keyname><forenames>Vincent</forenames></author><author><keyname>Mary</keyname><forenames>Arnaud</forenames></author><author><keyname>Nourine</keyname><forenames>Lhouari</forenames></author><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author></authors><title>A Polynomial Delay Algorithm for Enumerating Minimal Dominating Sets in
  Chordal Graphs</title><categories>cs.DS cs.DM</categories><comments>13 pages, 1 figure, submitted</comments><msc-class>68P05, 68R10, 05C69, 05C85, 05C30</msc-class><acm-class>E.1; F.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An output-polynomial algorithm for the listing of minimal dominating sets in
graphs is a challenging open problem and is known to be equivalent to the
well-known Transversal problem which asks for an output-polynomial algorithm
for listing the set of minimal hitting sets in hypergraphs. We give a
polynomial delay algorithm to list the set of minimal dominating sets in
chordal graphs, an important and well-studied graph class where such an
algorithm was open for a while.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2037</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2037</id><created>2014-07-08</created><updated>2014-07-28</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Bauer</keyname><forenames>Johann</forenames></author></authors><title>Which of the world's institutions employ the most highly cited
  researchers? An analysis of the data from highlycited.com</title><categories>cs.DL physics.soc-ph stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A few weeks ago, Thomson Reuters published a list of the highly cited
researchers worldwide (highlycited.com). Since the data is freely available for
downloading and includes the names of the researchers' institutions, we
produced a ranking of the institutions on the basis of the number of highly
cited researchers per institution. This ranking is intended to be a helpful
amendment of other available institutional rankings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2041</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2041</id><created>2014-07-08</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>AlSalem</keyname><forenames>Adel I.</forenames></author></authors><title>ImpNet: Programming Software-Defied Networks Using Imperative Techniques</title><categories>cs.PL cs.NI</categories><comments>12 pages, 12 figures, extended and revised version of reference [22]
  in the paper. arXiv admin note: substantial text overlap with arXiv:1403.8028</comments><journal-ref>WSEAS Transactions on Computers, ISSN / E-ISSN: 1109-2750 /
  2224-2872, Volume 13, 2014, Art. #35, pp. 402-413</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software and hardware components are basic parts of modern networks. However
the software compo- nent is typical sealed and function-oriented. Therefore it
is very difficult to modify these components. This badly affected networking
innovations. Moreover, this resulted in network policies having complex
interfaces that are not user-friendly and hence resulted in huge and
complicated flow tables on physical switches of networks. This greatly degrades
the network performance in many cases. Software-Defined Networks (SDNs) is a
modern architecture of networks to overcome issues mentioned above. The idea of
SDN is to add to the network a controller device that manages all the other
devices on the network including physical switches of the network. One of the
main tasks of the managing process is switch learning; achieved via programming
physical switches of the network by adding or removing rules for
packet-processing to/from switches, more specifically to/from their flow
tables. A high-level imperative network programming language, called ImpNet, is
presented in this paper. ImpNet enables writing efficient, yet simple, and
powerful programs to run on the controller to control all other network devices
including switches. ImpNet is compositional, simply-structured, expressive, and
more importantly imperative. The syntax of ImpNet together two types of
operational semantics to contracts of ImpNet are presented in the paper. The
proposed semantics are of the static and dynamic types. Two modern application
programmed using ImpNet are shown in the paper as well. The semantics of the
applications are shown in the paper also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2044</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2044</id><created>2014-07-08</created><updated>2014-08-07</updated><authors><author><keyname>Dridi</keyname><forenames>Mohamed H.</forenames></author></authors><title>Tracking Individual Targets in High Density Crowd Scenes Analysis of a
  Video Recording in Hajj 2009</title><categories>cs.CV physics.soc-ph</categories><comments>20 pages, 17 figures, correction of some references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a number of methods (manual, semi-automatic and
automatic) for tracking individual targets in high density crowd scenes where
thousand of people are gathered. The necessary data about the motion of
individuals and a lot of other physical information can be extracted from
consecutive image sequences in different ways, including optical flow and block
motion estimation. One of the famous methods for tracking moving objects is the
block matching method. This way to estimate subject motion requires the
specification of a comparison window which determines the scale of the
estimate. In this work we present a real-time method for pedestrian recognition
and tracking in sequences of high resolution images obtained by a stationary
(high definition) camera located in different places on the Haram mosque in
Mecca. The objective is to estimate pedestrian velocities as a function of the
local density.The resulting data of tracking moving pedestrians based on video
sequences are presented in the following section. Through the evaluated system
the spatio-temporal coordinates of each pedestrian during the Tawaf ritual are
established. The pilgrim velocities as function of the local densities in the
Mataf area (Haram Mosque Mecca) are illustrated and very precisely documented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2053</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2053</id><created>2014-07-08</created><authors><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Limouzy</keyname><forenames>Vincent</forenames></author><author><keyname>Mary</keyname><forenames>Arnaud</forenames></author><author><keyname>Nourine</keyname><forenames>Lhouari</forenames></author></authors><title>On the Enumeration of Minimal Dominating Sets and Related Notions</title><categories>cs.DM cs.DS math.CO</categories><comments>15 pages, 3 figures, In revision</comments><msc-class>68R05, 68R10, 05C30, 05C69, 05C85</msc-class><acm-class>F.0; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dominating set $D$ in a graph is a subset of its vertex set such that each
vertex is either in $D$ or has a neighbour in $D$. In this paper, we are
interested in the enumeration of (inclusion-wise) minimal dominating sets in
graphs, called the Dom-Enum problem. It is well known that this problem can be
polynomially reduced to the Trans-Enum problem in hypergraphs, i.e., the
problem of enumerating all minimal transversals in a hypergraph. Firstly we
show that the Trans-Enum problem can be polynomially reduced to the Dom-Enum
problem. As a consequence there exists an output-polynomial time algorithm for
the Trans-Enum problem if and only if there exists one for the Dom-Enum
problem. Secondly, we study the Dom-Enum problem in some graph classes. We give
an output-polynomial time algorithm for the Dom-Enum problem in split graphs,
and introduce the completion of a graph to obtain an output-polynomial time
algorithm for the Dom-Enum problem in $P_6$-free chordal graphs, a proper
superclass of split graphs. Finally, we investigate the complexity of the
enumeration of (inclusion-wise) minimal connected dominating sets and minimal
total dominating sets of graphs. We show that there exists an output-polynomial
time algorithm for the Dom-Enum problem (or equivalently Trans-Enum problem) if
and only if there exists one for the following enumeration problems: minimal
total dominating sets, minimal total dominating sets in split graphs, minimal
connected dominating sets in split graphs, minimal dominating sets in
co-bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2063</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2063</id><created>2014-07-08</created><updated>2015-06-02</updated><authors><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Raghvendra</keyname><forenames>Sharath</forenames></author></authors><title>Approximation and Streaming Algorithms for Projective Clustering via
  Random Projections</title><categories>cs.CG</categories><comments>Canadian Conference on Computational Geometry (CCCG 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in $\mathbb{R}^d$. In the projective
clustering problem, given $k, q$ and norm $\rho \in [1,\infty]$, we have to
compute a set $\mathcal{F}$ of $k$ $q$-dimensional flats such that $(\sum_{p\in
P}d(p, \mathcal{F})^\rho)^{1/\rho}$ is minimized; here $d(p, \mathcal{F})$
represents the (Euclidean) distance of $p$ to the closest flat in
$\mathcal{F}$. We let $f_k^q(P,\rho)$ denote the minimal value and interpret
$f_k^q(P,\infty)$ to be $\max_{r\in P}d(r, \mathcal{F})$. When $\rho=1,2$ and
$\infty$ and $q=0$, the problem corresponds to the $k$-median, $k$-mean and the
$k$-center clustering problems respectively.
  For every $0 &lt; \epsilon &lt; 1$, $S\subset P$ and $\rho \ge 1$, we show that the
orthogonal projection of $P$ onto a randomly chosen flat of dimension
$O(((q+1)^2\log(1/\epsilon)/\epsilon^3) \log n)$ will $\epsilon$-approximate
$f_1^q(S,\rho)$. This result combines the concepts of geometric coresets and
subspace embeddings based on the Johnson-Lindenstrauss Lemma. As a consequence,
an orthogonal projection of $P$ to an $O(((q+1)^2 \log
((q+1)/\epsilon)/\epsilon^3) \log n)$ dimensional randomly chosen subspace
$\epsilon$-approximates projective clusterings for every $k$ and $\rho$
simultaneously. Note that the dimension of this subspace is independent of the
number of clusters~$k$.
  Using this dimension reduction result, we obtain new approximation and
streaming algorithms for projective clustering problems. For example, given a
stream of $n$ points, we show how to compute an $\epsilon$-approximate
projective clustering for every $k$ and $\rho$ simultaneously using only
$O((n+d)((q+1)^2\log ((q+1)/\epsilon))/\epsilon^3 \log n)$ space. Compared to
standard streaming algorithms with $\Omega(kd)$ space requirement, our approach
is a significant improvement when the number of input points and their
dimensions are of the same order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2068</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2068</id><created>2014-07-08</created><authors><author><keyname>Novara</keyname><forenames>Carlo</forenames></author><author><keyname>Formentin</keyname><forenames>Simone</forenames></author></authors><title>Data-driven controller design for nonlinear systems: a two degrees of
  freedom architecture</title><categories>cs.SY math.OC</categories><comments>5 pages, 3 figures, not published anywhere else yet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the D2-IBC (Data-Driven Inversion Based Control) approach for
nonlinear control is introduced and analyzed. The method does not require any
a-priori knowledge of the system dynamics and relies on a two degrees of
freedom scheme, with a nonlinear controller and a linear controller running in
parallel. In particular, the former is devoted to stabilize the system around a
trajectory of interest, whereas the latter is used to boost the closed-loop
performance. The paper also presents a thorough stability and performance
analysis of the closed-loop system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2073</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2073</id><created>2014-07-08</created><authors><author><keyname>Edes</keyname><forenames>Mine</forenames></author><author><keyname>&#xd6;zturan</keyname><forenames>Can</forenames></author><author><keyname>Halilo&#x11f;lu</keyname><forenames>T&#xfc;rkan</forenames></author><author><keyname>Luna</keyname><forenames>Augustin</forenames></author><author><keyname>Nussinov</keyname><forenames>Ruth</forenames></author></authors><title>MIMTool: A Tool for Drawing Molecular Interaction Maps</title><categories>cs.CE q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: To understand protein function, it is important to study protein-
protein interaction networks. These networks can be represented in network
diagrams called protein interaction maps that can lead to better understanding
by visualization. We address the problem of drawing of protein interactions in
Kohn's Molecular Interaction Map (MIM) notation. Even though there are some
existing tools for graphical visualization of protein interactions in general,
there is no tool that can draw protein interactions with MIM notation with full
support. Results: MIMTool was developed for drawing protein interaction maps in
Kohn's MIM notation. MIMTool was developed using the Qt toolkit libraries and
introduces several unique features such as full interactivity, object dragging,
ability to export files in MIMML, SBML and line drawing with automatic bending
and crossover minimization, which are not available in other diagram editors.
MIMTool also has a unique orthogonal edge drawing method that is both easy and
more flexible than other orthogonal drawing methods present in other
interaction drawing tools. Conclusions: MIMTool facilitates faster drawing,
updating and exchanging of MIMs. Among its several features, it also includes a
semi-automatic drawing algorithm that makes use of shortest path algorithm for
constructing lines with small number of bends and crossings. MIMTool
contributes a much needed software tool that was missing and will facilitate
wider adoption of Kohn's MIM notation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2074</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2074</id><created>2014-07-08</created><authors><author><keyname>Brix</keyname><forenames>Tobias</forenames></author><author><keyname>Pra&#xdf;ni</keyname><forenames>J&#xf6;rg-Stefan</forenames></author><author><keyname>Hinrichs</keyname><forenames>Klaus</forenames></author></authors><title>Visualization of Large Volumetric Multi-Channel Microscopy Data Streams
  on Standard PCs</title><categories>cs.GR</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Visualization of multi-channel microscopy data plays a vital role
in biological research. With the ever-increasing resolution of modern
microscopes the data set size of the scanned specimen grows steadily. On
commodity hardware this size easily exceeds the available main memory and the
even more limited GPU memory. Common volume rendering techniques require the
entire data set to be present in the GPU memory. Existing out-of-core rendering
approaches for large volume data sets either are limited to single-channel
volumes, or require a computer cluster, or have long preprocessing times.
Results: We introduce a ray-casting technique for rendering large volumetric
multi-channel microscopy data streams on commodity hardware. The volumetric
data is managed at different levels of detail by an octree structure. In
contrast to previous octree-based techniques, the octree is built incrementally
and therefore supports streamed microscopy data as well as data set sizes
exceeding the available main memory. Furthermore, our approach allows the user
to interact with the partially rendered data set at all stages of the octree
construction. After a detailed description of our method, we present
performance results for different multi-channel data sets with a size of up to
24 GB on a standard desktop PC. Conclusions: Our rendering technique allows
biologists to visualize their scanned specimen on their standard desktop
computers without high-end hardware requirements. Furthermore, the user can
interact with the data set during the initial loading to explore the already
loaded parts, change rendering parameters like color maps or adjust clipping
planes. Thus, the time of biologists being idle is reduced. Also, streamed data
can be visualized to detect and stop flawed scans early during the scan
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2077</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2077</id><created>2014-07-08</created><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>A Cyber-Physical System-based Approach for Industrial Automation Systems</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial automation systems (IASs) are commonly developed using the
languages defined by the IEC 61131 standard and are executed on PLCs. In this
paper, a system-based approach for the development of IASs is adopted. A
framework is described to refine the UML model of the software part, which is
extracted from the SysML system model, and get the implementation code. Two
implementation alternatives are considered to exploit PLCs but also the recent
deluge of embedded boards in the market. For PLC targets, the new version of
IEC 61131 that supports Object-Orientation is adopted, while Java is used for
embedded boards. The case study was developed as a lab exercise for teaching
the various technologies that address challenges in the domain of
cyber-physical systems where Internet of Things (IoT) would be the glue
regarding their cyber interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2082</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2082</id><created>2014-07-08</created><authors><author><keyname>Bhairannawar</keyname><forenames>Satish S</forenames></author><author><keyname>R</keyname><forenames>Rathan</forenames></author><author><keyname>B</keyname><forenames>Raja K</forenames></author><author><keyname>R</keyname><forenames>Venugopal K</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>FPGA Based Efficient Multiplier for Image Processing Applications Using
  Recursive Error Free Mitchell Log Multiplier and KOM Architecture</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Digital Image processing applications like medical imaging, satellite
imaging, Biometric trait images etc., rely on multipliers to improve the
quality of image. However, existing multiplication techniques introduce errors
in the output with consumption of more time, hence error free high speed
multipliers has to be designed. In this paper we propose FPGA based Recursive
Error Free Mitchell Log Multiplier (REFMLM) for image Filters. The 2x2 error
free Mitchell log multiplier is designed with zero error by introducing error
correction term is used in higher order Karastuba-Ofman Multiplier (KOM)
Architectures. The higher order KOM multipliers is decomposed into number of
lower order multipliers using radix 2 till basic multiplier block of order 2x2
which is designed by error free Mitchell log multiplier. The 8x8 REFMLM is
tested for Gaussian filter to remove noise in fingerprint image. The Multiplier
is synthesized using Spartan 3 FPGA family device XC3S1500-5fg320. It is
observed that the performance parameters such as area utilization, speed, error
and PSNR are better in the case of proposed architecture compared to existing
architectures
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2084</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2084</id><created>2014-07-08</created><authors><author><keyname>Zeckzer</keyname><forenames>Dirk</forenames></author><author><keyname>Gerighausen</keyname><forenames>Daniel</forenames></author><author><keyname>Steiner</keyname><forenames>Lydia</forenames></author><author><keyname>Prohaska</keyname><forenames>Sonja J.</forenames></author></authors><title>Analyzing Chromatin Using Tiled Binned Scatterplot Matrices</title><categories>cs.CE q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Over the last years, more and more biological data became
available. Besides the pure amount of new data, also its dimensionality - the
number of different attributes per data point - increased. Recently, especially
the amount of data on chromatin and its modifications increased considerably.
In the field of epigenetics, appropriate visualization tools designed for
highlighting the different aspects of epigenetic data are currently not
available. Results: We present a tool called TiBi-Scatter enabling correlation
analysis in 2D. This approach allows for analyzing multidimensional data while
keeping the use of resources such as memory small. Thus, it is in particular
applicable to large data sets. Conclusions: TiBi-Scatter is a resource-friendly
and easy to use tool that allows for the hypothesis-free analysis of large
multidimensional biological data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2089</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2089</id><created>2014-07-08</created><authors><author><keyname>Wait</keyname><forenames>Eric</forenames></author><author><keyname>Winter</keyname><forenames>Mark</forenames></author><author><keyname>Bjornsson</keyname><forenames>Chris</forenames></author><author><keyname>Kokovay</keyname><forenames>Erzsebet</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Goderie</keyname><forenames>Susan</forenames></author><author><keyname>Temple</keyname><forenames>Sally</forenames></author><author><keyname>Cohen</keyname><forenames>Andrew</forenames></author></authors><title>Visualization and Correction of Automated Segmentation, Tracking and
  Lineaging from 5-D Stem Cell Image Sequences</title><categories>cs.GR</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Results: We present an application that enables the quantitative analysis of
multichannel 5-D (x, y, z, t, channel) and large montage confocal fluorescence
microscopy images. The image sequences show stem cells together with blood
vessels, enabling quantification of the dynamic behaviors of stem cells in
relation to their vascular niche, with applications in developmental and cancer
biology. Our application automatically segments, tracks, and lineages the image
sequence data and then allows the user to view and edit the results of
automated algorithms in a stereoscopic 3-D window while simultaneously viewing
the stem cell lineage tree in a 2-D window. Using the GPU to store and render
the image sequence data enables a hybrid computational approach. An
inference-based approach utilizing user-provided edits to automatically correct
related mistakes executes interactively on the system CPU while the GPU handles
3-D visualization tasks. Conclusions: By exploiting commodity computer gaming
hardware, we have developed an application that can be run in the laboratory to
facilitate rapid iteration through biological experiments. There is a pressing
need for visualization and analysis tools for 5-D live cell image data. We
combine accurate unsupervised processes with an intuitive visualization of the
results. Our validation interface allows for each data set to be corrected to
100% accuracy, ensuring that downstream data analysis is accurate and
verifiable. Our tool is the first to combine all of these aspects, leveraging
the synergies obtained by utilizing validation information from stereo
visualization to improve the low level image processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2098</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2098</id><created>2014-07-08</created><authors><author><keyname>J&#xe4;ger</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Peltzer</keyname><forenames>Alexander</forenames></author><author><keyname>Nieselt</keyname><forenames>Kay</forenames></author></authors><title>inPHAP: Interactive visualization of genotype and phased haplotype data</title><categories>cs.CE q-bio.GN</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: To understand individual genomes it is necessary to look at the
variations that lead to changes in phenotype and possibly to disease. However,
genotype information alone is often not sufficient and additional knowledge
regarding the phase of the variation is needed to make correct interpretations.
Interactive visualizations, that allow the user to explore the data in various
ways, can be of great assistance in the process of making well informed
decisions. But, currently there is a lack for visualizations that are able to
deal with phased haplotype data. Results: We present inPHAP, an interactive
visualization tool for genotype and phased haplotype data. inPHAP features a
variety of interaction possibilities such as zooming, sorting, filtering and
aggregation of rows in order to explore patterns hidden in large genetic data
sets. As a proof of concept, we apply inPHAP to the phased haplotype data set
of Phase 1 of the 1000 Genomes Project. Thereby, inPHAP's ability to show
genetic variations on the population as well as on the individuals level is
demonstrated for several disease related loci. Conclusions: As of today, inPHAP
is the only visual analytical tool that allows the user to explore unphased and
phased haplotype data interactively. Due to its highly scalable design, inPHAP
can be applied to large datasets with up to 100 GB of data, enabling users to
visualize even large scale input data. inPHAP closes the gap between common
visualization tools for unphased genotype data and introduces several new
features, such as the visualization of phased data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2101</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2101</id><created>2014-07-08</created><authors><author><keyname>Dinkla</keyname><forenames>Kasper</forenames></author><author><keyname>El-Kebir</keyname><forenames>Mohammed</forenames></author><author><keyname>Bucur</keyname><forenames>Cristina-Iulia</forenames></author><author><keyname>Siderius</keyname><forenames>Marco</forenames></author><author><keyname>Smit</keyname><forenames>Martine J.</forenames></author><author><keyname>Westenberg</keyname><forenames>Michel A.</forenames></author><author><keyname>Klau</keyname><forenames>Gunnar W.</forenames></author></authors><title>eXamine: Exploring annotated modules in networks</title><categories>cs.CE cs.SI q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Biological networks have a growing importance for the
interpretation of high-throughput omics data. Integrative network analysis
makes use of statistical and combinatorial methods to extract smaller
subnetwork modules, and performs enrichment analysis to annotate the modules
with ontology terms or other available knowledge. This process results in an
annotated module, which retains the original network structure and includes
enrichment information as a set system. A major bottleneck is a lack of tools
that allow exploring both network structure of extracted modules and its
annotations. Results:
Thispaperpresentsavisualanalysisapproachthattargetssmallmoduleswithmanyset-based
annotations, and which displays the annotations as contours on top of a
node-link diagram. We introduce an extension of self-organizing maps to lay out
nodes, links, and contours in a unified way. An implementation of this approach
is freely available as the Cytoscape app eXamine. Conclusions: eXamine
accurately conveys small and annotated modules consisting of several dozens of
proteins and annotations. We demonstrate that eXamine facilitates the
interpretation of integrative network analysis results in a guided case study.
This study has resulted in a novel biological insight regarding the
virally-encoded G-protein coupled receptor US28.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2106</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2106</id><created>2014-07-08</created><updated>2014-07-11</updated><authors><author><keyname>Calautti</keyname><forenames>Marco</forenames></author><author><keyname>Greco</keyname><forenames>Sergio</forenames></author><author><keyname>Spezzano</keyname><forenames>Francesca</forenames></author><author><keyname>Trubitsyna</keyname><forenames>Irina</forenames></author></authors><title>Checking Termination of Bottom-Up Evaluation of Logic Programs with
  Function Symbols</title><categories>cs.LO</categories><comments>36 pages, 7 figures</comments><journal-ref>Theory and Practice of Logic Programming 15 (2014) 854-889</journal-ref><doi>10.1017/S1471068414000623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been an increasing interest in the bottom-up evaluation
of the semantics of logic programs with complex terms. The presence of function
symbols in the program may render the ground instantiation infinite, and
finiteness of models and termination of the evaluation procedure, in the
general case, are not guaranteed anymore. Since the program termination problem
is undecidable in the general case, several decidable criteria (called program
termination criteria) have been recently proposed. However, current conditions
are not able to identify even simple programs, whose bottom-up execution always
terminates. The paper introduces new decidable criteria for checking
termination of logic programs with function symbols under bottom-up evaluation,
by deeply analyzing the program structure. First, we analyze the propagation of
complex terms among arguments by means of the extended version of the argument
graph called propagation graph. The resulting criterion, called
Gamma-acyclicity, generalizes most of the decidable criteria proposed so far.
Next, we study how rules may activate each other and define a more powerful
criterion, called safety. This criterion uses the so-called safety function
able to analyze how rules may activate each other and how the presence of some
arguments in a rule limits its activation. We also study the application of the
proposed criteria to bound queries and show that the safety criterion is
well-suited to identify relevant classes of programs and bound queries.
Finally, we propose a hierarchy of classes of terminating programs, called
k-safety, where the k-safe class strictly includes the (k-1)-safe class. Note:
To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2107</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2107</id><created>2014-07-08</created><authors><author><keyname>Ding</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Huang</keyname><forenames>Kun</forenames></author><author><keyname>Machiraju</keyname><forenames>Raghu</forenames></author></authors><title>iGPSe: A Visual Analytic System for Integrative Genomic Based Cancer
  Patient Stratification</title><categories>cs.GR cs.HC q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Cancers are highly heterogeneous with different subtypes. These
subtypes often possess different genetic variants, present different
pathological phenotypes, and most importantly, show various clinical outcomes
such as varied prognosis and response to treatment and likelihood for
recurrence and metastasis. Recently, integrative genomics (or panomics)
approaches are often adopted with the goal of combining multiple types of omics
data to identify integrative biomarkers for stratification of patients into
groups with different clinical outcomes. Results: In this paper we present a
visual analytic system called Interactive Genomics Patient Stratification
explorer (iGPSe) which significantly reduces the computing burden for
biomedical researchers in the process of exploring complicated integrative
genomics data. Our system integrates unsupervised clustering with graph and
parallel sets visualization and allows direct comparison of clinical outcomes
via survival analysis. Using a breast cancer dataset obtained from the The
Cancer Genome Atlas (TCGA) project, we are able to quickly explore different
combinations of gene expression (mRNA) and microRNA features and identify
potential combined markers for survival prediction. Conclusions: Visualization
plays an important role in the process of stratifying given population
patients. Visual tools allowed for the selection of possibly features across
various datasets for the given patient population. We essentially made a case
for visualization for a very important problem in translational informatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2109</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2109</id><created>2014-07-08</created><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Morteza</forenames></author><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author><author><keyname>Sohler</keyname><forenames>Christian</forenames></author></authors><title>Planar Graphs: Random Walks and Bipartiteness Testing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of property testing in arbitrary planar graphs. We
prove that bipartiteness can be tested in constant time: for every planar graph
$G$ and $\varepsilon&gt;0$, we can distinguish in constant time between the case
that $G$ is bipartite and the case that $G$ is $\varepsilon$-far from
bipartite. The previous bound for this class of graphs was
$\tilde{O}(\sqrt{n})$, where $n$ is the number of vertices, and the
constant-time testability was only known for planar graphs with bounded degree.
Our approach extends to arbitrary minor-free graphs.
  Our algorithm is based on random walks. The challenge here is to analyze
random walks for graphs that have good separators, i.e., bad expansion.
Standard techniques that use a fast convergence of random walks to a uniform
distribution do not work in this case. Informally, our approach is to
self-reduce the problem of finding an odd-length cycle in a multigraph $G$
induced by a collection of cycles to the same problem on another multigraph
$G'$ induced by a set of shorter odd-length cycles, in such a way that when a
random walk finds a cycle in $G'$ with probability $p &gt; 0$, then it does so in
$G$ with probability $\lambda(p)&gt;0$. This reduction is applied until the cycles
collapse to self-loops, in which case they can be easily detected.
  While the analysis presented in this paper applies only to testing
bipartiteness, we believe that the techniques developed will find applications
to testing other properties in arbitrary planar (or minor-free) graphs, in a
similar way as in the past the advances in testing bipartiteness led to the
development of testing algorithms for more complex graph properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2110</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2110</id><created>2014-07-08</created><authors><author><keyname>Ray</keyname><forenames>William C.</forenames></author><author><keyname>Wolock</keyname><forenames>Samuel L.</forenames></author><author><keyname>Callahan</keyname><forenames>Nicholas W</forenames></author><author><keyname>Dong</keyname><forenames>Min</forenames></author><author><keyname>Li</keyname><forenames>Q. Quinn</forenames></author><author><keyname>Liang</keyname><forenames>Chun</forenames></author><author><keyname>Magliery</keyname><forenames>Thomas J</forenames></author><author><keyname>Bartlett</keyname><forenames>Christopher W.</forenames></author></authors><title>Addressing the unmet need for visualizing Conditional Random Fields in
  Biological Data</title><categories>cs.GR q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The biological world is replete with phenomena that appear to be
ideally modeled and analyzed by one archetypal statistical framework - the
Graphical Probabilistic Model (GPM). The structure of GPMs is a uniquely good
match for biological problems that range from aligning sequences to modeling
the genome-to-phenome relationship. The fundamental questions that GPMs address
involve making decisions based on a complex web of interacting factors.
Unfortunately, while GPMs ideally fit many questions in biology, they are not
an easy solution to apply. Building a GPM is not a simple task for an end user.
Moreover, applying GPMs is also impeded by the insidious fact that the complex
web of interacting factors inherent to a problem might be easy to define and
also intractable to compute upon. Discussion: We propose that the visualization
sciences can contribute to many domains of the bio-sciences, by developing
tools to address archetypal representation and user interaction issues in GPMs,
and in particular a variety of GPM called a Conditional Random Field(CRF). CRFs
bring additional power, and additional complexity, because the CRF dependency
network can be conditioned on the query data. Conclusions: In this manuscript
we examine the shared features of several biological problems that are amenable
to modeling with CRFs, highlight the challenges that existing visualization and
visual analytics paradigms induce for these data, and document an experimental
solution called StickWRLD which, while leaving room for improvement, has been
successfully applied in several biological research projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2112</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2112</id><created>2014-07-08</created><authors><author><keyname>Feigelman</keyname><forenames>Justin</forenames></author><author><keyname>Theis</keyname><forenames>Fabian J.</forenames></author><author><keyname>Marr</keyname><forenames>Carsten</forenames></author></authors><title>MCA: Multiresolution Correlation Analysis, a graphical tool for
  subpopulation identification in single-cell gene expression data</title><categories>cs.GR cs.HC q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Biological data often originate from samples containing mixtures
of subpopulations, corresponding e.g. to distinct cellular phenotypes. However,
identification of distinct subpopulations may be difficult if biological
measurements yield distributions that are not easily separable. Results: We
present Multiresolution Correlation Analysis (MCA), a method for visually
identifying subpopulations based on the local pairwise correlation between
covariates, without needing to define an a priori interaction scale. We
demonstrate that MCA facilitates the identification of differentially regulated
subpopulations in simulated data from a small gene regulatory network, followed
by application to previously published single-cell qPCR data from mouse
embryonic stem cells. We show that MCA recovers previously identified
subpopulations, provides additional insight into the underlying correlation
structure, reveals potentially spurious compartmentalizations, and provides
insight into novel subpopulations. Conclusions: MCA is a useful method for the
identification of subpopulations in low-dimensional expression data, as
emerging from qPCR or FACS measurements. With MCA it is possible to investigate
the robustness of covariate correlations with respect subpopulations,
graphically identify outliers, and identify factors contributing to
differential regulation between pairs of covariates. MCA thus provides a
framework for investigation of expression correlations for genes of interests
and biological hypothesis generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2117</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2117</id><created>2014-07-08</created><authors><author><keyname>Taylor</keyname><forenames>Andy</forenames></author><author><keyname>McLeod</keyname><forenames>Kenneth</forenames></author><author><keyname>Armit</keyname><forenames>Chris</forenames></author><author><keyname>Baldock</keyname><forenames>Richard</forenames></author><author><keyname>Burger</keyname><forenames>Albert</forenames></author></authors><title>Visualization of gene expression information within the context of the
  mouse anatomy</title><categories>cs.HC cs.GR q-bio.QM</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The eMouse Atlas of Gene Expression (EMAGE) is an online resource
that publishes the results of in situ gene expression experiments on the
developmental mouse. The resource provides comprehensive search facilities, but
few analytical tools or visual mechanisms for navigating the data set. To deal
with the missing visual navigation, this paper explores the application of
sunburst and icicle visualizations within EMAGE. Results: A prototype solution
delivered a simple user interface that helps the user query EMAGE and generate
a sunburst/icicle diagram. An evaluation featuring test subjects from the EMAGE
staff studied the visualizations and provided a range of suggested
improvements. Moreover the evaluation discovered that in addition to providing
a visual means of walking through the data, when grouped, the sunburst delivers
an interactive overview that assists with analysing sets of related genes.
Conclusions: The sunburst and icicle visualizations have been shown to be
effective tools for summarising gene expression data. The sunburst with its
space saving radial layout was found especially useful for providing an
overview of gene families or pathways. Work is ongoing to integrate these
visualizations into EMAGE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2125</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2125</id><created>2014-07-08</created><authors><author><keyname>Parashar</keyname><forenames>Anubha</forenames></author><author><keyname>Kumar</keyname><forenames>Susheel</forenames></author><author><keyname>Bhaskar</keyname><forenames>Vinay S</forenames></author><author><keyname>Chinia</keyname><forenames>Rajni</forenames></author></authors><title>Noisy Distance Measurements Using 3-D Localization with Rb-Rf Methods</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are dynamically formed over the varying topologies.
Wireless sensor networks can assist in conducting the rescue operations and can
provide search in timely manner. Long time monitoring applications are
environment monitoring, security surveillance and habitat monitoring. Further,
where it can be deployed in time critical situations when disaster happens. As
we are dealing with the human lives here, we can not just rely on the
localization schemes that depend upon the connectivity information Rf i.e.
range-free algorithms only. Further, rescue operations are carried out in
highly noisy environments, so distance based Rb(range-based) localization
algorithms generate high error in distance measurements. An efficient algorithm
is needed that can measure the location of the sensor nodes near to the living
being or being attached to them in 3-D space with a high accuracy. To achieve
such kind of accuracy a combination of both the strategies is required. The
proposed method which incorporates both the Rb(range-based)&amp;Rfrange-free
strategies that successfully localizes nodes in a sensor network with noisy
distance measurements. We also have depicted the effect of scalability on the
performance of the algorithm. Results show that as the scalability of the
network increases with the number of beacon nodes; the performance of the
algorithm goes high above 90 percent . The granularity of the areas estimated
may be easily adjusted by changing the system parameters which makes the
proposed algorithm flexible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2136</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2136</id><created>2014-07-08</created><updated>2015-08-04</updated><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Zeman</keyname><forenames>Peter</forenames></author></authors><title>Automorphism Groups of Geometrically Represented Graphs</title><categories>math.CO cs.DM</categories><msc-class>05C62, 08A35, 20D45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique to determine the automorphism group of a
geometrically represented graph, by understanding the structure of the induced
action on all geometric representations. Using this, we characterize
automorphism groups of interval, permutation and circle graphs. We combine
techniques from group theory (products, homomorphisms, actions) with data
structures from computer science (PQ-trees, split trees, modular trees) that
encode all geometric representations.
  We prove that interval graphs have the same automorphism groups as trees, and
for a given interval graph, we construct a tree with the same automorphism
group which answers a question of Hanlon [Trans. Amer. Math. Soc 272(2), 1982].
For permutation and circle graphs, we give an inductive characterization by
semidirect and wreath products. We also prove that every abstract group can be
realized by the automorphism group of a comparability graph/poset of the
dimension at most four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2143</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2143</id><created>2014-07-08</created><authors><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Guo</keyname><forenames>Jiong</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Parameterized Algorithmics for Computational Social Choice: Nine
  Research Challenges</title><categories>cs.MA cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational Social Choice is an interdisciplinary research area involving
Economics, Political Science, and Social Science on the one side, and
Mathematics and Computer Science (including Artificial Intelligence and
Multiagent Systems) on the other side. Typical computational problems studied
in this field include the vulnerability of voting procedures against attacks,
or preference aggregation in multi-agent systems. Parameterized Algorithmics is
a subfield of Theoretical Computer Science seeking to exploit meaningful
problem-specific parameters in order to identify tractable special cases of in
general computationally hard problems. In this paper, we propose nine of our
favorite research challenges concerning the parameterized complexity of
problems appearing in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2147</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2147</id><created>2014-07-08</created><authors><author><keyname>Smith</keyname><forenames>James</forenames></author></authors><title>Gender Prediction in Social Media</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we explore the task of gender classification using limited
network data with an application to Fotolog. We take a heuristic approach to
automating gender inference based on username, followers and network structure.
We test our approach on a subset of 100,000 nodes and analyze our results to
find that there is a lot of value in these limited information and that there
is great promise in further pursuing this approach to classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2149</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2149</id><created>2014-07-08</created><updated>2015-05-19</updated><authors><author><keyname>Gazda</keyname><forenames>Maciej</forenames></author><author><keyname>Willemse</keyname><forenames>Tim A. C.</forenames></author></authors><title>Strategy Derivation for Small Progress Measures</title><categories>cs.LO cs.GT</categories><comments>polished the text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small Progress Measures is one of the most efficient parity game solving
algorithms. The original algorithm provides the full solution (winning regions
and strategies) in $O(dm \cdot (n/\lceil d / 2 \rceil)^{\lceil d/2 \rceil})$
time, and requires a re-run of the algorithm on one of the winning regions. We
provide a novel operational interpretation of progress measures, and modify the
algorithm so that it derives the winning strategies for both players in one
pass. This reduces the upper bound on strategy derivation for SPM to $O(dm
\cdot (n/\lfloor d / 2 \rfloor)^{\lfloor d/2 \rfloor})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2151</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2151</id><created>2014-07-08</created><authors><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>Time lower bounds for nonadaptive turnstile streaming algorithms</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say a turnstile streaming algorithm is &quot;non-adaptive&quot; if, during updates,
the memory cells written and read depend only on the index being updated and
random coins tossed at the beginning of the stream (and not on the memory
contents of the algorithm). Memory cells read during queries may be decided
upon adaptively. All known turnstile streaming algorithms in the literature are
non-adaptive.
  We prove the first non-trivial update time lower bounds for both randomized
and deterministic turnstile streaming algorithms, which hold when the
algorithms are non-adaptive. While there has been abundant success in proving
space lower bounds, there have been no non-trivial update time lower bounds in
the turnstile model. Our lower bounds hold against classically studied problems
such as heavy hitters, point query, entropy estimation, and moment estimation.
In some cases of deterministic algorithms, our lower bounds nearly match known
upper bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2161</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2161</id><created>2014-07-08</created><authors><author><keyname>Scholz</keyname><forenames>Christoph</forenames></author><author><keyname>Atzmueller</keyname><forenames>Martin</forenames></author><author><keyname>Stumme</keyname><forenames>Gerd</forenames></author></authors><title>Link Prediction and the Role of Stronger Ties in Networks of
  Face-to-Face Proximity</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the structures why links are formed is an important and
prominent research topic. In this paper, we therefore consider the link
prediction problem in face-to-face contact networks, and analyze the
predictability of new and recurring links. Furthermore, we study additional
influence factors, and the role of stronger ties in these networks.
Specifically, we compare neighborhood-based and path-based network proximity
measures in a threshold-based analysis for capturing temporal dynamics. The
results and insights of the analysis are a first step onto predictability
applications for human contact networks, for example, for improving
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2168</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2168</id><created>2014-07-08</created><authors><author><keyname>Dreyfus</keyname><forenames>Emmanuel</forenames></author></authors><title>TLS hardening</title><categories>cs.CR</categories><comments>19 pages</comments><journal-ref>BSD magazine (June 2014) 6-18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document presents TLS and how to make it secure enough as of 2014
Spring. Of course all the information given here will rot with time. Protocols
known as secure will be cracked and will be replaced with better versions.
Fortunately we will see that there are ways to assess the current security of
your setup, but this explains why you may have to read further from this
document to get the up to date knowledge on TLS security.
  We will first introduce the TLS protocol and its underlying components: X.509
certificates, ciphers, and protocol versions. Next we will have a look at TLS
hardening for web servers, and how to plug various vulnerabilities: CRIME,
BREACH, BEAST, session renegotiation, Heartbleed, and others. We will finally
see how the know-how acquired on hardening web servers can be used for other
protocols and tools such as Dovecot, Sendmail, SquirrelMail, RoundCube, and
OpenVPN.
  We assume you already maintain services that use TLS, and have basic TCP/IP
network knowledge. Some information will also be useful for the application
developer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2169</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2169</id><created>2014-07-08</created><authors><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE</affiliation></author><author><keyname>Tamas</keyname><forenames>Wani W.</forenames><affiliation>SPE</affiliation></author><author><keyname>Nivet</keyname><forenames>Marie Laure</forenames><affiliation>SPE</affiliation></author><author><keyname>Notton</keyname><forenames>Gilles</forenames><affiliation>SPE</affiliation></author><author><keyname>Paoli</keyname><forenames>Christophe</forenames><affiliation>SPE</affiliation></author><author><keyname>Balu</keyname><forenames>Aur&#xe9;lia</forenames><affiliation>SPE</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author></authors><title>Meteorological time series forecasting with pruned multi-layer
  perceptron and 2-stage Levenberg-Marquardt method</title><categories>cs.NE cs.SY</categories><comments>International Journal of Modelling, Identification and Control
  (2014). arXiv admin note: substantial text overlap with arXiv:1308.1940</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Multi-Layer Perceptron (MLP) defines a family of artificial neural networks
often used in TS modeling and forecasting. Because of its &quot;black box&quot; aspect,
many researchers refuse to use it. Moreover, the optimization (often based on
the exhaustive approach where &quot;all&quot; configurations are tested) and learning
phases of this artificial intelligence tool (often based on the
Levenberg-Marquardt algorithm; LMA) are weaknesses of this approach
(exhaustively and local minima). These two tasks must be repeated depending on
the knowledge of each new problem studied, making the process, long, laborious
and not systematically robust. In this paper a pruning process is proposed.
This method allows, during the training phase, to carry out an inputs selecting
method activating (or not) inter-nodes connections in order to verify if
forecasting is improved. We propose to use iteratively the popular damped
least-squares method to activate inputs and neurons. A first pass is applied to
10% of the learning sample to determine weights significantly different from 0
and delete other. Then a classical batch process based on LMA is used with the
new MLP. The validation is done using 25 measured meteorological TS and
cross-comparing the prediction results of the classical LMA and the 2-stage
LMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2170</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2170</id><created>2014-07-08</created><updated>2014-11-25</updated><authors><author><keyname>Tolias</keyname><forenames>Giorgos</forenames><affiliation>INRIA</affiliation></author><author><keyname>Furon</keyname><forenames>Teddy</forenames><affiliation>INRIA</affiliation></author><author><keyname>J&#xe9;gou</keyname><forenames>Herv&#xe9;</forenames><affiliation>INRIA</affiliation></author></authors><title>Orientation covariant aggregation of local descriptors with embeddings</title><categories>cs.CV</categories><comments>European Conference on Computer Vision (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image search systems based on local descriptors typically achieve orientation
invariance by aligning the patches on their dominant orientations. Albeit
successful, this choice introduces too much invariance because it does not
guarantee that the patches are rotated consistently. This paper introduces an
aggregation strategy of local descriptors that achieves this covariance
property by jointly encoding the angle in the aggregation stage in a continuous
manner. It is combined with an efficient monomial embedding to provide a
codebook-free method to aggregate local descriptors into a single vector
representation. Our strategy is also compatible and employed with several
popular encoding methods, in particular bag-of-words, VLAD and the Fisher
vector. Our geometric-aware aggregation strategy is effective for image search,
as shown by experiments performed on standard benchmarks for image and
particular object retrieval, namely Holidays and Oxford buildings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2178</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2178</id><created>2014-07-08</created><updated>2015-02-22</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author></authors><title>Restricted Isometry Property for General p-Norms</title><categories>cs.DS cs.DM cs.IT math.IT math.NA math.PR</categories><comments>An extended abstract of this paper is to appear at the 31st
  International Symposium on Computational Geometry (SoCG 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Restricted Isometry Property (RIP) is a fundamental property of a matrix
which enables sparse recovery. Informally, an $m \times n$ matrix satisfies RIP
of order $k$ for the $\ell_p$ norm, if $\|Ax\|_p \approx \|x\|_p$ for every
vector $x$ with at most $k$ non-zero coordinates.
  For every $1 \leq p &lt; \infty$ we obtain almost tight bounds on the minimum
number of rows $m$ necessary for the RIP property to hold. Prior to this work,
only the cases $p = 1$, $1 + 1 / \log k$, and $2$ were studied. Interestingly,
our results show that the case $p = 2$ is a &quot;singularity&quot; point: the optimal
number of rows $m$ is $\widetilde{\Theta}(k^{p})$ for all $p\in
[1,\infty)\setminus \{2\}$, as opposed to $\widetilde{\Theta}(k)$ for $k=2$.
  We also obtain almost tight bounds for the column sparsity of RIP matrices
and discuss implications of our results for the Stable Sparse Recovery problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2188</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2188</id><created>2014-07-08</created><authors><author><keyname>Lang</keyname><forenames>John C.</forenames></author><author><keyname>Abrams</keyname><forenames>Daniel M.</forenames></author><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author></authors><title>The influence of societal individualism on a century of tobacco use:
  modelling the prevalence of smoking</title><categories>math.DS cs.SI physics.soc-ph</categories><msc-class>91D10 (Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smoking of tobacco is predicted to cause approximately six million deaths
worldwide in 2014. Responding effectively to this epidemic requires a thorough
understanding of how smoking behaviour is transmitted and modified. Here, we
present a new mathematical model of the social dynamics that cause cigarette
smoking to spread in a population. Our model predicts that more individualistic
societies will show faster adoption and cessation of smoking. Evidence from a
new century-long composite data set on smoking prevalence in 25 countries
supports the model, with direct implications for public health interventions
around the world. Our results suggest that differences in culture between
societies can measurably affect the temporal dynamics of a social spreading
process, and that these effects can be understood via a quantitative
mathematical model matched to observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2190</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2190</id><created>2014-06-11</created><authors><author><keyname>Alam</keyname><forenames>Shahid</forenames></author></authors><title>Is Fortran Still Relevant? Comparing Fortran with Java and C++</title><categories>cs.PL cs.SE</categories><journal-ref>International Journal of Software Engineering &amp; Application, pages
  25-45, Volume 5, No 3, 2014</journal-ref><doi>10.5121/ijsea.2014.5303</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a comparative study to evaluate and compare Fortran with
the two most popular programming languages Java and C++. Fortran has gone
through major and minor extensions in the years 2003 and 2008. (1) How much
have these extensions made Fortran comparable to Java and C++? (2) What are the
differences and similarities, in supporting features like: Templates, object
constructors and destructors, abstract data types and dynamic binding? These
are the main questions we are trying to answer in this study. An
object-oriented ray tracing application is implemented in these three languages
to compare them. By using only one program we ensured there was only one set of
requirements thus making the comparison homogeneous. Based on our literature
survey this is the first study carried out to compare these languages by
applying software metrics to the ray tracing application and comparing these
results with the similarities and differences found in practice. We motivate
the language implementers and compiler developers, by providing binary analysis
and profiling of the application, to improve Fortran object handling and
processing, and hence making it more prolific and general. This study
facilitates and encourages the reader to further explore, study and use these
languages more effectively and productively, especially Fortran.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2201</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2201</id><created>2014-07-08</created><updated>2015-06-29</updated><authors><author><keyname>George</keyname><forenames>Geordie</forenames></author><author><keyname>Mungara</keyname><forenames>Ratheesh K.</forenames></author><author><keyname>Lozano</keyname><forenames>Angel</forenames></author></authors><title>An Analytical Framework for Device-to-Device Communication in Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 13 figures, to appear in the IEEE Transactions on Wireless
  Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework that enables characterizing analytically the
spectral efficiency achievable by D2D (device-to-device) communication
integrated with a cellular network. This framework is based on a stochastic
geometry formulation with a novel approach to the modeling of interference and
with the added possibility of incorporating exclusion regions to protect
cellular receivers from excessive interference from active D2D transmitters. To
illustrate the potential of the framework, a number of examples are provided.
These examples confirm the potential of D2D communication in situations of
strong traffic locality as well as the effectiveness of properly sized
exclusion regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2206</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2206</id><created>2014-07-06</created><authors><author><keyname>KP</keyname><forenames>Sanil Shanker</forenames></author><author><keyname>Turner</keyname><forenames>Aaron</forenames></author><author><keyname>Sherly</keyname><forenames>Elizabeth</forenames></author><author><keyname>Austin</keyname><forenames>Jim</forenames></author></authors><title>Sequential Data Mining using Correlation Matrix Memory</title><categories>cs.OH</categories><comments>Networking and Information Technology (ICNIT), 2010 International
  Conference on</comments><doi>10.1109/ICNIT.2010.5508469</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes a method for sequential data mining using correlation
matrix memory. Here, we use the concept of the Logical Match to mine the
indices of the sequential pattern. We demonstrate the uniqueness of the method
with both the artificial and the real datum taken from NCBI databank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2207</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2207</id><created>2014-07-05</created><authors><author><keyname>Kushwah</keyname><forenames>Atul Singh</forenames></author><author><keyname>Rathore</keyname><forenames>Priya</forenames></author><author><keyname>Kumar</keyname><forenames>Ramsewak</forenames></author></authors><title>Performance Estimation of 2*4 MIMO-MC-CDMA Using Convolution Code in
  Different Modulation Technique using ZF Detection Scheme</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages,10 figures, 2 tables, International Journal of Engineering
  Trends and Technology (IJETT) Volume 10 Number 13 Apr 2014</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V10(13), 621-625 April 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V10P324</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we estimate the performance of 2*4 MIMO-MC-CDMA system using
convolution code in MATLAB which highly reduces BER. MC-CDMA (Multi Carrier
Code Division for Multiple Access) is a multiuser and multiple access system
which is formed by the combination of OFDM and CDMA and convolution encoding
scheme is used in encoder of CDMA as FEC (Forward Error Correction) code to
reduce BER (Bit Error Rate). MC-CDMA system is a multicarrier system in which
single wideband frequency selective carrier is converted into parallel
narrowband flat fading multiple sub-carriers to optimize the performance of
system. Now this system further improved by combination of 2*4 MIMO (Multiple
Input Multiple Output) system which utilizes ZF (Zero Forcing) decoder at the
receiver to reduce BER and also half rate convolutionally encoded Alamouti STBC
(Space Time Block Code) block code as transmit diversity of MIMO for multiple
transmission of data through multiple transmit antenna. Main advantage of using
MIMO-MC-CDMA using convolution code is to reduce the complexity of system and
to reduce BER with increased gain. In this paper we analyze system performance
in different modulation schemes like, QPSK, 8PSK, 8QAM, 16QAM, 32QAM and 64QAM
in Rayleigh fading channel using MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2208</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2208</id><created>2014-06-27</created><authors><author><keyname>&#xd6;zger</keyname><forenames>Zeynep &#xd6;demi&#x15f;</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author><author><keyname>Dougherty</keyname><forenames>Steven</forenames></author></authors><title>On Codes Over $\mathbb{Z}_{p^{s}}$ with the Extended Lee Weight</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider codes over $\mathbb{Z}_{p^s}$ with the extended Lee weight. We
find Singleton bounds with respect to this weight and define MLDS and MLDR
codes accordingly. We also consider the kernels of these codes and the notion
of independence of vectors in this space. We investigate the linearity and
duality of the Gray images of codes over $\mathbb{Z}_{p^s}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2210</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2210</id><created>2014-07-08</created><updated>2014-10-13</updated><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Group Minds and the Case of Wikipedia</title><categories>q-bio.NC cs.GT cs.SI physics.soc-ph q-bio.PE</categories><comments>21 pages, 6 figures; matches published version</comments><report-no>SFI Working Paper #14-10-037</report-no><journal-ref>Human Computation (2014) 1:1:5-29</journal-ref><doi>10.15346/hc.v1i1.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-level cognitive states are widely observed in human social systems, but
their discussion is often ruled out a priori in quantitative approaches. In
this paper, we show how reference to the irreducible mental states and
psychological dynamics of a group is necessary to make sense of large scale
social phenomena. We introduce the problem of mental boundaries by reference to
a classic problem in the evolution of cooperation. We then provide an explicit
quantitative example drawn from ongoing work on cooperation and conflict among
Wikipedia editors, showing how some, but not all, effects of individual
experience persist in the aggregate. We show the limitations of methodological
individualism, and the substantial benefits that come from being able to refer
to collective intentions, and attributions of cognitive states of the form
&quot;what the group believes&quot; and &quot;what the group values&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2217</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2217</id><created>2014-07-08</created><authors><author><keyname>Larbi</keyname><forenames>Ibtissam</forenames><affiliation>LTT</affiliation></author><author><keyname>Benmammar</keyname><forenames>Badr</forenames><affiliation>LTT</affiliation></author></authors><title>N\'egociation de spectre dans les r\'eseaux de radio cognitive</title><categories>cs.MA</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we propose a technique using negotiation based on multi-agent
system (MAS) in the context of cognitive radio network (CRN). The agents are
particularly suited to provide responsive solutions to complex problems such as
the negotiation of the spectrum in CRN. We have implemented our proposed
solution with JADE (Java Agent Development Framework) and we have also evaluate
the proposed solution to show its interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2220</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2220</id><created>2014-07-08</created><updated>2014-07-09</updated><authors><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Ma</keyname><forenames>Qiang</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Thompson</keyname><forenames>Brian</forenames></author></authors><title>Modeling Collaboration in Academia: A Game Theoretic Approach</title><categories>cs.SI cs.DL cs.GT</categories><comments>Presented at the 1st WWW Workshop on Big Scholarly Data (2014). 6
  pages, 5 figures</comments><acm-class>J.4</acm-class><journal-ref>Proceedings of the Companion Publication of the 23rd International
  Conference on World Wide Web (WWW 2014), pgs 1177-1182</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we aim to understand the mechanisms driving academic
collaboration. We begin by building a model for how researchers split their
effort between multiple papers, and how collaboration affects the number of
citations a paper receives, supported by observations from a large real-world
publication and citation dataset, which we call the h-Reinvestment model. Using
tools from the field of Game Theory, we study researchers' collaborative
behavior over time under this model, with the premise that each researcher
wants to maximize his or her academic success. We find analytically that there
is a strong incentive to collaborate rather than work in isolation, and that
studying collaborative behavior through a game-theoretic lens is a promising
approach to help us better understand the nature and dynamics of academic
collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2221</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2221</id><created>2014-07-06</created><authors><author><keyname>Simon</keyname><forenames>Laurent</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Nouviale</keyname><forenames>Florian</forenames><affiliation>INSA Rennes, INRIA - IRISA</affiliation></author><author><keyname>Gaugne</keyname><forenames>Ronan</forenames><affiliation>UR1</affiliation></author><author><keyname>Gouranton</keyname><forenames>Val&#xe9;rie</forenames><affiliation>INSA Rennes, INRIA - IRISA</affiliation></author></authors><title>Sonic interaction with a virtual orchestra of factory machinery</title><categories>cs.MM cs.SD</categories><comments>Sonic Interaction for Virtual Environments, Minneapolis : United
  States (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an immersive application where users receive sound and
visual feedbacks on their interactions with a virtual environment. In this
application, the users play the part of conductors of an orchestra of factory
machines since each of their actions on interaction devices triggers a pair of
visual and audio responses. Audio stimuli were spatialized around the listener.
The application was exhibited during the 2013 Science and Music day and
designed to be used in a large immersive system with head tracking, shutter
glasses and a 10.2 loudspeaker configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2227</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2227</id><created>2014-06-20</created><authors><author><keyname>A</keyname><forenames>Arun Kumar</forenames></author><author><keyname>Philip</keyname><forenames>Ninan Sajeeth</forenames></author><author><keyname>Samar</keyname><forenames>Vincent J</forenames></author><author><keyname>Desjardins</keyname><forenames>James A</forenames></author><author><keyname>Segalowitz</keyname><forenames>Sidney J</forenames></author></authors><title>A Wavelet Based Algorithm for the Identification of Oscillatory
  Event-Related Potential Components</title><categories>cs.OH q-bio.NC</categories><comments>Journal of neuroscience methods 06/2014</comments><doi>10.1016/j.jneumeth.2014.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event Related Potentials (ERPs) are very feeble alterations in the ongoing
Electroencephalogram (EEG) and their detection is a challenging problem. Based
on the unique time-based parameters derived from wavelet coefficients and the
asymmetry property of wavelets a novel algorithm to separate ERP components in
single-trial EEG data is described. Though illustrated as a specific
application to N170 ERP detection, the algorithm is a generalized approach that
can be easily adapted to isolate different kinds of ERP components. The
algorithm detected the N170 ERP component with a high level of accuracy. We
demonstrate that the asymmetry method is more accurate than the matching
wavelet algorithm and t-CWT method by 48.67 and 8.03 percent respectively. This
paper provides an off-line demonstration of the algorithm and considers issues
related to the extension of the algorithm to real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2232</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2232</id><created>2014-06-01</created><authors><author><keyname>Safta</keyname><forenames>Cosmin</forenames></author><author><keyname>Chen</keyname><forenames>Richard L.</forenames></author><author><keyname>Najm</keyname><forenames>Habib N.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>watson</keyname><forenames>Jean-paul</forenames></author></authors><title>Toward Using Surrogates to Accelerate Solution of Stochastic Electricity
  Grid Operations Problems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic unit commitment models typically handle uncertainties in forecast
demand by considering a finite number of realizations from a stochastic process
model for loads. Accurate evaluations of expectations or higher moments for the
quantities of interest require a prohibitively large number of model
evaluations. In this paper we propose an alternative approach based on using
surrogate models valid over the range of the forecast uncertainty. We consider
surrogate models based on Polynomial Chaos expansions, constructed using sparse
quadrature methods. Considering expected generation cost, we demonstrate the
approach can lead to several orders of magnitude reduction in computational
cost relative to using Monte Carlo sampling on the original model, for a given
target error threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2237</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2237</id><created>2014-07-06</created><authors><author><keyname>KP</keyname><forenames>Sanil Shanker</forenames></author><author><keyname>Sherly</keyname><forenames>Elizabeth</forenames></author><author><keyname>Austin</keyname><forenames>Jim</forenames></author></authors><title>An Algorithm for Alignment-free Sequence Comparison using Logical Match</title><categories>cs.CE q-bio.QM</categories><comments>Computer and Automation Engineering (ICCAE), 2010 The 2nd
  International Conference on</comments><doi>10.1109/ICCAE.2010.5452072</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes an algorithm for alignment-free sequence comparison using
Logical Match. Here, we compute the score using fuzzy membership values which
generate automatically from the number of matches and mismatches. We
demonstrate the method with both the artificial and real datum. The results
show the uniqueness of the proposed method by analyzing DNA sequences taken
from NCBI databank with a novel computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2241</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2241</id><created>2014-07-07</created><authors><author><keyname>Drakopoulos</keyname><forenames>Kimon</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Tsitsiklis</keyname><forenames>John N.</forenames></author></authors><title>An efficient curing policy for epidemics on graphs</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a dynamic policy for the rapid containment of a contagion process
modeled as an SIS epidemic on a bounded degree undirected graph with n nodes.
We show that if the budget $r$ of curing resources available at each time is
${\Omega}(W)$, where $W$ is the CutWidth of the graph, and also of order
${\Omega}(\log n)$, then the expected time until the extinction of the epidemic
is of order $O(n/r)$, which is within a constant factor from optimal, as well
as sublinear in the number of nodes. Furthermore, if the CutWidth increases
only sublinearly with n, a sublinear expected time to extinction is possible
with a sublinearly increasing budget $r$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2275</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2275</id><created>2014-07-08</created><authors><author><keyname>Lewis</keyname><forenames>Ryan H.</forenames></author><author><keyname>Zomorodian</keyname><forenames>Afra</forenames></author></authors><title>Multicore Homology via Mayer Vietoris</title><categories>cs.CG math.AT</categories><comments>For source code and data sets, see http://www.ctl.appliedtopology.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the parallel computation of homology using the
Mayer-Vietoris principle. We present a two stage approach for parallelizing
persistence. In the first stage, we produce a cover of the input cell complex
by overlapping subspaces. In the second stage, we use this cover to build the
Mayer-Vietoris blowup complex, a topological space, which organizes the various
subspaces needed for employing the Mayer-Vietoris principle. Next, we compute
the homology of each subspace in the blowup complex in parallel and then glue
these results together in serial. We show how to use the persistence algorithm
to organize these computations. In the first stage, any algorithm can be used
to produce a cover of the input complex. We describe an algorithm for producing
a cover of a space with a simple structure and bounded overlap based on graph
partitions. Additionally, we present a simplistic model for the problem of
finding covers appropriate for parallel algorithms and show that finding such
covers is NP-Hard. Finally, we present a second parallel homology algorithm.
This algorithm avoids the explicit construction of the blowup complex saving
space. We implement our algorithms for multicore computers, and compare them
against each other as well as existing serial and parallel algorithms with a
suite of experiments. We achieve roughly 8x speedup of the homology
computations on a 10-dimensional complex with about 46 million simplices using
11 cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2279</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2279</id><created>2014-07-08</created><authors><author><keyname>Grahne</keyname><forenames>Gosta</forenames></author><author><keyname>Onet</keyname><forenames>Adrian</forenames></author></authors><title>The data-exchange chase under the microscope</title><categories>cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.6682</comments><acm-class>H.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we take closer look at recent developments for the chase
procedure, and provide additional results. Our analysis allows us create a
taxonomy of the chase variations and the properties they satisfy. Two of the
most central problems regarding the chase is termination, and discovery of
restricted classes of sets of dependencies that guarantee termination of the
chase. The search for the restricted classes has been motivated by a fairly
recent result that shows that it is undecidable to determine whether the chase
with a given dependency set will terminate on a given instance. There is a
small dissonance here, since the quest has been for classes of sets of
dependencies guaranteeing termination of the chase on all instances, even
though the latter problem was not known to be undecidable. We resolve the
dissonance in this paper by showing that determining whether the chase with a
given set of dependencies terminates on all instances is coRE-complete. For the
hardness proof we use a reduction from word rewriting systems, thereby also
showing the close connection between the chase and word rewriting. The same
reduction also gives us the aforementioned instance-dependent RE-completeness
result as a byproduct. For one of the restricted classes guaranteeing
termination on all instances, the stratified sets dependencies, we provide new
complexity results for the problem of testing whether a given set of
dependencies belongs to it. These results rectify some previous claims that
have occurred in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2283</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2283</id><created>2014-07-08</created><updated>2015-06-05</updated><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author><author><keyname>Chuah</keyname><forenames>Chen-Nee</forenames></author></authors><title>Quantitative Group Testing for Heavy Hitter Detection</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quantitative group testing problem where the objective is to
identify defective items in a given population based on results of tests
performed on subsets of the population. Under the quantitative group testing
model, the result of each test reveals the number of defective items in the
tested group. The minimum number of tests achievable by nested test plans was
established by Aigner and Schughart in 1985 within a minimax framework. The
optimal nested test plan offering this performance, however, was not obtained.
In this work, we establish the optimal nested test plan in closed form. This
optimal nested test plan is also asymptotically (as the population size grows
to infinity) optimal among all test plans. We then focus on the application of
heavy hitter detection problem for traffic monitoring and anomaly detection in
the Internet and other communication networks. For such applications, it is
often the case that a few abnormal traffic flows with exceptionally high volume
(referred to as heavy hitters) make up most of the traffic seen by the entire
network. Since the volume of heavy hitters is much higher than that of normal
flows, the number of heavy hitters in a group of flows can be accurately
estimated from the aggregated traffic load. Other potential applications
include detecting idle channels in the radio spectrum in the high SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2322</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2322</id><created>2014-07-08</created><updated>2014-10-21</updated><authors><author><keyname>Zhao</keyname><forenames>Tao</forenames></author><author><keyname>Wu</keyname><forenames>Jian</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Energy-Delay Tradeoffs of Virtual Base Stations With a
  Computational-Resource-Aware Energy Consumption Model</title><categories>cs.NI</categories><comments>5 pages, 3 figures, accepted by ICCS'14</comments><doi>10.1109/ICCS.2014.7024759</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The next generation (5G) cellular network faces the challenges of efficiency,
flexibility, and sustainability to support data traffic in the mobile Internet
era. To tackle these challenges, cloud-based cellular architectures have been
proposed where virtual base stations (VBSs) play a key role. VBSs bring further
energy savings but also demands a new energy consumption model as well as the
optimization of computational resources. This paper studies the energy-delay
tradeoffs of VBSs with delay tolerant traffic. We propose a
computational-resource-aware energy consumption model to capture the total
energy consumption of a VBS and reflect the dynamic allocation of computational
resources including the number of CPU cores and the CPU speed. Based on the
model, we analyze the energy-delay tradeoffs of a VBS considering BS sleeping
and state switching cost to minimize the weighted sum of power consumption and
average delay. We derive the explicit form of the optimal data transmission
rate and find the condition under which the energy optimal rate exists and is
unique. Opportunities to reduce the average delay and achieve energy savings
simultaneously are observed. We further propose an efficient algorithm to
jointly optimize the data rate and the number of CPU cores. Numerical results
validate our theoretical analyses and under a typical simulation setting we
find more than 60% energy savings can be achieved by VBSs compared with
conventional base stations under the EARTH model, which demonstrates the great
potential of VBSs in 5G cellular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2323</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2323</id><created>2014-07-08</created><updated>2014-10-07</updated><authors><author><keyname>Lecuyer</keyname><forenames>Mathias</forenames></author><author><keyname>Ducoffe</keyname><forenames>Guillaume</forenames></author><author><keyname>Lan</keyname><forenames>Francis</forenames></author><author><keyname>Papancea</keyname><forenames>Andrei</forenames></author><author><keyname>Petsios</keyname><forenames>Theofilos</forenames></author><author><keyname>Spahn</keyname><forenames>Riley</forenames></author><author><keyname>Chaintreau</keyname><forenames>Augustin</forenames></author><author><keyname>Geambasu</keyname><forenames>Roxana</forenames></author></authors><title>XRay: Enhancing the Web's Transparency with Differential Correlation</title><categories>cs.NI cs.CY</categories><comments>Extended version of a paper presented at the 23rd USENIX Security
  Symposium (USENIX Security 14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's Web services - such as Google, Amazon, and Facebook - leverage user
data for varied purposes, including personalizing recommendations, targeting
advertisements, and adjusting prices. At present, users have little insight
into how their data is being used. Hence, they cannot make informed choices
about the services they choose. To increase transparency, we developed XRay,
the first fine-grained, robust, and scalable personal data tracking system for
the Web. XRay predicts which data in an arbitrary Web account (such as emails,
searches, or viewed products) is being used to target which outputs (such as
ads, recommended products, or prices). XRay's core functions are service
agnostic and easy to instantiate for new services, and they can track data
within and across services. To make predictions independent of the audited
service, XRay relies on the following insight: by comparing outputs from
different accounts with similar, but not identical, subsets of data, one can
pinpoint targeting through correlation. We show both theoretically, and through
experiments on Gmail, Amazon, and YouTube, that XRay achieves high precision
and recall by correlating data from a surprisingly small number of extra
accounts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2330</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2330</id><created>2014-07-08</created><authors><author><keyname>Pears</keyname><forenames>Russel</forenames></author><author><keyname>Finlay</keyname><forenames>Jacqui</forenames></author><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author></authors><title>Synthetic Minority Over-sampling TEchnique(SMOTE) for Predicting
  Software Build Outcomes</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research we use a data stream approach to mining data and construct
Decision Tree models that predict software build outcomes in terms of software
metrics that are derived from source code used in the software construction
process. The rationale for using the data stream approach was to track the
evolution of the prediction model over time as builds are incrementally
constructed from previous versions either to remedy errors or to enhance
functionality. As the volume of data available for mining from the software
repository that we used was limited, we synthesized new data instances through
the application of the SMOTE oversampling algorithm. The results indicate that
a small number of the available metrics have significance for prediction
software build outcomes. It is observed that classification accuracy steadily
improves after approximately 900 instances of builds have been fed to the
classifier. At the end of the data streaming process classification accuracies
of 80% were achieved, though some bias arises due to the distribution of data
across the two classes over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2334</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2334</id><created>2014-07-08</created><authors><author><keyname>Valkonen</keyname><forenames>Tuomo</forenames></author></authors><title>The jump set under geometric regularisation. Part 2: Higher-order
  approaches</title><categories>math.FA cs.CV</categories><comments>Part 1: arXiv 1407.1531 [math.FA]</comments><msc-class>26B30, 49Q20, 65J20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part 1, we developed a new technique based on Lipschitz pushforwards for
proving the jump set containment property $\mathcal{H}^{m-1}(J_u \setminus
J_f)=0$ of solutions $u$ to total variation denoising. We demonstrated that the
technique also applies to Huber-regularised TV. Now, in this Part 2, we extend
the technique to higher-order regularisers. We are not quite able to prove the
property for total generalised variation (TGV) based on the symmetrised
gradient for the second-order term. We show that the property holds under three
conditions: First, the solution $u$ is locally bounded. Second, the
second-order variable is of locally bounded variation, $w \in
\mbox{BV}_\mbox{loc}(\Omega; \mathbb{R}^m)$, instead of just bounded
deformation, $w \in \mbox{BD}(\Omega)$. Third, $w$ does not jump on $J_u$
parallel to it. The second condition can be achieved for non-symmetric TGV.
Both the second and third condition can be achieved if we change the Radon (or
$L^1$) norm of the symmetrised gradient $Ew$ into an $L^p$ norm, $p&gt;1$, in
which case Korn's inequality holds. We also consider the application of the
technique to infimal convolution TV, and study the limiting behaviour of the
singular part of $D u$, as the second parameter of $\mbox{TGV}^2$ goes to zero.
Unsurprisingly, it vanishes, but in numerical discretisations the situation
looks quite different. Finally, our work additionally includes a result on
TGV-strict approximation in $\mbox{BV}(\Omega)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2337</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2337</id><created>2014-07-08</created><authors><author><keyname>Zhang</keyname><forenames>Hong</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Blaise</keyname><forenames>Sebastien</forenames></author></authors><title>High Order Implicit-Explicit General Linear Methods with Optimized
  Stability Regions</title><categories>cs.NA math.NA</categories><msc-class>65C20, 65M60, 86A10, 35L65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the numerical solution of partial differential equations using a
method-of-lines approach, the availability of high order spatial discretization
schemes motivates the development of sophisticated high order time integration
methods. For multiphysics problems with both stiff and non-stiff terms
implicit-explicit (IMEX) time stepping methods attempt to combine the lower
cost advantage of explicit schemes with the favorable stability properties of
implicit schemes. Existing high order IMEX Runge Kutta or linear multistep
methods, however, suffer from accuracy or stability reduction.
  This work shows that IMEX general linear methods (GLMs) are competitive
alternatives to classic IMEX schemes for large problems arising in practice.
High order IMEX-GLMs are constructed in the framework developed by the authors
[34]. The stability regions of the new schemes are optimized numerically. The
resulting IMEX-GLMs have similar stability properties as IMEX Runge-Kutta
methods, but they do not su?er from order reduction, and are superior in terms
of accuracy and efficiency. Numerical experiments with two and three
dimensional test problems illustrate the potential of the new schemes to speed
up complex applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2343</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2343</id><created>2014-07-08</created><authors><author><keyname>Chaudhury</keyname><forenames>Kunal Narayan</forenames></author></authors><title>PatchLift: Fast and Exact Computation of Patch Distances using Lifting,
  with Applications to Non-Local Means</title><categories>cs.CV</categories><comments>9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a fast algorithm called PatchLift for computing
distances between patches extracted from a one-dimensional signal. PatchLift is
based on the observation that the patch distances can be expressed in terms of
simple moving sums of an image, which is derived from the one-dimensional
signal via lifting. We apply PatchLift to develop a separable extension of the
classical Non-Local Means (NLM) algorithm which is at least 100 times faster
than NLM for standard parameter settings. The PSNR obtained using the proposed
extension is typically close to (and often larger than) the PSNRs obtained
using the original NLM. We provide some simulations results to demonstrate the
acceleration achieved using separability and PatchLift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2351</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2351</id><created>2014-07-09</created><updated>2015-02-15</updated><authors><author><keyname>Lima</keyname><forenames>Markus V. S.</forenames></author><author><keyname>Martins</keyname><forenames>Wallace A.</forenames></author><author><keyname>Nunes</keyname><forenames>Leonardo O.</forenames></author><author><keyname>Biscainho</keyname><forenames>Luiz W. P.</forenames></author><author><keyname>Ferreira</keyname><forenames>Tadeu N.</forenames></author><author><keyname>Costa</keyname><forenames>Maur&#xed;cio V. M.</forenames></author><author><keyname>Lee</keyname><forenames>Bowon</forenames></author></authors><title>Efficient Steered-Response Power Methods for Sound Source Localization
  Using Microphone Arrays</title><categories>cs.SD</categories><comments>14 pages, 9 figures, 5 tables</comments><journal-ref>IEEE Signal Processing Letters (Volume:22 , Issue: 8 ), Aug. 2015</journal-ref><doi>10.1109/LSP.2014.2385864</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient method based on the steered-response power
(SRP) technique for sound source localization using microphone arrays: the
volumetric SRP (V-SRP). As compared to the SRP, by deploying a sparser
volumetric grid, the V-SRP achieves a significant reduction of the
computational complexity without sacrificing the accuracy of the location
estimates. By appending a fine search step to the V-SRP, its refined version
(RV-SRP) improves on the compromise between complexity and accuracy.
Experiments conducted in both simulated- and real-data scenarios demonstrate
the benefits of the proposed approaches. Specifically, the RV-SRP is shown to
outperform the SRP in accuracy at a computational cost of about ten times
lower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2356</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2356</id><created>2014-07-09</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Wang</keyname><forenames>Xiumin</forenames></author></authors><title>Statistical Precoder Design for Space-Time-Frequency Block Codes in
  Multiuser MISO-MC-CDMA Systems</title><categories>cs.IT math.IT</categories><comments>10 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a space-time-frequency joint block coding (STFBC)
scheme to exploit the essential space-time-frequency degrees of freedom of
multiuser MISO-MC-CDMA systems. Specifically, we use a series of orthogonal
random codes to spread the space time code over several sub-carriers to obtain
multi-diversity gains, while multiuser parallel transmission is applied over
the same sub-carriers by making use of multiple orthogonal code channels.
Furthermore, to improve the system performance, we put forward to linear
precoding to the predetermined orthogonal STFBC, including transmitting
directions selection and power allocation over these directions. We propose a
precoder design method by making use of channel statistical information in time
domain based on the Kronecker correlation model for the channels, so feedback
amount can be decreased largely in multi-carrier systems. In addition, we give
the performance analysis from the perspectives of diversity order and coding
gain, respectively. Moreover, through asymptotic analysis, we derive some
simple precoder design methods, while guaranteeing a good performance. Finally,
numerical results validate our theoretical claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2357</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2357</id><created>2014-07-09</created><authors><author><keyname>Lopes</keyname><forenames>Minal</forenames></author><author><keyname>Sarwade</keyname><forenames>Nisha</forenames></author></authors><title>Cryptography from Quantum mechanical viewpoint</title><categories>cs.CR</categories><comments>International Journal on Cryptography and Information Security
  (IJCIS), Vol. 4, No. 2, June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptography is an art and science of secure communication. Here the sender
and receiver are guaranteed the security through encryption of their data, with
the help of a common key. Both the parties should agree on this key prior to
communication. The cryptographic systems which perform these tasks are designed
to keep the key secret while assuming that the algorithm used for encryption
and decryption is public. Thus key exchange is a very sensitive issue. In
modern cryptographic algorithms this security is based on the mathematical
complexity of the algorithm. But quantum computation is expected to
revolutionize computing paradigm in near future. This presents a challenge
amongst the researchers to develop new cryptographic techniques that can
survive the quantum computing era. This paper reviews the radical use of
quantum mechanics for cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2358</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2358</id><created>2014-07-09</created><authors><author><keyname>Chakraverty</keyname><forenames>Mayank</forenames></author></authors><title>A Compact Model of Silicon-Based Nanowire Field Effect Transistor for
  Circuit Simulation and Design</title><categories>cs.ET cond-mat.mes-hall cond-mat.mtrl-sci cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the conventional silicon metal-oxide-semiconductor field-effect transistor
(MOSFET) approaches its scaling limits; many novel device structures are being
extensively explored. Among them, the silicon nanowire transistor (SNWT) has
attracted broad attention. To understand device physics in depth and to assess
the performance limits of SNWTs, simulation is becoming increasingly important.
The objectives of this work are: 1) to theoretically explore the essential
physics of SNWTs (e.g., electrostatics, transport and band structure) by
performing computer-based simulations, and 2) to assess the performance limits
and scaling potentials of SNWTs and to address the SNWT design issues. The
computer based simulations carried out are essentially based on DFT using NEGF
formalism. A silicon nanowire has been modeled as PN diode (Zener Diode), PIN
diode, PIP &amp; NIN diode configurations by selectively doping the nanowire and
simulated by biasing one end of the nanowire to ground and sweeping the other
end of the nanowire from -1 V to 1 V to obtain the electrical characteristics
of the respective diodes. In order to determine the effectiveness of the
modeled diodes in silicon nanowire, the same diodes have been modeled using a
germanium nanowire by selective doping and simulated in the same manner to
obtain the electrical characteristics of the germanium nanowire based diodes
which has been used as a reference to analyze the characteristics obtained
using silicon nanowire. The modeled diodes are extremely small in dimension
when compared to the conventional bulk silicon and germanium based diodes. This
work is followed by modeling and simulation of a gate all around nanowire field
effect transistor using two different gate dielectrics, followed by temperature
dependence of the nanowire FET characteristics and the off state current and
conductance variation using the two dielectrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2377</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2377</id><created>2014-07-09</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Nesic</keyname><forenames>Dragan</forenames></author></authors><title>Hands-Off Control as Green Control</title><categories>cs.SY math.OC</categories><comments>SICE Control Division Multi Symposium 2014 (Japanese domestic
  conference); English translation from Japanese article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we introduce a new paradigm of control, called hands-off
control, which can save energy and reduce CO2 emissions in control systems. A
hands-off control is defined as a control that has a much shorter support than
the horizon length. The maximum hands-off control is the minimum support (or
sparsest) control among all admissible controls. With maximum hands-off
control, actuators in the feedback control system can be stopped during time
intervals over which the control values are zero. We show the maximum hands-off
control is given by L1 optimal control, for which we also show numerical
computation formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2379</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2379</id><created>2014-07-09</created><authors><author><keyname>Zeinolabedin</keyname><forenames>Narges</forenames></author><author><keyname>Mehrvarz</keyname><forenames>Soroush Afiati</forenames></author><author><keyname>Rahbar</keyname><forenames>Neda</forenames></author></authors><title>How COBIT Can Complement ITIL TO Achieve BIT</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Strategic alignment is a conviction that is considered extremely important in
understanding how organizations can apply their arrangement of information
technology (IT) into substantial boosts in achievement. To attain alignment
advantage, Information Technology Infrastructure Library (ITIL) prepares a
framework of best practice approach for IT Service Management in all countries
and Control Objectives for Information and Related Technology (COBIT) is an IT
governance framework and aiding toolset that permits managers to stretch the
gap between control prerequisites, technical matters and business risks. The
purpose of this paper is to recognize how COBIT can complement ITIL to attain
Business-IT Alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2390</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2390</id><created>2014-07-09</created><authors><author><keyname>Prasanna</keyname><forenames>SRM</forenames></author><author><keyname>Devi</keyname><forenames>Rituparna</forenames></author><author><keyname>Das</keyname><forenames>Deepjoy</forenames></author><author><keyname>Ghosh</keyname><forenames>Subhankar</forenames></author><author><keyname>Naik</keyname><forenames>Krishna</forenames></author></authors><title>Online Stroke and Akshara Recognition GUI in Assamese Language Using
  Hidden Markov Model</title><categories>cs.CV</categories><comments>6 pages, 9 figures, International Journal of Scientific and Research
  Publications, Volume 4, Issue 1, January 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work describes the development of Online Assamese Stroke &amp; Akshara
Recognizer based on a set of language rules. In handwriting literature strokes
are composed of two coordinate trace in between pen down and pen up labels. The
Assamese aksharas are combination of a number of strokes, the maximum number of
strokes taken to make a combination being eight. Based on these combinations
eight language rule models have been made which are used to test if a set of
strokes form a valid akshara. A Hidden Markov Model is used to train 181
different stroke patterns which generates a model used during stroke level
testing. Akshara level testing is performed by integrating a GUI (provided by
CDAC-Pune) with the Binaries of HTK toolkit classifier, HMM train model and the
language rules using a dynamic linked library (dll). We have got a stroke level
performance of 94.14% and akshara level performance of 84.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2391</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2391</id><created>2014-07-09</created><authors><author><keyname>Kalokidou</keyname><forenames>Vaia</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Piechocki</keyname><forenames>Robert</forenames></author></authors><title>Blind Interference Alignment in General Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, accepted to IEEE PIMRC'14</comments><journal-ref>Proceedings of IEEE PIMRC 2014, pages 816-820</journal-ref><doi>10.1109/PIMRC.2014.7136277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks have a key role in the design of future mobile
communication networks, since the employment of small cells around a macrocell
enhances the network's efficiency and decreases complexity and power demand.
Moreover, research on Blind Interference Alignment (BIA) has shown that optimal
Degrees of Freedom (DoF) can be achieved in certain network architectures, with
no requirement of Channel State Information (CSI) at the transmitters. Our
contribution is a generalised model of BIA in a heterogeneous network with one
macrocell with K users and K femtocells each with one user, by using Kronecker
(Tensor) Product representation. We introduce a solution on how to vary
beamforming vectors under power constraints to maximize the sum rate of the
network and how optimal DoF can be achieved over K+1 time slots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2394</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2394</id><created>2014-07-09</created><authors><author><keyname>Takemoto</keyname><forenames>Kazushi</forenames></author><author><keyname>Matsuda</keyname><forenames>Takahiro</forenames></author><author><keyname>Hara</keyname><forenames>Shinsuke</forenames></author><author><keyname>Takizawa</keyname><forenames>Kenichi</forenames></author><author><keyname>Ono</keyname><forenames>Fumie</forenames></author><author><keyname>Miura</keyname><forenames>Ryu</forenames></author></authors><title>Multi-Dimensional Wireless Tomography with Tensor-Based Compressed
  Sensing</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages, 14 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless tomography is a technique for inferring a physical environment
within a monitored region by analyzing RF signals traversed across the region.
In this paper, we consider wireless tomography in a two and higher
dimensionally structured monitored region, and propose a multi-dimensional
wireless tomography scheme based on compressed sensing to estimate a spatial
distribution of shadowing loss in the monitored region. In order to estimate
the spatial distribution, we consider two compressed sensing frameworks:
vector-based compressed sensing and tensor-based compressed sensing. When the
shadowing loss has a high spatial correlation in the monitored region, the
spatial distribution has a sparsity in its frequency domain. Existing wireless
tomography schemes are based on the vector-based compressed sensing and
estimates the distribution by utilizing the sparsity. On the other hand, the
proposed scheme is based on the tensor-based compressed sensing, which
estimates the distribution by utilizing its low-rank property. We reveal that
the tensor-based compressed sensing has a potential for highly accurate
estimation as compared with the vector-based compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2396</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2396</id><created>2014-07-09</created><updated>2015-05-13</updated><authors><author><keyname>Wang</keyname><forenames>Zhenghuan</forenames></author><author><keyname>Liu</keyname><forenames>Heng</forenames></author><author><keyname>Xu</keyname><forenames>Shengxin</forenames></author><author><keyname>Bu</keyname><forenames>Xiangyuan</forenames></author><author><keyname>An</keyname><forenames>Jianping</forenames></author></authors><title>Device-free Localization using Received Signal Strength Measurements in
  Radio Frequency Network</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to some mistakes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-free localization (DFL) based on the received signal strength (RSS)
measurements of radio frequency (RF)links is the method using RSS variation due
to the presence of the target to localize the target without attaching any
device. The majority of DFL methods utilize the fact the link will experience
great attenuation when obstructed. Thus that localization accuracy depends on
the model which describes the relationship between RSS loss caused by
obstruction and the position of the target. The existing models is too rough to
explain some phenomenon observed in the experiment measurements. In this paper,
we propose a new model based on diffraction theory in which the target is
modeled as a cylinder instead of a point mass. The proposed model can will
greatly fits the experiment measurements and well explain the cases like link
crossing and walking along the link line. Because the measurement model is
nonlinear, particle filtering tracing is used to recursively give the
approximate Bayesian estimation of the position. The posterior Cramer-Rao lower
bound (PCRLB) of proposed tracking method is also derived. The results of field
experiments with 8 radio sensors and a monitored area of 3.5m 3.5m show that
the tracking error of proposed model is improved by at least 36 percent in the
single target case and 25 percent in the two targets case compared to other
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2407</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2407</id><created>2014-07-09</created><authors><author><keyname>Paveti&#x107;</keyname><forenames>Filip</forenames></author><author><keyname>&#x17d;u&#x17e;i&#x107;</keyname><forenames>Goran</forenames></author><author><keyname>&#x160;iki&#x107;</keyname><forenames>Mile</forenames></author></authors><title>$LCSk$++: Practical similarity metric for long strings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present $LCSk$++: a new metric for measuring the similarity
of long strings, and provide an algorithm for its efficient computation. With
ever increasing size of strings occuring in practice, e.g. large genomes of
plants and animals, classic algorithms such as Longest Common Subsequence (LCS)
fail due to demanding computational complexity. Recently, Benson et al. defined
a similarity metric named $LCSk$. By relaxing the requirement that the
$k$-length substrings should not overlap, we extend their definition into a new
metric. An efficient algorithm is presented which computes $LCSk$++ with
complexity of $O((|X|+|Y|)\log(|X|+|Y|))$ for strings $X$ and $Y$ under a
realistic random model. The algorithm has been designed with implementation
simplicity in mind. Additionally, we describe how it can be adjusted to compute
$LCSk$ as well, which gives an improvement of the $O(|X|\dot|Y|)$ algorithm
presented in the original $LCSk$ paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2412</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2412</id><created>2014-07-09</created><authors><author><keyname>Gulhane</keyname><forenames>Monali</forenames></author><author><keyname>Mohod</keyname><forenames>P. S.</forenames></author></authors><title>Intelligent Fatigue Detection and Automatic Vehicle Control System</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes method for detecting the early signs of fatigue in train
drivers. As soon as the train driver is falling in symptoms of fatigue
immediate message will be transfer to the control room indicating the status of
the drivers. In addition of the advance technology of heart rate sensors is
also added in the system for correct detection of status of driver if in either
case driver is falling to fatigue due to any sever medical problems .The
fatigue is detected in the system by the image processing method of comparing
the image(frames) in the video and by using the human features we are able to
estimate the indirect way of detecting fatigue. The technique also focuses on
modes of person when driving the train i.e. awake, drowsy state or sleepy and
sleep state. The system is very efficient to detect the fatigue and control the
train also train can be controlled if it cross any such signal by which the
train may collide on another train.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2415</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2415</id><created>2014-07-09</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>FIR Digital Filter Design by Sampled-Data H-infinity Discretization</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>The 19th IFAC World Congress; 6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FIR (finite impulse response) digital filter design is a fundamental problem
in signal processing. In particular, FIR approximation of analog filters (or
systems) is ubiquitous not only in signal processing but also in digital
implementation of controllers. In this article, we propose a new design method
of an FIR digital filter that optimally approximates a given analog filter in
the sense of minimizing the H-infinity norm of the sampled-data error system.
By using the lifting technique and the KYP (Kalman-Yakubovich-Popov) lemma, we
reduce the H-infinity optimization to a convex optimization described by an LMI
(linear matrix inequality). We also extend the method to multi-rate and
multi-delay systems. A design example is shown to illustrate the effectiveness
of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2416</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2416</id><created>2014-07-09</created><updated>2015-01-25</updated><authors><author><keyname>Guo</keyname><forenames>Jin-Li</forenames></author><author><keyname>Suo</keyname><forenames>Qi</forenames></author></authors><title>Brand effect versus competitiveness in hypernetworks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>11 pages, 5 figures, Chaos: An Interdisciplinary Journal of Nonlinear
  Science, 2015</comments><journal-ref>CHAOS 25, 023102 (2015)</journal-ref><doi>10.1063/1.4907016</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A few of evolving models in hypernetworks have been proposed based on uniform
growth. In order to better depict the growth mechanism and competitive aspect
of real hypernetworks, we propose a model in term of the non-uniform growth.
Besides hyperdegrees, the other two important factors are introduced to
underlie preferential attachment. One dimension is the brand effect and the
other is the competitiveness. Our model can accurately describe the evolution
of real hypernetworks. The paper analyzes the model and calculates the
stationary average hyperdegree distribution of the hypernetwork by using
Poisson process theory and a continuous technique. We also address the limit in
which this model has a condensation. The theoretical analyses agree with
numerical simulations. Our model is universal, in that the standard
preferential attachment, the fitness model in complex networks and scale-free
model in hypernetworks can all be seen as degenerate cases of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2417</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2417</id><created>2014-07-09</created><updated>2015-07-31</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Strong Converse Theorems for Classes of Multimessage Multicast Networks:
  A R\'enyi Divergence Approach</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, Jul 18, 2014.
  Revised on Jul 31, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes that the strong converse holds for some classes of
discrete memoryless multimessage multicast networks (DM-MMNs) whose
corresponding cut-set bounds are tight, i.e., coincide with the set of
achievable rate tuples. The strong converse for these classes of DM-MMNs
implies that all sequences of codes with rate tuples belonging to the exterior
of the cut-set bound have average error probabilities that necessarily tend to
one (and are not simply bounded away from zero). Examples in the classes of
DM-MMNs include wireless erasure networks, DM-MMNs consisting of independent
discrete memoryless channels (DMCs) as well as single-destination DM-MMNs
consisting of independent DMCs with destination feedback. Our elementary proof
technique leverages properties of the R\'enyi divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2421</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2421</id><created>2014-07-09</created><authors><author><keyname>Luhach</keyname><forenames>Ashish Kr.</forenames></author><author><keyname>Dwivedi</keyname><forenames>Sanjay K</forenames></author><author><keyname>Jha</keyname><forenames>C K</forenames></author></authors><title>Designing and implementing the logical security framework for e-commerce
  based on service oriented architecture</title><categories>cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid evolution of information technology has contributed to the evolution of
more sophisticated E- commerce system with the better transaction time and
protection. The currently used E-commerce models lack in quality properties
such as logical security because of their poor designing and to face the highly
equipped and trained intruders. This editorial proposed a security framework
for small and medium sized E-commerce, based on service oriented architecture
and gives an analysis of the eminent security attacks which can be averted. The
proposed security framework will be implemented and validated on an open source
E-commerce, and the results achieved so far are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2423</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2423</id><created>2014-07-09</created><authors><author><keyname>Luhach</keyname><forenames>Ashish Kr.</forenames></author><author><keyname>Dwivedi</keyname><forenames>Sanjay K.</forenames></author><author><keyname>Jha</keyname><forenames>C. K.</forenames></author></authors><title>Desiging a logical security framework for e-commerce system based on soa</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid increases in information technology also changed the existing markets
and transformed them into e- markets (e-commerce) from physical markets.
Equally with the e-commerce evolution, enterprises have to recover a safer
approach for implementing E-commerce and maintaining its logical security. SOA
is one of the best techniques to fulfill these requirements. SOA holds the
vantage of being easy to use, flexible, and recyclable. With the advantages,
SOA is also endowed with ease for message tampering and unauthorized access.
This causes the security technology implementation of E-commerce very difficult
at other engineering sciences. This paper discusses the importance of using SOA
in E-commerce and identifies the flaws in the existing security analysis of
E-commerce platforms. On the foundation of identifying defects, this editorial
also suggested an implementation design of the logical security framework for
SOA supported E-commerce system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2433</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2433</id><created>2014-07-09</created><updated>2015-05-17</updated><authors><author><keyname>Foster</keyname><forenames>Peter</forenames></author><author><keyname>Dixon</keyname><forenames>Simon</forenames></author><author><keyname>Klapuri</keyname><forenames>Anssi</forenames></author></authors><title>Identifying Cover Songs Using Information-Theoretic Measures of
  Similarity</title><categories>cs.IR cs.LG stat.ML</categories><comments>13 pages, 5 figures, 4 tables. v3: Accepted version</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  vol. 23 no. 6, pp. 993-1005, 2015</journal-ref><doi>10.1109/TASLP.2015.2416655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates methods for quantifying similarity between audio
signals, specifically for the task of of cover song detection. We consider an
information-theoretic approach, where we compute pairwise measures of
predictability between time series. We compare discrete-valued approaches
operating on quantised audio features, to continuous-valued approaches. In the
discrete case, we propose a method for computing the normalised compression
distance, where we account for correlation between time series. In the
continuous case, we propose to compute information-based measures of similarity
as statistics of the prediction error between time series. We evaluate our
methods on two cover song identification tasks using a data set comprised of
300 Jazz standards and using the Million Song Dataset. For both datasets, we
observe that continuous-valued approaches outperform discrete-valued
approaches. We consider approaches to estimating the normalised compression
distance (NCD) based on string compression and prediction, where we observe
that our proposed normalised compression distance with alignment (NCDA)
improves average performance over NCD, for sequential compression algorithms.
Finally, we demonstrate that continuous-valued distances may be combined to
improve performance with respect to baseline approaches. Using a large-scale
filter-and-refine approach, we demonstrate state-of-the-art performance for
cover song identification using the Million Song Dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2437</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2437</id><created>2014-07-09</created><authors><author><keyname>Alowayr</keyname><forenames>Ali</forenames></author><author><keyname>Badii</keyname><forenames>Atta</forenames></author></authors><title>Review of monitoring tools for e-learning platforms</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of e-learning technologies has made it viable for
developments in education and technology to be combined in order to fulfil
educational needs worldwide. E-learning consists of informal learning
approaches and emerging technologies to support the delivery of learning
skills, materials, collaboration and knowledge sharing. E-learning is a
holistic approach that covers a wide range of courses, technologies and
infrastructures to provide an effective learning environment. The Learning
Management System (LMS) is the core of the entire e-learning process along with
technology, content, and services. This paper investigates the role of
model-driven personalisation support modalities in providing enhanced levels of
learning and trusted assimilation in an e-learning delivery context. We present
an analysis of the impact of an integrated learning path that an e-learning
system may employ to track activities and evaluate the performance of learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2461</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2461</id><created>2014-07-09</created><authors><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio M.</forenames></author><author><keyname>Genuzio</keyname><forenames>Marco</forenames></author></authors><title>Eulerian digraphs and Dyck words, a bijection</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this work is to establish a bijection between Dyck words and
a family of Eulerian digraphs. We do so by providing two algorithms
implementing such bijection in both directions. The connection between Dyck
words and Eulerian digraphs exploits a novel combinatorial structure: a binary
matrix, we call Dyck matrix, representing the cycles of an Eulerian digraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2479</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2479</id><created>2014-07-09</created><updated>2015-02-11</updated><authors><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Making the Most of Your Samples</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of setting a price for a potential buyer with a
valuation drawn from an unknown distribution $D$. The seller has &quot;data&quot;' about
$D$ in the form of $m \ge 1$ i.i.d. samples, and the algorithmic challenge is
to use these samples to obtain expected revenue as close as possible to what
could be achieved with advance knowledge of $D$.
  Our first set of results quantifies the number of samples $m$ that are
necessary and sufficient to obtain a $(1-\epsilon)$-approximation. For example,
for an unknown distribution that satisfies the monotone hazard rate (MHR)
condition, we prove that $\tilde{\Theta}(\epsilon^{-3/2})$ samples are
necessary and sufficient. Remarkably, this is fewer samples than is necessary
to accurately estimate the expected revenue obtained by even a single reserve
price. We also prove essentially tight sample complexity bounds for regular
distributions, bounded-support distributions, and a wide class of irregular
distributions. Our lower bound approach borrows tools from differential privacy
and information theory, and we believe it could find further applications in
auction theory.
  Our second set of results considers the single-sample case. For regular
distributions, we prove that no pricing strategy is better than
$\tfrac{1}{2}$-approximate, and this is optimal by the Bulow-Klemperer theorem.
For MHR distributions, we show how to do better: we give a simple pricing
strategy that guarantees expected revenue at least $0.589$ times the maximum
possible. We also prove that no pricing strategy achieves an approximation
guarantee better than $\frac{e}{4} \approx .68$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2482</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2482</id><created>2014-07-09</created><authors><author><keyname>Dyachkov</keyname><forenames>A. G.</forenames></author><author><keyname>Vorobyev</keyname><forenames>I. V.</forenames></author><author><keyname>Polyanskii</keyname><forenames>N. A.</forenames></author><author><keyname>Shchukin</keyname><forenames>V. Yu.</forenames></author></authors><title>Almost Disjunctive List-Decoding Codes</title><categories>cs.IT math.IT</categories><comments>17 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary code is said to be a disjunctive list-decoding $s_L$-code, $s\ge1$,
$L\ge1$, (briefly, LD $s_L$-code) if the code is identified by the incidence
matrix of a family of finite sets in which the union of any $s$ sets can cover
not more than $L-1$ other sets of the family. In this paper, we introduce a
natural {\em probabilistic} generalization of LD $s_L$-code when the code is
said to be an almost disjunctive LD $s_L$-code if the unions of {\em almost
all} $s$ sets satisfy the given condition. We develop a random coding method
based on the ensemble of binary constant-weight codes to obtain lower bounds on
the capacity and error probability exponent of such codes. For the considered
ensemble our lower bounds are asymptotically tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2483</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2483</id><created>2014-07-09</created><updated>2014-07-12</updated><authors><author><keyname>Visweswaran</keyname><forenames>Shyam</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>Counting Markov Blanket Structures</title><categories>stat.ML cs.AI cs.LG</categories><comments>5 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Learning Markov blanket (MB) structures has proven useful in performing
feature selection, learning Bayesian networks (BNs), and discovering causal
relationships. We present a formula for efficiently determining the number of
MB structures given a target variable and a set of other variables. As
expected, the number of MB structures grows exponentially. However, we show
quantitatively that there are many fewer MB structures that contain the target
variable than there are BN structures that contain it. In particular, the ratio
of BN structures to MB structures appears to increase exponentially in the
number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2487</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2487</id><created>2014-07-09</created><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Maceli</keyname><forenames>Peter</forenames></author><author><keyname>Stacho</keyname><forenames>Juraj</forenames></author><author><keyname>Zhong</keyname><forenames>Mingxian</forenames></author></authors><title>4-coloring $P_6$-free graphs with no induced 5-cycles</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the 4-coloring problem can be solved in polynomial time for
graphs with no induced 5-cycle $C_5$ and no induced 6-vertex path $P_6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2490</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2490</id><created>2014-07-09</created><updated>2015-03-26</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>On Gridless Sparse Methods for Line Spectral Estimation From Complete
  and Incomplete Data</title><categories>cs.IT math.IT stat.ML</categories><comments>15 pages, double-column, 7 figures, accepted by IEEE Transaction on
  Signal Processing in March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned about sparse, continuous frequency estimation in line
spectral estimation, and focused on developing gridless sparse methods which
overcome grid mismatches and correspond to limiting scenarios of existing
grid-based approaches, e.g., $\ell_1$ optimization and SPICE, with an
infinitely dense grid. We generalize AST (atomic-norm soft thresholding) to the
case of nonconsecutively sampled data (incomplete data) inspired by recent
atomic norm based techniques. We present a gridless version of SPICE (gridless
SPICE, or GLS), which is applicable to both complete and incomplete data
without the knowledge of noise level. We further prove the equivalence between
GLS and atomic norm-based techniques under different assumptions of noise.
Moreover, we extend GLS to a systematic framework consisting of model order
selection and robust frequency estimation, and present feasible algorithms for
AST and GLS. Numerical simulations are provided to validate our theoretical
analysis and demonstrate performance of our methods compared to existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2506</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2506</id><created>2014-07-09</created><updated>2015-09-18</updated><authors><author><keyname>Xu</keyname><forenames>Ming</forenames></author><author><keyname>Wu</keyname><forenames>Jianping</forenames></author><author><keyname>Du</keyname><forenames>Yiman</forenames></author><author><keyname>Wang</keyname><forenames>Haohan</forenames></author><author><keyname>Qi</keyname><forenames>Geqi</forenames></author><author><keyname>Hu</keyname><forenames>Kezhen</forenames></author><author><keyname>Xiao</keyname><forenames>Yunpeng</forenames></author></authors><title>Discovery of Important Crossroads in Road Network using Massive Taxi
  Trajectories</title><categories>cs.AI cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major problem in road network analysis is discovery of important
crossroads, which can provide useful information for transport planning.
However, none of existing approaches addresses the problem of identifying
network-wide important crossroads in real road network. In this paper, we
propose a novel data-driven based approach named CRRank to rank important
crossroads. Our key innovation is that we model the trip network reflecting
real travel demands with a tripartite graph, instead of solely analysis on the
topology of road network. To compute the importance scores of crossroads
accurately, we propose a HITS-like ranking algorithm, in which a procedure of
score propagation on our tripartite graph is performed. We conduct experiments
on CRRank using a real-world dataset of taxi trajectories. Experiments verify
the utility of CRRank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2510</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2510</id><created>2014-05-15</created><authors><author><keyname>Bras</keyname><forenames>Ronan Le</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>On the Erdos Discrepancy Problem</title><categories>cs.DM</categories><comments>8 pages; 0 figure; Submitted on April 14, 2014 to the 20th
  International Conference on Principles and Practice of Constraint Programming</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the Erd\H{o}s discrepancy conjecture, for any infinite $\pm 1$
sequence, there exists a homogeneous arithmetic progression of unbounded
discrepancy. In other words, for any $\pm 1$ sequence $(x_1,x_2,...)$ and a
discrepancy $C$, there exist integers $m$ and $d$ such that $|\sum_{i=1}^m x_{i
\cdot d}| &gt; C$. This is an $80$-year-old open problem and recent development
proved that this conjecture is true for discrepancies up to $2$. Paul Erd\H{o}s
also conjectured that this property of unbounded discrepancy even holds for the
restricted case of completely multiplicative sequences (CMSs), namely sequences
$(x_1,x_2,...)$ where $x_{a \cdot b} = x_{a} \cdot x_{b}$ for any $a,b \geq 1$.
The longest CMS with discrepancy $2$ has been proven to be of size $246$. In
this paper, we prove that any completely multiplicative sequence of size
$127,646$ or more has discrepancy at least $4$, proving the Erd\H{o}s
discrepancy conjecture for CMSs of discrepancies up to $3$. In addition, we
prove that this bound is tight and increases the size of the longest known
sequence of discrepancy $3$ from $17,000$ to $127,645$. Finally, we provide
inductive construction rules as well as streamlining methods to improve the
lower bounds for sequences of higher discrepancies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2511</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2511</id><created>2014-05-16</created><updated>2015-03-09</updated><authors><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames><affiliation>INRIA Paris-Rocquencourt, LIAFA</affiliation></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>INRIA Paris-Rocquencourt, LIAFA</affiliation></author><author><keyname>Pajak</keyname><forenames>Dominik</forenames></author></authors><title>Distinguishing Views in Symmetric Networks: A Tight Lower Bound</title><categories>cs.DM cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The view of a node in a port-labeled network is an infinite tree encoding all
walks in the network originating from this node. We prove that for any integers
$n\geq D\geq 1$, there exists a port-labeled network with at most $n$ nodes and
diameter at most $D$ which contains a pair of nodes whose (infinite) views are
different, but whose views truncated to depth $\Omega(D\log (n/D))$ are
identical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2515</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2515</id><created>2014-07-09</created><updated>2015-03-14</updated><authors><author><keyname>Tabourier</keyname><forenames>Lionel</forenames></author><author><keyname>Bernardes</keyname><forenames>Daniel Faria</forenames></author><author><keyname>Libert</keyname><forenames>Anne-Sophie</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>RankMerging: A supervised learning-to-rank framework to predict links in
  large social network</title><categories>cs.SI cs.IR cs.LG physics.soc-ph</categories><comments>29 pages, short version published in the Proceedings of DyNakII
  (PKDD'14 workshop), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncovering unknown or missing links in social networks is a difficult task
because of their sparsity and because links may represent different types of
relationships, characterized by different structural patterns. In this paper,
we define a simple yet efficient supervised learning-to-rank framework, called
RankMerging, which aims at combining information provided by various
unsupervised rankings. We illustrate our method on three different kinds of
social networks and show that it substantially improves the performances of
unsupervised metrics of ranking. We also compare it to other combination
strategies based on standard methods. Finally, we explore various aspects of
RankMerging, such as feature selection and parameter estimation and discuss its
area of relevance: the prediction of an adjustable number of links on large
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2518</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2518</id><created>2014-07-09</created><authors><author><keyname>Barton</keyname><forenames>Richard J.</forenames></author></authors><title>Improved Estimation of the Spectral Efficiency Versus Energy-Per-Bit
  Tradeoff in the Wideband Regime</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new lower bound on spectral efficiency in the low-power
wideband regime is derived and utilized to develop an improved estimate of the
behavior of spectral efficiency above but in the vicinity of the Shannon limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2521</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2521</id><created>2014-07-08</created><authors><author><keyname>Wood</keyname><forenames>Lloyd</forenames></author><author><keyname>Lou</keyname><forenames>Yuxuan</forenames></author><author><keyname>Olusola</keyname><forenames>Opeoluwa</forenames></author></authors><title>Revisiting elliptical satellite orbits to enhance the O3b constellation</title><categories>astro-ph.IM cs.NI</categories><comments>Author postprint with colour figures</comments><journal-ref>Journal of the British Interplanetary Society, vol. 67, no. 3, pp.
  110-118, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an addition of known elliptical orbits to the new equatorial O3b
satellite constellation, extending O3b to cover high latitudes and the Earth's
poles. We simulate the O3b constellation and compare this to recent measurement
of the first real Internet traffic across the newly deployed O3b network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2523</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2523</id><created>2014-07-09</created><authors><author><keyname>Chambers</keyname><forenames>Erin Wolf</forenames></author><author><keyname>Letscher</keyname><forenames>David</forenames></author></authors><title>Persistent Homology Over Directed Acyclic Graphs</title><categories>cs.CG math.AT</categories><comments>15 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define persistent homology groups over any set of spaces which have
inclusions defined so that the underlying graph between the spaces is directed
and acyclic. This method simultaneously generalizes standard persistent
homology, zigzag persistence and multidimensional persistence to arbitrary
directed acyclic graphs, and it also allows the study of arbitrary families of
topological spaces or point-cloud data. We give an algorithm to compute the
persistent homology groups simultaneously for all subgraphs which contain a
single source and a single sink in $O(n^4)$ time, as well as an algorithm to
compute persistence for any arbitrary subgraph in the same running time. We
then demonstrate as an application of these tools a method to overlay two
distinct filtrations of the same underlying space, which allows us to calculate
significant barcodes using considerably fewer points than standard persistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2524</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2524</id><created>2014-07-09</created><authors><author><keyname>Newman</keyname><forenames>Alantha</forenames></author></authors><title>An improved analysis of the M\&quot;omke-Svensson algorithm for graph-TSP on
  subquartic graphs</title><categories>cs.DS</categories><comments>To appear in Proceedings of ESA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, M\&quot;omke and Svensson presented a beautiful new approach for the
traveling salesman problem on a graph metric (graph-TSP), which yielded a
$\frac{4}{3}$-approximation guarantee on subcubic graphs as well as a
substantial improvement over the $\frac{3}{2}$-approximation guarantee of
Christofides' algorithm on general graphs. The crux of their approach is to
compute an upper bound on the minimum cost of a circulation in a particular
network, $C(G,T)$, where $G$ is the input graph and $T$ is a carefully chosen
spanning tree. The cost of this circulation is directly related to the number
of edges in a tour output by their algorithm. Mucha subsequently improved the
analysis of the circulation cost, proving that M\&quot;omke and Svensson's algorithm
for graph-TSP has an approximation ratio of at most $\frac{13}{9}$ on general
graphs.
  This analysis of the circulation is local, and vertices with degree four and
five can contribute the most to its cost. Thus, hypothetically, there could
exist a subquartic graph (a graph with degree at most four at each vertex) for
which Mucha's analysis of the M\&quot;omke-Svensson algorithm is tight. We show that
this is not the case and that M\&quot;omke and Svensson's algorithm for graph-TSP
has an approximation guarantee of at most $\frac{46}{33}$ on subquartic graphs.
To prove this, we present a different method to upper bound the minimum cost of
a circulation on the network $C(G,T)$. Our approximation guarantee actually
holds for all graphs that have an optimal solution to a standard linear
programming relaxation of graph-TSP with subquartic support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2535</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2535</id><created>2014-07-09</created><authors><author><keyname>Lima</keyname><forenames>Antonio</forenames></author><author><keyname>Rossi</keyname><forenames>Luca</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Coding Together at Scale: GitHub as a Collaborative Social Network</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>10 pages, 12 figures, 1 table. In Proceedings of 8th AAAI
  International Conference on Weblogs and Social Media (ICWSM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GitHub is the most popular repository for open source code. It has more than
3.5 million users, as the company declared in April 2013, and more than 10
million repositories, as of December 2013. It has a publicly accessible API
and, since March 2012, it also publishes a stream of all the events occurring
on public projects. Interactions among GitHub users are of a complex nature and
take place in different forms. Developers create and fork repositories, push
code, approve code pushed by others, bookmark their favorite projects and
follow other developers to keep track of their activities.
  In this paper we present a characterization of GitHub, as both a social
network and a collaborative platform. To the best of our knowledge, this is the
first quantitative study about the interactions happening on GitHub. We analyze
the logs from the service over 18 months (between March 11, 2012 and September
11, 2013), describing 183.54 million events and we obtain information about
2.19 million users and 5.68 million repositories, both growing linearly in
time. We show that the distributions of the number of contributors per project,
watchers per project and followers per user show a power-law-like shape. We
analyze social ties and repository-mediated collaboration patterns, and we
observe a remarkably low level of reciprocity of the social connections. We
also measure the activity of each user in terms of authored events and we
observe that very active users do not necessarily have a large number of
followers. Finally, we provide a geographic characterization of the centers of
activity and we investigate how distance influences collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2537</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2537</id><created>2014-07-09</created><authors><author><keyname>Bluemlein</keyname><forenames>Johannes</forenames></author><author><keyname>De Freitas</keyname><forenames>Abilio</forenames></author><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Recent Symbolic Summation Methods to Solve Coupled Systems of
  Differential and Difference Equations</title><categories>cs.SC hep-ph math-ph math.MP</categories><report-no>DESY 14--122, DO-TH 14/13, SFB/CPP-14-039, LPN14-086</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline a new algorithm to solve coupled systems of differential equations
in one continuous variable $x$ (resp. coupled difference equations in one
discrete variable $N$) depending on a small parameter $\epsilon$: given such a
system and given sufficiently many initial values, we can determine the first
coefficients of the Laurent-series solutions in $\epsilon$ if they are
expressible in terms of indefinite nested sums and products. This systematic
approach is based on symbolic summation algorithms in the context of difference
rings/fields and uncoupling algorithms. The proposed method gives rise to new
interesting applications in connection with integration by parts (IBP) methods.
As an illustrative example, we will demonstrate how one can calculate the
$\epsilon$-expansion of a ladder graph with 6 massive fermion lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2538</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2538</id><created>2014-07-09</created><updated>2015-04-27</updated><authors><author><keyname>Chen</keyname><forenames>Liang-Chieh</forenames></author><author><keyname>Schwing</keyname><forenames>Alexander G.</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Learning Deep Structured Models</title><categories>cs.LG</categories><comments>11 pages including reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in real-world applications involve predicting several random
variables which are statistically related. Markov random fields (MRFs) are a
great mathematical tool to encode such relationships. The goal of this paper is
to combine MRFs with deep learning algorithms to estimate complex
representations while taking into account the dependencies between the output
random variables. Towards this goal, we propose a training algorithm that is
able to learn structured models jointly with deep features that form the MRF
potentials. Our approach is efficient as it blends learning and inference and
makes use of GPU acceleration. We demonstrate the effectiveness of our
algorithm in the tasks of predicting words from noisy images, as well as
multi-class classification of Flickr photographs. We show that joint learning
of the deep features and the MRF parameters results in significant performance
gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2541</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2541</id><created>2014-07-09</created><authors><author><keyname>Finlay</keyname><forenames>Jacqui</forenames></author><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author><author><keyname>Pears</keyname><forenames>Russel</forenames></author></authors><title>Mining Software Metrics from Jazz</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe the extraction of source code metrics from the
Jazz repository and the application of data mining techniques to identify the
most useful of those metrics for predicting the success or failure of an
attempt to construct a working instance of the software product. We present
results from a systematic study using the J48 classification method. The
results indicate that only a relatively small number of the available software
metrics that we considered have any significance for predicting the outcome of
a build. These significant metrics are discussed and implication of the results
discussed, particularly the relative difficulty of being able to predict failed
build attempts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2542</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2542</id><created>2014-07-09</created><authors><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>A Theorem on the Asymptotic Outage Behavior of Fixed-Gain
  Amplify-and-Forward Relay Systems</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure, accepted for publication in IEEE Communications
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theorem that describes the high signal-to-noise ratio (SNR) outage behavior
of fixed-gain amplify-and-forward (FGAF) relay systems is given. Qualitatively,
the theorem states that the outage probability decays according to a power law,
where the power is dictated by the poles of the moments of the underlying
per-hop fading distributions. The power law decay is dampened by a logarithmic
factor when the leading pole (furthest to the right in the plane) is of order
two or more. The theorem is easy to apply and several examples are presented to
this effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2544</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2544</id><created>2014-07-09</created><authors><author><keyname>Porta</keyname><forenames>Josep M.</forenames></author><author><keyname>Jaillet</keyname><forenames>L&#xe9;onard</forenames></author></authors><title>Sampling Strategies for Path Planning under Kinematic Constraints</title><categories>cs.RO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-known weakness of the probabilistic path planners is the so-called
narrow passage problem, where a region with a relatively low probability of
being sampled must be explored to find a solution path. Many strategies have
been proposed to alleviate this problem, most of them based on biasing the
sampling distribution. When kinematic constraints appear in the problem, the
configuration space typically becomes a non-parametrizable, implicit manifold.
Unfortunately, this invalidates most of the existing sampling bias approaches,
which rely on an explicit parametrization of the space to explore. In this
paper, we propose and evaluate three novel strategies to bias the sampling
under the presence of narrow passages in kinematically-constrained systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2565</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2565</id><created>2014-07-09</created><updated>2014-07-10</updated><authors><author><keyname>Becchetti</keyname><forenames>L.</forenames></author><author><keyname>Clementi</keyname><forenames>A.</forenames></author><author><keyname>Natale</keyname><forenames>E.</forenames></author><author><keyname>Pasquale</keyname><forenames>F.</forenames></author><author><keyname>Silvestri</keyname><forenames>R.</forenames></author></authors><title>Plurality Consensus in the Gossip Model</title><categories>cs.DC</categories><comments>Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Plurality Consensus in the Gossip Model over a network of $n$
anonymous agents. Each agent supports an initial opinion or color. We assume
that at the onset, the number of agents supporting the plurality color exceeds
that of the agents supporting any other color by a sufficiently-large bias. The
goal is to provide a protocol that, with high probability, brings the system
into the configuration in which all agents support the (initial) plurality
color. We consider the Undecided-State Dynamics, a well-known protocol which
uses just one more state (the undecided one) than those necessary to store
colors. We show that the speed of convergence of this protocol depends on the
initial color configuration as a whole, not just on the gap between the
plurality and the second largest color community. This dependence is best
captured by a novel notion we introduce, namely, the monochromatic distance
${md}(\bar{\mathbf{c}})$ which measures the distance of the initial color
configuration $\bar{ \mathbf {c}}$ from the closest monochromatic one. In the
complete graph, we prove that, for a wide range of the input parameters, this
dynamics converges within $O({md}(\bar {\mathbf {c}}) \log {n})$ rounds. We
prove that this upper bound is almost tight in the strong sense: Starting from
any color configuration $\bar {\mathbf {c}}$, the convergence time is
$\Omega({md}(\bar {\mathbf {c}}))$. Finally, we adapt the Undecided-State
Dynamics to obtain a fast, random walk-based protocol for plurality consensus
on regular expanders. This protocol converges in $O({md}(\bar {\mathbf {c}})
\mathrm{polylog}(n))$ rounds using only $\mathrm{polylog}(n)$ local memory. A
key-ingredient to achieve the above bounds is a new analysis of the maximum
node congestion that results from performing $n$ parallel random walks on
regular expanders. All our bounds hold with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2569</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2569</id><created>2014-07-09</created><updated>2015-02-18</updated><authors><author><keyname>Li</keyname><forenames>Jerry</forenames></author><author><keyname>Peebles</keyname><forenames>John</forenames></author></authors><title>Replacing Mark Bits with Randomness in Fibonacci Heaps</title><categories>cs.DS</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Fibonacci heap is a deterministic data structure implementing a priority
queue with optimal amortized operation costs. An unfortunate aspect of
Fibonacci heaps is that they must maintain a &quot;mark bit&quot; which serves only to
ensure efficiency of heap operations, not correctness. Karger proposed a simple
randomized variant of Fibonacci heaps in which mark bits are replaced by coin
flips. This variant still has expected amortized cost $O(1)$ for insert,
decrease-key, and merge. Karger conjectured that this data structure has
expected amortized cost $O(\log s)$ for delete-min, where $s$ is the number of
heap operations.
  We give a tight analysis of Karger's randomized Fibonacci heaps, resolving
Karger's conjecture. Specifically, we obtain matching upper and lower bounds of
$\Theta(\log^2 s / \log \log s)$ for the runtime of delete-min. We also prove a
tight lower bound of $\Omega(\sqrt{n})$ on delete-min in terms of the number of
heap elements $n$. The request sequence used to prove this bound also solves an
open problem of Fredman on whether cascading cuts are necessary. Finally, we
give a simple additional modification to these heaps which yields a tight
runtime $O(\log^2 n / \log \log n)$ for delete-min.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2572</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2572</id><created>2014-07-09</created><updated>2014-08-15</updated><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Azad</keyname><forenames>Babak</forenames></author><author><keyname>Mogharreb</keyname><forenames>Iraj</forenames></author><author><keyname>Jamali</keyname><forenames>Shahram</forenames></author></authors><title>Classifiers fusion method to recognize handwritten persian numerals</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 5 and 6, and some mistake in Table 1 information. please
  let me for changing this information and updating this paper</comments><journal-ref>International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 3,
  No. 3, June 2014</journal-ref><doi>10.5121/ijci.2014.3301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of Persian handwritten characters has been considered as a
significant field of research for the last few years under pattern analysing
technique. In this paper, a new approach for robust handwritten Persian
numerals recognition using strong feature set and a classifier fusion method is
scrutinized to increase the recognition percentage. For implementing the
classifier fusion technique, we have considered k nearest neighbour (KNN),
linear classifier (LC) and support vector machine (SVM) classifiers. The
innovation of this tactic is to attain better precision with few features using
classifier fusion method. For evaluation of the proposed method we considered a
Persian numerals database with 20,000 handwritten samples. Spending 15,000
samples for training stage, we verified our technique on other 5,000 samples,
and the correct recognition ratio achieved approximately 99.90%. Additional, we
got 99.97% exactness using four-fold cross validation procedure on 20,000
databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2575</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2575</id><created>2014-07-09</created><updated>2016-02-27</updated><authors><author><keyname>Pourmiri</keyname><forenames>Ali</forenames></author></authors><title>Balanced Allocation on Graphs: A Random Walk Approach</title><categories>cs.DS cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose algorithms for allocating $n$ sequential balls into
$n$ bins that are interconnected as a $d$-regular $n$-vertex graph $G$, where
$d\ge3$ can be any integer.Let $l$ be a given positive integer. In each round
$t$, $1\le t\le n$, ball $t$ picks a node of $G$ uniformly at random and
performs a non-backtracking random walk of length $l$ from the chosen node.Then
it allocates itself on one of the visited nodes with minimum load (ties are
broken uniformly at random). Suppose that $G$ has a sufficiently large girth
and $d=\omega(\log n)$. Then we establish an upper bound for the maximum number
of balls at any bin after allocating $n$ balls by the algorithm, called {\it
maximum load}, in terms of $l$ with high probability. We also show that the
upper bound is at most an $O(\log\log n)$ factor above the lower bound that is
proved for the algorithm. In particular, we show that if we set $l=\lfloor(\log
n)^{\frac{1+\epsilon}{2}}\rfloor$, for every constant $\epsilon\in (0, 1)$, and
$G$ has girth at least $\omega(l)$, then the maximum load attained by the
algorithm is bounded by $O(1/\epsilon)$ with high probability.Finally, we
slightly modify the algorithm to have similar results for balanced allocation
on $d$-regular graph with $d\in[3, O(\log n)]$ and sufficiently large girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2576</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2576</id><created>2014-07-09</created><authors><author><keyname>Kanoria</keyname><forenames>Yash</forenames></author><author><keyname>Saban</keyname><forenames>Daniela</forenames></author><author><keyname>Sethuraman</keyname><forenames>Jay</forenames></author></authors><title>The size of the core in assignment markets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assignment markets involve matching with transfers, as in labor markets and
housing markets. We consider a two-sided assignment market with agent types and
stochastic structure similar to models used in empirical studies, and
characterize the size of the core in such markets. Each agent has a randomly
drawn productivity with respect to each type of agent on the other side. The
value generated from a match between a pair of agents is the sum of the two
productivity terms, each of which depends only on the type but not the identity
of one of the agents, and a third deterministic term driven by the pair of
types. We allow the number of agents to grow, keeping the number of agent types
fixed. Let $n$ be the number of agents and $K$ be the number of types on the
side of the market with more types. We find, under reasonable assumptions, that
the relative variation in utility per agent over core outcomes is bounded as
$O^*(1/n^{1/K})$, where polylogarithmic factors have been suppressed. Further,
we show that this bound is tight in worst case. We also provide a tighter bound
under more restrictive assumptions. Our results provide partial justification
for the typical assumption of a unique core outcome in empirical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2587</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2587</id><created>2014-07-09</created><authors><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>The Impact of Network Flows on Community Formation in Models of Opinion
  Dynamics</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>accepted for publication in The Journal of Mathematical Sociology.
  arXiv admin note: text overlap with arXiv:1201.2383</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study dynamics of opinion formation in a network of coupled agents. As the
network evolves to a steady state, opinions of agents within the same community
converge faster than those of other agents. This framework allows us to study
how network topology and network flow, which mediates the transfer of opinions
between agents, both affect the formation of communities. In traditional models
of opinion dynamics, agents are coupled via conservative flows, which result in
one-to-one opinion transfer. However, social interactions are often
non-conservative, resulting in one-to-many transfer of opinions. We study
opinion formation in networks using one-to-one and one-to-many interactions and
show that they lead to different community structure within the same network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2602</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2602</id><created>2014-07-10</created><updated>2015-01-06</updated><authors><author><keyname>Weizman</keyname><forenames>Lior</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Bashat</keyname><forenames>Dafna Ben</forenames></author></authors><title>Compressed sensing for longitudinal MRI: An adaptive-weighted approach</title><categories>physics.med-ph cs.CV</categories><doi>10.1118/1.4928148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: Repeated brain MRI scans are performed in many clinical scenarios,
such as follow up of patients with tumors and therapy response assessment. In
this paper, the authors show an approach to utilize former scans of the patient
for the acceleration of repeated MRI scans.
  Methods: The proposed approach utilizes the possible similarity of the
repeated scans in longitudinal MRI studies. Since similarity is not guaranteed,
sampling and reconstruction are adjusted during acquisition to match the actual
similarity between the scans. The baseline MR scan is utilized both in the
sampling stage, via adaptive sampling, and in the reconstruction stage, with
weighted reconstruction. In adaptive sampling, k-space sampling locations are
optimized during acquisition. Weighted reconstruction uses the locations of the
nonzero coefficients in the sparse domains as a prior in the recovery process.
The approach was tested on 2D and 3D MRI scans of patients with brain tumors.
  Results: The longitudinal adaptive CS MRI (LACS-MRI) scheme provides
reconstruction quality which outperforms other CS-based approaches for rapid
MRI. Examples are shown on patients with brain tumors and demonstrate improved
spatial resolution. Compared with data sampled at Nyquist rate, LACS-MRI
exhibits Signal-to-Error Ratio (SER) of 24.8dB with undersampling factor of
16.6 in 3D MRI.
  Conclusions: The authors have presented a novel method for image
reconstruction utilizing similarity of scans in longitudinal MRI studies, where
possible. The proposed approach can play a major part and significantly reduce
scanning time in many applications that consist of disease follow-up and
monitoring of longitudinal changes in brain MRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2628</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2628</id><created>2014-07-09</created><authors><author><keyname>Nguyen</keyname><forenames>Dan</forenames></author><author><keyname>Tran</keyname><forenames>Le-Nam</forenames></author><author><keyname>Pirinen</keyname><forenames>Pekka</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>On the Spectral Efficiency of Full-Duplex Small Cell Wireless Systems</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Wireless
  Communications, 15 pages, 8 figures</comments><doi>10.1109/TWC.2014.2334610</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the spectral efficiency of full-duplex small cell wireless
systems, in which a full-duplex capable base station (BS) is designed to
send/receive data to/from multiple halfduplex users on the same system
resources. The major hurdle for designing such systems is due to the
self-interference at the BS and co-channel interference among users. Hence, we
consider a joint beamformer design to maximize the spectral efficiency subject
to certain power constraints. The design problem is first formulated as a
rank-constrained optimization one, and the rank relaxation method is then
applied. However the relaxed problem is still nonconvex, and thus optimal
solutions are hard to find. Herein, we propose two provably convergent
algorithms to obtain suboptimal solutions. Based on the concept of the
difference of convex functions programming, we approximate the design problem
by a determinant maximization program in each iteration of the first algorithm.
The second method is built upon the sequential parametric convex approximation
method, which allows us to transform the relaxed problem into a semidefinite
program in each iteration. Extensive numerical experiments under small cell
setups illustrate that the full-duplex system with the proposed algorithms can
achieve a large gain over the half-duplex one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2630</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2630</id><created>2014-07-09</created><authors><author><keyname>Mansoor</keyname><forenames>Awais</forenames></author><author><keyname>Patsekin</keyname><forenames>Valery</forenames></author><author><keyname>Scherl</keyname><forenames>Dale</forenames></author><author><keyname>Robinson</keyname><forenames>J. Paul</forenames></author><author><keyname>Rajwa</keyname><forenames>Bartlomiej</forenames></author></authors><title>A Statistical Modeling Approach to Computer-Aided Quantification of
  Dental Biofilm</title><categories>cs.CV</categories><comments>10 pages, 7 figures, Journal of Biomedical and Health Informatics
  2014. keywords: {Biomedical imaging;Calibration;Dentistry;Estimation;Image
  segmentation;Manuals;Teeth},
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6758338&amp;isnumber=6363502</comments><doi>10.1109/JBHI.2014.2310204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biofilm is a formation of microbial material on tooth substrata. Several
methods to quantify dental biofilm coverage have recently been reported in the
literature, but at best they provide a semi-automated approach to
quantification with significant input from a human grader that comes with the
graders bias of what are foreground, background, biofilm, and tooth.
Additionally, human assessment indices limit the resolution of the
quantification scale; most commercial scales use five levels of quantification
for biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, current
state-of-the-art techniques in automatic plaque quantification fail to make
their way into practical applications owing to their inability to incorporate
human input to handle misclassifications. This paper proposes a new interactive
method for biofilm quantification in Quantitative light-induced fluorescence
(QLF) images of canine teeth that is independent of the perceptual bias of the
grader. The method partitions a QLF image into segments of uniform texture and
intensity called superpixels; every superpixel is statistically modeled as a
realization of a single 2D Gaussian Markov random field (GMRF) whose parameters
are estimated; the superpixel is then assigned to one of three classes
(background, biofilm, tooth substratum) based on the training set of data. The
quantification results show a high degree of consistency and precision. At the
same time, the proposed method gives pathologists full control to post-process
the automatic quantification by flipping misclassified superpixels to a
different state (background, tooth, biofilm) with a single click, providing
greater usability than simply marking the boundaries of biofilm and tooth as
done by current state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2636</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2636</id><created>2014-07-09</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Ashok</forenames></author><author><keyname>Samsi</keyname><forenames>Siddharth</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author></authors><title>Parallel MATLAB Techniques</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter, we show why parallel MATLAB is useful, provide a comparison
of the different parallel MATLAB choices, and describe a number of applications
in Signal and Image Processing: Audio Signal Processing, Synthetic Aperture
Radar (SAR) Processing and Superconducting Quantum Interference Filters
(SQIFs). Each of these applications have been parallelized using different
methods (Task parallel and Data parallel techniques). The applications
presented may be considered representative of type of problems faced by signal
and image processing researchers. This chapter will also strive to serve as a
guide to new signal and image processing parallel programmers, by suggesting a
parallelization strategy that can be employed when developing a general
parallel algorithm. The objective of this chapter is to help signal and image
processing algorithm developers understand the advantages of using parallel
MATLAB to tackle larger problems while staying within the powerful environment
of MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2640</identifier>
 <datestamp>2014-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2640</id><created>2014-07-09</created><updated>2014-10-24</updated><authors><author><keyname>Kannan</keyname><forenames>Sampath</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Approximately Stable, School Optimal, and Student-Truthful Many-to-One
  Matchings (via Differential Privacy)</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mechanism for computing asymptotically stable school optimal
matchings, while guaranteeing that it is an asymptotic dominant strategy for
every student to report their true preferences to the mechanism. Our main tool
in this endeavor is differential privacy: we give an algorithm that coordinates
a stable matching using differentially private signals, which lead to our
truthfulness guarantee. This is the first setting in which it is known how to
achieve nontrivial truthfulness guarantees for students when computing school
optimal matchings, assuming worst- case preferences (for schools and students)
in large markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2641</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2641</id><created>2014-07-09</created><updated>2015-02-12</updated><authors><author><keyname>Kannan</keyname><forenames>Sampath</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Private Pareto Optimal Exchange</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of implementing an individually rational,
asymptotically Pareto optimal allocation in a barter-exchange economy where
agents are endowed with goods and have preferences over the goods of others,
but may not use money as a medium of exchange. Because one of the most
important instantiations of such economies is kidney exchange -- where the
&quot;input&quot;to the problem consists of sensitive patient medical records -- we ask
to what extent such exchanges can be carried out while providing formal privacy
guarantees to the participants. We show that individually rational allocations
cannot achieve any non-trivial approximation to Pareto optimality if carried
out under the constraint of differential privacy -- or even the relaxation of
\emph{joint} differential privacy, under which it is known that asymptotically
optimal allocations can be computed in two-sided markets, where there is a
distinction between buyers and sellers and we are concerned only with privacy
of the buyers~\citep{Matching}. We therefore consider a further relaxation that
we call \emph{marginal} differential privacy -- which promises, informally,
that the privacy of every agent $i$ is protected from every other agent $j \neq
i$ so long as $j$ does not collude or share allocation information with other
agents. We show that, under marginal differential privacy, it is possible to
compute an individually rational and asymptotically Pareto optimal allocation
in such exchange economies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2646</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2646</id><created>2014-07-09</created><authors><author><keyname>Perov</keyname><forenames>Yura N.</forenames></author><author><keyname>Wood</keyname><forenames>Frank D.</forenames></author></authors><title>Learning Probabilistic Programs</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a technique for generalising from data in which models are
samplers represented as program text. We establish encouraging empirical
results that suggest that Markov chain Monte Carlo probabilistic programming
inference techniques coupled with higher-order probabilistic programming
languages are now sufficiently powerful to enable successful inference of this
kind in nontrivial domains. We also introduce a new notion of probabilistic
program compilation and show how the same machinery might be used in the future
to compile probabilistic programs for efficient reusable predictive inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2649</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2649</id><created>2014-07-09</created><authors><author><keyname>Bozkurt</keyname><forenames>Alican</forenames></author><author><keyname>Duygulu</keyname><forenames>Pinar</forenames></author><author><keyname>Cetin</keyname><forenames>A. Enis</forenames></author></authors><title>Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recognizing fonts has become an important task in document analysis, due to
the increasing number of available digital documents in different fonts and
emphases. A generic font-recognition system independent of language, script and
content is desirable for processing various types of documents. At the same
time, categorizing calligraphy styles in handwritten manuscripts is important
for palaeographic analysis, but has not been studied sufficiently in the
literature. We address the font-recognition problem as analysis and
categorization of textures. We extract features using complex wavelet transform
and use support vector machines for classification. Extensive experimental
evaluations on different datasets in four languages and comparisons with
state-of-the-art studies show that our proposed method achieves higher
recognition accuracy while being computationally simpler. Furthermore, on a new
dataset generated from Ottoman manuscripts, we show that the proposed method
can also be used for categorizing Ottoman calligraphy with high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2650</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2650</id><created>2014-07-09</created><updated>2015-08-23</updated><authors><author><keyname>Murfet</keyname><forenames>Daniel</forenames></author></authors><title>Logic and linear algebra: an introduction</title><categories>math.LO cs.LO math.CT</categories><comments>v2: the article has been substantially rewritten to improve the
  exposition, some false statements about cut-elimination were corrected, a new
  section about second-order linear logic was added, and the material on
  geometry of interaction has been removed (to be published elsewhere)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an introduction to logic tailored for algebraists, explaining how
proofs in linear logic can be viewed as algorithms for constructing morphisms
in symmetric closed monoidal categories with additional structure. This is made
explicit by showing how to represent proofs in linear logic as linear maps
between vector spaces. The interesting part of this vector space semantics is
based on the cofree cocommutative coalgebra of Sweedler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2657</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2657</id><created>2014-07-09</created><updated>2014-07-11</updated><authors><author><keyname>Zhang</keyname><forenames>Chicheng</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author></authors><title>Beyond Disagreement-based Agnostic Active Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study agnostic active learning, where the goal is to learn a classifier in
a pre-specified hypothesis class interactively with as few label queries as
possible, while making no assumptions on the true function generating the
labels. The main algorithms for this problem are {\em{disagreement-based active
learning}}, which has a high label requirement, and {\em{margin-based active
learning}}, which only applies to fairly restricted settings. A major challenge
is to find an algorithm which achieves better label complexity, is consistent
in an agnostic setting, and applies to general classification problems.
  In this paper, we provide such an algorithm. Our solution is based on two
novel contributions -- a reduction from consistent active learning to
confidence-rated prediction with guaranteed error, and a novel confidence-rated
predictor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2662</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2662</id><created>2014-07-09</created><updated>2015-07-01</updated><authors><author><keyname>Beimel</keyname><forenames>Amos</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Stemmer</keyname><forenames>Uri</forenames></author></authors><title>Learning Privately with Labeled and Unlabeled Examples</title><categories>cs.LG cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A private learner is an algorithm that given a sample of labeled individual
examples outputs a generalizing hypothesis while preserving the privacy of each
individual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a generic
construction of private learners, in which the sample complexity is (generally)
higher than what is needed for non-private learners. This gap in the sample
complexity was then further studied in several followup papers, showing that
(at least in some cases) this gap is unavoidable. Moreover, those papers
considered ways to overcome the gap, by relaxing either the privacy or the
learning guarantees of the learner.
  We suggest an alternative approach, inspired by the (non-private) models of
semi-supervised learning and active-learning, where the focus is on the sample
complexity of labeled examples whereas unlabeled examples are of a
significantly lower cost. We consider private semi-supervised learners that
operate on a random sample, where only a (hopefully small) portion of this
sample is labeled. The learners have no control over which of the sample
elements are labeled. Our main result is that the labeled sample complexity of
private learners is characterized by the VC dimension.
  We present two generic constructions of private semi-supervised learners. The
first construction is of learners where the labeled sample complexity is
proportional to the VC dimension of the concept class, however, the unlabeled
sample complexity of the algorithm is as big as the representation length of
domain elements. Our second construction presents a new technique for
decreasing the labeled sample complexity of a given private learner, while
roughly maintaining its unlabeled sample complexity. In addition, we show that
in some settings the labeled sample complexity does not depend on the privacy
parameters of the learner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2674</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2674</id><created>2014-07-09</created><authors><author><keyname>Beimel</keyname><forenames>Amos</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Stemmer</keyname><forenames>Uri</forenames></author></authors><title>Private Learning and Sanitization: Pure vs. Approximate Differential
  Privacy</title><categories>cs.LG cs.CR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the sample complexity of private learning [Kasiviswanathan et al.
2008] and sanitization~[Blum et al. 2008] under pure $\epsilon$-differential
privacy [Dwork et al. TCC 2006] and approximate
$(\epsilon,\delta)$-differential privacy [Dwork et al. Eurocrypt 2006]. We show
that the sample complexity of these tasks under approximate differential
privacy can be significantly lower than that under pure differential privacy.
  We define a family of optimization problems, which we call Quasi-Concave
Promise Problems, that generalizes some of our considered tasks. We observe
that a quasi-concave promise problem can be privately approximated using a
solution to a smaller instance of a quasi-concave promise problem. This allows
us to construct an efficient recursive algorithm solving such problems
privately. Specifically, we construct private learners for point functions,
threshold functions, and axis-aligned rectangles in high dimension. Similarly,
we construct sanitizers for point functions and threshold functions.
  We also examine the sample complexity of label-private learners, a relaxation
of private learning where the learner is required to only protect the privacy
of the labels in the sample. We show that the VC dimension completely
characterizes the sample complexity of such learners, that is, the sample
complexity of learning with label privacy is equal (up to constants) to
learning without privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2676</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2676</id><created>2014-07-09</created><updated>2014-07-13</updated><authors><author><keyname>Ryzhov</keyname><forenames>Ilya O.</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author><author><keyname>Powell</keyname><forenames>Warren B.</forenames></author></authors><title>A New Optimal Stepsize For Approximate Dynamic Programming</title><categories>math.OC cs.AI cs.LG cs.SY stat.ML</categories><comments>Matlab files are included with the paper source</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate dynamic programming (ADP) has proven itself in a wide range of
applications spanning large-scale transportation problems, health care, revenue
management, and energy systems. The design of effective ADP algorithms has many
dimensions, but one crucial factor is the stepsize rule used to update a value
function approximation. Many operations research applications are
computationally intensive, and it is important to obtain good results quickly.
Furthermore, the most popular stepsize formulas use tunable parameters and can
produce very poor results if tuned improperly. We derive a new stepsize rule
that optimizes the prediction error in order to improve the short-term
performance of an ADP algorithm. With only one, relatively insensitive tunable
parameter, the new rule adapts to the level of noise in the problem and
produces faster convergence in numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2683</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2683</id><created>2014-07-10</created><authors><author><keyname>Shang</keyname><forenames>Jiaxing</forenames></author><author><keyname>Liu</keyname><forenames>Lianchen</forenames></author><author><keyname>Xie</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Zhen</forenames></author><author><keyname>Miao</keyname><forenames>Jiajia</forenames></author><author><keyname>Fang</keyname><forenames>Xuelin</forenames></author><author><keyname>Wu</keyname><forenames>Cheng</forenames></author></authors><title>A Real-Time Detecting Algorithm for Tracking Community Structure of
  Dynamic Networks</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 6 figures, 3 tables, 6th SNA-KDD Workshop (2012)</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a simple but efficient real-time detecting algorithm is
proposed for tracking community structure of dynamic networks. Community
structure is intuitively characterized as divisions of network nodes into
subgroups, within which nodes are densely connected while between which they
are sparsely connected. To evaluate the quality of community structure of a
network, a metric called modularity is proposed and many algorithms are
developed on optimizing it. However, most of the modularity based algorithms
deal with static networks and cannot be performed frequently, due to their high
computing complexity. In order to track the community structure of dynamic
networks in a fine-grained way, we propose a modularity based algorithm that is
incremental and has very low computing complexity. In our algorithm we adopt a
two-step approach. Firstly we apply the algorithm of Blondel et al for
detecting static communities to obtain an initial community structure. Then,
apply our incremental updating strategies to track the dynamic communities. The
performance of our algorithm is measured in terms of the modularity. We test
the algorithm on tracking community structure of Enron Email and three other
real world datasets. The experimental results show that our algorithm can keep
track of community structure in time and outperform the well known CNM
algorithm in terms of modularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2694</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2694</id><created>2014-07-10</created><authors><author><keyname>Gupta</keyname><forenames>Pooja</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Mathur</keyname><forenames>Iti</forenames></author></authors><title>Quality Estimation Of Machine Translation Outputs Through Stemming</title><categories>cs.CL</categories><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.4, No.3, June 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Machine Translation is the challenging problem for Indian languages. Every
day we can see some machine translators being developed, but getting a high
quality automatic translation is still a very distant dream . The correct
translated sentence for Hindi language is rarely found. In this paper, we are
emphasizing on English-Hindi language pair, so in order to preserve the correct
MT output we present a ranking system, which employs some machine learning
techniques and morphological features. In ranking no human intervention is
required. We have also validated our results by comparing it with human
ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2697</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2697</id><created>2014-07-10</created><authors><author><keyname>Defazio</keyname><forenames>Aaron J.</forenames></author><author><keyname>Caetano</keyname><forenames>Tiberio S.</forenames></author></authors><title>A Convex Formulation for Learning Scale-Free Networks via Submodular
  Relaxation</title><categories>cs.LG stat.ML</categories><journal-ref>Advances in Neural Information Processing Systems 25 (NIPS 2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem in statistics and machine learning is the determination of
network structure from data. We consider the case where the structure of the
graph to be reconstructed is known to be scale-free. We show that in such cases
it is natural to formulate structured sparsity inducing priors using submodular
functions, and we use their Lov\'asz extension to obtain a convex relaxation.
For tractable classes such as Gaussian graphical models, this leads to a convex
optimization problem that can be efficiently solved. We show that our method
results in an improvement in the accuracy of reconstructed networks for
synthetic data. We also show how our prior encourages scale-free
reconstructions on a bioinfomatics dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2700</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2700</id><created>2014-07-10</created><authors><author><keyname>Sulong</keyname><forenames>Ghazali</forenames></author><author><keyname>Ebrahim</keyname><forenames>Anwar Yahy</forenames></author><author><keyname>Jehanzeb</keyname><forenames>Muhammad</forenames></author></authors><title>Offline handwritten signature identification using adaptive window
  positioning techniques</title><categories>cs.CV</categories><comments>13 pages, 9 figures, 2 tables, Offline Handwritten Signature, GPDS
  dataset, Verification, Identification, Adaptive window positioning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents to address this challenge, we have proposed the use of
Adaptive Window Positioning technique which focuses on not just the meaning of
the handwritten signature but also on the individuality of the writer. This
innovative technique divides the handwritten signature into 13 small windows of
size nxn(13x13).This size should be large enough to contain ample information
about the style of the author and small enough to ensure a good identification
performance.The process was tested with a GPDS data set containing 4870
signature samples from 90 different writers by comparing the robust features of
the test signature with that of the user signature using an appropriate
classifier. Experimental results reveal that adaptive window positioning
technique proved to be the efficient and reliable method for accurate signature
feature extraction for the identification of offline handwritten signatures.The
contribution of this technique can be used to detect signatures signed under
emotional duress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2704</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2704</id><created>2014-07-10</created><updated>2014-07-20</updated><authors><author><keyname>Qin</keyname><forenames>Yongrui</forenames></author><author><keyname>Sheng</keyname><forenames>Quan Z.</forenames></author><author><keyname>Falkner</keyname><forenames>Nickolas J. G.</forenames></author><author><keyname>Dustdar</keyname><forenames>Schahram</forenames></author><author><keyname>Wang</keyname><forenames>Hua</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>When Things Matter: A Data-Centric View of the Internet of Things</title><categories>cs.DB cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent advances in radio-frequency identification (RFID), low-cost
wireless sensor devices, and Web technologies, the Internet of Things (IoT)
approach has gained momentum in connecting everyday objects to the Internet and
facilitating machine-to-human and machine-to-machine communication with the
physical world. While IoT offers the capability to connect and integrate both
digital and physical entities, enabling a whole new class of applications and
services, several significant challenges need to be addressed before these
applications and services can be fully realized. A fundamental challenge
centers around managing IoT data, typically produced in dynamic and volatile
environments, which is not only extremely large in scale and volume, but also
noisy, and continuous. This article surveys the main techniques and
state-of-the-art research efforts in IoT from data-centric perspectives,
including data stream processing, data storage models, complex event
processing, and searching in IoT. Open research issues for IoT data management
are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2705</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2705</id><created>2014-07-10</created><authors><author><keyname>Benmammar</keyname><forenames>Badr</forenames><affiliation>LTT</affiliation></author><author><keyname>Amraoui</keyname><forenames>Asma</forenames><affiliation>LTT</affiliation></author></authors><title>R\'eseaux de radio cognitive : Allocation des ressources radio et
  acc\`es dynamique au spectre</title><categories>cs.MA cs.NI</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first chapter of this report, we provide an overview on mobile and
wireless networks, with special focus on the IEEE 802.22 norm, which is a norm
dedicated to cognitive radio (CR). Chapter 2 goes into detail about CR and
Chapter 3 is devoted to the presentation of the concept of agents and in
particular the concept of multi-agent systems (MAS). Finally, Chapter 4
provides a state of the art on the use of artificial intelligence techniques,
particularly MAS for radio resource allocation and dynamic spectrum access in
the field of CR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2710</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2710</id><created>2014-07-10</created><authors><author><keyname>Defazio</keyname><forenames>Aaron J.</forenames></author><author><keyname>Caetano</keyname><forenames>Tib&#xe9;rio S.</forenames></author><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Finito: A Faster, Permutable Incremental Gradient Method for Big Data
  Problems</title><categories>cs.LG stat.ML</categories><journal-ref>International Conference on Machine Learning 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in optimization theory have shown that smooth strongly convex
finite sums can be minimized faster than by treating them as a black box
&quot;batch&quot; problem. In this work we introduce a new method in this class with a
theoretical convergence rate four times faster than existing methods, for sums
with sufficiently many terms. This method is also amendable to a sampling
without replacement scheme that in practice gives further speed-ups. We give
empirical results showing state of the art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2717</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2717</id><created>2014-07-10</created><authors><author><keyname>Babatunde</keyname><forenames>Olabenjo</forenames></author><author><keyname>Al-Debagy</keyname><forenames>Omar</forenames></author></authors><title>A Comparative Review Of Internet Protocol Version 4 (IPv4) and Internet
  Protocol Version 6 (IPv6)</title><categories>cs.NI</categories><comments>4 pages, 2 tables, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>Olabenjo Babatunde , Omar Al-Debagy.&quot;A Comparative Review Of
  Internet Protocol Version 4 (IPv4) and Internet Protocol Version 6 (IPv6)&quot;.
  International Journal of Computer Trends and Technology (IJCTT) V13(1):10-13,
  July 2014</journal-ref><doi>10.14445/22312803/IJCTT-V13P103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computers and devices are becoming more connected to the internet in
recent years; the use of the Internet Protocol (IP) has made the connectivity
and identification of these devices possible in large scale. In this paper, we
will discuss the evolution of Internet Protocol version 4 (IPv4), its features,
issues and limitations and how Internet Protocol version 6 (IPv6) tends to
solve some of these issues including the differences and transition between
these two protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2721</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2721</id><created>2014-07-10</created><updated>2014-08-25</updated><authors><author><keyname>Barz</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Rodner</keyname><forenames>Erik</forenames></author><author><keyname>Denzler</keyname><forenames>Joachim</forenames></author></authors><title>ARTOS -- Adaptive Real-Time Object Detection System</title><categories>cs.CV</categories><comments>http://cvjena.github.io/artos/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ARTOS is all about creating, tuning, and applying object detection models
with just a few clicks. In particular, ARTOS facilitates learning of models for
visual object detection by eliminating the burden of having to collect and
annotate a large set of positive and negative samples manually and in addition
it implements a fast learning technique to reduce the time needed for the
learning step.
  A clean and friendly GUI guides the user through the process of model
creation, adaptation of learned models to different domains using in-situ
images, and object detection on both offline images and images from a video
stream. A library written in C++ provides the main functionality of ARTOS with
a C-style procedural interface, so that it can be easily integrated with any
other project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2723</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2723</id><created>2014-07-10</created><authors><author><keyname>Vr&#x161;ek</keyname><forenames>Jan</forenames><affiliation>NTIS -- New Technologies for the Information Society, Faculty of Applied Sciences, University of West Bohemia, Plze&#x148;, Czech Republic</affiliation></author><author><keyname>L&#xe1;vi&#x10d;ka</keyname><forenames>Miroslav</forenames><affiliation>NTIS -- New Technologies for the Information Society, Faculty of Applied Sciences, University of West Bohemia, Plze&#x148;, Czech Republic</affiliation><affiliation>Department of Mathematics, Faculty of Applied Sciences, University of West Bohemia, Plze&#x148;, Czech Republic</affiliation></author></authors><title>Determining surfaces of revolution from their implicit equations</title><categories>cs.SC cs.GR math.AG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Results of number of geometric operations (often used in technical practise,
as e.g. the operation of blending) are in many cases surfaces described
implicitly. Then it is a challenging task to recognize the type of the obtained
surface, find its characteristics and for the rational surfaces compute also
their parameterizations. In this contribution we will focus on surfaces of
revolution. These objects, widely used in geometric modelling, are generated by
rotating a generatrix around a given axis. If the generatrix is an algebraic
curve then so is also the resulting surface, described uniquely by a polynomial
which can be found by some well-established implicitation technique. However,
starting from a polynomial it is not known how to decide if the corresponding
algebraic surface is rotational or not. Motivated by this, our goal is to
formulate a simple and efficient algorithm whose input is a polynomial with the
coefficients from some subfield of $\mathbb{R}$ and the output is the answer
whether the shape is a surface of revolution. In the affirmative case we also
find the equations of its axis and generatrix. Furthermore, we investigate the
problem of rationality and unirationality of surfaces of revolution and show
that this question can be efficiently answered discussing the rationality of a
certain associated planar curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2729</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2729</id><created>2014-07-10</created><authors><author><keyname>Rana</keyname><forenames>Manisha</forenames></author><author><keyname>Tanwar</keyname><forenames>Rohit</forenames></author></authors><title>Genetic Algorithm in Audio Steganography</title><categories>cs.MM</categories><comments>6 pages,2 figures Published with International Journal of Engineering
  Trends and Technology (IJETT). arXiv admin note: text overlap with
  arXiv:1003.4084, arXiv:1205.2800 by other authors without attribution</comments><journal-ref>IJETT, V13(1),29-34 July 2014</journal-ref><doi>10.14445/22315381/IJETT-V13P206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of communication technology,data is exchanged digitally
over the network. At the other side the technology is also proven as a tool for
unauthorized access to attackers. Thus the security of data to be transmitted
digitally should get prime focus. Data hiding is the common approach to secure
data. In steganography technique, the existence of data is concealed. GA is an
emerging component of AI to provide suboptimal solutions. In this paper the use
of GA in Steganography is explored to find future scope of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2730</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2730</id><created>2014-07-10</created><authors><author><keyname>Zamani</keyname><forenames>Majid</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author><author><keyname>Girard</keyname><forenames>Antoine</forenames></author></authors><title>Symbolic Models for Stochastic Switched Systems: A Discretization and a
  Discretization-Free Approach</title><categories>math.OC cs.LO cs.SY</categories><comments>25 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1302.3868</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic switched systems are a relevant class of stochastic hybrid systems
with probabilistic evolution over a continuous domain and control-dependent
discrete dynamics over a finite set of modes. In the past few years several
different techniques have been developed to assist in the stability analysis of
stochastic switched systems. However, more complex and challenging objectives
related to the verification of and the controller synthesis for logic
specifications have not been formally investigated for this class of systems as
of yet. With logic specifications we mean properties expressed as formulae in
linear temporal logic or as automata on infinite strings. This paper addresses
these complex objectives by constructively deriving approximately equivalent
(bisimilar) symbolic models of stochastic switched systems. More precisely,
this paper provides two different symbolic abstraction techniques: one requires
state space discretization, but the other one does not require any space
discretization which can be potentially more efficient than the first one when
dealing with higher dimensional stochastic switched systems. Both techniques
provide finite symbolic models that are approximately bisimilar to stochastic
switched systems under some stability assumptions on the concrete model. This
allows formally synthesizing controllers (switching signals) that are valid for
the concrete system over the finite symbolic model, by means of mature
automata-theoretic techniques in the literature. The effectiveness of the
results are illustrated by synthesizing switching signals enforcing logic
specifications for two case studies including temperature control of a six-room
building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2733</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2733</id><created>2014-07-10</created><updated>2014-07-19</updated><authors><author><keyname>Yi</keyname><forenames>Wentan</forenames></author><author><keyname>Chen</keyname><forenames>Shaozhen</forenames></author></authors><title>Improved Results on Integral and Zero-correlation Linear Cryptanalysis
  of the Block Cipher MIBS</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the authors. I am sorry that some
  mistakes have been made in the section 3 of the paper. I have to withdraw it</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIBS is a light weight block cipher aimed at extremely constrained resources
environments such as RFID tags and sensor networks. In this paper, we focus on
improved key-recovery attacks on reduced-round MIBS with integral and
zero-correlation linear cryptanalysis. By exploring the key-expanding
properties and choosing suitable linear approximations with
zero-correlation,13-round zero-correlation linear cryptanalysis were presented.
Furthermore, we deduced some integral distinguishers from 8-round
zero-correlation linear approximations using the relations between them, and as
applications, we applied these integral distinguishers to the cryptanalysis of
MIBS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2736</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2736</id><created>2014-07-10</created><authors><author><keyname>Sundararajan</keyname><forenames>Ramasubramanian</forenames></author><author><keyname>Patel</keyname><forenames>Hima</forenames></author><author><keyname>Srivastava</keyname><forenames>Manisha</forenames></author></authors><title>A multi-instance learning algorithm based on a stacked ensemble of lazy
  learners</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This document describes a novel learning algorithm that classifies &quot;bags&quot; of
instances rather than individual instances. A bag is labeled positive if it
contains at least one positive instance (which may or may not be specifically
identified), and negative otherwise. This class of problems is known as
multi-instance learning problems, and is useful in situations where the class
label at an instance level may be unavailable or imprecise or difficult to
obtain, or in situations where the problem is naturally posed as one of
classifying instance groups. The algorithm described here is an ensemble-based
method, wherein the members of the ensemble are lazy learning classifiers
learnt using the Citation Nearest Neighbour method. Diversity among the
ensemble members is achieved by optimizing their parameters using a
multi-objective optimization method, with the objectives being to maximize
Class 1 accuracy and minimize false positive rate. The method has been found to
be effective on the Musk1 benchmark dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2742</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2742</id><created>2014-07-10</created><updated>2014-07-26</updated><authors><author><keyname>Gawronski</keyname><forenames>P.</forenames></author><author><keyname>Nawojczyk</keyname><forenames>M.</forenames></author><author><keyname>Kulakowski</keyname><forenames>K.</forenames></author></authors><title>Opinion formation in an open system and the spiral of silence</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new model is formulated of the sociological effect of the spiral of
silence, introduced by Elisabeth Noelle-Neumann in 1974. The probability that a
new opinion is openly expressed decreases with the difference between this new
opinion and the perceived opinion of the majority. We also assume that the
system is open, i.e. some people enter and some leave during the process of the
opinion formation. An influence of a leader is simulated by a comparison of two
runs of the simulation, where the leader has different opinion in each run. The
difference of the mean expressed opinions in these two runs persists long after
the leader's leave.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2747</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2747</id><created>2014-07-10</created><authors><author><keyname>Abid</keyname><forenames>Sohail</forenames></author><author><keyname>Shafi</keyname><forenames>Imran</forenames></author><author><keyname>Abid</keyname><forenames>Shahid</forenames></author></authors><title>Improving energy efficiency in MANET's for healthcare environments</title><categories>cs.NI</categories><comments>10 pages, 10 figures, International Journal of Mobile Network
  Communications &amp; Telematics</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics ( IJMNCT) Vol. 4, No.3, June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now a day ad hoc mobile networks (MANETs) have lots of routing protocols, but
no one can meet maximum performance. Some are good in a small network; some are
suitable in large networks, and some give better performance in location or
global networks. Today modern and innovative applications for health care
environments based on a wireless network are being developed in the commercial
sectors. The emerging wireless networks are rapidly becoming a fundamental part
of every single field of life. Our proposed DEERP framework gives a better
performance as compared to other routing protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2758</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2758</id><created>2014-07-10</created><updated>2015-11-13</updated><authors><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Mobility-Aware Uplink Interference Model for 5G Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>14 pages,12 figures</comments><doi>10.1109/TWC.2015.2500566</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the surging demand for throughput, 5G cellular networks need to be
more heterogeneous and much denser, by deploying more and more small cells. In
particular, the number of users in each small cell can change dramatically due
to users' mobility, resulting in random and time varying uplink interference.
This paper considers the uplink interference in a 5G heterogeneous network
which is jointly covered by one macro cell and several small cells. Based on
the L\'{e}vy flight moving model, a mobility-aware interference model is
proposed to characterize the uplink interference from macro cell users to small
cell users. In this model, the total uplink interference is characterized by
its moment generating function, for both closed subscriber group (CSG) and open
subscriber group (CSG) femto cells. In addition, the proposed interference
model is a function of basic step length, which is a key velocity parameter of
L\'{e}vy flights. It is shown by both theoretical analysis and simulation
results that the proposed interference model provides a flexible way of
evaluating the system performance in terms of success probability and average
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2774</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2774</id><created>2014-07-10</created><updated>2015-04-28</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Perkins</keyname><forenames>Will</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Subsampled Power Iteration: a Unified Algorithm for Block Models and
  Planted CSP's</title><categories>cs.DS math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for recovering planted solutions in two well-known
models, the stochastic block model and planted constraint satisfaction
problems, via a common generalization in terms of random bipartite graphs. Our
algorithm matches up to a constant factor the best-known bounds for the number
of edges (or constraints) needed for perfect recovery and its running time is
linear in the number of edges used. The time complexity is significantly better
than both spectral and SDP-based approaches.
  The main contribution of the algorithm is in the case of unequal sizes in the
bipartition (corresponding to odd uniformity in the CSP). Here our algorithm
succeeds at a significantly lower density than the spectral approaches,
surpassing a barrier based on the spectral norm of a random matrix.
  Other significant features of the algorithm and analysis include (i) the
critical use of power iteration with subsampling, which might be of independent
interest; its analysis requires keeping track of multiple norms of an evolving
solution (ii) it can be implemented statistically, i.e., with very limited
access to the input distribution (iii) the algorithm is extremely simple to
implement and runs in linear time, and thus is practical even for very large
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2776</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2776</id><created>2014-07-10</created><authors><author><keyname>Khaligh-Razavi</keyname><forenames>Seyed-Mahdi</forenames></author></authors><title>What you need to know about the state-of-the-art computational models of
  object-vision: A tour through the models</title><categories>cs.CV cs.AI cs.LG q-bio.NC</categories><comments>36 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models of object vision have been of great interest in computer vision and
visual neuroscience. During the last decades, several models have been
developed to extract visual features from images for object recognition tasks.
Some of these were inspired by the hierarchical structure of primate visual
system, and some others were engineered models. The models are varied in
several aspects: models that are trained by supervision, models trained without
supervision, and models (e.g. feature extractors) that are fully hard-wired and
do not need training. Some of the models come with a deep hierarchical
structure consisting of several layers, and some others are shallow and come
with only one or two layers of processing. More recently, new models have been
developed that are not hand-tuned but trained using millions of images, through
which they learn how to extract informative task-related features. Here I will
survey all these different models and provide the reader with an intuitive, as
well as a more detailed, understanding of the underlying computations in each
of the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2791</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2791</id><created>2014-07-10</created><authors><author><keyname>Sun</keyname><forenames>Ruoyu</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Joint Downlink Base Station Association and Power Control for Max-Min
  Fairness: Computation and Complexity</title><categories>cs.IT math.IT</categories><comments>24 pages, 7 figures, a shorter version submitted to IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a heterogeneous network (HetNet) with a large number of low power base
stations (BSs), proper user-BS association and power control is crucial to
achieving desirable system performance. In this paper, we systematically study
the joint BS association and power allocation problem for a downlink cellular
network under the max-min fairness criterion. First, we show that this problem
is NP-hard. Second, we show that the upper bound of the optimal value can be
easily computed, and propose a two-stage algorithm to find a high-quality
suboptimal solution. Simulation results show that the proposed algorithm is
near-optimal in the high-SNR regime. Third, we show that the problem under some
additional mild assumptions can be solved to global optima in polynomial time
by a semi-distributed algorithm. This result is based on a transformation of
the original problem to an assignment problem with gains $\log(g_{ij})$, where
$\{g_{ij}\}$ are the channel gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2795</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2795</id><created>2014-07-10</created><authors><author><keyname>Billings</keyname><forenames>Jay Jay</forenames></author><author><keyname>Deyton</keyname><forenames>Jordan H.</forenames></author><author><keyname>Hull</keyname><forenames>S. Forest</forenames></author><author><keyname>Lingerfelt</keyname><forenames>Eric J.</forenames></author><author><keyname>Wojtowicz</keyname><forenames>Anna</forenames></author></authors><title>A domain-specific analysis system for examining nuclear reactor
  simulation data for light-water and sodium-cooled fast reactors</title><categories>cs.CE physics.ins-det</categories><comments>Article on NiCE's Reactor Analyzer. 23 pages. Keywords: modeling,
  simulation, analysis, visualization, input-output</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Building a new generation of fission reactors in the United States presents
many technical and regulatory challenges. One important challenge is the need
to share and present results from new high-fidelity, high-performance
simulations in an easily usable way. Since modern multiscale, multi-physics
simulations can generate petabytes of data, they will require the development
of new techniques and methods to reduce the data to familiar quantities of
interest (e.g., pin powers, temperatures) with a more reasonable resolution and
size. Furthermore, some of the results from these simulations may be new
quantities for which visualization and analysis techniques are not immediately
available in the community and need to be developed.
  This paper describes a new system for managing high-performance simulation
results in a domain-specific way that naturally exposes quantities of interest
for light water and sodium-cooled fast reactors. It describes requirements to
build such a system and the technical challenges faced in its development at
all levels (simulation, user interface, etc.). An example comparing results
from two different simulation suites for a single assembly in a light-water
reactor is presented, along with a detailed discussion of the system's
requirements and design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2799</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2799</id><created>2014-07-10</created><authors><author><keyname>Bus&#xe9;</keyname><forenames>Laurent</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Karasoulou</keyname><forenames>Anna</forenames><affiliation>Athens</affiliation></author></authors><title>Resultant of an equivariant polynomial system with respect to the
  symmetric group</title><categories>math.AC cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a system of n homogeneous polynomials in n variables which is
equivariant with respect to the canonical actions of the symmetric group of n
symbols on the variables and on the polynomials, it is proved that its
resultant can be decomposed into a product of several smaller resultants that
are given in terms of some divided differences. As an application, we obtain a
decomposition formula for the discriminant of a multivariate homogeneous
symmetric polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2801</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2801</id><created>2014-07-10</created><updated>2014-12-13</updated><authors><author><keyname>Laurent</keyname><forenames>Monique</forenames></author><author><keyname>Seminaroti</keyname><forenames>Matteo</forenames></author></authors><title>The quadratic assignment problem is easy for Robinsonian matrices with
  Toeplitz structure</title><categories>math.OC cs.DM</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new polynomially solvable case of the Quadratic Assignment
Problem in Koopmans-Beckman form $QAP(A,B)$, by showing that the identity
permutation is optimal when $A$ and $B$ are respectively a Robinson similarity
and dissimilarity matrix and one of $A$ or $B$ is a Toeplitz matrix. A Robinson
(dis)similarity matrix is a symmetric matrix whose entries (increase) decrease
monotonically along rows and columns when moving away from the diagonal, and
such matrices arise in the classical seriation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2802</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2802</id><created>2014-07-10</created><authors><author><keyname>Benoit</keyname><forenames>Alexandre</forenames><affiliation>LAAS</affiliation></author><author><keyname>Joldes</keyname><forenames>Mioara</forenames><affiliation>LAAS</affiliation></author><author><keyname>Mezzarobba</keyname><forenames>Marc</forenames><affiliation>LIP6</affiliation></author></authors><title>Rigorous uniform approximation of D-finite functions using Chebyshev
  expansions</title><categories>cs.SC cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of numerical methods exists for computing polynomial
approximations of solutions of ordinary differential equations based on
Chebyshev series expansions or Chebyshev interpolation polynomials. We consider
the application of such methods in the context of rigorous computing (where we
need guarantees on the accuracy of the result), and from the complexity point
of view. It is well-known that the order-n truncation of the Chebyshev
expansion of a function over a given interval is a near-best uniform polynomial
approximation of the function on that interval. In the case of solutions of
linear differential equations with polynomial coefficients, the coefficients of
the expansions obey linear recurrence relations with polynomial coefficients.
Unfortunately, these recurrences do not lend themselves to a direct recursive
computation of the coefficients, owing among other things to a lack of initial
conditions. We show how they can nevertheless be used, as part of a validated
process, to compute good uniform approximations of D-finite functions together
with rigorous error bounds, and we study the complexity of the resulting
algorithms. Our approach is based on a new view of a classical numerical method
going back to Clenshaw, combined with a functional enclosure method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2806</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2806</id><created>2014-07-10</created><authors><author><keyname>Mary</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Gaudel</keyname><forenames>Romaric</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Philippe</keyname><forenames>Preux</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author></authors><title>Bandits Warm-up Cold Recommender Systems</title><categories>cs.LG cs.IR stat.ML</categories><proxy>ccsd</proxy><report-no>RR-8563</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the cold start problem in recommendation systems assuming no
contextual information is available neither about users, nor items. We consider
the case in which we only have access to a set of ratings of items by users.
Most of the existing works consider a batch setting, and use cross-validation
to tune parameters. The classical method consists in minimizing the root mean
square error over a training subset of the ratings which provides a
factorization of the matrix of ratings, interpreted as a latent representation
of items and users. Our contribution in this paper is 5-fold. First, we
explicit the issues raised by this kind of batch setting for users or items
with very few ratings. Then, we propose an online setting closer to the actual
use of recommender systems; this setting is inspired by the bandit framework.
The proposed methodology can be used to turn any recommender system dataset
(such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit a
strong and insightful link between contextual bandit algorithms and matrix
factorization; this leads us to a new algorithm that tackles the
exploration/exploitation dilemma associated to the cold start problem in a
strikingly new perspective. Finally, experimental evidence confirm that our
algorithm is effective in dealing with the cold start problem on publicly
available datasets. Overall, the goal of this paper is to bridge the gap
between recommender systems based on matrix factorizations and those based on
contextual bandits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2807</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2807</id><created>2014-07-10</created><authors><author><keyname>Oliveira</keyname><forenames>Allan</forenames></author><author><keyname>Araujo</keyname><forenames>Regina</forenames></author></authors><title>A human centered perspective of E-maintenance</title><categories>cs.HC</categories><comments>Presented in the IX Workshop of Virtual and Augmented Reality - 2012</comments><acm-class>D.2.2; H.5.2; J.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-maintenance is a technology aiming to organize and structure the ICT during
the whole life cycle of the product, to develop a maintenance support system
that is effective and efficient. A current challenge of E-maintenance is the
development of generic visualization solutions for users responsible for the
maintenance. AR can be a potential technology for E-maintenance visualization,
since it can bring knowledge to the real physical world, to assist the
technician perform his/her work without the need to interrupt to consult
manuals for information. This paper proposes a methodology for the development
of advanced interfaces for human aware E-maintenance so that complex
maintenance processes can be made safer, better quality, faster, anytime and
anywhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2812</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2812</id><created>2014-07-10</created><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Yuan</keyname><forenames>Ming</forenames></author></authors><title>Rate-Optimal Detection of Very Short Signal Segments</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a range of applications in engineering and genomics, we consider
in this paper detection of very short signal segments in three settings:
signals with known shape, arbitrary signals, and smooth signals. Optimal rates
of detection are established for the three cases and rate-optimal detectors are
constructed. The detectors are easily implementable and are based on scanning
with linear and quadratic statistics. Our analysis reveals both similarities
and differences in the strategy and fundamental difficulty of detection among
these three settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2827</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2827</id><created>2014-07-10</created><authors><author><keyname>Mart&#xed;n-Mart&#xed;n</keyname><forenames>Alberto</forenames></author><author><keyname>Ayll&#xf3;n</keyname><forenames>Juan Manuel</forenames></author><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Google Scholar Metrics 2014: a low cost bibliometric tool</title><categories>cs.DL</categories><comments>37 pages, 10 tables, 2 figures, 5 appendixes</comments><report-no>EC3 Working Paper 17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the main features of the third edition of Google Scholar Metrics
(GSM), released in June 2014, focusing on its more important changes,
strengths, and weaknesses. Additionally, we present some figures that outline
the dimensions of this new edition, and we compare them to those of previous
editions. Principal among these figures are the number of visualized
publications, publication types, languages, and the maximum and minimum
h5-index and h5-median values by language, subject area, and subcategory. This
new edition is marked by continuity. There is nothing new other than the
updating of the time frame (2009-2013) and the removal of some redundant
subcategories (from 268 to 261) for English written publications. Google has
just updated the data, which means that some of the errors discussed in
previous studies still persist. To sum up, GSM is a minimalist information
product with few features, closed (it cannot be customized by the user), and
simple (navigating it only takes a few clicks). For these reasons, we consider
it a 'low cost' bibliometric tool, and propose a list of features it should
incorporate in order to stop being labeled as such. Notwithstanding the above,
this product presents a stability in its bibliometric indicators that supports
its ability to measure and track the impact of scientific publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2837</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2837</id><created>2014-07-10</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Catanese</keyname><forenames>Salvatore</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author></authors><title>Visualizing criminal networks reconstructed from mobile phone records</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>6 pages, 4 figures, DataWiz 2014 (held in conjunction with ACM
  Hypertext 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the fight against the racketeering and terrorism, knowledge about the
structure and the organization of criminal networks is of fundamental
importance for both the investigations and the development of efficient
strategies to prevent and restrain crimes. Intelligence agencies exploit
information obtained from the analysis of large amounts of heterogeneous data
deriving from various informative sources including the records of phone
traffic, the social networks, surveillance data, interview data, experiential
police data, and police intelligence files, to acquire knowledge about criminal
networks and initiate accurate and destabilizing actions. In this context,
visual representation techniques coordinate the exploration of the structure of
the network together with the metrics of social network analysis. Nevertheless,
the utility of visualization tools may become limited when the dimension and
the complexity of the system under analysis grow beyond certain terms. In this
paper we show how we employ some interactive visualization techniques to
represent criminal and terrorist networks reconstructed from phone traffic
data, namely foci, fisheye and geo-mapping network layouts. These methods allow
the exploration of the network through animated transitions among visualization
models and local enlargement techniques in order to improve the comprehension
of interesting areas. By combining the features of the various visualization
models it is possible to gain substantial enhancements with respect to classic
visualization models, often unreadable in those cases of great complexity of
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2844</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2844</id><created>2014-07-10</created><authors><author><keyname>Iwata</keyname><forenames>Satoru</forenames></author><author><keyname>Newman</keyname><forenames>Alantha</forenames></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author></authors><title>Graph-TSP from Steiner Cycles</title><categories>cs.DS</categories><comments>Proceedings of WG 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for the traveling salesman problem with graph metric
based on Steiner cycles. A Steiner cycle is a cycle that is required to contain
some specified subset of vertices. For a graph $G$, if we can find a spanning
tree $T$ and a simple cycle that contains the vertices with odd-degree in $T$,
then we show how to combine the classic &quot;double spanning tree&quot; algorithm with
Christofides' algorithm to obtain a TSP tour of length at most $\frac{4n}{3}$.
We use this approach to show that a graph containing a Hamiltonian path has a
TSP tour of length at most $4n/3$.
  Since a Hamiltonian path is a spanning tree with two leaves, this motivates
the question of whether or not a graph containing a spanning tree with few
leaves has a short TSP tour. The recent techniques of M\&quot;omke and Svensson
imply that a graph containing a depth-first-search tree with $k$ leaves has a
TSP tour of length $4n/3 + O(k)$. Using our approach, we can show that a
$2(k-1)$-vertex connected graph that contains a spanning tree with at most $k$
leaves has a TSP tour of length $4n/3$. We also explore other conditions under
which our approach results in a short tour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2845</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2845</id><created>2014-07-10</created><authors><author><keyname>Agreste</keyname><forenames>Santa</forenames></author><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Ursino</keyname><forenames>Domenico</forenames></author></authors><title>XML Matchers: approaches and challenges</title><categories>cs.DB cs.AI cs.IR cs.LG</categories><comments>34 pages, 8 tables, 7 figures</comments><journal-ref>Knowledge-based systems 66: 190-209, 2014</journal-ref><doi>10.1016/j.knosys.2014.04.044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schema Matching, i.e. the process of discovering semantic correspondences
between concepts adopted in different data source schemas, has been a key topic
in Database and Artificial Intelligence research areas for many years. In the
past, it was largely investigated especially for classical database models
(e.g., E/R schemas, relational databases, etc.). However, in the latest years,
the widespread adoption of XML in the most disparate application fields pushed
a growing number of researchers to design XML-specific Schema Matching
approaches, called XML Matchers, aiming at finding semantic matchings between
concepts defined in DTDs and XSDs. XML Matchers do not just take well-known
techniques originally designed for other data models and apply them on
DTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical
structure of a DTD/XSD) to improve the performance of the Schema Matching
process. The design of XML Matchers is currently a well-established research
area. The main goal of this paper is to provide a detailed description and
classification of XML Matchers. We first describe to what extent the
specificities of DTDs/XSDs impact on the Schema Matching task. Then we
introduce a template, called XML Matcher Template, that describes the main
components of an XML Matcher, their role and behavior. We illustrate how each
of these components has been implemented in some popular XML Matchers. We
consider our XML Matcher Template as the baseline for objectively comparing
approaches that, at first glance, might appear as unrelated. The introduction
of this template can be useful in the design of future XML Matchers. Finally,
we analyze commercial tools implementing XML Matchers and introduce two
challenging issues strictly related to this topic, namely XML source clustering
and uncertainty management in XML Matchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2854</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2854</id><created>2014-07-10</created><updated>2014-08-27</updated><authors><author><keyname>Denny</keyname><forenames>Matthew J.</forenames></author></authors><title>Graph Compartmentalization</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a concept and measure of graph compartmentalization.
This new measure allows for principled comparison between graphs of arbitrary
structure, unlike existing measures such as graph modularity. The proposed
measure is invariant to graph size and number of groups and can be calculated
analytically, facilitating measurement on very large graphs. I also introduce a
block model generative process for compartmentalized graphs as a benchmark on
which to validate the proposed measure. Simulation results demonstrate improved
performance of the new measure over modularity in recovering the degree of
compartmentalization of graphs simulated from the generative model. I also
explore an application to the measurement of political polarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2855</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2855</id><created>2014-07-10</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author></authors><title>Learning Valuation Distributions from Partial Observation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Auction theory traditionally assumes that bidders' valuation distributions
are known to the auctioneer, such as in the celebrated, revenue-optimal Myerson
auction. However, this theory does not describe how the auctioneer comes to
possess this information. Recently, Cole and Roughgarden [2014] showed that an
approximation based on a finite sample of independent draws from each bidder's
distribution is sufficient to produce a near-optimal auction. In this work, we
consider the problem of learning bidders' valuation distributions from much
weaker forms of observations. Specifically, we consider a setting where there
is a repeated, sealed-bid auction with $n$ bidders, but all we observe for each
round is who won, but not how much they bid or paid. We can also participate
(i.e., submit a bid) ourselves, and observe when we win. From this information,
our goal is to (approximately) recover the inherently recoverable part of the
underlying bid distributions. We also consider extensions where different
subsets of bidders participate in each round, and where bidders' valuations
have a common-value component added to their independent private values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2857</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2857</id><created>2014-07-10</created><authors><author><keyname>Malandrino</keyname><forenames>Francesco</forenames></author><author><keyname>Casetti</keyname><forenames>Claudio</forenames></author><author><keyname>Chiasserini</keyname><forenames>Carla-Fabiana</forenames></author><author><keyname>Zhou</keyname><forenames>Siyuan</forenames></author></authors><title>Real-Time Scheduling for Content Broadcasting in LTE</title><categories>cs.NI</categories><comments>6 pages; IEEE 22nd International Symposium on Modeling, Analysis and
  Simulation of Computer and Telecommunication Systems (MASCOTS 2014)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Broadcasting capabilities are one of the most promising features of upcoming
LTE-Advanced networks. However, the task of scheduling broadcasting sessions is
far from trivial, since it affects the available resources of several
contiguous cells as well as the amount of resources that can be devoted to
unicast traffic. In this paper, we present a compact, convenient model for
broadcasting in LTE, as well as a set of efficient algorithms to define
broadcasting areas and to actually perform content scheduling. We study the
performance of our algorithms in a realistic scenario, deriving interesting
insights on the possible trade-offs between effectiveness and computational
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2873</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2873</id><created>2014-07-10</created><authors><author><keyname>Kulikov</keyname><forenames>Sergey</forenames></author></authors><title>Possibilities of technologization of philosophical knowledge</title><categories>cs.AI</categories><comments>6 pages, in Russian, conference &quot;Constraction of Man&quot; (Russia, Tomsk,
  2011, April 26-29)</comments><msc-class>03B42</msc-class><doi>10.13140/2.1.3036.7521</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Article purpose is the analysis of a question of possibility of
technologization of philosophical knowledge. We understand the organization of
cognitive activity which is guided by the set of methods guaranteed bringing to
successful (i.e. to precisely corresponding set parameters) to applied results
as technologization. Transformation of sense of philosophy allows revealing
possibilities of its technologization. The leading role in this process is
played by philosophy of science which creates conditions for such
transformation. At the same time there is justified an appeal to branch
combination theory of the directions of scientific knowledge and partial
refusal of understanding of philosophy as synthetic knowledge in which the main
task is permission, instead of generation of paradoxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2875</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2875</id><created>2014-07-10</created><updated>2015-11-08</updated><authors><author><keyname>Brown</keyname><forenames>Matthew F.</forenames></author></authors><title>Quantum Dynamics, Minkowski-Hilbert space, and A Quantum Stochastic
  Duhamel Principle</title><categories>math-ph cs.IT math.DS math.IT math.MP quant-ph</categories><msc-class>81S22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we shall re-visit the well-known Schr\&quot;odinger and Lindblad
dynamics of quantum mechanics. However, these equations may be realized as the
consequence of a more general, underlying dynamical process. In both cases we
shall see that the evolution of a quantum state $P_\psi=\varrho(0)$ has the not
so well-known pseudo-quadratic form
$\partial_t\varrho(t)=\mathbf{V}^\star\varrho(t)\mathbf{V}$ where $\mathbf{V}$
is a vector operator in a complex Minkowski space and the pseudo-adjoint
$\mathbf{V}^\star$ is induced by the Minkowski metric $\boldsymbol{\eta}$. The
interesting thing about this formalism is that its derivation has very deep
roots in a new understanding of the differential calculus of time. This
Minkowski-Hilbert representation of quantum dynamics is called the
\emph{Belavkin Formalism}; a beautiful, but not well understood theory of
mathematical physics that understands that both deterministic and stochastic
dynamics may be `unraveled' in a second-quantized Minkowski space. Working in
such a space provided the author with the means to construct a QS (quantum
stochastic) Duhamel principle and known applications to a Schr\&quot;odinger
dynamics perturbed by a continual measurement process are considered. What is
not known, but presented here, is the role of the Lorentz transform in quantum
measurement, and the appearance of Riemannian geometry in quantum measurement
is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2877</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2877</id><created>2014-07-10</created><authors><author><keyname>Casey</keyname><forenames>William</forenames></author><author><keyname>Shelmire</keyname><forenames>Aaron</forenames></author></authors><title>Signature Limits: An Entire Map of Clone Features and their Discovery in
  Nearly Linear Time</title><categories>cs.CR</categories><comments>17 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of creating entire and complete maps of software code
clones (copy features in data) in a corpus of binary artifacts of unknown
provenance. We report on a practical methodology, which employs enhanced suffix
data structures and partial orderings of clones to compute a compact
representation of most interesting clones features in data. The enumeration of
clone features is useful for malware triage and prioritization when human
exploration, testing and verification is the most costly factor. We further
show that the enhanced arrays may be used for discovery of provenance relations
in data and we introduce two distinct Jaccard similarity coefficients to
measure code similarity in binary artifacts. We illustrate the use of these
tools on real malware data including a retro-diction experiment for measuring
and enumerating evidence supporting common provenance in {\it Stuxnet} and {\it
Duqu}. The results indicate the practicality and efficacy of mapping completely
the clone features in data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2883</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2883</id><created>2014-07-10</created><authors><author><keyname>Singhal</keyname><forenames>Ayush</forenames></author><author><keyname>Roy</keyname><forenames>Atanu</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author></authors><title>Understanding Co-evolution in Large Multi-relational Social Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding dynamics of evolution in large social networks is an important
problem. In this paper, we characterize evolution in large multi-relational
social networks. The proliferation of online media such as Twitter, Facebook,
Orkut and MMORPGs\footnote{Massively Multi-player Online Role Playing Games}
have created social networking data at an unprecedented scale. Sony's Everquest
2 is one such example. We used game multi-relational networks to reveal the
dynamics of evolution in a multi-relational setting by macroscopic study of the
game network. Macroscopic analysis involves fragmenting the network into
smaller portions for studying the dynamics within these sub-networks, referred
to as `communities'. From an evolutionary perspective of multi-relational
network analysis, we have made the following contributions. Specifically, we
formulated and analyzed various metrics to capture evolutionary properties of
networks. We find that co-evolution rates in trust based `communities' are
approximately $60\%$ higher than the trade based `communities'. We also find
that the trust and trade connections within the `communities' reduce as their
size increases. Finally, we study the interrelation between the dynamics of
trade and trust within `communities' and find interesting results about the
precursor relationship between the trade and the trust dynamics within the
`communities'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2884</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2884</id><created>2014-07-10</created><updated>2014-09-27</updated><authors><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Minimum Input Selection for Structural Controllability</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a linear system $\dot{x} = Ax$, where $A$ is an $n \times n$ matrix
with $m$ nonzero entries, we consider the problem of finding the smallest set
of state variables to affect with an input so that the resulting system is
structurally controllable. We further assume we are given a set of &quot;forbidden
state variables&quot; $F$ which cannot be affected with an input and which we have
to avoid in our selection. Our main result is that this problem can be solved
deterministically in $O(n+m \sqrt{n})$ operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2889</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2889</id><created>2014-07-10</created><authors><author><keyname>Kouzinopoulos</keyname><forenames>Charalampos S.</forenames></author><author><keyname>Assael</keyname><forenames>John-Alexander M.</forenames></author><author><keyname>Pyrgiotis</keyname><forenames>Themistoklis K.</forenames></author><author><keyname>Margaritis</keyname><forenames>Konstantinos G.</forenames></author></authors><title>A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber
  Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence
  Database</title><categories>cs.DC cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple matching algorithms are used to locate the occurrences of patterns
from a finite pattern set in a large input string. Aho-Corasick and Wu-Manber,
two of the most well known algorithms for multiple matching require an
increased computing power, particularly in cases where large-size datasets must
be processed, as is common in computational biology applications. Over the past
years, Graphics Processing Units (GPUs) have evolved to powerful parallel
processors outperforming Central Processing Units (CPUs) in scientific
calculations. Moreover, multiple GPUs can be used in parallel, forming hybrid
computer cluster configurations to achieve an even higher processing
throughput. This paper evaluates the speedup of the parallel implementation of
the Aho-Corasick and Wu-Manber algorithms on a hybrid GPU cluster, when used to
process a snapshot of the Expressed Sequence Tags of the human genome and for
different problem parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2893</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2893</id><created>2014-07-10</created><updated>2016-01-05</updated><authors><author><keyname>Klug</keyname><forenames>Michael</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author></authors><title>Understanding the group dynamics and success of teams</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>19 pages, 4 figures, supporting information included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex problems often require coordinated group effort and can consume
significant resources, yet our understanding of how teams form and succeed has
been limited by a lack of large-scale, quantitative data. We analyze activity
traces and success levels for ~150,000 self-organized, online team projects.
While larger teams tend to be more successful, workload is highly focused
across the team, with only a few members performing most work. We find that
highly successful teams are significantly more focused than average teams of
the same size, that their members have worked on more diverse sets of projects,
and the members of highly successful teams are more likely to be core members
or 'leads' of other teams. The relations between team success and size, focus
and especially team experience cannot be explained by confounding factors such
as team age or external contributions from non-team members nor by group
mechanisms such as social loafing. Taken together, these features point to
organizational principles that may maximize the success of collaborative
endeavors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2896</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2896</id><created>2014-07-10</created><updated>2016-02-05</updated><authors><author><keyname>Li</keyname><forenames>Yanbo</forenames></author><author><keyname>Littlefield</keyname><forenames>Zakary</forenames></author><author><keyname>Bekris</keyname><forenames>Kostas E.</forenames></author></authors><title>Asymptotically Optimal Sampling-based Kinodynamic Planning</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling-based algorithms are viewed as practical solutions for
high-dimensional motion planning. Recent progress has taken advantage of random
geometric graph theory to show how asymptotic optimality can also be achieved
with these methods. Achieving this desirable property for systems with dynamics
requires solving a two-point boundary value problem (BVP) in the state space of
the underlying dynamical system. It is difficult, however, if not impractical,
to generate a BVP solver for a variety of important dynamical models of robots
or physically simulated ones. Thus, an open challenge was whether it was even
possible to achieve optimality guarantees when planning for systems without
access to a BVP solver. This work resolves the above question and describes how
to achieve asymptotic optimality for kinodynamic planning using incremental
sampling-based planners by introducing a new rigorous framework. Two new
methods, Stable Sparse-RRT (SST) and SST*, result from this analysis, which are
asymptotically near-optimal and optimal, respectively. The techniques are shown
to converge fast to high-quality paths, while they maintain only a sparse set
of samples, which makes them computationally efficient. The good performance of
the planners is confirmed by experimental results using dynamical systems
benchmarks, as well as physically simulated robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2899</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2899</id><created>2014-07-10</created><authors><author><keyname>Montoya</keyname><forenames>Gabriela</forenames><affiliation>LINA</affiliation></author><author><keyname>Skaf-Molli</keyname><forenames>Hala</forenames><affiliation>LINA</affiliation></author><author><keyname>Molli</keyname><forenames>Pascal</forenames><affiliation>LINA</affiliation></author><author><keyname>Vidal</keyname><forenames>Maria-Esther</forenames></author></authors><title>Fedra: Query Processing for SPARQL Federations with Divergence</title><categories>cs.DB</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data replication and deployment of local SPARQL endpoints improve scalability
and availability of public SPARQL endpoints, making the consumption of Linked
Data a reality. This solution requires synchronization and specific query
processing strategies to take advantage of replication. However, existing
replication aware techniques in federations of SPARQL endpoints do not consider
data dynamicity. We propose Fedra, an approach for querying federations of
endpoints that benefits from replication. Participants in Fedra federations can
copy fragments of data from several datasets, and describe them using
provenance and views. These descriptions enable Fedra to reduce the number of
selected endpoints while satisfying user divergence requirements. Experiments
on real-world datasets suggest savings of up to three orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2904</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2904</id><created>2014-07-10</created><authors><author><keyname>Honeine</keyname><forenames>Paul</forenames></author></authors><title>An eigenanalysis of data centering in machine learning</title><categories>stat.ML cs.CV cs.LG math.SP math.ST stat.TH</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many pattern recognition methods rely on statistical information from
centered data, with the eigenanalysis of an empirical central moment, such as
the covariance matrix in principal component analysis (PCA), as well as partial
least squares regression, canonical-correlation analysis and Fisher
discriminant analysis. Recently, many researchers advocate working on
non-centered data. This is the case for instance with the singular value
decomposition approach, with the (kernel) entropy component analysis, with the
information-theoretic learning framework, and even with nonnegative matrix
factorization. Moreover, one can also consider a non-centered PCA by using the
second-order non-central moment.
  The main purpose of this paper is to bridge the gap between these two
viewpoints in designing machine learning methods. To provide a study at the
cornerstone of kernel-based machines, we conduct an eigenanalysis of the inner
product matrices from centered and non-centered data. We derive several results
connecting their eigenvalues and their eigenvectors. Furthermore, we explore
the outer product matrices, by providing several results connecting the largest
eigenvectors of the covariance matrix and its non-centered counterpart. These
results lay the groundwork to several extensions beyond conventional centering,
with the weighted mean shift, the rank-one update, and the multidimensional
scaling. Experiments conducted on simulated and real data illustrate the
relevance of this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2905</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2905</id><created>2014-07-10</created><authors><author><keyname>Brown</keyname><forenames>Jed</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Smith</keyname><forenames>Barry F.</forenames></author></authors><title>Run-time extensibility and librarization of simulation software</title><categories>cs.SE cs.CE cs.MS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Build-time configuration and environment assumptions are hampering progress
and usability in scientific software. That which would be utterly unacceptable
in non-scientific software somehow passes for the norm in scientific packages.
The community needs reusable software packages that are easy use and flexible
enough to accommodate next-generation simulation and analysis demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2907</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2907</id><created>2014-07-10</created><authors><author><keyname>Goswami</keyname><forenames>Mayank</forenames></author><author><keyname>Gr&#xf8;nlund</keyname><forenames>Allan</forenames></author><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>Approximate Range Emptiness in Constant Time and Optimal Space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the \emph{$\varepsilon$-approximate range emptiness}
problem, where the task is to represent a set $S$ of $n$ points from
$\{0,\ldots,U-1\}$ and answer emptiness queries of the form &quot;$[a ; b]\cap S
\neq \emptyset$ ?&quot; with a probability of \emph{false positives} allowed. This
generalizes the functionality of \emph{Bloom filters} from single point queries
to any interval length $L$. Setting the false positive rate to $\varepsilon/L$
and performing $L$ queries, Bloom filters yield a solution to this problem with
space $O(n \lg(L/\varepsilon))$ bits, false positive probability bounded by
$\varepsilon$ for intervals of length up to $L$, using query time $O(L
\lg(L/\varepsilon))$. Our first contribution is to show that the space/error
trade-off cannot be improved asymptotically: Any data structure for answering
approximate range emptiness queries on intervals of length up to $L$ with false
positive probability $\varepsilon$, must use space $\Omega(n
\lg(L/\varepsilon)) - O(n)$ bits. On the positive side we show that the query
time can be improved greatly, to constant time, while matching our space lower
bound up to a lower order additive term. This result is achieved through a
succinct data structure for (non-approximate 1d) range emptiness/reporting
queries, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2912</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2912</id><created>2014-07-10</created><updated>2015-06-22</updated><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Malizia</keyname><forenames>Enrico</forenames></author></authors><title>Achieving New Upper Bounds for the Hypergraph Duality Problem through
  Logic</title><categories>cs.CC cs.DS cs.LO</categories><comments>Restructured a bit the presentation. Added a new appendix section</comments><acm-class>F.2.2; F.4.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hypergraph duality problem DUAL is defined as follows: given two simple
hypergraphs $\mathcal{G}$ and $\mathcal{H}$, decide whether $\mathcal{H}$
consists precisely of all minimal transversals of $\mathcal{G}$ (in which case
we say that $\mathcal{G}$ is the dual of $\mathcal{H}$). This problem is
equivalent to decide whether two given non-redundant monotone DNFs are dual. It
is known that non-DUAL, the complementary problem to DUAL, is in
$\mathrm{GC}(\log^2 n,\mathrm{PTIME})$, where $\mathrm{GC}(f(n),\mathcal{C})$
denotes the complexity class of all problems that after a nondeterministic
guess of $O(f(n))$ bits can be decided (checked) within complexity class
$\mathcal{C}$. It was conjectured that non-DUAL is in $\mathrm{GC}(\log^2
n,\mathrm{LOGSPACE})$. In this paper we prove this conjecture and actually
place the non-DUAL problem into the complexity class $\mathrm{GC}(\log^2
n,\mathrm{TC}^0)$ which is a subclass of $\mathrm{GC}(\log^2
n,\mathrm{LOGSPACE})$. We here refer to the logtime-uniform version of
$\mathrm{TC}^0$, which corresponds to $\mathrm{FO(COUNT)}$, i.e., first order
logic augmented by counting quantifiers. We achieve the latter bound in two
steps. First, based on existing problem decomposition methods, we develop a new
nondeterministic algorithm for non-DUAL that requires to guess $O(\log^2 n)$
bits. We then proceed by a logical analysis of this algorithm, allowing us to
formulate its deterministic part in $\mathrm{FO(COUNT)}$. From this result, by
the well known inclusion $\mathrm{TC}^0\subseteq\mathrm{LOGSPACE}$, it follows
that DUAL belongs also to $\mathrm{DSPACE}[\log^2 n]$. Finally, by exploiting
the principles on which the proposed nondeterministic algorithm is based, we
devise a deterministic algorithm that, given two hypergraphs $\mathcal{G}$ and
$\mathcal{H}$, computes in quadratic logspace a transversal of $\mathcal{G}$
missing in $\mathcal{H}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2917</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2917</id><created>2014-07-08</created><authors><author><keyname>Shastri</keyname><forenames>Bhavin J.</forenames></author><author><keyname>Tait</keyname><forenames>Alexander N.</forenames></author><author><keyname>Nahmias</keyname><forenames>Mitchell A.</forenames></author><author><keyname>Prucnal</keyname><forenames>Paul R.</forenames></author></authors><title>Photonic spike processing: ultrafast laser neurons and an integrated
  photonic network</title><categories>q-bio.NC cs.ET physics.optics</categories><comments>11 pages, 8 figures</comments><journal-ref>IEEE Photonics Society Newsletter, vol. 28, no. 3, pp. 4--11, Jun.
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The marriage of two vibrant fields---photonics and neuromorphic
processing---is fundamentally enabled by the strong analogies within the
underlying physics between the dynamics of biological neurons and lasers, both
of which can be understood within the framework of nonlinear dynamical systems
theory. Whereas neuromorphic engineering exploits the biophysics of neuronal
computation algorithms to provide a wide range of computing and signal
processing applications, photonics offer an alternative approach to
neuromorphic systems by exploiting the high speed, high bandwidth, and low
crosstalk available to photonic interconnects which potentially grants the
capacity for complex, ultrafast categorization and decision-making. Here we
highlight some recent progress on this exciting field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2918</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2918</id><created>2014-07-09</created><authors><author><keyname>Talukdar</keyname><forenames>Gitimoni</forenames></author><author><keyname>Borah</keyname><forenames>Pranjal Protim</forenames></author><author><keyname>Baruah</keyname><forenames>Arup</forenames></author></authors><title>A Survey of Named Entity Recognition in Assamese and other Indian
  Languages</title><categories>cs.CL</categories><comments>ICONACC-2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named Entity Recognition is always important when dealing with major Natural
Language Processing tasks such as information extraction, question-answering,
machine translation, document summarization etc so in this paper we put forward
a survey of Named Entities in Indian Languages with particular reference to
Assamese. There are various rule-based and machine learning approaches
available for Named Entity Recognition. At the very first of the paper we give
an idea of the available approaches for Named Entity Recognition and then we
discuss about the related research in this field. Assamese like other Indian
languages is agglutinative and suffers from lack of appropriate resources as
Named Entity Recognition requires large data sets, gazetteer list, dictionary
etc and some useful feature like capitalization as found in English cannot be
found in Assamese. Apart from this we also describe some of the issues faced in
Assamese while doing Named Entity Recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2919</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2919</id><created>2014-07-08</created><authors><author><keyname>Pan</keyname><forenames>Weike</forenames></author></authors><title>Collaborative Recommendation with Auxiliary Data: A Transfer Learning
  View</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent recommendation technology has been playing an increasingly
important role in various industry applications such as e-commerce product
promotion and Internet advertisement display. Besides users' feedbacks (e.g.,
numerical ratings) on items as usually exploited by some typical recommendation
algorithms, there are often some additional data such as users' social circles
and other behaviors. Such auxiliary data are usually related to users'
preferences on items behind the numerical ratings. Collaborative recommendation
with auxiliary data (CRAD) aims to leverage such additional information so as
to improve the personalization services, which have received much attention
from both researchers and practitioners.
  Transfer learning (TL) is proposed to extract and transfer knowledge from
some auxiliary data in order to assist the learning task on some target data.
In this paper, we consider the CRAD problem from a transfer learning view,
especially on how to achieve knowledge transfer from some auxiliary data.
First, we give a formal definition of transfer learning for CRAD (TL-CRAD).
Second, we extend the existing categorization of TL techniques (i.e., adaptive,
collective and integrative knowledge transfer algorithm styles) with three
knowledge transfer strategies (i.e., prediction rule, regularization and
constraint). Third, we propose a novel generic knowledge transfer framework for
TL-CRAD. Fourth, we describe some representative works of each specific
knowledge transfer strategy of each algorithm style in detail, which are
expected to inspire further works. Finally, we conclude the paper with some
summary discussions and several future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2921</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2921</id><created>2014-07-10</created><authors><author><keyname>Sarkis</keyname><forenames>Gabi</forenames></author><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Increasing the Speed of Polar List Decoders</title><categories>cs.IT math.IT</categories><comments>Submitted to the 2014 IEEE International Workshop on Signal
  Processing Systems (SiPS 2014)</comments><doi>10.1109/SiPS.2014.6986089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a simplified successive cancellation list decoder
that uses a Chase-like decoding process to achieve a six time improvement in
speed compared to successive cancellation list decoding while maintaining the
same error-correction performance advantage over standard
successive-cancellation polar decoders. We discuss the algorithm and detail the
data structures and methods used to obtain this speed-up. We also propose an
adaptive decoding algorithm that significantly improves the throughput while
retaining the error-correction performance. Simulation results over the
additive white Gaussian noise channel are provided and show that the proposed
system is up to 16 times faster than an LDPC decoder of the same frame size,
code rate, and similar error-correction performance, making it more suitable
for use as a software decoding solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2925</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2925</id><created>2014-07-09</created><authors><author><keyname>Ion</keyname><forenames>Stelian</forenames></author><author><keyname>Marinescu</keyname><forenames>Dorin</forenames></author><author><keyname>Cruceanu</keyname><forenames>Stefan-Gicu</forenames></author><author><keyname>Iordache</keyname><forenames>Virgil</forenames></author></authors><title>A data porting tool for coupling models with different discretization
  needs</title><categories>cs.MS</categories><journal-ref>Environmental Modelling &amp; Software 62 (2014) 240-252</journal-ref><doi>10.1016/j.envsoft.2014.09.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presented work is part of a larger research program dealing with
developing tools for coupling biogeochemical models in contaminated landscapes.
The specific objective of this article is to provide the researchers a tool to
build hexagonal raster using information from a rectangular raster data (e.g.
GIS format), data porting. This tool involves a computational algorithm and an
open source software (written in C). The method of extending the reticulated
functions defined on 2D networks is an essential key of this algorithm and can
also be used for other purposes than data porting. The algorithm allows one to
build the hexagonal raster with a cell size independent from the geometry of
the rectangular raster. The extended function is a bi-cubic spline which can
exactly reconstruct polynomials up to degree three in each variable. We
validate the method by analyzing errors in some theoretical case studies
followed by other studies with real terrain elevation data. We also introduce
and briefly present an iterative water routing method and use it for validation
on a case with concrete terrain data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2929</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2929</id><created>2014-07-10</created><authors><author><keyname>Curticapean</keyname><forenames>Radu</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Complexity of counting subgraphs: only the boundedness of the
  vertex-cover number counts</title><categories>cs.CC cs.DS</categories><comments>42 pages, 8 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a class $\mathcal{H}$ of graphs, #Sub$(\mathcal{H})$ is the counting
problem that, given a graph $H\in \mathcal{H}$ and an arbitrary graph $G$, asks
for the number of subgraphs of $G$ isomorphic to $H$. It is known that if
$\mathcal{H}$ has bounded vertex-cover number (equivalently, the size of the
maximum matching in $\mathcal{H}$ is bounded), then #Sub$(\mathcal{H})$ is
polynomial-time solvable. We complement this result with a corresponding lower
bound: if $\mathcal{H}$ is any recursively enumerable class of graphs with
unbounded vertex-cover number, then #Sub$(\mathcal{H})$ is #W[1]-hard
parameterized by the size of $H$ and hence not polynomial-time solvable and not
even fixed-parameter tractable, unless FPT = #W[1].
  As a first step of the proof, we show that counting $k$-matchings in
bipartite graphs is #W[1]-hard. Recently, Curticapean [ICALP 2013] proved the
#W[1]-hardness of counting $k$-matchings in general graphs; our result
strengthens this statement to bipartite graphs with a considerably simpler
proof and even shows that, assuming the Exponential Time Hypothesis (ETH),
there is no $f(k)n^{o(k/\log k)}$ time algorithm for counting $k$-matchings in
bipartite graphs for any computable function $f(k)$. As a consequence, we
obtain an independent and somewhat simpler proof of the classical result of
Flum and Grohe [SICOMP 2004] stating that counting paths of length $k$ is
#W[1]-hard, as well as a similar almost-tight ETH-based lower bound on the
exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2932</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2932</id><created>2014-07-10</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>h-index Research in Scientometrics: A Summary</title><categories>cs.DL</categories><comments>Accepted for publication in the Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Letter to the Editor shortly summing up ten or so years of research into
the h-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2961</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2961</id><created>2014-07-10</created><authors><author><keyname>Ghassabeh</keyname><forenames>Youness Aliyari</forenames></author></authors><title>On the Convergence of the Mean Shift Algorithm in the One-Dimensional
  Space</title><categories>cs.CV</categories><comments>13 pages, 10 figures, Published in Pattern Recognition Letters</comments><journal-ref>Pattern Recognition Letters, 2013, vol. 34(12)</journal-ref><doi>10.1016/j.patrec.2013.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mean shift algorithm is a non-parametric and iterative technique that has
been used for finding modes of an estimated probability density function. It
has been successfully employed in many applications in specific areas of
machine vision, pattern recognition, and image processing. Although the mean
shift algorithm has been used in many applications, a rigorous proof of its
convergence is still missing in the literature. In this paper we address the
convergence of the mean shift algorithm in the one-dimensional space and prove
that the sequence generated by the mean shift algorithm is a monotone and
convergent sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2970</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2970</id><created>2014-07-10</created><authors><author><keyname>Gathen</keyname><forenames>Joachim von zur</forenames></author><author><keyname>Ziegler</keyname><forenames>Konstantin</forenames></author></authors><title>Survey on counting special types of polynomials</title><categories>math.AC cs.SC</categories><comments>to appear in Jaime Gutierrez, Josef Schicho &amp; Martin Weimann
  (editors), Computer Algebra and Polynomials, Lecture Notes in Computer
  Science</comments><msc-class>11T06 (Primary), 05A15, 12Y05 (Secondary)</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most integers are composite and most univariate polynomials over a finite
field are reducible. The Prime Number Theorem and a classical result of
Gau{\ss} count the remaining ones, approximately and exactly.
  For polynomials in two or more variables, the situation changes dramatically.
Most multivariate polynomials are irreducible. This survey presents counting
results for some special classes of multivariate polynomials over a finite
field, namely the the reducible ones, the s-powerful ones (divisible by the
s-th power of a nonconstant polynomial), the relatively irreducible ones
(irreducible but reducible over an extension field), the decomposable ones, and
also for reducible space curves. These come as exact formulas and as
approximations with relative errors that essentially decrease exponentially in
the input size.
  Furthermore, a univariate polynomial f is decomposable if f = g o h for some
nonlinear polynomials g and h. It is intuitively clear that the decomposable
polynomials form a small minority among all polynomials. The tame case, where
the characteristic p of Fq does not divide n = deg f, is fairly
well-understood, and we obtain closely matching upper and lower bounds on the
number of decomposable polynomials. In the wild case, where p does divide n,
the bounds are less satisfactory, in particular when p is the smallest prime
divisor of n and divides n exactly twice. The crux of the matter is to count
the number of collisions, where essentially different (g, h) yield the same f.
We present a classification of all collisions at degree n = p^2 which yields an
exact count of those decomposable polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2971</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2971</id><created>2014-07-10</created><authors><author><keyname>Shimobaba</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Makowski</keyname><forenames>Michal</forenames></author><author><keyname>Kakue</keyname><forenames>Takashi</forenames></author><author><keyname>Okada</keyname><forenames>Naohisa</forenames></author><author><keyname>Endo</keyname><forenames>Yutaka</forenames></author><author><keyname>Hirayam</keyname><forenames>Ryuji</forenames></author><author><keyname>Hiyama</keyname><forenames>Daisuke</forenames></author><author><keyname>Hasegawa</keyname><forenames>Satoki</forenames></author><author><keyname>Nagahama</keyname><forenames>Yuki</forenames></author><author><keyname>Ito</keyname><forenames>Tomoyoshi</forenames></author></authors><title>Numerical investigation of lensless zoomable holographic multiple
  projections to tilted planes</title><categories>physics.optics cs.GR cs.MM</categories><doi>10.1016/j.optcom.2014.07.081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper numerically investigates the feasibility of lensless zoomable
holographic multiple projections to tilted planes. We have already developed
lensless zoomable holographic single projection using scaled diffraction, which
calculates diffraction between parallel planes with different sampling pitches.
The structure of this zoomable holographic projection is very simple because it
does not need a lens; however, it only projects a single image to a plane
parallel to the hologram. The lensless zoomable holographic projection in this
paper is capable of projecting multiple images onto tilted planes
simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2981</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2981</id><created>2014-07-10</created><authors><author><keyname>Khalil</keyname><forenames>Mohamed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author></authors><title>The asymmetric DoF Region for the 3-user MxN Interference Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the 3-user Gaussian MIMO interference channel is considered.
The asymmetric distribution of the DoF, where different users have different
number of DoF, is studied. Two cases are presented, the first is when all
transmitters and receivers have equal number of antennas $M$, the other when
the transmitters have $M$ antennas each, while the receivers have $N$ antennas
each. It is assumed that the channel coefficients are constant and known to all
transmitters and receivers. The region of the achievable DoF tuple $(d_1; d_2;
d_3)$ is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2987</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2987</id><created>2014-07-10</created><authors><author><keyname>Golge</keyname><forenames>Eren</forenames></author><author><keyname>Duygulu</keyname><forenames>Pinar</forenames></author></authors><title>FAME: Face Association through Model Evolution</title><categories>cs.CV cs.AI cs.IR cs.LG</categories><comments>Draft version of the study</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We attack the problem of learning face models for public faces from
weakly-labelled images collected from web through querying a name. The data is
very noisy even after face detection, with several irrelevant faces
corresponding to other people. We propose a novel method, Face Association
through Model Evolution (FAME), that is able to prune the data in an iterative
way, for the face models associated to a name to evolve. The idea is based on
capturing discriminativeness and representativeness of each instance and
eliminating the outliers. The final models are used to classify faces on novel
datasets with possibly different characteristics. On benchmark datasets, our
results are comparable to or better than state-of-the-art studies for the task
of face identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2988</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2988</id><created>2014-07-10</created><authors><author><keyname>Barthe</keyname><forenames>Gilles</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Arias</keyname><forenames>Emilio Jes&#xfa;s Gallego</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Kunz</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Proving differential privacy in Hoare logic</title><categories>cs.LO cs.CR</categories><comments>Published at the Computer Security Foundations Symposium (CSF), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a rigorous, worst-case notion of privacy-preserving
computation. Informally, a probabilistic program is differentially private if
the participation of a single individual in the input database has a limited
effect on the program's distribution on outputs. More technically, differential
privacy is a quantitative 2-safety property that bounds the distance between
the output distributions of a probabilistic program on adjacent inputs. Like
many 2-safety properties, differential privacy lies outside the scope of
traditional verification techniques. Existing approaches to enforce privacy are
based on intricate, non-conventional type systems, or customized relational
logics. These approaches are difficult to implement and often cumbersome to
use.
  We present an alternative approach that verifies differential privacy by
standard, non-relational reasoning on non-probabilistic programs. Our approach
transforms a probabilistic program into a non-probabilistic program which
simulates two executions of the original program. We prove that if the target
program is correct with respect to a Hoare specification, then the original
probabilistic program is differentially private. We provide a variety of
examples from the differential privacy literature to demonstrate the utility of
our approach. Finally, we compare our approach with existing verification
techniques for privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2989</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2989</id><created>2014-07-10</created><authors><author><keyname>Jayaweera</keyname><forenames>A. J. P. M. P.</forenames></author><author><keyname>Dias</keyname><forenames>N. G. J.</forenames></author></authors><title>Hidden Markov Model Based Part of Speech Tagger for Sinhala Language</title><categories>cs.CL</categories><comments>This paper contains 15 Pages, the paper was presented at ICONACC
  2014, organized by Manipur University, Imphal, India</comments><acm-class>I.2.7</acm-class><journal-ref>International Journal on Natural Language Computing (IJNLC) Vol.
  3, No.3, June 2014. Pages 9-23</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a fundamental lexical semantics of Sinhala language
and a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala
language. In any Natural Language processing task, Part of Speech is a very
vital topic, which involves analysing of the construction, behaviour and the
dynamics of the language, which the knowledge could utilized in computational
linguistics analysis and automation applications. Though Sinhala is a
morphologically rich and agglutinative language, in which words are inflected
with various grammatical features, tagging is very essential for further
analysis of the language. Our research is based on statistical based approach,
in which the tagging process is done by computing the tag sequence probability
and the word-likelihood probability from the given corpus, where the linguistic
knowledge is automatically extracted from the annotated corpus. The current
tagger could reach more than 90% of accuracy for known words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2990</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2990</id><created>2014-07-10</created><authors><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Achieving the Uniform Rate Region of General Multiple Access Channels by
  Polar Coding</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Communications, July 9, 2014. arXiv
  admin note: substantial text overlap with arXiv:1307.2889</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of polar coding for transmission over $m$-user
multiple access channels. In the proposed scheme, all users encode their
messages using a polar encoder, while a joint successive cancellation decoder
is deployed at the receiver. The encoding is done separately across the users
and is independent of the target achievable rate, in the sense that the encoder
core is the regular Ar{\i}kan's polarization matrix. For the code construction,
the positions of information bits and frozen bits for each of the users are
decided jointly. This is done by treating the whole polar transformation across
all the $m$ users as a single polar transformation with a certain base code. We
prove that the covering radius of the dominant face of the uniform rate region
is upper bounded by $r = \frac{(m-1)\sqrt{m}}{L}$, where $L$ represents the
length of the base code. We then prove that the proposed polar coding scheme
achieves the whole uniform rate region, with small enough resolution
characterized by $r$, by changing the decoding order in the joint successive
cancellation decoder. The encoding and decoding complexities are $O(N \log N)$,
where $N$ is the code block length, and the asymptotic block error probability
of $O(2^{-N^{0.5 - \epsilon}})$ is guaranteed. Examples of achievable rates for
the case of $3$-user multiple access channel are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.2991</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.2991</id><created>2014-07-10</created><updated>2015-08-29</updated><authors><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author><author><keyname>Shin</keyname><forenames>Junghwan</forenames></author></authors><title>Price of Anarchy with Heterogeneous Latency Functions</title><categories>cs.GT</categories><comments>totally 25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the price of anarchy (PoA) in multi-commodity flows
where the latency or delay function on an edge has a heterogeneous dependency
on the flow commodities, i.e. when the delay on each link is dependent on the
flow of individual commodities, rather than on the aggregate flow. An
application of this study is the performance analysis of a network with
differentiated traffic that may arise when traffic is prioritized according to
some type classification. This study has implications in the debate on
net-neutrality. We provide price of anarchy bounds for networks with $k$ (types
of) commodities where each link is associated with heterogeneous polynomial
delays, i.e. commodity $i$ on edge $e$ faces delay specified by
$g_{i1}(e)f^{\theta}_1(e) + g_{i2}(e)f^{\theta}_2(e) + \ldots +
g_{ik}(e)f^{\theta}_k(e) + c_i(e), $ where $f_i(e)$ is the flow of the $i$th
commodity through edge $e$, $\theta \in {\cal N}$, $g_{i1}(e), g_{i2}(e),
\ldots, g_{ik}(e)$ and $c_i(e)$ are nonnegative constants. We consider both
atomic and non-atomic flows.
  For networks with decomposable delay functions where the delay induced by a
particular commodity is the same, i.e. delays on edge $e$ are defined by
$a_1(e)f_1^\theta(e) + a_2(e)f_2^\theta(e) + \ldots + c(e)$ where $\forall j ,
\forall e: g_{1j}(e) = g_{2j}(e) = \ldots = a_j(e)$, we show an improved bound
on the price of anarchy.
  Further, we show bounds on the price of anarchy for uniform latency functions
where each edge of the network has the same delay function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3000</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3000</id><created>2014-07-10</created><authors><author><keyname>Szerlip</keyname><forenames>Paul</forenames></author><author><keyname>Stanley</keyname><forenames>Kenneth O.</forenames></author></authors><title>A Proposed Infrastructure for Adding Online Interaction to Any
  Evolutionary Domain</title><categories>cs.NE</categories><comments>Presented at WebAL-1: Workshop on Artificial Life and the Web 2014
  (arXiv:1406.2507)</comments><report-no>WebAL1/2014/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address the difficulty of creating online collaborative evolutionary
systems, this paper presents a new prototype library called Worldwide
Infrastructure for Neuroevolution (WIN) and its accompanying site WIN Online
(http://winark.org/). The WIN library is a collection of software packages
built on top of Node.js that reduce the complexity of creating fully
persistent, online, and interactive (or automated) evolutionary platforms
around any domain. WIN Online is the public interface for WIN, providing an
online collection of domains built with the WIN library that lets novice and
expert users browse and meaningfully contribute to ongoing experiments. The
long term goal of WIN is to make it trivial to connect any platform to the
world, providing both a stream of online users, and archives of data and
discoveries for later extension by humans or computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3004</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3004</id><created>2014-07-10</created><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Fasoulakis</keyname><forenames>Michail</forenames></author><author><keyname>Jurdzi&#x144;ski</keyname><forenames>Marcin</forenames></author></authors><title>Approximate well-supported Nash equilibria in symmetric bimatrix games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\varepsilon$-well-supported Nash equilibrium is a strong notion of
approximation of a Nash equilibrium, where no player has an incentive greater
than $\varepsilon$ to deviate from any of the pure strategies that she uses in
her mixed strategy. The smallest constant $\varepsilon$ currently known for
which there is a polynomial-time algorithm that computes an
$\varepsilon$-well-supported Nash equilibrium in bimatrix games is slightly
below $2/3$. In this paper we study this problem for symmetric bimatrix games
and we provide a polynomial-time algorithm that gives a
$(1/2+\delta)$-well-supported Nash equilibrium, for an arbitrarily small
positive constant $\delta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3006</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3006</id><created>2014-07-10</created><authors><author><keyname>Zareh</keyname><forenames>Mehran</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>Franceschelli</keyname><forenames>Mauro</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Seatzu</keyname><forenames>Carla</forenames></author></authors><title>Consensus in multi-agent systems with second-order dynamics and
  non-periodic sampled-data exchange</title><categories>cs.SY</categories><comments>The 19th IEEE International Conference on Emerging Technologies and
  Factory Automation (ETFA'2014), Barcelona (Spain)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper consensus in second-order multi-agent systems with a
non-periodic sampled-data exchange among agents is investigated. The sampling
is random with bounded inter-sampling intervals. It is assumed that each agent
has exact knowledge of its own state at all times. The considered local
interaction rule is PD-type. The characterization of the convergence properties
exploits a Lyapunov-Krasovskii functional method, sufficient conditions for
stability of the consensus protocol to a time-invariant value are derived.
Numerical simulations are presented to corroborate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3008</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3008</id><created>2014-07-10</created><updated>2015-07-09</updated><authors><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Staelin</keyname><forenames>Carl</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author><author><keyname>Yousefi</keyname><forenames>Arman</forenames></author></authors><title>Bigtable Merge Compaction</title><categories>cs.DS</categories><msc-class>68W27, 68P15, 68R05</msc-class><acm-class>F.1.2; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoSQL databases are widely used for massive data storage and real-time web
applications. Yet important aspects of these data structures are not well
understood. For example, NoSQL databases write most of their data to a
collection of files on disk, meanwhile periodically compacting subsets of these
files. A compaction policy must choose which files to compact, and when to
compact them, without knowing the future workload. Although these choices can
affect computational efficiency by orders of magnitude, existing literature
lacks tools for designing and analyzing online compaction policies --- policies
are now chosen largely by trial and error.
  Here we introduce tools for the design and analysis of compaction policies
for Google Bigtable, propose new policies, give average-case and worst-case
competitive analyses, and present preliminary empirical benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3015</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3015</id><created>2014-07-10</created><updated>2014-11-05</updated><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Nearly Linear-Work Algorithms for Mixed Packing/Covering and
  Facility-Location Linear Programs</title><categories>cs.DS</categories><msc-class>90-08, 90C05, 49M29, 65K05</msc-class><acm-class>F.2.1; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the first nearly linear-time approximation algorithms for
explicitly given mixed packing/covering linear programs, and for (non-metric)
fractional facility location. We also describe the first parallel algorithms
requiring only near-linear total work and finishing in polylog time. The
algorithms compute $(1+\epsilon)$-approximate solutions in time (and work)
$O^*(N/\epsilon^2)$, where $N$ is the number of non-zeros in the constraint
matrix. For facility location, $N$ is the number of eligible client/facility
pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3023</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3023</id><created>2014-07-11</created><updated>2014-11-08</updated><authors><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author><author><keyname>Yang</keyname><forenames>Xiu</forenames></author><author><keyname>Oseledets</keyname><forenames>Ivan V.</forenames></author><author><keyname>Karniadakis</keyname><forenames>George Em</forenames></author><author><keyname>Daniel</keyname><forenames>Luca</forenames></author></authors><title>Enabling High-Dimensional Hierarchical Uncertainty Quantification by
  ANOVA and Tensor-Train Decomposition</title><categories>cs.CE math.NA</categories><comments>14 pages (IEEE double column), 11 figure, accepted by IEEE Trans CAD
  of Integrated Circuits and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical uncertainty quantification can reduce the computational cost of
stochastic circuit simulation by employing spectral methods at different
levels. This paper presents an efficient framework to simulate hierarchically
some challenging stochastic circuits/systems that include high-dimensional
subsystems. Due to the high parameter dimensionality, it is challenging to both
extract surrogate models at the low level of the design hierarchy and to handle
them in the high-level simulation. In this paper, we develop an efficient
ANOVA-based stochastic circuit/MEMS simulator to extract efficiently the
surrogate models at the low level. In order to avoid the curse of
dimensionality, we employ tensor-train decomposition at the high level to
construct the basis functions and Gauss quadrature points. As a demonstration,
we verify our algorithm on a stochastic oscillator with four MEMS capacitors
and 184 random parameters. This challenging example is simulated efficiently by
our simulator at the cost of only 10 minutes in MATLAB on a regular personal
computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3026</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3026</id><created>2014-07-11</created><authors><author><keyname>Sundararajan</keyname><forenames>Ramasubramanian</forenames></author><author><keyname>Patel</keyname><forenames>Hima</forenames></author><author><keyname>Shanbhag</keyname><forenames>Dattesh</forenames></author><author><keyname>Vaidya</keyname><forenames>Vivek</forenames></author></authors><title>An SVM Based Approach for Cardiac View Planning</title><categories>cs.LG cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider the problem of automatically prescribing oblique planes (short
axis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging
(MRI). A concern with technologist-driven acquisitions of these planes is the
quality and time taken for the total examination. We propose an automated
solution incorporating anatomical features external to the cardiac region. The
solution uses support vector machine regression models wherein complexity and
feature selection are optimized using multi-objective genetic algorithms.
Additionally, we examine the robustness of our approach by training our models
on images with additive Rician-Gaussian mixtures at varying Signal to Noise
(SNR) levels. Our approach has shown promising results, with an angular
deviation of less than 15 degrees on 90% cases across oblique planes, measured
in terms of average 6-fold cross validation performance -- this is generally
within acceptable bounds of variation as specified by clinicians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3028</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3028</id><created>2014-07-11</created><updated>2014-12-10</updated><authors><author><keyname>Renault</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Ziliotto</keyname><forenames>Bruno</forenames></author></authors><title>Hidden Stochastic Games and Limit Equilibrium Payoffs</title><categories>math.OC cs.GT</categories><msc-class>91A05, 91A15, 91A20, 91A28</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider 2-player stochastic games with perfectly observed actions, and
study the limit, as the discount factor goes to one, of the equilibrium payoffs
set. In the usual setup where current states are observed by the players, we
show that the set of stationary equilibrium payoffs always converges, and
provide a simple example where the set of equilibrium payoffs has no limit. We
then introduce the more general model of hidden stochastic game, where the
players publicly receive imperfect signals over current states. In this setup
we present an example where not only the limit set of equilibrium payoffs does
not exist, but there is no converging selection of equilibrium payoffs. This
second example is robust in many aspects, in particular to perturbations of the
payoffs and to the introduction of correlation or communication devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3041</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3041</id><created>2014-07-11</created><updated>2014-07-31</updated><authors><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author><author><keyname>Laura</keyname><forenames>Luigi</forenames></author><author><keyname>Parotsidis</keyname><forenames>Nikos</forenames></author></authors><title>2-Edge Connectivity in Directed Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge and vertex connectivity are fundamental concepts in graph theory. While
they have been thoroughly studied in the case of undirected graphs,
surprisingly not much has been investigated for directed graphs. In this paper
we study $2$-edge connectivity problems in directed graphs and, in particular,
we consider the computation of the following natural relation: We say that two
vertices $v$ and $w$ are $2$-edge-connected if there are two edge-disjoint
paths from $v$ to $w$ and two edge-disjoint paths from $w$ to $v$. This
relation partitions the vertices into blocks such that all vertices in the same
block are $2$-edge-connected. Differently from the undirected case, those
blocks do not correspond to the $2$-edge-connected components of the graph. We
show how to compute this relation in linear time so that we can report in
constant time if two vertices are $2$-edge-connected. We also show how to
compute in linear time a sparse certificate for this relation, i.e., a subgraph
of the input graph that has $O(n)$ edges and maintains the same
$2$-edge-connected blocks as the input graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3058</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3058</id><created>2014-07-11</created><updated>2014-07-25</updated><authors><author><keyname>Matsumoto</keyname><forenames>Kengo</forenames></author></authors><title>Pushdown automata, lambda-graph systems and C*-algebras</title><categories>math.OA cs.FL math.DS</categories><comments>This paper has been withdrawn by the author due to some inaccuracies</comments><msc-class>Primary 37B10, 68Q40, Secondary 37A55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $\lambda$-graph system is a labeled Bratteli diagram with some additional
structure, which presents a subshift and yields a $C^*$-algebra. In this paper,
we construct a $\lambda$-graph system from a pushdown automaton, such that the
accepted language by the automaton coincides with the language of admissible
words of the presented subshift by the $\lambda$-graph system. The
$\lambda$-graph systems for pushdown automata accepting the languages of
Markov-Dyck shifts and sofic-Dyck shifts are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3063</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3063</id><created>2014-07-11</created><authors><author><keyname>Blomer</keyname><forenames>Jakob</forenames><affiliation>CERN</affiliation></author><author><keyname>Berzano</keyname><forenames>Dario</forenames><affiliation>CERN</affiliation></author><author><keyname>Buncic</keyname><forenames>Predrag</forenames><affiliation>CERN</affiliation></author><author><keyname>Charalampidis</keyname><forenames>Ioannis</forenames><affiliation>CERN</affiliation></author><author><keyname>Ganis</keyname><forenames>Gerardo</forenames><affiliation>CERN</affiliation></author><author><keyname>Lestaris</keyname><forenames>George</forenames><affiliation>CERN</affiliation></author><author><keyname>Meusel</keyname><forenames>Ren&#xe9;</forenames><affiliation>CERN</affiliation></author></authors><title>The Need for a Versioned Data Analysis Software Environment</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Scientific results in high-energy physics and in many other fields often rely
on complex software stacks. In order to support reproducibility and scrutiny of
the results, it is good practice to use open source software and to cite
software packages and versions. With ever-growing complexity of scientific
software on one side and with IT life-cycles of only a few years on the other
side, however, it turns out that despite source code availability the setup and
the validation of a minimal usable analysis environment can easily become
prohibitively expensive. We argue that there is a substantial gap between
merely having access to versioned source code and the ability to create a data
analysis runtime environment. In order to preserve all the different variants
of the data analysis runtime environment, we developed a snapshotting file
system optimized for software distribution. We report on our experience in
preserving the analysis environment for high-energy physics such as the
software landscape used to discover the Higgs boson at the Large Hadron
Collider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3064</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3064</id><created>2014-07-11</created><updated>2014-08-13</updated><authors><author><keyname>Lu</keyname><forenames>Zhenqi</forenames></author><author><keyname>Ying</keyname><forenames>Rendong</forenames></author><author><keyname>Jiang</keyname><forenames>Sumxin</forenames></author><author><keyname>Liu</keyname><forenames>Peilin</forenames></author><author><keyname>Yu</keyname><forenames>Wenxian</forenames></author></authors><title>Distributed Compressed Sensing off the Grid</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><doi>10.1109/LSP.2014.2349904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter investigates the joint recovery of a frequency-sparse signal
ensemble sharing a common frequency-sparse component from the collection of
their compressed measurements. Unlike conventional arts in compressed sensing,
the frequencies follow an off-the-grid formulation and are continuously valued
in $\left\lbrack 0,1 \right\rbrack$. As an extension of atomic norm, the
concatenated atomic norm minimization approach is proposed to handle the exact
recovery of signals, which is reformulated as a computationally tractable
semidefinite program. The optimality of the proposed approach is characterized
using a dual certificate. Numerical experiments are performed to illustrate the
effectiveness of the proposed approach and its advantage over separate
recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3068</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3068</id><created>2014-07-11</created><updated>2014-07-28</updated><authors><author><keyname>Stollenga</keyname><forenames>Marijn</forenames></author><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Gomez</keyname><forenames>Faustino</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Deep Networks with Internal Selective Attention through Feedback
  Connections</title><categories>cs.CV cs.LG cs.NE</categories><comments>13 pages, 3 figures</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional convolutional neural networks (CNN) are stationary and
feedforward. They neither change their parameters during evaluation nor use
feedback from higher to lower layers. Real brains, however, do. So does our
Deep Attention Selective Network (dasNet) architecture. DasNets feedback
structure can dynamically alter its convolutional filter sensitivities during
classification. It harnesses the power of sequential processing to improve
classification performance, by allowing the network to iteratively focus its
internal attention on some of its convolutional filters. Feedback is trained
through direct policy search in a huge million-dimensional parameter space,
through scalable natural evolution strategies (SNES). On the CIFAR-10 and
CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3077</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3077</id><created>2014-07-11</created><authors><author><keyname>Yoon</keyname><forenames>Yourim</forenames></author><author><keyname>Kim</keyname><forenames>Yong-Hyuk</forenames></author></authors><title>Charge Scheduling of an Energy Storage System under Time-of-use Pricing
  and a Demand Charge</title><categories>cs.NE</categories><comments>13 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3083</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3083</id><created>2014-07-11</created><authors><author><keyname>Demleitner</keyname><forenames>Markus</forenames></author><author><keyname>Greene</keyname><forenames>Gretchen</forenames></author><author><keyname>Sidaner</keyname><forenames>Pierre Le</forenames></author><author><keyname>Plante</keyname><forenames>Raymond L.</forenames></author></authors><title>The Virtual Observatory Registry</title><categories>astro-ph.IM cs.DL</categories><msc-class>68U35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Virtual Observatory (VO), the Registry provides the mechanism with
which users and applications discover and select resources -- typically, data
and services -- that are relevant for a particular scientific problem. Even
though the VO adopted technologies in particular from the bibliographic
community where available, building the Registry system involved a major
standardisation effort, involving about a dozen interdependent standard texts.
This paper discusses the server-side aspects of the standards and their
application, as regards the functional components (registries), the resource
records in both format and content, the exchange of resource records between
registries (harvesting), as well as the creation and management of the
identifiers used in the system based on the notion of authorities. Registry
record authors, registry operators or even advanced users thus receive a big
picture serving as a guideline through the body of relevant standard texts. To
complete this picture, we also mention common usage patterns and open issues as
appropriate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3091</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3091</id><created>2014-07-11</created><authors><author><keyname>Assi</keyname><forenames>Rawad Abou</forenames></author><author><keyname>Zaraket</keyname><forenames>Fadi A.</forenames></author><author><keyname>Masri</keyname><forenames>Wes</forenames></author></authors><title>UCov: a User-Defined Coverage Criterion for Test Case Intent
  Verification</title><categories>cs.SE</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of regression testing is to ensure that the behavior of existing
code is not altered by new program changes. The primary focus of regression
testing should be on code associated with: a) earlier bug fixes; and b)
particular application scenarios considered to be important by the tester.
Existing coverage criteria do not enable such focus, e.g., 100% branch coverage
does not guarantee that a given bug fix is exercised or a given application
scenario is tested. Therefore, there is a need for a complementary coverage
criterion in which the user can define a test requirement characterizing a
given behavior to be covered as opposed to choosing from a pool of pre-defined
and generic program elements. We propose UCov, a user-defined coverage
criterion wherein a test requirement is an execution pattern of program
elements and predicates. Our proposed criterion is not meant to replace
existing criteria, but to complement them as it focuses the testing on
important code patterns that could go untested otherwise. UCov supports test
case intent verification. For example, following a bug fix, the testing team
may augment the regression suite with the test case that revealed the bug.
However, this test case might become obsolete due to code modifications not
related to the bug. But if an execution pattern characterizing the bug was
defined by the user, UCov would determine that test case intent verification
failed. We implemented our methodology for the Java platform and applied it
onto two real life case studies. Our implementation comprises the following: 1)
an Eclipse plugin allowing the user to easily specify non-trivial test
requirements; 2) the ability of cross referencing test requirements across
subsequent versions of a given program; and 3) the ability of checking whether
user-defined test requirements were satisfied, i.e., test case intent
verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3121</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3121</id><created>2014-07-11</created><updated>2015-01-16</updated><authors><author><keyname>Keiren</keyname><forenames>Jeroen J. A.</forenames></author></authors><title>Benchmarks for Parity Games (extended version)</title><categories>cs.LO cs.GT</categories><comments>The corresponding tool and benchmarks are available from
  https://github.com/jkeiren/paritygame-generator. This is an extended version
  of the paper that has been accepted for FSEN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a benchmark suite for parity games that includes all benchmarks
that have been used in the literature, and make it available online. We give an
overview of the parity games, including a description of how they have been
generated. We also describe structural properties of parity games, and using
these properties we show that our benchmarks are representative. With this work
we provide a starting point for further experimentation with parity games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3123</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3123</id><created>2014-07-11</created><authors><author><keyname>Drees</keyname><forenames>Maximilian</forenames></author><author><keyname>Riechers</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Skopalik</keyname><forenames>Alexander</forenames></author></authors><title>Budget-restricted utility games with ordered strategic decisions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of budget games. Players choose a set of tasks and
each task has a certain demand on every resource in the game. Each resource has
a budget. If the budget is not enough to satisfy the sum of all demands, it has
to be shared between the tasks. We study strategic budget games, where the
budget is shared proportionally. We also consider a variant in which the order
of the strategic decisions influences the distribution of the budgets. The
complexity of the optimal solution as well as existence, complexity and quality
of equilibria are analyzed. Finally, we show that the time an ordered budget
game needs to convergence towards an equilibrium may be exponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3124</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3124</id><created>2014-07-11</created><updated>2014-08-22</updated><authors><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Tensor Networks for Big Data Analytics and Large-Scale Optimization
  Problems</title><categories>cs.NA math.NA</categories><comments>arXiv admin note: text overlap with arXiv:1403.2048</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we review basic and emerging models and associated algorithms
for large-scale tensor networks, especially Tensor Train (TT) decompositions
using novel mathematical and graphical representations. We discus the concept
of tensorization (i.e., creating very high-order tensors from lower-order
original data) and super compression of data achieved via quantized tensor
train (QTT) networks. The purpose of a tensorization and quantization is to
achieve, via low-rank tensor approximations &quot;super&quot; compression, and
meaningful, compact representation of structured data. The main objective of
this paper is to show how tensor networks can be used to solve a wide class of
big data optimization problems (that are far from tractable by classical
numerical methods) by applying tensorization and performing all operations
using relatively small size matrices and tensors and applying iteratively
optimized and approximative tensor contractions.
  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product
states (MPS), matrix product operators (MPO), basic tensor operations,
tensorization, distributed representation od data optimization problems for
very large-scale problems: generalized eigenvalue decomposition (GEVD),
PCA/SVD, canonical correlation analysis (CCA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3128</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3128</id><created>2014-07-11</created><authors><author><keyname>Kulacka</keyname><forenames>Agnieszka</forenames></author></authors><title>A tableau for set-satisfiability for extended fuzzy logic BL</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tableau calculus for finding a model for a
set-satisfiable finite set of formulas of an extended fuzzy logic BL, a fuzzy
logic BL with additional Baaz connective and the involutive negation, if such a
model exists. The calculus is a generalisation of a tableau calculus for BL,
which is based on the decomposition theorem for a continuous t-norm. The
aforementioned tableau calculus for BL is used to prove that a formula A of the
extended BL is valid with respect to all continuous t-norms or to find a
continuous t-norm * and assignment V of propositional atoms to [0,1] such that
*-evaluation V*(A)&lt;1. The tableau calculus presented in this paper enables for
a finite set of formulas F of the extended BL and a subset K of [0,1] to find a
continuous t-norm * and assignment V of propositional atoms to [0,1] such that
*-evaluation V*(A) belongs to K for all formulas A that belong to F, or
alternatively to show that such a model does not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3130</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3130</id><created>2014-07-11</created><updated>2014-07-16</updated><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Allocation in Practice</title><categories>cs.AI cs.CC cs.GT</categories><comments>To appear in Proc. of 37th edition of the German Conference on
  Artificial Intelligence (KI 2014), Springer LNCS</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How do we allocate scarcere sources? How do we fairly allocate costs? These
are two pressing challenges facing society today. I discuss two recent projects
at NICTA concerning resource and cost allocation. In the first, we have been
working with FoodBank Local, a social startup working in collaboration with
food bank charities around the world to optimise the logistics of collecting
and distributing donated food. Before we can distribute this food, we must
decide how to allocate it to different charities and food kitchens. This gives
rise to a fair division problem with several new dimensions, rarely considered
in the literature. In the second, we have been looking at cost allocation
within the distribution network of a large multinational company. This also has
several new dimensions rarely considered in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3144</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3144</id><created>2014-07-11</created><updated>2015-02-06</updated><authors><author><keyname>Ceccarello</keyname><forenames>Matteo</forenames></author><author><keyname>Pietracaprina</keyname><forenames>Andrea</forenames></author><author><keyname>Pucci</keyname><forenames>Geppino</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Space and Time Efficient Parallel Graph Decomposition, Clustering, and
  Diameter Approximation</title><categories>cs.DC cs.DS</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel parallel decomposition strategy for unweighted, undirected
graphs, based on growing disjoint connected clusters from batches of centers
progressively selected from yet uncovered nodes. With respect to similar
previous decompositions, our strategy exercises a tighter control on both the
number of clusters and their maximum radius.
  We present two important applications of our parallel graph decomposition:
(1) $k$-center clustering approximation; and (2) diameter approximation. In
both cases, we obtain algorithms which feature a polylogarithmic approximation
factor and are amenable to a distributed implementation that is geared for
massive (long-diameter) graphs. The total space needed for the computation is
linear in the problem size, and the parallel depth is substantially sublinear
in the diameter for graphs with low doubling dimension. To the best of our
knowledge, ours are the first parallel approximations for these problems which
achieve sub-diameter parallel time, for a relevant class of graphs, using only
linear space. Besides the theoretical guarantees, our algorithms allow for a
very simple implementation on clustered architectures: we report on extensive
experiments which demonstrate their effectiveness and efficiency on large
graphs as compared to alternative known approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3145</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3145</id><created>2014-07-11</created><authors><author><keyname>Waldon</keyname><forenames>Shawn M.</forenames></author><author><keyname>Thompson</keyname><forenames>Peter M.</forenames></author><author><keyname>Hahn</keyname><forenames>Patrick J.</forenames></author><author><keyname>Taylor</keyname><forenames>Russell M.</forenames><suffix>II</suffix></author></authors><title>SketchBio: A Scientist's 3D Interface for Molecular Modeling and
  Animation</title><categories>cs.GR</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Because of the difficulties involved in learning and using 3D
modeling and rendering software, many scientists hire programmers or animators
to create models and animations. This both slows the discovery process and
provides opportunities for miscommunication. Working with multiple
collaborators, we developed a set of design goals for a tool that would enable
them to directly construct models and animations. Results: We present
SketchBio, a tool that incorporates state-of-the-art bimanual interaction and
drop shadows to enable rapid construction of molecular structures and
animations. It includes three novel features: crystal by example, pose-mode
physics, and spring-based layout that accelerate operations common in the
formation of molecular models. We present design decisions and their
consequences, including cases where iterative design was required to produce
effective approaches. Conclusions: The design decisions, novel features, and
inclusion of state-of-the-art techniques enabled SketchBio to meet all of its
design goals. These features and decisions can be incorporated into existing
and new tools to improve their effectiveness
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3164</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3164</id><created>2014-07-11</created><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Marc</keyname><forenames>Tilen</forenames></author><author><keyname>Ostermeier</keyname><forenames>Lydia</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>The Relaxed Square Property</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph products are characterized by the existence of non-trivial equivalence
relations on the edge set of a graph that satisfy a so-called square property.
We investigate here a generalization, termed RSP-relations. The class of graphs
with non-trivial RSP-relations in particular includes graph bundles.
Furthermore, RSP-relations are intimately related with covering graph
constructions. For K_23-free graphs finest RSP-relations can be computed in
polynomial-time. In general, however, they are not unique and their number may
even grow exponentially. They behave well for graph products, however, in sense
that a finest RSP-relations can be obtained easily from finest RSP-relations on
the prime factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3175</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3175</id><created>2014-07-11</created><updated>2015-01-29</updated><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>Universal covers, color refinement, and two-variable counting logic:
  Lower bounds for the depth</title><categories>cs.LO cs.DC</categories><comments>25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a connected graph $G$ and its vertex $x$, let $U_x(G)$ denote the
universal cover of $G$ obtained by unfolding $G$ into a tree starting from $x$.
Let $T=T(n)$ be the minimum number such that, for graphs $G$ and $H$ with at
most $n$ vertices each, the isomorphism of $U_x(G)$ and $U_y(H)$ surely follows
from the isomorphism of these rooted trees truncated at depth $T$. Motivated by
applications in theory of distributed computing, Norris [Discrete Appl. Math.
1995] asks if $T(n)\le n$. We answer this question in the negative by
establishing that $T(n)=(2-o(1))n$. Our solution uses basic tools of finite
model theory such as a bisimulation version of the Immerman-Lander 2-pebble
counting game.
  The graphs $G_n$ and $H_n$ we construct to prove the lower bound for $T(n)$
also show some other tight lower bounds. Both having $n$ vertices, $G_n$ and
$H_n$ can be distinguished in 2-variable counting logic only with quantifier
depth $(1-o(1))n$. It follows that color refinement, the classical procedure
used in isomorphism testing and other areas for computing the coarsest
equitable partition of a graph, needs $(1-o(1))n$ rounds to achieve color
stabilization on each of $G_n$ and $H_n$. Somewhat surprisingly, this number of
rounds is not enough for color stabilization on the disjoint union of $G_n$ and
$H_n$, where $(2-o(1))n$ rounds are needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3176</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3176</id><created>2014-07-11</created><authors><author><keyname>Mansoor</keyname><forenames>Awais</forenames></author><author><keyname>Bagci</keyname><forenames>Ulas</forenames></author><author><keyname>Foster</keyname><forenames>Brent</forenames></author><author><keyname>Xu</keyname><forenames>Ziyue</forenames></author><author><keyname>Douglas</keyname><forenames>Deborah</forenames></author><author><keyname>Solomon</keyname><forenames>Jeffrey M.</forenames></author><author><keyname>Udupa</keyname><forenames>Jayaram K.</forenames></author><author><keyname>Mollura</keyname><forenames>Daniel J.</forenames></author></authors><title>CIDI-Lung-Seg: A Single-Click Annotation Tool for Automatic Delineation
  of Lungs from CT Scans</title><categories>cs.CV</categories><comments>4 pages, 6 figures; to appear in the proceedings of 36th Annual
  International Conference of the IEEE Engineering in Medicine and Biology
  Society (EMBC 2014)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Accurate and fast extraction of lung volumes from computed tomography (CT)
scans remains in a great demand in the clinical environment because the
available methods fail to provide a generic solution due to wide anatomical
variations of lungs and existence of pathologies. Manual annotation, current
gold standard, is time consuming and often subject to human bias. On the other
hand, current state-of-the-art fully automated lung segmentation methods fail
to make their way into the clinical practice due to their inability to
efficiently incorporate human input for handling misclassifications and praxis.
This paper presents a lung annotation tool for CT images that is interactive,
efficient, and robust. The proposed annotation tool produces an &quot;as accurate as
possible&quot; initial annotation based on the fuzzy-connectedness image
segmentation, followed by efficient manual fixation of the initial extraction
if deemed necessary by the practitioner. To provide maximum flexibility to the
users, our annotation tool is supported in three major operating systems
(Windows, Linux, and the Mac OS X). The quantitative results comparing our free
software with commercially available lung segmentation tools show higher degree
of consistency and precision of our software with a considerable potential to
enhance the performance of routine clinical tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3178</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3178</id><created>2014-07-11</created><authors><author><keyname>Xiong</keyname><forenames>Tingyao</forenames></author><author><keyname>Hall</keyname><forenames>Jonathan I.</forenames></author></authors><title>Modifications on Character Sequences and Construction of Large Even
  Length Binary Sequences</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been noticed that all the known binary sequences having the asymptotic
merit factor $\ge 6$ are the modifications to the real primitive characters. In
this paper, we give a new modification of the character sequences at length
$N=p_1p_2\dots p_r$, where $p_i$'s are distinct odd primes and $r$ is finite.
Based on these new modifications, for $N=p_1p_2\dots p_r$ with $p_i$'s distinct
odd primes, we can construct a binary sequence of length $2N$ with asymptotic
merit factor $6.0$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3179</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3179</id><created>2014-07-11</created><authors><author><keyname>Mansoor</keyname><forenames>Awais</forenames></author><author><keyname>Bagci</keyname><forenames>Ulas</forenames></author><author><keyname>Mollura</keyname><forenames>Daniel J.</forenames></author></authors><title>Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation</title><categories>cs.CV</categories><comments>4 pages, 5 figures; to appear in the proceedings of 36th Annual
  International Conference of the IEEE Engineering in Medicine and Biology
  Society (EMBC 2014)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Accurate delineation of pathological lungs from computed tomography (CT)
images remains mostly unsolved because available methods fail to provide a
reliable generic solution due to high variability of abnormality appearance.
Local descriptor-based classification methods have shown to work well in
annotating pathologies; however, these methods are usually computationally
intensive which restricts their widespread use in real-time or near-real-time
clinical applications. In this paper, we present a novel approach for fast,
accurate, reliable segmentation of pathological lungs from CT scans by
combining region-based segmentation method with local descriptor classification
that is performed on an optimized sampling grid. Our method works in two
stages; during stage one, we adapted the fuzzy connectedness (FC) image
segmentation algorithm to perform initial lung parenchyma extraction. In the
second stage, texture-based local descriptors are utilized to segment abnormal
imaging patterns using a near optimal keypoint analysis by employing centroid
of supervoxel as grid points. The quantitative results show that our
pathological lung segmentation method is fast, robust, and improves on current
standards and has potential to enhance the performance of routine clinical
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3189</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3189</id><created>2014-07-11</created><authors><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author></authors><title>A modern resistive magnetohydrodynamics solver using C++ and the Boost
  library</title><categories>cs.NA cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the implementation of our C++ resistive
magnetohydrodynamics solver. The framework developed facilitates the separation
of the code implementing the specific numerical method and the physical model,
on the one hand, from the handling of boundary conditions and the management of
the computational domain, on the other hand. In particular, this will allow us
to use finite difference stencils which are only defined in the interior of the
domain (the boundary conditions are handled automatically). We will discuss
this and other design considerations and their impact on performance in some
detail. In addition, we provide a documentation of the code developed and
demonstrate that a performance comparable to Fortran can be achieved, while
still maintaining a maximum of code readability and extensibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3191</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3191</id><created>2014-07-11</created><authors><author><keyname>Steorts</keyname><forenames>Rebecca C.</forenames></author><author><keyname>Ventura</keyname><forenames>Samuel L.</forenames></author><author><keyname>Sadinle</keyname><forenames>Mauricio</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E.</forenames></author></authors><title>A Comparison of Blocking Methods for Record Linkage</title><categories>cs.DB stat.AP</categories><comments>22 pages, 2 tables, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Record linkage seeks to merge databases and to remove duplicates when unique
identifiers are not available. Most approaches use blocking techniques to
reduce the computational complexity associated with record linkage. We review
traditional blocking techniques, which typically partition the records
according to a set of field attributes, and consider two variants of a method
known as locality sensitive hashing, sometimes referred to as &quot;private
blocking.&quot; We compare these approaches in terms of their recall, reduction
ratio, and computational complexity. We evaluate these methods using different
synthetic datafiles and conclude with a discussion of privacy-related issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3193</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3193</id><created>2014-07-11</created><authors><author><keyname>Mansoor</keyname><forenames>Awais</forenames></author><author><keyname>Bagci</keyname><forenames>Ulas</forenames></author><author><keyname>Mollura</keyname><forenames>Daniel J.</forenames></author></authors><title>Optimally Stabilized PET Image Denoising Using Trilateral Filtering</title><categories>cs.CV</categories><comments>8 pages, 3 figures; to appear in the Lecture Notes in Computer
  Science (MICCAI 2014)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Low-resolution and signal-dependent noise distribution in positron emission
tomography (PET) images makes denoising process an inevitable step prior to
qualitative and quantitative image analysis tasks. Conventional PET denoising
methods either over-smooth small-sized structures due to resolution limitation
or make incorrect assumptions about the noise characteristics. Therefore,
clinically important quantitative information may be corrupted. To address
these challenges, we introduced a novel approach to remove signal-dependent
noise in the PET images where the noise distribution was considered as
Poisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation
(GAT) was used to stabilize varying nature of the PET noise. Other than noise
stabilization, it is also desirable for the noise removal filter to preserve
the boundaries of the structures while smoothing the noisy regions. Indeed, it
is important to avoid significant loss of quantitative information such as
standard uptake value (SUV)-based metrics as well as metabolic lesion volume.
To satisfy all these properties, we extended bilateral filtering method into
trilateral filtering through multiscaling and optimal Gaussianization process.
The proposed method was tested on more than 50 PET-CT images from various
patients having different cancers and achieved the superior performance
compared to the widely used denoising techniques in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3200</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3200</id><created>2014-07-10</created><updated>2015-05-28</updated><authors><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>LIF</affiliation></author><author><keyname>Das</keyname><forenames>Shantanu</forenames><affiliation>LIF</affiliation></author><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames><affiliation>MPII</affiliation></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>INRIA Paris-Rocquencourt, LIAFA</affiliation></author><author><keyname>Labourel</keyname><forenames>Arnaud</forenames><affiliation>LIF</affiliation></author><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemyslaw</forenames></author></authors><title>Lock-in Problem for Parallel Rotor-router Walks</title><categories>cs.DM cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rotor-router model, also called the Propp machine, was introduced as a
deterministic alternative to the random walk. In this model, a group of
identical tokens are initially placed at nodes of the graph. Each node
maintains a cyclic ordering of the outgoing arcs, and during consecutive turns
the tokens are propagated along arcs chosen according to this ordering in
round-robin fashion. The behavior of the model is fully deterministic. Yanovski
et al.(2003) proved that a single rotor-router walk on any graph with m edges
and diameter $D$ stabilizes to a traversal of an Eulerian circuit on the set of
all 2m directed arcs on the edge set of the graph, and that such periodic
behaviour of the system is achieved after an initial transient phase of at most
2mD steps. The case of multiple parallel rotor-routers was studied
experimentally, leading Yanovski et al. to the conjecture that a system of $k
\textgreater{} 1$ parallel walks also stabilizes with a period of length at
most $2m$ steps. In this work we disprove this conjecture, showing that the
period of parallel rotor-router walks can in fact, be superpolynomial in the
size of graph. On the positive side, we provide a characterization of the
periodic behavior of parallel router walks, in terms of a structural property
of stable states called a subcycle decomposition. This property provides us the
tools to efficiently detect whether a given system configuration corresponds to
the transient or to the limit behavior of the system. Moreover, we provide
polynomial upper bounds of $O(m^4 D^2 + mD \log k)$ and $O(m^5 k^2)$ on the
number of steps it takes for the system to stabilize. Thus, we are able to
predict any future behavior of the system using an algorithm that takes
polynomial time and space. In addition, we show that there exists a separation
between the stabilization time of the single-walk and multiple-walk
rotor-router systems, and that for some graphs the latter can be asymptotically
larger even for the case of $k = 2$ walks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3208</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3208</id><created>2014-07-11</created><authors><author><keyname>Ruttenberg</keyname><forenames>Brian E.</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Decision-Making with Complex Data Structures using Probabilistic
  Programming</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing decision-theoretic reasoning frameworks such as decision networks
use simple data structures and processes. However, decisions are often made
based on complex data structures, such as social networks and protein
sequences, and rich processes involving those structures. We present a
framework for representing decision problems with complex data structures using
probabilistic programming, allowing probabilistic models to be created with
programming language constructs such as data structures and control flow. We
provide a way to use arbitrary data types with minimal effort from the user,
and an approximate decision-making algorithm that is effective even when the
information space is very large or infinite. Experimental results show our
algorithm working on problems with very large information spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3211</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3211</id><created>2014-07-09</created><authors><author><keyname>Karaaslan</keyname><forenames>Faruk</forenames></author></authors><title>Possibility neutrosophic soft sets with applications in decision making
  and similarity measure</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, concept of possibility neutrosophic soft set and its
operations are defined, and their properties are studied. An application of
this theory in decision making is investigated. Also a similarity measure of
two possibility neutrosophic soft sets is introduced and discussed. Finally an
application of this similarity measure is given to select suitable person for
position in a firm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3213</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3213</id><created>2014-07-09</created><authors><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>LIP</affiliation></author></authors><title>Symbolic Algorithms for Language Equivalence and Kleene Algebra with
  Tests</title><categories>cs.FL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first propose algorithms for checking language equivalence of finite
automata over a large alphabet. We use symbolic automata, where the transition
function is compactly represented using a (multi-terminal) binary decision
diagrams (BDD). The key idea consists in computing a bisimulation by exploring
reachable pairs symbolically, so as to avoid redundancies. This idea can be
combined with already existing optimisations, and we show in particular a nice
integration with the disjoint sets forest data-structure from Hopcroft and
Karp's standard algorithm. Then we consider Kleene algebra with tests (KAT), an
algebraic theory that can be used for verification in various domains ranging
from compiler optimisation to network programming analysis. This theory is
decidable by reduction to language equivalence of automata on guarded strings,
a particular kind of automata that have exponentially large alphabets. We
propose several methods allowing to construct symbolic automata out of KAT
expressions, based either on Brzozowski's derivatives or standard automata
constructions. All in all, this results in efficient algorithms for deciding
equivalence of KAT expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3234</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3234</id><created>2014-07-11</created><authors><author><keyname>Shen</keyname><forenames>Yi</forenames></author><author><keyname>Han</keyname><forenames>Bin</forenames></author><author><keyname>Braverman</keyname><forenames>Elena</forenames></author></authors><title>Image Inpainting Using Directional Tensor Product Complex Tight
  Framelets</title><categories>cs.IT cs.CV math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are particularly interested in the image inpainting problem
using directional complex tight wavelet frames. Under the assumption that frame
coefficients of images are sparse, several iterative thresholding algorithms
for the image inpainting problem have been proposed in the literature. The
outputs of such iterative algorithms are closely linked to solutions of several
convex minimization models using the balanced approach which simultaneously
combines the $l_1$-regularization for sparsity of frame coefficients and the
$l_2$-regularization for smoothness of the solution. Due to the redundancy of a
tight frame, elements of a tight frame could be highly correlated and
therefore, their corresponding frame coefficients of an image are expected to
close to each other. This is called the grouping effect in statistics. In this
paper, we establish the grouping effect property for frame-based convex
minimization models using the balanced approach. This result on grouping effect
partially explains the effectiveness of models using the balanced approach for
several image restoration problems. Inspired by recent development on
directional tensor product complex tight framelets (TP-CTFs) and their
impressive performance for the image denoising problem, in this paper we
propose an iterative thresholding algorithm using a single tight frame derived
from TP-CTFs for the image inpainting problem. Experimental results show that
our proposed algorithm can handle well both cartoons and textures
simultaneously and performs comparably and often better than several well-known
frame-based iterative thresholding algorithms for the image inpainting problem
without noise. For the image inpainting problem with additive zero-mean i.i.d.
Gaussian noise, our proposed algorithm using TP-CTFs performs superior than
other known state-of-the-art frame-based image inpainting algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3239</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3239</id><created>2014-06-25</created><authors><author><keyname>McCormack</keyname><forenames>Daniel</forenames></author></authors><title>Boolean Algebraic Programs as a Methodology for Symbolically
  Demonstrating Lower and Upper Bounds of Algorithms and Determinism</title><categories>cs.DS</categories><comments>13 pages</comments><msc-class>03G05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lower and upper bound of any given algorithm is one of the most crucial
pieces of information needed when evaluating the computational effectiveness
for said algorithm. Here a novel method of Boolean Algebraic Programming for
symbolic manipulation of Machines, Functions, and Inputs is presented which
allows for direct analysis of time complexities and proof of deterministic
methodologies. It is demonstrated through the analysis of a particular problem
which is proven and solved through the application of Boolean algebraic
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3242</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3242</id><created>2014-07-11</created><authors><author><keyname>La Rocca</keyname><forenames>Marcello</forenames></author></authors><title>Density Adaptive Parallel Clustering</title><categories>cs.DS cs.LG stat.ML</categories><comments>5 pages, 4 figures</comments><msc-class>91C20</msc-class><acm-class>H.3.3; D.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are going to introduce a new nearest neighbours based
approach to clustering, and compare it with previous solutions; the resulting
algorithm, which takes inspiration from both DBscan and minimum spanning tree
approaches, is deterministic but proves simpler, faster and doesnt require to
set in advance a value for k, the number of clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3247</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3247</id><created>2014-07-11</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Gudmundsson</keyname><forenames>Joachim</forenames></author><author><keyname>Mackenzie</keyname><forenames>Simon</forenames></author><author><keyname>Mattei</keyname><forenames>Nicholas</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Computational Aspects of Multi-Winner Approval Voting</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational aspects of three prominent voting rules that use
approval ballots to elect multiple winners. These rules are satisfaction
approval voting, proportional approval voting, and reweighted approval voting.
We first show that computing the winner for proportional approval voting is
NP-hard, closing a long standing open problem. As none of the rules are
strategyproof, even for dichotomous preferences, we study various strategic
aspects of the rules. In particular, we examine the computational complexity of
computing a best response for both a single agent and a group of agents. In
many settings, we show that it is NP-hard for an agent or agents to compute how
best to vote given a fixed set of approval ballots from the other agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3257</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3257</id><created>2014-07-11</created><updated>2014-12-22</updated><authors><author><keyname>Martinez-Mateo</keyname><forenames>Jesus</forenames></author><author><keyname>Pacher</keyname><forenames>Christoph</forenames></author><author><keyname>Peev</keyname><forenames>Momtchil</forenames></author><author><keyname>Ciurana</keyname><forenames>Alex</forenames></author><author><keyname>Martin</keyname><forenames>Vicente</forenames></author></authors><title>Demystifying the Information Reconciliation Protocol Cascade</title><categories>quant-ph cs.IT math.IT</categories><comments>30 pages, 13 figures, 3 tables</comments><journal-ref>Quantum Information &amp; Computation, vol. 15, no. 5&amp;6, pp. 453-477
  (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascade is an information reconciliation protocol proposed in the context of
secret key agreement in quantum cryptography. This protocol allows removing
discrepancies in two partially correlated sequences that belong to distant
parties, connected through a public noiseless channel. It is highly
interactive, thus requiring a large number of channel communications between
the parties to proceed and, although its efficiency is not optimal, it has
become the de-facto standard for practical implementations of information
reconciliation in quantum key distribution. The aim of this work is to analyze
the performance of Cascade, to discuss its strengths, weaknesses and
optimization possibilities, comparing with some of the modified versions that
have been proposed in the literature. When looking at all design trade-offs, a
new view emerges that allows to put forward a number of guidelines and propose
near optimal parameters for the practical implementation of Cascade improving
performance significantly in comparison with all previous proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3262</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3262</id><created>2014-06-25</created><authors><author><keyname>Boyer</keyname><forenames>Brice</forenames><affiliation>LJK</affiliation></author><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Giorgi</keyname><forenames>Pascal</forenames><affiliation>LIRMM</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Saunders</keyname><forenames>B. David</forenames><affiliation>CIS</affiliation></author></authors><title>Elements of Design for Containers and Solutions in the LinBox Library</title><categories>cs.MS cs.SC cs.SE</categories><comments>8 pages, 4th International Congress on Mathematical Software, Seoul :
  Korea, Republic Of (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe in this paper new design techniques used in the \cpp exact linear
algebra library \linbox, intended to make the library safer and easier to use,
while keeping it generic and efficient. First, we review the new simplified
structure for containers, based on our \emph{founding scope allocation} model.
We explain design choices and their impact on coding: unification of our matrix
classes, clearer model for matrices and submatrices, \etc Then we present a
variation of the \emph{strategy} design pattern that is comprised of a
controller--plugin system: the controller (solution) chooses among plug-ins
(algorithms) that always call back the controllers for subtasks. We give
examples using the solution \mul. Finally we present a benchmark architecture
that serves two purposes: Providing the user with easier ways to produce
graphs; Creating a framework for automatically tuning the library and
supporting regression testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3263</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3263</id><created>2014-07-11</created><updated>2014-09-15</updated><authors><author><keyname>An</keyname><forenames>Hyung-Chan</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>LP-Based Algorithms for Capacitated Facility Location</title><categories>cs.DS</categories><comments>25 pages, 6 figures; minor revisions</comments><msc-class>68W25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear programming has played a key role in the study of algorithms for
combinatorial optimization problems. In the field of approximation algorithms,
this is well illustrated by the uncapacitated facility location problem. A
variety of algorithmic methodologies, such as LP-rounding and primal-dual
method, have been applied to and evolved from algorithms for this problem.
Unfortunately, this collection of powerful algorithmic techniques had not yet
been applicable to the more general capacitated facility location problem. In
fact, all of the known algorithms with good performance guarantees were based
on a single technique, local search, and no linear programming relaxation was
known to efficiently approximate the problem.
  In this paper, we present a linear programming relaxation with constant
integrality gap for capacitated facility location. We demonstrate that the
fundamental theories of multi-commodity flows and matchings provide key
insights that lead to the strong relaxation. Our algorithmic proof of
integrality gap is obtained by finally accessing the rich toolbox of LP-based
methodologies: we present a constant factor approximation algorithm based on
LP-rounding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3267</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3267</id><created>2014-07-10</created><authors><author><keyname>Wu</keyname><forenames>Kan</forenames></author><author><keyname>Zhao</keyname><forenames>Ning</forenames></author></authors><title>Analysis and Approximation of Dual Tandem Queues with Finite Buffer
  Capacity</title><categories>cs.PF math.PR</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tandem queues with finite buffer capacity commonly exist in practical
applications. By viewing a tandem queue as an integrated system, an innovative
approach has been developed to analyze its performance through the insight from
reduction method. In our approach, the starvation at the bottleneck caused by
service time randomness is modeled and captured by interruptions. Fundamental
properties of tandem queues with finite buffer capacity are examined. We show
that in general system service rate of a dual tandem queue with finite buffer
capacity is equal or smaller than its bottleneck service rate, and virtual
interruptions, which are the extra idle period at the bottleneck caused by the
non-bottlenecks, depend on arrival rates. Hence, system service rate is a
function of arrival rate when the buffer capacity of a tandem queue is finite.
Approximation for the mean queue time of a dual tandem queue has been developed
through the concept of virtual interruptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3268</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3268</id><created>2014-07-11</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Examples for counterintuitive behavior of the new citation-rank
  indicator P100 for bibliometric evaluations</title><categories>cs.DL physics.soc-ph</categories><comments>9 pages, 5 tables, 4 figures; accepted for publication in Journal of
  Informetrics</comments><journal-ref>J. Informetrics 8, 738-748 (2014)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A new percentile-based rating scale P100 has recently been proposed to
describe the citation impact in terms of the distribution of the unique
citation values. Here I investigate P100 for 5 example datasets, two simple
fictitious models and three larger empirical samples. Counterintuitive behavior
is demonstrated in the model datasets, pointing to difficulties when the
evolution with time of the indicator is analyzed or when different fields or
publication years are compared. It is shown that similar problems can occur for
the three larger datasets of empirical citation values. Further, it is observed
that the performance evalution result in terms of percentiles can be influenced
by selecting different journals for publication of a manuscript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3269</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3269</id><created>2014-07-11</created><authors><author><keyname>Ren</keyname><forenames>Guanjiao</forenames></author><author><keyname>Chen</keyname><forenames>Weihai</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sakyasingha</forenames></author><author><keyname>Kolodziejski</keyname><forenames>Christoph</forenames></author><author><keyname>W&#xf6;rg&#xf6;tter</keyname><forenames>Florentin</forenames></author><author><keyname>Manoonpong</keyname><forenames>Poramate</forenames></author></authors><title>Multiple chaotic central pattern generators with learning for legged
  locomotion and malfunction compensation</title><categories>cs.AI cs.LG cs.NE cs.RO</categories><comments>48 pages, 16 figures, Information Sciences 2014</comments><acm-class>I.2.9; I.2.6</acm-class><doi>10.1016/j.ins.2014.05.001</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An originally chaotic system can be controlled into various periodic
dynamics. When it is implemented into a legged robot's locomotion control as a
central pattern generator (CPG), sophisticated gait patterns arise so that the
robot can perform various walking behaviors. However, such a single chaotic CPG
controller has difficulties dealing with leg malfunction. Specifically, in the
scenarios presented here, its movement permanently deviates from the desired
trajectory. To address this problem, we extend the single chaotic CPG to
multiple CPGs with learning. The learning mechanism is based on a simulated
annealing algorithm. In a normal situation, the CPGs synchronize and their
dynamics are identical. With leg malfunction or disability, the CPGs lose
synchronization leading to independent dynamics. In this case, the learning
mechanism is applied to automatically adjust the remaining legs' oscillation
frequencies so that the robot adapts its locomotion to deal with the
malfunction. As a consequence, the trajectory produced by the multiple chaotic
CPGs resembles the original trajectory far better than the one produced by only
a single CPG. The performance of the system is evaluated first in a physical
simulation of a quadruped as well as a hexapod robot and finally in a real
six-legged walking machine called AMOSII. The experimental results presented
here reveal that using multiple CPGs with learning is an effective approach for
adaptive locomotion generation where, for instance, different body parts have
to perform independent movements for malfunction compensation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3286</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3286</id><created>2014-07-11</created><authors><author><keyname>Sastry</keyname><forenames>Srikanth</forenames></author><author><keyname>Widder</keyname><forenames>Josef</forenames></author></authors><title>Solvability-Based Comparison of Failure Detectors</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Failure detectors are oracles that have been introduced to provide processes
in asynchronous systems with information about faults. This information can
then be used to solve problems otherwise unsolvable in asynchronous systems. A
natural question is on the &quot;minimum amount of information&quot; a failure detector
has to provide for a given problem. This question is classically addressed
using a relation that states that a failure detector D is stronger (that is,
provides &quot;more, or better, information&quot;) than a failure detector D' if D can be
used to implement D'. It has recently been shown that this classic
implementability relation has some drawbacks. To overcome this, different
relations have been defined, one of which states that a failure detector D is
stronger than D' if D can solve all the time-free problems solvable by D'. In
this paper we compare the implementability-based hierarchy of failure detectors
to the hierarchy based on solvability. This is done by introducing a new proof
technique for establishing the solvability relation. We apply this technique to
known failure detectors from the literature and demonstrate significant
differences between the hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3289</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3289</id><created>2014-07-11</created><updated>2014-10-31</updated><authors><author><keyname>Wager</keyname><forenames>Stefan</forenames></author><author><keyname>Fithian</keyname><forenames>William</forenames></author><author><keyname>Wang</keyname><forenames>Sida</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Altitude Training: Strong Bounds for Single-Layer Dropout</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Advances in Neural Information Processing Systems (NIPS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout training, originally designed for deep neural networks, has been
successful on high-dimensional single-layer natural language tasks. This paper
proposes a theoretical explanation for this phenomenon: we show that, under a
generative Poisson topic model with long documents, dropout training improves
the exponent in the generalization bound for empirical risk minimization.
Dropout achieves this gain much like a marathon runner who practices at
altitude: once a classifier learns to perform reasonably well on training
examples that have been artificially corrupted by dropout, it will do very well
on the uncorrupted test set. We also show that, under similar conditions,
dropout preserves the Bayes decision boundary and should therefore induce
minimal bias in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3324</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3324</id><created>2014-07-11</created><authors><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Effects of Mobility on User Energy Consumption and Total Throughput in a
  Massive MIMO System</title><categories>cs.IT math.IT</categories><comments>Submitted to ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Macroscopic mobility of wireless users is important to determine the
performance and energy effciency of a wireless network, because of the temporal
correlations it introduces in the consumed power and throughput. In this work
we introduce a methodology that obtains the long time statistics of such
metrics in a network. After describing the general approach, we present a
specific example of the uplink channel of a mobile user in the vicinity of a
massive MIMO base-station antenna array. To guarantee a fixed SINR and rate,
the user inverts the path-loss channel power, while moving around in the cell.
To calculate the long time distribution of the consumed energy of the user, we
assume his movement follows a Brownian motion, and then map the problem to the
solution of the minimum eigenvalue of a partial differential equation, which
can be solved either analytically, or numerically very fast. We also treat the
throughput of a single user. We then discuss the results and how they can be
generalized if the mobility is assumed to be a Levy random walk. We also
provide a roadmap to use this technique when one considers multiple users and
base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3334</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3334</id><created>2014-07-11</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Offline to Online Conversion</title><categories>cs.LG cs.IT math.IT math.ST stat.CO stat.TH</categories><comments>20 LaTeX pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of converting offline estimators into an online
predictor or estimator with small extra regret. Formally this is the problem of
merging a collection of probability measures over strings of length 1,2,3,...
into a single probability measure over infinite sequences. We describe various
approaches and their pros and cons on various examples. As a side-result we
give an elementary non-heuristic purely combinatoric derivation of Turing's
famous estimator. Our main technical contribution is to determine the
computational complexity of online estimators with good guarantees in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3335</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3335</id><created>2014-07-11</created><authors><author><keyname>Zheng</keyname><forenames>Yuanshi</forenames></author><author><keyname>Ma</keyname><forenames>Jingying</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Consensus of switched multi-agent systems</title><categories>cs.SY</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the consensus problem of switched multi-agent
system composed of continuous-time and discrete-time subsystems. By combining
the classical consensus protocols of continuous-time and discrete-time
multi-agent systems, we propose a linear consensus protocol for switched
multi-agent system. Based on the graph theory and Lyapunov theory, we prove
that the consensus of switched multi-agent system is solvable under arbitrary
switching with undirected connected graph, directed graph and switching
topologies, respectively. Simulation examples are also provided to demonstrate
the effectiveness of the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3338</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3338</id><created>2014-07-11</created><authors><author><keyname>Bhawalkar</keyname><forenames>Kshipra</forenames></author><author><keyname>Hummel</keyname><forenames>Patrick</forenames></author><author><keyname>Vassilvitskii</keyname><forenames>Sergei</forenames></author></authors><title>Value of Targeting</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We undertake a formal study of the value of targeting data to an advertiser.
As expected, this value is increasing in the utility difference between
realizations of the targeting data and the accuracy of the data, and depends on
the distribution of competing bids. However, this value may vary
non-monotonically with an advertiser's budget. Similarly, modeling the values
as either private or correlated, or allowing other advertisers to also make use
of the data, leads to unpredictable changes in the value of data. We address
questions related to multiple data sources, show that utility of additional
data may be non-monotonic, and provide tradeoffs between the quality and the
price of data sources. In a game-theoretic setting, we show that advertisers
may be worse off than if the data had not been available at all. We also ask
whether a publisher can infer the value an advertiser would place on targeting
data from the advertiser's bidding behavior and illustrate that this is
impossible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3340</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3340</id><created>2014-07-12</created><authors><author><keyname>Chang</keyname><forenames>Hsien-Chih</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>Detecting Weakly Simple Polygons</title><categories>cs.CG</categories><comments>25 pages and 13 figures, submitted to SODA 2015</comments><doi>10.1137/1.9781611973730.110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A closed curve in the plane is weakly simple if it is the limit (in the
Fr\'echet metric) of a sequence of simple closed curves. We describe an
algorithm to determine whether a closed walk of length n in a simple plane
graph is weakly simple in O(n log n) time, improving an earlier O(n^3)-time
algorithm of Cortese et al. [Discrete Math. 2009]. As an immediate corollary,
we obtain the first efficient algorithm to determine whether an arbitrary
n-vertex polygon is weakly simple; our algorithm runs in O(n^2 log n) time. We
also describe algorithms that detect weak simplicity in O(n log n) time for two
interesting classes of polygons. Finally, we discuss subtle errors in several
previously published definitions of weak simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3341</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3341</id><created>2014-07-12</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Extreme State Aggregation Beyond MDPs</title><categories>cs.AI cs.LG</categories><comments>28 LaTeX pages. 8 Theorems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Reinforcement Learning setup where an agent interacts with an
environment in observation-reward-action cycles without any (esp.\ MDP)
assumptions on the environment. State aggregation and more generally feature
reinforcement learning is concerned with mapping histories/raw-states to
reduced/aggregated states. The idea behind both is that the resulting reduced
process (approximately) forms a small stationary finite-state MDP, which can
then be efficiently solved or learnt. We considerably generalize existing
aggregation results by showing that even if the reduced process is not an MDP,
the (q-)value functions and (optimal) policies of an associated MDP with same
state-space size solve the original problem, as long as the solution can
approximately be represented as a function of the reduced states. This implies
an upper bound on the required state space size that holds uniformly for all RL
problems. It may also explain why RL algorithms designed for MDPs sometimes
perform well beyond MDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3342</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3342</id><created>2014-07-12</created><authors><author><keyname>Elmasry</keyname><forenames>Amr</forenames></author><author><keyname>Juhl</keyname><forenames>Daniel Dahl</forenames></author><author><keyname>Katajainen</keyname><forenames>Jyrki</forenames></author><author><keyname>Satti</keyname><forenames>Srinivasa Rao</forenames></author></authors><title>Selection from read-only memory with limited workspace</title><categories>cs.DS</categories><comments>16 pages, 1 figure, Preliminary version appeared in COCOON-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an unordered array of $N$ elements drawn from a totally ordered set and
an integer $k$ in the range from $1$ to $N$, in the classic selection problem
the task is to find the $k$-th smallest element in the array. We study the
complexity of this problem in the space-restricted random-access model: The
input array is stored on read-only memory, and the algorithm has access to a
limited amount of workspace. We prove that the linear-time prune-and-search
algorithm---presented in most textbooks on algorithms---can be modified to use
$\Theta(N)$ bits instead of $\Theta(N)$ words of extra space. Prior to our
work, the best known algorithm by Frederickson could perform the task with
$\Theta(N)$ bits of extra space in $O(N \lg^{*} N)$ time. Our result separates
the space-restricted random-access model and the multi-pass streaming model,
since we can surpass the $\Omega(N \lg^{*} N)$ lower bound known for the latter
model. We also generalize our algorithm for the case when the size of the
workspace is $\Theta(S)$ bits, where $\lg^3{N} \leq S \leq N$. The running time
of our generalized algorithm is $O(N \lg^{*}(N/S) + N (\lg N) / \lg{} S)$,
slightly improving over the $O(N \lg^{*}(N (\lg N)/S) + N (\lg N) / \lg{} S)$
bound of Frederickson's algorithm. To obtain the improvements mentioned above,
we developed a new data structure, called the wavelet stack, that we use for
repeated pruning. We expect the wavelet stack to be a useful tool in other
applications as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3345</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3345</id><created>2014-07-12</created><updated>2016-02-24</updated><authors><author><keyname>Sarkar</keyname><forenames>Camellia</forenames></author><author><keyname>Jalan</keyname><forenames>Sarika</forenames></author></authors><title>Social patterns revealed through random matrix theory</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><comments>22 pages, 7 figures</comments><journal-ref>EPL 108, 48003 (2014)</journal-ref><doi>10.1209/0295-5075/108/48003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the tremendous advancements in the field of network theory, very few
studies have taken weights in the interactions into consideration that emerge
naturally in all real world systems. Using random matrix analysis of a weighted
social network, we demonstrate the profound impact of weights in interactions
on emerging structural properties. The analysis reveals that randomness
existing in particular time frame affects the decisions of individuals
rendering them more freedom of choice in situations of financial security.
While the structural organization of networks remain same throughout all
datasets, random matrix theory provides insight into interaction pattern of
individual of the society in situations of crisis. It has also been
contemplated that individual accountability in terms of weighted interactions
remains as a key to success unless segregation of tasks comes into play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3347</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3347</id><created>2014-07-12</created><authors><author><keyname>Thakur</keyname><forenames>Ajay Kumar</forenames></author><author><keyname>Banik</keyname><forenames>Biswajit</forenames></author><author><keyname>Pant</keyname><forenames>Pankaj Kumar</forenames></author><author><keyname>Sankini</keyname><forenames>Dhanashree</forenames></author><author><keyname>Walia</keyname><forenames>Dipesh</forenames></author><author><keyname>Milkoori</keyname><forenames>Renuka</forenames></author></authors><title>Case Study Of GIPSY and MARF</title><categories>cs.SE</categories><comments>46 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metrics are used mainly to predict software engineering efforts such as
maintenance effort, error Prone ness, and error rate. This document emphasis on
experimental study based on two open source systems namely MARF and GIPSY. With
the help of various research papers we were able to analyze and give priorities
to various metrics that are implemented with JDeodrant. LOGISCOPE and McCabe
tools are used to identify problematic classes with help of Kiviat graph and
average Cyclomatic Complexity that further are implemented with highest
priority metric with JDeodrant. To obtain accurate results we collected data
using different tools. The analysis of the two systems is done as a conclusion
of study using different tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3360</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3360</id><created>2014-07-12</created><authors><author><keyname>Harvey</keyname><forenames>David</forenames></author><author><keyname>van der Hoeven</keyname><forenames>Joris</forenames></author><author><keyname>Lecerf</keyname><forenames>Gr&#xe9;goire</forenames></author></authors><title>Even faster integer multiplication</title><categories>cs.CC cs.SC math.NT</categories><msc-class>68W30, 68Q17, 68W40</msc-class><acm-class>G.1.0; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new proof of F\&quot;urer's bound for the cost of multiplying n-bit
integers in the bit complexity model. Unlike F\&quot;urer, our method does not
require constructing special coefficient rings with &quot;fast&quot; roots of unity.
Moreover, we prove the more explicit bound O(n log n K^(log^* n))$ with K = 8.
We show that an optimised variant of F\&quot;urer's algorithm achieves only K = 16,
suggesting that the new algorithm is faster than F\&quot;urer's by a factor of
2^(log^* n). Assuming standard conjectures about the distribution of Mersenne
primes, we give yet another algorithm that achieves K = 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3361</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3361</id><created>2014-07-12</created><authors><author><keyname>Harvey</keyname><forenames>David</forenames></author><author><keyname>van der Hoeven</keyname><forenames>Joris</forenames></author><author><keyname>Lecerf</keyname><forenames>Gr&#xe9;goire</forenames></author></authors><title>Faster polynomial multiplication over finite fields</title><categories>cs.CC cs.SC math.NT</categories><msc-class>68W30, 68Q17, 68W40</msc-class><acm-class>G.1.0; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let p be a prime, and let M_p(n) denote the bit complexity of multiplying two
polynomials in F_p[X] of degree less than n. For n large compared to p, we
establish the bound M_p(n) = O(n log n 8^(log^* n) log p), where log^* is the
iterated logarithm. This is the first known F\&quot;urer-type complexity bound for
F_p[X], and improves on the previously best known bound M_p(n) = O(n log n log
log n log p).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3366</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3366</id><created>2014-07-12</created><authors><author><keyname>Bagaria</keyname><forenames>Sankalp</forenames></author></authors><title>Authenticating Transactions using Bank-Verified Biometrics</title><categories>cs.CR cs.CY</categories><comments>3 pages</comments><msc-class>00B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a scheme by which banks can collect and verify
biometric data eg, fingerprints, directly from their customers and use it to
authenticate their transactions made through PoS/ ATM/ online console. We
propose building a network of computers called BioNet to allow such
transactions to be made online across the world. A BioNet server will be able
to do 4 million transactions per second using GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3373</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3373</id><created>2014-07-12</created><authors><author><keyname>Jia</keyname><forenames>Yuhan</forenames></author><author><keyname>Wu</keyname><forenames>Jianping</forenames></author><author><keyname>Du</keyname><forenames>Yiman</forenames></author><author><keyname>Qi</keyname><forenames>Geqi</forenames></author></authors><title>Car-following model on two lanes and stability analysis</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering lateral influence from adjacent lane, an improved car-following
model is developed in this paper. Then linear and non-linear stability analyses
are carried out. The modified Korteweg-de Vries (MKdV) equation is derived with
the kink-antikink soliton solution. Numerical simulations are implemented and
the result shows good consistency with theoretical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3374</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3374</id><created>2014-07-12</created><authors><author><keyname>Jia</keyname><forenames>Yuhan</forenames></author><author><keyname>Wu</keyname><forenames>Jianping</forenames></author><author><keyname>Du</keyname><forenames>Yiman</forenames></author></authors><title>An improved car-following model considering variable safety headway
  distance</title><categories>cs.NA</categories><doi>10.4006/0836-1398-27.4.616</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering high speed following on expressway or highway, an improved
car-following model is developed in this paper by introducing variable safety
headway distance. Stability analysis of the new model is carried out using the
control theory method. Finally, numerical simulations are implemented and the
results show good consistency with theoretical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3377</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3377</id><created>2014-07-12</created><authors><author><keyname>Edelkamp</keyname><forenames>Stefan</forenames></author><author><keyname>Katajainen</keyname><forenames>Jyrki</forenames></author><author><keyname>Elmasry</keyname><forenames>Amr</forenames></author></authors><title>Strengthened Lazy Heaps: Surpassing the Lower Bounds for Binary Heaps</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $n$ denote the number of elements currently in a data structure. An
in-place heap is stored in the first $n$ locations of an array, uses $O(1)$
extra space, and supports the operations: minimum, insert, and extract-min. We
introduce an in-place heap, for which minimum and insert take $O(1)$ worst-case
time, and extract-min takes $O(\lg{} n)$ worst-case time and involves at most
$\lg{} n + O(1)$ element comparisons. The achieved bounds are optimal to within
additive constant terms for the number of element comparisons. In particular,
these bounds for both insert and extract-min -and the time bound for insert-
surpass the corresponding lower bounds known for binary heaps, though our data
structure is similar. In a binary heap, when viewed as a nearly complete binary
tree, every node other than the root obeys the heap property, i.e. the element
at a node is not smaller than that at its parent. To surpass the lower bound
for extract-min, we reinforce a stronger property at the bottom levels of the
heap that the element at any right child is not smaller than that at its left
sibling. To surpass the lower bound for insert, we buffer insertions and allow
$O(\lg^2{} n)$ nodes to violate heap order in relation to their parents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3383</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3383</id><created>2014-07-12</created><authors><author><keyname>van der Hoeven</keyname><forenames>Joris</forenames></author><author><keyname>Lecerf</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Quintin</keyname><forenames>Guillaume</forenames></author></authors><title>Modular SIMD arithmetic in Mathemagix</title><categories>cs.MS</categories><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modular integer arithmetic occurs in many algorithms for computer algebra,
cryptography, and error correcting codes. Although recent microprocessors
typically offer a wide range of highly optimized arithmetic functions, modular
integer operations still require dedicated implementations. In this article, we
survey existing algorithms for modular integer arithmetic, and present detailed
vectorized counterparts. We also present several applications, such as fast
modular Fourier transforms and multiplication of integer polynomials and
matrices. The vectorized algorithms have been implemented in C++ inside the
free computer algebra and analysis system Mathemagix. The performance of our
implementation is illustrated by various benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3386</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3386</id><created>2014-07-12</created><updated>2014-07-22</updated><authors><author><keyname>Aliakbary</keyname><forenames>Sadegh</forenames></author><author><keyname>Habibi</keyname><forenames>Jafar</forenames></author><author><keyname>Movaghar</keyname><forenames>Ali</forenames></author></authors><title>Feature Extraction from Degree Distribution for Comparison and Analysis
  of Complex Networks</title><categories>cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.3625</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degree distribution is an important characteristic of complex networks.
In many data analysis applications, the networks should be represented as
fixed-length feature vectors and therefore the feature extraction from the
degree distribution is a necessary step. Moreover, many applications need a
similarity function for comparison of complex networks based on their degree
distributions. Such a similarity measure has many applications including
classification and clustering of network instances, evaluation of network
sampling methods, anomaly detection, and study of epidemic dynamics. The
existing methods are unable to effectively capture the similarity of degree
distributions, particularly when the corresponding networks have different
sizes. Based on our observations about the structure of the degree
distributions in networks over time, we propose a feature extraction and a
similarity function for the degree distributions in complex networks. We
propose to calculate the feature values based on the mean and standard
deviation of the node degrees in order to decrease the effect of the network
size on the extracted features. The proposed method is evaluated using
different artificial and real network datasets, and it outperforms the state of
the art methods with respect to the accuracy of the distance function and the
effectiveness of the extracted features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3392</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3392</id><created>2014-07-12</created><updated>2015-07-17</updated><authors><author><keyname>Sellami</keyname><forenames>Khaled</forenames></author><author><keyname>Ahmed-Nacer</keyname><forenames>Mohamed</forenames></author><author><keyname>Tiako</keyname><forenames>Pierre</forenames></author></authors><title>From Social Network to Semantic Social Network in Recommender System</title><categories>cs.SI cs.IR</categories><comments>International Journal of Computer Science Issues (IJCSI),2012, 9(4)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due the success of emerging Web 2.0, and different social network Web sites
such as Amazon and movie lens, recommender systems are creating unprecedented
opportunities to help people browsing the web when looking for relevant
information, and making choices. Generally, these recommender systems are
classified in three categories: content based, collaborative filtering, and
hybrid based recommendation systems. Usually, these systems employ standard
recommendation methods such as artificial neural networks, nearest neighbor, or
Bayesian networks. However, these approaches are limited compared to methods
based on web applications, such as social networks or semantic web. In this
paper, we propose a novel approach for recommendation systems called semantic
social recommendation systems that enhance the analysis of social networks
exploiting the power of semantic social network analysis. Experiments on
real-world data from Amazon examine the quality of our recommendation method as
well as the performance of our recommendation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3398</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3398</id><created>2014-07-12</created><updated>2014-07-17</updated><authors><author><keyname>Govind</keyname><forenames>D.</forenames></author><author><keyname>Biju</keyname><forenames>Anju Susan</forenames></author><author><keyname>Smily</keyname><forenames>Aguthu</forenames></author></authors><title>Speech Polarity Detection Using Hilbert Phase Information</title><categories>cs.SD</categories><comments>4 pages, 2 figures,
  http://nlp.amrita.edu:8080/TTS/polarityprograms.zip</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The objective of the present work is to propose a method to automatically
detect polarity of the speech signals by estimating instants of significant
excitation of the vocaltract and the cosine phase of the analytic signal
representation. The phase changes in the analytic signal around the Hilbert
envelope (HE) peaks are found to vary according to the polarity of the given
speech signal. The relevant HE peaks for the Hilbert phase analysis are
selected by estimating the instants of significant excitation in speech. The
speech polarity identification rate obtained for the proposed method is almost
equal to the state of the art residual skewness method for speech polarity
detection. The proposed method also provides the same results for the polarity
detection in electro-glottogram signals. Finally, the robustness of the
proposed method is confirmed from the reduced detection error rates obtained in
noisy environments with various signal to noise ratios (SNRs). The MATLAB codes
used for implementing the proposed method are available for download from the
following link: http://nlp.amrita.edu:8080/TTS/polarityprograms.zip
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3399</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3399</id><created>2014-07-12</created><updated>2014-11-04</updated><authors><author><keyname>Chen</keyname><forenames>Xianjie</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Articulated Pose Estimation by a Graphical Model with Image Dependent
  Pairwise Relations</title><categories>cs.CV</categories><comments>NIPS 2014 Camera Ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for estimating articulated human pose from a single
static image based on a graphical model with novel pairwise relations that make
adaptive use of local image measurements. More precisely, we specify a
graphical model for human pose which exploits the fact the local image
measurements can be used both to detect parts (or joints) and also to predict
the spatial relationships between them (Image Dependent Pairwise Relations).
These spatial relationships are represented by a mixture model. We use Deep
Convolutional Neural Networks (DCNNs) to learn conditional probabilities for
the presence of parts and their spatial relationships within image patches.
Hence our model combines the representational flexibility of graphical models
with the efficiency and statistical power of DCNNs. Our method significantly
outperforms the state of the art methods on the LSP and FLIC datasets and also
performs very well on the Buffy dataset without any training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3410</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3410</id><created>2014-07-12</created><authors><author><keyname>Li</keyname><forenames>Kezhi</forenames></author><author><keyname>Sundin</keyname><forenames>Martin</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author></authors><title>Alternating Strategies Are Good For Low-Rank Matrix Reconstruction</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article focuses on the problem of reconstructing low-rank matrices from
underdetermined measurements using alternating optimization strategies. We
endeavour to combine an alternating least-squares based estimation strategy
with ideas from the alternating direction method of multipliers (ADMM) to
recover structured low-rank matrices, such as Hankel structure. We show that
merging these two alternating strategies leads to a better performance than the
existing alternating least squares (ALS) strategy. The performance is evaluated
via numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3416</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3416</id><created>2014-07-12</created><updated>2014-09-19</updated><authors><author><keyname>Bellin</keyname><forenames>Gianluigi</forenames><affiliation>University of Verona</affiliation></author></authors><title>Categorical Proof Theory of Co-Intuitionistic Linear Logic</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  10, 2014) lmcs:1186</journal-ref><doi>10.2168/LMCS-10(3:16)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To provide a categorical semantics for co-intuitionistic logic one has to
face the fact, noted by Tristan Crolard, that the definition of co-exponents as
adjuncts of coproducts does not work in the category Set, where coproducts are
disjoint unions. Following the familiar construction of models of
intuitionistic linear logic with exponential&quot;!&quot;, we build models of
co-intuitionistic logic in symmetric monoidal left-closed categories with
additional structure, using a variant of Crolard's term assignment to
co-intuitionistic logic in the construction of a free category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3421</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3421</id><created>2014-07-12</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author></authors><title>Stochastic bridges of linear systems</title><categories>cs.SY math-ph math.MP math.PR</categories><comments>11 pages, 4 figures</comments><msc-class>60G15, 93E03, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a generalization of the Brownian bridge as a stochastic process that
models the position and velocity of inertial particles between the two
end-points of a time interval. The particles experience random acceleration and
are assumed to have known states at the boundary. Thus, the movement of the
particles can be modeled as an Ornstein-Uhlenbeck process conditioned on
position and velocity measurements at the two end-points. It is shown that
optimal stochastic control provides a stochastic differential equation (SDE)
that generates such a bridge as a degenerate diffusion process. Generalizations
to higher order linear diffusions are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3422</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3422</id><created>2014-07-12</created><updated>2016-02-28</updated><authors><author><keyname>Melnyk</keyname><forenames>Igor</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author></authors><title>A Spectral Algorithm for Inference in Hidden Semi-Markov Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden semi-Markov models (HSMMs) are latent variable models which allow
latent state persistence and can be viewed as a generalization of the popular
hidden Markov models (HMMs). In this paper, we introduce a novel spectral
algorithm to perform inference in HSMMs. Unlike expectation maximization (EM),
our approach correctly estimates the probability of given observation sequence
based on a set of training sequences. Our approach is based on estimating
moments from the sample, whose number of dimensions depends only
logarithmically on the maximum length of the hidden state persistence.
Moreover, the algorithm requires only a few matrix inversions and is therefore
computationally efficient. Empirical evaluations on synthetic and real data
demonstrate the advantage of the algorithm over EM in terms of speed and
accuracy, especially for large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3429</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3429</id><created>2014-07-13</created><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author></authors><title>The Tractability Frontier of Graph-Like First-Order Query Sets</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study first-order model checking, by which we refer to the problem of
deciding whether or not a given first-order sentence is satisfied by a given
finite structure. In particular, we aim to understand on which sets of
sentences this problem is tractable, in the sense of parameterized complexity
theory. To this end, we define the notion of a graph-like sentence set, which
definition is inspired by previous work on first-order model checking wherein
the permitted connectives and quantifiers were restricted. Our main theorem is
the complete tractability classification of such graph-like sentence sets,
which is (to our knowledge) the first complexity classification theorem
concerning a class of sentences that has no restriction on the connectives and
quantifiers. To present and prove our classification, we introduce and develop
a novel complexity-theoretic framework which is built on parameterized
complexity and includes new notions of reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3433</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3433</id><created>2014-07-13</created><updated>2014-07-17</updated><authors><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>List decoding Reed-Muller codes over small fields</title><categories>cs.CC cs.IT math.IT</categories><comments>fixed a bug in the proof of claim 5.6 (now lemma 5.5)</comments><msc-class>68P30, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The list decoding problem for a code asks for the maximal radius up to which
any ball of that radius contains only a constant number of codewords. The list
decoding radius is not well understood even for well studied codes, like
Reed-Solomon or Reed-Muller codes.
  Fix a finite field $\mathbb{F}$. The Reed-Muller code
$\mathrm{RM}_{\mathbb{F}}(n,d)$ is defined by $n$-variate degree-$d$
polynomials over $\mathbb{F}$. In this work, we study the list decoding radius
of Reed-Muller codes over a constant prime field $\mathbb{F}=\mathbb{F}_p$,
constant degree $d$ and large $n$. We show that the list decoding radius is
equal to the minimal distance of the code.
  That is, if we denote by $\delta(d)$ the normalized minimal distance of
$\mathrm{RM}_{\mathbb{F}}(n,d)$, then the number of codewords in any ball of
radius $\delta(d)-\varepsilon$ is bounded by $c=c(p,d,\varepsilon)$ independent
of $n$. This resolves a conjecture of Gopalan-Klivans-Zuckerman [STOC 2008],
who among other results proved it in the special case of
$\mathbb{F}=\mathbb{F}_2$; and extends the work of Gopalan [FOCS 2010] who
proved the conjecture in the case of $d=2$.
  We also analyse the number of codewords in balls of radius exceeding the
minimal distance of the code. For $e \leq d$, we show that the number of
codewords of $\mathrm{RM}_{\mathbb{F}}(n,d)$ in a ball of radius $\delta(e) -
\varepsilon$ is bounded by $\exp(c \cdot n^{d-e})$, where
$c=c(p,d,\varepsilon)$ is independent of $n$. The dependence on $n$ is tight.
This extends the work of Kaufman-Lovett-Porat [IEEE Inf. Theory 2012] who
proved similar bounds over $\mathbb{F}_2$.
  The proof relies on several new ingredients: an extension of the
Frieze-Kannan weak regularity to general function spaces, higher-order Fourier
analysis, and an extension of the Schwartz-Zippel lemma to compositions of
polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3434</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3434</id><created>2014-07-13</created><authors><author><keyname>Jembre</keyname><forenames>Yalew Zelalem</forenames></author><author><keyname>Choi</keyname><forenames>Young-June</forenames></author></authors><title>Enhanced Out-of-Band Sensing Algorithm for Cognitive Radio Networks</title><categories>cs.NI</categories><comments>the 6th International Conference on Ubiquitous Information
  Technologies and Application, Seoul, Korea, Dec. 15-16, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio networks (CRN), Out-of-Band (OoB) spectrum sensing
provides seamless communication. Cognitive radio (CR) users, so called
secondary users (SUs), should avoid interference with primary users (PUs), the
owner of the licensed band, while trying to access the unused licensed or
unlicensed band, for spectrum utilization. When PUs request to access their
band, SUs need to vacate the band, thus it is inconvenient to provide seamless
communication without OoB sensing. In this paper, we suggest an OoB sensing
algorithm to guarantee seamless communication and also minimize the
interference of SUs on PUs. Also we obtain analysis-based achievable throughput
by considering the OoB sensing duration. To verify our algorithm, we perform
simulation and find that the effect due to OoB sensing on the aggregate
throughput is insignificant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3435</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3435</id><created>2014-07-13</created><authors><author><keyname>Liao</keyname><forenames>Yun</forenames></author><author><keyname>Wang</keyname><forenames>Tianyu</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Listen-and-Talk: Full-duplex Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>in proceeding of IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In traditional cognitive radio networks, secondary users (SUs) typically
access the spectrum of primary users (PUs) by a two-stage &quot;listen-before-talk&quot;
(LBT) protocol, i.e., SUs sense the spectrum holes in the first stage before
transmit in the second stage. In this paper, we propose a novel
&quot;listen-and-talk&quot; (LAT) protocol with the help of the full-duplex (FD)
technique that allows SUs to simultaneously sense and access the vacant
spectrum. Analysis of sensing performance and SU's throughput are given for the
proposed LAT protocol. And we find that due to self-interference caused by FD,
increasing transmitting power of SUs does not always benefit to SU's
throughput, which implies the existence of a power-throughput tradeoff.
Besides, though the LAT protocol suffers from self-interference, it allows
longer transmission time, while the performance of the traditional LBT protocol
is limited by channel spatial correction and relatively shorter transmission
period. To this end, we also present an adaptive scheme to improve SUs'
throughput by switching between the LAT and LBT protocols. Numerical results
are provided to verify the proposed methods and the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3462</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3462</id><created>2014-07-13</created><updated>2015-08-10</updated><authors><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Semi-Streaming Algorithms for Annotated Graph Streams</title><categories>cs.DS</categories><comments>This update includes some additional discussion of the results
  proven. The result on counting triangles was previously included in an ECCC
  technical report by Chakrabarti et al. available at
  http://eccc.hpi-web.de/report/2013/180/. That report has been superseded by
  this manuscript, and the CCC 2015 paper &quot;Verifiable Stream Computation and
  Arthur-Merlin Communication&quot; by Chakrabarti et al</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considerable effort has been devoted to the development of streaming
algorithms for analyzing massive graphs. Unfortunately, many results have been
negative, establishing that a wide variety of problems require $\Omega(n^2)$
space to solve. One of the few bright spots has been the development of
semi-streaming algorithms for a handful of graph problems -- these algorithms
use space $O(n\cdot\text{polylog}(n))$.
  In the annotated data streaming model of Chakrabarti et al., a
computationally limited client wants to compute some property of a massive
input, but lacks the resources to store even a small fraction of the input, and
hence cannot perform the desired computation locally. The client therefore
accesses a powerful but untrusted service provider, who not only performs the
requested computation, but also proves that the answer is correct.
  We put forth the notion of semi-streaming algorithms for annotated graph
streams (semi-streaming annotation schemes for short). These are protocols in
which both the client's space usage and the length of the proof are $O(n \cdot
\text{polylog}(n))$. We give evidence that semi-streaming annotation schemes
represent a substantially more robust solution concept than does the standard
semi-streaming model. On the positive side, we give semi-streaming annotation
schemes for two dynamic graph problems that are intractable in the standard
model: (exactly) counting triangles, and (exactly) computing maximum matchings.
The former scheme answers a question of Cormode. On the negative side, we
identify for the first time two natural graph problems (connectivity and
bipartiteness in a certain edge update model) that can be solved in the
standard semi-streaming model, but cannot be solved by annotation schemes of
&quot;sub-semi-streaming&quot; cost. That is, these problems are just as hard in the
annotations model as they are in the standard model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3474</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3474</id><created>2014-07-13</created><authors><author><keyname>Eiwen</keyname><forenames>Daniel</forenames></author><author><keyname>Tauboeck</keyname><forenames>Georg</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author><author><keyname>Feichtinger</keyname><forenames>Hans Georg</forenames></author></authors><title>Multichannel group sparsity methods for compressive channel estimation
  in doubly selective multicarrier MIMO systems</title><categories>cs.IT math.IT</categories><comments>43 pages, 7 figures, submitted to IEEE Trans. Inf. Theory on December
  14, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider compressive channel estimation within pulse-shaping multicarrier
multiple-input multiple-output (MIMO) systems transmitting over doubly
selective MIMO channels. This setup includes MIMO orthogonal frequency-division
multiplexing (MIMO-OFDM) systems as a special case. We demonstrate that the
individual component channels tend to exhibit an approximate joint group
sparsity structure in the delay-Doppler domain. Motivated by this insight, we
develop a compressive channel estimator that exploits the joint group sparsity
structure for improved performance. The proposed channel estimator uses the
methodology of multichannel group sparse compressed sensing (MGCS), which is
derived by combining the existing methodologies of group sparse compressed
sensing and multichannel compressed sensing. We derive an upper bound on the
channel estimation error and analyze the estimator's computational complexity.
The performance of the estimator is then further improved by replacing the
Fourier basis used in the basic MGCS-based channel estimator by an alternative
basis yielding enhanced joint group sparsity. We propose an iterative basis
optimization algorithm that is able to utilize prior statistical information if
available and amounts to a sequence of convex programming problems. Finally,
simulations using a geometry-based channel simulator demonstrate the
performance gains that can be achieved by leveraging the group sparsity, joint
sparsity, and joint group sparsity of the component channels as well as the
additional performance gains resulting from the use of the optimized basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3479</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3479</id><created>2014-07-13</created><updated>2014-07-20</updated><authors><author><keyname>Amorim</keyname><forenames>C&#xe1;ssio Sozinho</forenames></author></authors><title>Bourdieu Dynamics of Fields from a Modified Axelrod Model</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pierre Bourdieu discussed how an individual's taste relates o his or her
social environment, and how the classification of \emph{distinct} and
\emph{vulgar}, among others, arises from at the same time as shapes this taste
in his work called \emph{La Distinction}. Robert Axelrod created a
computational model with local convergence and global polarization properties
to describe the dissemination of culture by simple selective interactions. In
this letter, Axelrod model is modified, while holding to the same original
principles, to describe Bourdieu theory. This allows to analyze how the
dynamics of society's tastes and trends may vary with a simple approach,
considering social structures and to understand which social forces are crucial
to change dynamics. Despite the relative simplicity, the present approach
clarifies symbolic power relations, a relevant issue for understanding power
relation both on large as well as on small and localized scale, with impact on
activities ranging from daily life matters to business, politics, and research.
This model sheds light on social issues, showing that a small amount of
conflict within a class plays a central role in the culture dynamics, being the
major responsible for continuous changes in distinction paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3485</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3485</id><created>2014-07-13</created><updated>2014-08-18</updated><authors><author><keyname>Weihrauch</keyname><forenames>Klaus</forenames><affiliation>University of Hagen</affiliation></author><author><keyname>Tavana-Roshandel</keyname><forenames>Nazanin</forenames><affiliation>IPM, Tehran, Iran</affiliation></author></authors><title>Representations of measurable sets in computable measure theory</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  19, 2014) lmcs:1022</journal-ref><doi>10.2168/LMCS-10(3:7)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is a fundamental study in computable measure theory. We use the
framework of TTE, the representation approach, where computability on an
abstract set X is defined by representing its elements with concrete &quot;names&quot;,
possibly countably infinite, over some alphabet {\Sigma}. As a basic
computability structure we consider a computable measure on a computable
$\sigma$-algebra. We introduce and compare w.r.t. reducibility several natural
representations of measurable sets. They are admissible and generally form four
different equivalence classes. We then compare our representations with those
introduced by Y. Wu and D. Ding in 2005 and 2006 and claim that one of our
representations is the most useful one for studying computability on measurable
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3487</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3487</id><created>2014-07-13</created><authors><author><keyname>Fursin</keyname><forenames>Grigori</forenames></author></authors><title>Collective Tuning Initiative</title><categories>cs.DC</categories><comments>GCC Developers' Summit'09, 14 June 2009, Montreal, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing systems rarely deliver best possible performance due to ever
increasing hardware and software complexity and limitations of the current
optimization technology. Additional code and architecture optimizations are
often required to improve execution time, size, power consumption, reliability
and other important characteristics of computing systems. However, it is often
a tedious, repetitive, isolated and time consuming process. In order to
automate, simplify and systematize program optimization and architecture
design, we are developing open-source modular plugin-based Collective Tuning
Infrastructure (CTI, http://cTuning.org) that can distribute optimization
process and leverage optimization experience of multiple users. CTI provides a
novel fully integrated, collaborative, &quot;one button&quot; approach to improve
existing underperfoming computing systems ranging from embedded architectures
to high-performance servers based on systematic iterative compilation,
statistical collective optimization and machine learning. Our experimental
results show that it is possible to reduce execution time (and code size) of
some programs from SPEC2006 and EEMBC among others by more than a factor of 2
automatically. It can also reduce development and testing time considerably.
Together with the first production quality machine learning enabled interactive
research compiler (MILEPOST GCC) this infrastructure opens up many research
opportunities to study and develop future realistic self-tuning and
self-organizing adaptive intelligent computing systems based on systematic
statistical performance evaluation and benchmarking. Finally, using common
optimization repository is intended to improve the quality and reproducibility
of the research on architecture and code optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3500</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3500</id><created>2014-07-13</created><authors><author><keyname>Sanyal</keyname><forenames>Swagato</forenames></author></authors><title>Sub-linear Upper Bounds on Fourier dimension of Boolean Functions in
  terms of Fourier sparsity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the Fourier dimension of any Boolean function with Fourier
sparsity $s$ is at most $O\left(s^{2/3}\right)$. Our proof method yields an
improved bound of $\widetilde{O}(\sqrt{s})$ assuming a conjecture of
Tsang~\etal~\cite{tsang}, that for every Boolean function of sparsity $s$ there
is an affine subspace of $\mathbb{F}_2^n$ of co-dimension $O(\poly\log s)$
restricted to which the function is constant. This conjectured bound is tight
upto poly-logarithmic factors as the Fourier dimension and sparsity of the
address function are quadratically separated. We obtain these bounds by
observing that the Fourier dimension of a Boolean function is equivalent to its
non-adaptive parity decision tree complexity, and then bounding the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3501</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3501</id><created>2014-07-13</created><updated>2015-05-27</updated><authors><author><keyname>Cully</keyname><forenames>Antoine</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author><author><keyname>Tarapore</keyname><forenames>Danesh</forenames></author><author><keyname>Mouret</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Robots that can adapt like animals</title><categories>cs.RO cs.AI cs.LG cs.NE q-bio.NC</categories><doi>10.1038/nature14422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As robots leave the controlled environments of factories to autonomously
function in more complex, natural environments, they will have to respond to
the inevitable fact that they will become damaged. However, while animals can
quickly adapt to a wide variety of injuries, current robots cannot &quot;think
outside the box&quot; to find a compensatory behavior when damaged: they are limited
to their pre-specified self-sensing abilities, can diagnose only anticipated
failure modes, and require a pre-programmed contingency plan for every type of
potential damage, an impracticality for complex robots. Here we introduce an
intelligent trial and error algorithm that allows robots to adapt to damage in
less than two minutes, without requiring self-diagnosis or pre-specified
contingency plans. Before deployment, a robot exploits a novel algorithm to
create a detailed map of the space of high-performing behaviors: This map
represents the robot's intuitions about what behaviors it can perform and their
value. If the robot is damaged, it uses these intuitions to guide a
trial-and-error learning algorithm that conducts intelligent experiments to
rapidly discover a compensatory behavior that works in spite of the damage.
Experiments reveal successful adaptations for a legged robot injured in five
different ways, including damaged, broken, and missing legs, and for a robotic
arm with joints broken in 14 different ways. This new technique will enable
more robust, effective, autonomous robots, and suggests principles that animals
may use to adapt to injury.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3504</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3504</id><created>2014-07-13</created><authors><author><keyname>Kang</keyname><forenames>Ross</forenames></author><author><keyname>Muller</keyname><forenames>Tobias</forenames></author><author><keyname>West</keyname><forenames>Douglas B.</forenames></author></authors><title>On r-dynamic Coloring of Grids</title><categories>math.CO cs.DM</categories><journal-ref>Discrete Applied Mathematics 186: 286-290, 2015</journal-ref><doi>10.1016/j.dam.2015.01.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An \textit{$r$-dynamic $k$-coloring} of a graph $G$ is a proper $k$-coloring
of $G$ such that every vertex in $V(G)$ has neighbors in at least
$\min\{d(v),r\}$ different color classes. The \textit{$r$-dynamic chromatic
number} of a graph $G$, written $\chi_r(G)$, is the least $k$ such that $G$ has
such a coloring. Proving a conjecture of Jahanbekam, Kim, O, and West, we show
that the $m$-by-$n$ grid has no $3$-dynamic $4$-coloring when $mn\equiv2\mod
4$. This completes the determination of the $r$-dynamic chromatic number of the
$m$-by-$n$ grid for all $r,m,n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3507</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3507</id><created>2014-07-13</created><updated>2014-07-29</updated><authors><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>Voicu</keyname><forenames>Dumitru V.</forenames></author></authors><title>Spanning Properties of Theta-Theta Graphs</title><categories>cs.CG</categories><comments>20 pages, 6 figures, 3 tables</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the spanning properties of Theta-Theta graphs. Similar in spirit
with the Yao-Yao graphs, Theta-Theta graphs partition the space around each
vertex into a set of k cones, for some fixed integer k &gt; 1, and select at most
one edge per cone. The difference is in the way edges are selected. Yao-Yao
graphs select an edge of minimum length, whereas Theta-Theta graphs select an
edge of minimum orthogonal projection onto the cone bisector. It has been
established that the Yao-Yao graphs with parameter k = 6k' have spanning ratio
11.67, for k' &gt;= 6. In this paper we establish a first spanning ratio of $7.82$
for Theta-Theta graphs, for the same values of $k$. We also extend the class of
Theta-Theta spanners with parameter 6k', and establish a spanning ratio of
$16.76$ for k' &gt;= 5. We surmise that these stronger results are mainly due to a
tighter analysis in this paper, rather than Theta-Theta being superior to
Yao-Yao as a spanner. We also show that the spanning ratio of Theta-Theta
graphs decreases to 4.64 as k' increases to 8. These are the first results on
the spanning properties of Theta-Theta graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3512</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3512</id><created>2014-07-13</created><authors><author><keyname>Delhibabu</keyname><forenames>Radhakrishnan</forenames></author><author><keyname>Behrend</keyname><forenames>Andreas</forenames></author></authors><title>A New Rational Algorithm for View Updating in Relational Databases</title><categories>cs.AI cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:1301.5154</comments><doi>10.1007/s10489-014-0579-0</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
In order to apply the rationality result of belief dynamics theory to various
practical problems, it should be generalized in two respects: first it should
allow a certain part of belief to be declared as immutable; and second, the
belief state need not be deductively closed. Such a generalization of belief
dynamics, referred to as base dynamics, is presented in this paper, along with
the concept of a generalized revision algorithm for knowledge bases (Horn or
Horn logic with stratified negation). We show that knowledge base dynamics has
an interesting connection with kernel change via hitting set and abduction. In
this paper, we show how techniques from disjunctive logic programming can be
used for efficient (deductive) database updates. The key idea is to transform
the given database together with the update request into a disjunctive
(datalog) logic program and apply disjunctive techniques (such as minimal model
reasoning) to solve the original update problem. The approach extends and
integrates standard techniques for efficient query answering and integrity
checking. The generation of a hitting set is carried out through a hyper
tableaux calculus and magic set that is focused on the goal of minimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3518</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3518</id><created>2014-07-13</created><authors><author><keyname>Savla</keyname><forenames>Ketan</forenames></author><author><keyname>Como</keyname><forenames>Giacomo</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A.</forenames></author></authors><title>Robust Network Routing under Cascading Failures</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dynamical model for cascading failures in single-commodity
network flows. In the proposed model, the network state consists of flows and
activation status of the links. Network dynamics is determined by a, possibly
state-dependent and adversarial, disturbance process that reduces flow capacity
on the links, and routing policies at the nodes that have access to the network
state, but are oblivious to the presence of disturbance. Under the proposed
dynamics, a link becomes irreversibly inactive either due to overload condition
on itself or on all of its immediate downstream links. The coupling between
link activation and flow dynamics implies that links to become inactive
successively are not necessarily adjacent to each other, and hence the pattern
of cascading failure under our model is qualitatively different than standard
cascade models. The magnitude of a disturbance process is defined as the sum of
cumulative capacity reductions across time and links of the network, and the
margin of resilience of the network is defined as the infimum over the
magnitude of all disturbance processes under which the links at the origin node
become inactive. We propose an algorithm to compute an upper bound on the
margin of resilience for the setting where the routing policy only has access
to information about the local state of the network. For the limiting case when
the routing policies update their action as fast as network dynamics, we
identify sufficient conditions on network parameters under which the upper
bound is tight under an appropriate routing policy. Our analysis relies on
making connections between network parameters and monotonicity in network state
evolution under proposed dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3519</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3519</id><created>2014-07-13</created><authors><author><keyname>Bourke</keyname><forenames>Timothy</forenames><affiliation>INRIA</affiliation></author><author><keyname>van Glabbeek</keyname><forenames>Robert J.</forenames><affiliation>NICTA</affiliation></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames><affiliation>NICTA</affiliation></author></authors><title>Showing invariance compositionally for a process algebra for network
  protocols</title><categories>cs.LO</categories><comments>The Isabelle/HOL source files, and a full proof document, are
  available in the Archive of Formal Proofs, at
  http://afp.sourceforge.net/entries/AWN.shtml</comments><acm-class>F.3.1; C.2.2</acm-class><journal-ref>Proc. Interactive Theorem Proving, ITP '14 (G. Klein &amp; R. Gamboa,
  eds.), LNCS 8558, Springer, 2014, pp. 144-159</journal-ref><doi>10.1007/978-3-319-08970-6_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the mechanization of a process algebra for Mobile Ad hoc
Networks and Wireless Mesh Networks, and the development of a compositional
framework for proving invariant properties. Mechanizing the core process
algebra in Isabelle/HOL is relatively standard, but its layered structure
necessitates special treatment. The control states of reactive processes, such
as nodes in a network, are modelled by terms of the process algebra. We propose
a technique based on these terms to streamline proofs of inductive invariance.
This is not sufficient, however, to state and prove invariants that relate
states across multiple processes (entire networks). To this end, we propose a
novel compositional technique for lifting global invariants stated at the level
of individual nodes to networks of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3535</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3535</id><created>2014-07-13</created><updated>2014-07-24</updated><authors><author><keyname>Mahmood</keyname><forenames>Arif</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author><author><keyname>Owens</keyname><forenames>Robyn</forenames></author></authors><title>Optimizing Auto-correlation for Fast Target Search in Large Search Space</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In remote sensing image-blurring is induced by many sources such as
atmospheric scatter, optical aberration, spatial and temporal sensor
integration. The natural blurring can be exploited to speed up target search by
fast template matching. In this paper, we synthetically induce additional
non-uniform blurring to further increase the speed of the matching process. To
avoid loss of accuracy, the amount of synthetic blurring is varied spatially
over the image according to the underlying content. We extend transitive
algorithm for fast template matching by incorporating controlled image blur. To
this end we propose an Efficient Group Size (EGS) algorithm which minimizes the
number of similarity computations for a particular search image. A larger
efficient group size guarantees less computations and more speedup. EGS
algorithm is used as a component in our proposed Optimizing auto-correlation
(OptA) algorithm. In OptA a search image is iteratively non-uniformly blurred
while ensuring no accuracy degradation at any image location. In each iteration
efficient group size and overall computations are estimated by using the
proposed EGS algorithm. The OptA algorithm stops when the number of
computations cannot be further decreased without accuracy degradation. The
proposed algorithm is compared with six existing state of the art exhaustive
accuracy techniques using correlation coefficient as the similarity measure.
Experiments on satellite and aerial image datasets demonstrate the
effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3538</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3538</id><created>2014-07-14</created><authors><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>On Uplink-Downlink Duality for Cellular IA</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our previous work we considered the uplink of a hexagonal cellular network
topology and showed that linear &quot;one-shot&quot; interference alignment (IA) schemes
are able to achieve the optimal degrees of freedom (DoFs) per user, under a
decoded-message passing framework that allows base-stations to exchange their
own decoded messages over local backhaul links. In this work, we provide the
dual framework for the downlink of cellular networks with the same backhaul
architecture, and show that for every &quot;one-shot&quot; IA scheme that can achieve $d$
DoFs per user in the uplink, there exists a dual &quot;one-shot&quot; IA scheme that can
achieve the same DoFs in the downlink. To enable &quot;Cellular IA&quot; for the
downlink, base-stations will now use the same local backhaul links to exchange
quantized versions of the dirty-paper precoded signals instead of user
messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3540</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3540</id><created>2014-07-14</created><authors><author><keyname>El-Gaaly</keyname><forenames>Tarek</forenames></author><author><keyname>Gluckman</keyname><forenames>Joshua</forenames></author></authors><title>Measuring Atmospheric Scattering from Digital Images of Urban Scenery
  using Temporal Polarization-Based Vision</title><categories>cs.CV</categories><comments>Masters in Computer Science Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particulate Matter (PM) is a form of air pollution that visually degrades
urban scenery and is hazardous to human health and the environment. Current
monitoring devices are limited in measuring average PM over large areas.
Quantifying the visual effects of haze in digital images of urban scenery and
correlating these effects to PM levels is a vital step in more practically
monitoring our environment. Current image haze extraction algorithms remove
haze from the scene for the sole purpose of enhancing vision. We present two
algorithms which bridge the gap between image haze extraction and environmental
monitoring. We provide a means of measuring atmospheric scattering from images
of urban scenery by incorporating temporal knowledge. In doing so, we also
present a method of recovering an accurate depthmap of the scene and recovering
the scene without the visual effects of haze. We compare our algorithm to three
known haze removal methods. The algorithms are composed of an optimization over
a model of haze formation in images and an optimization using a constraint of
constant depth over a sequence of images taken over time. These algorithms not
only measure atmospheric scattering, but also recover a more accurate depthmap
and dehazed image. The measurements of atmospheric scattering this research
produces, can be directly correlated to PM levels and therefore pave the way to
monitoring the health of the environment by visual means. Accurate atmospheric
sensing from digital images is a challenging and under-researched problem. This
work provides an important step towards a more practical and accurate visual
means of measuring PM from digital images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3543</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3543</id><created>2014-07-14</created><authors><author><keyname>Gawthrop</keyname><forenames>Peter</forenames></author><author><keyname>Gollee</keyname><forenames>Henrik</forenames></author><author><keyname>Loram</keyname><forenames>Ian</forenames></author></authors><title>Intermittent Control in Man and Machine</title><categories>q-bio.QM cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intermittent control has a long history in the physiological literature and
there is strong experimental evidence that some human control systems are
intermittent. Intermittent control has also appeared in various forms in the
engineering literature. This article discusses a particular mathematical model
of Event-driven Intermittent Control which brings together engineering and
physiological insights and builds on and extends previous work in this area.
Illustrative examples of the properties of Intermittent Control in a
physiological context are given together with suggestions for future research
directions in both physiology and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3552</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3552</id><created>2014-07-14</created><authors><author><keyname>Bai</keyname><forenames>Shuotian</forenames></author><author><keyname>Gao</keyname><forenames>Rui</forenames></author><author><keyname>Hao</keyname><forenames>Bibo</forenames></author><author><keyname>Yuan</keyname><forenames>Sha</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Identifying Social Satisfaction from Social Media</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the critical need to identify social situation and instability
factors by acquiring public social satisfaction in this research. However,
subject to the large amount of manual work cost in subject recruitment and data
processing, conventional self-reported method cannot be implemented in real
time or applied in large scale investigation. To solve the problem, this paper
proposed an approach to predict users' social satisfaction, especially for the
economy-related satisfaction based on users' social media records. We recruited
2,018 Cantonese active participants from each city in Guangdong province
according to the population distribution. Both behavioral and linguistic
features of the participants are extracted from the online records of social
media, i.e., Sina Weibo. Regression models are used to predict Sina Weibo
users' social satisfaction. Furthermore, we consult the economic indexes of
Guangdong in 2012, and calculate the correlations between these indexes and the
predicted social satisfaction. Results indicate that social satisfaction can be
significantly expressed by specific social media features; local economy
satisfaction has significant positive correlations with several local economy
indexes, which supports that it is reliable to predict social satisfaction from
social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3556</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3556</id><created>2014-07-14</created><authors><author><keyname>Hamedazimi</keyname><forenames>Navid</forenames></author><author><keyname>Gupta</keyname><forenames>Himanshu</forenames></author></authors><title>Optimal Spectrum Management in Two-User Interference Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem of optimal spectrum management in
continuous frequency domain in multiuser interference channels. The objective
is to maximize the weighted sum of user capacities. Our main results are as
follows: (i) For frequency-selective channels, we prove that in an optimal
solution, each user uses maximum power; this result also generalizes to the
cases where the objective is to maximize the weighted product (i.e.,
proportional fairness) of user capacities. (ii) For the special case of two
users in flat channels, we solve the problem optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3561</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3561</id><created>2014-07-14</created><authors><author><keyname>Benet</keyname><forenames>Juan</forenames></author></authors><title>IPFS - Content Addressed, Versioned, P2P File System</title><categories>cs.NI cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The InterPlanetary File System (IPFS) is a peer-to-peer distributed file
system that seeks to connect all computing devices with the same system of
files. In some ways, IPFS is similar to the Web, but IPFS could be seen as a
single BitTorrent swarm, exchanging objects within one Git repository. In other
words, IPFS provides a high throughput content-addressed block storage model,
with content-addressed hyper links. This forms a generalized Merkle DAG, a data
structure upon which one can build versioned file systems, blockchains, and
even a Permanent Web. IPFS combines a distributed hashtable, an incentivized
block exchange, and a self-certifying namespace. IPFS has no single point of
failure, and nodes do not need to trust each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3566</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3566</id><created>2014-07-14</created><authors><author><keyname>Ye</keyname><forenames>Yuanqing</forenames></author><author><keyname>Huang</keyname><forenames>Xin</forenames></author><author><keyname>Wen</keyname><forenames>Ting</forenames></author><author><keyname>Huang</keyname><forenames>Jiaqing</forenames></author><author><keyname>Uwitonze</keyname><forenames>Alfred</forenames></author></authors><title>Performance Comparison between Network Coding in Space and Routing in
  Space</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding in geometric space, a new research direction also known as
Space Information Flow, is a promising research field which shows the
superiority of network coding in space over routing in space. Present
literatures proved that given six terminal nodes, network coding in space is
strictly superior to routing in space in terms of single-source multicast in
regular (5+1) model, in which five terminal nodes forms a regular pentagon
centered at a terminal node. In order to compare the performance between
network coding in space and routing in space, this paper quantitatively studies
two classes of network coding in space and optimal routing in space when any
terminal node moves arbitrarily in two-dimensional Euclidean space, and cost
advantage is used as the metric. Furthermore, the upper-bound of cost advantage
is figured out as well as the region where network coding in space is superior
to routing in space. Several properties of Space Information Flow are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3567</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3567</id><created>2014-07-14</created><updated>2014-11-12</updated><authors><author><keyname>Mosonyi</keyname><forenames>Mil&#xe1;n</forenames></author><author><keyname>Ogawa</keyname><forenames>Tomohiro</forenames></author></authors><title>The strong converse exponent of quantum hypothesis testing for
  correlated quantum states</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v2: Minor changes, added proof of Lemma A.2. 25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two general approaches to obtain the strong converse rate of
quantum hypothesis testing for correlated quantum states. One approach requires
that the states satisfy a certain factorization property; typical examples of
such states are the temperature states of translation-invariant finite-range
interactions on a spin chain. The other approach requires the differentiability
of a regularized R\'enyi $\alpha$-divergence in the parameter $\alpha$; typical
examples of such states include temperature states of non-interacting fermionic
lattice systems, and classical irreducible Markov chains. In all cases, we get
that the strong converse exponent is equal to the Hoeffding anti-divergence,
which in turn is obtained from the regularized R\'enyi divergences of the two
states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3609</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3609</id><created>2014-07-14</created><authors><author><keyname>Binu</keyname><forenames>V P</forenames></author><author><keyname>Sreekumar</keyname><forenames>A</forenames></author></authors><title>Generalized Secret Sharing using Permutation Ordered Binary System</title><categories>cs.CR</categories><comments>ISBN: 978-93-83459-32-2- conference proceedings Date: 27, March 2014
  - 28, March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret sharing is a method of dividing a secret among n par- ticipants and
allows only qualified subset to reconstruct the secret and hence provides
better reliability and availability of secret data.In the generalized secret
sharing scheme, a monotone access structure of the set of participants is
considered. The access structure specifies a qualified subset of participants
who can reconstruct the secret from their shares.Generalized secret sharing
schemes can be efficiently implemented by using (n; n) scheme.We have developed
an efficient (n; n) scheme using Permutation Ordered Binary (POB) number system
which is then combined with cumulative arrays to obtain a generalized secret
sharing scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3619</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3619</id><created>2014-07-14</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>On the Power of Adaptivity in Matrix Completion and Approximation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the related tasks of matrix completion and matrix approximation
from missing data and propose adaptive sampling procedures for both problems.
We show that adaptive sampling allows one to eliminate standard incoherence
assumptions on the matrix row space that are necessary for passive sampling
procedures. For exact recovery of a low-rank matrix, our algorithm judiciously
selects a few columns to observe in full and, with few additional measurements,
projects the remaining columns onto their span. This algorithm exactly recovers
an $n \times n$ rank $r$ matrix using $O(nr\mu_0 \log^2(r))$ observations,
where $\mu_0$ is a coherence parameter on the column space of the matrix. In
addition to completely eliminating any row space assumptions that have pervaded
the literature, this algorithm enjoys a better sample complexity than any
existing matrix completion algorithm. To certify that this improvement is due
to adaptive sampling, we establish that row space coherence is necessary for
passive sampling algorithms to achieve non-trivial sample complexity bounds.
  For constructing a low-rank approximation to a high-rank input matrix, we
propose a simple algorithm that thresholds the singular values of a zero-filled
version of the input matrix. The algorithm computes an approximation that is
nearly as good as the best rank-$r$ approximation using $O(nr\mu \log^2(n))$
samples, where $\mu$ is a slightly different coherence parameter on the matrix
columns. Again we eliminate assumptions on the row space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3631</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3631</id><created>2014-07-14</created><authors><author><keyname>Cariolaro</keyname><forenames>David</forenames></author><author><keyname>Shen</keyname><forenames>Zhaiming</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author></authors><title>Group Testing with Pools of Fixed Size</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical combinatorial (adaptive) group testing problem, one is given
two integers \(d\) and \(n\), where \(0\le d\le n\), and a population of \(n\)
items, exactly \(d\) of which are known to be defective. The question is to
devise an optimal sequential algorithm that, at each step, tests a subset of
the population and determines whether such subset is contaminated (i.e.
contains defective items) or otherwise. The problem is solved only when the
\(d\) defective items are identified. The minimum number of steps that an
optimal sequential algorithm takes in general (i.e. in the worst case) to solve
the problem is denoted by \(M(d, n)\). The computation of \(M(d, n)\) appears
to be very difficult and a general formula is known only for \(d = 1\). We
consider here a variant of the original problem, where the size of the subsets
to be tested is restricted to be a fixed positive integer \(k\). The
corresponding minimum number of tests by a sequential optimal algorithm is
denoted by \(M^{\lbrack k\rbrack}(d, n)\). In this paper we start the
investigation of the function \(M^{\lbrack k\rbrack}(d, n)\).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3636</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3636</id><created>2014-07-14</created><authors><author><keyname>&#x160;i&#x161;ovi&#x107;</keyname><forenames>Sabina</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author></authors><title>Toward Network-based Keyword Extraction from Multitopic Web Documents</title><categories>cs.CL cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyse the selectivity measure calculated from the complex
network in the task of the automatic keyword extraction. Texts, collected from
different web sources (portals, forums), are represented as directed and
weighted co-occurrence complex networks of words. Words are nodes and links are
established between two nodes if they are directly co-occurring within the
sentence. We test different centrality measures for ranking nodes - keyword
candidates. The promising results are achieved using the selectivity measure.
Then we propose an approach which enables extracting word pairs according to
the values of the in/out selectivity and weight measures combined with
filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3641</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3641</id><created>2014-07-14</created><authors><author><keyname>Ban</keyname><forenames>Amir</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author></authors><title>Market Share Indicates Quality</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Market share and quality, or customer satisfaction, go together. Yet
inferring one from the other appears difficult. Indeed, such an inference would
need detailed information about customer behavior, and might be clouded by
modes of behavior such as herding (following popularity) or elitism, where
customers avoid popular products. We investigate a fixed-price model where
customers are informed about their history with products and about market share
data. We find that it is in fact correct to make a Bayesian inference that the
product with the higher market share has the better quality under few and
unrestrictive assumptions on customer behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3646</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3646</id><created>2014-07-14</created><authors><author><keyname>Lipinski</keyname><forenames>Z.</forenames></author></authors><title>Stability of routing strategies for the maximum lifetime problem in
  ad-hoc wireless networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the maximum lifetime problem for a one-dimensional, regular ad-hoc
wireless network with one data collector $L_N$ for any data transmission cost
energy matrix which elements $E_{i,j}$ are superadditive functions, i.e.,
satisfy the inequality $\forall_{i\leq j\leq k}\;E_{i,j}+E_{j,k}\leq E_{i,k}$.
We analyze stability of the solution under modification of two sets of
parameters, the amount of data $Q_i$, $i\in [1,N]$ generated by each node and
location of the nodes $x_i$ in the network. We assume, that the data
transmission cost energy matrix $E_{i,j}$ is a function of a distance between
network nodes and thus the change of the node location causes change of
$E_{i,j}$. We say, that a solution $q(t_0)$ of the maximum network lifetime
problem is stable under modification of a given parameter $t_0$ in the
stability region $U(t_0)$, if the data flow matrix $q(t)$ is a solution of the
problem for any $t\in U(t_0)$. In the paper we estimate the size of the
stability region $U(Q^0,d^0)$ for the solution of the maximum network lifetime
problem for the $L_N$ network in the neighborhoods of the points $Q^0_i=1$,
$d^0_i=0$, where $d_i\in (-1,1)$ describes the shift of the nodes from their
initial location $x_i^0=i$, i.e., $x_i=i-d_i$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3647</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3647</id><created>2014-07-11</created><authors><author><keyname>Zhang</keyname><forenames>Aixian</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author></authors><title>A New Criterion on Normal Bases of Finite Field Extensions</title><categories>math.NT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new criterion on normal bases of finite field extension $\mathbb{F}_{q^n} /
\mathbb{F}_{q}$ is presented and explicit criterions for several particular
finite field extensions are derived from this new criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3657</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3657</id><created>2014-07-11</created><authors><author><keyname>Mahmoodi</keyname><forenames>Mahdi</forenames><affiliation>Kharazmi University of Tehran</affiliation></author><author><keyname>Jahromi</keyname><forenames>Gelayol Safavi</forenames><affiliation>Semnan University</affiliation></author></authors><title>A New Fuzzy DEMATEL-TODIM Hybrid Method for evaluation criteria of
  Knowledge management in supply chain</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge management (KM) adoption in the supply chain network needs a good
investment as well as few changes in the culture of the entire SC. Knowledge
management is the process of creating, distributing and transferring
information. The goal of this study is to Rank KM criteria in supply chain
network in Iran which is important for firms these days. Criterion used in this
paper were extracted from the literature review and were confirmed by supply
chain experts. The proposed approach for ranking and finding out about these
criterion is hybrid fuzzy DEMATEL-TODIM, with using fuzzy number as data for
our studies we could avoid uncertainty. The data was gathered from PhD. And Ms.
Students in industrial engineering of Kharrazmi university of Tehran and PhD.
And Ms. Students of the management department of Semnan university. A new
hybrid approach was used for achieving the results of this study. This new
hybrid approach ranks data criteria respect to each other, then by using TODIM
for ranking respect to the best situation (gains), the rates of criterion were
determined which is a very important advantage
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3660</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3660</id><created>2014-07-04</created><authors><author><keyname>Kumar</keyname><forenames>Charan</forenames></author><author><keyname>Kumar</keyname><forenames>Dinesh</forenames></author><author><keyname>Reddy</keyname><forenames>Arun Kumar</forenames></author></authors><title>Concrete Attribute-Based Encryption Scheme with Verifiable Outsourced
  Decryption</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As more sensitive data is shared and stored by third-party sites on the
internet, there will be a need to encrypt data stored at these sites. One
drawback of encrypting data is that it can be selectively shared only at a
coarse-grained level. Attribute based encryption is a public-key-based
one-to-many encryption that allows users to encrypt and decry pt data based on
user attributes. A promising application of ABE is flexible access control of
encrypted data stored in the cloud using access policies and ascribed
attributes associated with private keys and cipher text. One.One of the main
efficiency drawbacks of the existing ABE schemes is that decryption involves
expensive pairing operations and the number of such operations grows with the
complexity of the access policy. Finally, we show an implementation scheme and
result of performance measurements, which indicates a significant reduction on
computing resources imposed on users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3661</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3661</id><created>2014-07-14</created><authors><author><keyname>Neuwirth</keyname><forenames>Christian</forenames></author><author><keyname>Peck</keyname><forenames>Angela</forenames></author><author><keyname>Simonovic</keyname><forenames>Slobodan</forenames></author></authors><title>Modeling structural change in spatial system dynamics: A Daisyworld
  example</title><categories>cs.CE</categories><doi>10.1016/j.envsoft.2014.11.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System dynamics (SD) is an effective approach for helping reveal the temporal
behavior of complex systems. Although there have been recent developments in
expanding SD to include systems' spatial dependencies, most applications have
been restricted to the simulation of diffusion processes; this is especially
true for models on structural change (e.g. LULC modeling). To address this
shortcoming, a Python program is proposed to tightly couple SD software to a
Geographic Information System (GIS). The approach provides the required
capacities for handling bidirectional and synchronized interactions of
operations between SD and GIS. In order to illustrate the concept and the
techniques proposed for simulating structural changes, a fictitious environment
called Daisyworld has been recreated in a spatial system dynamics (SSD)
environment. The comparison of spatial and non-spatial simulations emphasizes
the importance of considering spatio-temporal feedbacks. Finally, practical
applications of structural change models in agriculture and disaster management
are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3664</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3664</id><created>2014-07-14</created><authors><author><keyname>Abdelsamea</keyname><forenames>Mohammed M.</forenames></author></authors><title>An Enhancement Neighborhood connected Segmentation for 2D-Cellular Image</title><categories>cs.CV</categories><comments>International Journal of International Journal of Bioscience,
  Biochemistry and Bioinformatics, 2011. arXiv admin note: text overlap with
  arXiv:1406.0074 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good segmentation result depends on a set of &quot;correct&quot; choice for the
seeds. When the input images are noisy, the seeds may fall on atypical pixels
that are not representative of the region statistics. This can lead to
erroneous segmentation results. In this paper, an automatic seeded region
growing algorithm is proposed for cellular image segmentation. First, the
regions of interest (ROIs) extracted from the preprocessed image. Second, the
initial seeds are automatically selected based on ROIs extracted from the
image. Third, the most reprehensive seeds are selected using a machine learning
algorithm. Finally, the cellular image is segmented into regions where each
region corresponds to a seed. The aim of the proposed is to automatically
extract the Region of Interests (ROI) from in the cellular images in terms of
overcoming the explosion, under segmentation and over segmentation problems.
Experimental results show that the proposed algorithm can improve the segmented
image and the segmented results are less noisy as compared to some existing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3673</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3673</id><created>2014-07-03</created><authors><author><keyname>Tyagi</keyname><forenames>Isha</forenames></author><author><keyname>Nautiyal</keyname><forenames>Ashish</forenames></author><author><keyname>Bijalwan</keyname><forenames>Vishwanath</forenames></author><author><keyname>Balodhi</keyname><forenames>Meenu</forenames></author></authors><title>Enhanced EZW Technique for Compression of Image by Setting Detail
  Retaining Pass Number</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For keeping the data secured and maintained, compression is most essential
aspect. For which efficiency is the important part to be researched
continuously until the satisfactory result is achieved. the optimized ratio of
data is necessary for compression and embedded transmission. In this paper the
main objective is to improve the execution time evolved in EZW compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3675</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3675</id><created>2014-07-05</created><authors><author><keyname>Vijayan</keyname><forenames>Archana</forenames></author><author><keyname>Salam</keyname><forenames>Vincy</forenames></author></authors><title>A New Approach for Super resolution by Using Web Images and FFT Based
  Image Registration</title><categories>cs.CV</categories><comments>7 pages,4 figures,Published with International Journal of Engineering
  Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V12(9),473-479 June 2014</journal-ref><doi>10.14445/22315381/IJETT-V12P289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preserving accuracy is a challenging issue in super resolution images. In
this paper, we propose a new FFT based image registration algorithm and a
sparse based super resolution algorithm to improve the accuracy of super
resolution image. Given a low resolution image, our approach initially extracts
the local descriptors from the input and then the local descriptors from the
whole correlated images using the SIFT algorithm. Once this is completed, it
will compare the local descriptors on the basis of a threshold value. The
retrieved images could be having different focal length, illumination,
inclination and size. To overcome the above differences of the retrieved
images, we propose a new FFT based image registration algorithm. After the
registration stage, we apply a sparse based super resolution on the images for
recreating images with better resolution compared to the input. Based on the
PSSNR calculation and SSIM comparison, we can see that the new methodology
creates a better image than the traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3679</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3679</id><created>2014-07-14</created><updated>2014-08-31</updated><authors><author><keyname>Weihrauch</keyname><forenames>Klaus</forenames><affiliation>University of Hagen</affiliation></author><author><keyname>Jafarikhah</keyname><forenames>Tahereh</forenames><affiliation>University of Tarbiat Modares</affiliation></author></authors><title>Computable Jordan Decomposition of Linear Continuous Functionals on
  $C[0;1]$</title><categories>cs.LO math.FA</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  2, 2014) lmcs:1117</journal-ref><doi>10.2168/LMCS-10(3:13)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By the Riesz representation theorem using the Riemann-Stieltjes integral,
linear continuous functionals on the set of continuous functions from the unit
interval into the reals can either be characterized by functions of bounded
variation from the unit interval into the reals, or by signed measures on the
Borel-subsets. Each of these objects has an (even minimal) Jordan decomposition
into non-negative or non-decreasing objects. Using the representation approach
to computable analysis, a computable version of the Riesz representation
theorem has been proved by Jafarikhah, Lu and Weihrauch. In this article we
extend this result. We study the computable relation between three Banach
spaces, the space of linear continuous functionals with operator norm, the
space of (normalized) functions of bounded variation with total variation norm,
and the space of bounded signed Borel measures with variation norm. We
introduce natural representations for defining computability. We prove that the
canonical linear bijections between these spaces and their inverses are
computable. We also prove that Jordan decomposition is computable on each of
these spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3681</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3681</id><created>2014-07-14</created><authors><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Pavol</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Radhakrishna</keyname><forenames>Arjun</forenames></author><author><keyname>Ryzhyk</keyname><forenames>Leonid</forenames></author><author><keyname>Tarrach</keyname><forenames>Thorsten</forenames></author></authors><title>Regression-free Synthesis for Concurrency</title><categories>cs.PL cs.LO</categories><comments>for source code see https://github.com/thorstent/ConRepair</comments><journal-ref>Computer Aided Verification, Lecture Notes in Computer Science
  Volume 8559, 2014, pp 568-584</journal-ref><doi>10.1007/978-3-319-08867-9_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While fixing concurrency bugs, program repair algorithms may introduce new
concurrency bugs. We present an algorithm that avoids such regressions. The
solution space is given by a set of program transformations we consider in for
repair process. These include reordering of instructions within a thread and
inserting atomic sections. The new algorithm learns a constraint on the space
of candidate solutions, from both positive examples (error-free traces) and
counterexamples (error traces). From each counterexample, the algorithm learns
a constraint necessary to remove the errors. From each positive examples, it
learns a constraint that is necessary in order to prevent the repair from
turning the trace into an error trace. We implemented the algorithm and
evaluated it on simplified Linux device drivers with known bugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3682</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3682</id><created>2014-07-14</created><updated>2014-07-15</updated><authors><author><keyname>Anagnostou</keyname><forenames>Paolo</forenames></author><author><keyname>Capocasa</keyname><forenames>Marco</forenames></author><author><keyname>Milia</keyname><forenames>Nicola</forenames></author><author><keyname>Sanna</keyname><forenames>Emanuele</forenames></author><author><keyname>Luzi</keyname><forenames>Daniela</forenames></author><author><keyname>Bisol</keyname><forenames>Giovanni Destro</forenames></author></authors><title>When data sharing gets close to 100%: what ancient human DNA studies can
  teach the Open Science movement</title><categories>q-bio.PE cs.DL</categories><comments>26 pages, 7 figures (1 supplementary), 6 Tables (5 supplementary of
  which 2 are available only upon request)</comments><doi>10.1371/journal.pone.0121409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study analyzes rates and ways of data sharing regarding mitochondrial, Y
chromosomal and autosomal polymorphisms in a total of 162 papers on human
ancient DNA published between 1988 and 2013. For the most part, data are
available in such a way as to make their scrutiny and reuse possible. The
estimated sharing rate is not far from totality (97.6% +/- 2.1%) and
substantially higher than observed in other fields of genetic research
(Evolutionary, Medical and Forensic Genetics). A questionnaire-based survey
suggests that the authors awareness of the importance of openness and
transparency for scientific progress is a fundamental factor for the
achievement of such a high sharing rate. Most data were made available through
body text, but the use of primary databases increased with the application of
complete mitochondrial and next generation sequencing methods. Our study
highlights three important aspects. First, we provide evidence that researchers
motivations are as necessary as stakeholders policies and norms to achieve very
high sharing rates. Second, careful analyses of the ways in which data are made
available are an important first step to maximize data findability,
accessibility, useability and preservation. Third and finally, the case of
human ancient DNA studies demonstrates how Open Science can foster scientific
advancements, showing that openness and transparency can help build rigorous
and reliable scientific practices even in the presence of complex experimental
challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3685</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3685</id><created>2014-07-14</created><authors><author><keyname>Bagnall</keyname><forenames>Anthony</forenames></author><author><keyname>Hills</keyname><forenames>Jon</forenames></author><author><keyname>Lines</keyname><forenames>Jason</forenames></author></authors><title>Finding Motif Sets in Time Series</title><categories>cs.LG cs.DB</categories><report-no>CMPC14-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-series motifs are representative subsequences that occur frequently in a
time series; a motif set is the set of subsequences deemed to be instances of a
given motif. We focus on finding motif sets. Our motivation is to detect motif
sets in household electricity-usage profiles, representing repeated patterns of
household usage.
  We propose three algorithms for finding motif sets. Two are greedy algorithms
based on pairwise comparison, and the third uses a heuristic measure of set
quality to find the motif set directly. We compare these algorithms on
simulated datasets and on electricity-usage data. We show that Scan MK, the
simplest way of using the best-matching pair to find motif sets, is less
accurate on our synthetic data than Set Finder and Cluster MK, although the
latter is very sensitive to parameter settings. We qualitatively analyse the
outputs for the electricity-usage data and demonstrate that both Scan MK and
Set Finder can discover useful motif sets in such data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3686</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3686</id><created>2014-07-14</created><authors><author><keyname>Gonz&#xe1;lez</keyname><forenames>Alejandro</forenames></author><author><keyname>Ramos</keyname><forenames>Sebastian</forenames></author><author><keyname>V&#xe1;zquez</keyname><forenames>David</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Antonio M.</forenames></author><author><keyname>Amores</keyname><forenames>Jaume</forenames></author></authors><title>Spatiotemporal Stacked Sequential Learning for Pedestrian Detection</title><categories>cs.CV</categories><comments>8 pages, 5 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedestrian classifiers decide which image windows contain a pedestrian. In
practice, such classifiers provide a relatively high response at neighbor
windows overlapping a pedestrian, while the responses around potential false
positives are expected to be lower. An analogous reasoning applies for image
sequences. If there is a pedestrian located within a frame, the same pedestrian
is expected to appear close to the same location in neighbor frames. Therefore,
such a location has chances of receiving high classification scores during
several frames, while false positives are expected to be more spurious. In this
paper we propose to exploit such correlations for improving the accuracy of
base pedestrian classifiers. In particular, we propose to use two-stage
classifiers which not only rely on the image descriptors required by the base
classifiers but also on the response of such base classifiers in a given
spatiotemporal neighborhood. More specifically, we train pedestrian classifiers
using a stacked sequential learning (SSL) paradigm. We use a new pedestrian
dataset we have acquired from a car to evaluate our proposal at different frame
rates. We also test on a well known dataset: Caltech. The obtained results show
that our SSL proposal boosts detection accuracy significantly with a minimal
impact on the computational cost. Interestingly, SSL improves more the accuracy
at the most dangerous situations, i.e. when a pedestrian is close to the
camera.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3692</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3692</id><created>2014-07-11</created><authors><author><keyname>Shaw</keyname><forenames>Paul D.</forenames></author><author><keyname>Graham</keyname><forenames>Martin</forenames></author><author><keyname>Kennedy</keyname><forenames>Jessie</forenames></author><author><keyname>Milne</keyname><forenames>Iain</forenames></author><author><keyname>Marshall</keyname><forenames>David F.</forenames></author></authors><title>Helium: Visualization of Large Scale Plant Pedigrees</title><categories>cs.HC</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Plant breeders are utilising an increasingly diverse range of
data types in order to identify lines that have desirable characteristics which
are suitable to be taken forward in plant breeding programmes. There are a
number of key morphological and physiological traits such as disease resistance
and yield that are required to be maintained, and improved upon if a commercial
variety is to be successful. Computational tools that provide the ability to
pull this data together, and integrate with pedigree structure, will enable
breeders to make better decisions on which plant lines are used in crossings to
meet both critical demands for increased yield/production and adaptation to
climate change. Results: We have used a large and unique set of experimental
barley (H. vulgare) data to develop a prototype pedigree visualization system
and performed a subjective user evaluation with domain experts to guide and
direct the development of an interactive pedigree visualization tool which we
have called Helium. Conclusions: We show that Helium allows users to easily
integrate a number of data types along with large plant pedigrees to offer an
integrated environment in which they can explore pedigree data. We have also
verified that users were happy with the abstract representation of pedigrees
that we have used in our visualization tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3695</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3695</id><created>2014-06-22</created><authors><author><keyname>Stankovi&#x107;</keyname><forenames>Isidora</forenames></author></authors><title>Recovery of Images with Missing Pixels using a Gradient Compressive
  Sensing Algorithm</title><categories>cs.CV</categories><comments>7 pages, 12 figures, A part of the Final year project at the
  University of Westminster, London, United Kingdom, supervised by Dragana
  Barjamovic (submitted 28. April 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the possibility of reconstruction of images
considering that they are sparse in the DCT transformation domain. Two
approaches are considered. One when the image is pre-processed in the DCT
domain, using 8x8 blocks. The image is made sparse by setting the smallest DCT
coefficients to zero. In the other case the original image is considered
without pre-processing, assuming the sparsity as intrinsic property of the
analyzed image. A gradient based algorithm is used to recover a large number of
missing pixels in the image. The case of a salt-and-paper noise affecting a
large number of pixels is easily reduced to the case of missing pixels and
considered within the same framework. The reconstruction of images affected
with salt-and-paper impulsive is compared with the images filtered using a
median filter. The same algorithm can be used considering transformation of the
whole image. Reconstructions of black and white and colour images are
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3698</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3698</id><created>2014-07-14</created><authors><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author></authors><title>Diffusion Adaptation Strategies for Distributed Estimation over Gaussian
  Markov Random Fields</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Signal Processing. arXiv admin
  note: text overlap with arXiv:1206.3099</comments><journal-ref>IEEE Taransactions on Signal Processing, vol. 62, no. 21, pp.
  5748-5760, Nov. 2014</journal-ref><doi>10.1109/TSP.2014.2356433</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to propose diffusion strategies for distributed
estimation over adaptive networks, assuming the presence of spatially
correlated measurements distributed according to a Gaussian Markov random field
(GMRF) model. The proposed methods incorporate prior information about the
statistical dependency among observations, while at the same time processing
data in real-time and in a fully decentralized manner. A detailed mean-square
analysis is carried out in order to prove stability and evaluate the
steady-state performance of the proposed strategies. Finally, we also
illustrate how the proposed techniques can be easily extended in order to
incorporate thresholding operators for sparsity recovery applications.
Numerical results show the potential advantages of using such techniques for
distributed learning in adaptive networks deployed over GMRF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3701</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3701</id><created>2014-07-14</created><authors><author><keyname>Durante</keyname><forenames>Joe</forenames></author><author><keyname>Whitehouse</keyname><forenames>Tyler</forenames></author><author><keyname>Serpa</keyname><forenames>F. G.</forenames></author><author><keyname>Javier</keyname><forenames>Artjay</forenames></author></authors><title>Expertise localization discovered through correlation of key term
  distribution and community detection in co-author networks</title><categories>cs.SI cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient and effective automatic method for determining the
research focus of scientific communities found in co-authorship networks. It
utilizes bibliographic data from a database to form the network, followed by
fastgreedy community detection to identify communities within large connected
components of the network. Text analysis techniques are used to identify
community-specific significant terms which represent the topic of the
community. In order to greatly reduce computation time, the `Topics' field of
each publication in the network is analyzed rather than its entire text. Using
this text analysis approach requires a certain level of statistical
confidence,therefore analyzing very small communities is not effective with
this technique. We find a minimum community size threshold of 8 coauthored
papers; below this value, the community's topic cannot be reliably identified
with this method. Additionally, we consider the implications this study has
regarding factors involved in the formation of scientific communities in
co-authorship networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3704</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3704</id><created>2014-07-14</created><authors><author><keyname>Barni</keyname><forenames>Mauro</forenames></author><author><keyname>Tondi</keyname><forenames>Benedetta</forenames></author></authors><title>Source Distinguishability under Distortion-Limited Attack: an Optimal
  Transport Perspective</title><categories>cs.SY cs.IT math.IT</categories><comments>IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the distinguishability of two sources in a Neyman-Pearson set-up
when an attacker is allowed to modify the output of one of the two sources
subject to a distortion constraint. By casting the problem in a game-theoretic
framework and by exploiting the parallelism between the attacker's goal and
Optimal Transport Theory, we introduce the concept of Security Margin defined
as the maximum average per-sample distortion introduced by the attacker for
which the two sources can be distinguished ensuring arbitrarily small, yet
positive, error exponents for type I and type II error probabilities. Several
versions of the problem are considered according to the available knowledge
about the sources and the type of distance used to define the distortion
constraint. We compute the security margin for some classes of sources and
derive a general upper bound assuming that the distortion is measured in terms
of the mean square error between the original and the attacked sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3706</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3706</id><created>2014-07-14</created><updated>2014-07-30</updated><authors><author><keyname>Pandolfi</keyname><forenames>L.</forenames></author></authors><title>Cosine operator and controllability of the wave equation with memory
  revisited</title><categories>math.OC cs.SY</categories><msc-class>93C05, 93C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controllability of the heat equations with memory (of Gurtin-Pipkin type) has
been studied using several methods with the following in common: the existing
results on controllability of the (memoryless) wave equation are lifted to the
equation with memory.
  Here we revisit the approach based on operator methods (cosine operators) and
we improve the results obtained in a previous paper (L. Pandolfi: &quot;The
controllability of the Gurtin-Pipkin equation: a cosine operator approach:,
Appl.Math. and Optim. 52, 143-165, 2005. In particular we identify the control
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3716</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3716</id><created>2014-07-14</created><updated>2014-10-26</updated><authors><author><keyname>Malek-Mohammadi</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Performance Guarantees for Schatten-$p$ Quasi-Norm Minimization in
  Recovery of Low-Rank Matrices</title><categories>cs.IT math.IT stat.ML</categories><comments>Submitted to Elsevier Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address some theoretical guarantees for Schatten-$p$ quasi-norm
minimization ($p \in (0,1]$) in recovering low-rank matrices from compressed
linear measurements. Firstly, using null space properties of the measurement
operator, we provide a sufficient condition for exact recovery of low-rank
matrices. This condition guarantees unique recovery of matrices of ranks equal
or larger than what is guaranteed by nuclear norm minimization. Secondly, this
sufficient condition leads to a theorem proving that all restricted isometry
property (RIP) based sufficient conditions for $\ell_p$ quasi-norm minimization
generalize to Schatten-$p$ quasi-norm minimization. Based on this theorem, we
provide a few RIP-based recovery conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3740</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3740</id><created>2014-07-14</created><updated>2015-07-27</updated><authors><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Space Lower Bounds for Itemset Frequency Sketches</title><categories>cs.DS</categories><comments>Minor corrections relative to previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a database, computing the fraction of rows that contain a query itemset
or determining whether this fraction is above some threshold are fundamental
operations in data mining. A uniform sample of rows is a good sketch of the
database in the sense that all sufficiently frequent itemsets and their
approximate frequencies are recoverable from the sample, and the sketch size is
independent of the number of rows in the original database. For many seemingly
similar problems there are better sketching algorithms than uniform sampling.
In this paper we show that for itemset frequency sketching this is not the
case. That is, we prove that there exist classes of databases for which uniform
sampling is a space optimal sketch for approximate itemset frequency analysis,
up to constant or iterated-logarithmic factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3745</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3745</id><created>2014-07-14</created><authors><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Holder</keyname><forenames>Lawrence</forenames></author><author><keyname>Chin</keyname><forenames>George</forenames></author><author><keyname>Mackey</keyname><forenames>Patrick</forenames></author><author><keyname>Agarwal</keyname><forenames>Khushbu</forenames></author><author><keyname>Feo</keyname><forenames>John</forenames></author></authors><title>Query Optimization for Dynamic Graphs</title><categories>cs.DB</categories><report-no>PNNL-SA-103238, Pacific Northwest National Laboratory, Richland, WA</report-no><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Given a query graph that represents a pattern of interest, the emerging
pattern detection problem can be viewed as a continuous query problem on a
dynamic graph. We present an incremental algorithm for continuous query
processing on dynamic graphs. The algorithm is based on the concept of query
decomposition; we decompose a query graph into smaller subgraphs and assemble
the result of sub-queries to find complete matches with the specified query.
The novelty of our work lies in using the subgraph distributional statistics
collected from the dynamic graph to generate the decomposition. We introduce a
&quot;Lazy Search&quot; algorithm where the search strategy is decided on a
vertex-to-vertex basis depending on the likelihood of a match in the vertex
neighborhood. We also propose a metric named &quot;Relative Selectivity&quot; that is
used to select between different query decomposition strategies. Our
experiments performed on real online news, network traffic stream and a
synthetic social network benchmark demonstrate 10-100x speedups over competing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3751</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3751</id><created>2014-07-14</created><authors><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Dowling</keyname><forenames>Chase</forenames></author></authors><title>Benchmarking Named Entity Disambiguation approaches for Streaming Graphs</title><categories>cs.CL cs.IR</categories><report-no>PNNL-23455 2014-05, Pacific Northwest National Laboratory, Richland,
  WA</report-no><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Named Entity Disambiaguation (NED) is a central task for applications dealing
with natural language text. Assume that we have a graph based knowledge base
(subsequently referred as Knowledge Graph) where nodes represent various real
world entities such as people, location, organization and concepts. Given data
sources such as social media streams and web pages Entity Linking is the task
of mapping named entities that are extracted from the data to those present in
the Knowledge Graph. This is an inherently difficult task due to several
reasons. Almost all these data sources are generated without any formal
ontology; the unstructured nature of the input, limited context and the
ambiguity involved when multiple entities are mapped to the same name make this
a hard task. This report looks at two state of the art systems employing two
distinctive approaches: graph based Accurate Online Disambiguation of Entities
(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a
statistical inference approach. We compare both approaches using the data set
and queries provided by the Knowledge Base Population (KBP) track at 2011 NIST
Text Analytics Conference (TAC). This report begins with an overview of the
respective approaches, followed by detailed description of the experimental
setup. It concludes with our findings from the benchmarking exercise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3757</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3757</id><created>2014-07-09</created><authors><author><keyname>Kolesar</keyname><forenames>Ivan</forenames></author><author><keyname>Parulek</keyname><forenames>Julius</forenames></author><author><keyname>Viola</keyname><forenames>Ivan</forenames></author><author><keyname>Bruckner</keyname><forenames>Stefan</forenames></author><author><keyname>Stavrum</keyname><forenames>Anne-Kristin</forenames></author><author><keyname>Hauser</keyname><forenames>Helwig</forenames></author></authors><title>Illustrating Polymerization using Three-level Model Fusion</title><categories>cs.HC</categories><comments>BioVis 2014 conference</comments><proxy>Jan Aerts</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in cell biology is steadily contributing new knowledge about many
different aspects of physiological processes like polymerization, both with
respect to the involved molecular structures as well as their related function.
Illustrations of the spatio-temporal development of such processes are not only
used in biomedical education, but also can serve scientists as an additional
platform for in-silico experiments. In this paper, we contribute a new,
three-level modeling approach to illustrate physiological processes from the
class of polymerization at different time scales. We integrate physical and
empirical modeling, according to which approach suits the different involved
levels of detail best, and we additionally enable a simple form of interactive
steering while the process is illustrated. We demonstrate the suitability of
our approach in the context of several polymerization processes and report from
a first evaluation with domain experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3764</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3764</id><created>2014-07-14</created><updated>2015-09-14</updated><authors><author><keyname>Betea</keyname><forenames>Dan</forenames></author><author><keyname>Boutillier</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Bouttier</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Chapuy</keyname><forenames>Guillaume</forenames></author><author><keyname>Corteel</keyname><forenames>Sylvie</forenames></author><author><keyname>Vuleti&#x107;</keyname><forenames>Mirjana</forenames></author></authors><title>Perfect sampling algorithm for Schur processes</title><categories>math.PR cond-mat.stat-mech cs.DM math.CO</categories><comments>26 pages, 22 figures (changes in v2: extended version with more
  details, new sections on symmetric Schur processes and other generalizations)</comments><report-no>IPhT-t14/101</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe random generation algorithms for a large class of random
combinatorial objects called Schur processes, which are sequences of random
(integer) partitions subject to certain interlacing conditions. This class
contains several fundamental combinatorial objects as special cases, such as
plane partitions, tilings of Aztec diamonds, pyramid partitions and more
generally steep domino tilings of the plane. Our algorithm, which is of
polynomial complexity, is both exact (i.e. the output follows exactly the
target probability law, which is either Boltzmann or uniform in our case), and
entropy optimal (i.e. it reads a minimal number of random bits as an input).
  The algorithm encompasses previous growth procedures for special Schur
processes related to the primal and dual RSK algorithm, as well as the famous
domino shuffling algorithm for domino tilings of the Aztec diamond. It can be
easily adapted to deal with symmetric Schur processes and general Schur
processes involving infinitely many parameters. It is more concrete and easier
to implement than Borodin's algorithm, and it is entropy optimal.
  At a technical level, it relies on unified bijective proofs of the different
types of Cauchy and Littlewood identities for Schur functions, and on an
adaptation of Fomin's growth diagram description of the RSK algorithm to that
setting. Simulations performed with this algorithm suggest interesting limit
shape phenomena for the corresponding tiling models, some of which are new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3809</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3809</id><created>2014-07-14</created><authors><author><keyname>Wism&#xfc;ller</keyname><forenames>Axel</forenames></author><author><keyname>Wang</keyname><forenames>Xixi</forenames></author><author><keyname>DSouza</keyname><forenames>Adora M.</forenames></author><author><keyname>Nagarajan</keyname><forenames>Mahesh B.</forenames></author></authors><title>A Framework for Exploring Non-Linear Functional Connectivity and
  Causality in the Human Brain: Mutual Connectivity Analysis (MCA) of
  Resting-State Functional MRI with Convergent Cross-Mapping and Non-Metric
  Clustering</title><categories>cs.NE q-bio.NC</categories><comments>Axel Wism\&quot;uller and Mahesh B. Nagarajan contributed equally to the
  preparation of this manuscript. Pre-publication draft: 18 pages, 6 figures, 1
  table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a computational framework for analysis and visualization of
non-linear functional connectivity in the human brain from resting state
functional MRI (fMRI) data for purposes of recovering the underlying network
community structure and exploring causality between network components. Our
proposed methodology of non-linear mutual connectivity analysis (MCA) involves
two computational steps. First, the pair-wise cross-prediction performance
between resting state fMRI pixel time series within the brain is evaluated. The
underlying network structure is subsequently recovered from the affinity matrix
constructed through MCA using non-metric network partitioning/clustering with
the so-called Louvain method. We demonstrate our methodology in the task of
identifying regions of the motor cortex associated with hand movement on
resting state fMRI data acquired from eight slice locations in four subjects.
For comparison, we also localized regions of the motor cortex through a
task-based fMRI sequence involving a finger-tapping stimulus paradigm. Finally,
we integrate convergent cross mapping (CCM) into the first step of MCA for
investigating causality between regions of the motor cortex. Results regarding
causation between regions of the motor cortex revealed a significant
directional variability and were not readily interpretable in a consistent
manner across all subjects. However, our results on whole-slice fMRI analysis
demonstrate that MCA-based model-free recovery of regions associated with the
primary motor cortex and supplementary motor area are in close agreement with
localization of similar regions achieved with a task-based fMRI acquisition.
Thus, we conclude that our computational framework MCA can extract and
visualize valuable information concerning the underlying network structure and
causation between different regions of the brain in resting state fMRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3820</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3820</id><created>2014-07-14</created><authors><author><keyname>Babu</keyname><forenames>Bofin</forenames></author><author><keyname>Kumar</keyname><forenames>Mohan</forenames></author></authors><title>FilterPlus: A real-time content filtering extension for Google Chrome</title><categories>cs.CR</categories><comments>chrome security extention</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content filtering in web browsers is a tedious process for most of the
people, because of several reasons. By blocking JavaScript, Cookies and Popups,
end users can ensure maximum protection from browser based attacks and
vulnerabilities. In order to accomplish this, we build an extension for Google
Chrome which allows users to have easy control over what they wish to recievce
from a web page. We also build this extension in such a way that it remembers
the choice of options made by the user for every URLs, thereby letting users
create rules for websites they visit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3832</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3832</id><created>2014-07-14</created><authors><author><keyname>Diakidoy</keyname><forenames>Irene-Anna</forenames></author><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author><author><keyname>Michael</keyname><forenames>Loizos</forenames></author><author><keyname>Miller</keyname><forenames>Rob</forenames></author></authors><title>Non-Monotonic Reasoning and Story Comprehension</title><categories>cs.AI</categories><acm-class>I.2.3; I.2.4</acm-class><journal-ref>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014), Vienna, 1719 July, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a Reasoning about Actions and Change framework integrated
with Default Reasoning, suitable as a Knowledge Representation and Reasoning
framework for Story Comprehension. The proposed framework, which is guided
strongly by existing knowhow from the Psychology of Reading and Comprehension,
is based on the theory of argumentation from AI. It uses argumentation to
capture appropriate solutions to the frame, ramification and qualification
problems and generalizations of these problems required for text comprehension.
In this first part of the study the work concentrates on the central problem of
integration (or elaboration) of the explicit information from the narrative in
the text with the implicit (in the readers mind) common sense world knowledge
pertaining to the topic(s) of the story given in the text. We also report on
our empirical efforts to gather background common sense world knowledge used by
humans when reading a story and to evaluate, through a prototype system, the
ability of our approach to capture both the majority and the variability of
understanding of a story by the human readers in the experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3836</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3836</id><created>2014-07-14</created><authors><author><keyname>Toth</keyname><forenames>David</forenames></author></authors><title>Imparo is complete by inverse subsumption</title><categories>cs.AI</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Inverse subsumption for complete explanatory induction Yamamoto et al.
investigate which inductive logic programming systems can learn a correct
hypothesis $H$ by using the inverse subsumption instead of inverse entailment.
We prove that inductive logic programming system Imparo is complete by inverse
subsumption for learning a correct definite hypothesis $H$ wrt the definite
background theory $B$ and ground atomic examples $E$, by establishing that
there exists a connected theory $T$ for $B$ and $E$ such that $H$ subsumes $T$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3840</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3840</id><created>2014-07-14</created><updated>2015-02-11</updated><authors><author><keyname>Liu</keyname><forenames>Lee-Kang</forenames></author><author><keyname>Chan</keyname><forenames>Stanley H.</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Q.</forenames></author></authors><title>Depth Reconstruction from Sparse Samples: Representation, Algorithm, and
  Sampling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development of 3D technology and computer vision applications have
motivated a thrust of methodologies for depth acquisition and estimation.
However, most existing hardware and software methods have limited performance
due to poor depth precision, low resolution and high computational cost. In
this paper, we present a computationally efficient method to recover dense
depth maps from sparse measurements. We make three contributions. First, we
provide empirical evidence that depth maps can be encoded much more sparsely
than natural images by using common dictionaries such as wavelets and
contourlets. We also show that a combined wavelet-contourlet dictionary
achieves better performance than using either dictionary alone. Second, we
propose an alternating direction method of multipliers (ADMM) to achieve fast
reconstruction. A multi-scale warm start procedure is proposed to speed up the
convergence. Third, we propose a two-stage randomized sampling scheme to
optimally choose the sampling locations, thus maximizing the reconstruction
performance for any given sampling budget. Experimental results show that the
proposed method produces high quality dense depth estimates, and is robust to
noisy measurements. Applications to real data in stereo matching are
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3841</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3841</id><created>2014-07-14</created><updated>2015-07-15</updated><authors><author><keyname>Kolte</keyname><forenames>Ritesh</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Capacity Approximations for Gaussian Relay Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a Gaussian relay network where a source node communicates to a
destination node with the help of several layers of relays. Recent work has
shown that compress-and-forward based strategies can achieve the capacity of
this network within an additive gap. Here, the relays quantize their received
signals at the noise level and map them to random Gaussian codebooks. The
resultant gap to capacity is independent of the SNRs of the channels in the
network and the topology but is linear in the total number of nodes.
  In this paper, we provide an improved lower bound on the rate achieved by
compress-and-forward based strategies (noisy network coding in particular) in
arbitrary Gaussian relay networks, whose gap to capacity depends on the network
not only through the total number of nodes but also through the degrees of
freedom of the min cut of the network. We illustrate that for many networks,
this refined lower bound can lead to a better approximation of the capacity. In
particular, we demonstrate that it leads to a logarithmic rather than linear
capacity gap in the total number of nodes for certain classes of layered
networks. The improvement comes from quantizing the received signals of the
relays at a resolution decreasing with the total number of nodes in the
network. This suggests that the rule-of-thumb in literature of quantizing the
received signals at the noise level can be highly suboptimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3845</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3845</id><created>2014-07-14</created><authors><author><keyname>Bezanson</keyname><forenames>Jeff</forenames></author><author><keyname>Chen</keyname><forenames>Jiahao</forenames></author><author><keyname>Karpinski</keyname><forenames>Stefan</forenames></author><author><keyname>Shah</keyname><forenames>Viral</forenames></author><author><keyname>Edelman</keyname><forenames>Alan</forenames></author></authors><title>Array operators using multiple dispatch: a design methodology for array
  implementations in dynamic languages</title><categories>cs.PL</categories><comments>6 pages, 2 figures, workshop paper for the ARRAY '14 workshop, June
  11, 2014, Edinburgh, United Kingdom</comments><acm-class>D.3.3</acm-class><doi>10.1145/2627373.2627383</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arrays are such a rich and fundamental data type that they tend to be built
into a language, either in the compiler or in a large low-level library.
Defining this functionality at the user level instead provides greater
flexibility for application domains not envisioned by the language designer.
Only a few languages, such as C++ and Haskell, provide the necessary power to
define $n$-dimensional arrays, but these systems rely on compile-time
abstraction, sacrificing some flexibility. In contrast, dynamic languages make
it straightforward for the user to define any behavior they might want, but at
the possible expense of performance.
  As part of the Julia language project, we have developed an approach that
yields a novel trade-off between flexibility and compile-time analysis. The
core abstraction we use is multiple dispatch. We have come to believe that
while multiple dispatch has not been especially popular in most kinds of
programming, technical computing is its killer application. By expressing key
functions such as array indexing using multi-method signatures, a surprising
range of behaviors can be obtained, in a way that is both relatively easy to
write and amenable to compiler analysis. The compact factoring of concerns
provided by these methods makes it easier for user-defined types to behave
consistently with types in the standard library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3850</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3850</id><created>2014-07-14</created><authors><author><keyname>G&#xfc;nnemann</keyname><forenames>Stephan</forenames></author><author><keyname>Kremer</keyname><forenames>Hardy</forenames></author><author><keyname>Hannen</keyname><forenames>Matthias</forenames></author><author><keyname>Seidl</keyname><forenames>Thomas</forenames></author></authors><title>KDD-SC: Subspace Clustering Extensions for Knowledge Discovery
  Frameworks</title><categories>cs.DB</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing high dimensional data is a challenging task. For these data it is
known that traditional clustering algorithms fail to detect meaningful
patterns. As a solution, subspace clustering techniques have been introduced.
They analyze arbitrary subspace projections of the data to detect clustering
structures.
  In this paper, we present our subspace clustering extension for KDD
frameworks, termed KDD-SC. In contrast to existing subspace clustering
toolkits, our solution neither is a standalone product nor is it tightly
coupled to a specific KDD framework. Our extension is realized by a common
codebase and easy-to-use plugins for three of the most popular KDD frameworks,
namely KNIME, RapidMiner, and WEKA. KDD-SC extends these frameworks such that
they offer a wide range of different subspace clustering functionalities. It
provides a multitude of algorithms, data generators, evaluation measures, and
visualization techniques specifically designed for subspace clustering. These
functionalities integrate seamlessly with the frameworks' existing features
such that they can be flexibly combined. KDD-SC is publicly available on our
website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3855</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3855</id><created>2014-07-14</created><updated>2015-04-14</updated><authors><author><keyname>Liang</keyname><forenames>Liu</forenames></author><author><keyname>Bi</keyname><forenames>Suzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Joint Power Control and Fronthaul Rate Allocation for Throughput
  Maximization in OFDMA-based Cloud Radio Access Network</title><categories>cs.IT math.IT</categories><comments>submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of cloud radio access network (C-RAN) is constrained by the
limited fronthaul link capacity under future heavy data traffic. To tackle this
problem, extensive efforts have been devoted to design efficient signal
quantization/compression techniques in the fronthaul to maximize the network
throughput. However, most of the previous results are based on
information-theoretical quantization methods, which are hard to implement due
to the extremely high complexity. In this paper, we consider using practical
uniform scalar quantization in the uplink communication of an orthogonal
frequency division multiple access (OFDMA) based C-RAN system, where the mobile
users are assigned with orthogonal sub-carriers for multiple access. In
particular, we consider joint wireless power control and fronthaul quantization
design over the sub-carriers to maximize the system end-to-end throughput.
Efficient algorithms are proposed to solve the joint optimization problem when
either information-theoretical or practical fronthaul quantization method is
applied. Interestingly, we find that the fronthaul capacity constraints have
significant impact to the optimal wireless power control policy. As a result,
the joint optimization shows significant performance gain compared with either
optimizing wireless power control or fronthaul quantization alone. Besides, we
also show that the proposed simple uniform quantization scheme performs very
close to the throughput performance upper bound, and in fact overlaps with the
upper bound when the fronthaul capacity is sufficiently large. Overall, our
results would help reveal practically achievable throughput performance of
C-RAN, and lead to more efficient deployment of C-RAN in the next-generation
wireless communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3857</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3857</id><created>2014-07-14</created><authors><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author></authors><title>A New Approach to Efficient Enumeration by Push-out Amortization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enumeration algorithms have been one of recent hot topics in theoretical
computer science. Different from other problems, enumeration has many
interesting aspects, such as the computation time can be shorter than the total
output size, by sophisticated ordering of output solutions. One more example is
that the recursion of the enumeration algorithm is often structured well, thus
we can have good amortized analysis, and interesting algorithms for reducing
the amortized complexity. However, there is a lack of deep studies from these
points of views; there are only few results on the fundamentals of enumeration,
such as a basic design of an algorithm that is applicable to many problems. In
this paper, we address new approaches on the complexity analysis, and propose a
new way of amortized analysis Push-Out Amortization for enumeration algorithms,
where the computation time of an iteration is amortized by using all its
descendant iterations. We clarify sufficient conditions on the enumeration
algorithm so that the amortized analysis works. By the amortization, we show
that many elimination orderings, matchings in a graph, connected vertex induced
subgraphs in a graph, and spanning trees can be enumerated in O(1) time for
each solution by simple algorithms with simple proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3859</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3859</id><created>2014-07-14</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames><affiliation>MIT</affiliation></author><author><keyname>Anderson</keyname><forenames>Christian</forenames><affiliation>MIT</affiliation></author><author><keyname>Arcand</keyname><forenames>William</forenames><affiliation>MIT</affiliation></author><author><keyname>Bestor</keyname><forenames>David</forenames><affiliation>MIT</affiliation></author><author><keyname>Bergeron</keyname><forenames>Bill</forenames><affiliation>MIT</affiliation></author><author><keyname>Byun</keyname><forenames>Chansup</forenames><affiliation>MIT</affiliation></author><author><keyname>Hubbell</keyname><forenames>Matthew</forenames><affiliation>MIT</affiliation></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames><affiliation>MIT</affiliation></author><author><keyname>Mullen</keyname><forenames>Julie</forenames><affiliation>MIT</affiliation></author><author><keyname>O'Gwynn</keyname><forenames>David</forenames><affiliation>MIT</affiliation></author><author><keyname>Prout</keyname><forenames>Andrew</forenames><affiliation>MIT</affiliation></author><author><keyname>Reuther</keyname><forenames>Albert</forenames><affiliation>MIT</affiliation></author><author><keyname>Rosa</keyname><forenames>Antonio</forenames><affiliation>MIT</affiliation></author><author><keyname>Yee</keyname><forenames>Charles</forenames><affiliation>MIT</affiliation></author></authors><title>D4M 2.0 Schema: A General Purpose High Performance Schema for the
  Accumulo Database</title><categories>cs.DB astro-ph.IM cs.DC</categories><comments>6 pages; IEEE HPEC 2013</comments><doi>10.1109/HPEC.2013.6670318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-traditional, relaxed consistency, triple store databases are the backbone
of many web companies (e.g., Google Big Table, Amazon Dynamo, and Facebook
Cassandra). The Apache Accumulo database is a high performance open source
relaxed consistency database that is widely used for government applications.
Obtaining the full benefits of Accumulo requires using novel schemas. The
Dynamic Distributed Dimensional Data Model (D4M)[http://d4m.mit.edu] provides a
uniform mathematical framework based on associative arrays that encompasses
both traditional (i.e., SQL) and non-traditional databases. For non-traditional
databases D4M naturally leads to a general purpose schema that can be used to
fully index and rapidly query every unique string in a dataset. The D4M 2.0
Schema has been applied with little or no customization to cyber,
bioinformatics, scientific citation, free text, and social media data. The D4M
2.0 Schema is simple, requires minimal parsing, and achieves the highest
published Accumulo ingest rates. The benefits of the D4M 2.0 Schema are
independent of the D4M interface. Any interface to Accumulo can achieve these
benefits by using the D4M 2.0 Schema
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3866</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3866</id><created>2014-07-14</created><authors><author><keyname>Yang</keyname><forenames>Chunliang</forenames></author></authors><title>An Enhanced Leakage-Based Precoding Scheme for Multi-User Multi-Layer
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an enhanced leakage-based precoding scheme, i.e.,
layer signal to leakage plus noise ratio (layer SLNR) scheme, for multi-user
multi-layer MIMO systems. Specifically, the layer SLNR scheme incorporates the
MIMO receiver structure into the precoder design procedure, which makes the
formulation of signal power and interference / leakage power more accurate.
Besides, the layer SLNR scheme not only takes into account the inter-layer
interference from different users, but also takes care of the inter-layer
interference from the same user which is usually assumed to be zero in previous
studies. As a result, the proposed layer SLNR scheme produces a good balance
between the layer signal power and layer interference power in a multi-user
multi-layer MIMO system, therefore achieves better system performance. The
effectiveness and superiority of the proposed layer SLNR scheme are validated
via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3867</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3867</id><created>2014-07-14</created><authors><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Part-based R-CNNs for Fine-grained Category Detection</title><categories>cs.CV</categories><comments>16 pages. To appear at European Conference on Computer Vision (ECCV),
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic part localization can facilitate fine-grained categorization by
explicitly isolating subtle appearance differences associated with specific
object parts. Methods for pose-normalized representations have been proposed,
but generally presume bounding box annotations at test time due to the
difficulty of object detection. We propose a model for fine-grained
categorization that overcomes these limitations by leveraging deep
convolutional features computed on bottom-up region proposals. Our method
learns whole-object and part detectors, enforces learned geometric constraints
between them, and predicts a fine-grained category from a pose-normalized
representation. Experiments on the Caltech-UCSD bird dataset confirm that our
method outperforms state-of-the-art fine-grained categorization methods in an
end-to-end evaluation without requiring a bounding box at test time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3879</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3879</id><created>2014-07-15</created><authors><author><keyname>Prajapati</keyname><forenames>Harshadkumar B.</forenames></author><author><keyname>Shah</keyname><forenames>Vipul A.</forenames></author></authors><title>Scheduling in Grid Computing Environment</title><categories>cs.DC</categories><comments>Fourth International Conference on Advanced Computing &amp; Communication
  Technologies (ACCT), 2014</comments><doi>10.1109/ACCT.2014.32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling in Grid computing has been active area of research since its
beginning. However, beginners find very difficult to understand related
concepts due to a large learning curve of Grid computing. Thus, there is a need
of concise understanding of scheduling in Grid computing area. This paper
strives to present concise understanding of scheduling and related
understanding of Grid computing system. The paper describes overall picture of
Grid computing and discusses important sub-systems that enable Grid computing
possible. Moreover, the paper also discusses concepts of resource scheduling
and application scheduling and also presents classification of scheduling
algorithms. Furthermore, the paper also presents methodology used for
evaluating scheduling algorithms including both real system and simulation
based approaches. The presented work on scheduling in Grid containing concise
understandings of scheduling system, scheduling algorithm, and scheduling
methodology would be very useful to users and researchers
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3881</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3881</id><created>2014-07-15</created><authors><author><keyname>Prajapati</keyname><forenames>Harshadkumar B.</forenames></author><author><keyname>Shah</keyname><forenames>Vipul A.</forenames></author></authors><title>Experimental Study of Remote Job Submission and Execution on LRM through
  Grid Computing Mechanisms</title><categories>cs.DC</categories><comments>Fourth International Conference on Advanced Computing &amp; Communication
  Technologies (ACCT), 2014</comments><doi>10.1109/ACCT.2014.31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote job submission and execution is fundamental requirement of distributed
computing done using Cluster computing. However, Cluster computing limits usage
within a single organization. Grid computing environment can allow use of
resources for remote job execution that are available in other organizations.
This paper discusses concepts of batch-job execution using LRM and using Grid.
The paper discusses two ways of preparing test Grid computing environment that
we use for experimental testing of concepts. This paper presents experimental
testing of remote job submission and execution mechanisms through LRM specific
way and Grid computing ways. Moreover, the paper also discusses various
problems faced while working with Grid computing environment and discusses
their trouble-shootings. The understanding and experimental testing presented
in this paper would become very useful to researchers who are new to the field
of job management in Grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3890</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3890</id><created>2014-07-15</created><authors><author><keyname>Feld</keyname><forenames>Gilles</forenames><affiliation>SATIE, ENS Cachan &amp; CNRS, France</affiliation></author><author><keyname>Fribourg</keyname><forenames>Laurent</forenames><affiliation>LSV, ENS de Cachan &amp; CNRS, France</affiliation></author><author><keyname>Labrousse</keyname><forenames>Denis</forenames><affiliation>SATIE, ENS Cachan &amp; CNRS, France</affiliation></author><author><keyname>Revol</keyname><forenames>Bertrand</forenames><affiliation>SATIE, ENS Cachan &amp; CNRS, France</affiliation></author><author><keyname>Soulat</keyname><forenames>Romain</forenames><affiliation>LSV, ENS de Cachan &amp; CNRS, France</affiliation></author></authors><title>Correct-by-design Control Synthesis for Multilevel Converters using
  State Space Decomposition</title><categories>cs.SY cs.SE</categories><comments>In Proceedings FSFMA 2014, arXiv:1407.1952</comments><proxy>EPTCS</proxy><acm-class>B.6.3; D.2.4; G.1.7</acm-class><journal-ref>EPTCS 156, 2014, pp. 5-16</journal-ref><doi>10.4204/EPTCS.156.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-power converters based on elementary switching cells are more and more
used in the industry of power electronics owing to various advantages such as
lower voltage stress and reduced power loss. However, the complexity of
controlling such converters is a major challenge that the power manufacturing
industry has to face with. The synthesis of industrial switching controllers
relies today on heuristic rules and empiric simulation. The state of the system
is not guaranteed to stay within the limits that are admissible for its correct
electrical behavior. We show here how to apply a formal method in order to
synthesize a correct-by-design control that guarantees that the power converter
will always stay within a predefined safe zone of variations for its input
parameters. The method is applied in order to synthesize a correct-by-design
control for 5-level and 7-level power converters with a flying capacitor
topology. We check the validity of our approach by numerical simulations for 5
and 7 levels. We also perform physical experimentations using a prototype built
by SATIE laboratory for 5 levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3891</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3891</id><created>2014-07-15</created><authors><author><keyname>Vanit-Anunchai</keyname><forenames>Somsak</forenames><affiliation>School of Telecommunication Engineering, Institute of Engineering, Suranaree University of Technology</affiliation></author></authors><title>Experience using Coloured Petri Nets to Model Railway Interlocking
  Tables</title><categories>cs.SE</categories><comments>In Proceedings FSFMA 2014, arXiv:1407.1952</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 156, 2014, pp. 17-28</journal-ref><doi>10.4204/EPTCS.156.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interlocking tables are the functional specification defining the routes on
which the passage of the train is allowed. Associated with the route, the
states and actions of all related signalling equipment are also specified. It
is well-known that designing and verifying the interlocking tables are labour
intensive, tedious and prone to errors. To assist the verification process and
detect errors rapidly, we formally model and analyse the interlocking tables
using Coloured Petri Nets (CPNs). Although a large interlocking table can be
easily modelled, analysing the model is rather difficult due to the state
explosion problem and undesired safe deadlocks. The safe deadlocks are when no
train collides but the train traffic cannot proceed any further. For ease of
analysis we incorporate automatic route setting and automatic route cancelling
functions into the model. These help reducing the number of the deadlocks. We
also exploit the new features of CPN Tools; prioritized transitions; inhibitor
arcs; and reset arcs. These help reducing the size of the state spaces. We also
include a fail safe specification called flank protection into the interlocking
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3892</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3892</id><created>2014-07-15</created><authors><author><keyname>Sharma</keyname><forenames>Asankhaya</forenames></author></authors><title>Verified Subtyping with Traits and Mixins</title><categories>cs.PL cs.LO cs.SE</categories><comments>In Proceedings FSFMA 2014, arXiv:1407.1952</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 156, 2014, pp. 45-51</journal-ref><doi>10.4204/EPTCS.156.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traits allow decomposing programs into smaller parts and mixins are a form of
composition that resemble multiple inheritance. Unfortunately, in the presence
of traits, programming languages like Scala give up on subtyping relation
between objects. In this paper, we present a method to check subtyping between
objects based on entailment in separation logic. We implement our method as a
domain specific language in Scala and apply it on the Scala standard library.
We have verified that 67% of mixins used in the Scala standard library do
indeed conform to subtyping between the traits that are used to build them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3896</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3896</id><created>2014-07-15</created><authors><author><keyname>Booth</keyname><forenames>Richard</forenames></author><author><keyname>Gabbay</keyname><forenames>Dov</forenames></author><author><keyname>Kaci</keyname><forenames>Souhila</forenames></author><author><keyname>Rienstra</keyname><forenames>Tjitze</forenames></author><author><keyname>van der Torre</keyname><forenames>Leendert</forenames></author></authors><title>Abduction and Dialogical Proof in Argumentation and Logic Programming</title><categories>cs.AI cs.LO</categories><comments>Appears in the Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model of abduction in abstract argumentation, where changes to
an argumentation framework act as hypotheses to explain the support of an
observation. We present dialogical proof theories for the main decision
problems (i.e., finding hypothe- ses that explain skeptical/credulous support)
and we show that our model can be instantiated on the basis of abductive logic
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3897</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3897</id><created>2014-07-15</created><updated>2014-10-02</updated><authors><author><keyname>O'Gorman</keyname><forenames>Bryan</forenames></author><author><keyname>Perdomo-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Babbush</keyname><forenames>Ryan</forenames></author><author><keyname>Aspuru-Guzik</keyname><forenames>Alan</forenames></author><author><keyname>Smelyanskiy</keyname><forenames>Vadim</forenames></author></authors><title>Bayesian Network Structure Learning Using Quantum Annealing</title><categories>quant-ph cs.LG</categories><journal-ref>Eur. Phys. J. Spec. Top., 225 (1), 163 (2015)</journal-ref><doi>10.1140/epjst/e2015-02349-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method for the problem of learning the structure of a Bayesian
network using the quantum adiabatic algorithm. We do so by introducing an
efficient reformulation of a standard posterior-probability scoring function on
graphs as a pseudo-Boolean function, which is equivalent to a system of 2-body
Ising spins, as well as suitable penalty terms for enforcing the constraints
necessary for the reformulation; our proposed method requires $\mathcal O(n^2)$
qubits for $n$ Bayesian network variables. Furthermore, we prove lower bounds
on the necessary weighting of these penalty terms. The logical structure
resulting from the mapping has the appealing property that it is
instance-independent for a given number of Bayesian network variables, as well
as being independent of the number of data cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3926</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3926</id><created>2014-07-15</created><updated>2015-06-19</updated><authors><author><keyname>Klimos</keyname><forenames>Miroslav</forenames></author><author><keyname>Kucera</keyname><forenames>Antonin</forenames></author></authors><title>Strategy Synthesis for General Deductive Games Based on SAT Solving</title><categories>cs.AI cs.GT</categories><comments>A preliminary version submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for modelling and solving deductive games,
where one player selects a secret code and the other player strives to discover
this code using a minimal number of allowed experiments that reveal some
partial information about the code. The framework is implemented in a software
tool Cobra, and its functionality is demonstrated by producing new results
about existing deductive games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3930</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3930</id><created>2014-07-15</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Sooda</keyname><forenames>Kavitha</forenames></author><author><keyname>S</keyname><forenames>Kavya B.</forenames></author><author><keyname>M</keyname><forenames>Sushma</forenames></author></authors><title>Comparative performance analysis of AntHocNet and DSR in optimal path
  route determination</title><categories>cs.NI</categories><comments>7 pages, 4 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:1101.0209 by other authors</comments><journal-ref>Anusandhana Journal, NMIT, Volume 1, Issue 2, December, 2012, pp
  1-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in current Internet routing is the application of
improved bio-inspired algorithms in deriving the path towards the destination.
Current routing trends determine the optimal path based on throughput, delay or
combination of these two metric. Here in this paper, we compare the performance
of the two algorithms, AntHocNet (AHN) and Dynamic Source Routing (DSR) for
determining the routing path. We introduce the goodput factor as one of the
metric for determining the path. Goodput is the number of useful information
bits, delivered by the network to a certain destination per unit time .The
results show that AHN outperformed DSR in terms of end-to-end delay and
overhead. Also the simulation results showed that overhead of AHN was 20.67%
lesser than DSR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3939</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3939</id><created>2014-07-15</created><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>DI-ENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Genuer</keyname><forenames>Robin</forenames><affiliation>ISPED, INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Analysis of purely random forests bias</title><categories>math.ST cs.LG stat.ME stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random forests are a very effective and commonly used statistical method, but
their full theoretical analysis is still an open problem. As a first step,
simplified models such as purely random forests have been introduced, in order
to shed light on the good performance of random forests. In this paper, we
study the approximation error (the bias) of some purely random forest models in
a regression framework, focusing in particular on the influence of the number
of trees in the forest. Under some regularity assumptions on the regression
function, we show that the bias of an infinite forest decreases at a faster
rate (with respect to the size of each tree) than a single tree. As a
consequence, infinite forests attain a strictly better risk rate (with respect
to the sample size) than single trees. Furthermore, our results allow to derive
a minimum number of trees sufficient to reach the same rate as an infinite
forest. As a by-product of our analysis, we also show a link between the bias
of purely random forests and the bias of some kernel estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3948</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3948</id><created>2014-07-15</created><authors><author><keyname>Matthies</keyname><forenames>Denys J. C.</forenames></author><author><keyname>Manke</keyname><forenames>Felix M.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Franz</forenames></author><author><keyname>Makri</keyname><forenames>Charalampia</forenames></author><author><keyname>Anthes</keyname><forenames>Christoph</forenames></author><author><keyname>Kranzlm&#xfc;ller</keyname><forenames>Dieter</forenames></author></authors><title>VR-Stepper: A Do-It-Yourself Game Interface For Locomotion In Virtual
  Environments</title><categories>cs.HC</categories><msc-class>68-01</msc-class><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to real world tasks, completing tasks in a virtual environment (VE)
seldom involves the whole spectrum of skills the human body offers. User input
in a VE is commonly accomplished through simple finger gestures, such as
walking in a scene by simply pressing a button, even if this kind of
interaction is not very suitable. In order to create a more intuitive and
natural interaction, diverse projects try to tackle the problem of locomotion
in VEs by trying to enable a natural walking movement, which is also supposed
to increase the level of immersion. Existing solutions such as treadmills are
still expensive and need additional fixation of the body. In this paper, we
describe a simple and inexpensive way to build a useful locomotion interface
using a conventional sports stepper and an Arduino. This device enables control
in a VE by walking-in-place and without the need for any additional fixation
gadgets. We conducted a user study with 10 participants to evaluate the
impression on the joy and ease of use, immersion and reliability in comparison
to other interfaces used for locomotion, such as the Wii Balance Board and a
Wand Joystick. We found out that the stepper is experienced slightly better in
terms of immersion and joy of use. Furthermore, found that pressing buttons on
a Joystick was perceived to be more reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3950</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3950</id><created>2014-07-15</created><authors><author><keyname>Drachen</keyname><forenames>Anders</forenames></author><author><keyname>Thurau</keyname><forenames>Christian</forenames></author><author><keyname>Sifa</keyname><forenames>Rafet</forenames></author><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>A Comparison of Methods for Player Clustering via Behavioral Telemetry</title><categories>cs.HC</categories><comments>Foundations of Digital Games 2013</comments><msc-class>N/A</msc-class><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of user behavior in digital games has been aided by the
introduction of user telemetry in game development, which provides
unprecedented access to quantitative data on user behavior from the installed
game clients of the entire population of players. Player behavior telemetry
datasets can be exceptionally complex, with features recorded for a varying
population of users over a temporal segment that can reach years in duration.
Categorization of behaviors, whether through descriptive methods (e.g.
segmention) or unsupervised/supervised learning techniques, is valuable for
finding patterns in the behavioral data, and developing profiles that are
actionable to game developers. There are numerous methods for unsupervised
clustering of user behavior, e.g. k-means/c-means, Non-negative Matrix
Factorization, or Principal Component Analysis. Although all yield behavior
categorizations, interpretation of the resulting categories in terms of actual
play behavior can be difficult if not impossible. In this paper, a range of
unsupervised techniques are applied together with Archetypal Analysis to
develop behavioral clusters from playtime data of 70,014 World of Warcraft
players, covering a five year interval. The techniques are evaluated with
respect to their ability to develop actionable behavioral profiles from the
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3956</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3956</id><created>2014-07-15</created><updated>2014-12-29</updated><authors><author><keyname>Schmitzer</keyname><forenames>Bernhard</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Globally Optimal Joint Image Segmentation and Shape Matching Based on
  Wasserstein Modes</title><categories>cs.CV</categories><comments>31 pages, 16 figures. Accepted by Journal of Mathematical Imaging and
  Vision, published online. Printed publication pending</comments><msc-class>49Q10, 62H35</msc-class><doi>10.1007/s10851-014-0546-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A functional for joint variational object segmentation and shape matching is
developed. The formulation is based on optimal transport w.r.t. geometric
distance and local feature similarity. Geometric invariance and modelling of
object-typical statistical variations is achieved by introducing degrees of
freedom that describe transformations and deformations of the shape template.
The shape model is mathematically equivalent to contour-based approaches but
inference can be performed without conversion between the contour and region
representations, allowing combination with other convex segmentation approaches
and simplifying optimization. While the overall functional is non-convex,
non-convexity is confined to a low-dimensional variable. We propose a locally
optimal alternating optimization scheme and a globally optimal branch and bound
scheme, based on adaptive convex relaxation. Combining both methods allows to
eliminate the delicate initialization problem inherent to many contour based
approaches while remaining computationally practical. The properties of the
functional, its ability to adapt to a wide range of input data structures and
the different optimization schemes are illustrated and compared by numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3957</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3957</id><created>2014-07-15</created><updated>2014-09-17</updated><authors><author><keyname>Adamczyk</keyname><forenames>Marek</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author><author><keyname>Zhang</keyname><forenames>Qiang</forenames></author></authors><title>Efficiency of Truthful and Symmetric Mechanisms in One-sided Matching</title><categories>cs.GT</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the efficiency (in terms of social welfare) of truthful and
symmetric mechanisms in one-sided matching problems with {\em dichotomous
preferences} and {\em normalized von Neumann-Morgenstern preferences}. We are
particularly interested in the well-known {\em Random Serial Dictatorship}
mechanism. For dichotomous preferences, we first show that truthful, symmetric
and optimal mechanisms exist if intractable mechanisms are allowed. We then
provide a connection to online bipartite matching. Using this connection, it is
possible to design truthful, symmetric and tractable mechanisms that extract
0.69 of the maximum social welfare, which works under assumption that agents
are not adversarial. Without this assumption, we show that Random Serial
Dictatorship always returns an assignment in which the expected social welfare
is at least a third of the maximum social welfare. For normalized von
Neumann-Morgenstern preferences, we show that Random Serial Dictatorship always
returns an assignment in which the expected social welfare is at least
$\frac{1}{e}\frac{\nu(\opt)^2}{n}$, where $\nu(\opt)$ is the maximum social
welfare and $n$ is the number of both agents and items. On the hardness side,
we show that no truthful mechanism can achieve a social welfare better than
$\frac{\nu(\opt)^2}{n}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3962</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3962</id><created>2014-07-15</created><authors><author><keyname>Ducoat</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>Rank weight hierarchy of some classes of cyclic codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the rank weight hierarchy, thus in particular the rank metric, of
cyclic codes over the finite field $\mathbb F_{q^m}$, $q$ a prime power, $m
\geq 2$. We establish the rank weight hierarchy for $[n,n-1]$ cyclic codes and
characterize $[n,k]$ cyclic codes of rank metric 1 when (1) $k=1$, (2) $n$ and
$q$ are coprime, and (3) the characteristic $char(\mathbb F_q)$ divides $n$.
Finally, for $n$ and $q$ coprime, cyclic codes of minimal $r$-rank are
characterized, and a refinement of the Singleton bound for the rank weight is
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3969</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3969</id><created>2014-07-15</created><authors><author><keyname>Ricca</keyname><forenames>Giorgio</forenames></author><author><keyname>Beltrametti</keyname><forenames>Mauro C.</forenames></author><author><keyname>Massone</keyname><forenames>Anna Maria</forenames></author></authors><title>An iterative approach to Hough transform without re-voting</title><categories>cs.CV</categories><msc-class>68T45, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many bone shapes in the human skeleton are characterized by profiles that can
be associated to equations of algebraic curves. Fixing the parameters in the
curve equation, by means of a classical pattern recognition procedure like the
Hough transform technique, it is then possible to associate an equation to a
specific bone profile. However, most skeleton districts are more accurately
described by piecewise defined curves. This paper utilizes an iterative
approach of the Hough transform without re-voting, to provide an efficient
procedure for describing the profile of a bone in the human skeleton as a
collection of different but continuously attached curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3975</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3975</id><created>2014-07-15</created><authors><author><keyname>Naseri</keyname><forenames>Sima</forenames></author><author><keyname>Yazdani</keyname><forenames>Somaie</forenames></author><author><keyname>Razeghi</keyname><forenames>Behrooz</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>A Generalized Write Channel Model for Bit-Patterned Media Recording</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE International Symposium on
  Information Theory and its Applications (ISITA 2014), Melbourne, Australia,
  Oct. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a generalized write channel model for bit-patterned
media recording by considering all sources of errors causing some extra
disturbances during write process, in addition to data dependent write
synchronization errors. We investigate information-theoretic bounds for this
new model according to various input distributions and also compare it
numerically to the last proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3986</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3986</id><created>2014-07-05</created><authors><author><keyname>Raveendran</keyname><forenames>Haritha</forenames></author><author><keyname>Thomas</keyname><forenames>Deepa</forenames></author></authors><title>Image Fusion Using LEP Filtering and Bilinear Interpolation</title><categories>cs.CV</categories><comments>5 pages. 4 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V12(9),427-431 June 2014</journal-ref><doi>10.14445/22315381/IJETT-V12P282</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Image Fusion is the process in which core information from a set of component
images is merged to form a single image, which is more informative and complete
than the component input images in quality and appearance. This paper presents
a fast and effective image fusion method for creating high quality fused images
by merging component images. In the proposed method, the input image is broken
down to a two-scale image representation with a base layer having large scale
variations in intensity, and a detail layer containing small scale details.
Here fusion of the base and detail layers is implemented by means of a Local
Edge preserving filtering based technique. The proposed method is an efficient
image fusion technique in which the noise component is very low and quality of
the resultant image is high so that it can be used for applications like
medical image processing, requiring very accurate edge preserved images.
Performance is tested by calculating PSNR and SSIM of images. The benefit of
the proposed method is that it removes noise without altering the underlying
structures of the image. This paper also presents an image zooming technique
using bilinear interpolation in which a portion of the input image is cropped
and bilinear interpolation is applied. Experimental results showed that the
when PSNR value is calculated, the noise is found to be very low for the
resultant image portion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3987</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3987</id><created>2014-05-29</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Soni</keyname><forenames>Ankita</forenames></author><author><keyname>Chandel</keyname><forenames>Shringarica</forenames></author><author><keyname>Hemrajani</keyname><forenames>Manas</forenames></author></authors><title>Routing Attacks in Wireless Sensor Networks: A Survey</title><categories>cs.CR cs.NI</categories><comments>IJCSIT April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSN) is an emerging technology now-a-days and has a
wide range of applications such as battlefield surveillance, traffic
surveillance, forest fire detection, flood detection etc. But wireless sensor
networks are susceptible to a variety of potential attacks which obstructs the
normal operation of the network. The security of a wireless sensor network is
compromised because of the random deployment of sensor nodes in open
environment, memory limitations, power limitations and unattended nature. This
paper focuses on various attacks that manifest in the network and provides a
tabular representation of the attacks, their effects and severity. The paper
depicts a comparison of attacks basis packet loss and packet corruption. Also,
the paper discusses the known defence mechanisms and countermeasures against
the attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.3995</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.3995</id><created>2014-07-15</created><authors><author><keyname>Martins</keyname><forenames>Carlos A. R.</forenames></author><author><keyname>da Silva</keyname><forenames>Eduardo Brandani</forenames></author></authors><title>Space-Time Codes from Spectral Norm: A Fresh Look</title><categories>cs.IT math.IT</categories><msc-class>94B60, 15B52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research proposes a natural environment for the space-time codes and
in this context it is obtained a new design criterion for space-time codes in
multi-antenna communication channels. The objective of this criterion is to
minimize the pairwise error probability of the maximum likelihood decoder,
endowed with the matrix spectrum norm. The random matrix theory is used and an
approximation function for the probability density function is obtained for the
largest eigenvalue of a Wishart Matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4000</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4000</id><created>2014-07-15</created><updated>2014-11-21</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author><author><keyname>Islam</keyname><forenames>R.</forenames></author><author><keyname>Mahmood</keyname><forenames>A.</forenames></author></authors><title>Uncertainty And Evolutionary Optimization: A Novel Approach</title><categories>cs.NE</categories><comments>In Proceedings of the The 9th IEEE Conference on Industrial
  Electronics and Applications (ICIEA 2014), IEEE Press, pp. 988-983, 2014</comments><msc-class>68T99</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms (EA) have been widely accepted as efficient solvers
for complex real world optimization problems, including engineering
optimization. However, real world optimization problems often involve uncertain
environment including noisy and/or dynamic environments, which pose major
challenges to EA-based optimization. The presence of noise interferes with the
evaluation and the selection process of EA, and thus adversely affects its
performance. In addition, as presence of noise poses challenges to the
evaluation of the fitness function, it may need to be estimated instead of
being evaluated. Several existing approaches attempt to address this problem,
such as introduction of diversity (hyper mutation, random immigrants, special
operators) or incorporation of memory of the past (diploidy, case based
memory). However, these approaches fail to adequately address the problem. In
this paper we propose a Distributed Population Switching Evolutionary Algorithm
(DPSEA) method that addresses optimization of functions with noisy fitness
using a distributed population switching architecture, to simulate a
distributed self-adaptive memory of the solution space. Local regression is
used in the pseudo-populations to estimate the fitness. Successful applications
to benchmark test problems ascertain the proposed method's superior performance
in terms of both robustness and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4017</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4017</id><created>2014-07-09</created><updated>2014-12-19</updated><authors><author><keyname>Ariananda</keyname><forenames>Dyonisius Dony</forenames></author><author><keyname>Romero</keyname><forenames>Daniel</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Compressive Periodogram Reconstruction Using Uniform Binning</title><categories>cs.OH math.SP</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two problems that show great similarities are examined. The
first problem is the reconstruction of the angular-domain periodogram from
spatial-domain signals received at different time indices. The second one is
the reconstruction of the frequency-domain periodogram from time-domain signals
received at different wireless sensors. We split the entire angular or
frequency band into uniform bins. The bin size is set such that the received
spectra at two frequencies or angles, whose distance is equal to or larger than
the size of a bin, are uncorrelated. These problems in the two different
domains lead to a similar circulant structure in the so-called coset
correlation matrix. This circulant structure allows for a strong compression
and a simple least-squares reconstruction method. The latter is possible under
the full column rank condition of the system matrix, which can be achieved by
designing the spatial or temporal sampling patterns based on a circular sparse
ruler. We analyze the statistical performance of the compressively
reconstructed periodogram including bias and variance. We further consider the
case when the bins are so small that the received spectra at two frequencies or
angles, with a spacing between them larger than the size of the bin, can still
be correlated. In this case, the resulting coset correlation matrix is
generally not circulant and thus a special approach is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4023</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4023</id><created>2014-07-15</created><updated>2014-09-03</updated><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Yan</keyname><forenames>Junjie</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Aggregate channel features for multi-view face detection</title><categories>cs.CV</categories><comments>8 pages, 6 figures. Submitted to International Joint Conference on
  Biometrics, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face detection has drawn much attention in recent decades since the seminal
work by Viola and Jones. While many subsequences have improved the work with
more powerful learning algorithms, the feature representation used for face
detection still can't meet the demand for effectively and efficiently handling
faces with large appearance variance in the wild. To solve this bottleneck, we
borrow the concept of channel features to the face detection domain, which
extends the image channel to diverse types like gradient magnitude and oriented
gradient histograms and therefore encodes rich information in a simple form. We
adopt a novel variant called aggregate channel features, make a full
exploration of feature design, and discover a multi-scale version of features
with better performance. To deal with poses of faces in the wild, we propose a
multi-view detection approach featuring score re-ranking and detection
adjustment. Following the learning pipelines in Viola-Jones framework, the
multi-view face detector using aggregate channel features shows competitive
performance against state-of-the-art algorithms on AFW and FDDB testsets, while
runs at 42 FPS on VGA images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4027</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4027</id><created>2014-07-15</created><authors><author><keyname>Banda</keyname><forenames>Peter</forenames></author><author><keyname>Blount</keyname><forenames>Drew</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author></authors><title>COEL: A Web-based Chemistry Simulation Framework</title><categories>cs.CE physics.chem-ph q-bio.MN</categories><comments>23 pages, 12 figures, 1 table</comments><journal-ref>CoSMoS 2014: Proceedings of the 7th Workshop on Complex Systems
  Modelling and Simulation, 35-60, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The chemical reaction network (CRN) is a widely used formalism to describe
macroscopic behavior of chemical systems. Available tools for CRN modelling and
simulation require local access, installation, and often involve local file
storage, which is susceptible to loss, lacks searchable structure, and does not
support concurrency. Furthermore, simulations are often single-threaded, and
user interfaces are non-trivial to use. Therefore there are significant hurdles
to conducting efficient and collaborative chemical research. In this paper, we
introduce a new enterprise chemistry simulation framework, COEL, which
addresses these issues. COEL is the first web-based framework of its kind. A
visually pleasing and intuitive user interface, simulations that run on a large
computational grid, reliable database storage, and transactional services make
COEL ideal for collaborative research and education. COEL's most prominent
features include ODE-based simulations of chemical reaction networks and
multicompartment reaction networks, with rich options for user interactions
with those networks. COEL provides DNA-strand displacement transformations and
visualization (and is to our knowledge the first CRN framework to do so), GA
optimization of rate constants, expression validation, an application-wide
plotting engine, and SBML/Octave/Matlab export. We also present an overview of
the underlying software and technologies employed and describe the main
architectural decisions driving our development. COEL is available at
http://coel-sim.org for selected research teams only. We plan to provide a part
of COEL's functionality to the general public in the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4032</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4032</id><created>2014-07-15</created><authors><author><keyname>Milchior</keyname><forenames>Arthur</forenames></author></authors><title>A Note on Higher Order and Variable Order Logic over Finite Models</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that descriptive complexity's result extends in High Order Logic to
capture the expressivity of Turing Machine which have a finite number of
alternation and whose time or space is bounded by a finite tower of
exponential. Hence we have a logical characterisation of ELEMENTARY. We also
consider the expressivity of some fixed point operators and of monadic high
order logic.
  Finally, we show that Variable Order logic over finite structures contain the
Analytical Hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4056</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4056</id><created>2014-07-15</created><authors><author><keyname>Ramasamy</keyname><forenames>Dinesh</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Scalable and Efficient Geographic Routing in Mobile Ad Hoc Wireless
  Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and evaluate a scalable position-publish and an accompanying
routing protocol which is efficient despite operating with imperfect
information regarding the destination's location. The traffic generated by our
position-publish protocol fits within the transport capacity of large mobile ad
hoc networks (MANETs) with constant communication bandwidth allocated for
routing overhead, even as the network size increases. The routing protocol
guarantees, with high probability, routes whose lengths are within a constant
&quot;stretch&quot; factor of the shortest path from source to destination. The key idea
underlying the scalability of the publish protocol is for each potential
destination node to send location updates (with frequency decaying with
distance) to a subset of network nodes, structured as annular regions around it
(the natural approach of updating circular regions in distance-dependent
fashion does not scale). The routing protocol must therefore account for the
fact that the source and/or relay nodes may not have estimates of the
destination's location (or may have stale estimates). Spatial and temporal
scaling of protocol parameters are chosen so as to guarantee scalability, route
reliability and route stretch, and these analytical design prescriptions are
verified using simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4062</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4062</id><created>2014-07-15</created><authors><author><keyname>Amaku</keyname><forenames>Marcos</forenames></author><author><keyname>Cipullo</keyname><forenames>Rafael I.</forenames></author><author><keyname>Grisi-Filho</keyname><forenames>Jos&#xe9; H. H.</forenames></author><author><keyname>Marques</keyname><forenames>Fernando S.</forenames></author><author><keyname>Ossada</keyname><forenames>Raul</forenames></author></authors><title>The friendship paradox in scale-free networks</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 2 figures</comments><journal-ref>Applied Mathematical Sciences, Vol. 8, 2014, no. 37, 1837 - 1845</journal-ref><doi>10.12988/ams.2014.4288</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Our friends have more friends than we do. That is the basis of the friendship
paradox. In mathematical terms, the mean number of friends of friends is higher
than the mean number of friends. In the present study, we analyzed the
relationship between the mean degree of vertices (individuals), &lt;k&gt;, and the
mean number of friends of friends, &lt;k_FF&gt;, in scale-free networks with degrees
ranging from a minimum degree (k_min) to a maximum degree (k_max). We deduced
an expression for &lt;k_FF&gt; - &lt;k&gt; for scale-free networks following a power-law
distribution with a given scaling parameter (alpha). Based on this expression,
we can quantify how the degree distribution of a scale-free network affects the
mean number of friends of friends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4066</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4066</id><created>2014-07-15</created><updated>2015-03-16</updated><authors><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Minors and dimension</title><categories>math.CO cs.DM</categories><comments>Major revision of the exposition and the proof</comments><msc-class>06A07, 05C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been known for 30 years that posets with bounded height and with cover
graphs of bounded maximum degree have bounded dimension. Recently, Streib and
Trotter proved that dimension is bounded for posets with bounded height and
planar cover graphs, and Joret et al. proved that dimension is bounded for
posets with bounded height and with cover graphs of bounded tree-width. In this
paper, it is proved that posets of bounded height whose cover graphs exclude a
fixed topological minor have bounded dimension. This generalizes all the
aforementioned results and verifies a conjecture of Joret et al. The proof
relies on the Robertson-Seymour and Grohe-Marx graph structure theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4070</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4070</id><created>2014-07-15</created><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>Fast matrix completion without the condition number</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first algorithm for Matrix Completion whose running time and
sample complexity is polynomial in the rank of the unknown target matrix,
linear in the dimension of the matrix, and logarithmic in the condition number
of the matrix. To the best of our knowledge, all previous algorithms either
incurred a quadratic dependence on the condition number of the unknown matrix
or a quadratic dependence on the dimension of the matrix in the running time.
Our algorithm is based on a novel extension of Alternating Minimization which
we show has theoretical guarantees under standard assumptions even in the
presence of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4071</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4071</id><created>2014-07-15</created><authors><author><keyname>Mura</keyname><forenames>Cameron</forenames></author></authors><title>Ten Simple Rules for Creating Biomolecular Graphics</title><categories>q-bio.BM cs.HC</categories><comments>3 pages, 0 figures; see also the full-length article in PLoS
  Computational Biology (cited below)</comments><journal-ref>PLoS Computational Biology (2010), 6(8): e1000918</journal-ref><doi>10.1371/journal.pcbi.1000918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One need only compare the number of three-dimensional molecular illustrations
in the first (1990) and third (2004) editions of Voet &amp; Voet's &quot;Biochemistry&quot;
in order to appreciate this field's profound communicative value in modern
biological sciences -- ranging from medicine, physiology, and cell biology, to
pharmaceutical chemistry and drug design, to structural and computational
biology. The clich\'e about a picture being worth a thousand words is quite
poignant here: The information 'content' of an effectively-constructed piece of
molecular graphics can be immense. Because biological function arises from
structure, it is difficult to overemphasize the utility of visualization and
graphics in molding our current understanding of the molecular nature of
biological systems. Nevertheless, creating effective molecular graphics is not
easy -- neither conceptually, nor in terms of effort required. The present
collection of Rules is meant as a guide for those embarking upon their first
molecular illustrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4075</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4075</id><created>2014-07-14</created><authors><author><keyname>Luo</keyname><forenames>Lianjie</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author><author><keyname>Wu</keyname><forenames>Chengyong</forenames></author><author><keyname>Long</keyname><forenames>Shun</forenames></author><author><keyname>Fursin</keyname><forenames>Grigori</forenames></author></authors><title>Finding representative sets of optimizations for adaptive
  multiversioning applications</title><categories>cs.PL cs.LG</categories><comments>3rd Workshop on Statistical and Machine Learning Approaches Applied
  to Architectures and Compilation (SMART'09), co-located with HiPEAC'09
  conference, Paphos, Cyprus, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative compilation is a widely adopted technique to optimize programs for
different constraints such as performance, code size and power consumption in
rapidly evolving hardware and software environments. However, in case of
statically compiled programs, it is often restricted to optimizations for a
specific dataset and may not be applicable to applications that exhibit
different run-time behavior across program phases, multiple datasets or when
executed in heterogeneous, reconfigurable and virtual environments. Several
frameworks have been recently introduced to tackle these problems and enable
run-time optimization and adaptation for statically compiled programs based on
static function multiversioning and monitoring of online program behavior. In
this article, we present a novel technique to select a minimal set of
representative optimization variants (function versions) for such frameworks
while avoiding performance loss across available datasets and code-size
explosion. We developed a novel mapping mechanism using popular decision tree
or rule induction based machine learning techniques to rapidly select best code
versions at run-time based on dataset features and minimize selection overhead.
These techniques enable creation of self-tuning static binaries or libraries
adaptable to changing behavior and environments at run-time using staged
compilation that do not require complex recompilation frameworks while
effectively outperforming traditional single-version non-adaptable code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4088</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4088</id><created>2014-07-15</created><authors><author><keyname>L&#xe8;bre</keyname><forenames>Marie-Ange</forenames><affiliation>CITI</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CITI</affiliation></author><author><keyname>M&#xe9;nard</keyname><forenames>Eric</forenames></author><author><keyname>Dillschneider</keyname><forenames>Julien</forenames></author><author><keyname>Denis</keyname><forenames>Richard</forenames></author></authors><title>VANET Applications: Hot Use Cases</title><categories>cs.CY cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current challenges of car manufacturers are to make roads safe, to achieve
free flowing traffic with few congestions, and to reduce pollution by an
effective fuel use. To reach these goals, many improvements are performed
in-car, but more and more approaches rely on connected cars with communication
capabilities between cars, with an infrastructure, or with IoT devices.
Monitoring and coordinating vehicles allow then to compute intelligent ways of
transportation. Connected cars have introduced a new way of thinking cars - not
only as a mean for a driver to go from A to B, but as smart cars - a user
extension like the smartphone today. In this report, we introduce concepts and
specific vocabulary in order to classify current innovations or ideas on the
emerging topic of smart car. We present a graphical categorization showing this
evolution in function of the societal evolution. Different perspectives are
adopted: a vehicle-centric view, a vehicle-network view, and a user-centric
view; described by simple and complex use-cases and illustrated by a list of
emerging and current projects from the academic and industrial worlds. We
identified an empty space in innovation between the user and his car:
paradoxically even if they are both in interaction, they are separated through
different application uses. Future challenge is to interlace social concerns of
the user within an intelligent and efficient driving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4094</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4094</id><created>2014-07-15</created><updated>2015-04-29</updated><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Dickerson</keyname><forenames>John P.</forenames></author><author><keyname>Haghtalab</keyname><forenames>Nika</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author></authors><title>Ignorance is Almost Bliss: Near-Optimal Stochastic Matching With Few
  Queries</title><categories>cs.DS</categories><doi>10.1145/2764468.2764479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic matching problem deals with finding a maximum matching in a
graph whose edges are unknown but can be accessed via queries. This is a
special case of stochastic $k$-set packing, where the problem is to find a
maximum packing of sets, each of which exists with some probability. In this
paper, we provide edge and set query algorithms for these two problems,
respectively, that provably achieve some fraction of the omniscient optimal
solution.
  Our main theoretical result for the stochastic matching (i.e., $2$-set
packing) problem is the design of an \emph{adaptive} algorithm that queries
only a constant number of edges per vertex and achieves a $(1-\epsilon)$
fraction of the omniscient optimal solution, for an arbitrarily small
$\epsilon&gt;0$. Moreover, this adaptive algorithm performs the queries in only a
constant number of rounds. We complement this result with a \emph{non-adaptive}
(i.e., one round of queries) algorithm that achieves a $(0.5 - \epsilon)$
fraction of the omniscient optimum. We also extend both our results to
stochastic $k$-set packing by designing an adaptive algorithm that achieves a
$(\frac{2}{k} - \epsilon)$ fraction of the omniscient optimal solution, again
with only $O(1)$ queries per element. This guarantee is close to the best known
polynomial-time approximation ratio of $\frac{3}{k+1} -\epsilon$ for the
\emph{deterministic} $k$-set packing problem [Furer and Yu, 2013]
  We empirically explore the application of (adaptations of) these algorithms
to the kidney exchange problem, where patients with end-stage renal failure
swap willing but incompatible donors. We show on both generated data and on
real data from the first 169 match runs of the UNOS nationwide kidney exchange
that even a very small number of non-adaptive edge queries per vertex results
in large gains in expected successful matches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4095</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4095</id><created>2014-07-15</created><authors><author><keyname>Fawzi</keyname><forenames>Hamza</forenames></author><author><keyname>Gouveia</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author><author><keyname>Robinson</keyname><forenames>Richard Z.</forenames></author><author><keyname>Thomas</keyname><forenames>Rekha R.</forenames></author></authors><title>Positive semidefinite rank</title><categories>math.OC cs.DM math.CO</categories><comments>35 pages</comments><journal-ref>Mathematical Programming 153(1) 133-177, 2015</journal-ref><doi>10.1007/s10107-015-0922-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let M be a p-by-q matrix with nonnegative entries. The positive semidefinite
rank (psd rank) of M is the smallest integer k for which there exist positive
semidefinite matrices $A_i, B_j$ of size $k \times k$ such that $M_{ij} =
\text{trace}(A_i B_j)$. The psd rank has many appealing geometric
interpretations, including semidefinite representations of polyhedra and
information-theoretic applications. In this paper we develop and survey the
main mathematical properties of psd rank, including its geometry, relationships
with other rank notions, and computational and algorithmic aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4106</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4106</id><created>2014-07-15</created><updated>2014-11-15</updated><authors><author><keyname>Hutton</keyname><forenames>Eric W. H.</forenames></author><author><keyname>Piper</keyname><forenames>Mark D.</forenames></author><author><keyname>Peckham</keyname><forenames>Scott D.</forenames></author><author><keyname>Overeem</keyname><forenames>Irina</forenames></author><author><keyname>Kettner</keyname><forenames>Albert J.</forenames></author><author><keyname>Syvitski</keyname><forenames>James P. M.</forenames></author></authors><title>Building Sustainable Software - The CSDMS Approach</title><categories>cs.SE</categories><comments>WSSSPE2</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  CSDMS, The Community Surface Dynamics Modeling System, is an NSF funded
project whose focus is to aid a diverse community of earth and ocean system
model users and developers to use and create robust software quickly. To this
end, CSDMS develops, integrates, archives and disseminates earth-system models
and tools to an international (67 country) community with the goal of building
the set of tools necessary to model the earth system. Modelers use CSDMS for
access to hundreds of open source surface-dynamics models and tools, as well as
model metadata. Such a model repository increases model transparency and helps
eliminate duplication by presenting the current state of modeling efforts. To
increase software sustainability, composability and interoperability, CSDMS
promotes standards that define common modeling interfaces, semantic mediation
between models, and model metadata. Through online resources and workshops,
CSDMS promotes software engineering best practices, which are unfamiliar to
many developers within our modeling community. For example, version control,
unit testing, continuous integration, test-driven development, and well-written
clean code are all topics of the educational mission of CSDMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4118</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4118</id><created>2014-07-15</created><updated>2015-11-20</updated><authors><author><keyname>Buisson</keyname><forenames>L. du</forenames></author><author><keyname>Sivanandam</keyname><forenames>N.</forenames></author><author><keyname>Bassett</keyname><forenames>B. A.</forenames></author><author><keyname>Smith</keyname><forenames>M.</forenames></author></authors><title>Machine Learning Classification of SDSS Transient Survey Images</title><categories>astro-ph.IM astro-ph.CO cs.CV</categories><comments>14 pages, 8 figures. In this version extremely minor adjustments to
  the paper were made - e.g. Figure 5 is now easier to view in greyscale</comments><journal-ref>L. du Buisson; N. Sivanandam; Bruce A. Bassett; M. Smith; Monthly
  Notices of the Royal Astronomical Society, 2015, 454 (2): 2026-2038</journal-ref><doi>10.1093/mnras/stv2041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that multiple machine learning algorithms can match human performance
in classifying transient imaging data from the Sloan Digital Sky Survey (SDSS)
supernova survey into real objects and artefacts. This is a first step in any
transient science pipeline and is currently still done by humans, but future
surveys such as the Large Synoptic Survey Telescope (LSST) will necessitate
fully machine-enabled solutions. Using features trained from eigenimage
analysis (principal component analysis, PCA) of single-epoch g, r and
i-difference images, we can reach a completeness (recall) of 96 per cent, while
only incorrectly classifying at most 18 per cent of artefacts as real objects,
corresponding to a precision (purity) of 84 per cent. In general, random
forests performed best, followed by the k-nearest neighbour and the SkyNet
artificial neural net algorithms, compared to other methods such as na\&quot;ive
Bayes and kernel support vector machine. Our results show that PCA-based
machine learning can match human success levels and can naturally be extended
by including multiple epochs of data, transient colours and host galaxy
information which should allow for significant further improvements, especially
at low signal-to-noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4139</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4139</id><created>2014-07-15</created><updated>2015-04-24</updated><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author></authors><title>Subjectivity, Bayesianism, and Causality</title><categories>cs.AI stat.ME stat.ML</categories><comments>21 pages, 21 figures. Submitted to Special Issue of Pattern
  Recognition Letters on &quot;Philosophical aspects of pattern recognition&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian probability theory is one of the most successful frameworks to model
reasoning under uncertainty. Its defining property is the interpretation of
probabilities as degrees of belief in propositions about the state of the world
relative to an inquiring subject. This essay examines the notion of
subjectivity by drawing parallels between Lacanian theory and Bayesian
probability theory, and concludes that the latter must be enriched with causal
interventions to model agency. The central contribution of this work is an
abstract model of the subject that accommodates causal interventions in a
measure-theoretic formalisation. This formalisation is obtained through a
game-theoretic Ansatz based on modelling the inside and outside of the subject
as an extensive-form game with imperfect information between two players.
Finally, I illustrate the expressiveness of this model with an example of
causal induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4149</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4149</id><created>2014-07-15</created><authors><author><keyname>Visala</keyname><forenames>Kari</forenames></author></authors><title>Hybrid Communication Architecture HCA</title><categories>cs.DC cs.NI</categories><comments>120 pages</comments><msc-class>68M10</msc-class><journal-ref>Master's thesis, Tampere University of Technology, 2006</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The beginning of the 21st century has seen many projects on distributed hash
tables, both research and commercial. One of their aims has been to replace the
first generation of file sharing software with scalable peer-to-peer
architectures. On other fronts, the same techniques are applied, for example,
to content delivery networks, streaming networks, cooperative caches,
distributed file systems, and grid computing architectures for scientific use.
This trend has emerged because with cooperative peers it is possible to
asymptotically enhance the use of resouces in sharing of data compared to the
basic client-server architecture.
  The need for distribution of data is wide and one could argue that it is as
fundamental a building block as the message passing of the Internet. As an
answer to this need a new scalable architecture is introduced: Hybrid
Communication Architecture (HCA), which provides both data sharing and message
passing as communication primitives for applications. HCA can be regarded as an
abstraction layer for communication which is further encapsulated by a
higher-level middleware. HCA is aimed at general use, and it is not designed
for any particular application. One key idea is to combine data sharing with
streaming since together they enable many applications not easily implementable
with only one of these features. For example, a game application could share
the game world state between clients and modify it by using streaming. The
other distinctive feature of the system is the use of knowledge of the physical
network topology in the optimization of the communication. With a feasible
business model, fault-tolerance, and security features, HCA is aimed eventually
for real-life adoption.
  This thesis presents the specification of the C++ client interface of HCA and
the architecture and protocol of the distributed nodes forming the
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4162</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4162</id><created>2014-07-15</created><authors><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author><author><keyname>Schmidt</keyname><forenames>Nico</forenames></author><author><keyname>Pfeifer</keyname><forenames>Rolf</forenames></author></authors><title>Measuring information transfer in a soft robotic arm</title><categories>cs.IT cs.RO math.IT nlin.AO</categories><comments>29 pages, 8 figures</comments><doi>10.1088/1748-3190/10/3/035007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft robots can exhibit diverse behaviors with simple types of actuation by
partially outsourcing control to the morphological and material properties of
their soft bodies, which is made possible by the tight coupling between
control, body, and environment. In this paper, we present a method that will
quantitatively characterize these diverse spatiotemporal dynamics of a soft
body based on the information-theoretic approach. In particular, soft bodies
have the ability to propagate the effect of actuation through the entire body,
with a certain time delay, due to their elasticity. Our goal is to capture this
delayed interaction in a quantitative manner based on a measure called
momentary information transfer. We extend this measure to soft robotic
applications and demonstrate its power using a physical soft robotic platform
inspired by the octopus. Our approach is illustrated in two ways. First, we
statistically characterize the delayed actuation propagation through the body
as a strength of information transfer. Second, we capture this information
propagation directly as local information dynamics. As a result, we show that
our approach can successfully characterize the spatiotemporal dynamics of the
soft robotic platform, explicitly visualizing how information transfers through
the entire body with delays. Further extension scenarios of our approach are
discussed for soft robotic applications in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4167</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4167</id><created>2014-07-15</created><authors><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author><author><keyname>Lynch</keyname><forenames>Nancy</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author><author><keyname>Musial</keyname><forenames>Peter</forenames></author></authors><title>A Coded Shared Atomic Memory Algorithm for Message Passing Architectures</title><categories>cs.DC cs.IT math.IT</categories><comments>Part of the results to appear in IEEE Network Computing and
  Applications (NCA), Aug 2014. This report supersedes MIT CSAIL technical
  report MIT-CSAIL-TR-2013-016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the communication and storage costs of emulating atomic
(linearizable) multi-writer multi-reader shared memory in distributed
message-passing systems. The paper contains three main contributions: (1) We
present a atomic shared-memory emulation algorithm that we call Coded Atomic
Storage (CAS). This algorithm uses erasure coding methods. In a storage system
with $N$ servers that is resilient to $f$ server failures, we show that the
communication cost of CAS is $\frac{N}{N-2f}$. The storage cost of CAS is
unbounded. (2) We present a modification of the CAS algorithm known as CAS with
Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer
$\delta$ and has a bounded storage cost. We show that in every execution where
the number of write operations that are concurrent with a read operation is no
bigger than $\delta$, the CASGC algorithm with parameter $\delta$ satisfies
atomicity and liveness. We explicitly characterize the storage cost of CASGC,
and show that it has the same communication cost as CAS. (3) We describe an
algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS)
algorithm that achieves a smaller communication cost than CAS and CASGC. In
particular, CCOAS incurs read and write communication costs of $\frac{N}{N-f}$
measured in terms of number of object values. We also discuss drawbacks of
CCOAS as compared with CAS and CASGC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4171</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4171</id><created>2014-07-15</created><authors><author><keyname>Hammouda</keyname><forenames>Marwan</forenames></author><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Peissig</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Effective Capacity in Cognitive Radio Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>Submitted and Accepted to IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate effective capacity by modeling a cognitive
radio broadcast channel with one secondary transmitter (ST) and two secondary
receivers (SRs) under quality-of-service constraints and interference power
limitations. We initially describe three different cooperative channel sensing
strategies with different hard-decision combining algorithms at the ST, namely
OR, Majority, and AND rules. Since the channel sensing occurs with possible
errors, we consider a combined interference power constraint by which the
transmission power of the secondary users (SUs) is bounded when the channel is
sensed as both busy and idle. Furthermore, regarding the channel sensing
decision and its correctness, there exist possibly four different transmission
scenarios. We provide the instantaneous ergodic capacities of the channel
between the ST and each SR in all of these scenarios. Granting that
transmission outage arises when the instantaneous transmission rate is greater
than the instantaneous ergodic capacity, we establish two different
transmission rate policies for the SUs when the channel is sensed as idle. One
of these policies features a greedy approach disregarding a possible
transmission outage, and the other favors a precautious manner to prevent this
outage. Subsequently, we determine the effective capacity region of this
channel model, and we attain the power allocation policies that maximize this
region. Finally, we present the numerical results. We first show the
superiority of Majority rule when the channel sensing results are good. Then,
we illustrate that a greedy transmission rate approach is more beneficial for
the SUs under strict interference power constraints, whereas sending with lower
rates will be more advantageous under loose interference constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4177</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4177</id><created>2014-07-15</created><authors><author><keyname>Hassan</keyname><forenames>Naveed Ul</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Saeed</keyname><forenames>Shayan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Power Control for Sum Rate Maximization on Interference Channels Under
  Sum Power Constraint</title><categories>cs.IT math.IT</categories><comments>17 pages, 8 figures, IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of power control for sum rate
maximization on multiple interfering links (TX-RX pairs)under sum power
constraint. We consider a single frequency network, where all pairs are
operating in same frequency band,thereby creating interference for each other.
We study the power allocation problem for sum rate maximization with and
without QoS requirements on individual links. When the objective is only sum
rate maximization without QoS guarantees, we develop an analytic solution to
decide optimal power allocation for two TX-RX pair problem. We also develop a
low complexity iterative algorithm for three TX-RX pair problem. For a generic
N&gt;3 TX-RX pair problem, we develop two low-complexity sub-optimal power
allocation algorithms. The first algorithm is based on the idea of making
clusters of two or three TX-RX pairs and then leverage the power allocation
results obtained for two and three TX-RX pair problems. The second algorithm is
developed by using a high SINR approximation and this algorithm can also be
implemented in a distributed manner by individual TXs. We then consider the
same problem but with additional QoS guarantees for individual links. We again
develop an analytic solution for two TX-RX pair problem, and a distributed
algorithm for N&gt;2 TX-RX pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4179</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4179</id><created>2014-07-15</created><authors><author><keyname>Sedenka</keyname><forenames>Jaroslav</forenames></author><author><keyname>Balagani</keyname><forenames>Kiran</forenames></author><author><keyname>Phoha</keyname><forenames>Vir</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author></authors><title>Privacy-Preserving Population-Enhanced Biometric Key Generation from
  Free-Text Keystroke Dynamics</title><categories>cs.CR</categories><journal-ref>Jaroslav Sedenka, Kiran Balagani, Vir Phoha and Paolo Gasti.
  Privacy-Preserving Population-Enhanced Biometric Key Generation from
  Free-Text Keystroke Dynamics. BTAS 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric key generation techniques are used to reliably generate
cryptographic material from biometric signals. Existing constructions require
users to perform a particular activity (e.g., type or say a password, or
provide a handwritten signature), and are therefore not suitable for generating
keys continuously. In this paper we present a new technique for biometric key
generation from free-text keystroke dynamics. This is the first technique
suitable for continuous key generation. Our approach is based on a scaled
parity code for key generation (and subsequent key reconstruction), and can be
augmented with the use of population data to improve security and reduce key
reconstruction error. In particular, we rely on linear discriminant analysis
(LDA) to obtain a better representation of discriminable biometric signals.
  To update the LDA matrix without disclosing user's biometric information, we
design a provably secure privacy-preserving protocol (PP-LDA) based on
homomorphic encryption. Our biometric key generation with PP-LDA was evaluated
on a dataset of 486 users. We report equal error rate around 5% when using LDA,
and below 7% without LDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4186</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4186</id><created>2014-07-15</created><authors><author><keyname>Connor</keyname><forenames>Andrew M.</forenames></author><author><keyname>Buchan</keyname><forenames>Jim</forenames></author><author><keyname>Petrova</keyname><forenames>Krassie</forenames></author></authors><title>Bridging the Research-Practice Gap in Requirements Engineering through
  Effective Teaching and Peer Learning</title><categories>cs.SE cs.CY</categories><comments>Proceedings of the 6th International Conference on Information
  Technology: New Generations (ITNG 2009)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the concept of the research practice gap as it is
perceived in the field of software requirements engineering. An analysis of
this gap has shown that two key causes for the research-practice gap are lack
of effective communication and the relatively light coverage of requirements
engineering material in University programmes. We discuss the design and
delivery of a Masters course in Software Requirements Engineering (SRE) that is
designed to overcome some of the issues that have caused the research-practice
gap. By encouraging students to share their experiences in a peer learning
environment, we aim to improve shared understanding between students (many of
whom are current industry practitioners) and researchers (including academic
staff members) to improve the potential for effective collaborations, whilst
simultaneously developing the requirements engineering skill sets of the
enrolled students. Feedback from students in the course is discussed and
directions for the future development of the curriculum and learning strategies
are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4194</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4194</id><created>2014-07-15</created><authors><author><keyname>Kang</keyname><forenames>Chaogui</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Wu</keyname><forenames>Lun</forenames></author></authors><title>Delineating Intra-Urban Spatial Connectivity Patterns by
  Travel-Activities: A Case Study of Beijing, China</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Travel activities have been widely applied to quantify spatial interactions
between places, regions and nations. In this paper, we model the spatial
connectivities between 652 Traffic Analysis Zones (TAZs) in Beijing by a taxi
OD dataset. First, we unveil the gravitational structure of intra-urban spatial
connectivities of Beijing. On overall, the inter-TAZ interactions are well
governed by the Gravity Model $G_{ij} = {\lambda}p_{i}p_{j}/d_{ij}$, where
$p_{i}$, $p_{j}$ are degrees of TAZ $i$, $j$ and $d_{ij}$ the distance between
them, with a goodness-of-fit around 0.8. Second, the network based analysis
well reveals the polycentric form of Beijing. Last, we detect the semantics of
inter-TAZ connectivities based on their spatiotemporal patterns. We further
find that inter-TAZ connections deviating from the Gravity Model can be well
explained by link semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4206</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4206</id><created>2014-07-16</created><authors><author><keyname>Xu</keyname><forenames>Yichao</forenames></author><author><keyname>Maeno</keyname><forenames>Kazuki</forenames></author><author><keyname>Nagahara</keyname><forenames>Hajime</forenames></author><author><keyname>Taniguchi</keyname><forenames>Rin-ichiro</forenames></author></authors><title>Mobile Camera Array Calibration for Light Field Acquisition</title><categories>cs.CV</categories><comments>11th International Conference on Quality Control by Artificial Vision
  (QCAV2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The light field camera is useful for computer graphics and vision
applications. Calibration is an essential step for these applications. After
calibration, we can rectify the captured image by using the calibrated camera
parameters. However, the large camera array calibration method, which assumes
that all cameras are on the same plane, ignores the orientation and intrinsic
parameters. The multi-camera calibration technique usually assumes that the
working volume and viewpoints are fixed. In this paper, we describe a
calibration algorithm suitable for a mobile camera array based light field
acquisition system. The algorithm performs in Zhang's style by moving a
checkerboard, and computes the initial parameters in closed form. Global
optimization is then applied to refine all the parameters simultaneously. Our
implementation is rather flexible in that users can assign the number of
viewpoints and refinement of intrinsic parameters is optional. Experiments on
both simulated data and real data acquired by a commercial product show that
our method yields good results. Digital refocusing application shows the
calibrated light field can well focus to the target object we desired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4225</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4225</id><created>2014-07-16</created><updated>2014-09-01</updated><authors><author><keyname>B&#xe9;rard</keyname><forenames>B&#xe9;atrice</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Sznajder</keyname><forenames>Nathalie</forenames></author></authors><title>Probabilistic Opacity for Markov Decision Processes</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opacity is a generic security property, that has been defined on (non
probabilistic) transition systems and later on Markov chains with labels. For a
secret predicate, given as a subset of runs, and a function describing the view
of an external observer, the value of interest for opacity is a measure of the
set of runs disclosing the secret. We extend this definition to the richer
framework of Markov decision processes, where non deterministic choice is
combined with probabilistic transitions, and we study related decidability
problems with partial or complete observation hypotheses for the schedulers. We
prove that all questions are decidable with complete observation and
$\omega$-regular secrets. With partial observation, we prove that all
quantitative questions are undecidable but the question whether a system is
almost surely non opaque becomes decidable for a restricted class of
$\omega$-regular secrets, as well as for all $\omega$-regular secrets under
finite-memory schedulers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4234</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4234</id><created>2014-07-16</created><authors><author><keyname>Weydert</keyname><forenames>Emil</forenames></author></authors><title>A Plausibility Semantics for Abstract Argumentation Frameworks</title><categories>cs.AI</categories><comments>Proceedings of the 15th International Workshop on Non-Monotonic
  Reasoning (NMR 2014). This is an improved and extended version of the
  author's ECSQARU 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and investigate a simple ranking-measure-based extension semantics
for abstract argumentation frameworks based on their generic instantiation by
default knowledge bases and the ranking construction semantics for default
reasoning. In this context, we consider the path from structured to logical to
shallow semantic instantiations. The resulting well-justified JZ-extension
semantics diverges from more traditional approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4235</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4235</id><created>2014-07-16</created><authors><author><keyname>Hatanaka</keyname><forenames>Tatsuhiko</forenames></author><author><keyname>Ito</keyname><forenames>Takehiro</forenames></author><author><keyname>Zhou</keyname><forenames>Xiao</forenames></author></authors><title>The List Coloring Reconfiguration Problem for Bounded Pathwidth Graphs</title><categories>cs.DS cs.DM</categories><doi>10.1587/transfun.E98.A.1168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of transforming one list (vertex) coloring of a graph
into another list coloring by changing only one vertex color assignment at a
time, while at all times maintaining a list coloring, given a list of allowed
colors for each vertex. This problem is known to be PSPACE-complete for
bipartite planar graphs. In this paper, we first show that the problem remains
PSPACE-complete even for bipartite series-parallel graphs, which form a proper
subclass of bipartite planar graphs. We note that our reduction indeed shows
the PSPACE-completeness for graphs with pathwidth two, and it can be extended
for threshold graphs. In contrast, we give a polynomial-time algorithm to solve
the problem for graphs with pathwidth one. Thus, this paper gives precise
analyses of the problem with respect to pathwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4242</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4242</id><created>2014-07-16</created><authors><author><keyname>Caires</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames></author></authors><title>A Typeful Characterization of Multiparty Structured Conversations Based
  on Binary Sessions</title><categories>cs.LO</categories><acm-class>D.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relating the specification of the global communication behavior of a
distributed system and the specifications of the local communication behavior
of each of its nodes/peers (e.g., to check if the former is realizable by the
latter under some safety and/or liveness conditions) is a challenging problem
addressed in many relevant scenarios. In the context of networked software
services, a widespread programming language-based approach relies on global
specifications defined by session types or behavioral contracts. Static type
checking can then be used to ensure that components follow the prescribed
interaction protocols. In the case of session types, developments have been
mostly framed within quite different type theories for either binary
(two-party) or multiparty (n-party) protocols. Unfortunately, the precise
relationship between analysis techniques for multiparty and binary protocols is
yet to be understood.
  In this work, we bridge this previously open gap in a principled way: we show
that the analysis of multiparty protocols can also be developed within a much
simpler type theory for binary protocols, ensuring protocol fidelity and
deadlock-freedom. We present characterization theorems which provide new
insights on the relation between two existing, yet very differently motivated,
session type systems---one based on linear logic, the other based on automata
theory---and suggest useful type-based verification techniques for multiparty
systems relying on reductions to the binary case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4245</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4245</id><created>2014-07-16</created><authors><author><keyname>Reshetova</keyname><forenames>Elena</forenames></author><author><keyname>Karhunen</keyname><forenames>Janne</forenames></author><author><keyname>Nyman</keyname><forenames>Thomas</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Security of OS-level virtualization technologies: Technical report</title><categories>cs.CR cs.DC cs.OS</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for flexible, low-overhead virtualization is evident on many fronts
ranging from high-density cloud servers to mobile devices. During the past
decade OS-level virtualization has emerged as a new, efficient approach for
virtualization, with implementations in multiple different Unix-based systems.
Despite its popularity, there has been no systematic study of OS-level
virtualization from the point of view of security. In this report, we conduct a
comparative study of several OS-level virtualization systems, discuss their
security and identify some gaps in current solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4259</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4259</id><created>2014-07-16</created><updated>2015-10-01</updated><authors><author><keyname>Bienvenu</keyname><forenames>Laurent</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>K-trivial, K-low and MLR-low sequences: a tutorial</title><categories>math.LO cs.IT math.IT</categories><comments>25 pages</comments><msc-class>03D30</msc-class><acm-class>F.4.1; H.1.1</acm-class><journal-ref>Fields of Logic and Computation, Lecture Notes in Computer
  Science, v.9300 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A remarkable achievement in algorithmic randomness and algorithmic
information theory was the discovery of the notions of K-trivial, K-low and
Martin-Lof-random-low sets: three different definitions turns out to be
equivalent for very non-trivial reasons. This paper, based on the course taught
by one of the authors (L.B.) in Poncelet laboratory (CNRS, Moscow) in 2014,
provides an exposition of the proof of this equivalence and some related
results. We assume that the reader is familiar with basic notions of
algorithmic information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4266</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4266</id><created>2014-07-16</created><authors><author><keyname>Espinha</keyname><forenames>Tiago</forenames></author><author><keyname>Zaidman</keyname><forenames>Andy</forenames></author><author><keyname>Gross</keyname><forenames>Hans-Gerhard</forenames></author></authors><title>Web API Fragility: How Robust is Your Web API Client</title><categories>cs.SE</categories><comments>Technical report</comments><report-no>TUD-SERG-2014-009</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web APIs provide a systematic and extensible approach for
application-to-application interaction. A large number of mobile applications
makes use of web APIs to integrate services into apps. Each Web API's evolution
pace is determined by their respective developer and mobile application
developers are forced to accompany the API providers in their software
evolution tasks. In this paper we investigate whether mobile application
developers understand and how they deal with the added distress of web APIs
evolving. In particular, we studied how robust 48 high profile mobile
applications are when dealing with mutated web API responses. Additionally, we
interviewed three mobile application developers to better understand their
choices and trade-offs regarding web API integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4277</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4277</id><created>2014-07-16</created><updated>2014-08-22</updated><authors><author><keyname>Shimoji</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Abe</keyname><forenames>Masato S.</forenames></author><author><keyname>Tsuji</keyname><forenames>Kazuki</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Global network structure of dominance hierarchy of ant workers</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><comments>5 figures, 2 tables, 4 supplementary figures, 2 supplementary tables</comments><journal-ref>Journal of the Royal Society Interface, 11, 20140599 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dominance hierarchy among animals is widespread in various species and
believed to serve to regulate resource allocation within an animal group.
Unlike small groups, however, detection and quantification of linear hierarchy
in large groups of animals are a difficult task. Here, we analyse
aggression-based dominance hierarchies formed by worker ants in Diacamma sp. as
large directed networks. We show that the observed dominance networks are
perfect or approximate directed acyclic graphs, which are consistent with
perfect linear hierarchy. The observed networks are also sparse and random but
significantly different from networks generated through thinning of the perfect
linear tournament (i.e., all individuals are linearly ranked and dominance
relationship exists between every pair of individuals). These results pertain
to global structure of the networks, which contrasts with the previous studies
inspecting frequencies of different types of triads. In addition, the
distribution of the out-degree (i.e., number of workers that the focal worker
attacks), not in-degree (i.e., number of workers that attack the focal worker),
of each observed network is right-skewed. Those having excessively large
out-degrees are located near the top, but not the top, of the hierarchy. We
also discuss evolutionary implications of the discovered properties of
dominance networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4286</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4286</id><created>2014-07-16</created><updated>2015-09-21</updated><authors><author><keyname>Ganardi</keyname><forenames>Moses</forenames></author><author><keyname>Hucke</keyname><forenames>Danny</forenames></author><author><keyname>Jez</keyname><forenames>Artur</forenames></author><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Noeth</keyname><forenames>Eric</forenames></author></authors><title>Constructing small tree grammars and small circuits for formulas</title><categories>cs.DS cs.CC cs.FL</categories><comments>A short version of this paper appeared in the Proceedings of FSTTCS
  2014</comments><msc-class>68P30, 68Q42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that every tree of size $n$ over a fixed set of $\sigma$
different ranked symbols can be decomposed (in linear time as well as in
logspace) into $O\big(\frac{n}{\log_\sigma n}\big) = O\big(\frac{n \log
\sigma}{\log n}\big)$ many hierarchically defined pieces. Formally, such a
hierarchical decomposition has the form of a straight-line linear context-free
tree grammar of size $O\big(\frac{n}{\log_\sigma n}\big)$, which can be used as
a compressed representation of the input tree. This generalizes an analogous
result for strings. Previous grammar-based tree compressors were not analyzed
for the worst-case size of the computed grammar, except for the top dag of
Bille et al., for which only the weaker upper bound of
$O\big(\frac{n}{\log_\sigma^{0.19} n}\big)$ (which was very recently improved
to $O\big(\frac{n \cdot \log \log_\sigma n}{\log_\sigma n}\big)$ by
H\&quot;ubschle-Schneider and Raman) for unranked and unlabelled trees has been
derived. The main result is used to show that every arithmetical formula of
size $n$, in which only $m \leq n$ different variables occur, can be
transformed (in linear time as well as in logspace) into an arithmetical
circuit of size $O\big(\frac{n \cdot \log m}{\log n}\big)$ and depth $O(\log
n)$. This refines a classical result of Brent from 1974, according to which an
arithmetical formula of size $n$ can be transformed into a logarithmic depth
circuit of size $O(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4293</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4293</id><created>2014-07-16</created><updated>2015-01-09</updated><authors><author><keyname>Hollanders</keyname><forenames>Romain</forenames></author><author><keyname>Gerencs&#xe9;r</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>A complexity analysis of Policy Iteration through combinatorial matrices
  arising from Unique Sink Orientations</title><categories>cs.DM cs.CC math.CO</categories><comments>21 pages, 6 colored figures, submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unique Sink Orientations (USOs) are an appealing abstraction of several major
optimization problems of applied mathematics such as for instance Linear
Programming (LP), Markov Decision Processes (MDPs) or 2-player Turn Based
Stochastic Games (2TBSGs). A polynomial time algorithm to find the sink of a
USO would translate into a strongly polynomial time algorithm to solve the
aforementioned problems---a major quest for all three cases. In addition, we
may translate MDPs and 2TBSGs into the problem of finding the sink of an
acyclic USO of a cube, which can be done using the well-known Policy Iteration
algorithm (PI). The study of its complexity is the object of this work. Despite
its exponential worst case complexity, the principle of PI is a powerful source
of inspiration for other methods.
  As our first contribution, we disprove Hansen and Zwick's conjecture claiming
that the number of steps of PI should follow the Fibonacci sequence in the
worst case. Our analysis relies on a new combinatorial formulation of the
problem---the so-called Order-Regularity formulation (OR). Then, for our second
contribution, we (exponentially) improve the $\Omega(1.4142^n)$ lower bound on
the number of steps of PI from Schurr and Szab\'o in the case of the OR
formulation and obtain an $\Omega(1.4269^n)$ bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4308</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4308</id><created>2014-07-16</created><authors><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>Some upper and lower bounds on PSD-rank</title><categories>cs.CC math.CO quant-ph</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positive semidefinite rank (PSD-rank) is a relatively new quantity with
applications to combinatorial optimization and communication complexity. We
first study several basic properties of PSD-rank, and then develop new
techniques for showing lower bounds on the PSD-rank. All of these bounds are
based on viewing a positive semidefinite factorization of a matrix $M$ as a
quantum communication protocol. These lower bounds depend on the entries of the
matrix and not only on its support (the zero/nonzero pattern), overcoming a
limitation of some previous techniques. We compare these new lower bounds with
known bounds, and give examples where the new ones are better. As an
application we determine the PSD-rank of (approximations of) some common
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4318</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4318</id><created>2014-07-16</created><authors><author><keyname>Sayir</keyname><forenames>Jossy</forenames></author></authors><title>The Role Model Estimator Revisited</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><journal-ref>Published at the IEEE International Symposium on Information
  Theory (ISIT 2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We re-visit the role model strategy introduced in an earlier paper, which
allows one to train an estimator for degraded observations by imitating a
reference estimator that has access to superior observations. We show that,
while it is true and surprising that this strategy yields the optimal Bayesian
estimator for the degraded observations, it in fact reduces to a much simpler
form in the non-parametric case, which corresponds to a type of Monte Carlo
integration. We then show an example for which only parametric estimation can
be implemented and discuss further applications for discrete parametric
estimation where the role model strategy does have its uses, although it loses
claim to optimality in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4328</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4328</id><created>2014-07-16</created><authors><author><keyname>Atkins</keyname><forenames>Caroline</forenames></author><author><keyname>Sayir</keyname><forenames>Jossy</forenames></author></authors><title>Density Evolution for SUDOKU Codes on the Erasure Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted for publication at the 8th International Symposium
  on Turbo Codes and Iterative Processing, August 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes based on SUDOKU puzzles are discussed, and belief propagation decoding
introduced for the erasure channel. Despite the non-linearity of the code
constraints, it is argued that density evolution can be used to analyse code
performance due to the invariance of the code under alphabet permutation. The
belief propagation decoder for erasure channels operates by exchanging messages
containing sets of possible values. Accordingly, density evolution tracks the
probability mass functions of the set cardinalities. The equations governing
the mapping of those probability mass functions are derived and calculated for
variable and constraint nodes, and decoding thresholds are computed for long
SUDOKU codes with random interleavers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4330</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4330</id><created>2014-07-16</created><authors><author><keyname>Varghese</keyname><forenames>Chris</forenames></author><author><keyname>Durrett</keyname><forenames>Rick</forenames></author></authors><title>Spatial networks evolving to reduce length</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by results of Henry, Pralat and Zhang (PNAS 108.21 (2011):
8605-8610), we propose a general scheme for evolving spatial networks in order
to reduce their total edge lengths. We study the properties of the equilbria of
two networks from this class, which interpolate between three well studied
objects: the Erd\H{o}s-R\'{e}nyi random graph, the random geometric graph, and
the minimum spanning tree. The first of our two evolutions can be used as a
model for a social network where individuals have fixed opinions about a number
of issues and adjust their ties to be connected to people with similar views.
The second evolution which preserves the connectivity of the network has
potential applications in the design of transportation networks and other
distribution systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4334</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4334</id><created>2014-07-16</created><authors><author><keyname>Jarynowski</keyname><forenames>Andrzej</forenames></author><author><keyname>Lopez-Nunez</keyname><forenames>Fco. Alejandro</forenames></author><author><keyname>Fan</keyname><forenames>Han</forenames></author></authors><title>How network temporal dynamics shape a mutualistic system with invasive
  species?</title><categories>q-bio.PE cs.MA math.DS nlin.AO</categories><comments>Icelab Networks and Ecology Workshop report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ecological networks allow us to study the structure and function of
ecosystems and gain insights on species resilience/stability. The study of this
ecological networks is usually a snapshop focused in a limited specific range
of space and time, prevent us to perceive the real dynamics of ecological
processes. By definition, an alien species has some ecological strategies and
traits that permit it to compete better than the native species (e.g. absence
of predators, different bloom period, high grow rate, etc.). Plant-pollinator
networks provide valuable services to whole ecosystems and the introduction of
an alien species may have different effects on the native network (competitive
facilitation, native species extinction, etc.). While scientists acknowledge
the significance of network connectivity in driving ecosystem services, the
inclusion of temporary networks in ecological models is still in its infancy.
We propose to use existing data on seasonality to develop a simulation platform
that show inference between temporality of networks and invasions traits. Our
focus is only to pick up some simple model to show, that theoretically temporal
aspect play a role (different extinction patterns) to encourage ecologist to
get involved in temporal networks. Moreover, the derived simulations could be
further extended and adjust to other ecological questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4335</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4335</id><created>2014-07-16</created><authors><author><keyname>Talukdar</keyname><forenames>Rehman</forenames></author><author><keyname>Saikia</keyname><forenames>Mridul</forenames></author></authors><title>Evolution and Innovation in 5G Cellular Communication System and Beyond:
  A Study</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the last few years there has been a phenomenal growth in the wireless
industry. Widespread wireless technologies, increasing variety of user-friendly
and multimedia- enabled terminals and wider availability of open source tools
for content generation has lead encouraged user-centric networks resulting in a
need for efficient network design. The objective of this paper is comprehensive
study related to 5G technology of mobile communication. Existing research work
in mobile communication is related to 5G technology. The major contribution of
this study is the key provisions of 5G (Fifth Generation) technology of mobile
communication, which is seen as consumer oriented. In 5G technology, the mobile
consumer has given utmost priority compared to others. In this context, the
existing and highly demanded technologies for 5G technologies has beed studied
extensively. Open challenges are highlighted for researcher for further study
of the emerging 5G systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4342</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4342</id><created>2014-07-16</created><updated>2014-07-20</updated><authors><author><keyname>Sayir</keyname><forenames>Jossy</forenames></author></authors><title>Non-binary LDPC decoding using truncated messages in the Walsh-Hadamard
  domain</title><categories>cs.IT math.IT</categories><comments>5 pages, accepted for publication at the International Symposium on
  Information Theory and its Applications (ISITA 2014), October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Extended Min-Sum (EMS) algorithm for non-binary low-density parity-check
(LDPC) defined over an alphabet of size $q$ operates on truncated messages of
length $q'$ to achieve a complexity of the order $q'^2$. In contrast,
Walsh-Hadamard (WH) transform based iterative decoders achieve a complexity of
the order $q\log q$, which is much larger for $q'&lt;&lt;q$. In this paper, we
demonstrate that considerable savings can be achieved by letting WH based
decoders operate on truncated messages as well. We concentrate on the direct WH
transform and compute the number of operations required if only $q'$ of the $q$
inputs are non-zero. Our paper does not cover the inverse WH transform and
hence further research is needed to construct WH based decoders that can
compete with the EMS algorithm on complexity terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4346</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4346</id><created>2014-07-16</created><authors><author><keyname>Palix</keyname><forenames>Nicolas</forenames><affiliation>Grenoble 1 UJF, LIG</affiliation></author><author><keyname>Thomas</keyname><forenames>Ga&#xeb;l</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Saha</keyname><forenames>Suman</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Calv&#xe8;s</keyname><forenames>Christophe</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Muller</keyname><forenames>Gilles</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Lawall</keyname><forenames>Julia L.</forenames><affiliation>LIP6, INRIA Paris-Rocquencourt</affiliation></author></authors><title>Faults in Linux 2.6</title><categories>cs.SE cs.OS</categories><proxy>ccsd</proxy><journal-ref>ACM Transactions on Computer Systems 32, 2 (2014) 1--40</journal-ref><doi>10.1145/2619090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In August 2011, Linux entered its third decade. Ten years before, Chou et al.
published a study of faults found by applying a static analyzer to Linux
versions 1.0 through 2.4.1. A major result of their work was that the drivers
directory contained up to 7 times more of certain kinds of faults than other
directories. This result inspired numerous efforts on improving the reliability
of driver code. Today, Linux is used in a wider range of environments, provides
a wider range of services, and has adopted a new development and release model.
What has been the impact of these changes on code quality? To answer this
question, we have transported Chou et al.'s experiments to all versions of
Linux 2.6; released between 2003 and 2011. We find that Linux has more than
doubled in size during this period, but the number of faults per line of code
has been decreasing. Moreover, the fault rate of drivers is now below that of
other directories, such as arch. These results can guide further development
and research efforts for the decade to come. To allow updating these results as
Linux evolves, we define our experimental protocol and make our checkers
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4355</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4355</id><created>2014-07-16</created><authors><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Shou</keyname><forenames>Biying</forenames></author></authors><title>Pricing for local and global WiFi markets</title><categories>cs.GT cs.NI</categories><comments>18 pages including appendices</comments><journal-ref>IEEE Transactions on Mobile Computing, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes two pricing schemes commonly used in WiFi markets: the
flat-rate and the usage-based pricing. The flat-rate pricing encourages the
maximum usage, while the usage-based pricing can flexibly attract more users
especially those with low valuations in mobile Internet access. First, we use
theoretical analysis to compare the two schemes and show that for a single
provider in a market, as long as the WiFi capacity is abundant, the flat-rate
pricing leads to more revenue. Second, we study how a global provider (e.g.,
Skype) collaborates with this monopolist in each local market to provide a
global WiFi service. We formulate {the interactions between the global and
local providers as a dynamic game. In Stage I, the global provider bargains
with the local provider in each market to determine the global WiFi service
price and revenue sharing agreement. In Stage II, local users and travelers
choose local or global WiFi services. We analytically show that the global
provider prefers to use the usage-based pricing to avoid a severe competition
with the local provider. At the equilibrium, the global provider always shares
the majority of his revenue with the local provider to incentivize the
cooperation. Finally, we analytically study how the interaction changes if the
local market has more than one local provider. In this case, the global
provider can integrate the coverages of multiple local providers and provide a
better service. Compared to the local monopoly case, local market competition
enables the global provider to share less revenue with each of the local
providers. However, we numerically show that the global provider's revenue
could decrease, as he shares his revenue with more providers and can only
charge a lower price.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4360</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4360</id><created>2014-07-16</created><authors><author><keyname>Cintra</keyname><forenames>Rosangela S.</forenames></author><author><keyname>Velho</keyname><forenames>Haroldo F. de Campos</forenames></author></authors><title>Data Assimilation by Artificial Neural Networks for an Atmospheric
  General Circulation Model: Conventional Observation</title><categories>cs.AI physics.ao-ph</categories><comments>17 pages, 16 figures, monthly weather review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach for employing artificial neural networks (NN)
to emulate an ensemble Kalman filter (EnKF) as a method of data assimilation.
The assimilation methods are tested in the Simplified Parameterizations
PrimitivE-Equation Dynamics (SPEEDY) model, an atmospheric general circulation
model (AGCM), using synthetic observational data simulating localization of
balloon soundings. For the data assimilation scheme, the supervised NN, the
multilayer perceptrons (MLP-NN), is applied. The MLP-NN are able to emulate the
analysis from the local ensemble transform Kalman filter (LETKF). After the
training process, the method using the MLP-NN is seen as a function of data
assimilation. The NN were trained with data from first three months of 1982,
1983, and 1984. A hind-casting experiment for the 1985 data assimilation cycle
using MLP-NN were performed with synthetic observations for January 1985. The
numerical results demonstrate the effectiveness of the NN technique for
atmospheric data assimilation. The results of the NN analyses are very close to
the results from the LETKF analyses, the differences of the monthly average of
absolute temperature analyses is of order 0.02. The simulations show that the
major advantage of using the MLP-NN is better computational performance, since
the analyses have similar quality. The CPU-time cycle assimilation with MLP-NN
is 90 times faster than cycle assimilation with LETKF for the numerical
experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4364</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4364</id><created>2014-07-16</created><authors><author><keyname>Fuad</keyname><forenames>Muhammad Marwan Muhammad</forenames></author></authors><title>One-Step or Two-Step Optimization and the Overfitting Phenomenon: A Case
  Study on Time Series Classification</title><categories>cs.AI</categories><journal-ref>Proceedings of the 6th International Conference on Agents and
  Artificial Intelligence -6 - 8 March, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the last few decades, optimization has been developing at a fast rate.
Bio-inspired optimization algorithms are metaheuristics inspired by nature.
These algorithms have been applied to solve different problems in engineering,
economics, and other domains. Bio-inspired algorithms have also been applied in
different branches of information technology such as networking and software
engineering. Time series data mining is a field of information technology that
has its share of these applications too. In previous works we showed how
bio-inspired algorithms such as the genetic algorithms and differential
evolution can be used to find the locations of the breakpoints used in the
symbolic aggregate approximation of time series representation, and in another
work we showed how we can utilize the particle swarm optimization, one of the
famous bio-inspired algorithms, to set weights to the different segments in the
symbolic aggregate approximation representation. In this paper we present, in
two different approaches, a new meta optimization process that produces optimal
locations of the breakpoints in addition to optimal weights of the segments.
The experiments of time series classification task that we conducted show an
interesting example of how the overfitting phenomenon, a frequently encountered
problem in data mining which happens when the model overfits the training set,
can interfere in the optimization process and hide the superior performance of
an optimization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4378</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4378</id><created>2014-07-14</created><authors><author><keyname>Cieslik</keyname><forenames>Marcin</forenames></author><author><keyname>Mura</keyname><forenames>Cameron</forenames></author></authors><title>PaPy: Parallel and Distributed Data-processing Pipelines in Python</title><categories>cs.PL q-bio.QM</categories><comments>7 pages, 5 figures, 2 tables, some use-cases; more at
  http://muralab.org/PaPy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PaPy, which stands for parallel pipelines in Python, is a highly flexible
framework that enables the construction of robust, scalable workflows for
either generating or processing voluminous datasets. A workflow is created from
user-written Python functions (nodes) connected by 'pipes' (edges) into a
directed acyclic graph. These functions are arbitrarily definable, and can make
use of any Python modules or external binaries. Given a user-defined topology
and collection of input data, functions are composed into nested higher-order
maps, which are transparently and robustly evaluated in parallel on a single
computer or on remote hosts. Local and remote computational resources can be
flexibly pooled and assigned to functional nodes, thereby allowing facile
load-balancing and pipeline optimization to maximize computational throughput.
Input items are processed by nodes in parallel, and traverse the graph in
batches of adjustable size -- a trade-off between lazy-evaluation, parallelism,
and memory consumption. The processing of a single item can be parallelized in
a scatter/gather scheme. The simplicity and flexibility of distributed
workflows using PaPy bridges the gap between desktop -&gt; grid, enabling this new
computing paradigm to be leveraged in the processing of large scientific
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4394</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4394</id><created>2014-07-16</created><updated>2014-07-18</updated><authors><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames></author><author><keyname>St&#xfc;ckrath</keyname><forenames>Jan</forenames></author></authors><title>Parameterized Verification of Graph Transformation Systems with Whole
  Neighbourhood Operations</title><categories>cs.LO</categories><comments>Extended version of a submittion accepted at RP'14 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of graph transformation systems in which rewrite
rules can be guarded by universally quantified conditions on the neighbourhood
of nodes. These conditions are defined via special graph patterns which may be
transformed by the rule as well. For the new class for graph rewrite rules, we
provide a symbolic procedure working on minimal representations of upward
closed sets of configurations. We prove correctness and effectiveness of the
procedure by a categorical presentation of rewrite rules as well as the
involved order, and using results for well-structured transition systems. We
apply the resulting procedure to the analysis of the Distributed Dining
Philosophers protocol on an arbitrary network structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4395</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4395</id><created>2014-07-16</created><authors><author><keyname>Jin</keyname><forenames>Ming</forenames></author><author><keyname>Jia</keyname><forenames>Ruoxi</forenames></author><author><keyname>Kang</keyname><forenames>Zhoayi</forenames></author><author><keyname>Konstantakopoulos</keyname><forenames>Ioannis C.</forenames></author><author><keyname>Spanos</keyname><forenames>Costas</forenames></author></authors><title>PresenceSense: Zero-training Algorithm for Individual Presence Detection
  based on Power Monitoring</title><categories>cs.HC</categories><comments>BuildSys 2014</comments><acm-class>I.5.2; I.5.4; H.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-intrusive presence detection of individuals in commercial buildings is
much easier to implement than intrusive methods such as passive infrared,
acoustic sensors, and camera. Individual power consumption, while providing
useful feedback and motivation for energy saving, can be used as a valuable
source for presence detection. We conduct pilot experiments in an office
setting to collect individual presence data by ultrasonic sensors, acceleration
sensors, and WiFi access points, in addition to the individual power monitoring
data. PresenceSense (PS), a semi-supervised learning algorithm based on power
measurement that trains itself with only unlabeled data, is proposed, analyzed
and evaluated in the study. Without any labeling efforts, which are usually
tedious and time consuming, PresenceSense outperforms popular models whose
parameters are optimized over a large training set. The results are interpreted
and potential applications of PresenceSense on other data sources are
discussed. The significance of this study attaches to space security, occupancy
behavior modeling, and energy saving of plug loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4409</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4409</id><created>2014-07-16</created><authors><author><keyname>Jia</keyname><forenames>Ruoxi</forenames></author><author><keyname>Jin</keyname><forenames>Ming</forenames></author><author><keyname>Spanos</keyname><forenames>Costas J.</forenames></author></authors><title>SoundLoc: Acoustic Method for Indoor Localization without Infrastructure</title><categories>cs.HC</categories><comments>BuildSys 2014</comments><acm-class>H.5.5; H.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying locations of occupants is beneficial to energy management in
buildings. A key observation in indoor environment is that distinct functional
areas are typically controlled by separate HVAC and lighting systems and room
level localization is sufficient to provide a powerful tool for energy usage
reduction by occupancy-based actuation of the building facilities. Based upon
this observation, this paper focuses on identifying the room where a person or
a mobile device is physically present. Existing room localization methods,
however, require special infrastructure to annotate rooms.
  SoundLoc is a room-level localization system that exploits the intrinsic
acoustic properties of individual rooms and obviates the needs for
infrastructures. As we show in the study, rooms' acoustic properties can be
characterized by Room Impulse Response (RIR). Nevertheless, obtaining precise
RIRs is a time-consuming and expensive process. The main contributions of our
work are the following. First, a cost-effective RIR measurement system is
implemented and the Noise Adaptive Extraction of Reverberation (NAER) algorithm
is developed to estimate room acoustic parameters in noisy conditions. Second,
a comprehensive physical and statistical analysis of features extracted from
RIRs is performed. Also, SoundLoc is evaluated using the dataset consisting of
ten (10) different rooms. The overall accuracy of 97.8% achieved demonstrates
the potential to be integrated into automatic mapping of building space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4416</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4416</id><created>2014-07-16</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>In Defense of MinHash Over SimHash</title><categories>stat.CO cs.DS cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing
(LSH) algorithms for large-scale data processing applications. Deciding which
LSH to use for a particular problem at hand is an important question, which has
no clear answer in the existing literature. In this study, we provide a
theoretical answer (validated by experiments) that MinHash virtually always
outperforms SimHash when the data are binary, as common in practice such as
search.
  The collision probability of MinHash is a function of resemblance similarity
($\mathcal{R}$), while the collision probability of SimHash is a function of
cosine similarity ($\mathcal{S}$). To provide a common basis for comparison, we
evaluate retrieval results in terms of $\mathcal{S}$ for both MinHash and
SimHash. This evaluation is valid as we can prove that MinHash is a valid LSH
with respect to $\mathcal{S}$, by using a general inequality $\mathcal{S}^2\leq
\mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}$. Our worst case analysis can
show that MinHash significantly outperforms SimHash in high similarity region.
  Interestingly, our intensive experiments reveal that MinHash is also
substantially better than SimHash even in datasets where most of the data
points are not too similar to each other. This is partly because, in practical
data, often $\mathcal{R}\geq \frac{\mathcal{S}}{z-\mathcal{S}}$ holds where $z$
is only slightly larger than 2 (e.g., $z\leq 2.1$). Our restricted worst case
analysis by assuming $\frac{\mathcal{S}}{z-\mathcal{S}}\leq \mathcal{R}\leq
\frac{\mathcal{S}}{2-\mathcal{S}}$ shows that MinHash indeed significantly
outperforms SimHash even in low similarity region.
  We believe the results in this paper will provide valuable guidelines for
search in practice, especially when the data are sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4420</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4420</id><created>2014-07-16</created><authors><author><keyname>Zhu</keyname><forenames>Fei</forenames></author><author><keyname>Honeine</keyname><forenames>Paul</forenames></author><author><keyname>Kallas</keyname><forenames>Maya</forenames></author></authors><title>Kernel nonnegative matrix factorization without the curse of the
  pre-image</title><categories>cs.CV cs.IT cs.LG cs.NE math.IT stat.ML</categories><comments>14 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nonnegative matrix factorization (NMF) is widely used in signal and image
processing, including bio-informatics, blind source separation and
hyperspectral image analysis in remote sensing. A great challenge arises when
dealing with a nonlinear formulation of the NMF. Within the framework of kernel
machines, the models suggested in the literature do not allow the
representation of the factorization matrices, which is a fallout of the curse
of the pre-image. In this paper, we propose a novel kernel-based model for the
NMF that does not suffer from the pre-image problem, by investigating the
estimation of the factorization matrices directly in the input space. For
different kernel functions, we describe two schemes for iterative algorithms:
an additive update rule based on a gradient descent scheme and a multiplicative
update rule in the same spirit as in the Lee and Seung algorithm. Within the
proposed framework, we develop several extensions to incorporate constraints,
including sparseness, smoothness, and spatial regularization with a
total-variation-like penalty. The effectiveness of the proposed method is
demonstrated with the problem of unmixing hyperspectral images, using
well-known real images and results with state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4422</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4422</id><created>2014-07-16</created><authors><author><keyname>Tomczak</keyname><forenames>Jakub M.</forenames></author><author><keyname>Gonczarek</keyname><forenames>Adam</forenames></author></authors><title>Subspace Restricted Boltzmann Machine</title><categories>cs.LG</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-order
Boltzmann machine where multiplicative interactions are between one visible and
two hidden units. There are two kinds of hidden units, namely, gate units and
subspace units. The subspace units reflect variations of a pattern in data and
the gate unit is responsible for activating the subspace units. Additionally,
the gate unit can be seen as a pooling feature. We evaluate the behavior of
subspaceRBM through experiments with MNIST digit recognition task, measuring
reconstruction error and classification error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4423</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4423</id><created>2014-07-16</created><authors><author><keyname>Allen</keyname><forenames>Sarah R.</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author></authors><title>Conditioning and covariance on caterpillars</title><categories>cs.IT cs.CC cs.DS math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X_1, \dots, X_n$ be joint $\{ \pm 1\}$-valued random variables. It is
known that conditioning on a random subset of $O(1/\epsilon^2)$ of them reduces
their average pairwise covariance to below $\epsilon$ (in expectation). We
conjecture that $O(1/\epsilon^2)$ can be improved to $O(1/\epsilon)$. The
motivation for the problem and our conjectured improvement comes from the
theory of global correlation rounding for convex relaxation hierarchies. We
suggest attempting the conjecture in the case that $X_1, \dots, X_n$ are the
leaves of an information flow tree. We prove the conjecture in the case that
the information flow tree is a caterpillar graph (similar to a two-state hidden
Markov model).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4425</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4425</id><created>2014-07-16</created><updated>2014-09-10</updated><authors><author><keyname>Ad&#xe1;mek</keyname><forenames>Ji&#x159;&#xed;</forenames><affiliation>Institut f&#xfc;r Theoretische Informatik, Technische Universit&#xe4;t Braunschweig, Germany</affiliation></author><author><keyname>Haddadi</keyname><forenames>Mahdie</forenames><affiliation>Department of Mathematics, Statistics and Computer Science, Semnan University, Iran</affiliation></author><author><keyname>Milius</keyname><forenames>Stefan</forenames><affiliation>Institut f&#xfc;r Theoretische Informatik, Technische Universitat Braunschweig, Germany</affiliation></author></authors><title>Corecursive Algebras, Corecursive Monads and Bloom Monads</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  11, 2014) lmcs:707</journal-ref><doi>10.2168/LMCS-10(3:19)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algebra is called corecursive if from every coalgebra a unique
coalgebra-to-algebra homomorphism exists into it. We prove that free
corecursive algebras are obtained as coproducts of the terminal coalgebra
(considered as an algebra) and free algebras. The monad of free corecursive
algebras is proved to be the free corecursive monad, where the concept of
corecursive monad is a generalization of Elgot's iterative monads, analogous to
corecursive algebras generalizing completely iterative algebras. We also
characterize the Eilenberg-Moore algebras for the free corecursive monad and
call them Bloom algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4430</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4430</id><created>2014-07-16</created><authors><author><keyname>Kang</keyname><forenames>Zhaoyi</forenames></author><author><keyname>Spanos</keyname><forenames>Costas J.</forenames></author></authors><title>Sequential Logistic Principal Component Analysis (SLPCA): Dimensional
  Reduction in Streaming Multivariate Binary-State System</title><categories>stat.ML cs.LG stat.AP</categories><comments>6 pages, 4 figures, conference submission</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sequential or online dimensional reduction is of interests due to the
explosion of streaming data based applications and the requirement of adaptive
statistical modeling, in many emerging fields, such as the modeling of energy
end-use profile. Principal Component Analysis (PCA), is the classical way of
dimensional reduction. However, traditional Singular Value Decomposition (SVD)
based PCA fails to model data which largely deviates from Gaussian
distribution. The Bregman Divergence was recently introduced to achieve a
generalized PCA framework. If the random variable under dimensional reduction
follows Bernoulli distribution, which occurs in many emerging fields, the
generalized PCA is called Logistic PCA (LPCA). In this paper, we extend the
batch LPCA to a sequential version (i.e. SLPCA), based on the sequential convex
optimization theory. The convergence property of this algorithm is discussed
compared to the batch version of LPCA (i.e. BLPCA), as well as its performance
in reducing the dimension for multivariate binary-state systems. Its
application in building energy end-use profile modeling is also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4443</identifier>
 <datestamp>2014-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4443</id><created>2014-07-16</created><authors><author><keyname>Kaufmann</keyname><forenames>Emilie</forenames><affiliation>LTCI</affiliation></author><author><keyname>Capp&#xe9;</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>IMT</affiliation></author></authors><title>On the Complexity of Best Arm Identification in Multi-Armed Bandit
  Models</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1405.3224</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic multi-armed bandit model is a simple abstraction that has
proven useful in many different contexts in statistics and machine learning.
Whereas the achievable limit in terms of regret minimization is now well known,
our aim is to contribute to a better understanding of the performance in terms
of identifying the m best arms. We introduce generic notions of complexity for
the two dominant frameworks considered in the literature: fixed-budget and
fixed-confidence settings. In the fixed-confidence setting, we provide the
first known distribution-dependent lower bound on the complexity that involves
information-theoretic quantities and holds when m is larger than 1 under
general assumptions. In the specific case of two armed-bandits, we derive
refined lower bounds in both the fixed-confidence and fixed-budget settings,
along with matching algorithms for Gaussian and Bernoulli bandit models. These
results show in particular that the complexity of the fixed-budget setting may
be smaller than the complexity of the fixed-confidence setting, contradicting
the familiar behavior observed when testing fully specified alternatives. In
addition, we also provide improved sequential stopping rules that have
guaranteed error probabilities and shorter average running times. The proofs
rely on two technical results that are of independent interest : a deviation
lemma for self-normalized sums (Lemma 19) and a novel change of measure
inequality for bandit models (Lemma 1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4446</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4446</id><created>2014-07-16</created><updated>2015-09-22</updated><authors><author><keyname>Han</keyname><forenames>Weidong</forenames></author><author><keyname>Rajan</keyname><forenames>Purnima</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author><author><keyname>Jedynak</keyname><forenames>Bruno M.</forenames></author></authors><title>Probabilistic Group Testing under Sum Observations: A Parallelizable
  2-Approximation for Entropy Loss</title><categories>cs.IT cs.LG math.IT math.OC math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of group testing with sum observations and noiseless
answers, in which we aim to locate multiple objects by querying the number of
objects in each of a sequence of chosen sets. We study a probabilistic setting
with entropy loss, in which we assume a joint Bayesian prior density on the
locations of the objects and seek to choose the sets queried to minimize the
expected entropy of the Bayesian posterior distribution after a fixed number of
questions. We present a new non-adaptive policy, called the dyadic policy, show
it is optimal among non-adaptive policies, and is within a factor of two of
optimal among adaptive policies. This policy is quick to compute, its
nonadaptive nature makes it easy to parallelize, and our bounds show it
performs well even when compared with adaptive policies. We also study an
adaptive greedy policy, which maximizes the one-step expected reduction in
entropy, and show that it performs at least as well as the dyadic policy,
offering greater query efficiency but reduced parallelism. Numerical
experiments demonstrate that both procedures outperform a divide-and-conquer
benchmark policy from the literature, called sequential bifurcation, and show
how these procedures may be applied in a stylized computer vision problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4450</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4450</id><created>2014-07-16</created><authors><author><keyname>Obiefuna</keyname><forenames>C. A</forenames></author><author><keyname>Offorma</keyname><forenames>G. C.</forenames></author></authors><title>Pre service Teachers Perception of using Mobile Devices in Teaching
  Climate Change in Primary Schools</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The realities of climate change are gradually dawning on everyone including
children. The need for a disaster reduction education requires the use of
mobile technologies to identify some of the impact of climate change within an
environment and create awareness on the dangers associated with climate change.
Since the pre service teachers will teach the primary school pupils, it is apt
that the use of mobile technologies should constitute part of their preparation
while in training. This paper examined pre service teachers perception of using
mobile technologies in teaching climate change in the primary school. One
hundred and fifty (150) pre service teachers in two Colleges of Education in
the erosion disaster zones of Anambra and Imo States in the south eastern state
of Nigeria were used for the study. Three research questions guided the study.
The study utilized a survey approach to collect and analyze the data. The
results from the study show that the pre-service teachers were confident that
the use of mobile devices will create significant climate change awareness.
However, the pre service teachers saw the need for using mobile devices fin
their preparation. Suggestions were made towards ensuring the integration of
mobile technology literacy in the pre service teacher education curriculum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4451</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4451</id><created>2014-07-16</created><authors><author><keyname>Shen</keyname><forenames>Zhesi</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Fan</keyname><forenames>Ying</forenames></author><author><keyname>Di</keyname><forenames>Zengru</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>Reconstructing propagation networks with natural diversity and
  identifying hidden sources</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages and 5 figures. For Supplementary information, please see
  http://www.nature.com/ncomms/2014/140711/ncomms5323/full/ncomms5323.html#t</comments><journal-ref>Zhesi Shen, Wen-Xu Wang, Ying-Fan, Zengru Di, and Ying-Cheng Lai,
  &quot;Reconstructing propagation networks with natural diversity and identifying
  hidden sources&quot;, Nature Communications 5, 4323 (2014)</journal-ref><doi>10.1038/ncomms5323</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our ability to uncover complex network structure and dynamics from data is
fundamental to understanding and controlling collective dynamics in complex
systems. Despite recent progress in this area, reconstructing networks with
stochastic dynamical processes from limited time series remains to be an
outstanding problem. Here we develop a framework based on compressed sensing to
reconstruct complex networks on which stochastic spreading dynamics take place.
We apply the methodology to a large number of model and real networks, finding
that a full reconstruction of inhomogeneous interactions can be achieved from
small amounts of polarized (binary) data, a virtue of compressed sensing.
Further, we demonstrate that a hidden source that triggers the spreading
process but is externally inaccessible can be ascertained and located with high
confidence in the absence of direct routes of propagation from it. Our approach
thus establishes a paradigm for tracing and controlling epidemic invasion and
information diffusion in complex networked systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4454</identifier>
 <datestamp>2014-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4454</id><created>2014-07-16</created><authors><author><keyname>Pighin</keyname><forenames>Daniele</forenames></author><author><keyname>Alfonseca</keyname><forenames>Enrique</forenames></author><author><keyname>Keppmann</keyname><forenames>Felix Leif</forenames></author><author><keyname>Trampus</keyname><forenames>Mitja</forenames></author></authors><title>Evaluation of the DiversiNews diversified news service</title><categories>cs.CY cs.HC</categories><comments>Technical report</comments><msc-class>67t99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we present the outcome of an extensive evaluation of the
DiversiNews platform [8, 10] for diversified browsing of news, developed in the
scope of the RENDER project. The evaluation was carried out along two main
directions: a component evaluation, in which we assessed the maturity of the
components underlying DiversiNews, and a user experience (UX) evaluation
involving users of online news services. The results of the evaluation confirm
the high value of DiversiNews as a novel paradigm for diversity-aware news
browsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4477</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4477</id><created>2014-07-16</created><authors><author><keyname>D'Amico</keyname><forenames>Antonio A.</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Convex separable problems with linear and box constraints in signal
  processing and communications</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Signal Process</comments><doi>10.1109/TSP.2014.2360143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we focus on separable convex optimization problems with box
constraints and a set of triangular linear constraints. The solution is given
in closed-form as a function of some Lagrange multipliers that can be computed
through an iterative procedure in a finite number of steps. Graphical
interpretations are given casting valuable insights into the proposed algorithm
and allowing to retain some of the intuition spelled out by the water-filling
policy. It turns out that it is not only general enough to compute the solution
to different instances of the problem at hand but also remarkably simple in the
way it operates. We also show how some power allocation problems in signal
processing and communications can be solved with the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4489</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4489</id><created>2014-07-16</created><authors><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author></authors><title>Coded Caching for Delay-Sensitive Content</title><categories>cs.IT cs.NI math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coded caching is a recently proposed technique that achieves significant
performance gains for cache networks compared to uncoded caching schemes.
However, this substantial coding gain is attained at the cost of large delivery
delay, which is not tolerable in delay-sensitive applications such as video
streaming. In this paper, we identify and investigate the tradeoff between the
performance gain of coded caching and the delivery delay. We propose a
computationally efficient caching algorithm that provides the gains of coding
and respects delay constraints. The proposed algorithm achieves the optimum
performance for large delay, but still offers major gains for small delay.
These gains are demonstrated in a practical setting with a video-streaming
prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4490</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4490</id><created>2014-07-16</created><authors><author><keyname>Ghosh</keyname><forenames>Shalini</forenames></author><author><keyname>Lincoln</keyname><forenames>Patrick</forenames></author><author><keyname>Petersen</keyname><forenames>Christian</forenames></author><author><keyname>Valdes</keyname><forenames>Alfonso</forenames></author></authors><title>Virus Detection in Multiplexed Nanowire Arrays using Hidden Semi-Markov
  models</title><categories>cs.AI q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of real-time detection of viruses
docking to nanowires, especially when multiple viruses dock to the same
nano-wire. The task becomes more complicated when there is an array of
nanowires coated with different antibodies, where different viruses can dock to
each coated nanowire at different binding strengths. We model the array
response to a viral agent as a pattern of conductance change over nanowires
with known modifier --- this representation permits analysis of the output of
such an array via belief network (Bayes) methods, as well as novel generative
models like the Hidden Semi-Markov Model (HSMM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4491</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4491</id><created>2014-07-16</created><authors><author><keyname>Sundman</keyname><forenames>Dennis</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Analysis of Democratic Voting Principles used in Distributed Greedy
  Algorithms</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key aspect for any greedy pursuit algorithm used in compressed sensing is a
good support-set detection method. For distributed compressed sensing, we
consider a setup where many sensors measure sparse signals that are correlated
via the existence of a signals' intersection support-set. This intersection
support-set is called the joint support-set. Estimation of the joint
support-set has a high impact on the performance of a distributed greedy
pursuit algorithm. This estimation can be achieved by exchanging local
support-set estimates followed by a (consensus) voting method. In this paper we
endeavor for a probabilistic analysis of two democratic voting principle that
we call majority and consensus voting. In our analysis, we first model the
input/output relation of a greedy algorithm (executed locally in a sensor) by a
single parameter known as probability of miss. Based on this model, we analyze
the voting principles and prove that the democratic voting principle has a
merit to detect the joint support-set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4498</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4498</id><created>2014-07-16</created><updated>2014-08-30</updated><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author></authors><title>Online Packet-Routing in Grids with Bounded Buffers</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present deterministic and randomized algorithms for the problem of online
packet routing in grids in the competitive network throughput model
\cite{AKOR}. In this model the network has nodes with bounded buffers and
bounded link capacities. The goal in this model is to maximize the throughput,
i.e., the number of delivered packets.
  Our deterministic algorithm is the first online algorithm with an
$O\left(\log^{O(1)}(n)\right)$ competitive ratio for uni-directional grids
(where $n$ denotes the size of the network). The deterministic online algorithm
is centralized and handles packets with deadlines. This algorithm is applicable
to various ranges of values of buffer sizes and communication link capacities.
In particular, it holds for buffer size and communication link capacity in the
range $[3 \ldots \log n]$.
  Our randomized algorithm achieves an expected competitive ratio of $O(\log
n)$ for the uni-directional line. This algorithm is applicable to a wide range
of buffer sizes and communication link capacities. In particular, it holds also
for unit size buffers and unit capacity links. This algorithm improves the best
previous $O(\log^2 n)$-competitive ratio of Azar and Zachut \cite{AZ}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4504</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4504</id><created>2014-07-16</created><updated>2014-09-02</updated><authors><author><keyname>Daneshmand</keyname><forenames>Amir</forenames></author><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Kungurtsev</keyname><forenames>Vyacheslav</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author></authors><title>Hybrid Random/Deterministic Parallel Algorithms for Nonconvex Big Data
  Optimization</title><categories>cs.DC math.OC</categories><comments>The order of the authors is alphabetical</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decomposition framework for the parallel optimization of the sum
of a differentiable {(possibly nonconvex)} function and a nonsmooth (possibly
nonseparable), convex one. The latter term is usually employed to enforce
structure in the solution, typically sparsity. The main contribution of this
work is a novel \emph{parallel, hybrid random/deterministic} decomposition
scheme wherein, at each iteration, a subset of (block) variables is updated at
the same time by minimizing local convex approximations of the original
nonconvex function. To tackle with huge-scale problems, the (block) variables
to be updated are chosen according to a \emph{mixed random and deterministic}
procedure, which captures the advantages of both pure deterministic and random
update-based schemes. Almost sure convergence of the proposed scheme is
established. Numerical results show that on huge-scale problems the proposed
hybrid random/deterministic algorithm outperforms both random and deterministic
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4515</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4515</id><created>2014-07-16</created><updated>2014-09-25</updated><authors><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Kuylenstierna</keyname><forenames>Dan</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Oscillator Phase Noise and Small-Scale Channel Fading in Higher
  Frequency Bands</title><categories>cs.IT math.IT</categories><comments>IEEE Global Telecommun. Conf. (GLOBECOM), Austin, TX, Dec. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the effect of oscillator phase noise and channel
variations due to fading on the performance of communication systems at
frequency bands higher than 10GHz. Phase noise and channel models are reviewed
and technology-dependent bounds on the phase noise quality of radio oscillators
are presented. Our study shows that, in general, both channel variations and
phase noise can have severe effects on the system performance at high
frequencies. Importantly, their relative severity depends on the application
scenario and system parameters such as center frequency and bandwidth. Channel
variations are seen to be more severe than phase noise when the relative
velocity between the transmitter and receiver is high. On the other hand,
performance degradation due to phase noise can be more severe when the center
frequency is increased and the bandwidth is kept a constant, or when
oscillators based on low power CMOS technology are used, as opposed to high
power GaN HEMT based oscillators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4518</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4518</id><created>2014-07-16</created><authors><author><keyname>Lemes</keyname><forenames>Leandro Cruvinel</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author></authors><title>Generalized weights and bounds for error probability over erasure
  channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New upper and lower bounds for the error probability over an erasure channel
are provided, making use of Wei's generalized weights, hierarchy and spectra.
In many situations the upper and lower bounds coincide and this allows
improvement of existing bounds. Results concerning MDS and AMDS codes are
deduced from those bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4519</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4519</id><created>2014-07-16</created><authors><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Estimation of phase noise in oscillators with colored noise sources</title><categories>cs.IT math.IT</categories><journal-ref>Communications Letters, IEEE (Volume:17 , Issue: 11, Nov. 2013,
  Pages: 2160-2163)</journal-ref><doi>10.1109/LCOMM.2013.091113.131850</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we study the design of algorithms for estimation of phase
noise (PN) with colored noise sources. A soft-input maximum a posteriori PN
estimator and a modified soft-input extended Kalman smoother are proposed. The
performance of the proposed algorithms are compared against those studied in
the literature, in terms of mean square error of PN estimation, and symbol
error rate of the considered communication system. The comparisons show that
considerable performance gains can be achieved by designing estimators that
employ correct knowledge of the PN statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4523</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4523</id><created>2014-07-16</created><authors><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Effect of Synchronizing Coordinated Base Stations on Phase Noise
  Estimation</title><categories>cs.IT math.IT</categories><journal-ref>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2013, Pages: 4938-4942</journal-ref><doi>10.1109/ICASSP.2013.6638600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of oscillator phase noise (PN) estimation
in coordinated multi-point (CoMP) transmission systems. Specifically, we
investigate the effect of phase synchronization between coordinated base
stations (BSs) on PN estimation at the user receiver (downlink channel). In
this respect, the Bayesian Cram\'er-Rao bound for PN estimation is derived
which is a function of the level of phase synchronization between the
coordinated BSs. Results show that quality of BS synchronization has a
significant effect on the PN estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4527</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4527</id><created>2014-07-16</created><updated>2014-09-09</updated><authors><author><keyname>Boyle</keyname><forenames>Bradford D.</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>Structural and Optimization Properties for Joint Selection of Source
  Rates and Network Flow</title><categories>cs.NI cs.IT math.IT</categories><comments>15 pages, 13 figures, Submitted to IEEE/ACM Transactions on
  Networking on 2014-07-16. Correction to Fig. 13. Fixed minor typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the optimal transmission of distributed correlated discrete
memoryless sources across a network with capacity constraints. We present
several previously undiscussed structural properties of the set of feasible
rates and transmission schemes. We extend previous results concerning the
intersection of polymatroids and contrapolymatroids to characterize when all of
the vertices of the Slepian-Wolf rate region are feasible for the capacity
constrained network. An explicit relationship between the conditional
independence relationships of the distributed sources and the number of
vertices for the Slepian-Wolf rate region are given. These properties are then
applied to characterize the optimal transmission rate and scheme and its
connection to the corner points of the Slepian-Wolf rate region. In particular,
we demonstrate that when the per-source compression costs are in tension with
the per-link flow costs the optimal flow/rate point need not coincide with a
vertex of the Slepian-Wolf rate region. Finally, we connect results for the
single-sink problem to the multi-sink problem by extending structural insights
and developing upper and lower bounds on the optimal cost of the multi-sink
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4556</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4556</id><created>2014-07-17</created><updated>2014-08-15</updated><authors><author><keyname>Rebiha</keyname><forenames>Rachid</forenames></author><author><keyname>Matringe</keyname><forenames>Nadir</forenames></author><author><keyname>Moura</keyname><forenames>Arnaldo Vieira</forenames></author></authors><title>Generating Asymptotically Non-Terminating Initial Values for Linear
  Programs</title><categories>cs.DM cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the notion of asymptotically non-terminating initial variable
values for linear loop programs. Those values are directly associated to
initial variable values for which the corresponding program does not terminate.
Our theoretical contributions provide us with powerful computational methods
for automatically generating sets of asymptotically non-terminating initial
variable values. Such sets are represented symbolically and exactly by a
semi-linear space, e.g., characterized by conjunctions and disjunctions of
linear equalities and inequalities. Moreover, by taking their complements, we
obtain a precise under-approximation of the set of inputs for which the program
does terminate. We can then reduce the termination problem of linear programs
to the emptiness check of a specific set of asymptotically non-terminating
initial variable values. Our static input data analysis is not restricted only
to programs where the variables are interpreted over the reals. We extend our
approach and provide new decidability results for the termination problem of
affine integer and rational programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4582</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4582</id><created>2014-07-17</created><updated>2015-01-13</updated><authors><author><keyname>Speidel</keyname><forenames>Leo</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Aihara</keyname><forenames>Kazuyuki</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Steady state and mean recurrence time for random walks on stochastic
  temporal networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>5 figures</comments><journal-ref>Physical Review E, 91, 012806 (2015)</journal-ref><doi>10.1103/PhysRevE.91.012806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random walks are basic diffusion processes on networks and have applications
in, for example, searching, navigation, ranking, and community detection.
Recent recognition of the importance of temporal aspects on networks spurred
studies of random walks on temporal networks. Here we theoretically study two
types of event-driven random walks on a stochastic temporal network model that
produces arbitrary distributions of interevent-times. In the so-called active
random walk, the interevent-time is reinitialized on all links upon each
movement of the walker. In the so-called passive random walk, the
interevent-time is only reinitialized on the link that has been used last time,
and it is a type of correlated random walk. We find that the steady state is
always the uniform density for the passive random walk. In contrast, for the
active random walk, it increases or decreases with the node's degree depending
on the distribution of interevent-times. The mean recurrence time of a node is
inversely proportional to the degree for both active and passive random walks.
Furthermore, the mean recurrence time does or does not depend on the
distribution of interevent-times for the active and passive random walks,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4593</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4593</id><created>2014-07-17</created><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author><author><keyname>Giakoumakis</keyname><forenames>Vassilis</forenames></author></authors><title>Weighted Efficient Domination for $(P_5+kP_2)$-Free Graphs in Polynomial
  Time</title><categories>cs.DM</categories><msc-class>68R10, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a finite undirected graph. A vertex {\em dominates} itself and all
its neighbors in $G$. A vertex set $D$ is an {\em efficient dominating set}
(\emph{e.d.}\ for short) of $G$ if every vertex of $G$ is dominated by exactly
one vertex of $D$. The \emph{Efficient Domination} (ED) problem, which asks for
the existence of an e.d.\ in $G$, is known to be \NP-complete even for very
restricted graph classes such as for claw-free graphs, for chordal graphs and
for $2P_3$-free graphs (and thus, for $P_7$-free graphs). We call a graph $F$ a
{\em linear forest} if $F$ is cycle- and claw-free, i.e., its components are
paths. Thus, the ED problem remains \NP-complete for $F$-free graphs, whenever
$F$ is not a linear forest. Let WED denote the vertex-weighted version of the
ED problem asking for an e.d. of minimum weight if one exists.
  In this paper, we show that WED is solvable in polynomial time for
$(P_5+kP_2)$-free graphs for every fixed $k$, which solves an open problem,
and, using modular decomposition, we improve known time bounds for WED on
$(P_4+P_2)$-free graphs, $(P_6,S_{1,2,2})$-free graphs, and on
$(2P_3,S_{1,2,2})$-free graphs and simplify proofs. For $F$-free graphs, the
only remaining open case is WED on $P_6$-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4607</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4607</id><created>2014-07-17</created><authors><author><keyname>Hartmann</keyname><forenames>Thomas</forenames></author><author><keyname>Fouquet</keyname><forenames>Francois</forenames></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames></author><author><keyname>Morin</keyname><forenames>Brice</forenames></author></authors><title>Leveraging Time Distortion for seamless Navigation into Data Space-Time
  Continuum</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent software systems continuously analyze their surrounding
environment and accordingly adapt their internal state. Depending on the
criticality index of the situation, the system should dynamically focus or
widen its analysis and reasoning scope. A naive -why have less when you can
have more- approach would consist in systematically sampling the context at a
very high rate and triggering the reasoning process regularly. This reasoning
process would then need to mine a huge amount of data, extract a relevant view,
and finally analyze this adequate view. This overall process would require some
heavy resources and/or be time-consuming, conflicting with the (near) real-time
response time requirements of intelligent systems. We claim that a continuous
and more flexible navigation into context models, in space and in time, can
significantly improve reasoning processes. This paper thus introduces a novel
modeling approach together with a navigation concept, which consider time and
locality as first-class properties crosscutting any model element, and enable
the seamless navigation of models in this space-time continuum. In particular,
we leverage a time-relative navigation (inspired by the space-time and
distortion theory [7]) able to efficiently empower continuous reasoning
processes. We integrate our approach into an open-source modeling framework and
evaluate it on a smart grid reasoning engine for electric load prediction. We
demonstrate that reasoners leveraging this distorted space-time continuum
outperform the full sampling approach, and is compatible with most of (near)
real-time requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4610</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4610</id><created>2014-07-17</created><updated>2015-05-27</updated><authors><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author><author><keyname>Hanel</keyname><forenames>Rudolf</forenames></author><author><keyname>Liu</keyname><forenames>Bo</forenames></author><author><keyname>Corominas-Murtra</keyname><forenames>Bernat</forenames></author></authors><title>Understanding Zipf's law of word frequencies through sample-space
  collapse in sentence formation</title><categories>physics.soc-ph cs.CL</categories><comments>7 pages, 4 figures. Accepted for publication in the Journal of the
  Royal Society Interface</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formation of sentences is a highly structured and history-dependent
process. The probability of using a specific word in a sentence strongly
depends on the 'history' of word-usage earlier in that sentence. We study a
simple history-dependent model of text generation assuming that the
sample-space of word usage reduces along sentence formation, on average. We
first show that the model explains the approximate Zipf law found in word
frequencies as a direct consequence of sample-space reduction. We then
empirically quantify the amount of sample-space reduction in the sentences of
ten famous English books, by analysis of corresponding word-transition tables
that capture which words can follow any given word in a text. We find a highly
nested structure in these transition tables and show that this `nestedness' is
tightly related to the power law exponents of the observed word frequency
distributions. With the proposed model it is possible to understand that the
nestedness of a text can be the origin of the actual scaling exponent, and that
deviations from the exact Zipf law can be understood by variations of the
degree of nestedness on a book-by-book basis. On a theoretical level we are
able to show that in case of weak nesting, Zipf's law breaks down in a fast
transition. Unlike previous attempts to understand Zipf's law in language the
sample-space reducing model is not based on assumptions of multiplicative,
preferential, or self-organised critical mechanisms behind language formation,
but simply used the empirically quantifiable parameter 'nestedness' to
understand the statistics of word frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4623</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4623</id><created>2014-07-17</created><updated>2014-11-01</updated><authors><author><keyname>Azimi-Tafreshi</keyname><forenames>N.</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Giant components in directed multiplex networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>8 pages, 9 figures</comments><journal-ref>Phys. Rev. E 90, 052809 (2014)</journal-ref><doi>10.1103/PhysRevE.90.052809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the complex global structure of giant components in directed
multiplex networks which generalizes the well-known bow-tie structure, generic
for ordinary directed networks. By definition, a directed multiplex network
contains vertices of one type and directed edges of $m$ different types. In
directed multiplex networks, we distinguish a set of different giant components
based on the existence of directed paths of different types between their
vertices, such that for each type of edges, the paths run entirely through only
edges of that type. If, in particular, $m=2$, we define a strongly viable
component as a set of vertices, in which for each type of edges, each two
vertices are interconnected by at least two directed paths in both directions,
running through the edges of only this type. We show that in this case, a
directed multiplex network contains, in total, $9$ different giant components
including the strongly viable component. In general, the total number of giant
components is $3^m$. For uncorrelated directed multiplex networks, we obtain
exactly the size and the emergence point of the strongly viable component and
estimate the sizes of other giant components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4626</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4626</id><created>2014-07-17</created><authors><author><keyname>Sergeev</keyname><forenames>Igor S.</forenames></author></authors><title>On relative OR-complexity of Boolean matrices and their complements</title><categories>cs.CC</categories><comments>3 pages, published in Russian in Proc. 17-th Int. Conf. on Problems
  of Theoretical Cybernetics (Kazan, 16-20 June 2014), 262-264</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct explicit Boolean square matrices whose rectifier complexity
(OR-complexity) differs significantly from the complexity of their complement
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4636</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4636</id><created>2014-07-17</created><authors><author><keyname>Quagliarella</keyname><forenames>Domenico</forenames></author><author><keyname>Petrone</keyname><forenames>Giovanni</forenames></author><author><keyname>Iaccarino</keyname><forenames>Gianluca</forenames></author></authors><title>Optimization Under Uncertainty Using the Generalized Inverse
  Distribution Function</title><categories>math.OC cs.NE</categories><comments>20 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework for robust optimization under uncertainty based on the use of the
generalized inverse distribution function (GIDF), also called quantile
function, is here proposed. Compared to more classical approaches that rely on
the usage of statistical moments as deterministic attributes that define the
objectives of the optimization process, the inverse cumulative distribution
function allows for the use of all the possible information available in the
probabilistic domain. Furthermore, the use of a quantile based approach leads
naturally to a multi-objective methodology which allows an a-posteriori
selection of the candidate design based on risk/opportunity criteria defined by
the designer. Finally, the error on the estimation of the objectives due to the
resolution of the GIDF will be proven to be quantifiable
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4640</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4640</id><created>2014-07-17</created><updated>2015-02-09</updated><authors><author><keyname>Sopin</keyname><forenames>Valerii</forenames></author></authors><title>A new algorithm for solving the rSUM problem</title><categories>cs.DS cs.CC cs.CG math.NT</categories><msc-class>68W01</msc-class><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A determined algorithm is presented for solving the rSUM problem for any
natural r with a sub-quadratic assessment of time complexity in some cases. In
terms of an amount of memory used the obtained algorithm is the nlog^3(n)
order.
  The idea of the obtained algorithm is based not considering integer numbers,
but rather k (is a natural) successive bits of these numbers in the binary
numeration system. It is shown that if a sum of integer numbers is equal to
zero, then the sum of numbers presented by any k successive bits of these
numbers must be sufficiently &quot;close&quot; to zero. This makes it possible to discard
the numbers, which a fortiori, do not establish the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4645</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4645</id><created>2014-07-17</created><updated>2015-05-27</updated><authors><author><keyname>Cerrito</keyname><forenames>Serenella</forenames></author><author><keyname>David</keyname><forenames>Am&#xe9;lie</forenames></author><author><keyname>Goranko</keyname><forenames>Valentin</forenames></author></authors><title>Optimal Tableaux Method for Constructive Satisfiability Testing and
  Model Synthesis in the Alternating-time Temporal Logic ATL+</title><categories>cs.LO</categories><comments>45 pages</comments><msc-class>03B44, 68Q60</msc-class><acm-class>F.4.1; F.4.3</acm-class><journal-ref>IJCAR 2014, Springer LNCS 8562. pp. 277-291, 2014</journal-ref><doi>10.1007/978-3-319-08587-6_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a sound, complete and practically implementable tableaux-based
decision method for constructive satisfiability testing and model synthesis in
the fragment ATL+ of the full Alternating time temporal logic ATL*. The method
extends in an essential way a previously developed tableaux-based decision
method for ATL and works in 2EXPTIME, which is the optimal worst case
complexity of the satisfiability problem for ATL+ . We also discuss how
suitable parametrizations and syntactic restrictions on the class of input ATL+
formulae can reduce the complexity of the satisfiability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4650</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4650</id><created>2014-07-17</created><authors><author><keyname>Shaw</keyname><forenames>Dipan Lal</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Islam</keyname><forenames>A. S. M. Sohidull</forenames></author><author><keyname>Karmaker</keyname><forenames>Shuvasish</forenames></author></authors><title>Protein Folding in the Hexagonal Prism Lattice with Diagonals</title><categories>cs.CE</categories><comments>12 page, 8 figure, ISSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting protein secondary structure using lattice model is one of the most
studied computational problem in bioinformatics. Here secondary structure or
three dimensional structure of protein is predicted from its amino acid
sequence. Secondary structure refers to local sub-structures of protein. Mostly
founded secondary structures are alpha helix and beta sheets. Since, it is a
problem of great potential complexity many simplified energy model have been
proposed in literature on basis of interaction of amino acid residue in
protein. Here we use well researched Hydrophobic-Polar (HP) energy model. In
this paper, we proposed hexagonal prism lattice with diagonal that can overcome
the problems of other lattice structure, e.g., parity problem. We give two
approximation algorithm for protein folding on this lattice. Our first
algorithm leads us to similar structure of helix structure which is commonly
found in protein structure. This motivated us to find next algorithm which
improves the algorithm ratio of 9/7.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4668</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4668</id><created>2014-07-17</created><authors><author><keyname>Zimmermann</keyname><forenames>Albrecht</forenames></author></authors><title>A feature construction framework based on outlier detection and
  discriminative pattern mining</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  No matter the expressive power and sophistication of supervised learning
algorithms, their effectiveness is restricted by the features describing the
data. This is not a new insight in ML and many methods for feature selection,
transformation, and construction have been developed. But while this is
on-going for general techniques for feature selection and transformation, i.e.
dimensionality reduction, work on feature construction, i.e. enriching the
data, is by now mainly the domain of image, particularly character,
recognition, and NLP.
  In this work, we propose a new general framework for feature construction.
The need for feature construction in a data set is indicated by class outliers
and discriminative pattern mining used to derive features on their
k-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, and
evaluate the usefulness of the derived features on a diverse collection of UCI
data sets. The derived features are more often useful than ones derived by
DC-Fringe, and our approach is much less likely to overfit. But while a weak
learner, Naive Bayes, benefits strongly from the feature construction, the
effect is less pronounced for C4.5, and almost vanishes for an SVM leaner.
  Keywords: feature construction, classification, outlier detection
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4692</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4692</id><created>2014-07-17</created><authors><author><keyname>Berardi</keyname><forenames>Stefano</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Steila</keyname><forenames>Silvia</forenames></author></authors><title>Proving termination with transition invariants of height omega</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Termination Theorem by Podelski and Rybalchenko states that the reduction
relations which are terminating from any initial state are exactly the
reduction relations whose transitive closure, restricted to the accessible
states, is included in some finite union of well-founded relations. An
alternative statement of the theorem is that terminating reduction relations
are precisely those having a &quot;disjunctively well-founded transition invariant&quot;.
From this result the same authors and Byron Cook designed an algorithm checking
a sufficient condition for termination for a while-if program. The algorithm
looks for a disjunctively well-founded transition invariant, made of
well-founded relations of height omega, and if it finds it, it deduces the
termination for the while-if program using the Termination Theorem.
  This raises an interesting question: What is the status of reduction
relations having a disjunctively well-founded transition invariant where each
relation has height omega? An answer to this question can lead to a
characterization of the set of while-if programs which the termination
algorithm can prove to be terminating. The goal of this work is to prove that
they are exactly the set of reduction relations having height omega^n for some
n &lt; omega. Besides, if all the relations in the transition invariant are
primitive recursive and the reduction relation is the graph of the restriction
to some primitive recursive set of a primitive recursive map, then a final
state is computable by some primitive recursive map in the initial state.
  As a corollary we derive that the set of functions, having at least one
implementation in Podelski Rybalchenko while-if language with a well-founded
disjunctively transition invariant where each relation has height omega, is
exactly the set of primitive recursive functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4694</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4694</id><created>2014-07-17</created><updated>2014-08-17</updated><authors><author><keyname>Shen</keyname><forenames>Kaiming</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Distributed Pricing-Based User Association for Downlink Heterogeneous
  Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Journal on Selected Areas in Communications, Special Issue on 5G
  Communication Systems, June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the optimization of the user and base-station (BS)
association in a wireless downlink heterogeneous cellular network under the
proportional fairness criterion. We first consider the case where each BS has a
single antenna and transmits at fixed power, and propose a distributed price
update strategy for a pricing-based user association scheme, in which the users
are assigned to the BS based on the value of a utility function minus a price.
The proposed price update algorithm is based on a coordinate descent method for
solving the dual of the network utility maximization problem, and it has a
rigorous performance guarantee. The main advantage of the proposed algorithm as
compared to the existing subgradient method for price update is that the
proposed algorithm is independent of parameter choices and can be implemented
asynchronously. Further, this paper considers the joint user association and BS
power control problem, and proposes an iterative dual coordinate descent and
the power optimization algorithm that significantly outperforms existing
approaches. Finally, this paper considers the joint user association and BS
beamforming problem for the case where the BSs are equipped with multiple
antennas and spatially multiplex multiple users. We incorporate dual coordinate
descent with the weighted minimum mean-squared error (WMMSE) algorithm, and
show that it achieves nearly the same performance as a computationally more
complex benchmark algorithm (which applies the WMMSE algorithm on the entire
network for BS association), while avoiding excessive BS handover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4705</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4705</id><created>2014-07-17</created><authors><author><keyname>Vickers</keyname><forenames>Paul</forenames></author><author><keyname>Laing</keyname><forenames>Chris</forenames></author><author><keyname>Fairfax</keyname><forenames>Tom</forenames></author></authors><title>Sonification of a Network's Self-Organized Criticality</title><categories>cs.HC cs.NI cs.SD</categories><acm-class>H.5.2; C.2.3; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication networks involve the transmission and reception of large
volumes of data. Research indicates that network traffic volumes will continue
to increase. These traffic volumes will be unprecedented and the behaviour of
global information infrastructures when dealing with these data volumes is
unknown. It has been shown that complex systems (including computer networks)
exhibit self-organized criticality under certain conditions. Given the
possibility in such systems of a sudden and spontaneous system reset the
development of techniques to inform system administrators of this behaviour
could be beneficial. This article focuses on the combination of two dissimilar
research concepts, namely sonification (a form of auditory display) and
self-organized criticality (SOC). A system is described that sonifies in real
time an information infrastructure's self-organized criticality to alert the
network administrators of both normal and abnormal network traffic and
operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4709</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4709</id><created>2014-07-17</created><authors><author><keyname>Bulitko</keyname><forenames>Vadim</forenames></author></authors><title>Flow for Meta Control</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The psychological state of flow has been linked to optimizing human
performance. A key condition of flow emergence is a match between the human
abilities and complexity of the task. We propose a simple computational model
of flow for Artificial Intelligence (AI) agents. The model factors the standard
agent-environment state into a self-reflective set of the agent's abilities and
a socially learned set of the environmental complexity. Maximizing the flow
serves as a meta control for the agent. We show how to apply the meta-control
policy to a broad class of AI control policies and illustrate our approach with
a specific implementation. Results in a synthetic testbed are promising and
open interesting directions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4711</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4711</id><created>2014-07-17</created><updated>2016-03-04</updated><authors><author><keyname>Kariv</keyname><forenames>Jonathan</forenames></author><author><keyname>van Alten</keyname><forenames>Clint</forenames></author><author><keyname>Yeroshkin</keyname><forenames>Dmytro</forenames></author></authors><title>On a Certain Cooperative Hat Game</title><categories>math.CO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2010, Lionel Levine introduced a cooperative game, and posed the question
of finding the optimal strategy. We provide an overview of prior work on this
question, and describe several strategies that give the best lower bound on the
probability of victory. We further generalize the problem to consider the case
of arbitrary color distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4721</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4721</id><created>2014-07-17</created><authors><author><keyname>Ablinger</keyname><forenames>Jakob</forenames></author><author><keyname>Bl&#xfc;mlein</keyname><forenames>Johannes</forenames></author><author><keyname>Raab</keyname><forenames>Clemens G.</forenames></author><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Nested (inverse) binomial sums and new iterated integrals for massive
  Feynman diagrams</title><categories>hep-th cs.SC</categories><comments>13 pages LATEX, one style file, Proceedings of Loops and Legs in
  Quantum Field Theory -- LL2014,27 April 2014 -- 02 May 2014 Weimar, Germany</comments><report-no>DESY 14--131, DO-TH 14/16, SFB/CPP-14-55, LPN14-093</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested sums containing binomial coefficients occur in the computation of
massive operator matrix elements. Their associated iterated integrals lead to
alphabets including radicals, for which we determined a suitable basis. We
discuss algorithms for converting between sum and integral representations,
mainly relying on the Mellin transform. To aid the conversion we worked out
dedicated rewrite rules, based on which also some general patterns emerging in
the process can be obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4723</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4723</id><created>2014-07-17</created><authors><author><keyname>Beliga</keyname><forenames>Slobodan</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Martin&#x10d;ci&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author></authors><title>Toward Selectivity Based Keyword Extraction for Croatian News</title><categories>cs.CL cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preliminary report on network based keyword extraction for Croatian is an
unsupervised method for keyword extraction from the complex network. We build
our approach with a new network measure the node selectivity, motivated by the
research of the graph based centrality approaches. The node selectivity is
defined as the average weight distribution on the links of the single node. We
extract nodes (keyword candidates) based on the selectivity value. Furthermore,
we expand extracted nodes to word-tuples ranked with the highest in/out
selectivity values. Selectivity based extraction does not require linguistic
knowledge while it is purely derived from statistical and structural
information en-compassed in the source text which is reflected into the
structure of the network. Obtained sets are evaluated on a manually annotated
keywords: for the set of extracted keyword candidates average F1 score is
24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates
average F1 score is 25,9% and average F2 score is 24,47%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4729</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4729</id><created>2014-07-17</created><authors><author><keyname>Lou</keyname><forenames>Yin</forenames></author><author><keyname>Bien</keyname><forenames>Jacob</forenames></author><author><keyname>Caruana</keyname><forenames>Rich</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author></authors><title>Sparse Partially Linear Additive Models</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized partially linear additive model (GPLAM) is a flexible and
interpretable approach to building predictive models. It combines features in
an additive manner, allowing them to have either a linear or nonlinear effect
on the response. However, the assignment of features to the linear and
nonlinear groups is typically assumed known. Thus, to make a GPLAM a viable
approach in situations in which little is known $apriori$ about the features,
one must overcome two primary model selection challenges: deciding which
features to include in the model and determining which features to treat
nonlinearly. We introduce sparse partially linear additive models (SPLAMs),
which combine model fitting and $both$ of these model selection challenges into
a single convex optimization problem. SPLAM provides a bridge between the Lasso
and sparse additive models. Through a statistical oracle inequality and
thorough simulation, we demonstrate that SPLAM can outperform other methods
across a broad spectrum of statistical regimes, including the high-dimensional
($p\gg N$) setting. We develop efficient algorithms that are applied to real
data sets with half a million samples and over 45,000 features with excellent
predictive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4735</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4735</id><created>2014-07-17</created><authors><author><keyname>Saini</keyname><forenames>Lalit Kumar</forenames></author><author><keyname>Shrivastava</keyname><forenames>Vishal</forenames></author></authors><title>A Survey of Digital Watermarking Techniques and its Applications</title><categories>cs.MM</categories><comments>4 Pages</comments><journal-ref>IJCST V2(3): Page(70-73) May-June 2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Digital media is the need of a people now a day as the alternate of paper
media.As the technology grown up digital media required protection while
transferring through internet or others mediums.Watermarking techniques have
been developed to fulfill this requirement.This paper aims to provide a
detailed survey of all watermarking techniques specially focuses on image
watermarking types and its applications in today world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4738</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4738</id><created>2014-07-17</created><authors><author><keyname>Saini</keyname><forenames>Lalit Kumar</forenames></author><author><keyname>Shrivastava</keyname><forenames>Vishal</forenames></author></authors><title>Analysis of Attacks on Hybrid DWT-DCT Algorithm for Digital Image
  Watermarking With MATLAB</title><categories>cs.CR cs.MM</categories><comments>4 Pages</comments><journal-ref>IJCST V2(3): Page(123-125) May-June 2014. ISSN: 2347-8578.
  www.ijcstjournal.org</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Watermarking algorithms needs properties of robustness and perceptibility.
But these properties are affected by different -2 types of attacks performed on
watermarked images. The goal of performing attacks is destroy the information
of watermark hidden in the watermarked image. So every Algorithms should be
previously tested by developers so that it would not affected by attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4739</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4739</id><created>2014-07-17</created><authors><author><keyname>Sarath</keyname><forenames>T.</forenames></author><author><keyname>Nagalakshmi</keyname><forenames>G.</forenames></author></authors><title>An landcover fuzzy logic classification by maximumlikelihood</title><categories>cs.CV cs.LG</categories><comments>5 Pages, 3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In present days remote sensing is most used application in many sectors. This
remote sensing uses different images like multispectral, hyper spectral or
ultra spectral. The remote sensing image classification is one of the
significant method to classify image. In this state we classify the maximum
likelihood classification with fuzzy logic. In this we experimenting fuzzy
logic like spatial, spectral texture methods in that different sub methods to
be used for image classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4744</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4744</id><created>2014-07-17</created><authors><author><keyname>Lemonnier</keyname><forenames>Remi</forenames></author><author><keyname>Scaman</keyname><forenames>Kevin</forenames></author><author><keyname>Vayatis</keyname><forenames>Nicolas</forenames></author></authors><title>Tight Bounds for Influence in Diffusion Networks and Application to Bond
  Percolation and Epidemiology</title><categories>math.PR cs.SI physics.soc-ph</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive theoretical bounds for the long-term influence of a
node in an Independent Cascade Model (ICM). We relate these bounds to the
spectral radius of a particular matrix and show that the behavior is
sub-critical when this spectral radius is lower than $1$. More specifically, we
point out that, in general networks, the sub-critical regime behaves in
$O(\sqrt{n})$ where $n$ is the size of the network, and that this upper bound
is met for star-shaped networks. We apply our results to epidemiology and
percolation on arbitrary networks, and derive a bound for the critical value
beyond which a giant connected component arises. Finally, we show empirically
the tightness of our bounds for a large family of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4755</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4755</id><created>2014-07-17</created><authors><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Wang</keyname><forenames>Chengu</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>On The Communication Complexity of Linear Algebraic Problems in the
  Message Passing Model</title><categories>cs.CC cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the communication complexity of linear algebraic problems over
finite fields in the multi-player message passing model, proving a number of
tight lower bounds. Specifically, for a matrix which is distributed among a
number of players, we consider the problem of determining its rank, of
computing entries in its inverse, and of solving linear equations. We also
consider related problems such as computing the generalized inner product of
vectors held on different servers. We give a general framework for reducing
these multi-player problems to their two-player counterparts, showing that the
randomized $s$-player communication complexity of these problems is at least
$s$ times the randomized two-player communication complexity. Provided the
problem has a certain amount of algebraic symmetry, which we formally define,
we can show the hardest input distribution is a symmetric distribution, and
therefore apply a recent multi-player lower bound technique of Phillips et al.
Further, we give new two-player lower bounds for a number of these problems. In
particular, our optimal lower bound for the two-player version of the matrix
rank problem resolves an open question of Sun and Wang.
  A common feature of our lower bounds is that they apply even to the special
&quot;threshold promise&quot; versions of these problems, wherein the underlying
quantity, e.g., rank, is promised to be one of just two values, one on each
side of some critical threshold. These kinds of promise problems are
commonplace in the literature on data streaming as sources of hardness for
reductions giving space lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4760</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4760</id><created>2014-07-17</created><authors><author><keyname>Scaman</keyname><forenames>Kevin</forenames></author><author><keyname>Kalogeratos</keyname><forenames>Argyris</forenames></author><author><keyname>Vayatis</keyname><forenames>Nicolas</forenames></author></authors><title>What Makes a Good Plan? An Efficient Planning Approach to Control
  Diffusion Processes in Networks</title><categories>math.OC cs.SI physics.soc-ph</categories><comments>18 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the quality of a large class of simple dynamic
resource allocation (DRA) strategies which we name priority planning. Their aim
is to control an undesired diffusion process by distributing resources to the
contagious nodes of the network according to a predefined priority-order. In
our analysis, we reduce the DRA problem to the linear arrangement of the nodes
of the network. Under this perspective, we shed light on the role of a
fundamental characteristic of this arrangement, the maximum cutwidth, for
assessing the quality of any priority planning strategy. Our theoretical
analysis validates the role of the maximum cutwidth by deriving bounds for the
extinction time of the diffusion process. Finally, using the results of our
analysis, we propose a novel and efficient DRA strategy, called Maximum
Cutwidth Minimization, that outperforms other competing strategies in our
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4764</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4764</id><created>2014-07-17</created><updated>2014-11-17</updated><authors><author><keyname>Chatfield</keyname><forenames>Ken</forenames></author><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Efficient On-the-fly Category Retrieval using ConvNets and GPUs</title><categories>cs.CV cs.LG cs.NE</categories><comments>Published in proceedings of ACCV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the gains in precision and speed, that can be obtained by
using Convolutional Networks (ConvNets) for on-the-fly retrieval - where
classifiers are learnt at run time for a textual query from downloaded images,
and used to rank large image or video datasets.
  We make three contributions: (i) we present an evaluation of state-of-the-art
image representations for object category retrieval over standard benchmark
datasets containing 1M+ images; (ii) we show that ConvNets can be used to
obtain features which are incredibly performant, and yet much lower dimensional
than previous state-of-the-art image representations, and that their
dimensionality can be reduced further without loss in performance by
compression using product quantization or binarization. Consequently, features
with the state-of-the-art performance on large-scale datasets of millions of
images can fit in the memory of even a commodity GPU card; (iii) we show that
an SVM classifier can be learnt within a ConvNet framework on a GPU in parallel
with downloading the new training images, allowing for a continuous refinement
of the model as more images become available, and simultaneous training and
ranking. The outcome is an on-the-fly system that significantly outperforms its
predecessors in terms of: precision of retrieval, memory requirements, and
speed, facilitating accurate on-the-fly learning and ranking in under a second
on a single GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4765</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4765</id><created>2014-07-17</created><authors><author><keyname>Kasheff</keyname><forenames>Zardosht</forenames></author><author><keyname>Walsh</keyname><forenames>Leif</forenames></author></authors><title>Ark: A Real-World Consensus Implementation</title><categories>cs.DC cs.DB</categories><comments>12 pages</comments><acm-class>H.2.4; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ark is an implementation of a consensus algorithm similar to Paxos and Raft,
designed as an improvement over the existing consensus algorithm used by
MongoDB and TokuMX.
  Ark was designed from first principles, improving on the election algorithm
used by TokuMX, to fix deficiencies in MongoDB's consensus algorithms that can
cause data loss. It ultimately has many similarities with Raft, but diverges in
a few ways, mainly to support other features like chained replication and
unacknowledged writes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4777</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4777</id><created>2014-07-17</created><updated>2015-03-27</updated><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Koren&#x10d;iak</keyname><forenames>&#x13d;ubo&#x161;</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>&#x158;eh&#xe1;k</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Optimizing Performance of Continuous-Time Stochastic Systems using
  Timeout Synthesis</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider parametric version of fixed-delay continuous-time Markov chains
(or equivalently deterministic and stochastic Petri nets, DSPN) where
fixed-delay transitions are specified by parameters, rather than concrete
values. Our goal is to synthesize values of these parameters that, for a given
cost function, minimise expected total cost incurred before reaching a given
set of target states. We show that under mild assumptions, optimal values of
parameters can be effectively approximated using translation to a Markov
decision process (MDP) whose actions correspond to discretized values of these
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4798</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4798</id><created>2014-07-17</created><authors><author><keyname>Del Pia</keyname><forenames>Alberto</forenames></author><author><keyname>Dey</keyname><forenames>Santanu S.</forenames></author><author><keyname>Molinaro</keyname><forenames>Marco</forenames></author></authors><title>Mixed-integer Quadratic Programming is in NP</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed-integer quadratic programming is the problem of optimizing a quadratic
function over points in a polyhedral set where some of the components are
restricted to be integral. In this paper, we prove that the decision version of
mixed-integer quadratic programming is in NP, thereby showing that it is
NP-complete. This is established by showing that if the decision version of
mixed-integer quadratic programming is feasible, then there exists a solution
of polynomial size. This result generalizes and unifies classical results that
quadratic programming is in NP and integer linear programming is in NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4831</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4831</id><created>2014-07-16</created><authors><author><keyname>Schumann</keyname><forenames>Laura</forenames></author><author><keyname>Stock</keyname><forenames>Wolfgang G.</forenames></author></authors><title>The Information Service Evaluation (ISE) Model</title><categories>cs.CY cs.DB</categories><comments>20 pp</comments><acm-class>H.3.4; H.3.5</acm-class><journal-ref>Webology, 11(1), June, 2014, Art. 115</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Information services are an inherent part of our everyday life. Especially
since ubiquitous cities are being developed all over the world their number is
increasing even faster. They aim at facilitating the production of information
and the access to the needed information and are supposed to make life easier.
Until today many different evaluation models (among others, TAM, TAM 2, TAM 3,
UTAUT and MATH) have been developed to measure the quality and acceptance of
these services. Still, they only consider subareas of the whole concept that
represents an information service. As a holistic and comprehensive approach,
the ISE Model studies five dimensions that influence adoption, use, impact and
diffusion of the information service: information service quality, information
user, information acceptance, information environment and time. All these
aspects have a great impact on the final grading and of the success (or
failure) of the service. Our model combines approaches, which study subjective
impressions of users (e.g., the perceived service quality), and
user-independent, more objective approaches (e.g., the degree of gamification
of a system). Furthermore, we adopt results of network economics, especially
the Success breeds success-principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4832</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4832</id><created>2014-07-16</created><authors><author><keyname>Coma-Puig</keyname><forenames>Bernat</forenames></author><author><keyname>Diaz-Aviles</keyname><forenames>Ernesto</forenames></author><author><keyname>Nejdl</keyname><forenames>Wolfgang</forenames></author></authors><title>Collaborative Filtering Ensemble for Personalized Name Recommendation</title><categories>cs.IR cs.AI cs.LG</categories><comments>Top-N recommendation; personalized ranking; given name recommendation</comments><acm-class>H.3.3; I.2.6</acm-class><journal-ref>Proceedings of the ECML PKDD Discovery Challenge - Recommending
  Given Names. Co-located with ECML PKDD 2013. Prague, Czech Republic,
  September 27, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Out of thousands of names to choose from, picking the right one for your
child is a daunting task. In this work, our objective is to help parents making
an informed decision while choosing a name for their baby. We follow a
recommender system approach and combine, in an ensemble, the individual
rankings produced by simple collaborative filtering algorithms in order to
produce a personalized list of names that meets the individual parents' taste.
Our experiments were conducted using real-world data collected from the query
logs of 'nameling' (nameling.net), an online portal for searching and exploring
names, which corresponds to the dataset released in the context of the ECML
PKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and
features fast training and prediction steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4833</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4833</id><created>2014-07-17</created><updated>2014-08-11</updated><authors><author><keyname>Nevzorova</keyname><forenames>Olga</forenames></author><author><keyname>Zhiltsov</keyname><forenames>Nikita</forenames></author><author><keyname>Kirillovich</keyname><forenames>Alexander</forenames></author><author><keyname>Lipachev</keyname><forenames>Evgeny</forenames></author></authors><title>$OntoMath^{PRO}$ Ontology: A Linked Data Hub for Mathematics</title><categories>cs.AI cs.DL cs.IR</categories><comments>15 pages, 6 images, 1 table, Knowledge Engineering and the Semantic
  Web - 5th International Conference</comments><msc-class>68T30</msc-class><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an ontology of mathematical knowledge concepts that
covers a wide range of the fields of mathematics and introduces a balanced
representation between comprehensive and sensible models. We demonstrate the
applications of this representation in information extraction, semantic search,
and education. We argue that the ontology can be a core of future integration
of math-aware data sets in the Web of Data and, therefore, provide mappings
onto relevant datasets, such as DBpedia and ScienceWISE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4835</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4835</id><created>2014-07-17</created><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author></authors><title>Partial Quantifier Elimination</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Partial Quantifier Elimination (PQE). Given
formula exists(X)[F(X,Y) &amp; G(X,Y)], where F, G are in conjunctive normal form,
the PQE problem is to find a formula F*(Y) such that F* &amp; exists(X)[G] is
logically equivalent to exists(X)[F &amp; G]. We solve the PQE problem by
generating and adding to F clauses over the free variables that make the
clauses of F with quantified variables redundant. The traditional Quantifier
Elimination problem (QE) is a special case of PQE where G is empty so all
clauses of the input formula with quantified variables need to be made
redundant. The importance of PQE is twofold. First, many problems are more
naturally formulated in terms of PQE rather than QE. Second, in many cases PQE
can be solved more efficiently than QE. We describe a PQE algorithm based on
the machinery of dependency sequents and give experimental results showing the
promise of PQE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4859</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4859</id><created>2014-07-17</created><authors><author><keyname>Majeti</keyname><forenames>Deepak</forenames></author><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Barik</keyname><forenames>Rajkishore</forenames></author><author><keyname>Sarkar</keyname><forenames>Vivek</forenames></author></authors><title>ADHA: Automatic Data layout framework for Heterogeneous Architectures</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data layouts play a crucial role in determining the performance of a given
application running on a given architecture. Existing parallel programming
frameworks for both multicore and heterogeneous systems leave the onus of
selecting a data layout to the programmer. Therefore, shifting the burden of
data layout selection to optimizing compilers can greatly enhance programmer
productivity and application performance. In this work, we introduce {\ADHA}: a
two-level hierarchal formulation of the data layout problem for modern
heterogeneous architectures. We have created a reference implementation of ADHA
in the Heterogeneous Habanero-C (H2C) parallel programming system. ADHA shows
significant performance benefits of up to 6.92$\times$ compared to manually
specified layouts for two benchmark programs running on a CPU+GPU heterogeneous
platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4863</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4863</id><created>2014-07-17</created><authors><author><keyname>Said</keyname><forenames>Gamal Abd El-Nasser A.</forenames></author><author><keyname>Mahmoud</keyname><forenames>Abeer M.</forenames></author><author><keyname>El-Horbaty</keyname><forenames>El-Sayed M.</forenames></author></authors><title>A Comparative Study of Meta-heuristic Algorithms for Solving Quadratic
  Assignment Problem</title><categories>cs.AI cs.NE</categories><comments>6 pages, 3 figures</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 5 No. 1, 2014</journal-ref><doi>10.14569/IJACSA.2014.050101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimization
problem, therefore, solving the QAP requires applying one or more of the
meta-heuristic algorithms. This paper presents a comparative study between
Meta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulated
annealing for solving a real-life (QAP) and analyze their performance in terms
of both runtime efficiency and solution quality. The results show that Genetic
Algorithm has a better solution quality while Tabu Search has a faster
execution time in comparison with other Meta-heuristic algorithms for solving
QAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4865</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4865</id><created>2014-07-17</created><updated>2014-08-14</updated><authors><author><keyname>Dixit</keyname><forenames>Pushkar</forenames></author><author><keyname>Singh</keyname><forenames>Nishant</forenames></author><author><keyname>Gupta</keyname><forenames>Jay Prakash</forenames></author></authors><title>Robust Lossless Semi Fragile Information Protection in Images</title><categories>cs.MM</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  diffusion process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet security finds it difficult to keep the information secure and to
maintain the integrity of the data. Sending messages over the internet secretly
is one of the major tasks as it is widely used for passing the message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4867</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4867</id><created>2014-07-17</created><updated>2014-08-14</updated><authors><author><keyname>Gupta</keyname><forenames>Jay Prakash</forenames></author><author><keyname>Dixit</keyname><forenames>Pushkar</forenames></author><author><keyname>Singh</keyname><forenames>Nishant</forenames></author><author><keyname>Semwal</keyname><forenames>Vijay Bhaskar</forenames></author></authors><title>Analysis of Gait Pattern to Recognize the Human Activities</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human activity recognition based on the computer vision is the process of
labelling image sequences with action labels. Accurate systems for this problem
are applied in areas such as visual surveillance, human computer interaction
and video retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4874</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4874</id><created>2014-07-17</created><authors><author><keyname>Wang</keyname><forenames>Zhenhua</forenames></author><author><keyname>Fan</keyname><forenames>Bin</forenames></author><author><keyname>Wu</keyname><forenames>Fuchao</forenames></author></authors><title>Affine Subspace Representation for Feature Description</title><categories>cs.CV</categories><comments>To Appear in the 2014 European Conference on Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel Affine Subspace Representation (ASR) descriptor
to deal with affine distortions induced by viewpoint changes. Unlike the
traditional local descriptors such as SIFT, ASR inherently encodes local
information of multi-view patches, making it robust to affine distortions while
maintaining a high discriminative ability. To this end, PCA is used to
represent affine-warped patches as PCA-patch vectors for its compactness and
efficiency. Then according to the subspace assumption, which implies that the
PCA-patch vectors of various affine-warped patches of the same keypoint can be
represented by a low-dimensional linear subspace, the ASR descriptor is
obtained by using a simple subspace-to-point mapping. Such a linear subspace
representation could accurately capture the underlying information of a
keypoint (local structure) under multiple views without sacrificing its
distinctiveness. To accelerate the computation of ASR descriptor, a fast
approximate algorithm is proposed by moving the most computational part (ie,
warp patch under various affine transformations) to an offline training stage.
Experimental results show that ASR is not only better than the state-of-the-art
descriptors under various image transformations, but also performs well without
a dedicated affine invariant detector when dealing with viewpoint changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4879</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4879</id><created>2014-07-17</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames><affiliation>Andrew</affiliation></author><author><keyname>Huang</keyname><forenames>Shisheng</forenames><affiliation>Andrew</affiliation></author><author><keyname>Yuen</keyname><forenames>Chau</forenames><affiliation>Andrew</affiliation></author><author><keyname>Jian</keyname><affiliation>Andrew</affiliation></author><author><keyname>Zhang</keyname></author><author><keyname>Smith</keyname><forenames>David B.</forenames></author></authors><title>Synthetic Generation of Solar States for Smart Grid: A Multiple Segment
  Markov Chain Apptoach</title><categories>cs.PF</categories><comments>6 pages, 4 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of photovoltaic (PV) sources is becoming very popular in smart grid
for their ecological benefits, with higher scalability and utilization for
local generation and delivery. PV can also potentially avoid the energy losses
that are normally associated with long-range grid distribution. The increased
penetration of solar panels, however, has introduced a need for solar energy
models that are capable of producing realistic synthetic data with small error
margins. Such models, for instance, can be used to design the appropriate size
of energy storage devices or to determine the maximum charging rate of a
PV-powered electric vehicle (EV) charging station. In this regard, this paper
proposes a stochastic model for solar generation using a Markov chain approach.
Based on real data, it is first shown that the solar states are
inter-dependent, and thus suitable for modeling using a Markov model. Then, the
probabilities of transition between states are shown to be heterogeneous over
different time segments. A model is proposed that captures the inter temporal
dependency of solar irradiance through segmentation of the Markov chain across
different times of the day. In the studied model, different state transition
matrices are constructed for different time segments, which the proposed
algorithm then uses to generate the solar states for different times of the
day. Numerical examples are provided to show the effectiveness of the proposed
synthetic generator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4881</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4881</id><created>2014-07-17</created><authors><author><keyname>Meade</keyname><forenames>Bernard F</forenames></author><author><keyname>Fluke</keyname><forenames>Christopher J</forenames></author><author><keyname>Manos</keyname><forenames>Steven</forenames></author><author><keyname>Sinnott</keyname><forenames>Richard O</forenames></author></authors><title>Are tiled display walls needed for astronomy?</title><categories>astro-ph.IM cs.HC</categories><comments>19 pages, 15 figures, accepted for publication in PASA (Publications
  of the Astronomical Society of Australia)</comments><doi>10.1017/pasa.2014.29</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Clustering commodity displays into a Tiled Display Wall (TDW) provides a
cost-effective way to create an extremely high resolution display, capable of
approaching the image sizes now gen- erated by modern astronomical instruments.
Astronomers face the challenge of inspecting single large images, many similar
images simultaneously, and heterogeneous but related content. Many research
institutions have constructed TDWs on the basis that they will improve the
scientific outcomes of astronomical imagery. We test this concept by presenting
sample images to astronomers and non- astronomers using a standard desktop
display (SDD) and a TDW. These samples include standard English words, wide
field galaxy surveys and nebulae mosaics from the Hubble telescope. These
experiments show that TDWs provide a better environment for searching for small
targets in large images than SDDs. It also shows that astronomers tend to be
better at searching images for targets than non-astronomers, both groups are
generally better when employing physical navigation as opposed to virtual
navigation, and that the combination of two non-astronomers using a TDW rivals
the experience of a single astronomer. However, there is also a large
distribution in aptitude amongst the participants and the nature of the content
also plays a significant role is success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4884</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4884</id><created>2014-07-18</created><authors><author><keyname>Peng</keyname><forenames>Jie</forenames></author><author><keyname>Tan</keyname><forenames>Chik How</forenames></author><author><keyname>Wang</keyname><forenames>Qichun</forenames></author></authors><title>A new construction of differentially 4-uniform permutations over
  $F_{2^{2k}}$</title><categories>cs.IT math.IT</categories><comments>18pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Permutations over $F_{2^{2k}}$ with low differential uniform, high algebraic
degree and high nonlinearity are of great cryptographical importance since they
can be chosen as the substitution boxes (S-boxes) for many block ciphers. A
well known example is that the Advanced Encryption Standard (AES) chooses a
differentially 4-uniform permutation, the multiplicative inverse function, as
its S-box. In this paper, we present a new construction of differentially
4-uniformity permutations over even characteristic finite fields and obtain
many new CCZ-inequivalent functions. All the functions are switching neighbors
in the narrow sense of the multiplicative inverse function and have the optimal
algebraic degree and high nonlinearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4885</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4885</id><created>2014-07-18</created><updated>2014-07-30</updated><authors><author><keyname>de Montjoye</keyname><forenames>Yves-Alexandre</forenames></author><author><keyname>Smoreda</keyname><forenames>Zbigniew</forenames></author><author><keyname>Trinquart</keyname><forenames>Romain</forenames></author><author><keyname>Ziemlicki</keyname><forenames>Cezary</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author></authors><title>D4D-Senegal: The Second Mobile Phone Data for Development Challenge</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The D4D-Senegal challenge is an open innovation data challenge on anonymous
call patterns of Orange's mobile phone users in Senegal. The goal of the
challenge is to help address society development questions in novel ways by
contributing to the socio-economic development and well-being of the Senegalese
population. Participants to the challenge are given access to three mobile
phone datasets. This paper describes the three datasets. The datasets are based
on Call Detail Records (CDR) of phone calls and text exchanges between more
than 9 million of Orange's customers in Senegal between January 1, 2013 to
December 31, 2013. The datasets are: (1) antenna-to-antenna traffic for 1666
antennas on an hourly basis, (2) fine-grained mobility data on a rolling 2-week
basis for a year with bandicoot behavioral indicators at individual level for
about 300,000 randomly sampled users, (3) one year of coarse-grained mobility
data at arrondissement level with bandicoot behavioral indicators at individual
level for about 150,000 randomly sampled users
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4896</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4896</id><created>2014-07-18</created><updated>2014-09-17</updated><authors><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin I. M.</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Spatial patterns of close relationships across the lifespan</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>9 pages, 7 figures</comments><journal-ref>Scientific Reports 4, 6988 (2014)</journal-ref><doi>10.1038/srep06988</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of close relationships is important for understanding the
migration patterns of individual life-courses. The bottom-up approach to this
subject by social scientists has been limited by sample size, while the more
recent top-down approach using large-scale datasets suffers from a lack of
detail about the human individuals. We incorporate the geographic and
demographic information of millions of mobile phone users with their
communication patterns to study the dynamics of close relationships and its
effect in their life-course migration. We demonstrate how the close age- and
sex-biased dyadic relationships are correlated with the geographic proximity of
the pair of individuals, e.g., young couples tend to live further from each
other than old couples. In addition, we find that emotionally closer pairs are
living geographically closer to each other. These findings imply that the
life-course framework is crucial for understanding the complex dynamics of
close relationships and their effect on the migration patterns of human
individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4898</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4898</id><created>2014-07-18</created><authors><author><keyname>Tofighi</keyname><forenames>Ghassem</forenames></author><author><keyname>Afarin</keyname><forenames>Nasser Ali</forenames></author><author><keyname>Raahemifar</keyname><forenames>Kamraan</forenames></author><author><keyname>Venetsanopoulos</keyname><forenames>Anastasios N.</forenames></author></authors><title>Hand Pointing Detection Using Live Histogram Template of Forehead Skin</title><categories>cs.CV</categories><comments>Accepted for oral presentation in DSP2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand pointing detection has multiple applications in many fields such as
virtual reality and control devices in smart homes. In this paper, we proposed
a novel approach to detect pointing vector in 2D space of a room. After
background subtraction, face and forehead is detected. In the second step,
forehead skin H-S plane histograms in HSV space is calculated. By using these
histogram templates of users skin, and back projection method, skin areas are
detected. The contours of hand are extracted using Freeman chain code
algorithm. Next step is finding fingertips. Points in hand contour which are
candidates for the fingertip can be found in convex defects of convex hull and
contour. We introduced a novel method for finding the fingertip based on the
special points on the contour and their relationships. Our approach detects
hand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4903</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4903</id><created>2014-07-18</created><authors><author><keyname>Wang</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Ma</keyname><forenames>Yutao</forenames></author></authors><title>An Analysis of Research in Software Engineering: Assessment and Trends</title><categories>cs.SE</categories><comments>25 pages, 10 figures, 3 tables</comments><msc-class>68N01</msc-class><acm-class>A.1; D.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Glass published the first report on the assessment of systems and software
engineering scholars and institutions two decades ago. The ongoing, annual
survey of publications in this field provides fund managers, young scholars,
graduate students, etc. with useful information for different purposes.
However, the studies have been questioned by some critics because of a few
shortcomings of the evaluation method. It is actually very hard to reach a
widely recognized consensus on such an assessment of scholars and institutions.
This paper presents a module and automated method for assessment and trends
analysis in software engineering compared with the prior studies. To achieve a
more reasonable evaluation result, we take into consideration more high-quality
publications, the rank of each publication analyzed, and the different roles of
authors named on each paper in question. According to the 7638 papers published
in 36 publications from 2008 to 2013, the statistics of research subjects
roughly follow power laws, implying the interesting Matthew Effect. We then
identify the Top 20 scholars, institutions and countries or regions in terms of
a new evaluation rule based on the frequently-used one. The top-ranked scholar
is Mark Harman of the University College London, UK, the top-ranked institution
is the University of California, USA, and the top-ranked country is the USA.
Besides, we also show two levels of trend changes based on the EI
classification system and user-defined uncontrolled keywords, as well as
noteworthy scholars and institutions in a specific research area. We believe
that our results would provide a valuable insight for young scholars and
graduate students to seek possible potential collaborators and grasp the
popular research topics in software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4908</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4908</id><created>2014-07-18</created><authors><author><keyname>Oancea</keyname><forenames>Bogdan</forenames></author><author><keyname>Dragoescu</keyname><forenames>Raluca Mariana</forenames></author></authors><title>Integrating R and Hadoop for Big Data Analysis</title><categories>cs.DC</categories><comments>Romanian Statistical Review no. 2 / 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Analyzing and working with big data could be very diffi cult using classical
means like relational database management systems or desktop software packages
for statistics and visualization. Instead, big data requires large clusters
with hundreds or even thousands of computing nodes. Offi cial statistics is
increasingly considering big data for deriving new statistics because big data
sources could produce more relevant and timely statistics than traditional
sources. One of the software tools successfully and wide spread used for
storage and processing of big data sets on clusters of commodity hardware is
Hadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS),
a resource-management platform and implements a version of the MapReduce
programming model for large scale data processing. In this paper we investigate
the possibilities of integrating Hadoop with R which is a popular software used
for statistical computing and data visualization. We present three ways of
integrating them: R with Streaming, Rhipe and RHadoop and we emphasize the
advantages and disadvantages of each solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4917</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4917</id><created>2014-07-18</created><authors><author><keyname>Kumar</keyname><forenames>Shrawan</forenames></author><author><keyname>Sanyal</keyname><forenames>Amitabha</forenames></author><author><keyname>Khedker</keyname><forenames>Uday</forenames></author></authors><title>Sliced Slices: Separating Data and Control Influences</title><categories>cs.SE cs.PL</categories><comments>10 pages, 5 figures, two algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backward slicing has been used extensively in program understanding,
debugging and scaling up of program analysis. For large programs, the size of
the conventional backward slice is about 25% of the program size. This may be
too large to be useful. Our investigations reveal that in general, the size of
a slice is influenced more by computations governing the control flow reaching
the slicing criterion than by the computations governing the values relevant to
the slicing criterion. We distinguish between the two by defining data slices
and control slices both of which are smaller than the conventional slices which
can be obtained by combining the two. This is useful because for many
applications, the individual data or control slices are sufficient.
  Our experiments show that for more than 50% of cases, the data slice is
smaller than 10% of the program in size. Besides, the time to compute data or
control slice is comparable to that for computing the conventional slice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4923</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4923</id><created>2014-07-18</created><authors><author><keyname>Diao</keyname><forenames>Wenrui</forenames></author><author><keyname>Liu</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zhou</keyname><forenames>Zhe</forenames></author><author><keyname>Zhang</keyname><forenames>Kehuan</forenames></author></authors><title>Your Voice Assistant is Mine: How to Abuse Speakers to Steal Information
  and Control Your Phone</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous research about sensor based attacks on Android platform focused
mainly on accessing or controlling over sensitive device components, such as
camera, microphone and GPS. These approaches get data from sensors directly and
need corresponding sensor invoking permissions.
  This paper presents a novel approach (GVS-Attack) to launch permission
bypassing attacks from a zero permission Android application (VoicEmployer)
through the speaker. The idea of GVS-Attack utilizes an Android system built-in
voice assistant module -- Google Voice Search. Through Android Intent
mechanism, VoicEmployer triggers Google Voice Search to the foreground, and
then plays prepared audio files (like &quot;call number 1234 5678&quot;) in the
background. Google Voice Search can recognize this voice command and execute
corresponding operations. With ingenious designs, our GVS-Attack can forge
SMS/Email, access privacy information, transmit sensitive data and achieve
remote control without any permission.
  Also we found a vulnerability of status checking in Google Search app, which
can be utilized by GVS-Attack to dial arbitrary numbers even when the phone is
securely locked with password. A prototype of VoicEmployer has been implemented
to demonstrate the feasibility of GVS-Attack in real world. In theory, nearly
all Android devices equipped with Google Services Framework can be affected by
GVS-Attack. This study may inspire application developers and researchers
rethink that zero permission doesn't mean safety and the speaker can be treated
as a new attack surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4937</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4937</id><created>2014-07-18</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Ehlers</keyname><forenames>R&#xfc;diger</forenames><affiliation>University of Bremen</affiliation></author><author><keyname>Jha</keyname><forenames>Susmit</forenames><affiliation>Intel Strategic CAD Lab</affiliation></author></authors><title>Proceedings 3rd Workshop on Synthesis</title><categories>cs.LO cs.FL cs.SE cs.SY</categories><proxy>EPTCS</proxy><acm-class>B.1.2; D.2.2; F.1.1; F.1.2; I.2.2</acm-class><journal-ref>EPTCS 157, 2014</journal-ref><doi>10.4204/EPTCS.157</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of synthesis, i.e., the process of automatically computing
implementations from their specifications, has recently gained a lot of
momentum in the contexts of software engineering and reactive system design.
While it is widely believed that, due to complexity/undecidability issues,
synthesis cannot completely replace manual engineering, it can assist the
process of designing the intricate pieces of code that most programmers find
challenging, or help with orchestrating tasks in reactive environments. The
SYNT workshop aims to bring together researchers interested in synthesis to
discuss and present ongoing and mature work on all aspects of automated
synthesis and its applications.
  The third iteration of the workshop took place in Vienna, Austria, and was
co-located with the 26th International Conference on Computer Aided
Verification, held in the context of the Vienna Summer of Logic in July 2014.
The workshop included eight contributed talks and four invited talks. In
addition, it featured a special session about the Syntax-Guided Synthesis
Competition (SyGuS) and the SyntComp Synthesis competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4945</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4945</id><created>2014-07-18</created><authors><author><keyname>Li</keyname><forenames>Yongkun</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Friends or Foes: Distributed and Randomized Algorithms to Determine
  Dishonest Recommenders in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Viral marketing is becoming important due to the popularity of online social
networks (OSNs). Companies may provide incentives (e.g., via free samples of a
product) to a small group of users in an OSN, and these users provide
recommendations to their friends, which eventually increases the overall sales
of a given product. Nevertheless, this also opens a door for &quot;malicious
behaviors&quot;: dishonest users may intentionally give misleading recommendations
to their friends so as to distort the normal sales distribution. In this paper,
we propose a detection framework to identify dishonest users in OSNs. In
particular, we present a set of fully distributed and randomized algorithms,
and also quantify the performance of the algorithms by deriving probability of
false positive, probability of false negative, and the distribution of number
of detection rounds. Extensive simulations are also carried out to illustrate
the impact of misleading recommendations and the effectiveness of our detection
algorithms. The methodology we present here will enhance the security level of
viral marketing in OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4958</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4958</id><created>2014-07-18</created><authors><author><keyname>Westerlund</keyname><forenames>Stefan</forenames></author><author><keyname>Harris</keyname><forenames>Christopher</forenames></author></authors><title>A Framework for HI Spectral Source Finding Using Distributed-Memory
  Supercomputing</title><categories>astro-ph.IM cs.DC</categories><comments>15 pages, 6 figures</comments><journal-ref>Publications of the Astronomical Society of Australia, 2014,
  Volume 31</journal-ref><doi>10.1017/pasa.2014.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The latest generation of radio astronomy interferometers will conduct all sky
surveys with data products consisting of petabytes of spectral line data.
Traditional approaches to identifying and parameterising the astrophysical
sources within this data will not scale to datasets of this magnitude, since
the performance of workstations will not keep up with the real-time generation
of data. For this reason, it is necessary to employ high performance computing
systems consisting of a large number of processors connected by a
high-bandwidth network. In order to make use of such supercomputers substantial
modifications must be made to serial source finding code. To ease the
transition, this work presents the Scalable Source Finder Framework, a
framework providing storage access, networking communication and data
composition functionality, which can support a wide range of source finding
algorithms provided they can be applied to subsets of the entire image.
Additionally, the Parallel Gaussian Source Finder was implemented using SSoFF,
utilising Gaussian filters, thresholding, and local statistics. PGSF was able
to search on a 256GB simulated dataset in under 24 minutes, significantly less
than the 8 to 12 hour observation that would generate such a dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4972</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4972</id><created>2014-07-18</created><authors><author><keyname>Borassi</keyname><forenames>Michele</forenames></author><author><keyname>Crescenzi</keyname><forenames>Pierluigi</forenames></author><author><keyname>Habib</keyname><forenames>Michel</forenames></author></authors><title>Into the Square - On the Complexity of Quadratic-Time Solvable Problems</title><categories>cs.CC</categories><comments>Submitted at SODA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper will analyze several quadratic-time solvable problems, and will
classify them into two classes: problems that are solvable in truly
subquadratic time (that is, in time $O(n^{2-\epsilon})$ for some $\epsilon&gt;0$)
and problems that are not, unless the well known Strong Exponential Time
Hypothesis (SETH) is false. In particular, we will prove that some
quadratic-time solvable problems are indeed easier than expected. We will
provide an algorithm that computes the transitive closure of a directed graph
in time $O(mn^{\frac{\omega+1}{4}})$, where $m$ denotes the number of edges in
the transitive closure and $\omega$ is the exponent for matrix multiplication.
As a side effect, we will prove that our algorithm runs in time
$O(n^{\frac{5}{3}})$ if the transitive closure is sparse. The same time bounds
hold if we want to check whether a graph is transitive, by replacing m with the
number of edges in the graph itself. As far as we know, this is the fastest
algorithm for sparse transitive digraph recognition. Finally, we will apply our
algorithm to the comparability graph recognition problem (dating back to 1941),
obtaining the first truly subquadratic algorithm. The second part of the paper
deals with hardness results. Starting from an artificial quadratic-time
solvable variation of the k-SAT problem, we will construct a graph of Karp
reductions, proving that a truly subquadratic-time algorithm for any of the
problems in the graph falsifies SETH. The analyzed problems are the following:
computing the subset graph, finding dominating sets, computing the betweenness
centrality of a vertex, computing the minimum closeness centrality, and
computing the hyperbolicity of a pair of vertices. We will also be able to
include in our framework three proofs already appeared in the literature,
concerning the graph diameter computation, local alignment of strings and
orthogonality of vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4979</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4979</id><created>2014-07-18</created><authors><author><keyname>Yi</keyname><forenames>Dong</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Deep Metric Learning for Practical Person Re-Identification</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various hand-crafted features and metric learning methods prevail in the
field of person re-identification. Compared to these methods, this paper
proposes a more general way that can learn a similarity metric from image
pixels directly. By using a &quot;siamese&quot; deep neural network, the proposed method
can jointly learn the color feature, texture feature and metric in a unified
framework. The network has a symmetry structure with two sub-networks which are
connected by Cosine function. To deal with the big variations of person images,
binomial deviance is used to evaluate the cost between similarities and labels,
which is proved to be robust to outliers.
  Compared to existing researches, a more practical setting is studied in the
experiments that is training and test on different datasets (cross dataset
person re-identification). Both in &quot;intra dataset&quot; and &quot;cross dataset&quot;
settings, the superiorities of the proposed method are illustrated on VIPeR and
PRID.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4989</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4989</id><created>2014-07-14</created><authors><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Liu</keyname><forenames>Weichu</forenames></author><author><keyname>Murata</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Wakita</keyname><forenames>Ken</forenames></author></authors><title>A framework for community detection in heterogeneous multi-relational
  networks</title><categories>cs.SI physics.soc-ph</categories><comments>27 pages, 10 figures</comments><journal-ref>Advances in Complex Systems, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a surge of interest in community detection in homogeneous
single-relational networks which contain only one type of nodes and edges.
However, many real-world systems are naturally described as heterogeneous
multi-relational networks which contain multiple types of nodes and edges. In
this paper, we propose a new method for detecting communities in such networks.
Our method is based on optimizing the composite modularity, which is a new
modularity proposed for evaluating partitions of a heterogeneous
multi-relational network into communities. Our method is parameter-free,
scalable, and suitable for various networks with general structure. We
demonstrate that it outperforms the state-of-the-art techniques in detecting
pre-planted communities in synthetic networks. Applied to a real-world Digg
network, it successfully detects meaningful communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4990</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4990</id><created>2014-07-14</created><authors><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Murata</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Wakita</keyname><forenames>Ken</forenames></author></authors><title>Detecting network communities beyond assortativity-related attributes</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 8 figures</comments><journal-ref>Physical Review E 90, 012806 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network science, assortativity refers to the tendency of links to exist
between nodes with similar attributes. In social networks, for example, links
tend to exist between individuals of similar age, nationality, location, race,
income, educational level, religious belief, and language. Thus, various
attributes jointly affect the network topology. An interesting problem is to
detect community structure beyond some specific assortativity-related
attributes $\rho$, i.e., to take out the effect of $\rho$ on network topology
and reveal the hidden community structure which are due to other attributes. An
approach to this problem is to redefine the null model of the modularity
measure, so as to simulate the effect of $\rho$ on network topology. However, a
challenge is that we do not know to what extent the network topology is
affected by $\rho$ and by other attributes. In this paper, we propose
Dist-Modularity which allows us to freely choose any suitable function to
simulate the effect of $\rho$. Such freedom can help us probe the effect of
$\rho$ and detect the hidden communities which are due to other attributes. We
test the effectiveness of Dist-Modularity on synthetic benchmarks and two
real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4992</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4992</id><created>2014-07-18</created><updated>2014-09-30</updated><authors><author><keyname>Bergmann</keyname><forenames>Frank T.</forenames></author><author><keyname>Adams</keyname><forenames>Richard</forenames></author><author><keyname>Moodie</keyname><forenames>Stuart</forenames></author><author><keyname>Cooper</keyname><forenames>Jonathan</forenames></author><author><keyname>Glont</keyname><forenames>Mihai</forenames></author><author><keyname>Golebiewski</keyname><forenames>Martin</forenames></author><author><keyname>Hucka</keyname><forenames>Michael</forenames></author><author><keyname>Laibe</keyname><forenames>Camille</forenames></author><author><keyname>Miller</keyname><forenames>Andrew K.</forenames></author><author><keyname>Nickerson</keyname><forenames>David P.</forenames></author><author><keyname>Olivier</keyname><forenames>Brett G.</forenames></author><author><keyname>Rodriguez</keyname><forenames>Nicolas</forenames></author><author><keyname>Sauro</keyname><forenames>Herbert M.</forenames></author><author><keyname>Scharm</keyname><forenames>Martin</forenames></author><author><keyname>Soiland-Reyes</keyname><forenames>Stian</forenames></author><author><keyname>Waltemath</keyname><forenames>Dagmar</forenames></author><author><keyname>Yvon</keyname><forenames>Florent</forenames></author><author><keyname>Nov&#xe8;re</keyname><forenames>Nicolas Le</forenames></author></authors><title>One file to share them all: Using the COMBINE Archive and the OMEX
  format to share all information about a modeling project</title><categories>cs.DL q-bio.MN</categories><comments>3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: With the ever increasing use of computational models in the
biosciences, the need to share models and reproduce the results of published
studies efficiently and easily is becoming more important. To this end, various
standards have been proposed that can be used to describe models, simulations,
data or other essential information in a consistent fashion. These constitute
various separate components required to reproduce a given published scientific
result.
  Results: We describe the Open Modeling EXchange format (OMEX). Together with
the use of other standard formats from the Computational Modeling in Biology
Network (COMBINE), OMEX is the basis of the COMBINE Archive, a single file that
supports the exchange of all the information necessary for a modeling and
simulation experiment in biology. An OMEX file is a ZIP container that includes
a manifest file, listing the content of the archive, an optional metadata file
adding information about the archive and its content, and the files describing
the model. The content of a COMBINE Archive consists of files encoded in
COMBINE standards whenever possible, but may include additional files defined
by an Internet Media Type. Several tools that support the COMBINE Archive are
available, either as independent libraries or embedded in modeling software.
  Conclusions: The COMBINE Archive facilitates the reproduction of modeling and
simulation experiments in biology by embedding all the relevant information in
one file. Having all the information stored and exchanged at once also helps in
building activity logs and audit trails. We anticipate that the COMBINE Archive
will become a significant help for modellers, as the domain moves to larger,
more complex experiments such as multi-scale models of organs, digital
organisms, and bioengineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.4999</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.4999</id><created>2014-07-18</created><authors><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author><author><keyname>Kir&#xe1;ly</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>On the tractability of some natural packing, covering and partitioning
  problems</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we fix 7 types of undirected graphs: paths, paths with
prescribed endvertices, circuits, forests, spanning trees, (not necessarily
spanning) trees and cuts. Given an undirected graph $G=(V,E)$ and two &quot;object
types&quot; $\mathrm{A}$ and $\mathrm{B}$ chosen from the alternatives above, we
consider the following questions. \textbf{Packing problem:} can we find an
object of type $\mathrm{A}$ and one of type $\mathrm{B}$ in the edge set $E$ of
$G$, so that they are edge-disjoint? \textbf{Partitioning problem:} can we
partition $E$ into an object of type $\mathrm{A}$ and one of type $\mathrm{B}$?
\textbf{Covering problem:} can we cover $E$ with an object of type
$\mathrm{A}$, and an object of type $\mathrm{B}$? This framework includes 44
natural graph theoretic questions. Some of these problems were well-known
before, for example covering the edge-set of a graph with two spanning trees,
or finding an $s$-$t$ path $P$ and an $s'$-$t'$ path $P'$ that are
edge-disjoint. However, many others were not, for example can we find an
$s$-$t$ path $P\subseteq E $ and a spanning tree $T\subseteq E$ that are
edge-disjoint? Most of these previously unknown problems turned out to be
NP-complete, many of them even in planar graphs. This paper determines the
status of these 44 problems. For the NP-complete problems we also investigate
the planar version, for the polynomial problems we consider the matroidal
generalization (wherever this makes sense).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5008</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5008</id><created>2014-07-17</created><authors><author><keyname>Chakravorty</keyname><forenames>Anurag A.</forenames></author><author><keyname>Suryawanshi</keyname><forenames>Raghwendra J.</forenames></author></authors><title>Data Transfer between Two USB Flash SCSI Disks using a Touch Screen</title><categories>cs.OH</categories><comments>5 pages, 7 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>IJETT, V13(2),71-75 July 2014</journal-ref><doi>10.14445/22315381/IJETT-V13P215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under normal circumstances, as an intermediate device, if we want to move or
copy data from one mass storage device to another, we use a computer in the
form of desktops, laptops, etc. We need a device which can be used as an
intermediate device, also which is a complete blend of hardware &amp; software.
This device is a gadget that can be used to transfer data between two flash
SCSI devices via a touch screen. This is a user friendly device which uses the
most popular bus USB (Universal Serial Bus) with Type-A connector. It is
governed by the USB 2.0 Protocol. One of the major advantage of this device is
its portability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5011</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5011</id><created>2014-07-18</created><updated>2015-04-26</updated><authors><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>Rotbart</keyname><forenames>Noy</forenames></author></authors><title>A simple and optimal ancestry labeling scheme for trees</title><categories>cs.DS</categories><comments>12 pages, 1 figure. To appear at ICALP'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a $\lg n + 2 \lg \lg n+3$ ancestry labeling scheme for trees. The
problem was first presented by Kannan et al. [STOC 88'] along with a simple $2
\lg n$ solution. Motivated by applications to XML files, the label size was
improved incrementally over the course of more than 20 years by a series of
papers. The last, due to Fraigniaud and Korman [STOC 10'], presented an
asymptotically optimal $\lg n + 4 \lg \lg n+O(1)$ labeling scheme using
non-trivial tree-decomposition techniques. By providing a framework
generalizing interval based labeling schemes, we obtain a simple, yet
asymptotically optimal solution to the problem. Furthermore, our labeling
scheme is attained by a small modification of the original $2 \lg n$ solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5030</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5030</id><created>2014-07-18</created><updated>2015-07-14</updated><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Haddad</keyname><forenames>Axel</forenames></author><author><keyname>Monmege</keyname><forenames>Benjamin</forenames></author></authors><title>To Reach or not to Reach? Efficient Algorithms for Total-Payoff Games</title><categories>cs.GT</categories><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative games are two-player zero-sum games played on directed weighted
graphs. Total-payoff games (that can be seen as a refinement of the
well-studied mean-payoff games) are the variant where the payoff of a play is
computed as the sum of the weights. Our aim is to describe the first
pseudo-polynomial time algorithm for total-payoff games in the presence of
arbitrary weights. It consists of a non-trivial application of the value
iteration paradigm. Indeed, it requires to study, as a milestone, a refinement
of these games, called min-cost reachability games, where we add a reachability
objective to one of the players. For these games, we give an efficient value
iteration algorithm to compute the values and optimal strategies (when they
exist), that runs in pseudo-polynomial time. We also propose heuristics
allowing one to possibly speed up the computations in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5032</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5032</id><created>2014-07-18</created><updated>2014-09-10</updated><authors><author><keyname>Zheng</keyname><forenames>Weiye</forenames></author><author><keyname>Wu</keyname><forenames>Wenchuan</forenames></author><author><keyname>Zhang</keyname><forenames>Boming</forenames></author><author><keyname>Sun</keyname><forenames>Hongbin</forenames></author><author><keyname>Yibing</keyname><forenames>Liu</forenames></author></authors><title>A Fully Distributed Reactive Power Optimization and Control Method for
  Active Distribution Networks</title><categories>math.OC cs.DC</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equations 11 and 26. Also, in P1, active powers have been optimized,
  which is not suitable. Some crucial assumptions about DGs are not explicitly
  addressed, either</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a fully distributed reactive power optimization algorithm
that can obtain the global optimum of non-convex problems for distribution
networks without a central coordinator. Second-order cone (SOC) relaxation is
used to achieve exact convexification. A fully distributed algorithm is then
formulated corresponding to the given division of areas based on an alternating
direction method of multipliers (ADMM) algorithm, which is greatly simplified
by exploiting the structure of active distribution networks (ADNs). The problem
is solved for each area with very little interchange of boundary information
between neighboring areas. The standard ADMM algorithm is extended using a
varying penalty parameter to improve convergence. The validity of the method is
demonstrated via numerical simulations on an IEEE 33-node distribution network,
a PG&amp;E 69-node distribution system, and an extended 137-node system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5035</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5035</id><created>2014-07-18</created><updated>2014-10-31</updated><authors><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Guadarrama</keyname><forenames>Sergio</forenames></author><author><keyname>Tzeng</keyname><forenames>Eric</forenames></author><author><keyname>Hu</keyname><forenames>Ronghang</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>LSDA: Large Scale Detection Through Adaptation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge in scaling object detection is the difficulty of obtaining
labeled images for large numbers of categories. Recently, deep convolutional
neural networks (CNNs) have emerged as clear winners on object classification
benchmarks, in part due to training with 1.2M+ labeled classification images.
Unfortunately, only a small fraction of those labels are available for the
detection task. It is much cheaper and easier to collect large quantities of
image-level labels from search engines than it is to collect detection data and
label it with precise bounding boxes. In this paper, we propose Large Scale
Detection through Adaptation (LSDA), an algorithm which learns the difference
between the two tasks and transfers this knowledge to classifiers for
categories without bounding box annotated data, turning them into detectors.
Our method has the potential to enable detection for the tens of thousands of
categories that lack bounding box annotations, yet have plenty of
classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge
demonstrates the efficacy of our approach. This algorithm enables us to produce
a &gt;7.6K detector by using available classification data from leaf nodes in the
ImageNet tree. We additionally demonstrate how to modify our architecture to
produce a fast detector (running at 2fps for the 7.6K detector). Models and
software are available at
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5040</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5040</id><created>2014-07-17</created><authors><author><keyname>Guo</keyname><forenames>Hongzhi</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author></authors><title>Channel Modeling for Metamaterial-Enhanced Underground Wireless
  Communications</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication is the prerequisite for the highly desired in-situ and
real-time monitoring capability in underground environments, including oil
reservoirs, groundwater aquifers, volcanos, among others. However, existing
wireless communication techniques do not work in such environments due to the
harsh transmission medium with very high material absorption and the
inaccessible nature of underground environment that requires extremely small
device size. Although Magnetic Induction (MI) communication has been shown to
be a promising technique in underground environments, the existing MI system
utilizes very large coil antennas, which are not suitable for deployment in
underground. In this paper, we propose a metamaterial enhanced magnetic
induction communication mechanism that can achieve over meter scale
communication range by using millimeter scale coil antennas in the harsh
underground environment. An analytical channel model for the new mechanism is
developed to explore the fundamentals of metamaterial enhanced MI communication
in various underground environments. The effects of important system and
environmental factors are quantitatively captured, including the operating
frequency, bandwidth, and parameters of metamaterial antennas, as well as
permittivity, permeability, and conductivity of underground medium. The
theoretical model is validated through the finite element simulation software,
COMSOL Multiphysics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5042</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5042</id><created>2014-07-18</created><authors><author><keyname>Tammelin</keyname><forenames>Oskari</forenames></author></authors><title>Solving Large Imperfect Information Games Using CFR+</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterfactual Regret Minimization and variants (e.g. Public Chance Sampling
CFR and Pure CFR) have been known as the best approaches for creating
approximate Nash equilibrium solutions for imperfect information games such as
poker. This paper introduces CFR$^+$, a new algorithm that typically
outperforms the previously known algorithms by an order of magnitude or more in
terms of computation time while also potentially requiring less memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5055</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5055</id><created>2014-06-30</created><updated>2014-11-03</updated><authors><author><keyname>Luo</keyname><forenames>Enming</forenames></author><author><keyname>Chan</keyname><forenames>Stanley H.</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Q.</forenames></author></authors><title>Adaptive Image Denoising by Targeted Databases</title><categories>cs.CV stat.ME</categories><comments>15 pages, 13 figures, 2 tables, journal</comments><doi>10.1109/TIP.2015.2414873</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a data-dependent denoising procedure to restore noisy images.
Different from existing denoising algorithms which search for patches from
either the noisy image or a generic database, the new algorithm finds patches
from a database that contains only relevant patches. We formulate the denoising
problem as an optimal filter design problem and make two contributions. First,
we determine the basis function of the denoising filter by solving a group
sparsity minimization problem. The optimization formulation generalizes
existing denoising algorithms and offers systematic analysis of the
performance. Improvement methods are proposed to enhance the patch search
process. Second, we determine the spectral coefficients of the denoising filter
by considering a localized Bayesian prior. The localized prior leverages the
similarity of the targeted database, alleviates the intensive Bayesian
computation, and links the new method to the classical linear minimum mean
squared error estimation. We demonstrate applications of the proposed method in
a variety of scenarios, including text images, multiview images and face
images. Experimental results show the superiority of the new algorithm over
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5068</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5068</id><created>2014-07-18</created><updated>2015-08-10</updated><authors><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Checking Whether an Automaton Is Monotonic Is NP-complete</title><categories>cs.FL</categories><comments>13 pages, 4 figures. CIAA 2015. The final publication is available at
  http://link.springer.com/chapter/10.1007/978-3-319-22360-5_23</comments><journal-ref>In Implementation and Application of Automata, volume 9223 of
  LNCS, pages 279-291, Springer, 2015</journal-ref><doi>10.1007/978-3-319-22360-5_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automaton is monotonic if its states can be arranged in a linear order
that is preserved by the action of every letter. We prove that the problem of
deciding whether a given automaton is monotonic is NP-complete. The same result
is obtained for oriented automata, whose states can be arranged in a cyclic
order. Moreover, both problems remain hard under the restriction to binary
input alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5074</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5074</id><created>2014-07-14</created><updated>2015-12-10</updated><authors><author><keyname>Mirzaei</keyname><forenames>Saber</forenames></author><author><keyname>Esposito</keyname><forenames>Flavio</forenames></author></authors><title>An Alloy Verification Model for Consensus-Based Auction Protocols</title><categories>cs.SE cs.DC</categories><doi>10.1109/ICDCSW.2015.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max Consensus-based Auction (MCA) protocols are an elegant approach to
establish conflict-free distributed allocations in a wide range of network
utility maximization problems. A set of agents independently bid on a set of
items, and exchange their bids with their first hop-neighbors for a distributed
(max-consensus) winner determination. The use of MCA protocols was proposed,
$e.g.$, to solve the task allocation problem for a fleet of unmanned aerial
vehicles, in smart grids, or in distributed virtual network management
applications. Misconfigured or malicious agents participating in a MCA, or an
incorrect instantiation of policies can lead to oscillations of the protocol,
causing, $e.g.$, Service Level Agreement (SLA) violations.
  In this paper, we propose a formal, machine-readable, Max-Consensus Auction
model, encoded in the Alloy lightweight modeling language. The model consists
of a network of agents applying the MCA mechanisms, instantiated with
potentially different policies, and a set of predicates to analyze its
convergence properties. We were able to verify that MCA is not resilient
against rebidding attacks, and that the protocol fails (to achieve a
conflict-free resource allocation) for some specific combinations of policies.
Our model can be used to verify, with a &quot;push-button&quot; analysis, the convergence
of the MCA mechanism to a conflict-free allocation of a wide range of policy
instantiations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5075</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5075</id><created>2014-07-18</created><authors><author><keyname>Alon</keyname><forenames>Noga</forenames></author><author><keyname>Basavaraju</keyname><forenames>Manu</forenames></author><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Mathew</keyname><forenames>Rogers</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author></authors><title>Separation dimension of bounded degree graphs</title><categories>math.CO cs.DM</categories><comments>One result proved in this paper is also present in arXiv:1212.6756</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 'separation dimension' of a graph $G$ is the smallest natural number $k$
for which the vertices of $G$ can be embedded in $\mathbb{R}^k$ such that any
pair of disjoint edges in $G$ can be separated by a hyperplane normal to one of
the axes. Equivalently, it is the smallest possible cardinality of a family
$\mathcal{F}$ of total orders of the vertices of $G$ such that for any two
disjoint edges of $G$, there exists at least one total order in $\mathcal{F}$
in which all the vertices in one edge precede those in the other. In general,
the maximum separation dimension of a graph on $n$ vertices is $\Theta(\log
n)$. In this article, we focus on bounded degree graphs and show that the
separation dimension of a graph with maximum degree $d$ is at most
$2^{9log^{\star} d} d$. We also demonstrate that the above bound is nearly
tight by showing that, for every $d$, almost all $d$-regular graphs have
separation dimension at least $\lceil d/2\rceil$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5080</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5080</id><created>2014-07-18</created><updated>2014-10-15</updated><authors><author><keyname>Sundar</keyname><forenames>Kaarthik</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author></authors><title>Multiple Depot Ring Star Problem: A polyhedral study and exact algorithm</title><categories>cs.DS cs.SY</categories><comments>Submitted to Optimization Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multiple Depot Ring-Star Problem (MDRSP) is an important combinatorial
optimization problem that arises in the context of optical fiber network
design, and in applications pertaining to collecting data using stationary
sensing devices and autonomous vehicles. Given the locations of a set of
customers and a set of depots, the goal is to (i) find a set of simple cycles
such that each cycle (ring) passes through a subset of customers and exactly
one depot, (ii) assign each non-visited customer to a visited customer or a
depot, and (iii) minimize the sum of the routing costs, i.e., the cost of the
cycles and the assignment costs. We present a mixed integer linear programming
formulation for the MDRSP and propose valid inequalities to strengthen the
linear programming relaxation. Furthermore, we present a polyhedral analysis
and derive facet-inducing results for the MDRSP. All these results are then
used to develop a branch-and-cut algorithm to obtain optimal solutions to the
MDRSP. The performance of the branch-and-cut algorithm is evaluated through
extensive computational experiments on several classes of test instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5083</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5083</id><created>2014-07-18</created><updated>2014-10-07</updated><authors><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author><author><keyname>Stein</keyname><forenames>Maya</forenames></author></authors><title>Partitioning two-coloured complete multipartite graphs into
  monochromatic paths and cycles</title><categories>math.CO cs.DM</categories><msc-class>05C38, 05C55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any complete $k$-partite graph $G$ on $n$ vertices, with $k \ge
3$, whose edges are two-coloured, can be covered with two vertex-disjoint
monochromatic paths of distinct colours. We prove this under the necessary
assumption that the largest partition class of $G$ contains at most $n/2$
vertices. This extends known results for complete and complete bipartite
graphs.
  Secondly, we show that in the same situation, all but $o(n)$ vertices of the
graph can be covered with two vertex-disjoint monochromatic cycles of distinct
colours, if colourings close to a split colouring are excluded. From this we
derive that the whole graph, if large enough, may be covered with 14
vertex-disjoint monochromatic cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5093</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5093</id><created>2014-07-18</created><authors><author><keyname>Horton</keyname><forenames>Michael</forenames></author><author><keyname>Gudmundsson</keyname><forenames>Joachim</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Estephan</keyname><forenames>Jo&#xeb;l</forenames></author></authors><title>Classification of Passes in Football Matches using Spatiotemporal Data</title><categories>cs.LG cs.CG</categories><comments>10 pages</comments><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A knowledgeable observer of a game of football (soccer) can make a subjective
evaluation of the quality of passes made between players during the game. We
investigate the problem of producing an automated system to make the same
evaluation of passes. We present a model that constructs numerical predictor
variables from spatiotemporal match data using feature functions based on
methods from computational geometry, and then learns a classification function
from labelled examples of the predictor variables. Furthermore, the learned
classifiers are analysed to determine if there is a relationship between the
complexity of the algorithm that computed the predictor variable and the
importance of the variable to the classifier. Experimental results show that we
are able to produce a classifier with 85.8% accuracy on classifying passes as
Good, OK or Bad, and that the predictor variables computed using complex
methods from computational geometry are of moderate importance to the learned
classifiers. Finally, we show that the inter-rater agreement on pass
classification between the machine classifier and a human observer is of
similar magnitude to the agreement between two observers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5104</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5104</id><created>2014-07-18</created><authors><author><keyname>Agrawal</keyname><forenames>Pulkit</forenames></author><author><keyname>Stansbury</keyname><forenames>Dustin</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author><author><keyname>Gallant</keyname><forenames>Jack L.</forenames></author></authors><title>Pixels to Voxels: Modeling Visual Representation in the Human Brain</title><categories>q-bio.NC cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human brain is adept at solving difficult high-level visual processing
problems such as image interpretation and object recognition in natural scenes.
Over the past few years neuroscientists have made remarkable progress in
understanding how the human brain represents categories of objects and actions
in natural scenes. However, all current models of high-level human vision
operate on hand annotated images in which the objects and actions have been
assigned semantic tags by a human operator. No current models can account for
high-level visual function directly in terms of low-level visual input (i.e.,
pixels). To overcome this fundamental limitation we sought to develop a new
class of models that can predict human brain activity directly from low-level
visual input (i.e., pixels). We explored two classes of models drawn from
computer vision and machine learning. The first class of models was based on
Fisher Vectors (FV) and the second was based on Convolutional Neural Networks
(ConvNets). We find that both classes of models accurately predict brain
activity in high-level visual areas, directly from pixels and without the need
for any semantic tags or hand annotation of images. This is the first time that
such a mapping has been obtained. The fit models provide a new platform for
exploring the functional principles of human vision, and they show that modern
methods of computer vision and machine learning provide important tools for
characterizing brain function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5107</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5107</id><created>2014-07-18</created><authors><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>PageRank beyond the Web</title><categories>cs.SI cs.CE cs.NA physics.soc-ph</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Google's PageRank method was developed to evaluate the importance of
web-pages via their link structure. The mathematics of PageRank, however, are
entirely general and apply to any graph or network in any domain. Thus,
PageRank is now regularly used in bibliometrics, social and information network
analysis, and for link prediction and recommendation. It's even used for
systems analysis of road networks, as well as biology, chemistry, neuroscience,
and physics. We'll see the mathematics and ideas that unite these diverse
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5111</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5111</id><created>2014-07-18</created><updated>2014-07-22</updated><authors><author><keyname>Ranjan</keyname><forenames>Harsh</forenames></author><author><keyname>Agarwal</keyname><forenames>Sumit</forenames></author><author><keyname>Singh</keyname><forenames>Niraj Kumar</forenames></author></authors><title>Design and Analysis of RS Sort</title><categories>cs.DS</categories><msc-class>68P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new comparison base stable sorting algorithm, named
RS sort. RS Sort involves only the comparison of pair of elements in an array
which ultimately sorts the array and does not involve the comparison of each
element with every other element. RS sort tries to build upon the relationship
established between the elements in each pass. Suppose there is an array
containing three elements a1, a2, a3 and if a relationship exist such that
a1&lt;a2 and a2&lt;a3 then it can be established that a1&lt;a3 and so there is no need
to compare a1 and a3. Sorting is a fundamental operation in computer science.
RS sort is analyzed both theoretically and empirically. We have performed its
Empirical analysis and compared its performance with the well-known quick sort
for various input types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5117</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5117</id><created>2014-07-18</created><updated>2014-09-02</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Smith</keyname><forenames>Arfon M.</forenames></author></authors><title>Implementing Transitive Credit with JSON-LD</title><categories>cs.CY cs.DL</categories><comments>accepted by WSSSPE2 - http://wssspe.researchcomputing.org.uk/wssspe2/</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Science and engineering research increasingly relies on activities that
facilitate research but are not currently rewarded or recognized, such as: data
sharing; developing common data resources, software and methodologies; and
annotating data and publications. To promote and advance these activities, we
must develop mechanisms for assigning credit, facilitate the appropriate
attribution of research outcomes, devise incentives for activities that
facilitate research, and allocate funds to maximize return on investment. In
this article, we focus on addressing the issue of assigning credit for both
direct and indirect contributions, specifically by using JSON-LD to implement a
prototype transitive credit system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5126</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5126</id><created>2014-07-18</created><authors><author><keyname>Tong</keyname><forenames>Guangmo</forenames></author><author><keyname>Liu</keyname><forenames>Cong</forenames></author></authors><title>Supporting Read/Write Applications in Embedded Real-time Systems via
  Suspension-aware Analysis</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many embedded real-time systems, applications often interact with I/O
devices via read/write operations, which may incur considerable suspension
delays. Unfortunately, prior analysis methods for validating timing correctness
in embedded systems become quite pessimistic when suspension delays are
present. In this paper, we consider the problem of supporting two common types
of I/O applications in a multiprocessor system, that is, write-only
applications and read-write applications. For the write-only application model,
we present a much improved analysis technique that results in only O(m)
suspension-related utilization loss, where m is the number of processors. For
the second application model, we present a flexible I/O placement strategy and
a corresponding new scheduling algorithm, which can completely circumvent the
negative impact due to read- and write-induced suspension delays. We illustrate
the feasibility of the proposed I/O-placement-based schedule via a case study
implementation. Furthermore, experiments presented herein show that the
improvement with respect to system utilization over prior methods is often
significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5128</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5128</id><created>2014-07-18</created><updated>2016-01-27</updated><authors><author><keyname>Gasarch</keyname><forenames>William</forenames></author></authors><title>A Sane Proof that COLk \le COL3</title><categories>cs.CC math.CO</categories><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let COLk be the set of all k-colorable graphs. It is easy to show that if a&lt;b
then COLa \le COLb (poly time reduction). Using the Cook-Levin theorem it is
easy to show that if 3 \le a&lt; b then COLb \le COLa. However this proof is
insane in that it translates a graph to a formula and then the formula to a
graph. We give a simple proof that COLk \le COL3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5136</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5136</id><created>2014-07-18</created><authors><author><keyname>Liu</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Rate-Compatible LDPC Codes Based on Puncturing and Extension Techniques
  for Short Block Lengths</title><categories>cs.IT math.IT</categories><comments>8 figures, 9 pages, AEU International Journal on Communications and
  Electronics, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate novel strategies for generating rate-compatible
(RC) irregular low-density parity-check (LDPC) codes with short/moderate block
lengths. We propose three puncturing and two extension schemes, which are
designed to determine the puncturing positions that minimize the performance
degradation and the extension that maximize the performance. The first
puncturing scheme employs a counting cycle algorithm and a grouping strategy
for variable nodes having short cycles of equal length in the Tanner Graph
(TG). The second scheme relies on a metric called Extrinsic Message Degree
(EMD) and the third scheme is a simulation-based exhaustive search to find the
best puncturing pattern among several random ones. In addition, we devise two
layer-structured extension schemes based on a counting cycle algorithm and an
EMD metric which are applied to design RC-LDPC codes. Simulation results show
that the proposed extension and puncturing techniques achieve greater rate
flexibility and good performance over the additive white Gaussian noise (AWGN)
channel, outperforming existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5144</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5144</id><created>2014-07-19</created><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Guzm&#xe1;n</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>Lower Bounds on the Oracle Complexity of Nonsmooth Convex Optimization
  via Information Theory</title><categories>math.OC cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an information-theoretic approach to lower bound the oracle
complexity of nonsmooth black box convex optimization, unifying previous lower
bounding techniques by identifying a combinatorial problem, namely string
guessing, as a single source of hardness. As a measure of complexity we use
distributional oracle complexity, which subsumes randomized oracle complexity
as well as worst-case oracle complexity. We obtain strong lower bounds on
distributional oracle complexity for the box $[-1,1]^n$, as well as for the
$L^p$-ball for $p \geq 1$ (for both low-scale and large-scale regimes),
matching worst-case lower bounds, and hence we close the gap between
distributional complexity, and in particular, randomized complexity, and
worst-case complexity. Furthermore, the bounds remain essentially the same for
high-probability and bounded-error oracle complexity, and even for combination
of the two, i.e., bounded-error high-probability oracle complexity. This
considerably extends the applicability of known bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5145</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5145</id><created>2014-07-19</created><authors><author><keyname>Hu</keyname><forenames>Yongtao</forenames></author><author><keyname>Kautz</keyname><forenames>Jan</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author></authors><title>Speaker-following Video Subtitles</title><categories>cs.HC cs.MM</categories><doi>10.1145/2632111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for improving the presentation of subtitles in video
(e.g. TV and movies). With conventional subtitles, the viewer has to constantly
look away from the main viewing area to read the subtitles at the bottom of the
screen, which disrupts the viewing experience and causes unnecessary eyestrain.
Our method places on-screen subtitles next to the respective speakers to allow
the viewer to follow the visual content while simultaneously reading the
subtitles. We use novel identification algorithms to detect the speakers based
on audio and visual information. Then the placement of the subtitles is
determined using global optimization. A comprehensive usability study indicated
that our subtitle placement method outperformed both conventional
fixed-position subtitling and another previous dynamic subtitling method in
terms of enhancing the overall viewing experience and reducing eyestrain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5155</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5155</id><created>2014-07-19</created><updated>2015-08-22</updated><authors><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>PANAMA</affiliation></author><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames><affiliation>CMAP</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>SIERRA, LIENS</affiliation></author></authors><title>Sparse and spurious: dictionary learning with noise and outliers</title><categories>cs.LG stat.ML</categories><comments>This is a substantially revised version of a first draft that
  appeared as a preprint titled &quot;Local stability and robustness of sparse
  dictionary learning in the presence of noise&quot;,
  http://hal.inria.fr/hal-00737152, IEEE Transactions on Information Theory,
  Institute of Electrical and Electronics Engineers (IEEE), 2015, pp.22</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular approach within the signal processing and machine learning
communities consists in modelling signals as sparse linear combinations of
atoms selected from a learned dictionary. While this paradigm has led to
numerous empirical successes in various fields ranging from image to audio
processing, there have only been a few theoretical arguments supporting these
evidences. In particular, sparse coding, or sparse dictionary learning, relies
on a non-convex procedure whose local minima have not been fully analyzed yet.
In this paper, we consider a probabilistic model of sparse signals, and show
that, with high probability, sparse coding admits a local minimum around the
reference dictionary generating the signals. Our study takes into account the
case of over-complete dictionaries, noisy signals, and possible outliers, thus
extending previous work limited to noiseless settings and/or under-complete
dictionaries. The analysis we conduct is non-asymptotic and makes it possible
to understand how the key quantities of the problem, such as the coherence or
the level of noise, can scale with respect to the dimension of the signals, the
number of atoms, the sparsity and the number of observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5158</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5158</id><created>2014-07-19</created><updated>2014-12-04</updated><authors><author><keyname>Richard</keyname><forenames>Emile</forenames><affiliation>LIGM</affiliation></author><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>LIGM</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Tight convex relaxations for sparse matrix factorization</title><categories>stat.ML cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a new atomic norm, we propose a new convex formulation for sparse
matrix factorization problems in which the number of nonzero elements of the
factors is assumed fixed and known. The formulation counts sparse PCA with
multiple factors, subspace clustering and low-rank sparse bilinear regression
as potential applications. We compute slow rates and an upper bound on the
statistical dimension of the suggested norm for rank 1 matrices, showing that
its statistical dimension is an order of magnitude smaller than the usual
$\ell\_1$-norm, trace norm and their combinations. Even though our convex
formulation is in theory hard and does not lead to provably polynomial time
algorithmic schemes, we propose an active set algorithm leveraging the
structure of the convex problem to solve it and show promising numerical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5161</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5161</id><created>2014-07-19</created><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Hua</keyname><forenames>Yingbo</forenames></author></authors><title>Channel Estimation and Optimal Training Design for Correlated MIMO
  Two-Way Relay Systems in Colored Environment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, while considering the impact of antenna correlation and the
interference from neighboring users, we analyze channel estimation and training
sequence design for multi-input multi-output (MIMO) two-way relay (TWR)
systems. To this end, we propose to decompose the bidirectional transmission
links into two phases, i.e., the multiple access (MAC) phase and the
broadcasting (BC) phase. By considering the Kronecker-structured channel model,
we derive the optimal linear minimum mean-square-error (LMMSE) channel
estimators. The corresponding training designs for the MAC and BC phases are
then formulated and solved to improve channel estimation accuracy. For the
general scenario of training sequence design for both phases, two iterative
training design algorithms are proposed that are verified to produce training
sequences that result in near optimal channel estimation performance.
Furthermore, for specific practical scenarios, where the covariance matrices of
the channel or disturbances are of particular structures, the optimal training
sequence design guidelines are derived. In order to reduce training overhead,
the minimum required training length for channel estimation in both the MAC and
BC phases are also derived. Comprehensive simulations are carried out to
demonstrate the effectiveness of the proposed training designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5166</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5166</id><created>2014-07-19</created><authors><author><keyname>Dima</keyname><forenames>C&#x103;t&#x103;lin</forenames></author><author><keyname>Maubert</keyname><forenames>Bastien</forenames></author><author><keyname>Pinchinat</keyname><forenames>Sophie</forenames></author></authors><title>The Expressive Power of Epistemic $\mu$-Calculus</title><categories>cs.LO</categories><acm-class>F.4.1; I.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the $\mu$-calculus notoriously subsumes Alternating-time Temporal Logic
(ATL), we show that the epistemic $\mu$-calculus does not subsume ATL with
imperfect information (ATL$_i$) for the synchronous perfect-recall semantics.
To prove this we first establish that jumping parity tree automata (JTA), a
recently introduced extension of alternating parity tree automata, are
expressively equivalent to the epistemic $\mu$-calculus, and this for any
knowledge semantics. Using this result we also show that, for bounded-memory
semantics, the epistemic $\mu$-calculus is not more expressive than the
standard $\mu$-calculus, and that its satisfiability problem is
EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5173</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5173</id><created>2014-07-19</created><authors><author><keyname>Deepu</keyname><forenames>C. J.</forenames></author></authors><title>An ECG-SoC with 535nW/channel lossless data compression for wearable
  sensors</title><categories>cs.AR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents a low power ECG recording Sys-tem-on-Chip (SoC) with
on-chip low complexity lossless ECG compression for data reduction in
wireless/ambulatory ECG sensor devices. The proposed algorithm uses a linear
slope predictor to estimate the ECG samples, and uses a novel low complexity
dynamic coding-packaging scheme to frame the resulting estimation error into
fixed-length 16-bit format. The proposed technique achieves an average
compression ratio of 2.25x on MIT/BIH ECG database. Implemented in 0.35 {\mu}m
process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for
4-channel, and consumes 535 nW/channel at 2.4V for ECG sampled at 512 Hz. Small
size and ultra-low power consumption makes the proposed technique suitable for
wearable ECG sensor application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5183</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5183</id><created>2014-07-19</created><authors><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author><author><keyname>Winlaw</keyname><forenames>Manda</forenames></author></authors><title>A Nonlinearly Preconditioned Conjugate Gradient Algorithm for Rank-R
  Canonical Tensor Approximation</title><categories>math.NA cs.NA math.OC</categories><msc-class>15A69, 65K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternating least squares (ALS) is often considered the workhorse algorithm
for computing the rank-R canonical tensor approximation, but for certain
problems its convergence can be very slow. The nonlinear conjugate gradient
(NCG) method was recently proposed as an alternative to ALS, but the results
indicated that NCG is usually not faster than ALS. To improve the convergence
speed of NCG, we consider a nonlinearly preconditioned nonlinear conjugate
gradient (PNCG) algorithm for computing the rank-R canonical tensor
decomposition. Our approach uses ALS as a nonlinear preconditioner in the NCG
algorithm. Alternatively, NCG can be viewed as an acceleration process for ALS.
We demonstrate numerically that the convergence acceleration mechanism in PNCG
often leads to important pay-offs for difficult tensor decomposition problems,
with convergence that is significantly faster and more robust than for the
stand-alone NCG or ALS algorithms. We consider several approaches for
incorporating the nonlinear preconditioner into the NCG algorithm that have
been described in the literature previously and have met with success in
certain application areas. However, it appears that the nonlinearly
preconditioned NCG approach has received relatively little attention in the
broader community and remains underexplored both theoretically and
experimentally. Thus, this paper serves several additional functions, by
providing in one place a concise overview of several PNCG variants and their
properties that have only been described in a few places scattered throughout
the literature, by systematically comparing the performance of these PNCG
variants for the tensor decomposition problem, and by drawing further attention
to the usefulness of nonlinearly preconditioned NCG as a general tool. In
addition, we briefly discuss the convergence of the PNCG algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5197</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5197</id><created>2014-07-19</created><authors><author><keyname>Vaish</keyname><forenames>Karan</forenames></author><author><keyname>Rajesh</keyname><forenames>Shah Mihir</forenames></author><author><keyname>Pasupatheeswaran</keyname><forenames>K.</forenames></author><author><keyname>Parashar</keyname><forenames>Anubha</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Jyoti</forenames></author></authors><title>Design and Autonomous Control of the Active Adaptive Suspension System
  Rudra Mars Rover</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi or completely autonomous unmanned vehicles, remotely driven or
controlled through artificial intelligence, are instrumental to foster space
exploration. One of the most essential tasks of a rover is terrain traversing
which requires the need of efficient suspension systems. This communication
presents a suspension system giving degrees of freedom to every wheel with the
help of linear actuators connected through bell crank levers. The actuation of
linear actuators directly varies the height of every wheel from the chassis
hence offering articulation to the rover. A control system is developed
offering an algorithm for its autonomous actuation. This system proves
instrumental for leveling of the chassis where any kind of slope, roll or
pitch, may impute abstaining of payloads from efficient working. This was tried
and tested successfully as a part of the rover developed by Team RUDRA from SRM
University, INDIA (first Team from Asia and finishing at the fifth position) at
University Rover Challenge 2013, held at UTAH, USA in May-June.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5211</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5211</id><created>2014-07-19</created><authors><author><keyname>Mura</keyname><forenames>Cameron</forenames></author></authors><title>Development &amp; Implementation of a PyMOL 'putty' Representation</title><categories>q-bio.BM cs.GR</categories><comments>3 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PyMOL molecular graphics program has been modi?ed to introduce a new
'putty' cartoon representation, akin to the 'sausage'-style representation of
the MOLMOL molecular visualization (MolVis) software package. This document
outlines the development and implementation of the putty representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5212</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5212</id><created>2014-07-19</created><authors><author><keyname>Khandwala</keyname><forenames>Kandarp</forenames></author><author><keyname>Sharma</keyname><forenames>Rudra</forenames></author><author><keyname>Rao</keyname><forenames>Snehal</forenames></author></authors><title>Context Aware Dynamic Traffic Signal Optimization</title><categories>cs.AI cs.NE</categories><report-no>17586-8253</report-no><journal-ref>International Journal of Computer Applications 100(13):24-28,
  August 2014</journal-ref><doi>10.5120/17586-8253</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional urban traffic control systems have been based on historical
traffic data. Later advancements made use of detectors, which enabled the
gathering of real time traffic data, in order to reorganize and calibrate
traffic signalization programs. Further evolvement provided the ability to
forecast traffic conditions, in order to develop traffic signalization programs
and strategies precomputed and applied at the most appropriate time frame for
the optimal control of the current traffic conditions. We, propose the next
generation of traffic control systems based on principles of Artificial
Intelligence and Context Awareness. Most of the existing algorithms use average
waiting time or length of the queue to assess an algorithms performance.
However, a low average waiting time may come at the cost of delaying other
vehicles indefinitely. In our algorithm, besides the vehicle queue, we use
fairness also as an important performance metric to assess an algorithms
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5218</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5218</id><created>2014-07-19</created><authors><author><keyname>Cieslik</keyname><forenames>Marcin</forenames></author><author><keyname>Derewenda</keyname><forenames>Zygmunt</forenames></author><author><keyname>Mura</keyname><forenames>Cameron</forenames></author></authors><title>Abstractions, Algorithms and Data Structures for Structural
  Bioinformatics in PyCogent</title><categories>q-bio.BM cs.SE</categories><comments>36 pages, 4 figures (including supplemental information)</comments><journal-ref>Journal of Applied Crystallography (2011), 44(2), 424-428</journal-ref><doi>10.1107/S0021889811004481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To facilitate flexible and efficient structural bioinformatics analyses, new
functionality for three-dimensional structure processing and analysis has been
introduced into PyCogent -- a popular feature-rich framework for sequence-based
bioinformatics, but one which has lacked equally powerful tools for handling
stuctural/coordinate-based data. Extensible Python modules have been developed,
which provide object-oriented abstractions (based on a hierarchical
representation of macromolecules), efficient data structures (e.g. kD-trees),
fast implementations of common algorithms (e.g. surface-area calculations),
read/write support for Protein Data Bank-related file formats and wrappers for
external command-line applications (e.g. Stride). Integration of this code into
PyCogent is symbiotic, allowing sequence-based work to benefit from
structure-derived data and, reciprocally, enabling structural studies to
leverage PyCogent's versatile tools for phylogenetic and evolutionary analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5220</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5220</id><created>2014-07-19</created><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author></authors><title>A three-state kinetic agent-based model to analyze tax evasion dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 6 figures, accepted for publication in Physica A</comments><journal-ref>Physica A 414, 321 (2014)</journal-ref><doi>10.1016/j.physa.2014.07.056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the problem of tax evasion on a fully-connected
population. For this purpose, we consider that the agents may be in three
different states, namely honest tax payers, tax evaders and undecided, that are
individuals in an intermediate class among honests and evaders. Every
individual can change his/her state following a kinetic exchange opinion
dynamics, where the agents interact by pairs with competitive negative (with
probability $q$) and positive (with probability $1-q$) couplings, representing
agreement/disagreement between pairs of agents. In addition, we consider the
punishment rules of the Zaklan econophysics model, for which there is a
probability $p_{a}$ of an audit each agent is subject to in every period and a
length of time $k$ detected tax evaders remain honest. Our results suggest that
below the critical point $q_{c}=1/4$ of the opinion dynamics the compliance is
high, and the punishment rules have a small effect in the population. On the
other hand, for $q&gt;q_{c}$ the tax evasion can be considerably reduced by the
enforcement mechanism. We also discuss the impact of the presence of the
undecided agents in the evolution of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5225</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5225</id><created>2014-07-19</created><updated>2015-06-26</updated><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Davis</keyname><forenames>Clayton</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>The Rise of Social Bots</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>11 pages, 2 figures, 1 table. 'Bot or Not?' is available at:
  http://truthy.indiana.edu/botornot/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Turing test aimed to recognize the behavior of a human from that of a
computer algorithm. Such challenge is more relevant than ever in today's social
media context, where limited attention and technology constrain the expressive
power of humans, while incentives abound to develop software agents mimicking
humans. These social bots interact, often unnoticed, with real people in social
media ecosystems, but their abundance is uncertain. While many bots are benign,
one can design harmful bots with the goals of persuading, smearing, or
deceiving. Here we discuss the characteristics of modern, sophisticated social
bots, and how their presence can endanger online ecosystems and our society. We
then review current efforts to detect social bots on Twitter. Features related
to content, network, sentiment, and temporal patterns of activity are imitated
by bots but at the same time can help discriminate synthetic behaviors from
human ones, yielding signatures of engineered social tampering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5234</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5234</id><created>2014-07-19</created><authors><author><keyname>Mantzel</keyname><forenames>William</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Compressed Subspace Matching on the Continuum</title><categories>cs.IT math.IT</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the general problem of matching a subspace to a signal in R^N
that has been observed indirectly (compressed) through a random projection. We
are interested in the case where the collection of K-dimensional subspaces is
continuously parameterized, i.e. naturally indexed by an interval from the real
line, or more generally a region of R^D. Our main results show that if the
dimension of the random projection is on the order of K times a geometrical
constant that describes the complexity of the collection, then the match
obtained from the compressed observation is nearly as good as one obtained from
a full observation of the signal. We give multiple concrete examples of
collections of subspaces for which this geometrical constant can be estimated,
and discuss the relevance of the results to the general problems of template
matching and source localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5238</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5238</id><created>2014-07-19</created><authors><author><keyname>Veeramachaneni</keyname><forenames>Kalyan</forenames></author><author><keyname>O'Reilly</keyname><forenames>Una-May</forenames></author><author><keyname>Taylor</keyname><forenames>Colin</forenames></author></authors><title>Towards Feature Engineering at Scale for Data from Massive Open Online
  Courses</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the process of engineering features for developing models that
improve our understanding of learners' online behavior in MOOCs. Because
feature engineering relies so heavily on human insight, we argue that extra
effort should be made to engage the crowd for feature proposals and even their
operationalization. We show two approaches where we have started to engage the
crowd. We also show how features can be evaluated for their relevance in
predictive accuracy. When we examined crowd-sourced features in the context of
predicting stopout, not only were they nuanced, but they also considered more
than one interaction mode between the learner and platform and how the learner
was relatively performing. We were able to identify different influential
features for stop out prediction that depended on whether a learner was in 1 of
4 cohorts defined by their level of engagement with the course discussion forum
or wiki. This report is part of a compendium which considers different aspects
of MOOC data science and stop out prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5242</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5242</id><created>2014-07-19</created><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>Object Proposal Generation using Two-Stage Cascade SVMs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object proposal algorithms have shown great promise as a first step for
object recognition and detection. Good object proposal generation algorithms
require high object recall rate as well as low computational cost, because
generating object proposals is usually utilized as a preprocessing step. The
problem of how to accelerate the object proposal generation and evaluation
process without decreasing recall is thus of great interest. In this paper, we
propose a new object proposal generation method using two-stage cascade SVMs,
where in the first stage linear filters are learned for predefined quantized
scales/aspect-ratios independently, and in the second stage a global linear
classifier is learned across all the quantized scales/aspect-ratios for
calibration, so that all the proposals can be compared properly. The proposals
with highest scores are our final output. Specifically, we explain our
scale/aspect-ratio quantization scheme, and investigate the effects of
combinations of $\ell_1$ and $\ell_2$ regularizers in cascade SVMs with/without
ranking constraints in learning. Comprehensive experiments on VOC2007 dataset
are conducted, and our results achieve the state-of-the-art performance with
high object recall rate and high computational efficiency. Besides, our method
has been demonstrated to be suitable for not only class-specific but also
generic object proposal generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5245</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5245</id><created>2014-07-20</created><updated>2016-01-18</updated><authors><author><keyname>Zhao</keyname><forenames>Ji</forenames></author><author><keyname>Wang</keyname><forenames>Liantao</forenames></author><author><keyname>Cabral</keyname><forenames>Ricardo</forenames></author><author><keyname>De la Torre</keyname><forenames>Fernando</forenames></author></authors><title>Feature and Region Selection for Visual Learning</title><categories>cs.CV cs.LG</categories><journal-ref>IEEE Transactions on Image Processing, 2016, vol. 25, pp.
  1084-1094</journal-ref><doi>10.1109/TIP.2016.2514503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual learning problems such as object classification and action recognition
are typically approached using extensions of the popular bag-of-words (BoW)
model. Despite its great success, it is unclear what visual features the BoW
model is learning: Which regions in the image or video are used to discriminate
among classes? Which are the most discriminative visual words? Answering these
questions is fundamental for understanding existing BoW models and inspiring
better models for visual recognition.
  To answer these questions, this paper presents a method for feature selection
and region selection in the visual BoW model. This allows for an intermediate
visualization of the features and regions that are important for visual
learning. The main idea is to assign latent weights to the features or regions,
and jointly optimize these latent variables with the parameters of a classifier
(e.g., support vector machine). There are four main benefits of our approach:
(1) Our approach accommodates non-linear additive kernels such as the popular
$\chi^2$ and intersection kernel; (2) our approach is able to handle both
regions in images and spatio-temporal regions in videos in a unified way; (3)
the feature selection problem is convex, and both problems can be solved using
a scalable reduced gradient method; (4) we point out strong connections with
multiple kernel learning and multiple instance learning approaches.
Experimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTube
illustrate the benefits of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5286</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5286</id><created>2014-07-20</created><updated>2016-02-05</updated><authors><author><keyname>Galeotti</keyname><forenames>Juan P.</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>May</keyname><forenames>Eva</forenames></author><author><keyname>Fraser</keyname><forenames>Gordon</forenames></author><author><keyname>Zeller</keyname><forenames>Andreas</forenames></author></authors><title>Inferring Loop Invariants by Mutation, Dynamic Analysis, and Static
  Checking</title><categories>cs.SE</categories><comments>Only change in v4: rectified May's affiliation</comments><journal-ref>IEEE Transactions on Software Engineering, 41(10):1019-1037,
  October 2015</journal-ref><doi>10.1109/TSE.2015.2431688</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verifiers that can prove programs correct against their full functional
specification require, for programs with loops, additional annotations in the
form of loop invariants---propeties that hold for every iteration of a loop. We
show that significant loop invariant candidates can be generated by
systematically mutating postconditions; then, dynamic checking (based on
automatically generated tests) weeds out invalid candidates, and static
checking selects provably valid ones. We present a framework that automatically
applies these techniques to support a program prover, paving the way for fully
automatic verification without manually written loop invariants: Applied to 28
methods (including 39 different loops) from various java.util classes
(occasionally modified to avoid using Java features not fully supported by the
static checker), our DYNAMATE prototype automatically discharged 97% of all
proof obligations, resulting in automatic complete correctness proofs of 25 out
of the 28 methods---outperforming several state-of-the-art tools for fully
automatic verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5298</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5298</id><created>2014-07-20</created><updated>2015-11-25</updated><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Molinaro</keyname><forenames>Marco</forenames></author></authors><title>How the Experts Algorithm Can Help Solve LPs Online</title><categories>cs.DS math.OC</categories><comments>An extended abstract appears in the 22nd European Symposium on
  Algorithms (ESA 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of solving packing/covering LPs online, when the
columns of the constraint matrix are presented in random order. This problem
has received much attention and the main focus is to figure out how large the
right-hand sides of the LPs have to be (compared to the entries on the
left-hand side of the constraints) to allow $(1+\epsilon)$-approximations
online. It is known that the right-hand sides have to be $\Omega(\epsilon^{-2}
\log m)$ times the left-hand sides, where $m$ is the number of constraints.
  In this paper we give a primal-dual algorithm that achieve this bound for
mixed packing/covering LPs. Our algorithms construct dual solutions using a
regret-minimizing online learning algorithm in a black-box fashion, and use
them to construct primal solutions. The adversarial guarantee that holds for
the constructed duals helps us to take care of most of the correlations that
arise in the algorithm; the remaining correlations are handled via martingale
concentration and maximal inequalities. These ideas lead to conceptually simple
and modular algorithms, which we hope will be useful in other contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5319</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5319</id><created>2014-07-20</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>V</keyname><forenames>Suma</forenames></author><author><keyname>R</keyname><forenames>Shashi Kumar N.</forenames></author></authors><title>Impact Analysis of Allocation of Resources by Project Manager on Success
  of Software Projects</title><categories>cs.SE</categories><comments>5 pages,5 figures, 1 tables, International Conference on Data Mining
  and Computer Engineering (ICDMCE'2012) December 21-22, 2012 Bangkok
  (Thailand)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generation Production of successful software project is one of the prime
considerations of software industry. Engineering high quality software products
is further influenced by several factors such as budget, schedule, resource
constraints etc. A project manager is responsible for estimation and allocation
of these resources in a project. Hence, role of project manager has a vital
influence on success of the project. This research comprises of an empirical
study of several projects developed in a product and service based CMMI Level 5
Software Company. The investigation result shows a significant impact of
aforementioned factors on the success of software and on the company. The
analysis further indicates the vital role of project managers in optimizing the
resource allocation towards development of software. This paper brings in
impact analysis of efficiency of project manager in effectively allocating
resources such as time, cost, number of developers etc. An awareness of
efficiency level of project manager in optimal allocation of resources enables
one to realize the desired level of quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5320</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5320</id><created>2014-07-20</created><authors><author><keyname>Sharma</keyname><forenames>Vivek</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>An Optimum Scheduling Approach for Creating Optimal Priority of Jobs
  with Business Values in Cloud Computing</title><categories>cs.DC cs.NI</categories><comments>8 pages, 6 figures, 6 tables, International Conference on Advances in
  Cloud Computing (ACC12)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Realizing an optimal task scheduling by taking into account the business
importance of jobs has become a matter of interest in pay and use model of
Cloud computing. Introduction of an appropriate model for an efficient task
scheduling technique could derive benefit to the service providers as well as
clients. In this paper, we have addressed two major challenges which has
implications on the performance of the Cloud system. One of the major issues is
handling technical aspects of distributing the tasks for targeted gains and the
second issue is related to the handling of the business priority for
concurrently resolving business complexity related to cloud consumers. A
coordinated scheduling can be achieved by considering the weightage of both
aspects viz. technical requirements and business requirements appropriately. It
can be done in such a way that it meets the QoS requirements of technical
domain as well as business domain. Along with the technical priority a business
Bp is required in creating a resultant priority which could be given to stages
of further processing, like task allocation and arbitration schemes. Here we
consider a technical priority Tp that is governed by a semi-adaptive scheduling
algorithm whereas the resultant priority is derived in which a Business
Priority Bp layer encapsulates the Technical Priority Tp to achieve the overall
priority of the incoming tasks. It results in a Hybrid priority creation, which
is a combination of both technical priority Tp and business priority Bp. By
taking into account the business priority of the jobs it is possible to achieve
a higher service level satisfaction for the tasks which are submitted with
their native technical priority. With this approach the waiting time of the
tasks tends to get reduced and it gives a better service level satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5323</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5323</id><created>2014-07-20</created><authors><author><keyname>R.</keyname><forenames>Shashikumar N.</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>V</keyname><forenames>Suma</forenames></author></authors><title>A Parametric Analysis of Project Management Performance to Enhance
  Software Development Process</title><categories>cs.SE</categories><comments>6 pages, 5 figures, 1 tables, IEEE International Conference on
  Advanced Research in Engineering and Technology (ICARET - 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Project Management process plays a significant role in effective development
of software projects. Key challenges in the project management process are the
estimation of time, cost, defect count, and subsequently selection of apt
developers. Therefore precise estimation of above stated factors decides the
success level of a project. This paper provides an empirical study of several
projects developed in a service oriented software company in order to
comprehend the project management process. The analysis throws light on the
existence of variation in the aforementioned factors between estimation and
observed results. It further captures the need for betterment of project
management process in estimation and allocation of resources in the realization
of high quality software product. The paper therefore aims to bring in an
improved awareness in software engineering personnel concerning the magnitude
and significance of better estimation and accurate allocation of resources for
developing successful project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5324</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5324</id><created>2014-07-20</created><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Azad</keyname><forenames>Babak</forenames></author><author><keyname>Kazerooni</keyname><forenames>Iman Tavakoli</forenames></author></authors><title>Optimized Method for Iranian Road Signs Detection and recognition system</title><categories>cs.CV</categories><journal-ref>International Journal of Research in Computer Science, 4 (1): pp.
  19-26, January 2014</journal-ref><doi>10.7815/ijorcs.41.2014.077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Road sign recognition is one of the core technologies in Intelligent
Transport Systems. In the current study, a robust and real-time method is
presented to identify and detect the roads speed signs in road image in
different situations. In our proposed method, first, the connected components
are created in the main image using the edge detection and mathematical
morphology and the location of the road signs extracted by the geometric and
color data; then the letters are segmented and recognized by Multiclass Support
Vector Machine (SVMs) classifiers. Regarding that the geometric and color
features ate properly used in detection the location of the road signs, so it
is not sensitive to the distance and noise and has higher speed and efficiency.
In the result part, the proposed approach is applied on Iranian road speed sign
database and the detection and recognition accuracy rate achieved 98.66% and
100% respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5327</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5327</id><created>2014-07-20</created><authors><author><keyname>Sooda</keyname><forenames>Kavitha</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>A Comparative Analysis for Determining the Optimal Path using PSO and GA</title><categories>cs.NI cs.NE</categories><comments>5 pages, 4 figures, 1 tables. arXiv admin note: substantial text
  overlap with arXiv:1107.1945</comments><journal-ref>pages: 8-12, International Journal of Computer Applications,
  Volume 32, No 4, October 2011 ISSN 0975 - 8887</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant research has been carried out recently to find the optimal path
in network routing. Among them, the evolutionary algorithm approach is an area
where work is carried out extensively. We in this paper have used particle
swarm optimization (PSO) and genetic algorithm (GA) for finding the optimal
path and the concept of region based network is introduced along with the use
of indirect encoding. We demonstrate the advantage of fitness value and hop
count in both PSO and GA. A comparative study of PSO and genetic algorithm (GA)
is carried out, and it was found that PSO converged to arrive at the optimal
path much faster than GA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5336</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5336</id><created>2014-07-20</created><updated>2015-10-30</updated><authors><author><keyname>Bonnet</keyname><forenames>Edouard</forenames></author><author><keyname>Foucaud</keyname><forenames>Florent</forenames></author><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>Complexity of Grundy coloring and its variants</title><categories>cs.DS cs.DM math.CO</categories><comments>24 pages, 7 figures. This version contains some new results and
  improvements. A short paper based on version v2 appeared in COCOON'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Grundy number of a graph is the maximum number of colors used by the
greedy coloring algorithm over all vertex orderings. In this paper, we study
the computational complexity of GRUNDY COLORING, the problem of determining
whether a given graph has Grundy number at least $k$. We also study the
variants WEAK GRUNDY COLORING (where the coloring is not necessarily proper)
and CONNECTED GRUNDY COLORING (where at each step of the greedy coloring
algorithm, the subgraph induced by the colored vertices must be connected).
  We show that GRUNDY COLORING can be solved in time $O^*(2.443^n)$ and WEAK
GRUNDY COLORING in time $O^*(2.716^n)$ on graphs of order $n$. While GRUNDY
COLORING and WEAK GRUNDY COLORING are known to be solvable in time
$O^*(2^{O(wk)})$ for graphs of treewidth $w$ (where $k$ is the number of
colors), we prove that under the Exponential Time Hypothesis (ETH), they cannot
be solved in time $O^*(2^{o(w\log w)})$. We also describe an
$O^*(2^{2^{O(k)}})$ algorithm for WEAK GRUNDY COLORING, which is therefore
$\fpt$ for the parameter $k$. Moreover, under the ETH, we prove that such a
running time is essentially optimal (this lower bound also holds for GRUNDY
COLORING). Although we do not know whether GRUNDY COLORING is in $\fpt$, we
show that this is the case for graphs belonging to a number of standard graph
classes including chordal graphs, claw-free graphs, and graphs excluding a
fixed minor. We also describe a quasi-polynomial time algorithm for GRUNDY
COLORING and WEAK GRUNDY COLORING on apex-minor graphs. In stark contrast with
the two other problems, we show that CONNECTED GRUNDY COLORING is
$\np$-complete already for $k=7$ colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5355</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5355</id><created>2014-07-20</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Liu</keyname><forenames>Tao</forenames></author></authors><title>Secure Wireless Information and Power Transfer in Large-Scale MIMO
  Relaying Systems with Imperfect CSI</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1401.3049</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of secure wireless information and
power transfer in a large-scale multiple-input multiple-output (LS-MIMO)
amplify-and-forward (AF) relaying system. The advantage of LS-MIMO relay is
exploited to enhance wireless security, transmission rate and energy
efficiency. In particular, the challenging issues incurred by short
interception distance and long transfer distance are well addressed
simultaneously. Under very practical assumptions, i.e., no eavesdropper's
channel state information (CSI) and imperfect legitimate channel CSI, this
paper investigates the impact of imperfect CSI, and obtains an explicit
expression of the secrecy outage capacity in terms of transmit power and
channel condition. Then, we propose an optimal power splitting scheme at the
relay to maximize the secrecy outage capacity. Finally, our theoretical claims
are validated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5358</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5358</id><created>2014-07-20</created><authors><author><keyname>Barreto</keyname><forenames>Andr&#xe9; M. S.</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Practical Kernel-Based Reinforcement Learning</title><categories>cs.LG cs.AI stat.ML</categories><msc-class>68T05 (Primary), 93E35, 90C40, 93E20, 49L20 (Secondary)</msc-class><acm-class>I.2.8; I.2.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel-based reinforcement learning (KBRL) stands out among reinforcement
learning algorithms for its strong theoretical guarantees. By casting the
learning problem as a local kernel approximation, KBRL provides a way of
computing a decision policy which is statistically consistent and converges to
a unique solution. Unfortunately, the model constructed by KBRL grows with the
number of sample transitions, resulting in a computational cost that precludes
its application to large-scale or on-line domains. In this paper we introduce
an algorithm that turns KBRL into a practical reinforcement learning tool.
Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a
transition matrix is represented as the product of two stochastic matrices, one
can swap the factors of the multiplication to obtain another transition matrix,
potentially much smaller, which retains some fundamental properties of its
precursor. KBSF exploits such an insight to compress the information contained
in KBRL's model into an approximator of fixed size. This makes it possible to
build an approximation that takes into account both the difficulty of the
problem and the associated computational cost. KBSF's computational complexity
is linear in the number of sample transitions, which is the best one can do
without discarding data. Moreover, the algorithm's simple mechanics allow for a
fully incremental implementation that makes the amount of memory used
independent of the number of sample transitions. The result is a kernel-based
reinforcement learning algorithm that can be applied to large-scale problems in
both off-line and on-line regimes. We derive upper bounds for the distance
between the value functions computed by KBRL and KBSF using the same data. We
also illustrate the potential of our algorithm in an extensive empirical study
in which KBSF is applied to difficult tasks based on real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5364</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5364</id><created>2014-07-20</created><authors><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Smarandache</keyname><forenames>Roxana</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Quasi-Cyclic LDPC Codes based on Pre-Lifted Protographs</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quasi-cyclic low-density parity-check (QC-LDPC) codes based on protographs
are of great interest to code designers because analysis and implementation are
facilitated by the protograph structure and the use of circulant permutation
matrices for protograph lifting. However, these restrictions impose undesirable
fixed upper limits on important code parameters, such as minimum distance and
girth. In this paper, we consider an approach to constructing QC-LDPC codes
that uses a two-step lifting procedure based on a protograph, and, by following
this method instead of the usual one-step procedure, we obtain improved minimum
distance and girth properties. We also present two new design rules for
constructing good QC-LDPC codes using this two-step lifting procedure, and in
each case we obtain a significant increase in minimum distance and achieve a
certain guaranteed girth compared to one-step circulant-based liftings. The
expected performance improvement is verified by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5366</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5366</id><created>2014-07-20</created><authors><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Spatially Coupled LDPC Codes Constructed from Protographs</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct protograph-based spatially coupled low-density
parity-check (SC-LDPC) codes by coupling together a series of L disjoint, or
uncoupled, LDPC code Tanner graphs into a single coupled chain. By varying L,
we obtain a flexible family of code ensembles with varying rates and frame
lengths that can share the same encoding and decoding architecture for
arbitrary L. We demonstrate that the resulting codes combine the best features
of optimized irregular and regular codes in one design: capacity approaching
iterative belief propagation (BP) decoding thresholds and linear growth of
minimum distance with block length. In particular, we show that, for
sufficiently large L, the BP thresholds on both the binary erasure channel
(BEC) and the binary-input additive white Gaussian noise channel (AWGNC)
saturate to a particular value significantly better than the BP decoding
threshold and numerically indistinguishable from the optimal maximum
a-posteriori (MAP) decoding threshold of the uncoupled LDPC code. When all
variable nodes in the coupled chain have degree greater than two,
asymptotically the error probability converges at least doubly exponentially
with decoding iterations and we obtain sequences of asymptotically good LDPC
codes with fast convergence rates and BP thresholds close to the Shannon limit.
Further, the gap to capacity decreases as the density of the graph increases,
opening up a new way to construct capacity achieving codes on memoryless
binary-input symmetric-output (MBS) channels with low-complexity BP decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5367</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5367</id><created>2014-07-20</created><authors><author><keyname>Agarwal</keyname><forenames>Sameer</forenames></author><author><keyname>Lee</keyname><forenames>Hon-leung</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author><author><keyname>Thomas</keyname><forenames>Rekha R.</forenames></author></authors><title>Certifying the Existence of Epipolar Matrices</title><categories>cs.CV math.AG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of point correspondences in two images, the existence of a
fundamental matrix is a necessary condition for the points to be the images of
a 3-dimensional scene imaged with two pinhole cameras. If the camera
calibration is known then one requires the existence of an essential matrix.
  We present an efficient algorithm, using exact linear algebra, for testing
the existence of a fundamental matrix. The input is any number of point
correspondences. For essential matrices, we characterize the solvability of the
Demazure polynomials. In both scenarios, we determine which linear subspaces
intersect a fixed set defined by non-linear polynomials. The conditions we
derive are polynomials stated purely in terms of image coordinates. They
represent a new class of two-view invariants, free of fundamental
(resp.~essential)~matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5373</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5373</id><created>2014-07-21</created><updated>2014-11-04</updated><authors><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Pierrakos</keyname><forenames>George</forenames></author><author><keyname>Psomas</keyname><forenames>Christos-Alexandros</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>On the Complexity of Dynamic Mechanism Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a dynamic mechanism design problem in which the designer wants
to offer for sale an item to an agent, and another item to the same agent at
some point in the future. The agent's joint distribution of valuations for the
two items is known, and the agent knows the valuation for the current item (but
not for the one in the future). The designer seeks to maximize expected
revenue, and the auction must be deterministic, truthful, and ex post
individually rational. The optimum mechanism involves a protocol whereby the
seller elicits the buyer's current valuation, and based on the bid makes two
take-it-or-leave-it offers, one for now and one for the future. We show that
finding the optimum deterministic mechanism in this situation - arguably the
simplest meaningful dynamic mechanism design problem imaginable - is NP-hard.
We also prove several positive results, among them a polynomial linear
programming-based algorithm for the optimum randomized auction (even for many
bidders and periods), and we show strong separations in revenue between
non-adaptive, adaptive, and randomized auctions, even when the valuations in
the two periods are uncorrelated. Finally, for the same problem in an
environment in which contracts cannot be enforced, and thus perfection of
equilibrium is necessary, we show that the optimum randomized mechanism
requires multiple rounds of cheap talk-like interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5374</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5374</id><created>2014-07-21</created><updated>2015-05-13</updated><authors><author><keyname>Giotis</keyname><forenames>Ioannis</forenames></author><author><keyname>Kirousis</keyname><forenames>Lefteris</forenames></author><author><keyname>Psaromiligkos</keyname><forenames>Kostas I.</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>On the Algorithmic Lov\'asz Local Lemma and Acyclic Edge Coloring</title><categories>cs.DM cs.DS math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algorithm for Lov\'{a}sz Local Lemma by Moser and Tardos gives a
constructive way to prove the existence of combinatorial objects that satisfy a
system of constraints. We present an alternative probabilistic analysis of the
algorithm that does not involve reconstructing the history of the algorithm. We
apply our technique to improve the best known upper bound to acyclic chromatic
index. Specifically we show that a graph with maximum degree $\Delta$ has an
acyclic proper edge coloring with at most $\lceil 3.74(\Delta-1)\rceil+1 $
colors, whereas the previously known best bound was $4(\Delta-1)$. The same
technique is also applied to improve corresponding bounds for graphs with
bounded girth. An interesting aspect of the latter application is that the
probability of the &quot;undesirable&quot; events do not have a uniform upper bound,
i.e., it constitutes a case of the asymmetric Lov\'{a}sz Local Lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5378</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5378</id><created>2014-07-21</created><updated>2015-08-18</updated><authors><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Schmidt</keyname><forenames>Judy</forenames></author></authors><title>Looking before leaping: Creating a software registry</title><categories>astro-ph.IM cs.DL</categories><comments>11 pages; submission for WSSSPE2. Revised after review for
  publication in the Journal of Open Research Software</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  What lessons can be learned from examining numerous efforts to create a
repository or directory of scientist-written software for a discipline?
Astronomy has seen a number of efforts to build such a resource, one of which
is the Astrophysics Source Code Library (ASCL). The ASCL (ascl.net) was founded
in 1999, had a period of dormancy, and was restarted in 2010. When taking over
responsibility for the ASCL in 2010, the new editor sought to answer the
opening question, hoping this would better inform the work to be done. We also
provide specific steps the ASCL is taking to try to improve code sharing and
discovery in astronomy and share recent improvements to the resource.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5380</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5380</id><created>2014-07-21</created><authors><author><keyname>Zhang</keyname><forenames>Dongmo</forenames></author><author><keyname>Thielsher</keyname><forenames>Michael</forenames></author></authors><title>Representing and Reasoning about Game Strategies</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  As a contribution to the challenge of building game-playing AI systems, we
develop and analyse a formal language for representing and reasoning about
strategies. Our logical language builds on the existing general Game
Description Language (GDL) and extends it by a standard modality for linear
time along with two dual connectives to express preferences when combining
strategies. The semantics of the language is provided by a standard
state-transition model. As such, problems that require reasoning about games
can be solved by the standard methods for reasoning about actions and change.
We also endow the language with a specific semantics by which strategy formulas
are understood as move recommendations for a player. To illustrate how our
formalism supports automated reasoning about strategies, we demonstrate two
example methods of implementation\/: first, we formalise the semantic
interpretation of our language in conjunction with game rules and strategy
rules in the Situation Calculus; second, we show how the reasoning problem can
be solved with Answer Set Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5383</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5383</id><created>2014-07-21</created><updated>2014-10-20</updated><authors><author><keyname>Santhanam</keyname><forenames>Narayana P.</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author><author><keyname>Woo</keyname><forenames>Jae Oh</forenames></author></authors><title>Redundancy of Exchangeable Estimators</title><categories>cs.IT math.IT</categories><comments>18 pages</comments><doi>10.3390/e16105339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exchangeable random partition processes are the basis for Bayesian approaches
to statistical inference in large alphabet settings. On the other hand, the
notion of the pattern of a sequence provides an information-theoretic framework
for data compression in large alphabet scenarios. Because data compression and
parameter estimation are intimately related, we study the redundancy of Bayes
estimators coming from Poisson-Dirichlet priors (or &quot;Chinese restaurant
processes&quot;) and the Pitman-Yor prior. This provides an understanding of these
estimators in the setting of unknown discrete alphabets from the perspective of
universal compression. In particular, we identify relations between alphabet
sizes and sample sizes where the redundancy is small, thereby characterizing
useful regimes for these estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5385</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5385</id><created>2014-07-21</created><authors><author><keyname>Akhtar</keyname><forenames>Md. Amir Khusru</forenames></author><author><keyname>Usmani</keyname><forenames>Arshad</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>Nontransitive Ranking to Enhance Routing Decision in MANETS</title><categories>cs.NI</categories><comments>7 Pages, IJCEA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ad hoc network is an infrastructureless network in which nodes perform
terminal as well as routing functions. A routing protocol is the only
substitute to complete the communications in the absence of an access point. In
spite of that mobile nodes or so called routers uses some mechanism for
calculating the best route when it has multiple routes for the same
destination. On the basis of one or more metrics routes are ranked from best to
worst. But, in an ad hoc network many factors can affect this decision, such as
the delay, load, route lifetime etc. Thus measuring and finding routes on the
basis of crisp mathematical model for all these attributes is complicated. That
why, the fuzzy approach for best route determination is required for MANET
because some of the metrics are fuzzy or vague and the classical ranking of
routes and transitivity in the ranking does not hold. The proposed
Nontransitive Route Ranking subjective comparison of one route with others and
performs nontransitive ranking to rank routes from best to worst. The pairwise
comparisons of each route with others give more accurate and fair comparison.
The proposed ranking is easier than classical ranking in which metrics have
assigned some value and these values are combined to obtain the ranking.
Experimental result shows the efficiency of the proposed model. Keywords:
Fuzzy, Rank, Nontransitive, Route, Ranking, Relativity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5392</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5392</id><created>2014-07-21</created><authors><author><keyname>Gasc&#xf3;n</keyname><forenames>Adri&#xe0;</forenames><affiliation>SRI International</affiliation></author><author><keyname>Tiwari</keyname><forenames>Ashish</forenames><affiliation>SRI International</affiliation></author></authors><title>Synthesis of a simple self-stabilizing system</title><categories>cs.LO cs.DC</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><acm-class>F.3.1; C.2.4</acm-class><journal-ref>EPTCS 157, 2014, pp. 5-16</journal-ref><doi>10.4204/EPTCS.157.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing importance of distributed systems as a computing
paradigm, a systematic approach to their design is needed. Although the area of
formal verification has made enormous advances towards this goal, the resulting
functionalities are limited to detecting problems in a particular design. By
means of a classical example, we illustrate a simple template-based approach to
computer-aided design of distributed systems based on leveraging the well-known
technique of bounded model checking to the synthesis setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5393</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5393</id><created>2014-07-21</created><authors><author><keyname>Wiklicky</keyname><forenames>Herbert</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Program Synthesis and Linear Operator Semantics</title><categories>cs.PL cs.PF</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 157, 2014, pp. 17-33</journal-ref><doi>10.4204/EPTCS.157.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For deterministic and probabilistic programs we investigate the problem of
program synthesis and program optimisation (with respect to non-functional
properties) in the general setting of global optimisation. This approach is
based on the representation of the semantics of programs and program fragments
in terms of linear operators, i.e. as matrices. We exploit in particular the
fact that we can automatically generate the representation of the semantics of
elementary blocks. These can then can be used in order to compositionally
assemble the semantics of a whole program, i.e. the generator of the
corresponding Discrete Time Markov Chain (DTMC). We also utilise a generalised
version of Abstract Interpretation suitable for this linear algebraic or
functional analytical framework in order to formulate semantical constraints
(invariants) and optimisation objectives (for example performance
requirements).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5395</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5395</id><created>2014-07-21</created><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames><affiliation>Graz University of Technology</affiliation></author><author><keyname>Ehlers</keyname><forenames>R&#xfc;diger</forenames><affiliation>University of Bremen / DFKI</affiliation></author><author><keyname>Jacobs</keyname><forenames>Swen</forenames><affiliation>Graz University of Technology</affiliation></author><author><keyname>K&#xf6;nighofer</keyname><forenames>Robert</forenames><affiliation>Graz University of Technology</affiliation></author></authors><title>How to Handle Assumptions in Synthesis</title><categories>cs.LO</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 157, 2014, pp. 34-50</journal-ref><doi>10.4204/EPTCS.157.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increased interest in reactive synthesis over the last decade has led to
many improved solutions but also to many new questions. In this paper, we
discuss the question of how to deal with assumptions on environment behavior.
We present four goals that we think should be met and review several different
possibilities that have been proposed. We argue that each of them falls short
in at least one aspect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5396</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5396</id><created>2014-07-21</created><authors><author><keyname>Bohy</keyname><forenames>Aaron</forenames><affiliation>Universit&#xe9; de Mons</affiliation></author><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames><affiliation>Universit&#xe9; de Mons</affiliation></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author></authors><title>Symblicit algorithms for optimal strategy synthesis in monotonic Markov
  decision processes</title><categories>cs.LO cs.DS cs.SY</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 157, 2014, pp. 51-67</journal-ref><doi>10.4204/EPTCS.157.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When treating Markov decision processes (MDPs) with large state spaces, using
explicit representations quickly becomes unfeasible. Lately, Wimmer et al. have
proposed a so-called symblicit algorithm for the synthesis of optimal
strategies in MDPs, in the quantitative setting of expected mean-payoff. This
algorithm, based on the strategy iteration algorithm of Howard and Veinott,
efficiently combines symbolic and explicit data structures, and uses binary
decision diagrams as symbolic representation. The aim of this paper is to show
that the new data structure of pseudo-antichains (an extension of antichains)
provides another interesting alternative, especially for the class of monotonic
MDPs. We design efficient pseudo-antichain based symblicit algorithms (with
open source implementations) for two quantitative settings: the expected
mean-payoff and the stochastic shortest path. For two practical applications
coming from automated planning and LTL synthesis, we report promising
experimental results w.r.t. both the run time and the memory consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5397</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5397</id><created>2014-07-21</created><authors><author><keyname>Jha</keyname><forenames>Susmit</forenames><affiliation>Strategic CAD Labs, Intel</affiliation></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames><affiliation>EECS, UC Berkeley</affiliation></author></authors><title>Are There Good Mistakes? A Theoretical Analysis of CEGIS</title><categories>cs.LO cs.AI cs.LG cs.PL</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 157, 2014, pp. 84-99</journal-ref><doi>10.4204/EPTCS.157.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterexample-guided inductive synthesis CEGIS is used to synthesize
programs from a candidate space of programs. The technique is guaranteed to
terminate and synthesize the correct program if the space of candidate programs
is finite. But the technique may or may not terminate with the correct program
if the candidate space of programs is infinite. In this paper, we perform a
theoretical analysis of counterexample-guided inductive synthesis technique. We
investigate whether the set of candidate spaces for which the correct program
can be synthesized using CEGIS depends on the counterexamples used in inductive
synthesis, that is, whether there are good mistakes which would increase the
synthesis power. We investigate whether the use of minimal counterexamples
instead of arbitrary counterexamples expands the set of candidate spaces of
programs for which inductive synthesis can successfully synthesize a correct
program. We consider two kinds of counterexamples: minimal counterexamples and
history bounded counterexamples. The history bounded counterexample used in any
iteration of CEGIS is bounded by the examples used in previous iterations of
inductive synthesis. We examine the relative change in power of inductive
synthesis in both cases. We show that the synthesis technique using minimal
counterexamples MinCEGIS has the same synthesis power as CEGIS but the
synthesis technique using history bounded counterexamples HCEGIS has different
power than that of CEGIS, but none dominates the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5399</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5399</id><created>2014-07-21</created><authors><author><keyname>Ehlers</keyname><forenames>R&#xfc;diger</forenames><affiliation>University of Bremen and DFKI</affiliation></author><author><keyname>Raman</keyname><forenames>Vasumathi</forenames><affiliation>California Institute of Technology</affiliation></author></authors><title>Low-Effort Specification Debugging and Analysis</title><categories>cs.SE cs.LO cs.RO</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 157, 2014, pp. 117-133</journal-ref><doi>10.4204/EPTCS.157.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive synthesis deals with the automated construction of implementations
of reactive systems from their specifications. To make the approach feasible in
practice, systems engineers need effective and efficient means of debugging
these specifications.
  In this paper, we provide techniques for report-based specification
debugging, wherein salient properties of a specification are analyzed, and the
result presented to the user in the form of a report. This provides a
low-effort way to debug specifications, complementing high-effort techniques
including the simulation of synthesized implementations.
  We demonstrate the usefulness of our report-based specification debugging
toolkit by providing examples in the context of generalized reactivity(1)
synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5404</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5404</id><created>2014-07-21</created><authors><author><keyname>Persya</keyname><forenames>A. Christy</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>Model based design of super schedulers managing catastrophic scenario in
  hard real time systems</title><categories>cs.DC</categories><comments>7 pages, 4 figures,Information Communication and Embedded Systems
  (ICICES), 2013, IEEE International Conference on, Chennai, India,
  pp.1149,1155, 21-22 Feb. 2013</comments><doi>10.1109/ICICES.2013.6508316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional design of real-time approaches depends heavily on the normal
performance of systems and it often becomes incapacitated in dealing with
catastrophic scenarios effectively. There are several investigations carried
out to effectively tackle large scale catastrophe of a plant and how real-time
systems must reorganize itself to respond optimally to changing scenarios to
reduce catastrophe and aid human intervention. The study presented here is in
this direction and the model accommodates catastrophe generated tasks while it
tries to minimize the total number of deadline miss, by dynamically scheduling
the unusual pattern of tasks. The problem is NP hard. We prove the methods for
an optimal scheduling algorithm. We also derive a model to maintain the
stability of the processes. Moreover, we study the problem of minimizing the
number of processors required for scheduling with a set of periodic and
sporadic hard real time tasks with primary/backup mechanism to achieve fault
tolerance. EDF scheduling algorithms are used on each processor to manage
scenario changes. Finally we present a simulation of super scheduler with
small, medium and large real time tasks pattern for catastrophe management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5407</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5407</id><created>2014-07-21</created><updated>2015-11-14</updated><authors><author><keyname>Spencer</keyname><forenames>J. S.</forenames></author><author><keyname>Blunt</keyname><forenames>N. S.</forenames></author><author><keyname>Vigor</keyname><forenames>W. A.</forenames></author><author><keyname>Malone</keyname><forenames>F. D.</forenames></author><author><keyname>Foulkes</keyname><forenames>W. M. C.</forenames></author><author><keyname>Shepherd</keyname><forenames>James J.</forenames></author><author><keyname>Thom</keyname><forenames>A. J. W.</forenames></author></authors><title>Open-source development experiences in scientific software: the HANDE
  quantum Monte Carlo project</title><categories>cs.SE</categories><comments>6 pages. Submission to WSSSPE2</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The HANDE quantum Monte Carlo project offers accessible stochastic algorithms
for general use for scientists in the field of quantum chemistry. HANDE is an
ambitious and general high-performance code developed by a
geographically-dispersed team with a variety of backgrounds in computational
science. In the course of preparing a public, open-source release, we have
taken this opportunity to step back and look at what we have done and what we
hope to do in the future. We pay particular attention to development processes,
the approach taken to train students joining the project, and how a flat
hierarchical structure aids communication
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5410</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5410</id><created>2014-07-21</created><updated>2014-11-10</updated><authors><author><keyname>Liu</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zhou</keyname><forenames>Zhe</forenames></author><author><keyname>Diao</keyname><forenames>Wenrui</forenames></author><author><keyname>Li</keyname><forenames>Zhou</forenames></author><author><keyname>Zhang</keyname><forenames>Kehuan</forenames></author></authors><title>An Empirical Study on Android for Saving Non-shared Data on Public
  Storage</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With millions of apps that can be downloaded from official or third-party
market, Android has become one of the most popular mobile platforms today.
These apps help people in all kinds of ways and thus have access to lots of
user's data that in general fall into three categories: sensitive data, data to
be shared with other apps, and non-sensitive data not to be shared with others.
For the first and second type of data, Android has provided very good storage
models: an app's private sensitive data are saved to its private folder that
can only be access by the app itself, and the data to be shared are saved to
public storage (either the external SD card or the emulated SD card area on
internal FLASH memory). But for the last type, i.e., an app's non-sensitive and
non-shared data, there is a big problem in Android's current storage model
which essentially encourages an app to save its non-sensitive data to shared
public storage that can be accessed by other apps. At first glance, it seems no
problem to do so, as those data are non-sensitive after all, but it implicitly
assumes that app developers could correctly identify all sensitive data and
prevent all possible information leakage from private-but-non-sensitive data.
In this paper, we will demonstrate that this is an invalid assumption with a
thorough survey on information leaks of those apps that had followed Android's
recommended storage model for non-sensitive data. Our studies showed that
highly sensitive information from billions of users can be easily hacked by
exploiting the mentioned problematic storage model. Although our empirical
studies are based on a limited set of apps, the identified problems are never
isolated or accidental bugs of those apps being investigated. On the contrary,
the problem is rooted from the vulnerable storage model recommended by Android.
To mitigate the threat, we also propose a defense framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5416</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5416</id><created>2014-07-21</created><authors><author><keyname>Marujo</keyname><forenames>Luis</forenames></author><author><keyname>Port&#xea;lo</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author><author><keyname>Trancoso</keyname><forenames>Isabel</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Privacy-Preserving Important Passage Retrieval</title><categories>cs.IR cs.CR</categories><comments>Secure Passage Retrieval, Important Passage Retrieval, KP-Centrality,
  Secure Binary Embeddings, Data Privacy, Automatic Key Phrase Extraction,
  Proceedings of SIGIR 2014 Workshop Privacy-Preserving IR: When Information
  Retrieval Meets Privacy and Security</comments><acm-class>H.3; I.2.7; K.4.1; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art important passage retrieval methods obtain very good
results, but do not take into account privacy issues. In this paper, we present
a privacy preserving method that relies on creating secure representations of
documents. Our approach allows for third parties to retrieve important passages
from documents without learning anything regarding their content. We use a
hashing scheme known as Secure Binary Embeddings to convert a key phrase and
bag-of-words representation to bit strings in a way that allows the computation
of approximate distances, instead of exact ones. Experiments show that our
secure system yield similar results to its non-private counterpart on both
clean text and noisy speech recognized text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5425</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5425</id><created>2014-07-21</created><authors><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Leonardos</keyname><forenames>Nikos</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author><author><keyname>Wang</keyname><forenames>Fengming</forenames></author></authors><title>Hellinger volume and number-on-the-forehead communication complexity</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-theoretic methods have proven to be a very powerful tool in
communication complexity, in particular giving an elegant proof of the linear
lower bound for the two-party disjointness function, and tight lower bounds on
disjointness in the multi-party number-in-the-hand (NIH) model. In this paper,
we study the applicability of information theoretic methods to the multi-party
number-on-the-forehead model (NOF), where determining the complexity of
disjointness remains an important open problem.
  There are two basic parts to the NIH disjointness lower bound: a direct sum
theorem and a lower bound on the one-bit AND function using a beautiful
connection between Hellinger distance and protocols revealed by Bar-Yossef,
Jayram, Kumar and Sivakumar [BYJKS04]. Inspired by this connection, we
introduce the notion of Hellinger volume. We show that it lower bounds the
information cost of multi-party NOF protocols and provide a small toolbox that
allows one to manipulate several Hellinger volume terms and lower bound a
Hellinger volume when the distributions involved satisfy certain conditions. In
doing so, we prove a new upper bound on the difference between the arithmetic
mean and the geometric mean in terms of relative entropy. We then apply these
new tools to obtain a lower bound on the informational complexity of the AND_k
function in the NOF setting. Finally, we discuss the difficulties of proving a
direct sum theorem for information cost in the NOF model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5442</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5442</id><created>2014-07-21</created><authors><author><keyname>Dhamal</keyname><forenames>Swapnil</forenames></author><author><keyname>Meghlan</keyname><forenames>Akanksha</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Cooperative Game Theoretic Solution Concepts for top-$k$ Problems</title><categories>cs.SI cs.GT physics.soc-ph</categories><comments>This is a work in progress. If you have any comments, suggestions, or
  doubts, please send an email to the first author. The first two authors have
  contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding the $k$ most critical nodes, referred to as the
$top\text{-}k$ problem, is a very important one in several contexts such as
information diffusion and preference aggregation in social networks, clustering
of data points, etc. It has been observed in the literature that the value
allotted to a node by most of the popular cooperative game theoretic solution
concepts, acts as a good measure of appropriateness of that node (or a data
point) to be included in the $top\text{-}k$ set, by itself. However, in
general, nodes having the highest $k$ values are not the desirable
$top\text{-}k$ nodes, because the appropriateness of a node to be a part of the
$top\text{-}k$ set depends on other nodes in the set. As this is not explicitly
captured by cooperative game theoretic solution concepts, it is necessary to
post-process the obtained values in order to output the suitable $top\text{-}k$
nodes. In this paper, we propose several such post-processing methods and give
reasoning behind each of them, and also propose a standalone algorithm that
combines cooperative game theoretic solution concepts with the popular greedy
hill-climbing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5444</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5444</id><created>2014-07-21</created><updated>2014-10-20</updated><authors><author><keyname>Arapinis</keyname><forenames>Myrto</forenames></author><author><keyname>Cheval</keyname><forenames>Vincent</forenames></author><author><keyname>Delaune</keyname><forenames>St&#xe9;phanie</forenames></author></authors><title>Composing security protocols: from confidentiality to privacy</title><categories>cs.CR</categories><comments>86 pages, 21 references</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security protocols are used in many of our daily-life applications, and our
privacy largely depends on their design. Formal verification techniques have
proved their usefulness to analyse these protocols, but they become so complex
that modular techniques have to be developed. We propose several results to
safely compose security protocols. We consider arbitrary primitives modeled
using an equational theory, and a rich process algebra close to the applied pi
calculus.
  Relying on these composition results, we are able to derive some security
properties on a protocol from the security analysis performed on each
sub-protocol individually. We consider parallel composition and the case of
key-exchange protocols. Our results apply to deal with confidentiality but also
privacy-type properties (e.g. anonymity, unlinkability) expressed using a
notion of equivalence. We illustrate the usefulness of our composition results
on protocols from the 3G phone application and electronic passport.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5447</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5447</id><created>2014-07-21</created><authors><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>Joint Channel Selection and Power Control in Infrastructureless Wireless
  Networks: A Multi-Player Multi-Armed Bandit Framework</title><categories>cs.GT</categories><doi>10.1109/TVT.2014.2369425</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of efficient resource allocation in dynamic
infrastructureless wireless networks. Assuming a reactive interference-limited
scenario, each transmitter is allowed to select one frequency channel (from a
common pool) together with a power level at each transmission trial; hence, for
all transmitters, not only the fading gain, but also the number of interfering
transmissions and their transmit powers are varying over time. Due to the
absence of a central controller and time-varying network characteristics, it is
highly inefficient for transmitters to acquire global channel and network
knowledge. Therefore a reasonable assumption is that transmitters have no
knowledge of fading gains, interference, and network topology. Each
transmitting node selfishly aims at maximizing its average reward (or
minimizing its average cost), which is a function of the action of that
specific transmitter as well as those of all other transmitters. This scenario
is modeled as a multi-player multi-armed adversarial bandit game, in which
multiple players receive an a priori unknown reward with an arbitrarily
time-varying distribution by sequentially pulling an arm, selected from a known
and finite set of arms. Since players do not know the arm with the highest
average reward in advance, they attempt to minimize their so-called regret,
determined by the set of players' actions, while attempting to achieve
equilibrium in some sense. To this end, we design in this paper two joint power
level and channel selection strategies. We prove that the gap between the
average reward achieved by our approaches and that based on the best fixed
strategy converges to zero asymptotically. Moreover, the empirical joint
frequencies of the game converge to the set of correlated equilibria. We
further characterize this set for two special cases of our designed game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5449</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5449</id><created>2014-07-21</created><authors><author><keyname>Tkachev</keyname><forenames>Ilya</forenames></author><author><keyname>Mereacre</keyname><forenames>Alexandru</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Quantitative model-checking of controlled discrete-time Markov processes</title><categories>math.PR cs.FL cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on optimizing probabilities of events of interest defined
over general controlled discrete-time Markov processes. It is shown that the
optimization over a wide class of $\omega$-regular properties can be reduced to
the solution of one of two fundamental problems: reachability and repeated
reachability. We provide a comprehensive study of the former problem and an
initial characterisation of the (much more involved) latter problem. A case
study elucidates concepts and techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5455</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5455</id><created>2014-07-21</created><updated>2014-07-29</updated><authors><author><keyname>Ferreri</keyname><forenames>Luca</forenames></author><author><keyname>Bajardi</keyname><forenames>Paolo</forenames></author><author><keyname>Giacobini</keyname><forenames>Mario</forenames></author><author><keyname>Perazzo</keyname><forenames>Silvia</forenames></author><author><keyname>Venturino</keyname><forenames>Ezio</forenames></author></authors><title>Interplay of network dynamics and ties heterogeneity on spreading
  dynamics</title><categories>physics.soc-ph cs.SI physics.bio-ph physics.data-an q-bio.PE</categories><comments>10 pages, 10 figures</comments><journal-ref>Physiscal Review E 90, 012812 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of a network dramatically affects the spreading phenomena
unfolding upon it. The contact distribution of the nodes has long been
recognized as the key ingredient in influencing the outbreak events. However,
limited knowledge is currently available on the role of the weight of the edges
on the persistence of a pathogen. At the same time, recent works showed a
strong influence of temporal network dynamics on disease spreading. In this
work we provide an analytical understanding, corroborated by numerical
simulations, about the conditions for infected stable state in weighted
networks. In particular, we reveal the role of heterogeneity of edge weights
and of the dynamic assignment of weights on the ties in the network in driving
the spread of the epidemic. In this context we show that when weights are
dynamically assigned to ties in the network an heterogeneous distribution is
able to hamper the diffusion of the disease, contrary to what happens when
weights are fixed in time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5456</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5456</id><created>2014-07-21</created><authors><author><keyname>Kaushik</keyname><forenames>Manju</forenames></author></authors><title>Research of Load Testing and Result Based on Loadrunner</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we made the plan of a load testing, and got results by means
of the LoadRunner which is an automatic load testing tool.We combined with the
characteristics of electronic commerce system and did the load testing and
analysis the result of load test by means of the LoadRunner. We fully described
the characteristics of the electronic commerce application, designed the
reasonable test cases,and simulated the practical scenario. In the process of
running Load Runner, we arranged the appropriate transactions and rendezvous,
and designed the truthful test network environment. The plan was applied to the
load testing phase of the telecommunication equipment sales system of special
products. We analyzed the load testing results, proposed the improving
measures, and realized the optimization of the telecommunication equipment
sales system and also found the defect of the system when the massive users
access the system and guided the system improvement using the test result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5483</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5483</id><created>2014-07-21</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>A RM-Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we propose a new hybrid code called &quot;RM-Polar&quot; codes. This new
codes are constructed by combining the construction of Reed-Muller (RM) code
and Polar code. It has much larger minimum Hamming distance than Polar codes,
therefore it has much better error performance than Polar codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5488</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5488</id><created>2014-07-21</created><authors><author><keyname>Paul</keyname><forenames>Satyabrata</forenames></author><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author><author><keyname>Pal</keyname><forenames>Anita</forenames></author></authors><title>L(2,1)-labelling of Circular-arc Graph</title><categories>cs.DM</categories><comments>12 pages</comments><journal-ref>Annals of Pure and Applied Mathematics, 5(2) (2014)208-219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An L(2,1)-labelling of a graph $G=(V, E)$ is $\lambda_{2,1}(G)$ a function
$f$ from the vertex set V (G) to the set of non-negative integers such that
adjacent vertices get numbers at least two apart, and vertices at distance two
get distinct numbers. The L(2,1)-labelling number denoted by $\lambda_{2,1}(G)$
of $G$ is the minimum range of labels over all such labelling. In this article,
it is shown that, for a circular-arc graph $G$, the upper bound of
$\lambda_{2,1}(G)$ is $\Delta+3\omega$, where $\Delta$ and $\omega$ represents
the maximum degree of the vertices and size of maximum clique respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5495</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5495</id><created>2014-07-21</created><updated>2014-07-22</updated><authors><author><keyname>Au</keyname><forenames>Kelvin</forenames></author><author><keyname>Zhang</keyname><forenames>Liqing</forenames></author><author><keyname>Nikopour</keyname><forenames>Hosein</forenames></author><author><keyname>Yi</keyname><forenames>Eric</forenames></author><author><keyname>Bayesteh</keyname><forenames>Alireza</forenames></author><author><keyname>Vilaipornsawai</keyname><forenames>Usa</forenames></author><author><keyname>Ma</keyname><forenames>Jianglei</forenames></author><author><keyname>Zhu</keyname><forenames>Peiying</forenames></author></authors><title>Uplink Contention Based SCMA for 5G Radio Access</title><categories>cs.IT math.IT</categories><comments>Submitted to Golobecom 5G workshop 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fifth generation (5G) wireless networks are expected to support very diverse
applications and terminals. Massive connectivity with a large number of devices
is an important requirement for 5G networks. Current LTE system is not able to
efficiently support massive connectivity, especially on the uplink (UL). Among
the issues arise due to massive connectivity is the cost of signaling overhead
and latency. In this paper, an uplink contention-based sparse code multiple
access (SCMA) design is proposed as a solution. First, the system design
aspects of the proposed multiple-access scheme are described. The SCMA
parameters can be adjusted to provide different levels of overloading, thus
suitable to meet the diverse traffic connectivity requirements. In addition,
the system-level evaluations of a small packet application scenario are
provided for contention-based UL SCMA. SCMA is compared to OFDMA in terms of
connectivity and drop rate under a tight latency requirement. The simulation
results demonstrate that contention-based SCMA can provide around 2.8 times
gain over contention-based OFDMA in terms of supported active users. The uplink
contention-based SCMA scheme can be a promising technology for 5G wireless
networks for data transmission with low signaling overhead, low delay, and
support of massive connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5513</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5513</id><created>2014-07-21</created><authors><author><keyname>Hur</keyname><forenames>Youngmi</forenames></author><author><keyname>Zheng</keyname><forenames>Fang</forenames></author></authors><title>Prime Coset Sum: A Systematic Method for Designing Multi-D Wavelet
  Filter Banks with Fast Algorithms</title><categories>math.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As constructing multi-D wavelets remains a challenging problem, we propose a
new method called prime coset sum to construct multi-D wavelets. Our method
provides a systematic way to construct multi-D non-separable wavelet filter
banks from two 1-D lowpass filters, with one of whom being interpolatory. Our
method has many important features including the following: 1) it works for any
spatial dimension, and any prime scalar dilation, 2) the vanishing moments of
the multi-D wavelet filter banks are guaranteed by certain properties of the
initial 1-D lowpass filters, and furthermore, 3) the resulting multi-D wavelet
filter banks are associated with fast algorithms that are faster than the
existing fast tensor product algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5514</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5514</id><created>2014-07-21</created><updated>2015-01-15</updated><authors><author><keyname>Dokmani&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>Scheibler</keyname><forenames>Robin</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Raking the Cocktail Party</title><categories>cs.SD cs.IT math.IT</categories><comments>12 pages, 11 figures, Accepted for publication in IEEE Journal on
  Selected Topics in Signal Processing (Special Issue on Spatial Audio)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the concept of an acoustic rake receiver---a microphone beamformer
that uses echoes to improve the noise and interference suppression. The rake
idea is well-known in wireless communications; it involves constructively
combining different multipath components that arrive at the receiver antennas.
Unlike spread-spectrum signals used in wireless communications, speech signals
are not orthogonal to their shifts. Therefore, we focus on the spatial
structure, rather than temporal. Instead of explicitly estimating the channel,
we create correspondences between early echoes in time and image sources in
space. These multiple sources of the desired and the interfering signal offer
additional spatial diversity that we can exploit in the beamformer design.
  We present several &quot;intuitive&quot; and optimal formulations of acoustic rake
receivers, and show theoretically and numerically that the rake formulation of
the maximum signal-to-interference-and-noise beamformer offers significant
performance boosts in terms of noise and interference suppression. Beyond
signal-to-noise ratio, we observe gains in terms of the \emph{perceptual
evaluation of speech quality} (PESQ) metric for the speech quality. We
accompany the paper by the complete simulation and processing chain written in
Python. The code and the sound samples are available online at
\url{http://lcav.github.io/AcousticRakeReceiver/}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5516</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5516</id><created>2014-07-21</created><updated>2015-09-18</updated><authors><author><keyname>Sorensen</keyname><forenames>D. C.</forenames></author><author><keyname>Embree</keyname><forenames>M.</forenames></author></authors><title>A DEIM Induced CUR Factorization</title><categories>math.NA cs.NA</categories><report-no>Rice University, Department of Computational and Applied Mathematics
  Report TR 14-04</report-no><msc-class>65F30, 15A23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a CUR matrix factorization based on the Discrete Empirical
Interpolation Method (DEIM). For a given matrix $A$, such a factorization
provides a low rank approximate decomposition of the form $A \approx C U R$,
where $C$ and $R$ are subsets of the columns and rows of $A$, and $U$ is
constructed to make $CUR$ a good approximation. Given a low-rank singular value
decomposition $A \approx V S W^T$, the DEIM procedure uses $V$ and $W$ to
select the columns and rows of $A$ that form $C$ and $R$. Through an error
analysis applicable to a general class of CUR factorizations, we show that the
accuracy tracks the optimal approximation error within a factor that depends on
the conditioning of submatrices of $V$ and $W$. For large-scale problems, $V$
and $W$ can be approximated using an incremental QR algorithm that makes one
pass through $A$. Numerical examples illustrate the favorable performance of
the DEIM-CUR method, compared to CUR approximations based on leverage scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5524</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5524</id><created>2014-07-21</created><authors><author><keyname>Givelberg</keyname><forenames>Edward</forenames></author></authors><title>Process-Oriented Parallel Programming with an Application to
  Data-Intensive Computing</title><categories>cs.PL cs.DC</categories><comments>20 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce process-oriented programming as a natural extension of
object-oriented programming for parallel computing. It is based on the
observation that every class of an object-oriented language can be instantiated
as a process, accessible via a remote pointer. The introduction of process
pointers requires no syntax extension, identifies processes with programming
objects, and enables processes to exchange information simply by executing
remote methods. Process-oriented programming is a high-level language
alternative to multithreading, MPI and many other languages, environments and
tools currently used for parallel computations. It implements natural
object-based parallelism using only minimal syntax extension of existing
languages, such as C++ and Python, and has therefore the potential to lead to
widespread adoption of parallel programming. We implemented a prototype system
for running processes using C++ with MPI and used it to compute a large
three-dimensional Fourier transform on a computer cluster built of commodity
hardware components. Three-dimensional Fourier transform is a prototype of a
data-intensive application with a complex data-access pattern. The
process-oriented code is only a few hundred lines long, and attains very high
data throughput by achieving massive parallelism and maximizing hardware
utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5527</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5527</id><created>2014-07-18</created><updated>2015-02-09</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Quality of Experience (QoE) beyond Quality of Service (QoS) as its
  baseline: QoE at the Interface of Experience Domains</title><categories>cs.HC cs.MM</categories><comments>31 pages, 3 figures, and 1 table. Working Paper WP-RFM-14-02</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a new approach to the definition of the quality of experience
is presented. By considering the quality of service as a baseline, that portion
of the QoE that can be inferred from the QoS is excluded, and then the rest of
the QoE is approached with the notion of QoE at a Boundary (QoEaaB). With the
QoEaaB as the core of the proposed approach, various potential boundaries, and
their associated unseen opportunities to improve the QoE are discussed. In
particular, property, contract, SLA, and content are explored in terms of their
boundaries and also their associated QoEaaB. With an interest in online video
delivery, management of resource sharing and isolation associated with
multi-tenant operations is considered. It is concluded that the proposed QoEaaB
can bring a new perspective in QoE modeling and assessment toward a more
enriched approach to improving the experience based on innovation and deep
connectivity among actors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5536</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5536</id><created>2014-07-21</created><updated>2014-07-21</updated><authors><author><keyname>Pawar</keyname><forenames>Kamlesh</forenames></author><author><keyname>Egan</keyname><forenames>Gary F.</forenames></author><author><keyname>Zhang</keyname><forenames>Jingxin</forenames></author></authors><title>Multichannel Compressive Sensing MRI Using Noiselet Encoding</title><categories>physics.med-ph cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The incoherence between measurement and sparsifying transform matrices and
the restricted isometry property (RIP) of measurement matrix are two of the key
factors in determining the performance of compressive sensing (CS). In CS-MRI,
the randomly under-sampled Fourier matrix is used as the measurement matrix and
the wavelet transform is usually used as sparsifying transform matrix. However,
the incoherence between the randomly under-sampled Fourier matrix and the
wavelet matrix is not optimal, which can deteriorate the performance of CS-MRI.
Using the mathematical result that noiselets are maximally incoherent with
wavelets, this paper introduces the noiselet unitary bases as the measurement
matrix to improve the incoherence and RIP in CS-MRI, and presents a method to
design the pulse sequence for the noiselet encoding. This novel encoding scheme
is combined with the multichannel compressive sensing (MCS) framework to take
the advantage of multichannel data acquisition used in MRI scanners. An
empirical RIP analysis is presented to compare the multichannel noiselet and
multichannel Fourier measurement matrices in MCS. Simulations are presented in
the MCS framework to compare the performance of noiselet encoding
reconstructions and Fourier encoding reconstructions at different acceleration
factors. The comparisons indicate that multichannel noiselet measurement matrix
has better RIP than that of its Fourier counterpart, and that noiselet encoded
MCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and
can achieve higher acceleration factors. To demonstrate the feasibility of the
proposed noiselet encoding scheme, two pulse sequences with tailored spatially
selective RF excitation pulses was designed and implemented on a 3T scanner to
acquire the data in the noiselet domain from a phantom and a human brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5537</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5537</id><created>2014-07-21</created><updated>2015-03-19</updated><authors><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Kulkarni</keyname><forenames>Mandar N.</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Tractable Model for Rate in Self-Backhauled Millimeter Wave Cellular
  Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmW) cellular systems will require high gain directional
antennas and dense base station (BS) deployments to overcome high near field
path loss and poor diffraction. As a desirable side effect, high gain antennas
provide interference isolation, providing an opportunity to incorporate
self-backhauling--BSs backhauling among themselves in a mesh architecture
without significant loss in throughput--to enable the requisite large BS
densities. The use of directional antennas and resource sharing between access
and backhaul links leads to coverage and rate trends that differ significantly
from conventional microwave ($\mu$W) cellular systems. In this paper, we
propose a general and tractable mmW cellular model capturing these key trends
and characterize the associated rate distribution. The developed model and
analysis is validated using actual building locations from dense urban settings
and empirically-derived path loss models. The analysis shows that in sharp
contrast to the interference limited nature of $\mu$W cellular networks, the
spectral efficiency of mmW networks (besides total rate) also increases with BS
density particularly at the cell edge. Increasing the system bandwidth,
although boosting median and peak rates, does not significantly influence the
cell edge rate. With self-backhauling, different combinations of the wired
backhaul fraction (i.e. the faction of BSs with a wired connection) and BS
density are shown to guarantee the same median rate (QoS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5547</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5547</id><created>2014-07-21</created><authors><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>State</keyname><forenames>Bogdan</forenames></author></authors><title>Reading the Source Code of Social Ties</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>10 pages, 8 figures, Proceedings of the 2014 ACM conference on Web
  (WebSci'14)</comments><acm-class>H.1.2</acm-class><doi>10.1145/2615569.2615672</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though online social network research has exploded during the past years, not
much thought has been given to the exploration of the nature of social links.
Online interactions have been interpreted as indicative of one social process
or another (e.g., status exchange or trust), often with little systematic
justification regarding the relation between observed data and theoretical
concept. Our research aims to breach this gap in computational social science
by proposing an unsupervised, parameter-free method to discover, with high
accuracy, the fundamental domains of interaction occurring in social networks.
By applying this method on two online datasets different by scope and type of
interaction (aNobii and Flickr) we observe the spontaneous emergence of three
domains of interaction representing the exchange of status, knowledge and
social support. By finding significant relations between the domains of
interaction and classic social network analysis issues (e.g., tie strength,
dyadic interaction over time) we show how the network of interactions induced
by the extracted domains can be used as a starting point for more nuanced
analysis of online social data that may one day incorporate the normative
grammar of social interaction. Our methods finds applications in online social
media services ranging from recommendation to visual link summarization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5553</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5553</id><created>2014-07-21</created><updated>2015-10-29</updated><authors><author><keyname>Ny</keyname><forenames>Jerome Le</forenames></author></authors><title>Privacy-Preserving Filtering for Event Streams</title><categories>cs.SY cs.DB</categories><comments>This version subsumes both the previous version and arXiv:1304.2313</comments><acm-class>C.3; G.3; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many large-scale information systems such as intelligent transportation
systems, smart grids or smart buildings collect data about the activities of
their users to optimize their operations. To encourage participation and
adoption of these systems, it is becoming increasingly important that the
design process take privacy issues into consideration. In a typical scenario,
signals originate from many sensors capturing events involving the users, and
several statistics of interest need to be continuously published in real-time.
This paper considers the problem of providing differential privacy guarantees
for such multi-input multi-output systems processing event streams. We show how
to construct and optimize various extensions of the zero-forcing equalization
mechanism, which we previously proposed for single-input single-output systems.
Some of these extensions can take a model of the input signals into account. We
illustrate our privacy-preserving filter design methodology through the problem
of privately monitoring and forecasting occupancy in a building equipped with
multiple motion detection sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5572</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5572</id><created>2014-07-21</created><updated>2015-08-20</updated><authors><author><keyname>Benammar</keyname><forenames>Meryem</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>Secrecy Capacity Region of Some Classes of Wiretap Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>19 pages, 8 figures, To appear in IEEE Trans. on Information Theory</comments><doi>10.1109/TIT.2015.2463279</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the secrecy capacity of the Wiretap Broadcast Channel
(WBC) with an external eavesdropper where a source wishes to communicate two
private messages over a Broadcast Channel (BC) while keeping them secret from
the eavesdropper. We derive a non-trivial outer bound on the secrecy capacity
region of this channel which, in absence of security constraints, reduces to
the best known outer bound to the capacity of the standard BC. An inner bound
is also derived which follows the behavior of both the best known inner bound
for the BC and the Wiretap Channel. These bounds are shown to be tight for the
deterministic BC with a general eavesdropper, the semi-deterministic BC with a
more-noisy eavesdropper and the Wiretap BC where users exhibit a less-noisiness
order between them. Finally, by rewriting our outer bound to encompass the
characteristics of parallel channels, we also derive the secrecy capacity
region of the product of two inversely less-noisy BCs with a more-noisy
eavesdropper. We illustrate our results by studying the impact of security
constraints on the capacity of the WBC with binary erasure (BEC) and binary
symmetric (BSC) components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5574</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5574</id><created>2014-07-21</created><authors><author><keyname>Kumar</keyname><forenames>Sandeep</forenames></author><author><keyname>Sharma</keyname><forenames>Vivek Kumar</forenames></author><author><keyname>Kumari</keyname><forenames>Rajani</forenames></author></authors><title>A Novel Hybrid Crossover based Artificial Bee Colony Algorithm for
  Optimization Problem</title><categories>cs.AI cs.NE</categories><doi>10.5120/14136-2266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial bee colony (ABC) algorithm has proved its importance in solving a
number of problems including engineering optimization problems. ABC algorithm
is one of the most popular and youngest member of the family of population
based nature inspired meta-heuristic swarm intelligence method. ABC has been
proved its superiority over some other Nature Inspired Algorithms (NIA) when
applied for both benchmark functions and real world problems. The performance
of search process of ABC depends on a random value which tries to balance
exploration and exploitation phase. In order to increase the performance it is
required to balance the exploration of search space and exploitation of optimal
solution of the ABC. This paper outlines a new hybrid of ABC algorithm with
Genetic Algorithm. The proposed method integrates crossover operation from
Genetic Algorithm (GA) with original ABC algorithm. The proposed method is
named as Crossover based ABC (CbABC). The CbABC strengthens the exploitation
phase of ABC as crossover enhances exploration of search space. The CbABC
tested over four standard benchmark functions and a popular continuous
optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5587</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5587</id><created>2014-07-21</created><updated>2015-06-28</updated><authors><author><keyname>Roux</keyname><forenames>Stephane Le</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Weihrauch degrees of finding equilibria in sequential games</title><categories>cs.LO cs.GT</categories><comments>An extended abstract of this work has appeared in the Proceedings of
  CiE 2015</comments><msc-class>03D30, 03E60, 03E15, 91A18, 91A44</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the degrees of non-computability (Weihrauch degrees) of finding
winning strategies (or more generally, Nash equilibria) in infinite sequential
games with certain winning sets (or more generally, outcome sets). In
particular, we show that as the complexity of the winning sets increases in the
difference hierarchy, the complexity of constructing winning strategies
increases in the effective Borel hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5593</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5593</id><created>2014-07-21</created><authors><author><keyname>Wallace</keyname><forenames>Tim</forenames></author><author><keyname>Sekmen</keyname><forenames>Ali</forenames></author></authors><title>Deterministic Versus Randomized Kaczmarz Iterative Projection</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kaczmarz's alternating projection method has been widely used for solving a
consistent (mostly over-determined) linear system of equations Ax=b. Because of
its simple iterative nature with light computation, this method was
successfully applied in computerized tomography. Since tomography generates a
matrix A with highly coherent rows, randomized Kaczmarz algorithm is expected
to provide faster convergence as it picks a row for each iteration at random,
based on a certain probability distribution. It was recently shown that picking
a row at random, proportional with its norm, makes the iteration converge
exponentially in expectation with a decay constant that depends on the scaled
condition number of A and not the number of equations. Since Kaczmarz's method
is a subspace projection method, the convergence rate for simple Kaczmarz
algorithm was developed in terms of subspace angles. This paper provides
analyses of simple and randomized Kaczmarz algorithms and explain the link
between them. It also propose new versions of randomization that may speed up
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5599</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5599</id><created>2014-07-21</created><updated>2015-09-10</updated><authors><author><keyname>Dai</keyname><forenames>Bo</forenames></author><author><keyname>Xie</keyname><forenames>Bo</forenames></author><author><keyname>He</keyname><forenames>Niao</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Raj</keyname><forenames>Anant</forenames></author><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Scalable Kernel Methods via Doubly Stochastic Gradients</title><categories>cs.LG stat.ML</categories><comments>32 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general perception is that kernel methods are not scalable, and neural
nets are the methods of choice for nonlinear learning problems. Or have we
simply not tried hard enough for kernel methods? Here we propose an approach
that scales up kernel methods using a novel concept called &quot;doubly stochastic
functional gradients&quot;. Our approach relies on the fact that many kernel methods
can be expressed as convex optimization problems, and we solve the problems by
making two unbiased stochastic approximations to the functional gradient, one
using random training points and another using random functions associated with
the kernel, and then descending using this noisy functional gradient. We show
that a function produced by this procedure after $t$ iterations converges to
the optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$,
and achieves a generalization performance of $O(1/\sqrt{t})$. This doubly
stochasticity also allows us to avoid keeping the support vectors and to
implement the algorithm in a small memory footprint, which is linear in number
of iterations and independent of data dimension. Our approach can readily scale
kernel methods up to the regimes which are dominated by neural nets. We show
that our method can achieve competitive performance to neural nets in datasets
such as 8 million handwritten digits from MNIST, 2.3 million energy materials
from MolecularSpace, and 1 million photos from ImageNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5609</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5609</id><created>2014-07-21</created><authors><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author><author><keyname>Pathak</keyname><forenames>Sudipta</forenames></author></authors><title>Efficient Algorithms for the Closest Pair Problem and Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The closest pair problem (CPP) is one of the well studied and fundamental
problems in computing. Given a set of points in a metric space, the problem is
to identify the pair of closest points. Another closely related problem is the
fixed radius nearest neighbors problem (FRNNP). Given a set of points and a
radius $R$, the problem is, for every input point $p$, to identify all the
other input points that are within a distance of $R$ from $p$. A naive
deterministic algorithm can solve these problems in quadratic time. CPP as well
as FRNNP play a vital role in computational biology, computational finance,
share market analysis, weather prediction, entomology, electro cardiograph,
N-body simulations, molecular simulations, etc. As a result, any improvements
made in solving CPP and FRNNP will have immediate implications for the solution
of numerous problems in these domains. We live in an era of big data and
processing these data take large amounts of time. Speeding up data processing
algorithms is thus much more essential now than ever before. In this paper we
present algorithms for CPP and FRNNP that improve (in theory and/or practice)
the best-known algorithms reported in the literature for CPP and FRNNP. These
algorithms also improve the best-known algorithms for related applications
including time series motif mining and the two locus problem in Genome Wide
Association Studies (GWAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5610</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5610</id><created>2014-07-21</created><authors><author><keyname>Gias</keyname><forenames>Alim Ul</forenames></author><author><keyname>Rahman</keyname><forenames>Rayhanur</forenames></author><author><keyname>Imran</keyname><forenames>Asif</forenames></author><author><keyname>Sakib</keyname><forenames>Kazi</forenames></author></authors><title>TFPaaS : Test-first Performance as a Service to Cloud for Software
  Testing Environment</title><categories>cs.SE</categories><journal-ref>International Journal of Web Applications, vol. 5, no. 4, pp.
  153-167, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance Testing is critical for applications like web services and
e-commerce platforms to ensure enhanced end user experience. In such cases,
starting to test the system's performance early should significantly reduce the
overall development cost. Test-first Performance (TFP) is one such paradigm
that allows performance testing right from the early stage of development.
Given such potential benefit, this paper proposes the design of a testing
framework IVRIDIO which introduces TFP as a Service (TFPaaS). IVRIDIO
incorporates the Plugin for TFP in the Cloud (PTFPC) aiming to provide instant
feedbacks - a prime requirement of TFP to immediately fix critical performance
issues. Furthermore, the Convention over Configuration (CoC) design paradigm
has been applied by introducing a configurable project template to maintain TFP
test cases. The prototyping details of the framework are given and the
variation of response time to the inclusion of PTFPC has also been discussed.
The Summated Usability Metric (SUM) score has been provided so that it can
later be used for comparing the PTFPC plugin's usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5616</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5616</id><created>2014-07-19</created><authors><author><keyname>Gholami</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author></authors><title>TW-TOA Based Positioning in the Presence of Clock Imperfections</title><categories>stat.AP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the positioning problem based on two-way time-of-arrival
(TW-TOA) measurements in asynchronous wireless sensor networks. Since the
optimal estimator for this problem involves difficult nonconvex optimization,
we propose two suboptimal estimators based on squared-range least squares and
least absolute mean of residual errors. The former approach is formulated as a
general trust region subproblem which can be solved exactly under mild
conditions. The latter approach is formulated as a difference of convex
functions programming (DCP), which can be solved using a concave-convex
procedure. Simulation results illustrate the high performance of the proposed
techniques, especially for the DCP approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5648</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5648</id><created>2014-07-21</created><updated>2014-09-07</updated><authors><author><keyname>Petre</keyname><forenames>Marian</forenames></author><author><keyname>Wilson</keyname><forenames>Greg</forenames></author></authors><title>Code Review For and By Scientists</title><categories>cs.SE</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe two pilot studies of code review by and for scientists. Our
principal findings are that scientists are enthusiastic, but need to be shown
code review in action, and that just-in-time review of small code changes is
more likely to succeed than large-scale end-of-work reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5656</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5656</id><created>2014-07-21</created><updated>2014-08-19</updated><authors><author><keyname>AlJadda</keyname><forenames>Khalifeh</forenames></author><author><keyname>Korayem</keyname><forenames>Mohammed</forenames></author><author><keyname>Ortiz</keyname><forenames>Camilo</forenames></author><author><keyname>Grainger</keyname><forenames>Trey</forenames></author><author><keyname>Miller</keyname><forenames>John A.</forenames></author><author><keyname>York</keyname><forenames>William S.</forenames></author></authors><title>PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical
  Data Problems</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the big data era, scalability has become a crucial requirement for any
useful computational model. Probabilistic graphical models are very useful for
mining and discovering data insights, but they are not scalable enough to be
suitable for big data problems. Bayesian Networks particularly demonstrate this
limitation when their data is represented using few random variables while each
random variable has a massive set of values. With hierarchical data - data that
is arranged in a treelike structure with several levels - one would expect to
see hundreds of thousands or millions of values distributed over even just a
small number of levels. When modeling this kind of hierarchical data across
large data sets, Bayesian networks become infeasible for representing the
probability distributions for the following reasons: i) Each level represents a
single random variable with hundreds of thousands of values, ii) The number of
levels is usually small, so there are also few random variables, and iii) The
structure of the network is predefined since the dependency is modeled top-down
from each parent to each of its child nodes, so the network would contain a
single linear path for the random variables from each parent to each child
node. In this paper we present a scalable probabilistic graphical model to
overcome these limitations for massive hierarchical data. We believe the
proposed model will lead to an easily-scalable, more readable, and expressive
implementation for problems that require probabilistic-based solutions for
massive amounts of hierarchical data. We successfully applied this model to
solve two different challenging probabilistic-based problems on massive
hierarchical data sets for different domains, namely, bioinformatics and latent
semantic discovery over search logs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5659</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5659</id><created>2014-07-21</created><updated>2014-08-26</updated><authors><author><keyname>Li</keyname><forenames>Congduan</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author><author><keyname>Walsh</keyname><forenames>John MacLaren</forenames></author></authors><title>Multilevel Diversity Coding Systems: Rate Regions, Codes, Computation, &amp;
  Forbidden Minors</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, 52 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate regions of multilevel diversity coding systems (MDCS), a sub-class
of the broader family of multi-source multi-sink networks with special
structure, are investigated. After showing how to enumerate all non-isomorphic
MDCS instances of a given size, the Shannon outer bound and several achievable
inner bounds based on linear codes are given for the rate region of each
non-isomorphic instance. For thousands of MDCS instances, the bounds match, and
hence exact rate regions are proven. Results gained from these computations are
summarized in key statistics involving aspects such as the sufficiency of
scalar binary codes, the necessary size of vector binary codes, etc. Also, it
is shown how to generate computer aided human readable converse proofs, as well
as how to construct the codes for an achievability proof. Based on this large
repository of rate regions, a series of results about general MDCS cases that
they inspired are introduced and proved. In particular, a series of embedding
operations that preserve the property of sufficiency of scalar or vector codes
are presented. The utility of these operations is demonstrated by boiling the
thousands of MDCS instances for which binary scalar codes are insufficient down
to 12 forbidden smallest embedded MDCS instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5661</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5661</id><created>2014-07-21</created><authors><author><keyname>Sawyer</keyname><forenames>Scott M.</forenames></author><author><keyname>O'Gwynn</keyname><forenames>B. David</forenames></author></authors><title>Evaluating Accumulo Performance for a Scalable Cyber Data Processing
  Pipeline</title><categories>cs.DB</categories><comments>To appear at 2014 IEEE High Performance Extreme Computing Conference
  (HPEC '14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming, big data applications face challenges in creating scalable data
flow pipelines, in which multiple data streams must be collected, stored,
queried, and analyzed. These data sources are characterized by their volume (in
terms of dataset size), velocity (in terms of data rates), and variety (in
terms of fields and types). For many applications, distributed NoSQL databases
are effective alternatives to traditional relational database management
systems. This paper considers a cyber situational awareness system that uses
the Apache Accumulo database to provide scalable data warehousing, real-time
data ingest, and responsive querying for human users and analytic algorithms.
We evaluate Accumulo's ingestion scalability as a function of number of client
processes and servers. We also describe a flexible data model with effective
techniques for query planning and query batching to deliver responsive results.
Query performance is evaluated in terms of latency of the client receiving
initial result sets. Accumulo performance is measured on a database of up to 8
nodes using real cyber data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5670</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5670</id><created>2014-07-21</created><authors><author><keyname>Poss</keyname><forenames>Raphael</forenames></author></authors><title>Rust for functional programmers</title><categories>cs.PL</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides an introduction to Rust, a systems language by Mozilla,
to programmers already familiar with Haskell, OCaml or other functional
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5674</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5674</id><created>2014-07-21</created><authors><author><keyname>Bhowmick</keyname><forenames>Santanu</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author><author><keyname>Xue</keyname><forenames>Shi-Ke</forenames></author></authors><title>A Constant-Factor Approximation for Multi-Covering with Disks</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider variants of the following multi-covering problem with disks. We
are given two point sets $Y$ (servers) and $X$ (clients) in the plane, a
coverage function $\kappa :X \rightarrow \mathcal{N}$, and a constant $\alpha
\geq 1$. Centered at each server is a single disk whose radius we are free to
set. The requirement is that each client $x \in X$ be covered by at least
$\kappa(x)$ of the server disks. The objective function we wish to minimize is
the sum of the $\alpha$-th powers of the disk radii. We present a polynomial
time algorithm for this problem achieving an $O(1)$ approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5699</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5699</id><created>2014-07-21</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Chai</keyname><forenames>Bo</forenames></author><author><keyname>Smith</keyname><forenames>David B.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Feasibility of Using Discriminate Pricing Schemes for Energy Trading in
  Smart Grid</title><categories>cs.SY</categories><comments>7 pages, 4 figures, 3 tables, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the feasibility of using a discriminate pricing
scheme to offset the inconvenience that is experienced by an energy user (EU)
in trading its energy with an energy controller in smart grid. The main
objective is to encourage EUs with small distributed energy resources (DERs),
or with high sensitivity to their inconvenience, to take part in the energy
trading via providing incentive to them with relatively higher payment at the
same time as reducing the total cost to the energy controller. The proposed
scheme is modeled through a two-stage Stackelberg game that describes the
energy trading between a shared facility authority (SFA) and EUs in a smart
community. A suitable cost function is proposed for the SFA to leverage the
generation of discriminate pricing according to the inconvenience experienced
by each EU. It is shown that the game has a unique sub-game perfect equilibrium
(SPE), under the certain condition at which the SFA's total cost is minimized,
and that each EU receives its best utility according to its associated
inconvenience for the given price. A backward induction technique is used to
derive a closed form expression for the price function at SPE, and thus the
dependency of price on an EU's different decision parameters is explained for
the studied system. Numerical examples are provided to show the beneficial
properties of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5701</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5701</id><created>2014-07-21</created><authors><author><keyname>Hanwell</keyname><forenames>Marcus D.</forenames></author><author><keyname>O'Leary</keyname><forenames>Patrick</forenames></author><author><keyname>O'Bara</keyname><forenames>Bob</forenames></author></authors><title>Sustainable Software Ecosystems: Software Engineers, Domain Scientists,
  and Engineers Collaborating for Science</title><categories>cs.SE</categories><comments>4 pages, submission for WSSSPE 2</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The development of scientific software is often a partnership between domain
scientists and scientific software engineers. It is especially important to
embrace these collaborations when developing advanced scientific software,
where sustainability, reproducibility, and extensibility are important. In the
ideal case, as discussed in this manuscript, this brings together teams
composed of the world's foremost scientific experts in a given field with
seasoned software developers experienced in forming highly collaborative teams
working on software to further scientific research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5711</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5711</id><created>2014-07-21</created><updated>2014-09-02</updated><authors><author><keyname>Xie</keyname><forenames>Ronggui</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author><author><keyname>Chen</keyname><forenames>Xiaohui</forenames></author></authors><title>A Novel Uplink Data Transmission Scheme For Small Packets In Massive
  MIMO System</title><categories>cs.IT math.IT</categories><comments>IEEE/CIC ICCC 2014 Symposium on Signal Processing for Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent terminals often produce a large number of data packets of small
lengths. For these packets, it is inefficient to follow the conventional medium
access control (MAC) protocols because they lead to poor utilization of service
resources. We propose a novel multiple access scheme that targets massive
multiple-input multiple-output (MIMO) systems based on compressive sensing
(CS). We employ block precoding in the time domain to enable the simultaneous
transmissions of many users, which could be even more than the number of
receive antennas at the base station. We develop a block-sparse system model
and adopt the block orthogonal matching pursuit (BOMP) algorithm to recover the
transmitted signals. Conditions for data recovery guarantees are identified and
numerical results demonstrate that our scheme is efficient for uplink small
packet transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5714</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5714</id><created>2014-07-21</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Gladyshev</keyname><forenames>Pavel</forenames></author></authors><title>Automated Inference of Past Action Instances in Digital Investigations</title><categories>cs.CR</categories><comments>International Journal of Information Security</comments><doi>10.1007/s10207-014-0249-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the amount of digital devices suspected of containing digital evidence
increases, case backlogs for digital investigations are also increasing in many
organizations. To ensure timely investigation of requests, this work proposes
the use of signature-based methods for automated action instance approximation
to automatically reconstruct past user activities within a compromised or
suspect system. This work specifically explores how multiple instances of a
user action may be detected using signature-based methods during a post-mortem
digital forensic analysis. A system is formally defined as a set of objects,
where a subset of objects may be altered on the occurrence of an action. A
novel action-trace update time threshold is proposed that enables objects to be
categorized by their respective update patterns over time. By integrating time
into event reconstruction, the most recent action instance approximation as
well as limited past instances of the action may be differentiated and their
time values approximated. After the formal theory if signature-based event
reconstruction is defined, a case study is given to evaluate the practicality
of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5716</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5716</id><created>2014-07-21</created><authors><author><keyname>Adhikary</keyname><forenames>Ansuman</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Massive-MIMO Meets HetNet: Interference Coordination Through Spatial
  Blanking</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the downlink performance of a heterogeneous cellular
network (HetNet) where both macro and small cells share the same spectrum and
hence interfere with each other. We assume that the users are concentrated at
certain areas in the cell, i.e., they form hotspots. While some of the hotspots
are assumed to have a small cell in their vicinity, the others are directly
served by the macrocell. Due to a relatively small area of each hotspot, the
users lying in a particular hotspot appear to be almost co-located to the
macrocells, which are typically deployed at some elevation. Assuming large
number of antennas at the macrocell, we exploit this directionality in the
channel vectors to obtain spatial blanking, i.e., concentrating transmission
energy only in certain directions while creating transmission opportunities for
the small cells lying in the other directions. In addition to this inherent
interference suppression, we also develop three low-complexity interference
coordination strategies: (i) turn off small cells based on the amount of
cross-tier interference they receive or cause to the scheduled macrocell
hotspots, (ii) schedule hotspots such that treating interference as noise is
approximately optimal for the resulting Gaussian interference channel, and
(iii) offload some of the macrocell hotspots to nearby small cells in order to
improve throughput fairness across all hotspots. For all these schemes, we
study the relative merits and demerits of uniform deployment of small cells vs.
deploying more small cells towards the cell center or the cell edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5718</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5718</id><created>2014-07-21</created><authors><author><keyname>Malekshan</keyname><forenames>K. Rahimi</forenames></author><author><keyname>Lahouti</keyname><forenames>F.</forenames></author></authors><title>Distributed Cross-layer Dynamic Route Selection in Wireless Multiuser
  Multihop Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Transaction on Wireless Comunications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless ad-hoc networks, forwarding data through intermediate relays
extends the coverage area and enhances the network throughput. We consider a
general wireless multiuser multihop transmission, where each data flow is
subject to a constraint on the end-to-end buffering delay and the associated
packet drop rate as a quality of service (QoS) requirement. The objective is to
maximize the weighted sum-rate between source destination pairs, while the
corresponding QoS requirements are satisfied. We introduce two new distributed
cross-layer dynamic route selection schemes in this setting that are designed
involving physical, MAC, and network layers. In the proposed opportunistic
cross-layer dynamic route selection scheme, routes are assigned dynamically
based on the state of network nodes' buffers and the instantaneous state of
fading channels. In the same setting, the proposed time division cross layer
dynamic route selection scheme utilizes the average quality of channels instead
for more efficient implementation. Detailed results and comparisons are
provided, which demonstrate the superior performance of the proposed
cross-layer dynamic route selection schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5719</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5719</id><created>2014-07-21</created><authors><author><keyname>Taylor</keyname><forenames>Tim</forenames></author></authors><title>Artificial Life and the Web: WebAL Comes of Age</title><categories>cs.NE cs.MA</categories><comments>Presented at WebAL-1: Workshop on Artificial Life and the Web 2014
  (arXiv:1406.2507)</comments><report-no>WebAL1/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A brief survey is presented of the first 18 years of web-based Artificial
Life (&quot;WebAL&quot;) research and applications, covering the period 1995-2013. The
survey is followed by a short discussion of common methodologies employed and
current technologies relevant to WebAL research. The paper concludes with a
quick look at what the future may hold for work in this exciting area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5732</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5732</id><created>2014-07-22</created><authors><author><keyname>Gupta</keyname><forenames>Sonali</forenames></author><author><keyname>Bhatia</keyname><forenames>Komal Kumar</forenames></author></authors><title>A Comparative Study of Hidden Web Crawlers</title><categories>cs.IR</categories><comments>8 pages, 8 figures</comments><journal-ref>Vol 12 number 3 , Jun 2014 V12(3):111-118</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large amount of data on the WWW remains inaccessible to crawlers of Web
search engines because it can only be exposed on demand as users fill out and
submit forms. The Hidden web refers to the collection of Web data which can be
accessed by the crawler only through an interaction with the Web-based search
form and not simply by traversing hyperlinks. Research on Hidden Web has
emerged almost a decade ago with the main line being exploring ways to access
the content in online databases that are usually hidden behind search forms.
The efforts in the area mainly focus on designing hidden Web crawlers that
focus on learning forms and filling them with meaningful values. The paper
gives an insight into the various Hidden Web crawlers developed for the purpose
giving a mention to the advantages and shortcoming of the techniques employed
in each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5736</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5736</id><created>2014-07-22</created><authors><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Arbel&#xe1;ez</keyname><forenames>Pablo</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Learning Rich Features from RGB-D Images for Object Detection and
  Segmentation</title><categories>cs.CV cs.RO</categories><comments>To appear in the European Conference on Computer Vision (ECCV), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of object detection for RGB-D images using
semantically rich image and depth features. We propose a new geocentric
embedding for depth images that encodes height above ground and angle with
gravity for each pixel in addition to the horizontal disparity. We demonstrate
that this geocentric embedding works better than using raw depth images for
learning feature representations with convolutional neural networks. Our final
object detection system achieves an average precision of 37.3%, which is a 56%
relative improvement over existing methods. We then focus on the task of
instance segmentation where we label pixels belonging to object instances found
by our detector. For this task, we propose a decision forest approach that
classifies pixels in the detection window as foreground or background using a
family of unary and binary tests that query shape and geocentric pose features.
Finally, we use the output from our object detectors in an existing superpixel
classification framework for semantic scene segmentation and achieve a 24%
relative improvement over current state-of-the-art for the object categories
that we study. We believe advances such as those represented in this paper will
facilitate the use of perception in fields like robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5739</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5739</id><created>2014-07-22</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Nguyen</keyname><forenames>Trung Thanh</forenames></author><author><keyname>Nguyen</keyname><forenames>Hoang Linh</forenames></author></authors><title>Global optimization using L\'evy flights</title><categories>cs.NE</categories><comments>12 pages, 6 figures, 4 algorithms,Proceedings of Second National
  Symposium on Research, Development and Application of Information and
  Communication Technology (ICT.rda'04), Hanoi, Sept 24-25, 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a class of enhanced diffusion processes in which random
walkers perform L\'evy flights and apply it for global optimization. L\'evy
flights offer controlled balance between exploitation and exploration. We
develop four optimization algorithms based on such properties. We compare new
algorithms with the well-known Simulated Annealing on hard test functions and
the results are very promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5747</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5747</id><created>2014-07-22</created><authors><author><keyname>Hosseini</keyname><forenames>Kianoush</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj S.</forenames></author></authors><title>Large-Scale MIMO versus Network MIMO for Multicell Interference
  Mitigation</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures; IEEE Journal of Selected Topics in Signal
  Processing, Special Issue on Signal Processing for Large-Scale MIMO
  Communications</comments><doi>10.1109/JSTSP.2014.2327594</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares two important downlink multicell interference mitigation
techniques, namely, large-scale (LS) multiple-input multiple-output (MIMO) and
network MIMO. We consider a cooperative wireless cellular system operating in
time-division duplex (TDD) mode, wherein each cooperating cluster includes $B$
base-stations (BSs), each equipped with multiple antennas and scheduling $K$
single-antenna users. In an LS-MIMO system, each BS employs $BM$ antennas not
only to serve its scheduled users, but also to null out interference caused to
the other users within the cooperating cluster using zero-forcing (ZF)
beamforming. In a network MIMO system, each BS is equipped with only $M$
antennas, but interference cancellation is realized by data and channel state
information exchange over the backhaul links and joint downlink transmission
using ZF beamforming. Both systems are able to completely eliminate
intra-cluster interference and to provide the same number of spatial degrees of
freedom per user. Assuming the uplink-downlink channel reciprocity provided by
TDD, both systems are subject to identical channel acquisition overhead during
the uplink pilot transmission stage. Further, the available sum power at each
cluster is fixed and assumed to be equally distributed across the downlink
beams in both systems. Building upon the channel distribution functions and
using tools from stochastic ordering, this paper shows, however, that from a
performance point of view, users experience better quality of service, averaged
over small-scale fading, under an LS-MIMO system than a network MIMO system.
Numerical simulations for a multicell network reveal that this conclusion also
holds true with regularized ZF beamforming scheme. Hence, given the likely
lower cost of adding excess number of antennas at each BS, LS-MIMO could be the
preferred route toward interference mitigation in cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5750</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5750</id><created>2014-07-22</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>Fibonacci Heaps Revisited</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fibonacci heap is a classic data structure that supports deletions in
logarithmic amortized time and all other heap operations in O(1) amortized
time. We explore the design space of this data structure. We propose a version
with the following improvements over the original: (i) Each heap is represented
by a single heap-ordered tree, instead of a set of trees. (ii) Each
decrease-key operation does only one cut and a cascade of rank changes, instead
of doing a cascade of cuts. (iii) The outcomes of all comparisons done by the
algorithm are explicitly represented in the data structure, so none are wasted.
We also give an example to show that without cascading cuts or rank changes,
both the original data structure and the new version fail to have the desired
efficiency, solving an open problem of Fredman. Finally, we illustrate the
richness of the design space by proposing several alternative ways to do
cascading rank changes, including a randomized strategy related to one
previously proposed by Karger. We leave the analysis of these alternatives as
intriguing open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5753</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5753</id><created>2014-07-22</created><authors><author><keyname>Kumar</keyname><forenames>Sandeep</forenames></author><author><keyname>Sharma</keyname><forenames>Vivek Kumar</forenames></author><author><keyname>Kumari</keyname><forenames>Rajani</forenames></author></authors><title>Improved Onlooker Bee Phase in Artificial Bee Colony Algorithm</title><categories>cs.NE</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1407.5574</comments><doi>10.5120/15579-4304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Bee Colony (ABC) is a distinguished optimization strategy that can
resolve nonlinear and multifaceted problems. It is comparatively a
straightforward and modern population based probabilistic approach for
comprehensive optimization. In the vein of the other population based
algorithms, ABC is moreover computationally classy due to its slow nature of
search procedure. The solution exploration equation of ABC is extensively
influenced by a arbitrary quantity which helps in exploration at the cost of
exploitation of the better search space. In the solution exploration equation
of ABC due to the outsized step size the chance of skipping the factual
solution is high. Therefore, here this paper improve onlooker bee phase with
help of a local search strategy inspired by memetic algorithm to balance the
diversity and convergence capability of the ABC. The proposed algorithm is
named as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 well
known un-biased test problems of diverse complexities and two engineering
optimization problems; results show that the anticipated algorithm go one
better than the basic ABC and its recent deviations in a good number of the
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5754</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5754</id><created>2014-07-22</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Tree-based iterated local search for Markov random fields with
  applications in image analysis</title><categories>cs.AI cs.CV math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{maximum a posteriori} (MAP) assignment for general structure Markov
random fields (MRFs) is computationally intractable. In this paper, we exploit
tree-based methods to efficiently address this problem. Our novel method, named
Tree-based Iterated Local Search (T-ILS) takes advantage of the tractability of
tree-structures embedded within MRFs to derive strong local search in an ILS
framework. The method efficiently explores exponentially large neighborhood and
does so with limited memory without any requirement on the cost functions. We
evaluate the T-ILS in a simulation of Ising model and two real-world problems
in computer vision: stereo matching, image denoising. Experimental results
demonstrate that our methods are competitive against state-of-the-art rivals
with a significant computational gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5759</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5759</id><created>2014-07-22</created><authors><author><keyname>Fortun</keyname><forenames>Denis</forenames><affiliation>INRIA</affiliation></author><author><keyname>Bouthemy</keyname><forenames>Patrick</forenames><affiliation>INRIA</affiliation></author><author><keyname>Kervrann</keyname><forenames>Charles</forenames><affiliation>INRIA</affiliation></author></authors><title>Aggregation of local parametric candidates with exemplar-based occlusion
  handling for optical flow</title><categories>cs.CV</categories><comments>Submission,IEEE Transactions on Image Processing (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handling all together large displacements, motion details and occlusions
remains an open issue for reliable computation of optical flow in a video
sequence. We propose a two-step aggregation paradigm to address this problem.
The idea is to supply local motion candidates at every pixel in a first step,
and then to combine them to determine the global optical flow field in a second
step. We exploit local parametric estimations combined with patch
correspondences and we experimentally demonstrate that they are sufficient to
produce highly accurate motion candidates. The aggregation step is designed as
the discrete optimization of a global regularized energy. The occlusion map is
estimated jointly with the flow field throughout the two steps. We propose a
generic exemplar-based approach for occlusion filling with motion vectors. We
achieve state-of-the-art results in computer vision benchmarks, with
particularly significant improvements in the case of large displacements and
occlusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5762</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5762</id><created>2014-07-22</created><updated>2015-02-01</updated><authors><author><keyname>Smith</keyname><forenames>Graeme</forenames></author><author><keyname>Sanders</keyname><forenames>J. W.</forenames></author><author><keyname>Li</keyname><forenames>Qin</forenames></author></authors><title>A macro-level model for investigating the effect of directional bias on
  network coverage</title><categories>cs.DC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random walks have been proposed as a simple method of efficiently searching,
or disseminating information throughout, communication and sensor networks. In
nature, animals (such as ants) tend to follow correlated random walks, i.e.,
random walks that are biased towards their current heading. In this paper, we
investigate whether or not complementing random walks with directional bias can
decrease the expected discovery and coverage times in networks.
  To do so, we develop a macro-level model of a directionally biased random
walk based on Markov chains. By focussing on regular, connected networks, the
model allows us to efficiently calculate expected coverage times for different
network sizes and biases. Our analysis shows that directional bias can
significantly reduce coverage time, but only when the bias is below a certain
value which is dependent on the network size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5764</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5764</id><created>2014-07-22</created><authors><author><keyname>Truyen</keyname><forenames>Tran The</forenames></author><author><keyname>Phung</keyname><forenames>Dinh Q.</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Preference Networks: Probabilistic Models for Recommendation Systems</title><categories>cs.IR cs.SI stat.AP</categories><comments>In Proc. of 6th Australasian Data Mining Conference (AusDM), Gold
  Coast, Australia, pages 195--202, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems are important to help users select relevant and
personalised information over massive amounts of data available. We propose an
unified framework called Preference Network (PN) that jointly models various
types of domain knowledge for the task of recommendation. The PN is a
probabilistic model that systematically combines both content-based filtering
and collaborative filtering into a single conditional Markov random field. Once
estimated, it serves as a probabilistic database that supports various useful
queries such as rating prediction and top-$N$ recommendation. To handle the
challenging problem of learning large networks of users and items, we employ a
simple but effective pseudo-likelihood with regularisation. Experiments on the
movie rating data demonstrate the merits of the PN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5773</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5773</id><created>2014-07-22</created><authors><author><keyname>Lee</keyname><forenames>Kwankyu</forenames></author></authors><title>Decoding of Differential AG Codes</title><categories>cs.IT math.IT</categories><comments>10 pages; submitted to a journal</comments><msc-class>94B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interpolation-based decoding that was developed for general evaluation AG
codes is shown to be equally applicable to general differential AG codes. A
performance analysis of the decoding algorithm, which is parallel to that of
its companion algorithm, is reported. In particular, the decoding capacities of
evaluation AG codes and differential AG codes are seen to be nicely
interrelated. As an interesting special case, a decoding algorithm for
classical Goppa codes is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5776</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5776</id><created>2014-07-22</created><authors><author><keyname>Kim</keyname><forenames>Na-Rae</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Symbol interval optimization for molecular communication with drift</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a symbol interval optimization algorithm in
molecular communication with drift. Proper symbol intervals are important in
practical communication systems since information needs to be sent as fast as
possible with low error rates. There is a trade-off, however, between symbol
intervals and inter-symbol interference (ISI) from Brownian motion. Thus, we
find proper symbol interval values considering the ISI inside two kinds of
blood vessels, and also suggest no ISI system for strong drift models. Finally,
an isomer-based molecule shift keying (IMoSK) is applied to calculate
achievable data transmission rates (achievable rates, hereafter). Normalized
achievable rates are also obtained and compared in one-symbol ISI and no ISI
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5783</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5783</id><created>2014-07-22</created><authors><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Andriyanova</keyname><forenames>Iryna</forenames></author><author><keyname>Piemontese</keyname><forenames>Amina</forenames></author></authors><title>Proving Threshold Saturation for Nonbinary SC-LDPC Codes on the Binary
  Erasure Channel</title><categories>cs.IT math.IT</categories><comments>in Proc. 2014 XXXIth URSI General Assembly and Scientific Symposium,
  URSI GASS, Beijing, China, August 16-23, 2014. Invited paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze nonbinary spatially-coupled low-density parity-check (SC-LDPC)
codes built on the general linear group for transmission over the binary
erasure channel. We prove threshold saturation of the belief propagation
decoding to the potential threshold, by generalizing the proof technique based
on potential functions recently introduced by Yedla et al.. The existence of
the potential function is also discussed for a vector sparse system in the
general case, and some existence conditions are developed. We finally give
density evolution and simulation results for several nonbinary SC-LDPC code
ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5807</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5807</id><created>2014-07-22</created><authors><author><keyname>Carron</keyname><forenames>Andrea</forenames></author><author><keyname>Todescato</keyname><forenames>Marco</forenames></author><author><keyname>Carli</keyname><forenames>Ruggero</forenames></author><author><keyname>Schenato</keyname><forenames>Luca</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author></authors><title>Multi-agents adaptive estimation and coverage control using Gaussian
  regression</title><categories>cs.MA cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a scenario where the aim of a group of agents is to perform the
optimal coverage of a region according to a sensory function. In particular,
centroidal Voronoi partitions have to be computed. The difficulty of the task
is that the sensory function is unknown and has to be reconstructed on line
from noisy measurements. Hence, estimation and coverage needs to be performed
at the same time. We cast the problem in a Bayesian regression framework, where
the sensory function is seen as a Gaussian random field. Then, we design a set
of control inputs which try to well balance coverage and estimation, also
discussing convergence properties of the algorithm. Numerical experiments show
the effectivness of the new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5813</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5813</id><created>2014-07-22</created><updated>2014-09-26</updated><authors><author><keyname>Qian</keyname><forenames>Xiangjun</forenames></author><author><keyname>Gregoire</keyname><forenames>Jean</forenames></author><author><keyname>Moutarde</keyname><forenames>Fabien</forenames></author><author><keyname>De La Fortelle</keyname><forenames>Arnaud</forenames></author></authors><title>Priority-based coordination of autonomous and legacy vehicles at
  intersection</title><categories>cs.RO cs.SY</categories><comments>put in other preprint server</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, researchers have proposed various autonomous intersection
management techniques that enable autonomous vehicles to cross the intersection
without traffic lights or stop signs. In particular, a priority-based
coordination system with provable collision-free and deadlock-free features has
been presented. In this paper, we extend the priority-based approach to support
legacy vehicles without compromising above-mentioned features. We make the
hypothesis that legacy vehicles are able to keep a safe distance from their
leading vehicles. Then we explore some special configurations of system that
ensures the safe crossing of legacy vehicles. We implement the extended system
in a realistic traffic simulator SUMO. Simulations are performed to demonstrate
the safety of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5819</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5819</id><created>2014-07-22</created><authors><author><keyname>Furusawa</keyname><forenames>Hitoshi</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Concurrent Dynamic Algebra</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reconstruct Peleg's concurrent dynamic logic in the context of modal
Kleene algebras. We explore the algebraic structure of its multirelational
semantics and develop an abstract axiomatisation of concurrent dynamic algebras
from that basis. In this axiomatisation, sequential composition is not
associative. It interacts with concurrent composition through a weak
distributivity law. The modal operators of concurrent dynamic algebra are
obtained from abstract axioms for domain and antidomain operators; the Kleene
star is modelled as a least fixpoint. Algebraic variants of Peleg's axioms are
shown to be valid in these algebras and their soundness is proved relative to
the multirelational model. Additional results include iteration principles for
the Kleene star and a refutation of variants of Segerberg's axiom in the
multirelational setting. The most important results have been verified formally
with Isabelle/HOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5820</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5820</id><created>2014-07-22</created><authors><author><keyname>Blomberg</keyname><forenames>Niclas</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>Approximate Regularization Path for Nuclear Norm Based H2 Model
  Reduction</title><categories>cs.SY math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns model reduction of dynamical systems using the nuclear
norm of the Hankel matrix to make a trade-off between model fit and model
complexity. This results in a convex optimization problem where this trade-off
is determined by one crucial design parameter. The main contribution is a
methodology to approximately calculate all solutions up to a certain tolerance
to the model reduction problem as a function of the design parameter. This is
called the regularization path in sparse estimation and is a very important
tool in order to find the appropriate balance between fit and complexity. We
extend this to the more complicated nuclear norm case. The key idea is to
determine when to exactly calculate the optimal solution using an upper bound
based on the so-called duality gap. Hence, by solving a fixed number of
optimization problems the whole regularization path up to a given tolerance can
be efficiently computed. We illustrate this approach on some numerical
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5833</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5833</id><created>2014-07-22</created><updated>2015-04-22</updated><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Lagoutte</keyname><forenames>Aur&#xe9;lie</forenames></author><author><keyname>Li</keyname><forenames>Zhentao</forenames></author><author><keyname>Parreau</keyname><forenames>Aline</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>Identifying codes in hereditary classes of graphs and VC-dimension</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An identifying code of a graph is a subset of its vertices such that every
vertex of the graph is uniquely identified by the set of its neighbours within
the code. We show a dichotomy for the size of the smallest identifying code in
classes of graphs closed under induced subgraphs. Our dichotomy is derived from
the VC-dimension of the considered class C, that is the maximum VC-dimension
over the hypergraphs formed by the closed neighbourhoods of elements of C. We
show that hereditary classes with infinite VC-dimension have infinitely many
graphs with an identifying code of size logarithmic in the number of vertices
while classes with finite VC-dimension have a polynomial lower bound.
  We then turn to approximation algorithms. We show that the problem of finding
a smallest identifying code in a given graph from some class is log-APX-hard
for any hereditary class of infinite VC-dimension. For hereditary classes of
finite VC-dimension, the only known previous results show that we can
approximate the identifying code problem within a constant factor in some
particular classes, e.g. line graphs, planar graphs and unit interval graphs.
We prove that it can be approximate within a factor 6 for interval graphs. In
contrast, we show that on C_4-free bipartite graphs (a class of finite
VC-dimension) it cannot be approximated to within a factor of c.log(|V|) for
some c&gt;0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5841</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5841</id><created>2014-07-22</created><updated>2014-07-27</updated><authors><author><keyname>Mousavi</keyname><forenames>Hamoon</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Mechanical Proofs of Properties of the Tribonacci Word</title><categories>cs.FL cs.DM math.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1406.0670</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We implement a decision procedure for answering questions about a class of
infinite words that might be called (for lack of a better name)
&quot;Tribonacci-automatic&quot;. This class includes, for example, the famous Tribonacci
word T = 0102010010202 ..., the fixed point of the morphism 0 -&gt; 01, 1 -&gt; 02, 2
-&gt; 0. We use it to reprove some old results about the Tribonacci word from the
literature, such as assertions about the occurrences in T of squares, cubes,
palindromes, and so forth. We also obtain some new results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5856</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5856</id><created>2014-07-22</created><authors><author><keyname>Villaverde</keyname><forenames>Alejandro F</forenames></author><author><keyname>Henriques</keyname><forenames>David</forenames></author><author><keyname>Smallbone</keyname><forenames>Kieran</forenames></author><author><keyname>Bongard</keyname><forenames>Sophia</forenames></author><author><keyname>Schmid</keyname><forenames>Joachim</forenames></author><author><keyname>Cicin-Sain</keyname><forenames>Damjan</forenames></author><author><keyname>Crombach</keyname><forenames>Anton</forenames></author><author><keyname>Saez-Rodriguez</keyname><forenames>Julio</forenames></author><author><keyname>Mauch</keyname><forenames>Klaus</forenames></author><author><keyname>Balsa-Canto</keyname><forenames>Eva</forenames></author><author><keyname>Mendes</keyname><forenames>Pedro</forenames></author><author><keyname>Jaeger</keyname><forenames>Johannes</forenames></author><author><keyname>Banga</keyname><forenames>Julio R</forenames></author></authors><title>BioPreDyn-bench: benchmark problems for kinetic modelling in systems
  biology</title><categories>q-bio.QM cs.CE q-bio.MN</categories><msc-class>92-08</msc-class><acm-class>G.1.6; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic modelling is one of the cornerstones of systems biology. Many
research efforts are currently being invested in the development and
exploitation of large-scale kinetic models. The associated problems of
parameter estimation (model calibration) and optimal experimental design are
particularly challenging. The community has already developed many methods and
software packages which aim to facilitate these tasks. However, there is a lack
of suitable benchmark problems which allow a fair and systematic evaluation and
comparison of these contributions. Here we present BioPreDyn-bench, a set of
challenging parameter estimation problems which aspire to serve as reference
test cases in this area. This set comprises six problems including medium and
large-scale kinetic models of the bacterium E. coli, baker's yeast S.
cerevisiae, the vinegar fly D. melanogaster, Chinese Hamster Ovary cells, and a
generic signal transduction network. The level of description includes
metabolism, transcription, signal transduction, and development. For each
problem we provide (i) a basic description and formulation, (ii)
implementations ready-to-run in several formats, (iii) computational results
obtained with specific solvers, (iv) a basic analysis and interpretation. This
suite of benchmark problems can be readily used to evaluate and compare
parameter estimation methods. Further, it can also be used to build test
problems for sensitivity and identifiability analysis, model reduction and
optimal experimental design methods. The suite, including codes and
documentation, can be freely downloaded from
http://www.iim.csic.es/%7egingproc/biopredynbench/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5878</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5878</id><created>2014-07-22</created><authors><author><keyname>Soeken</keyname><forenames>Mathias</forenames></author><author><keyname>Abdessaied</keyname><forenames>Nabila</forenames></author><author><keyname>Drechsler</keyname><forenames>Rolf</forenames></author></authors><title>A framework for reversible circuit complexity</title><categories>cs.ET quant-ph</categories><comments>6 pages, 4 figures, accepted for Int'l Workshop on Boolean Problems
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible single-target gates are a generalization of Toffoli gates which
are a helpful formal representation for the description of synthesis algorithms
but are too general for an actual implementation based on some technology.
There is an exponential lower bound on the number of Toffoli gates required to
implement any reversible function, however, there is also a linear upper bound
on the number of single-target gates which can be proven using a constructive
proof based on a former presented synthesis algorithm. Since single-target
gates can be mapped to a cascade of Toffoli gates, this synthesis algorithm
provides an interesting framework for reversible circuit complexity. The paper
motivates this framework and illustrates first possible applications based on
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5883</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5883</id><created>2014-07-22</created><authors><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Wang</keyname><forenames>Ligong</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author></authors><title>Toward Photon-Efficient Key Distribution over Optical Channels</title><categories>cs.IT math.IT quant-ph</categories><comments>In IEEE Transactions on Information Theory; same version except that
  labels are corrected for Schemes S-1, S-2, and S-3, which appear as S-3, S-4,
  and S-5 in the Transactions</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 60, No. 8, pp. 4958
  - 4972, Aug. 2014</journal-ref><doi>10.1109/TIT.2014.2331060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the distribution of a secret key over an optical
(bosonic) channel in the regime of high photon efficiency, i.e., when the
number of secret key bits generated per detected photon is high. While in
principle the photon efficiency is unbounded, there is an inherent tradeoff
between this efficiency and the key generation rate (with respect to the
channel bandwidth). We derive asymptotic expressions for the optimal generation
rates in the photon-efficient limit, and propose schemes that approach these
limits up to certain approximations. The schemes are practical, in the sense
that they use coherent or temporally-entangled optical states and direct
photodetection, all of which are reasonably easy to realize in practice, in
conjunction with off-the-shelf classical codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5889</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5889</id><created>2014-07-22</created><authors><author><keyname>Saini</keyname><forenames>Anish</forenames></author><author><keyname>Mishra</keyname><forenames>Atul</forenames></author></authors><title>Domain-partitioned element management systems employing mobile agents
  for distributed network management</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1202.1941</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network management systems based on mobile agents are efficiently a better
alternative than typical client/server based architectures. Centralized
management models like SNMP or CMIP based management models suffer from
scalability and flexibility issues which are addressed to great extent by flat
bed or static mid-level manager models based on mobile agents, yet the use of
mobile agents to distribute and delegate management tasks for above stated
agent-based management frameworks like initial flat bed models and static
mid-level managers cannot efficiently meet the demands of current networks
which are growing in size and complexity. In view of the above mentioned
limitations, we proposed a domain partitioned network management model based-on
mobile agent &amp; Element Management Systems in order to minimize management data
flow to a centralized server. Intelligent agent allocated to specific EMS
performs local network management and reports the results to the superior
manager and finally the global manager performs global network management using
those submitted management results. Experimental results of various scenarios
of the proposed model have been presented to support the arguments given in
favor of the prototype system based on mobile agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5891</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5891</id><created>2014-07-22</created><authors><author><keyname>Nussbaumer</keyname><forenames>Alexander</forenames><affiliation>Knowledge Technologies Institute, Graz University of Technology, Austria</affiliation></author><author><keyname>Kravcik</keyname><forenames>Milos</forenames><affiliation>ACIS Group, Informatik 5, RWTH Aachen University, Germany</affiliation></author><author><keyname>Renzel</keyname><forenames>Dominik</forenames><affiliation>ACIS Group, Informatik 5, RWTH Aachen University, Germany</affiliation></author><author><keyname>Klamma</keyname><forenames>Ralf</forenames><affiliation>ACIS Group, Informatik 5, RWTH Aachen University, Germany</affiliation></author><author><keyname>Berthold</keyname><forenames>Marcel</forenames><affiliation>Department of Psychology, University of Graz, Austria</affiliation></author><author><keyname>Albert</keyname><forenames>Dietrich</forenames><affiliation>Knowledge Technologies Institute, Graz University of Technology, Austria</affiliation><affiliation>Department of Psychology, University of Graz, Austria</affiliation></author></authors><title>A Framework for Facilitating Self-Regulation in Responsive Open Learning
  Environments</title><categories>cs.CY</categories><acm-class>H.3.5; H.5.3; K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies have shown that the application of Self-Regulated Learning (SRL)
increases the effectiveness of education. However, this is quite challenging to
be facilitated with learning technologies like Learning Management Systems
(LMS) that lack an individualised approach as well as a right balance between
the learner's freedom and guidance. Personalisation and adaptive technologies
have a high potential to support SRL in Personal Learning Environments (PLE),
which enable customisation and guidance of various strengths and at various
levels with SRL widgets. The main contribution of our paper is a framework that
integrates guidance and reflection support for SRL in PLEs. Therefore, we have
elaborated an operational SRL model. On that basis we have implemented a system
with a learner model, SRL widgets, monitoring and analytic tools, as well as
recommendation functionalities. We present concrete examples from both informal
and formal learning settings. Moreover, we present analytic results from our
SRL system - lab experiments and a public installation. With such a complex
setting we are coming close to the realisation of Responsive Open Learning
Environments (ROLE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5896</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5896</id><created>2014-07-22</created><authors><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames></author></authors><title>Complexity Bounds for Ordinal-Based Termination</title><categories>cs.LO</categories><comments>Invited talk at the 8th International Workshop on Reachability
  Problems (RP 2014, 22-24 September 2014, Oxford)</comments><acm-class>F.2.0; F.3.1</acm-class><journal-ref>Proceedings of RP 2014, Lecture Notes in Computer Science 8762,
  pp. 1--19, 2014</journal-ref><doi>10.1007/978-3-319-11439-2_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  `What more than its truth do we know if we have a proof of a theorem in a
given formal system?' We examine Kreisel's question in the particular context
of program termination proofs, with an eye to deriving complexity bounds on
program running times.
  Our main tool for this are length function theorems, which provide complexity
bounds on the use of well quasi orders. We illustrate how to prove such
theorems in the simple yet until now untreated case of ordinals. We show how to
apply this new theorem to derive complexity bounds on programs when they are
proven to terminate thanks to a ranking function into some ordinal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5902</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5902</id><created>2014-07-22</created><authors><author><keyname>Murugesan</keyname><forenames>N.</forenames></author><author><keyname>Sundaram</keyname><forenames>O. V. Shanmuga</forenames></author></authors><title>Some Properties of Brzozowski Derivatives of Regular Expressions</title><categories>cs.FL</categories><comments>5 pages</comments><msc-class>68Q45, 68Q70</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Brzozowski derivatives of a regular expression are developed for constructing
deterministic automata from the given regular expression in the algebraic way.
In this paper,some lemmas of the regular expressions are discussed and the
regular languages of the derivatives are illustrated. Also the generalizations
of the Brzozowski derivatives are proved as theorems with help of properties
and known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5903</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5903</id><created>2014-07-22</created><updated>2014-07-29</updated><authors><author><keyname>Ortega</keyname><forenames>Felipe</forenames></author><author><keyname>Convertino</keyname><forenames>Gregorio</forenames></author><author><keyname>Zancanaro</keyname><forenames>Massimo</forenames></author><author><keyname>Piccardi</keyname><forenames>Tiziano</forenames></author></authors><title>Assessing the Performance of Question-and-Answer Communities Using
  Survival Analysis</title><categories>cs.HC cs.CY</categories><comments>10 pages, 3 figures, example code</comments><acm-class>H.3.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Question-&amp;-Answer (QA) websites have emerged as efficient platforms for
knowledge sharing and problem solving. In particular, the Stack Exchange
platform includes some of the most popular QA communities to date, such as
Stack Overflow. Initial metrics used to assess the performance of these
communities include summative statistics like the percentage of resolved
questions or the average time to receive and validate correct answers. However,
more advanced methods for longitudinal data analysis can provide further
insights on the QA process, by enabling identification of key predictive
factors and systematic comparison of performance across different QA
communities. In this paper, we apply survival analysis to a selection of
communities from the Stack Exchange platform. We illustrate the advantages of
using the proposed methodology to characterize and evaluate the performance of
QA communities, and then point to some implications for the design and
management of QA platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5908</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5908</id><created>2014-07-19</created><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author></authors><title>Exploiting Smoothness in Statistical Learning, Sequential Prediction,
  and Stochastic Optimization</title><categories>cs.LG</categories><comments>Ph.D. Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last several years, the intimate connection between convex
optimization and learning problems, in both statistical and sequential
frameworks, has shifted the focus of algorithmic machine learning to examine
this interplay. In particular, on one hand, this intertwinement brings forward
new challenges in reassessment of the performance of learning algorithms
including generalization and regret bounds under the assumptions imposed by
convexity such as analytical properties of loss functions (e.g., Lipschitzness,
strong convexity, and smoothness). On the other hand, emergence of datasets of
an unprecedented size, demands the development of novel and more efficient
optimization algorithms to tackle large-scale learning problems.
  The overarching goal of this thesis is to reassess the smoothness of loss
functions in statistical learning, sequential prediction/online learning, and
stochastic optimization and explicate its consequences. In particular we
examine how smoothness of loss function could be beneficial or detrimental in
these settings in terms of sample complexity, statistical consistency, regret
analysis, and convergence rate, and investigate how smoothness can be leveraged
to devise more efficient learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5910</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5910</id><created>2014-07-22</created><updated>2015-06-17</updated><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Reilly</keyname><forenames>John</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author><author><keyname>Li</keyname><forenames>Xue</forenames></author><author><keyname>Soar</keyname><forenames>Jeffrey</forenames></author></authors><title>Affect Sensing on Smartphone - Possibilities of Understanding Cognitive
  Decline in Aging Population</title><categories>cs.CY cs.HC</categories><comments>This paper has been withdrawn due to some conceptual error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to increasing sensing capacity, smartphones offer unprecedented
opportunity to monitor human health. Affect sensing is one such essential
monitoring that can be achieved on smartphones. Information about affect can be
useful for many modern applications. In particular, it can be potentially used
for understanding cognitive decline in aging population. In this paper we
present an overview of the existing literature that offer affect sensing on
smartphone platform. Most importantly, we present the challenges that need to
be addressed to make affect sensing on smartphone a reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5917</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5917</id><created>2014-07-22</created><updated>2015-12-12</updated><authors><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Prencipe</keyname><forenames>Giuseppe</forenames></author><author><keyname>Santoro</keyname><forenames>Nicola</forenames></author><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Distributed Computing by Mobile Robots: Uniform Circle Formation</title><categories>cs.DC cs.CG cs.MA</categories><comments>76 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of $n\neq 4$ simple autonomous mobile robots (asynchronous, no
common coordinate system, no identities, no central coordination, no direct
communication, no memory of the past, deterministic) initially in distinct
locations, moving freely in the plane and able to sense the positions of the
other robots. We study the primitive task of the robots arranging themselves on
the vertices of a regular $n$-gon not fixed in advance (Uniform Circle
Formation). In the literature, the existing algorithmic contributions are
limited to restricted sets of initial configurations of the robots and to more
powerful robots. The question of whether such simple robots could
deterministically form a uniform circle has remained open. In this paper, we
constructively prove that indeed the Uniform Circle Formation problem is
solvable for any initial configuration of the robots without any additional
assumption. In addition to closing a long-standing problem, the result of this
paper also implies that, for pattern formation, asynchrony is not a
computational handicap, and that additional powers such as chirality and
rigidity are computationally irrelevant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5931</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5931</id><created>2014-07-21</created><authors><author><keyname>Kumar</keyname><forenames>Sandeep</forenames></author><author><keyname>Jadon</keyname><forenames>Pooja</forenames></author></authors><title>A Novel Hybrid Algorithm for Permutation Flow Shop Scheduling</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present scenario the recent engineering and industrial built-up units
are facing hodgepodge of problems in a lot of aspects such as machining time,
electricity, man power, raw material and customers constraints. The job-shop
scheduling is one of the most significant industrial behaviours, particularly
in manufacturing planning. This paper proposes the permutation flow shop
sequencing problem with the objective of makespan minimization using the new
modified proposed method of johnsons algorithm as well as the guptas heuristic
algorithm. This paper involves the determination of the order of processing of
n jobs in m machines. Although since the problem is known to be np-hard for
three or more machines, that produces near optimal solution of the given
problem. The proposed method is very simple and easy to understand followed by
a numerical illustration is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5947</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5947</id><created>2014-07-22</created><authors><author><keyname>Banelli</keyname><forenames>Paolo</forenames></author><author><keyname>Buzzi</keyname><forenames>Stefano</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Modenini</keyname><forenames>Andrea</forenames></author><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author><author><keyname>Ugolini</keyname><forenames>Alessandro</forenames></author></authors><title>Modulation Formats and Waveforms for the Physical Layer of 5G Wireless
  Networks: Who Will be the Heir of OFDM?</title><categories>cs.IT math.IT</categories><comments>to appear IEEE Signal Processing Magazine, special issue on Signal
  Processing for the 5G Revolution, November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  5G cellular communications promise to deliver the gigabit experience to
mobile users, with a capacity increase of up to three orders of magnitude with
respect to current LTE systems. There is widespread agreement that such an
ambitious goal will be realized through a combination of innovative techniques
involving different network layers. At the physical layer, the OFDM modulation
format, along with its multiple-access strategy OFDMA, is not taken for
granted, and several alternatives promising larger values of spectral
efficiency are being considered. This paper provides a review of some
modulation formats suited for 5G, enriched by a comparative analysis of their
performance in a cellular environment, and by a discussion on their
interactions with specific 5G ingredients. The interaction with a massive MIMO
system is also discussed by employing real channel measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5949</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5949</id><created>2014-07-22</created><updated>2014-12-18</updated><authors><author><keyname>Prasad</keyname><forenames>Sharat C.</forenames></author><author><keyname>Prasad</keyname><forenames>Piyush</forenames></author></authors><title>Deep Recurrent Neural Networks for Time Series Prediction</title><categories>cs.NE</categories><comments>Preliminary, submitted to IEEE TNNLS</comments><msc-class>62M45, 82C32, 92B20</msc-class><acm-class>C.1.3; F.1.1; I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ability of deep networks to extract high level features and of recurrent
networks to perform time-series inference have been studied. In view of
universality of one hidden layer network at approximating functions under weak
constraints, the benefit of multiple layers is to enlarge the space of
dynamical systems approximated or, given the space, reduce the number of units
required for a certain error. Traditionally shallow networks with manually
engineered features are used, back-propagation extent is limited to one and
attempt to choose a large number of hidden units to satisfy the Markov
condition is made. In case of Markov models, it has been shown that many
systems need to be modeled as higher order. In the present work, we present
deep recurrent networks with longer backpropagation through time extent as a
solution to modeling systems that are high order and to predicting ahead. We
study epileptic seizure suppression electro-stimulator. Extraction of manually
engineered complex features and prediction employing them has not allowed small
low-power implementations as, to avoid possibility of surgery, extraction of
any features that may be required has to be included. In this solution, a
recurrent neural network performs both feature extraction and prediction. We
prove analytically that adding hidden layers or increasing backpropagation
extent increases the rate of decrease of approximation error. A Dynamic
Programming (DP) training procedure employing matrix operations is derived. DP
and use of matrix operations makes the procedure efficient particularly when
using data-parallel computing. The simulation studies show the geometry of the
parameter space, that the network learns the temporal structure, that
parameters converge while model output displays same dynamic behavior as the
system and greater than .99 Average Detection Rate on all real seizure data
tried.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5953</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5953</id><created>2014-07-22</created><authors><author><keyname>Enge</keyname><forenames>Andreas</forenames><affiliation>INRIA Bordeaux - Sud-Ouest, IMB</affiliation></author><author><keyname>Milan</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>INRIA Futurs</affiliation></author></authors><title>Implementing cryptographic pairings at standard security levels</title><categories>math.NT cs.CR cs.MS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study reports on an implementation of cryptographic pairings in a
general purpose computer algebra system. For security levels equivalent to the
different AES flavours, we exhibit suitable curves in parametric families and
show that optimal ate and twisted ate pairings exist and can be efficiently
evaluated. We provide a correct description of Miller's algorithm for signed
binary expansions such as the NAF and extend a recent variant due to Boxall et
al. to addition-subtraction chains. We analyse and compare several algorithms
proposed in the literature for the final exponentiation. Finally, we ive
recommendations on which curve and pairing to choose at each security level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5961</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5961</id><created>2014-07-21</created><authors><author><keyname>Brenguier</keyname><forenames>Romain</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author></authors><title>AbsSynthe: abstract synthesis from succinct safety specifications</title><categories>cs.LO</categories><comments>In Proceedings SYNT 2014, arXiv:1407.4937</comments><proxy>EPTCS</proxy><acm-class>F.3.1,B.1.2,B.6.3</acm-class><journal-ref>EPTCS 157, 2014, pp. 100-116</journal-ref><doi>10.4204/EPTCS.157.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a synthesis algorithm for safety specifications
described as circuits. Our algorithm is based on fixpoint computations,
abstraction and refinement, it uses binary decision diagrams as symbolic data
structure. We evaluate our tool on the benchmarks provided by the organizers of
the synthesis competition organized within the SYNT'14 workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5965</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5965</id><created>2014-07-22</created><authors><author><keyname>Smith</keyname><forenames>Steven Thomas</forenames></author></authors><title>Optimization Techniques on Riemannian Manifolds</title><categories>math.OC cs.CG cs.NA math.DG math.DS</categories><comments>Hamiltonian and Gradient Flows, Algorithms, and Control, Fields
  Institute Communications, Volume 3, AMS (1994)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The techniques and analysis presented in this paper provide new methods to
solve optimization problems posed on Riemannian manifolds. A new point of view
is offered for the solution of constrained optimization problems. Some
classical optimization techniques on Euclidean space are generalized to
Riemannian manifolds. Several algorithms are presented and their convergence
properties are analyzed employing the Riemannian structure of the manifold.
Specifically, two apparently new algorithms, which can be thought of as
Newton's method and the conjugate gradient method on Riemannian manifolds, are
presented and shown to possess, respectively, quadratic and superlinear
convergence. Examples of each method on certain Riemannian manifolds are given
with the results of numerical experiments. Rayleigh's quotient defined on the
sphere is one example. It is shown that Newton's method applied to this
function converges cubically, and that the Rayleigh quotient iteration is an
efficient approximation of Newton's method. The Riemannian version of the
conjugate gradient method applied to this function gives a new algorithm for
finding the eigenvectors corresponding to the extreme eigenvalues of a
symmetric matrix. Another example arises from extremizing the function
$\mathop{\rm tr} {\Theta}^{\scriptscriptstyle\rm T}Q{\Theta}N$ on the special
orthogonal group. In a similar example, it is shown that Newton's method
applied to the sum of the squares of the off-diagonal entries of a symmetric
matrix converges cubically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5976</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5976</id><created>2014-07-22</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Stieger</keyname><forenames>James</forenames></author><author><keyname>Burns</keyname><forenames>Joseph E.</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Detection of Sclerotic Spine Metastases via Random Aggregation of Deep
  Convolutional Neural Network Classifications</title><categories>cs.CV</categories><comments>This paper will be presented at &quot;Computational Methods and Clinical
  Applications for Spine Imaging&quot; workshop held in conjunction with MICCAI 2014</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Automated detection of sclerotic metastases (bone lesions) in Computed
Tomography (CT) images has potential to be an important tool in clinical
practice and research. State-of-the-art methods show performance of 79%
sensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume.
We design a two-tiered coarse-to-fine cascade framework to first operate a
highly sensitive candidate generation system at a maximum sensitivity of ~92%
but with high FP level (~50 per patient). Regions of interest (ROI) for lesion
candidates are generated in this step and function as input for the second
tier. In the second tier we generate N 2D views, via scale, random
translations, and rotations with respect to each ROI centroid coordinates.
These random views are used to train a deep Convolutional Neural Network (CNN)
classifier. In testing, the CNN is employed to assign individual probabilities
for a new set of N random views that are averaged at each ROI to compute a
final per-candidate classification probability. This second tier behaves as a
highly selective process to reject difficult false positives while preserving
high sensitivities. We validate the approach on CT images of 59 patients (49
with sclerotic metastases and 10 normal controls). The proposed method reduces
the number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing a
sensitivity rates of 60%, 70%, and 80% respectively in testing. The
Area-Under-the-Curve (AUC) is 0.834. The results show marked improvement upon
previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5978</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5978</id><created>2014-07-22</created><updated>2014-07-24</updated><authors><author><keyname>Marangoni-Simonsen</keyname><forenames>David</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Sequential Changepoint Approach for Online Community Detection</title><categories>stat.ML cs.LG cs.SI math.ST stat.TH</categories><comments>Submitted to 2014 INFORMS Workshop on Data Mining and Analytics and
  an IEEE journal</comments><doi>10.1109/LSP.2014.2381553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new algorithms for detecting the emergence of a community in large
networks from sequential observations. The networks are modeled using
Erdos-Renyi random graphs with edges forming between nodes in the community
with higher probability. Based on statistical changepoint detection
methodology, we develop three algorithms: the Exhaustive Search (ES), the
mixture, and the Hierarchical Mixture (H-Mix) methods. Performance of these
methods is evaluated by the average run length (ARL), which captures the
frequency of false alarms, and the detection delay. Numerical comparisons show
that the ES method performs the best; however, it is exponentially complex. The
mixture method is polynomially complex by exploiting the fact that the size of
the community is typically small in a large network. However, it may react to a
group of active edges that do not form a community. This issue is resolved by
the H-Mix method, which is based on a dendrogram decomposition of the network.
We present an asymptotic analytical expression for ARL of the mixture method
when the threshold is large. Numerical simulation verifies that our
approximation is accurate even in the non-asymptotic regime. Hence, it can be
used to determine a desired threshold efficiently. Finally, numerical examples
show that the mixture and the H-Mix methods can both detect a community quickly
with a lower complexity than the ES method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5981</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5981</id><created>2014-07-22</created><updated>2014-09-16</updated><authors><author><keyname>Crick</keyname><forenames>Tom</forenames></author><author><keyname>Hall</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Ishtiaq</keyname><forenames>Samin</forenames></author></authors><title>&quot;Can I Implement Your Algorithm?&quot;: A Model for Reproducible Research
  Software</title><categories>cs.SE cs.CE</categories><comments>Accepted for the 2nd Workshop on Sustainable Software for Science:
  Practice and Experiences (WSSSPE2); 5 pages, LaTeX</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The reproduction and replication of novel results has become a major issue
for a number of scientific disciplines. In computer science and related
computational disciplines such as systems biology, the issues closely revolve
around the ability to implement novel algorithms and approaches. Taking an
approach from the literature and applying it to a new codebase frequently
requires local knowledge missing from the published manuscripts and project
websites. Alongside this issue, benchmarking, and the development of fair ---
and widely available --- benchmark sets present another barrier.
  In this paper, we outline several suggestions to address these issues, driven
by specific examples from a range of scientific domains. Finally, based on
these suggestions, we propose a new open platform for scientific software
development which effectively isolates specific dependencies from the
individual researcher and their workstation and allows faster, more powerful
sharing of the results of scientific software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.5988</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.5988</id><created>2014-07-22</created><authors><author><keyname>Slavnov</keyname><forenames>Sergey</forenames></author></authors><title>Linear logic with idempotent exponential modalities: a note</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we discuss a variant of linear logic with idempotent exponential
modalities. We propose a sequent calculus system and discuss its semantics. We
also give a concrete relational model for this calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6019</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6019</id><created>2014-07-22</created><authors><author><keyname>Moro</keyname><forenames>Nicolas</forenames><affiliation>LIP6, SAS-ENSMSE, CTReg</affiliation></author><author><keyname>Heydemann</keyname><forenames>Karine</forenames><affiliation>LIP6</affiliation></author><author><keyname>Dehbaoui</keyname><forenames>Amine</forenames><affiliation>SERMA</affiliation></author><author><keyname>Robisson</keyname><forenames>Bruno</forenames><affiliation>SAS-ENSMSE, CTReg</affiliation></author><author><keyname>Encrenaz</keyname><forenames>Emmanuelle</forenames><affiliation>LIP6</affiliation></author></authors><title>Experimental evaluation of two software countermeasures against fault
  attacks</title><categories>cs.CR</categories><comments>6 pages, 2014 IEEE International Symposium on Hardware-Oriented
  Security and Trust (HOST), Arlington : United States (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Injection of transient faults can be used as a way to attack embedded
systems. On embedded processors such as microcontrollers, several studies
showed that such a transient fault injection with glitches or electromagnetic
pulses could corrupt either the data loads from the memory or the assembly
instructions executed by the circuit. Some countermeasure schemes which rely on
temporal redundancy have been proposed to handle this issue. Among them,
several schemes add this redundancy at assembly instruction level. In this
paper, we perform a practical evaluation for two of those countermeasure
schemes by using a pulsed electromagnetic fault injection process on a 32-bit
microcontroller. We provide some necessary conditions for an efficient
implementation of those countermeasure schemes in practice. We also evaluate
their efficiency and highlight their limitations. To the best of our knowledge,
no experimental evaluation of the security of such instruction-level
countermeasure schemes has been published yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6027</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6027</id><created>2014-07-22</created><authors><author><keyname>Besana</keyname><forenames>Alberto</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Cristina</forenames></author></authors><title>Modeling languages from graph networks</title><categories>cs.CL math.CO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model and compute the probability distribution of the letters in random
generated words in a language by using the theory of set partitions, Young
tableaux and graph theoretical representation methods. This has been of
interest for several application areas such as network systems, bioinformatics,
internet search, data mining and computacional linguistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6034</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6034</id><created>2014-07-22</created><authors><author><keyname>Meyfroyt</keyname><forenames>Thomas M. M.</forenames></author><author><keyname>Borst</keyname><forenames>Sem C.</forenames></author><author><keyname>Boxma</keyname><forenames>Onno J.</forenames></author><author><keyname>Denteneer</keyname><forenames>Dee</forenames></author></authors><title>Data Dissemination Performance in Large-Scale Sensor Networks</title><categories>cs.NI math.PR</categories><msc-class>90B18</msc-class><acm-class>C.2.1</acm-class><journal-ref>ACM SIGMETRICS Performance Evaluation Review, Volume 42 Issue 1,
  June 2014, Pages 395-406</journal-ref><doi>10.1145/2637364.2591981</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the use of wireless sensor networks increases, the need for
(energy-)efficient and reliable broadcasting algorithms grows. Ideally, a
broadcasting algorithm should have the ability to quickly disseminate data,
while keeping the number of transmissions low. In this paper we develop a model
describing the message count in large-scale wireless sensor networks. We focus
our attention on the popular Trickle algorithm, which has been proposed as a
suitable communication protocol for code maintenance and propagation in
wireless sensor networks. Besides providing a mathematical analysis of the
algorithm, we propose a generalized version of Trickle, with an additional
parameter defining the length of a listen-only period. This generalization
proves to be useful for optimizing the design and usage of the algorithm. For
single-cell networks we show how the message count increases with the size of
the network and how this depends on the Trickle parameters. Furthermore, we
derive distributions of inter-broadcasting times and investigate their
asymptotic behavior. Our results prove conjectures made in the literature
concerning the effect of a listen-only period. Additionally, we develop an
approximation for the expected number of transmissions in multi-cell networks.
All results are validated by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6056</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6056</id><created>2014-07-22</created><authors><author><keyname>Bouchboua</keyname><forenames>Ahmed</forenames></author><author><keyname>Ouremchi</keyname><forenames>Rabah</forenames></author><author><keyname>Messaoudi</keyname><forenames>Fay&#xe7;al</forenames></author><author><keyname>Ghazi</keyname><forenames>Mohammed El</forenames></author></authors><title>Generation of pedagogical content based on the learning style of
  learners in a dynamic adaptive hypermedia environment</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several researches in psychology and science of education affirm the impact
of learning style on the learning process and encourage its taking into account
in the teaching strategies in order to facilitate the task for learners and
improve their results. This article deals with the relationship between the
characteristics of the learner, the teaching materials and the context in which
takes place the learning in order to allow a better adaptivity. The latter is
ensured thanks to the most important element of our system, which is the
generator of course, the latter allows you to offer a hypermedia virtual,
therefore the pages and the links will be built dynamically, taking into
account the learning style and the cognitive status of the learner. As well and
after having completed a first filter on the fragments, in order to select
those corresponding to the course, there will be applied a second filter to
select the fragments corresponding to the learning style of the learner to
retain only those in accordance with responsive to the level of knowledge
required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6062</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6062</id><created>2014-07-22</created><authors><author><keyname>Botelho</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Bessani</keyname><forenames>Alysson</forenames></author><author><keyname>Ramos</keyname><forenames>Fernando M. V.</forenames></author><author><keyname>Ferreira</keyname><forenames>Paulo</forenames></author></authors><title>SMaRtLight: A Practical Fault-Tolerant SDN Controller</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increase in the number of SDN-based deployments in production networks is
triggering the need to consider fault-tolerant designs of controller
architectures. Commercial SDN controller solutions incorporate fault tolerance,
but there has been little discussion in the SDN literature on the design of
such systems and the tradeoffs involved. To fill this gap, we present a
by-construction design of a fault-tolerant controller, and materialize it by
proposing and formalizing a practical architecture for small to medium-sized
scale networks. A central component of our particular design is a replicated
shared database that stores all network state. Contrary to the more common
primary-backup approaches, the proposed design guarantees a smooth transition
in case of failures and avoids the need of an additional coordination service.
Our preliminary results show that the performance of our solution fulfills the
demands of the target networks. We hope this paper to be a first step in what
we consider a necessary discussion on how to build robust SDNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6064</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6064</id><created>2014-07-22</created><updated>2014-12-08</updated><authors><author><keyname>Chua</keyname><forenames>Freddy Chong Tat</forenames></author><author><keyname>Lim</keyname><forenames>Ee-Peng</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Detecting Flow Anomalies in Distributed Systems</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Deep within the networks of distributed systems, one often finds anomalies
that affect their efficiency and performance. These anomalies are difficult to
detect because the distributed systems may not have sufficient sensors to
monitor the flow of traffic within the interconnected nodes of the networks.
Without early detection and making corrections, these anomalies may aggravate
over time and could possibly cause disastrous outcomes in the system in the
unforeseeable future. Using only coarse-grained information from the two end
points of network flows, we propose a network transmission model and a
localization algorithm, to detect the location of anomalies and rank them using
a proposed metric within distributed systems. We evaluate our approach on
passengers' records of an urbanized city's public transportation system and
correlate our findings with passengers' postings on social media microblogs.
Our experiments show that the metric derived using our localization algorithm
gives a better ranking of anomalies as compared to standard deviation measures
from statistical models. Our case studies also demonstrate that transportation
events reported in social media microblogs matches the locations of our detect
anomalies, suggesting that our algorithm performs well in locating the
anomalies within distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6067</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6067</id><created>2014-07-22</created><authors><author><keyname>Reis</keyname><forenames>Marcelo S.</forenames></author><author><keyname>Ferreira</keyname><forenames>Carlos E.</forenames></author><author><keyname>Barrera</keyname><forenames>Junior</forenames></author></authors><title>The U-curve optimization problem: improvements on the original algorithm
  and time complexity analysis</title><categories>cs.LG cs.CV</categories><comments>Original results from the Ph.D. thesis of Marcelo S. Reis. This
  thesis can be accessed through the following link:
  http://www.teses.usp.br/teses/disponiveis/45/45134/tde-05022013-123757/en.php</comments><msc-class>68T10</msc-class><acm-class>I.5.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The U-curve optimization problem is characterized by a decomposable in
U-shaped curves cost function over the chains of a Boolean lattice. This
problem can be applied to model the classical feature selection problem in
Machine Learning. Recently, the U-Curve algorithm was proposed to give optimal
solutions to the U-curve problem. In this article, we point out that the
U-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS)
algorithm, which is actually optimal. We also present the results of optimal
and suboptimal experiments, in which UCS is compared with the UBB optimal
branch-and-bound algorithm and the SFFS heuristic, respectively. We show that,
in both experiments, $\proc{UCS}$ had a better performance than its competitor.
Finally, we analyze the obtained results and point out improvements on UCS that
might enhance the performance of this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6071</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6071</id><created>2014-07-22</created><updated>2015-07-15</updated><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>Deep Community Detection</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 13 figures, journal submission and supplementary file
  (Figures 11-13), to appear in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2458782</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deep community in a graph is a connected component that can only be seen
after removal of nodes or edges from the rest of the graph. This paper
formulates the problem of detecting deep communities as multi-stage node
removal that maximizes a new centrality measure, called the local Fiedler
vector centrality (LFVC), at each stage. The LFVC is associated with the
sensitivity of algebraic connectivity to node or edge removals. We prove that a
greedy node/edge removal strategy, based on successive maximization of LFVC,
has bounded performance loss relative to the optimal, but intractable,
combinatorial batch removal strategy. Under a stochastic block model framework,
we show that the greedy LFVC strategy can extract deep communities with
probability one as the number of observations becomes large. We apply the
greedy LFVC strategy to real-world social network datasets. Compared with
conventional community detection methods we demonstrate improved ability to
identify important communities and key members in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6073</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6073</id><created>2014-07-22</created><authors><author><keyname>Pavlichin</keyname><forenames>Dmitri S.</forenames></author><author><keyname>Mabuchi</keyname><forenames>Hideo</forenames></author></authors><title>Optical modular arithmetic</title><categories>quant-ph cs.ET</categories><comments>12 pages, 6 figures</comments><journal-ref>Proceedings SPIE 9083, Micro- and Nanotechnology Sensors, Systems,
  and Applications VI, 908315 (June 4, 2014)</journal-ref><doi>10.1117/12.2051108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanoscale integrated photonic devices and circuits offer a path to ultra-low
power computation at the few-photon level. Here we propose an optical circuit
that performs a ubiquitous operation: the controlled, random-access readout of
a collection of stored memory phases or, equivalently, the computation of the
inner product of a vector of phases with a binary &quot;selector&quot; vector, where the
arithmetic is done modulo 2pi and the result is encoded in the phase of a
coherent field. This circuit, a collection of cascaded interferometers driven
by a coherent input field, demonstrates the use of coherence as a computational
resource, and of the use of recently-developed mathematical tools for modeling
optical circuits with many coupled parts. The construction extends in a
straightforward way to the computation of matrix-vector and matrix-matrix
products, and, with the inclusion of an optical feedback loop, to the
computation of a &quot;weighted&quot; readout of stored memory phases. We note some
applications of these circuits for error correction and for computing tasks
requiring fast vector inner products, e.g. statistical classification and some
machine learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6074</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6074</id><created>2014-07-22</created><authors><author><keyname>Chen</keyname><forenames>Shi</forenames></author><author><keyname>Ilany</keyname><forenames>Amiyaal</forenames></author><author><keyname>White</keyname><forenames>Brad J.</forenames></author><author><keyname>Sanderson</keyname><forenames>Michael W.</forenames></author><author><keyname>Lanzas</keyname><forenames>Cristina</forenames></author></authors><title>Spatial-Temporal Dynamics of High-Resolution Animal Social Networks:
  What Can We Learn from Domestic Animals?</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies of animal social networks have significantly increased our
understanding of animal behavior, social interactions, and many important
ecological and epidemiological processes. However, most of the studies are at
low temporal and spatial resolution due to the difficulty in recording accurate
contact information. Domestic animals such as cattle have social behavior and
serve as an excellent study system because their position can be explicitly and
continuously tracked, allowing their social networks to be accurately
constructed. We used radio-frequency tags to accurately track cattle position
and analyze high-resolution cattle social networks. We tested the hypothesis of
temporal stationarity and spatial homogeneity in these high-resolution networks
and demonstrated substantial spatial-temporal heterogeneity during different
daily time periods (feeding and non-feeding) and in different areas of the pen
(grain bunk, water trough, hay bunk, and other general pen area). The social
network structure is analyzed using global network characteristics (network
density, exponential random graph model structure), subgroup clustering
(modularity), triadic property (transitivity), and dyadic interactions
(correlation coefficient from a quadratic assignment procedure). Cattle tend to
have the strongest and most consistent contacts with others around the hay bunk
during the feeding time. These results cannot be determined from data at lower
spatial (aggregated at entire pen level) or temporal (aggregated at daily
level) resolution. These results reveal new insights for real-time animal
social network structure dynamics, providing more accurate descriptions that
allow more accurate modeling of multiple (both direct and indirect) disease
transmission pathways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6075</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6075</id><created>2014-07-22</created><updated>2015-02-20</updated><authors><author><keyname>Khanafer</keyname><forenames>Ali</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author></authors><title>Robust Distributed Averaging: When are Potential-Theoretic Strategies
  Optimal?</title><categories>cs.SY math.OC</categories><comments>32 pages, 1 figure, submitted to IEEE Transactions on Automatic
  Control. arXiv admin note: text overlap with arXiv:1304.0055</comments><msc-class>91A43</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the interaction between a network designer and an adversary over a
dynamical network. The network consists of nodes performing continuous-time
distributed averaging. The adversary strategically disconnects a set of links
to prevent the nodes from reaching consensus. Meanwhile, the network designer
assists the nodes in reaching consensus by changing the weights of a limited
number of links in the network. We formulate two Stackelberg games to describe
this competition where the order in which the players act is reversed in the
two problems. Although the canonical equations provided by the Pontryagin's
maximum principle seem to be intractable, we provide an alternative
characterization for the optimal strategies that makes connection to potential
theory. Finally, we provide a sufficient condition for the existence of a
saddle-point equilibrium for the underlying zero-sum game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6076</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6076</id><created>2014-07-22</created><updated>2015-02-20</updated><authors><author><keyname>Khanafer</keyname><forenames>Ali</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author><author><keyname>Gharesifard</keyname><forenames>Bahman</forenames></author></authors><title>Stability of Epidemic Models over Directed Graphs: A Positive Systems
  Approach</title><categories>cs.SY cs.SI math.OC</categories><comments>13 pages, 5 figures, submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stability properties of a susceptible-infected-susceptible (SIS)
diffusion model, so-called the $n$-intertwined Markov model, over arbitrary
directed network topologies. As in the majority of the work on infection spread
dynamics, this model exhibits a threshold phenomenon. When the curing rates in
the network are high, the disease-free state is the unique equilibrium over the
network. Otherwise, an endemic equilibrium state emerges, where some infection
remains within the network. Using notions from positive systems theory, {we
provide novel proofs for the global asymptotic stability of the equilibrium
points in both cases over strongly connected networks based on the value of the
basic reproduction number, a fundamental quantity in the study of epidemics.}
When the network topology is weakly connected, we provide conditions for the
existence, uniqueness, and global asymptotic stability of an endemic state, and
we study the stability of the disease-free state. Finally, we demonstrate that
the $n$-intertwined Markov model can be viewed as a best-response dynamical
system of a concave game among the nodes. This characterization allows us to
cast new infection spread dynamics; additionally, we provide a sufficient
condition for the global convergence to the disease-free state, which can be
checked in a distributed fashion. Several simulations demonstrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6078</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6078</id><created>2014-07-22</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Extra Gain:Improved Sparse Channel Estimation Using Reweighted l_1-norm
  Penalized LMS/F Algorithm</title><categories>cs.IT math.IT</categories><comments>5pages, 6 figures, submitted for ICCC2014@Shanghai. arXiv admin note:
  text overlap with arXiv:1401.3566 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The channel estimation is one of important techniques to ensure reliable
broadband signal transmission. Broadband channels are often modeled as a sparse
channel. Comparing with traditional dense-assumption based linear channel
estimation methods, e.g., least mean square/fourth (LMS/F) algorithm,
exploiting sparse structure information can get extra performance gain. By
introducing l_1-norm penalty, two sparse LMS/F algorithms, (zero-attracting
LMSF, ZA-LMS/F and reweighted ZA-LMSF, RZA-LMSF), have been proposed [1].
Motivated by existing reweighted l_1-norm (RL1) sparse algorithm in compressive
sensing [2], we propose an improved channel estimation method using RL1 sparse
penalized LMS/F (RL1-LMS/F) algorithm to exploit more efficient sparse
structure information. First, updating equation of RL1-LMS/F is derived.
Second, we compare their sparse penalize strength via figure example. Finally,
computer simulation results are given to validate the superiority of proposed
method over than conventional two methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6079</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6079</id><created>2014-07-22</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Zhu</keyname><forenames>Xiao-mei</forenames></author><author><keyname>Chen</keyname><forenames>Zhang-xin</forenames></author></authors><title>Novel Realization of Adaptive Sparse Sensing with Sparse Least Mean
  Fourth Algorithm</title><categories>cs.IT math.IT</categories><comments>5 figures, 9 figures, submitted for WCSP2014@Hefei. arXiv admin note:
  substantial text overlap with arXiv:1403.0190</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nonlinear sparse sensing (NSS) techniques have been adopted for realizing
compressive sensing (CS) in many applications such as Radar imaging and sparse
channel estimation. Unlike the NSS, in this paper, we propose an adaptive
sparse sensing (ASS) approach using reweighted zero-attracting normalized least
mean fourth (RZA-NLMF) algorithm which depends on several given parameters,
i.e., reweighted factor, regularization parameter and initial step-size. First,
based on the independent assumption, Cramer Rao lower bound (CRLB) is derived
as for the performance comparisons. In addition, reweighted factor selection
method is proposed for achieving robust estimation performance. Finally, to
verify the algorithm, Monte Carlo based computer simulations are given to show
that the ASS achieves much better mean square error (MSE) performance than the
NSS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6081</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6081</id><created>2014-07-22</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Shan</keyname><forenames>Lin</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Adaptive MIMO Channel Estimation using Sparse Variable Step-Size NLMS
  Algorithms</title><categories>cs.IT math.IT</categories><comments>5 papges, 6 figures, submitted for ICCS2014@Macau. arXiv admin note:
  substantial text overlap with arXiv:1311.1314</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  To estimate multiple-input multiple-output (MIMO) channels, invariable
step-size normalized least mean square (ISSNLMS) algorithm was applied to
adaptive channel estimation (ACE). Since the MIMO channel is often described by
sparse channel model due to broadband signal transmission, such sparsity can be
exploited by adaptive sparse channel estimation (ASCE) methods using sparse
ISS-NLMS algorithms. It is well known that step-size is a critical parameter
which controls three aspects: algorithm stability, estimation performance and
computational cost. The previous approaches can exploit channel sparsity but
their step-sizes are keeping invariant which unable balances well the three
aspects and easily cause either estimation performance loss or instability. In
this paper, we propose two stable sparse variable step-size NLMS (VSS-NLMS)
algorithms to improve the accuracy of MIMO channel estimators. First, ASCE for
estimating MIMO channels is formulated in MIMO systems. Second, different
sparse penalties are introduced to VSS-NLMS algorithm for ASCE. In addition,
difference between sparse ISSNLMS algorithms and sparse VSS-NLMS ones are
explained. At last, to verify the effectiveness of the proposed algorithms for
ASCE, several selected simulation results are shown to prove that the proposed
sparse VSS-NLMS algorithms can achieve better estimation performance than the
conventional methods via mean square error (MSE) and bit error rate (BER)
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6082</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6082</id><created>2014-07-22</created><authors><author><keyname>Milevskiy</keyname><forenames>Igor</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author></authors><title>Joint Energy-based Detection and Classificationon of Multilingual Text
  Lines</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new hierarchical MDL-based model for a joint detection
and classi?cation of multilingual text lines in im- ages taken by hand-held
cameras. The majority of related text detec- tion methods assume alphabet-based
writing in a single language, e.g. in Latin. They use simple clustering
heuristics speci?c to such texts: prox- imity between letters within one line,
larger distance between separate lines, etc. We are interested in a
significantly more ambiguous problem where images combine alphabet and
logographic characters from multiple languages and typographic rules vary a lot
(e.g. English, Korean, and Chinese). Complexity of detecting and classifying
text lines in multiple languages calls for a more principled approach based on
information- theoretic principles. Our new MDL model includes data costs
combining geometric errors with classi?cation likelihoods and a hierarchical
sparsity term based on label costs. This energy model can be e?ciently
minimized by fusion moves. We demonstrate robustness of the proposed algorithm
on a large new database of multilingual text images collected in the pub- lic
transit system of Seoul.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6083</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6083</id><created>2014-07-22</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author></authors><title>Affine Combination of Two Adaptive Sparse Filters for Estimating Large
  Scale MIMO Channels</title><categories>cs.IT math.IT</categories><comments>7 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Large scale multiple-input multiple-output (MIMO) system is considered one of
promising technologies for realizing next-generation wireless communication
system (5G) to increasing the degrees of freedom in space and enhancing the
link reliability while considerably reducing the transmit power. However, large
scale MIMO system design also poses a big challenge to traditional
one-dimensional channel estimation techniques due to high complexity and curse
of dimensionality problems which are caused by long delay spread as well as
large number antenna. Since large scale MIMO channels often exhibit sparse
or/and cluster-sparse structure, in this paper, we propose a simple affine
combination of adaptive sparse channel estimation method for reducing
complexity and exploiting channel sparsity in the large scale MIMO system.
First, problem formulation and standard affine combination of adaptive least
mean square (LMS) algorithm are introduced. Then we proposed an effective
affine combination method with two sparse LMS filters and designed an
approximate optimum affine combiner according to stochastic gradient search
method as well. Later, to validate the proposed algorithm for estimating large
scale MIMO channel, computer simulations are provided to confirm effectiveness
of the proposed algorithm which can achieve better estimation performance than
the conventional one as well as traditional method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6085</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6085</id><created>2014-07-22</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Shan</keyname><forenames>Lin</forenames></author></authors><title>Block Bayesian Sparse Learning Algorithms With Application to Estimating
  Channels in OFDM Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, will be presented in WPMC2014@Sydney, Australia</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cluster-sparse channels often exist in frequencyselective fading broadband
communication systems. The main reason is received scattered waveform exhibits
cluster structure which is caused by a few reflectors near the receiver.
Conventional sparse channel estimation methods have been proposed for general
sparse channel model which without considering the potential cluster-sparse
structure information. In this paper, we investigate the cluster-sparse channel
estimation (CS-CE) problems in the state of the art orthogonal
frequencydivision multiplexing (OFDM) systems. Novel Bayesian clustersparse
channel estimation (BCS-CE) methods are proposed to exploit the cluster-sparse
structure by using block sparse Bayesian learning (BSBL) algorithm. The
proposed methods take advantage of the cluster correlation in training matrix
so that they can improve estimation performance. In addition, different from
our previous method using uniform block partition information, the proposed
methods can work well when the prior block partition information of channels is
unknown. Computer simulations show that the proposed method has a superior
performance when compared with the previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6089</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6089</id><created>2014-07-22</created><updated>2015-02-07</updated><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Learning Rank Functionals: An Empirical Study</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ranking is a key aspect of many applications, such as information retrieval,
question answering, ad placement and recommender systems. Learning to rank has
the goal of estimating a ranking model automatically from training data. In
practical settings, the task often reduces to estimating a rank functional of
an object with respect to a query. In this paper, we investigate key issues in
designing an effective learning to rank algorithm. These include data
representation, the choice of rank functionals, the design of the loss function
so that it is correlated with the rank metrics used in evaluation. For the loss
function, we study three techniques: approximating the rank metric by a smooth
function, decomposition of the loss into a weighted sum of element-wise losses
and into a weighted sum of pairwise losses. We then present derivations of
piecewise losses using the theory of high-order Markov chains and Markov random
fields. In experiments, we evaluate these design aspects on two tasks: answer
ranking in a Social Question Answering site, and Web Information Retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6090</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6090</id><created>2014-07-22</created><authors><author><keyname>Chaturvedi</keyname><forenames>Jyoti</forenames></author><author><keyname>Parashar</keyname><forenames>Anubha</forenames></author><author><keyname>Manjrekar</keyname><forenames>Amrita A</forenames></author><author><keyname>Bhaskar</keyname><forenames>Vinay S</forenames></author></authors><title>Social and Business Intelligence Analysis Using PSO</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to elaborate swarm intelligence for business
intelligence decision making and the business rules management improvement.
.The swarm optimization, which is highly influenced by the behavior of
creature, performs in group. The Spatial data is defined as data that is
represented by 2D or 3D images. SQL Server supports only 2D images till now. As
we know that location is an essential part of any organizational data as well
as business data enterprises maintain customer address lists, own property,
ship goods from and to warehouses, manage transport flows among their
workforce, and perform many other activities. By means to say a lot of spatial
data is used and processed by enterprises, organizations and other bodies in
order to make the things more visible and self descriptive. From the
experiments, we found that PSO is can facilitate the intelligence in social and
business behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6094</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6094</id><created>2014-07-22</created><authors><author><keyname>Gopakumar</keyname><forenames>Shivapratap</forenames></author><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Stabilizing Sparse Cox Model using Clinical Structures in Electronic
  Medical Records</title><categories>stat.ML cs.LG</categories><comments>Submitted to International Workshop on Pattern Recognition for
  Healthcare Analytics 2014, Sweden. Contains 4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stability in clinical prediction models is crucial for transferability
between studies, yet has received little attention. The problem is paramount in
high dimensional data which invites sparse models with feature selection
capability. We introduce an effective method to stabilize sparse Cox model of
time-to-events using clinical structures inherent in Electronic Medical
Records. Model estimation is stabilized using a feature graph derived from two
types of EMR structures: temporal structure of disease and intervention
recurrences, and hierarchical structure of medical knowledge and practices. We
demonstrate the efficacy of the method in predicting time-to-readmission of
heart failure patients. On two stability measures - the Jaccard index and the
Consistency index - the use of clinical structures significantly increased
feature stability without hurting discriminative power. Our model reported a
competitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6099</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6099</id><created>2014-07-22</created><authors><author><keyname>Macdonell</keyname><forenames>S. G.</forenames></author><author><keyname>Min</keyname><forenames>K.</forenames></author><author><keyname>Connor</keyname><forenames>A. M.</forenames></author></authors><title>Autonomous requirements specification processing using natural language
  processing</title><categories>cs.CL cs.SE</categories><comments>Proceedings of the ISCA 14th International Conferenceon Intelligent
  and Adaptive Systems and Software Engineering (IASSE 2005)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our ongoing research that centres on the application of natural
language processing (NLP) to software engineering and systems development
activities. In particular, this paper addresses the use of NLP in the
requirements analysis and systems design processes. We have developed a
prototype toolset that can assist the systems analyst or software engineer to
select and verify terms relevant to a project. In this paper we describe the
processes employed by the system to extract and classify objects of interest
from requirements documents. These processes are illustrated using a small
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6100</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6100</id><created>2014-07-22</created><authors><author><keyname>Limbu</keyname><forenames>Dilip K.</forenames></author><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author><author><keyname>MacDonell</keyname><forenames>Stephen G.</forenames></author></authors><title>A framework for contextual information retrieval from the WWW</title><categories>cs.IR</categories><comments>Proceedings of the 14th International Conference on Adaptive Systems
  and Software Engineering (IASSE 2005)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines are the most commonly used type of tool for finding relevant
information on the Internet. However, today's search engines are far from
perfect. Typical search queries are short, often one or two words, and can be
ambiguous therefore returning inappropriate results. Contextual information
retrieval (CIR) is a critical technique for these search engines to facilitate
queries and return relevant information. Despite its importance, little
progress has been made in CIR due to the difficulty of capturing and
representing contextual information about users. Numerous contextual
information retrieval approaches exist today, but to the best of our knowledge
none of them offer a similar service to the one proposed in this paper.
  This paper proposes an alternative framework for contextual information
retrieval from the WWW. The framework aims to improve query results (or make
search results more relevant) by constructing a contextual profile based on a
user's behaviour, their preferences, and a shared knowledge base, and using
this information in the search engine framework to find and return relevant
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6101</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6101</id><created>2014-07-22</created><authors><author><keyname>Limbu</keyname><forenames>Dilip K.</forenames></author><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author><author><keyname>Pears</keyname><forenames>Russel</forenames></author><author><keyname>MacDonell</keyname><forenames>Stephen G.</forenames></author></authors><title>Improving web search using contextual retrieval</title><categories>cs.IR</categories><journal-ref>Proceedings of the 6th International Conference on Information
  Technology: New Generations (ITNG 2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual retrieval is a critical technique for today's search engines in
terms of facilitating queries and returning relevant information. This paper
reports on the development and evaluation of a system designed to tackle some
of the challenges associated with contextual information retrieval from the
World Wide Web (WWW). The developed system has been designed with a view to
capturing both implicit and explicit user data which is used to develop a
personal contextual profile. Such profiles can be shared across multiple users
to create a shared contextual knowledge base. These are used to refine search
queries and improve both the search results for a user as well as their search
experience. An empirical study has been undertaken to evaluate the system
against a number of hypotheses. In this paper, results related to one are
presented that support the claim that users can find information more readily
using the contextual search system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6102</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6102</id><created>2014-07-22</created><authors><author><keyname>Talbot</keyname><forenames>Alison</forenames></author><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author></authors><title>Requirements engineering current practice and capability in small and
  medium software development enterprises in New Zealand</title><categories>cs.SE</categories><comments>Proceedings of the 9th ACIS Conference on Software Engineering
  Research, Management &amp; Applications (SERA 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents research on current industry practices with respect to
requirements engineering as implemented within software development companies
in New Zealand. A survey instrument is designed and deployed. The results are
analysed and compared against what is internationally considered &quot;best
practice&quot; and previous New Zealand and Australian studies. An attempt is made
to assess the requirements engineering capability of New Zealand companies
using both formal and informal frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6103</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6103</id><created>2014-07-22</created><authors><author><keyname>Schmidt</keyname><forenames>Frederik</forenames></author><author><keyname>MacDonell</keyname><forenames>Stephen G.</forenames></author><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author></authors><title>An automatic architecture reconstruction and refactoring framework</title><categories>cs.SE</categories><journal-ref>Software Engineering Research,Management and Applications 2011,
  Springer Berlin / Heidelberg. 377: 95-111</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of sources have noted that a substantial proportion of non trivial
software systems fail due to unhindered architectural erosion. This design
deterioration leads to low maintainability, poor testability and reduced
development speed. The erosion of software systems is often caused by
inadequate understanding, documentation and maintenance of the desired
implementation architecture. If the desired architecture is lost or the
deterioration is advanced, the reconstruction of the desired architecture and
the realignment of this desired architecture with the physical architecture
both require substantial manual analysis and implementation effort. This paper
describes the initial development of a framework for automatic software
architecture reconstruction and source code migration. This framework offers
the potential to reconstruct the conceptual architecture of software systems
and to automatically migrate the physical architecture of a software system
toward a conceptual architecture model. The approach is implemented within a
proof of concept prototype which is able to analyze java system and reconstruct
a conceptual architecture for these systems as well as to refactor the system
towards a conceptual architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6104</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6104</id><created>2014-07-22</created><authors><author><keyname>Connor</keyname><forenames>Andy M.</forenames></author><author><keyname>Finlay</keyname><forenames>Jacqui</forenames></author><author><keyname>Pears</keyname><forenames>Russel</forenames></author></authors><title>Mining developer communication data streams</title><categories>cs.SE</categories><journal-ref>Proceedings of the Fourth International Conference on Computer
  Science and Information Technology (CCSIT 2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the concepts of modelling a software development project
as a process that results in the creation of a continuous stream of data. In
terms of the Jazz repository used in this research, one aspect of that stream
of data would be developer communication. Such data can be used to create an
evolving social network characterized by a range of metrics. This paper
presents the application of data stream mining techniques to identify the most
useful metrics for predicting build outcomes. Results are presented from
applying the Hoeffding Tree classification method used in conjunction with the
Adaptive Sliding Window (ADWIN) method for detecting concept drift. The results
indicate that only a small number of the available metrics considered have any
significance for predicting the outcome of a build.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6112</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6112</id><created>2014-07-23</created><authors><author><keyname>Babatunde</keyname><forenames>Olabenjo</forenames></author><author><keyname>Mbarouk</keyname><forenames>Salim</forenames></author></authors><title>A review of Plesiochronous Digital Hierarchy (PDH) and Synchronous
  Digital Hierarchy (SDH)</title><categories>cs.NI</categories><comments>5 pages, 2 tables, Published with International Journal of Scientific
  Research Engineering &amp; Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement in telecommunications, packet traffic is rapidly
becoming the mainstream of data traffic. The use and deployment of Synchronous
Digital Hierarchy (SDH) networks for interconnection has gained traction
worldwide due to its flexibility and standard for interconnecting multiple
vendors, low operating cost and the high quality of service it provides.
Plesiochronous Digital Hierarchy (PDH) on the other hand has been used before
the introduction of the SDH standard and it also provides a means to transport
large quantity of data via digital equipment such as radio wave systems, optic
fibre and microwaves. In this paper, we shall discuss both PDH and SDH
technologies and identify some of the features of both and the issues in PDH
that brought about the introduction of the SDH technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6116</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6116</id><created>2014-07-23</created><authors><author><keyname>Selim</keyname><forenames>Md.</forenames></author><author><keyname>Siddik</keyname><forenames>Saeed</forenames></author><author><keyname>Gias</keyname><forenames>Alim Ul</forenames></author><author><keyname>Abdullah-Al-Wadud</keyname><forenames>M.</forenames></author><author><keyname>Khaled</keyname><forenames>Shah Mostafa</forenames></author></authors><title>A Genetic Algorithm for Software Design Migration from Structured to
  Object Oriented Paradigm</title><categories>cs.SE cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential benefit of migrating software design from Structured to Object
Oriented Paradigm is manifolded including modularity, manageability and
extendability. This design migration should be automated as it will reduce the
time required in manual process. Our previous work has addressed this issue in
terms of optimal graph clustering problem formulated by a quadratic Integer
Program (IP). However, it has been realized that solution to the IP is
computationally hard and thus heuristic based methods are required to get a
near optimal solution. This paper presents a Genetic Algorithm (GA) for optimal
clustering with an objective of maximizing intra-cluster edges whereas
minimizing the inter-cluster ones. The proposed algorithm relies on fitness
based parent selection and cross-overing cluster elements to reach an optimal
solution step by step. The scheme was implemented and tested against a set of
real and synthetic data. The experimental results show that GA outperforms our
previous works based on Greedy and Monte Carlo approaches by 40% and 49.5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6124</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6124</id><created>2014-07-23</created><authors><author><keyname>Chu</keyname><forenames>Duc-Hiep</forenames></author><author><keyname>Jaffar</keyname><forenames>Joxan</forenames></author><author><keyname>Trinh</keyname><forenames>Minh-Thai</forenames></author></authors><title>Automating Proofs of Data-Structure Properties in Imperative Programs</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of automated reasoning about dynamically manipulated
data structures. The state-of-the-art methods are limited to the
unfold-and-match (U+M) paradigm, where predicates are transformed via
(un)folding operations induced from their definitions before being treated as
uninterpreted. However, proof obligations from verifying programs with
iterative loops and multiple function calls often do not succumb to this
paradigm. Our contribution is a proof method which -- beyond U+M -- performs
automatic formula re-writing by treating previously encountered obligations in
each proof path as possible induction hypotheses. This enables us, for the
first time, to systematically reason about a wide range of obligations, arising
from practical program verification. We demonstrate the power of our proof
rules on commonly used lemmas, thereby close the remaining gaps in existing
state-of-the-art systems. Another impact, probably more important, is that our
method regains the power of compositional reasoning, and shows that the usage
of user-provided lemmas is no longer needed for the existing set of benchmarks.
This not only removes the burden of coming up with the appropriate lemmas, but
also significantly boosts up the verification process, since lemma
applications, coupled with unfolding, often induce very large search space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6125</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6125</id><created>2014-07-23</created><updated>2014-08-26</updated><authors><author><keyname>Colombo</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Vlassis</keyname><forenames>Nikos</forenames></author></authors><title>Spectral Sequence Motif Discovery</title><categories>q-bio.QM cs.CE</categories><comments>20 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence discovery tools play a central role in several fields of
computational biology. In the framework of Transcription Factor binding
studies, motif finding algorithms of increasingly high performance are required
to process the big datasets produced by new high-throughput sequencing
technologies. Most existing algorithms are computationally demanding and often
cannot support the large size of new experimental data. We present a new motif
discovery algorithm that is built on a recent machine learning technique,
referred to as Method of Moments. Based on spectral decompositions, this method
is robust under model misspecification and is not prone to locally optimal
solutions. We obtain an algorithm that is extremely fast and designed for the
analysis of big sequencing data. In a few minutes, we can process datasets of
hundreds of thousand sequences and extract motif profiles that match those
computed by various state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6126</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6126</id><created>2014-07-23</created><updated>2015-10-16</updated><authors><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Self-motions of pentapods with linear platform</title><categories>cs.RO</categories><comments>28 pages, 5 figures</comments><msc-class>53A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a full classification of all pentapods with linear platform
possessing a self-motion beside the trivial rotation about the platform. Recent
research necessitates a contemporary and accurate re-examination of old results
on this topic given by Darboux, Mannheim, Duporcq and Bricard, which also takes
the coincidence of platform anchor points into account. For our study we use
bond theory with respect to a novel kinematic mapping for pentapods with linear
platform, beside the method of singular-invariant leg-rearrangements. Based on
our results we design pentapods with linear platform, which have a simplified
direct kinematics concerning their number of (real) solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6128</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6128</id><created>2014-07-23</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Permutation Models for Collaborative Ranking</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of collaborative filtering where ranking information is
available. Focusing on the core of the collaborative ranking process, the user
and their community, we propose new models for representation of the underlying
permutations and prediction of ranks. The first approach is based on the
assumption that the user makes successive choice of items in a stage-wise
manner. In particular, we extend the Plackett-Luce model in two ways -
introducing parameter factoring to account for user-specific contribution, and
modelling the latent community in a generative setting. The second approach
relies on log-linear parameterisation, which relaxes the discrete-choice
assumption, but makes learning and inference much more involved. We propose
MCMC-based learning and inference methods and derive linear-time prediction
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6131</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6131</id><created>2014-07-23</created><authors><author><keyname>Du</keyname><forenames>Ye</forenames></author></authors><title>The Discrete Sell or Hold Problem with Constraints on Asset Values</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discrete sell or hold problem (DSHP), which is introduced in \cite{H12},
is studied under the constraint that each asset can only take a constant number
of different values. We show that if each asset can take only two values, the
problem becomes polynomial-time solvable. However, even if each asset can take
three different values, DSHP is still NP-hard. An approximation algorithm is
also given under this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6132</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6132</id><created>2014-07-23</created><updated>2015-01-23</updated><authors><author><keyname>Halperin</keyname><forenames>Dan</forenames></author><author><keyname>Kerber</keyname><forenames>Michael</forenames></author><author><keyname>Shaharabani</keyname><forenames>Doron</forenames></author></authors><title>The Offset Filtration of Convex Objects</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider offsets of a union of convex objects. We aim for a filtration, a
sequence of nested cell complexes, that captures the topological evolution of
the offsets for increasing radii. We describe methods to compute a filtration
based on the Voronoi partition with respect to the given convex objects. We
prove that, in two and three dimensions, the size of the filtration is
proportional to the size of the Voronoi diagram. Our algorithm runs in
$\Theta(n \log{n})$ in the $2$-dimensional case and in expected time $O(n^{3 +
\epsilon})$, for any $\epsilon &gt; 0$, in the $3$-dimensional case. Our approach
is inspired by alpha-complexes for point sets, but requires more involved
machinery and analysis primarily since Voronoi regions of general convex
objects do not form a good cover. We show by experiments that our approach
results in a similarly fast and topologically more stable method for computing
a filtration compared to approximating the input by point samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6140</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6140</id><created>2014-07-23</created><authors><author><keyname>Wasa</keyname><forenames>Kunihiro</forenames></author><author><keyname>Arimura</keyname><forenames>Hiroki</forenames></author><author><keyname>Uno</keyname><forenames>Takeaki</forenames></author></authors><title>Efficient Enumeration of Induced Subtrees in a K-Degenerate Graph</title><categories>cs.DS</categories><doi>10.1007/978-3-319-13075-0_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of enumerating all induced subtrees in
an input k-degenerate graph, where an induced subtree is an acyclic and
connected induced subgraph. A graph G = (V, E) is a k-degenerate graph if for
any its induced subgraph has a vertex whose degree is less than or equal to k,
and many real-world graphs have small degeneracies, or very close to small
degeneracies. Although, the studies are on subgraphs enumeration, such as
trees, paths, and matchings, but the problem addresses the subgraph
enumeration, such as enumeration of subgraphs that are trees. Their induced
subgraph versions have not been studied well. One of few example is for
chordless paths and cycles. Our motivation is to reduce the time complexity
close to O(1) for each solution. This type of optimal algorithms are proposed
many subgraph classes such as trees, and spanning trees. Induced subtrees are
fundamental object thus it should be studied deeply and there possibly exist
some efficient algorithms. Our algorithm utilizes nice properties of
k-degeneracy to state an effective amortized analysis. As a result, the time
complexity is reduced to O(k) time per induced subtree. The problem is solved
in constant time for each in planar graphs, as a corollary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6144</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6144</id><created>2014-07-23</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Pachocki</keyname><forenames>Jakub W.</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>On the String Consensus Problem and the Manhattan Sequence Consensus
  Problem</title><categories>cs.DS</categories><comments>accepted to SPIRE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Manhattan Sequence Consensus problem (MSC problem) we are given $k$
integer sequences, each of length $l$, and we are to find an integer sequence
$x$ of length $l$ (called a consensus sequence), such that the maximum
Manhattan distance of $x$ from each of the input sequences is minimized. For
binary sequences Manhattan distance coincides with Hamming distance, hence in
this case the string consensus problem (also called string center problem or
closest string problem) is a special case of MSC. Our main result is a
practically efficient $O(l)$-time algorithm solving MSC for $k\le 5$ sequences.
Practicality of our algorithms has been verified experimentally. It improves
upon the quadratic algorithm by Amir et al.\ (SPIRE 2012) for string consensus
problem for $k=5$ binary strings. Similarly as in Amir's algorithm we use a
column-based framework. We replace the implied general integer linear
programming by its easy special cases, due to combinatorial properties of the
MSC for $k\le 5$. We also show that for a general parameter $k$ any instance
can be reduced in linear time to a kernel of size $k!$, so the problem is
fixed-parameter tractable. Nevertheless, for $k\ge 4$ this is still too large
for any naive solution to be feasible in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6153</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6153</id><created>2014-07-23</created><authors><author><keyname>Obryk</keyname><forenames>Robert</forenames></author></authors><title>Write-and-f-array: implementation and an application</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new shared memory object: the write-and-f-array, provide its
wait-free implementation and use it to construct an improved wait-free
implementation of the fetch-and-add object. The write-and-f-array generalizes
single-writer write-and-snapshot object in a similar way that the f-array
generalizes the multi-writer snapshot object. More specifically, a
write-and-f-array is parameterized by an associative operator $f$ and is
conceptually an array with two atomic operations:
  - write-and-f modifies a single array's element and returns the result of
applying $f$ to all the elements,
  - read returns the result of applying $f$ to all the array's elements.
  We provide a wait-free implementation of an $N$-element write-and-f-array
with $O(N \log N)$ memory complexity, $O(\log^3 N)$ step complexity of the
write-and-f operation and $O(1)$ step complexity of the read operation. The
implementation uses CAS objects and requires their size to be $\Omega(\log M)$,
where $M$ is the total number of write-and-f operations executed. We also show,
how it can be modified to achieve $O(\log^2 N)$ step complexity of write-and-f,
while increasing the memory complexity to $O(N \log^2 N)$.
  The write-and-f-array can be applied to create a fetch-and-add object for $P$
processes with $O(P \log P)$ memory complexity and $O(\log^3 P)$ step
complexity of the fetch-and-add operation. This is the first implementation of
fetch-and-add with polylogarithmic step complexity and subquadratic memory
complexity that can be implemented without CAS or LL/SC objects of unrealistic
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6154</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6154</id><created>2014-07-23</created><authors><author><keyname>Blasco</keyname><forenames>Pol</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>Content-Level Selective Offloading in Heterogeneous Networks:
  Multi-armed Bandit Optimization and Regret Bounds</title><categories>cs.IT cs.LG math.IT</categories><comments>submitted for publication. arXiv admin note: text overlap with
  arXiv:1402.3247</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider content-level selective offloading of cellular downlink traffic
to a wireless infostation terminal which stores high data-rate content in its
cache memory. Cellular users in the vicinity of the infostation can directly
download the stored content from the infostation through a broadband connection
(e.g., WiFi), reducing the latency and load on the cellular network. The goal
of the infostation cache controller (CC) is to store the most popular content
in the cache memory such that the maximum amount of traffic is offloaded to the
infostation. In practice, the popularity profile of the files is not known by
the CC, which observes only the instantaneous demands for those contents stored
in the cache. Hence, the cache content placement is optimised based on the
demand history and on the cost associated to placing each content in the cache.
By refreshing the cache content at regular time intervals, the CC gradually
learns the popularity profile, while at the same time exploiting the limited
cache capacity in the best way possible. This is formulated as a multi-armed
bandit (MAB) problem with switching cost. Several algorithms are presented to
decide on the cache content over time. The performance is measured in terms of
cache efficiency, defined as the amount of net traffic that is offloaded to the
infostation. In addition to theoretical regret bounds, the proposed algorithms
are analysed through numerical simulations. In particular, the impact of system
parameters, such as the number of files, number of users, cache size, and
skewness of the popularity profile, on the performance is studied numerically.
It is shown that the proposed algorithms learn the popularity profile quickly
for a wide range of system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6166</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6166</id><created>2014-07-23</created><authors><author><keyname>Schlesinger</keyname><forenames>Michail</forenames></author><author><keyname>Flach</keyname><forenames>Boris</forenames></author><author><keyname>Vodolazskiy</keyname><forenames>Evgeniy</forenames></author></authors><title>M-best solutions for a class of fuzzy constraint satisfaction problems</title><categories>cs.AI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article considers one of the possible generalizations of constraint
satisfaction problems where relations are replaced by multivalued membership
functions. In this case operations of disjunction and conjunction are replaced
by maximum and minimum, and consistency of a solution becomes multivalued
rather than binary. The article studies the problem of finding d most
admissible solutions for a given d. A tractable subclass of these problems is
defined by the concepts of invariants and polymorphisms similar to the classic
constraint satisfaction approach. These concepts are adapted in two ways.
Firstly, the correspondence of &quot;invariant-polymorphism&quot; is generalized to
(min,max) semirings. Secondly, we consider non-uniform polymorphisms, where
each variable has its own operator, in contrast to the case of one operator
common for all variables. The article describes an algorithm that finds $d$
most admissible solutions in polynomial time, provided that the problem is
invariant with respect to some non-uniform majority operator. It is essential
that this operator needs not to be known for the algorithm to work. Moreover,
even a guarantee for the existence of such an operator is not necessary. The
algorithm either finds the solution or discards the problem. The latter is
possible only if the problem has no majority polymorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6169</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6169</id><created>2014-07-23</created><updated>2015-09-21</updated><authors><author><keyname>Find</keyname><forenames>Magnus Gausdal</forenames></author><author><keyname>Boyar</keyname><forenames>Joan</forenames></author></authors><title>Multiplicative Complexity of Vector Valued Boolean Functions</title><categories>cs.CC</categories><comments>Extended version of the paper &quot;The Relationship Between
  Multiplicative Complexity and Nonlinearity&quot;, MFCS2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multiplicative complexity of Boolean functions with multiple
bits of output, studying how large a multiplicative complexity is necessary and
sufficient to provide a desired nonlinearity. For so-called $\Sigma\Pi\Sigma$
circuits, we show that there is a tight connection between error correcting
codes and circuits computing functions with high nonlinearity. Combining this
with known coding theory results, we show that functions with $n$ inputs and
$n$ outputs with the highest possible nonlinearity must have at least $2.32n$
AND gates. We further show that one cannot prove stronger lower bounds by only
appealing to the nonlinearity of a function; we show a bilinear circuit
computing a function with almost optimal nonlinearity with the number of AND
gates being exactly the length of such a shortest code.
  Additionally we provide a function which, for general circuits, has
multiplicative complexity at least $2n-3$.
  Finally we study the multiplicative complexity of &quot;almost all'' functions. We
show that every function with $n$ bits of input and $m$ bits of output can be
computed using at most $2.5(1+o(1))\sqrt{m2^n}$ AND gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6174</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6174</id><created>2014-07-23</created><authors><author><keyname>Cakir</keyname><forenames>Fatih</forenames></author><author><keyname>Sclaroff</keyname><forenames>Stan</forenames></author></authors><title>Visual Word Selection without Re-Coding and Re-Pooling</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bag-of-Words (BoW) representation is widely used in computer vision. The
size of the codebook impacts the time and space complexity of the applications
that use BoW. Thus, given a training set for a particular computer vision task,
a key problem is pruning a large codebook to select only a subset of visual
words. Evaluating possible selections of words to be included in the pruned
codebook can be computationally prohibitive; in a brute-force scheme,
evaluating each pruned codebook requires re-coding of all features extracted
from training images to words in the candidate codebook and then re-pooling the
words to obtain a representation of each image, e.g., histogram of visual word
frequencies. In this paper, a method is proposed that selects and evaluates a
subset of words from an initially large codebook, without the need for
re-coding or re-pooling. Formulations are proposed for two commonly-used
schemes: hard and soft (kernel) coding of visual words with average-pooling.
The effectiveness of these formulations is evaluated on the 15 Scenes and
Caltech 10 benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6178</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6178</id><created>2014-07-23</created><authors><author><keyname>Jaberi</keyname><forenames>Raed</forenames></author></authors><title>Computing the $2$-blocks of directed graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a directed graph. A \textit{$2$-directed block} in $G$ is a
maximal vertex set $C^{2d}\subseteq V$ with $|C^{2d}|\geq 2$ such that for each
pair of distinct vertices $x,y \in C^{2d}$, there exist two vertex-disjoint
paths from $x$ to $y$ and two vertex-disjoint paths from $y$ to $x$ in $G$. In
contrast to the $2$-vertex-connected components of $G$, the subgraphs induced
by the $2$-directed blocks may consist of few or no edges. In this paper we
present two algorithms for computing the $2$-directed blocks of $G$ in
$O(\min\lbrace m,(t_{sap}+t_{sb})n\rbrace n)$ time, where $t_{sap}$ is the
number of the strong articulation points of $G$ and $t_{sb}$ is the number of
the strong bridges of $G$. Furthermore, we study two related concepts: the
$2$-strong blocks and the $2$-edge blocks of $G$. We give two algorithms for
computing the $2$-strong blocks of $G$ in $O( \min \lbrace m,t_{sap} n\rbrace
n)$ time and we show that the $2$-edge blocks of $G$ can be computed in $O(\min
\lbrace m, t_{sb} n \rbrace n)$ time. In this paper we also study some
optimization problems related to the strong articulation points and the
$2$-blocks of a directed graph. Given a strongly connected graph $G=(V,E)$,
find a minimum cardinality set $E^{*}\subseteq E$ such that $G^{*}=(V,E^{*})$
is strongly connected and the strong articulation points of $G$ coincide with
the strong articulation points of $G^{*}$. This problem is called minimum
strongly connected spanning subgraph with the same strong articulation points.
We show that there is a linear time $17/3$ approximation algorithm for this
NP-hard problem. We also consider the problem of finding a minimum strongly
connected spanning subgraph with the same $2$-blocks in a strongly connected
graph $G$. We present approximation algorithms for three versions of this
problem, depending on the type of $2$-blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6180</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6180</id><created>2014-07-23</created><authors><author><keyname>Ablinger</keyname><forenames>Jakob</forenames></author></authors><title>The package HarmonicSums: Computer Algebra and Analytic aspects of
  Nested Sums</title><categories>cs.SC hep-ph</categories><comments>10 pages</comments><journal-ref>PoS(LL2014)019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the essential functionality of the computer algebra
package HarmonicSums. On the one hand HarmonicSums can work with nested sums
such as harmonic sums and their generalizations and on the other hand it can
treat iterated integrals of the Poincare and Chen-type, such as harmonic
polylogarithms and their generalizations. The interplay of these
representations and the analytic aspects are illustrated by concrete examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6182</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6182</id><created>2014-07-23</created><authors><author><keyname>S</keyname><forenames>Lakshmi Prabha</forenames></author><author><keyname>Janakiraman</keyname><forenames>T. N.</forenames></author></authors><title>Existence of Comfortable Team in some Special Social Networks</title><categories>cs.SI</categories><comments>10 pages and 6 figures. arXiv admin note: substantial text overlap
  with arXiv:1406.1012, arXiv:1405.4534</comments><msc-class>91D30, 05C69, 05C76, 05C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comfortability is one of the important attributes (characteristics) for a
successful team work in any organization. It is necessary to find a comfortable
and successful team in any given social network. We have introduced
&quot;comfortability&quot; as a new SNA index. Comfortable team exists only in some
social networks. In this paper, we analyze the existence of comfortable team in
product graphs, such as strong product and Lexicographic product of two given
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6183</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6183</id><created>2014-07-23</created><authors><author><keyname>La Rocca</keyname><forenames>Marcello</forenames></author><author><keyname>Cantone</keyname><forenames>Domenico</forenames></author></authors><title>NeatSort - A practical adaptive algorithm</title><categories>cs.DS</categories><comments>23 pages, 20 figures</comments><msc-class>68W01, 68W40</msc-class><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new adaptive sorting algorithm which is optimal for most
disorder metrics and, more important, has a simple and quick implementation. On
input $X$, our algorithm has a theoretical $\Omega (|X|)$ lower bound and a
$\mathcal{O}(|X|\log|X|)$ upper bound, exhibiting amazing adaptive properties
which makes it run closer to its lower bound as disorder (computed on different
metrics) diminishes. From a practical point of view, \textit{NeatSort} has
proven itself competitive with (and often better than) \textit{qsort} and any
\textit{Random Quicksort} implementation, even on random arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6185</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6185</id><created>2014-07-23</created><updated>2015-11-03</updated><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Martin</keyname><forenames>Stefano</forenames></author></authors><title>Relative generalized Hamming weights of q-ary Reed-Muller codes</title><categories>cs.IT math.IT</categories><comments>41 pages. A discussion has been added on how to use secret sharing
  schemes from q-ary Reed-Muller codes in connection with distributed storage</comments><msc-class>94A62, 11T71, 94B65, 68P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coset constructions of $q$-ary Reed-Muller codes can be used to store secrets
on a distributed storage system in such a way that only parties with access to
a large part of the system can obtain information while still allowing for
local error-correction. In this paper we determine the relative generalized
Hamming weights of these codes which can be translated into a detailed
description of the information leakage [2, 24, 21, 11].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6190</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6190</id><created>2014-07-23</created><authors><author><keyname>Pal</keyname><forenames>Madhumangal</forenames></author><author><keyname>Rashmanlou</keyname><forenames>Hossein</forenames></author></authors><title>Irregular Interval Valued Fuzzy Graphs</title><categories>cs.DM</categories><comments>11 pages. arXiv admin note: substantial text overlap with
  arXiv:1209.1682; and text overlap with arXiv:1205.6123 by other authors</comments><journal-ref>Annals of Pure and Applied Mathematics, 3(1) (2013) 56-66</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define irregular interval-valued fuzzy graphs and their
various classifications. Size of regular interval-valued fuzzy graphs is
derived. The relation between highly and neighbourly irregular interval-valued
fuzzy graphs are established. Some basic theorems related to the stated graphs
have also been presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6202</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6202</id><created>2014-07-23</created><updated>2014-07-25</updated><authors><author><keyname>Matculevich</keyname><forenames>Svetlana</forenames></author><author><keyname>Repin</keyname><forenames>Sergey</forenames></author></authors><title>Estimates of the distance to the exact solution of parabolic problems
  based on local Poincar\'e type inequalities</title><categories>math.FA cs.NA</categories><comments>Error in the specification of the area of research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the paper is to derive two-sided bounds of the distance between
the exact solution of the evolutionary reaction-diffusion problem with mixed
Dirichlet--Robin boundary conditions and any function in the admissible energy
space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6220</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6220</id><created>2014-07-23</created><updated>2014-09-09</updated><authors><author><keyname>Schossau</keyname><forenames>Jory</forenames></author><author><keyname>Wilson</keyname><forenames>Greg</forenames></author></authors><title>Which Sustainable Software Practices Do Scientists Find Most Useful?</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We studied scientists who attended two-day workshops on basic software skills
to determine which tools and practices they found most useful. Our pre- and
post-workshop surveys showed increases in self-reported familiarity, while our
interviews showed that participants found learning Python more useful than
learning the Unix shell, that they found pointers to further resources very
valuable, and that background material---the &quot;why&quot; behind the skills---was also
very valuable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6225</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6225</id><created>2014-07-23</created><authors><author><keyname>Xia</keyname><forenames>Hongxing</forenames></author><author><keyname>Natarajan</keyname><forenames>Balasubramaniam</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author></authors><title>Feasibility of Simultaneous Information and Energy Transfer in LTE-A
  Small Cell Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous information and energy transfer is attracting much attention as
an effective method to provide green energy supply for mobiles. However the
very low power level of the harvested energy from RF spectrum limits the
application of such technique. Thanks to the improvement of sensitivity and
efficiency of RF energy harvesting circuit as well as the dense deployment of
small cells base stations, the SIET becomes more practical. In this paper, we
propose a unified receiver model for SIET in LTE-A small cell base staion
networks, formulate the feasibility problem with Poisson point process model
and analysis the feasibility for a special and practical senario. The results
shows that it is feasible for mobiles to charge the secondary battery wih
harvested energy from BSs, but it is still infeasible to directly charge the
primary battery or operate without any battery at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6239</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6239</id><created>2014-07-23</created><updated>2014-09-05</updated><authors><author><keyname>Ordu&#xf1;a-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayll&#xf3;n</keyname><forenames>Juan Manuel</forenames></author><author><keyname>Mart&#xed;n-Mart&#xed;n</keyname><forenames>Alberto</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>About the size of Google Scholar: playing the numbers</title><categories>cs.DL</categories><comments>42 pages, 18 figures, 8 tables, 3 appendixes</comments><report-no>EC3 Working Papers 18</report-no><journal-ref>Sep 2015. Scientometrics, 104(3), pp 931-949</journal-ref><doi>10.1007/s11192-015-1614-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of academic search engines (Google Scholar and Microsoft
Academic Search essentially) has revived and increased the interest in the size
of the academic web, since their aspiration is to index the entirety of current
academic knowledge. The search engine functionality and human search patterns
lead us to believe, sometimes, that what you see in the search engine's results
page is all that really exists. And, even when this is not true, we wonder
which information is missing and why. The main objective of this working paper
is to calculate the size of Google Scholar at present (May 2014). To do this,
we present, apply and discuss up to 4 empirical methods: Khabsa &amp; Giles's
method, an estimate based on empirical data, and estimates based on direct
queries and absurd queries. The results, despite providing disparate values,
place the estimated size of Google Scholar in about 160 million documents.
However, the fact that all methods show great inconsistencies, limitations and
uncertainties, makes us wonder why Google does not simply provide this
information to the scientific community if the company really knows this
figure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6245</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6245</id><created>2014-07-23</created><authors><author><keyname>van der Walt</keyname><forenames>Stefan</forenames></author><author><keyname>Sch&#xf6;nberger</keyname><forenames>Johannes L.</forenames></author><author><keyname>Nunez-Iglesias</keyname><forenames>Juan</forenames></author><author><keyname>Boulogne</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Warner</keyname><forenames>Joshua D.</forenames></author><author><keyname>Yager</keyname><forenames>Neil</forenames></author><author><keyname>Gouillart</keyname><forenames>Emmanuelle</forenames></author><author><keyname>Yu</keyname><forenames>Tony</forenames></author><author><keyname>contributors</keyname><forenames>the scikit-image</forenames></author></authors><title>scikit-image: Image processing in Python</title><categories>cs.MS cs.CV</categories><comments>Distributed under Creative Commons CC-BY 4.0. Published in PeerJ</comments><doi>10.7717/peerj.453</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  scikit-image is an image processing library that implements algorithms and
utilities for use in research, education and industry applications. It is
released under the liberal &quot;Modified BSD&quot; open source license, provides a
well-documented API in the Python programming language, and is developed by an
active, international team of collaborators. In this paper we highlight the
advantages of open source to achieve the goals of the scikit-image library, and
we showcase several real-world image processing applications that use
scikit-image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6251</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6251</id><created>2014-07-23</created><updated>2014-12-25</updated><authors><author><keyname>Lenz</keyname><forenames>Philip</forenames></author><author><keyname>Geiger</keyname><forenames>Andreas</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory
  and Computation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most popular approaches to multi-target tracking is
tracking-by-detection. Current min-cost flow algorithms which solve the data
association problem optimally have three main drawbacks: they are
computationally expensive, they assume that the whole video is given as a
batch, and they scale badly in memory and computation with the length of the
video sequence. In this paper, we address each of these issues, resulting in a
computationally and memory-bounded solution. First, we introduce a dynamic
version of the successive shortest-path algorithm which solves the data
association problem optimally while reusing computation, resulting in
significantly faster inference than standard solvers. Second, we address the
optimal solution to the data association problem when dealing with an incoming
stream of data (i.e., online setting). Finally, we present our main
contribution which is an approximate online solution with bounded memory and
computation which is capable of handling videos of arbitrarily length while
performing tracking in real time. We demonstrate the effectiveness of our
algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art
performance, while being significantly faster than existing solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6255</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6255</id><created>2014-07-17</created><authors><author><keyname>Cowen</keyname><forenames>Robert</forenames></author></authors><title>Adaptive Fault Diagnosis using Self-Referential Reasoning</title><categories>cs.OH</categories><comments>This article will appear in a book on Self-Referential Reasoning
  dedicated to Raymond Smullyan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem is to determine which processors are reliable in a remote
location by asking &quot;Yes or No&quot; questions. The processors are of three types:
those that always tell the truth, those that always lie, and those the
sometimes tell the truth and sometimes lie. Using self-referential reasoning,
along with earlier techniques, we can regard both the truth-tellers and liars
as reliable and thus the tackle situations when fewer than half the processors
are truth-tellers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6256</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6256</id><created>2014-07-13</created><authors><author><keyname>Yang</keyname><forenames>Yingxiang</forenames></author><author><keyname>Herrera</keyname><forenames>Carlos</forenames></author><author><keyname>Eagle</keyname><forenames>Nathan</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta C.</forenames></author></authors><title>Limits of Predictability in Commuting Flows in the Absence of Data for
  Calibration</title><categories>physics.soc-ph cs.SI</categories><comments>25 pages, 5 figures, Scientific Reports, 4 (2014)</comments><doi>10.1038/srep05662</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of commuting flows at different spatial scales is a
fundamental problem for different areas of study. Many current methods rely on
parameters requiring calibration from empirical trip volumes. Their values are
often not generalizable to cases without calibration data. To solve this
problem we develop a statistical expression to calculate commuting trips with a
quantitative functional form to estimate the model parameter when empirical
trip data is not available. We calculate commuting trip volumes at scales from
within a city to an entire country, introducing a scaling parameter alpha to
the recently proposed parameter free radiation model. The model requires only
widely available population and facility density distributions. The parameter
can be interpreted as the influence of the region scale and the degree of
heterogeneity in the facility distribution. We explore in detail the scaling
limitations of this problem, namely under which conditions the proposed model
can be applied without trip data for calibration. On the other hand, when
empirical trip data is available, we show that the proposed model's estimation
accuracy is as good as other existing models. We validated the model in
different regions in the U.S., then successfully applied it in three different
countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6257</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6257</id><created>2014-07-21</created><authors><author><keyname>Said</keyname><forenames>Gamal Abd El-Nasser A.</forenames></author><author><keyname>Mahmoud</keyname><forenames>Abeer M.</forenames></author><author><keyname>El-Horbaty</keyname><forenames>El-Sayed M.</forenames></author></authors><title>Simulation and optimization of container terminal operations: a case
  study</title><categories>cs.OH</categories><comments>10 pages, 3 figures</comments><journal-ref>International Journal of Computer Science Engineering and
  Information Technology Research(IJCSEITR),Vol.4,Issue 4,2014,pp.85-94</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Container terminals are facing a set of interrelated problems. Container
handling problems at container terminals are NP-hard problems. The docking time
of container ships at the port must be optimized. In this paper we have built a
simulation model that integrates all the activities of a container terminal.
The proposed approach is applied on a real case study data of container
terminal at El-Dekheilla port. The results show that the proposed approach
reduced the ship turnaround time in port where 51% reduction in ship service
time (loading/unloading) in port is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6266</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6266</id><created>2014-07-23</created><authors><author><keyname>Malekshan</keyname><forenames>K. Rahimi</forenames></author><author><keyname>Zhuang</keyname><forenames>W.</forenames></author><author><keyname>Lostanlen</keyname><forenames>Y.</forenames></author></authors><title>An Energy Efficient MAC Protocol for Fully Connected Wireless Ad Hoc
  Networks</title><categories>cs.NI</categories><comments>Published in IEEE Transaction on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency is an important performance measure of wireless network
protocols, especially for battery-powered mobile devices such as smartphones.
This paper presents a new energy-efficient medium access control (MAC) scheme
for fully connected wireless ad hoc networks. The proposed scheme reduces
energy consumption by putting radio interfaces in the sleep state periodically
and by reducing transmission collisions, which results in high throughput and
low packet transmission delay. The proposed MAC scheme can also address the
energy saving in realtime traffics which require very low packet transmission
delay. An analytical model is established to evaluate the performance of the
proposed MAC scheme. Analytical and simulation results demonstrate that the
proposed scheme has a significantly lower power consumption, achieves
substantially higher throughput, and has lower packet transmission delay in
comparison with existing power saving MAC protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6267</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6267</id><created>2014-07-23</created><updated>2016-02-08</updated><authors><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Sandholm</keyname><forenames>William H.</forenames></author></authors><title>Learning in games via reinforcement and regularization</title><categories>math.OC cs.GT cs.LG</categories><comments>39 pages, 6 figures</comments><msc-class>91A26, 37N40 (Primary), 91A22, 90C25, 68T05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a class of reinforcement learning dynamics where players
adjust their strategies based on their actions' cumulative payoffs over time -
specifically, by playing mixed strategies that maximize their expected
cumulative payoff minus a regularization term. A widely studied example is
exponential reinforcement learning, a process induced by an entropic
regularization term which leads mixed strategies to evolve according to the
replicator dynamics. However, in contrast to the class of regularization
functions used to define smooth best responses in models of stochastic
fictitious play, the functions used in this paper need not be infinitely steep
at the boundary of the simplex; in fact, dropping this requirement gives rise
to an important dichotomy between steep and nonsteep cases. In this general
framework, we extend several properties of exponential learning, including the
elimination of dominated strategies, the asymptotic stability of strict Nash
equilibria, and the convergence of time-averaged trajectories in zero-sum games
with an interior Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6288</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6288</id><created>2014-07-23</created><updated>2014-07-23</updated><authors><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Subspace Learning From Bits</title><categories>stat.ML cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple sensing and estimation framework to faithfully
recover the principal subspace of high-dimensional datasets or data streams
from a collection of one-bit measurements from distributed sensors based on
comparing accumulated energy projections of their data samples of dimension n
over pairs of randomly selected directions. By leveraging low-dimensional
structures, the top eigenvectors of a properly designed surrogate matrix is
shown to recover the principal subspace of rank $r$ as soon as the number of
bit measurements exceeds the order of $nr^2 \log n$, which can be much smaller
than the ambient dimension of the covariance matrix. The sample complexity to
obtain reliable comparison outcomes is also obtained. Furthermore, we develop a
low-complexity online algorithm to track the principal subspace that allows new
bit measurements arrive sequentially. Numerical examples are provided to
validate the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6290</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6290</id><created>2014-07-23</created><authors><author><keyname>Alajmi</keyname><forenames>Naser</forenames></author></authors><title>Wireless Sensor Networks Attacks and Solutions</title><categories>cs.CR</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A few years ago, wireless sensor networks (WSNs) used by only military. Now,
we have seen many of organizations use WSNs for some purposes such as weather,
pollution, traffic control, and healthcare. Security is becoming on these days
a major concern for wireless sensor network. In this paper I focus on the
security types of attacks and their detection. This paper anatomizes the
security requirements and security attacks in wireless sensor networks. Also,
indicate to the benchmarks for the security in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6295</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6295</id><created>2014-07-23</created><updated>2014-10-19</updated><authors><author><keyname>Vilaca</keyname><forenames>Xavier</forenames></author><author><keyname>Rodrigues</keyname><forenames>Luis</forenames></author></authors><title>On the Range of Equilibria Utilities of a Repeated Epidemic
  Dissemination Game with a Mediator</title><categories>cs.DC cs.GT</categories><comments>14 pages, 2 algorithms, accepted in ICDCN'15, proofs of D4 and D5
  improved, improved definition of Non-disclosure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider eager-push epidemic dissemination in a complete graph. Time is
divided into synchronous stages. In each stage, a source disseminates $\nu$
events. Each event is sent by the source, and forwarded by each node upon its
first reception, to $f$ nodes selected uniformly at random, where $f$ is the
fanout. We use Game Theory to study the range of $f$ for which equilibria
strategies exist, assuming that players are either rational or obedient to the
protocol, and that they do not collude. We model interactions as an infinitely
repeated game. We devise a monitoring mechanism that extends the repeated game
with communication rounds used for exchanging monitoring information, and
define strategies for this extended game. We assume the existence of a trusted
mediator, that players are computationally bounded such that they cannot break
the cryptographic primitives used in our mechanism, and that symmetric
ciphering is cheap. Under these assumptions, we show that, if the size of the
stream is sufficiently large and players attribute enough value to future
utilities, then the defined strategies are Sequential Equilibria of the
extended game for any value of $f$. Moreover, the utility provided to each
player is arbitrarily close to that provided in the original game. This shows
that we can persuade rational nodes to follow a dissemination protocol that
uses any fanout, while arbitrarily minimising the relative overhead of
monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6296</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6296</id><created>2014-07-23</created><updated>2015-01-12</updated><authors><author><keyname>Marra</keyname><forenames>Monica</forenames></author></authors><title>The recent Italian regulations about the open-access availability of
  publicly-funded research publications, and the documentation landscape in
  astrophysics</title><categories>cs.DL astro-ph.IM physics.soc-ph</categories><comments>To be published in the proceedings of: 16th International Conference
  on Grey Literature &quot;Grey Literature Lobby: Engines and Requesters for
  Change&quot;, December 8-9, 2014, Library of Congress, Washington D.C., USA</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In October 2013 Italy enacted Law 112 2013, containing the first national
regulations about the open access availability of publicly-funded research
results (publications). The impact of these new regulations with the specific
situation of that open access discipline which is astrophysics, has been
considered. Under a strictly technical point of view, in the light of the new
dispositions the open nature of a part of the astrophysical scholarly
literature which has been made available online free to the reader during the
last twenty years, might be questionable. Some possible ways to make
astrophysicists' scholarly dissemination entirely compliant with law
requirements are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6297</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6297</id><created>2014-07-23</created><updated>2015-01-22</updated><authors><author><keyname>Sarzynska</keyname><forenames>Marta</forenames></author><author><keyname>Leicht</keyname><forenames>Elizabeth A.</forenames></author><author><keyname>Chowell</keyname><forenames>Gerardo</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Null Models for Community Detection in Spatially-Embedded, Temporal
  Networks</title><categories>physics.soc-ph cs.SI nlin.AO physics.bio-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of networks, it is often insightful to use algorithms to
determine mesoscale features such as &quot;community structure&quot;, in which densely
connected sets of nodes constitute &quot;communities&quot; that have sparse connections
to other communities. The most popular way of detecting communities
algorithmically is to optimize the quality function known as modularity. When
optimizing modularity, one compares the actual connections in a (static or
time-dependent) network to the connections obtained from a random-graph
ensemble that acts as a null model. The communities are then the sets of nodes
that are connected to each other densely relative to what is expected from the
null model. Clearly, the process of community detection depends fundamentally
on the choice of null model, so it is important to develop and analyze novel
null models that take into account appropriate features of the system under
study. In this paper, we investigate the effects of using null models that take
incorporate spatial information, and we propose a novel null model based on the
radiation model of population spread. We also develop novel synthetic spatial
benchmark networks in which the connections between entities are based on
distance or flux between nodes, and we compare the performance of both static
and time-dependent radiation null models to the standard (&quot;Newman-Girvan&quot;) null
model for modularity optimization and a recently-proposed gravity null model.
In our comparisons, we use both the above synthetic benchmarks and
time-dependent correlation networks that we construct using countrywide dengue
fever incidence data for Peru. We also evaluate a recently-proposed correlation
null model, which was developed specifically for correlation networks that are
constructed from time series, on the epidemic-correlation data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6315</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6315</id><created>2014-07-23</created><authors><author><keyname>Kumar</keyname><forenames>Deepak</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A G</forenames></author></authors><title>Quadratically constrained quadratic programming for classification using
  particle swarms and applications</title><categories>cs.AI cs.LG cs.NE math.OC</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle swarm optimization is used in several combinatorial optimization
problems. In this work, particle swarms are used to solve quadratic programming
problems with quadratic constraints. The approach of particle swarms is an
example for interior point methods in optimization as an iterative technique.
This approach is novel and deals with classification problems without the use
of a traditional classifier. Our method determines the optimal hyperplane or
classification boundary for a data set. In a binary classification problem, we
constrain each class as a cluster, which is enclosed by an ellipsoid. The
estimation of the optimal hyperplane between the two clusters is posed as a
quadratically constrained quadratic problem. The optimization problem is solved
in distributed format using modified particle swarms. Our method has the
advantage of using the direction towards optimal solution rather than searching
the entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid
datasets show that the proposed method works better than a neural network and
the performance is close to that of SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6317</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6317</id><created>2014-07-23</created><authors><author><keyname>Rao</keyname><forenames>Ch. Srinivasa</forenames></author><author><keyname>Babu</keyname><forenames>B. Raveendra</forenames></author></authors><title>A Fuzzy Differential Evolution Algorithm for Job Scheduling on
  Computational Grids</title><categories>cs.DC</categories><comments>6 pages, 9 figures</comments><doi>10.14445/22312803/IJCTT-V13P116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid computing is the recently growing area of computing that share data,
storage, computing across geographically dispersed area. This paper proposes a
novel fuzzy approach using Differential Evolution (DE) for scheduling jobs on
computational grids. The fuzzy based DE generates an optimal plan to complete
the jobs within a minimum period of time. We evaluate the performance of the
proposed fuzzy based DE algorithm with Genetic Algorithm (GA), Simulated
Annealing (SA), Differential Evolution and fuzzy PSO. Experimental results have
shown that the new algorithm produces more optimal solutions for the job
scheduling problems compared to other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6318</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6318</id><created>2014-07-23</created><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Davami</keyname><forenames>Fatemeh</forenames></author></authors><title>A robust and adaptable method for face detection based on Color
  Probabilistic Estimation Technique</title><categories>cs.CV</categories><journal-ref>International Journal of Research in Computer Science A Unit of
  White Globe Publications, Volume 3, Issue 6, 2013</journal-ref><doi>10.7815/ijorcs.36.2013.072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human face perception is currently an active research area in the computer
vision community. Skin detection is one of the most important and primary
stages for this purpose. So far, many approaches are proposed to done this
case. Near all of these methods have tried to find best match intensity
distribution with skin pixels based on popular color spaces such as RGB, HSI or
YCBCR. Results show that these methods cannot provide an accurate approach for
every kind of skin. In this paper, an approach is proposed to solve this
problem using a color probabilistic estimation technique. This approach is
including two stages. In the first one, the skin intensity distribution is
estimated using some train photos of pure skin, and at the second stage, the
skin pixels are detected using Gaussian model and optimal threshold tuning.
Then from the skin region facial features have been extracted to get the face
from the skin region. In the results section, the proposed approach is applied
on FEI database and the accuracy rate reached 99.25%. The proposed approach can
be used for all kinds of skin using train stage which is the main advantage
among the other advantages, such as Low noise sensitivity and low computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6321</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6321</id><created>2014-07-23</created><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Nazari</keyname><forenames>Majid</forenames></author></authors><title>Novel and Automatic Parking Inventory System Based on Pattern
  Recognition and Directional Chain Code</title><categories>cs.CV</categories><journal-ref>2013 First International Conference on computer, Information
  Technology and Digital Media</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to design an efficient vehicle license plate
recognition System and to implement it for automatic parking inventory system.
The system detects the vehicle first and then captures the image of the front
view of the vehicle. Vehicle license plate is localized and characters are
segmented. For finding the place of plate, a novel and real time method is
expressed. A new and robust technique based on directional chain code is used
for character recognition. The resulting vehicle number is then compared with
the available database of all the vehicles so as to come up with information
about the vehicle type and to charge entrance cost accordingly. The system is
then allowed to open parking barrier for the vehicle and generate entrance cost
receipt. The vehicle information (such as entrance time, date, and cost amount)
is also stored in the database to maintain the record. The hardware and
software integrated system is implemented and a working prototype model is
developed. Under the available database, the average accuracy of locating
vehicle license plate obtained 100%. Using 70% samples of character for
training, we tested our scheme on whole samples and obtained 100% correct
recognition rate. Further we tested our character recognition stage on Persian
vehicle data set and we achieved 99% correct recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6327</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6327</id><created>2014-07-22</created><updated>2015-12-16</updated><authors><author><keyname>Wild</keyname><forenames>Marcel</forenames></author></authors><title>Compressed representation of learning spaces</title><categories>cs.DS</categories><comments>28 pages, 5 figures. This substantially extends the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Spaces are certain set systems that are applied in the mathematical
modeling of education. We propose a suitable compression (without loss of
information) of such set systems to facilitate their logical and statistical
analysis. Under certain circumstances compression is the prerequisite to
calculate the Learning Space in the first place. There are connections to the
dual framework of Formal Concept Analysis and in particular to so called
attribute exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6328</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6328</id><created>2014-07-23</created><updated>2014-08-28</updated><authors><author><keyname>Feldman</keyname><forenames>Moran</forenames></author><author><keyname>Izsak</keyname><forenames>Rani</forenames></author></authors><title>Constrained Monotone Function Maximization and the Supermodular Degree</title><categories>cs.DS</categories><comments>23 pages</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximizing a constrained monotone set function has many
practical applications and generalizes many combinatorial problems.
Unfortunately, it is generally not possible to maximize a monotone set function
up to an acceptable approximation ratio, even subject to simple constraints.
One highly studied approach to cope with this hardness is to restrict the set
function. An outstanding disadvantage of imposing such a restriction on the set
function is that no result is implied for set functions deviating from the
restriction, even slightly. A more flexible approach, studied by Feige and
Izsak, is to design an approximation algorithm whose approximation ratio
depends on the complexity of the instance, as measured by some complexity
measure. Specifically, they introduced a complexity measure called supermodular
degree, measuring deviation from submodularity, and designed an algorithm for
the welfare maximization problem with an approximation ratio that depends on
this measure.
  In this work, we give the first (to the best of our knowledge) algorithm for
maximizing an arbitrary monotone set function, subject to a k-extendible
system. This class of constraints captures, for example, the intersection of
k-matroids (note that a single matroid constraint is sufficient to capture the
welfare maximization problem). Our approximation ratio deteriorates gracefully
with the complexity of the set function and k. Our work can be seen as
generalizing both the classic result of Fisher, Nemhauser and Wolsey, for
maximizing a submodular set function subject to a k-extendible system, and the
result of Feige and Izsak for the welfare maximization problem. Moreover, when
our algorithm is applied to each one of these simpler cases, it obtains the
same approximation ratio as of the respective original work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6329</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6329</id><created>2014-07-23</created><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>Perfect codes in Doob graphs</title><categories>math.CO cs.IT math.IT</categories><comments>11pp</comments><msc-class>94B05, 94B25, 05B40</msc-class><doi>10.1007/s10623-015-0066-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study $1$-perfect codes in Doob graphs $D(m,n)$. We show that such codes
that are linear over $GR(4^2)$ exist if and only if $n=(4^{g+d}-1)/3$ and
$m=(4^{g+2d}-4^{g+d})/6$ for some integers $g \ge 0$ and $d&gt;0$. We also prove
necessary conditions on $(m,n)$ for $1$-perfect codes that are linear over
$Z_4$ (we call such codes additive) to exist in $D(m,n)$ graphs; for some of
these parameters, we show the existence of codes. For every $m$ and $n$
satisfying $2m+n=(4^t-1)/3$ and $m \le (4^t-5\cdot 2^{t-1}+1)/9$, we prove the
existence of $1$-perfect codes in $D(m,n)$, without the restriction to admit
some group structure. Keywords: perfect codes, Doob graphs, distance regular
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6333</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6333</id><created>2014-07-23</created><authors><author><keyname>Katz</keyname><forenames>Daniel Martin</forenames></author><author><keyname>Bommarito</keyname><forenames>Michael J</forenames><suffix>II</suffix></author><author><keyname>Blackman</keyname><forenames>Josh</forenames></author></authors><title>Predicting the Behavior of the Supreme Court of the United States: A
  General Approach</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 6 figures; source available at
  https://github.com/mjbommar/scotus-predict</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building upon developments in theoretical and applied machine learning, as
well as the efforts of various scholars including Guimera and Sales-Pardo
(2011), Ruger et al. (2004), and Martin et al. (2004), we construct a model
designed to predict the voting behavior of the Supreme Court of the United
States. Using the extremely randomized tree method first proposed in Geurts, et
al. (2006), a method similar to the random forest approach developed in Breiman
(2001), as well as novel feature engineering, we predict more than sixty years
of decisions by the Supreme Court of the United States (1953-2013). Using only
data available prior to the date of decision, our model correctly identifies
69.7% of the Court's overall affirm and reverse decisions and correctly
forecasts 70.9% of the votes of individual justices across 7,700 cases and more
than 68,000 justice votes. Our performance is consistent with the general level
of prediction offered by prior scholars. However, our model is distinctive as
it is the first robust, generalized, and fully predictive model of Supreme
Court voting behavior offered to date. Our model predicts six decades of
behavior of thirty Justices appointed by thirteen Presidents. With a more sound
methodological foundation, our results represent a major advance for the
science of quantitative legal prediction and portend a range of other potential
applications, such as those described in Katz (2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6342</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6342</id><created>2014-07-15</created><authors><author><keyname>Kumar</keyname><forenames>M V Achutha Kiran</forenames><affiliation>Intel Technologies Ind Pvt Ltd</affiliation></author><author><keyname>Gupta</keyname><forenames>Aarti</forenames><affiliation>Intel Technologies Ind Pvt Ltd</affiliation></author><author><keyname>Bindumadhava</keyname><forenames>S S</forenames><affiliation>Intel Technologies Ind Pvt Ltd</affiliation></author></authors><title>RTL2RTL Formal Equivalence: Boosting the Design Confidence</title><categories>cs.SE cs.AR</categories><comments>In Proceedings FSFMA 2014, arXiv:1407.1952</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 156, 2014, pp. 29-44</journal-ref><doi>10.4204/EPTCS.156.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing design complexity driven by feature and performance requirements
and the Time to Market (TTM) constraints force a faster design and validation
closure. This in turn enforces novel ways of identifying and debugging
behavioral inconsistencies early in the design cycle. Addition of incremental
features and timing fixes may alter the legacy design behavior and would
inadvertently result in undesirable bugs. The most common method of verifying
the correctness of the changed design is to run a dynamic regression test suite
before and after the intended changes and compare the results, a method which
is not exhaustive. Modern Formal Verification (FV) techniques involving new
methods of proving Sequential Hardware Equivalence enabled a new set of
solutions for the given problem, with complete coverage guarantee. Formal
Equivalence can be applied for proving functional integrity after design
changes resulting from a wide variety of reasons, ranging from simple pipeline
optimizations to complex logic redistributions. We present here our experience
of successfully applying the RTL to RTL (RTL2RTL) Formal Verification across a
wide spectrum of problems on a Graphics design. The RTL2RTL FV enabled checking
the design sanity in a very short time, thus enabling faster and safer design
churn. The techniques presented in this paper are applicable to any complex
hardware design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6350</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6350</id><created>2014-07-23</created><authors><author><keyname>Infeld</keyname><forenames>Ewa J.</forenames></author></authors><title>Symmetric Disclosure: a Fresh Look at k-Anonymity</title><categories>cs.CR cs.DB</categories><acm-class>C.2.1; G.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We analyze how the sparsity of a typical aggregate social relation impacts
the network overhead of online communication systems designed to provide
k-anonymity. Once users are grouped in anonymity sets there will likely be few
related pairs of users between any two particular sets, and so the sets need to
be large in order to provide cover traffic between them. We can reduce the
associated overhead by having both parties in a communication specify both the
origin and the target sets of the communication. We propose to call this
communication primitive &quot;symmetric disclosure.&quot; If in order to retrieve
messages a user specifies a group from which he expects to receive them, the
negative impact of the sparsity is offset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6374</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6374</id><created>2014-07-23</created><authors><author><keyname>Lin</keyname><forenames>Trista</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Rivano</keyname><forenames>Herv&#xe9;</forenames><affiliation>CITI Insa Lyon / Inria Grenoble Rh&#xf4;ne-Alpes</affiliation></author><author><keyname>Mou&#xeb;l</keyname><forenames>Fr&#xe9;d&#xe9;ric Le</forenames><affiliation>CSE, CITI</affiliation></author></authors><title>How to Choose the Relevant MAC Protocol for Wireless Smart Parking Urban
  Networks?</title><categories>cs.NI</categories><comments>The 11th ACM International Symposium on Performance Evaluation of
  Wireless Ad Hoc, Sensor, and Ubiquitous Networks (2014)</comments><proxy>ccsd</proxy><doi>10.1145/2653481.2653484</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parking sensor network is rapidly deploying around the world and is regarded
as one of the first implemented urban services in smart cities. To provide the
best network performance, the MAC protocol shall be adaptive enough in order to
satisfy the traffic intensity and variation of parking sensors. In this paper,
we study the heavy-tailed parking and vacant time models from SmartSantander,
and then we apply the traffic model in the simulation with four different kinds
of MAC protocols, that is, contention-based, schedule-based and two hybrid
versions of them. The result shows that the packet interarrival time is no
longer heavy-tailed while collecting a group of parking sensors, and then
choosing an appropriate MAC protocol highly depends on the network
configuration. Also, the information delay is bounded by traffic and MAC
parameters which are important criteria while the timely message is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6384</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6384</id><created>2014-07-21</created><authors><author><keyname>Said</keyname><forenames>Gamal Abd El-Nasser A.</forenames></author><author><keyname>Mahmoud</keyname><forenames>Abeer M.</forenames></author><author><keyname>El-Horbaty</keyname><forenames>El-Sayed M.</forenames></author></authors><title>Solving container terminals problems using computer-based modeling</title><categories>cs.OH</categories><comments>11 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1407.6257</comments><journal-ref>International Journal of Computer Science and Engineering(IJCSE),
  Vol.3, Issue 3,2014, pp.91-100</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the applications of different techniques for solving
container terminals problems. We have built a simulation model that can be used
to analyze the performance of container terminal operations. The proposed
approach is intended to be applied for a real case study in Alexandria
Container Terminal (ACT) at Alexandria port. The implementation of our approach
shows that a proposed model increases the efficiency of Alexandria container
terminal at Alexandria port.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6396</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6396</id><created>2014-07-23</created><updated>2014-11-04</updated><authors><author><keyname>Meyfroyt</keyname><forenames>Thomas M. M.</forenames></author><author><keyname>Borst</keyname><forenames>Sem C.</forenames></author><author><keyname>Boxma</keyname><forenames>Onno J.</forenames></author><author><keyname>Denteneer</keyname><forenames>Dee</forenames></author></authors><title>A Data Propagation Model for Wireless Gossiping</title><categories>cs.NI math.PR</categories><msc-class>60K20</msc-class><acm-class>C.2.1</acm-class><journal-ref>Performance Evaluation 85-86 (2015) 19-32</journal-ref><doi>10.1016/j.peva.2015.01.001.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks require communication protocols for efficiently
propagating data in a distributed fashion. The Trickle algorithm is a popular
protocol serving as the basis for many of the current standard communication
protocols. In this paper we develop a mathematical model describing how Trickle
propagates new data across a network consisting of nodes placed on a line. The
model is analyzed and asymptotic results on the hop count and end-to-end delay
distributions in terms of the Trickle parameters and network density are given.
Additionally, we show that by only a small modification of the Trickle
algorithm the expected end-to-end delay can be greatly decreased. Lastly, we
demonstrate how one can derive the exact hop count and end-to-end delay
distributions for small network sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6404</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6404</id><created>2014-07-23</created><authors><author><keyname>Yu</keyname><forenames>Dan</forenames></author><author><keyname>Chakravorty</keyname><forenames>Suman</forenames></author></authors><title>An autoregressive (AR) model based stochastic unknown input realization
  and filtering technique</title><categories>math.DS cs.SY</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the state estimation problem of linear discrete-time
systems with stochastic unknown inputs. The unknown input is a wide-sense
stationary process while no other prior informaton needs to be known. We
propose an autoregressive (AR) model based unknown input realization technique
which allows us to recover the input statistics from the output data by solving
an appropriate least squares problem, then fit an AR model to the recovered
input statistics and construct an innovations model of the unknown inputs using
the eigensystem realization algorithm (ERA). An augmented state system is
constructed and the standard Kalman filter is applied for state estimation. A
reduced order model (ROM) filter is also introduced to reduce the computational
cost of the Kalman filter. Two numerical examples are given to illustrate the
procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6406</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6406</id><created>2014-07-23</created><authors><author><keyname>Peters</keyname><forenames>Kirstin</forenames></author><author><keyname>Yonova-Karbe</keyname><forenames>Tsvetelina</forenames></author><author><keyname>Nestmann</keyname><forenames>Uwe</forenames></author></authors><title>Matching in the Pi-Calculus (Technical Report)</title><categories>cs.LO</categories><comments>This report extends a paper in EXPRESS/SOS'14 and provides the
  missing proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study whether, in the pi-calculus, the match prefix---a conditional
operator testing two names for (syntactic) equality---is expressible via the
other operators. Previously, Carbone and Maffeis proved that matching is not
expressible this way under rather strong requirements (preservation and
reflection of observables). Later on, Gorla developed a by now widely-tested
set of criteria for encodings that allows much more freedom (e.g. instead of
direct translations of observables it allows comparison of calculi with respect
to reachability of successful states). In this paper, we offer a considerably
stronger separation result on the non-expressibility of matching using only
Gorla's relaxed requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6416</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6416</id><created>2014-07-23</created><authors><author><keyname>Araki</keyname><forenames>Toru</forenames></author><author><keyname>Osawa</keyname><forenames>Shingo</forenames></author><author><keyname>Shimizu</keyname><forenames>Takashi</forenames></author></authors><title>On the distance preserving trees in graphs</title><categories>cs.DM math.CO</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a vertex $v$ of a graph $G$, a spanning tree $T$ of $G$ is
distance-preserving from $v$ if, for any vertex $w$, the distance from $v$ to
$w$ on $T$ is the same as the distance from $v$ to $w$ on $G$. If two vertices
$u$ and $v$ are distinct, then two distance-preserving spanning trees $T_{u}$
from $u$ and $T_{v}$ from $v$ are distinct in general. A purpose of this paper
is to give a characterization for a given weighted graph $G$ to have a spanning
tree $T$ such that $T$ is a distance-preserving spanning tree from distinct two
vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6423</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6423</id><created>2014-07-23</created><authors><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Jiang</keyname><forenames>Longyu</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Performance evaluation of wavelet scattering network in image texture
  classification in various color spaces</title><categories>cs.CV</categories><comments>6 pages, 4 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Texture plays an important role in many image analysis applications. In this
paper, we give a performance evaluation of color texture classification by
performing wavelet scattering network in various color spaces. Experimental
results on the KTH_TIPS_COL database show that opponent RGB based wavelet
scattering network outperforms other color spaces. Therefore, when dealing with
the problem of color texture classification, opponent RGB based wavelet
scattering network is recommended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6426</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6426</id><created>2014-07-23</created><authors><author><keyname>Ferreira</keyname><forenames>Ana Sofia Rufino</forenames></author><author><keyname>Hsia</keyname><forenames>Justin</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author><author><keyname>Maharbiz</keyname><forenames>Michel</forenames></author><author><keyname>Arkin</keyname><forenames>Adam</forenames></author></authors><title>Pattern Formation with a Compartmental Lateral Inhibition System</title><categories>math.DS cs.SY</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a compartmental lateral inhibition system that generates
contrasting patterns of gene expression between neighboring compartments. The
system consists of a set of compartments interconnected by channels. Each
compartment contains a colony of cells that produce diffusible molecules to be
detected by the neighboring colony, and each cell is equipped with an
inhibitory circuit that reduces its production when the detected signal is
stronger. We develop a technique to analyze the steady-state patterns emerging
from this lateral inhibition system and apply it to a specific implementation.
The analysis shows that the proposed system indeed exhibits contrasting
patterns within realistic parameter ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6432</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6432</id><created>2014-07-23</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Learning Structured Outputs from Partial Labels using Forest Ensemble</title><categories>stat.ML cs.CV cs.LG</categories><comments>Conference version appeared in Truyen et al, AdaBoost.MRF: Boosted
  Markov random forests and application to multilevel activity recognition.
  CVPR'06</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning structured outputs with general structures is computationally
challenging, except for tree-structured models. Thus we propose an efficient
boosting-based algorithm AdaBoost.MRF for this task. The idea is based on the
realization that a graph is a superimposition of trees. Different from most
existing work, our algorithm can handle partial labelling, and thus is
particularly attractive in practice where reliable labels are often sparsely
observed. In addition, our method works exclusively on trees and thus is
guaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor video
surveillance scenario, where activities are modelled at multiple levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6439</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6439</id><created>2014-07-23</created><updated>2014-09-18</updated><authors><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Sadeghian</keyname><forenames>Amir Abbas</forenames></author><author><keyname>Shan</keyname><forenames>Zifei</forenames></author><author><keyname>Shin</keyname><forenames>Jaeho</forenames></author><author><keyname>Wang</keyname><forenames>Feiran</forenames></author><author><keyname>Wu</keyname><forenames>Sen</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author></authors><title>Feature Engineering for Knowledge Base Construction</title><categories>cs.DB cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge base construction (KBC) is the process of populating a knowledge
base, i.e., a relational database together with inference rules, with
information extracted from documents and structured sources. KBC blurs the
distinction between two traditional database problems, information extraction
and information integration. For the last several years, our group has been
building knowledge bases with scientific collaborators. Using our approach, we
have built knowledge bases that have comparable and sometimes better quality
than those constructed by human volunteers. In contrast to these knowledge
bases, which took experts a decade or more human years to construct, many of
our projects are constructed by a single graduate student.
  Our approach to KBC is based on joint probabilistic inference and learning,
but we do not see inference as either a panacea or a magic bullet: inference is
a tool that allows us to be systematic in how we construct, debug, and improve
the quality of such systems. In addition, inference allows us to construct
these systems in a more loosely coupled way than traditional approaches. To
support this idea, we have built the DeepDive system, which has the design goal
of letting the user &quot;think about features---not algorithms.&quot; We think of
DeepDive as declarative in that one specifies what they want but not how to get
it. We describe our approach with a focus on feature engineering, which we
argue is an understudied problem relative to its importance to end-to-end
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6447</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6447</id><created>2014-07-24</created><updated>2014-08-12</updated><authors><author><keyname>Oka</keyname><forenames>Mizuki</forenames></author><author><keyname>Hashimoto</keyname><forenames>Yasuhiro</forenames></author><author><keyname>Ikegami</keyname><forenames>Takashi</forenames></author></authors><title>Self-organization on social media: endo-exo bursts and baseline
  fluctuations</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at WebAL-1: Workshop on Artificial Life and the Web 2014
  (arXiv:1406.2507)</comments><report-no>WebAL1/2014/07</report-no><doi>10.1371/journal.pone.0109293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A salient dynamic property of social media is bursting behavior. In this
paper, we study bursting behavior in terms of the temporal relation between a
preceding baseline fluctuation and the successive burst response using a
frequency time series of 3,000 keywords on Twitter. We found that there is a
fluctuation threshold up to which the burst size increases as the fluctuation
increases and that above the threshold, there appears a variety of burst sizes.
We call this threshold the critical threshold. Investigating this threshold in
relation to endogenous bursts and exogenous bursts based on peak ratio and
burst size reveals that the bursts below this threshold are endogenously caused
and above this threshold, exogenous bursts emerge. Analysis of the 3,000
keywords shows that all the nouns have both endogenous and exogenous origins of
bursts and that each keyword has a critical threshold in the baseline
fluctuation value to distinguish between the two. Having a threshold for an
input value for activating the system implies that Twitter is an excitable
medium. These findings are useful for characterizing how excitable a keyword is
on Twitter and could be used, for example, to predict the response to
particular information on social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6453</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6453</id><created>2014-07-24</created><authors><author><keyname>Jacobs</keyname><forenames>Frederic</forenames></author></authors><title>Providing better confidentiality and authentication on the Internet
  using Namecoin and MinimaLT</title><categories>cs.CR cs.NI</categories><comments>Paper available on GitHub: https://github.com/FredericJacobs/safeweb</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we introduce a duo of improvements for the Internet that would
lead to better security. The authentication model on the Internet is broken and
TLS connections have a considerable overhead. We try to address those issues
with changes in both the application layer, discussing a replacement for the
DNS system, and in the transport layer, a drop-in replacement for TCP built on
top of UDP so that it can run on today's internet infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6456</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6456</id><created>2014-07-24</created><authors><author><keyname>Adeogun</keyname><forenames>Ramoni</forenames></author><author><keyname>Teal</keyname><forenames>Paul</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel</forenames></author></authors><title>An Asymptotic Bound on Estimation and Prediction of Mobile MIMO-OFDM
  Wireless Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive an asymptotic closed--form expression for the error
bound on extrapolation of doubly selective mobile MIMO wireless channels. The
bound shows the relationship between the prediction error and system design
parameters such as bandwidth, number of antenna elements, and number of
frequency and temporal pilots, thereby providing useful insights into the
effects of these parameters on prediction performance. Numerical simulations
show that the asymptotic bound provides a good approximation to previously
derived bounds while eliminating the need for repeated computation and
dependence on channel parameters such as angles of arrival and departure,
delays and Doppler shifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6463</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6463</id><created>2014-07-24</created><authors><author><keyname>Economou</keyname><forenames>Frossie</forenames></author><author><keyname>Hoblitt</keyname><forenames>Joshua C.</forenames></author><author><keyname>Norris</keyname><forenames>Pat</forenames></author></authors><title>Your data is your dogfood: DevOps in the astronomical observatory</title><categories>astro-ph.IM cs.SE</categories><comments>7 pages, invited talk at Software and Cyberinfrastructure for
  Astronomy III, SPIE Astronomical Telescopes and Instrumentation conference,
  June 2014, Paper ID 9152-38</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  DevOps is the contemporary term for a software development culture that
purposefully blurs distinction between software development and IT operations
by treating &quot;infrastructure as code.&quot; DevOps teams typically implement
practices summarised by the colloquial directive to &quot;eat your own dogfood;&quot;
meaning that software tools developed by a team should be used internally
rather thrown over the fence to operations or users. We present a brief
overview of how DevOps techniques bring proven software engineering practices
to IT operations. We then discuss the application of these practices to
astronomical observatories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6470</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6470</id><created>2014-07-24</created><authors><author><keyname>D'Angelo</keyname><forenames>Gabriele</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author></authors><title>New Trends in Parallel and Distributed Simulation: from Many-Cores to
  Cloud Computing</title><categories>cs.DC cs.AR</categories><comments>To appear in Simulation Modelling Practice and Theory (SIMPAT),
  Elsevier, 2014</comments><doi>10.1016/j.simpat.2014.06.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in computing architectures and networking are bringing
parallel computing systems to the masses so increasing the number of potential
users of these kinds of systems. In particular, two important technological
evolutions are happening at the ends of the computing spectrum: at the &quot;small&quot;
scale, processors now include an increasing number of independent execution
units (cores), at the point that a mere CPU can be considered a parallel
shared-memory computer; at the &quot;large&quot; scale, the Cloud Computing paradigm
allows applications to scale by offering resources from a large pool on a
pay-as-you-go model. Multi-core processors and Clouds both require applications
to be suitably modified to take advantage of the features they provide. In this
paper, we analyze the state of the art of parallel and distributed simulation
techniques, and assess their applicability to multi-core architectures or
Clouds. It turns out that most of the current approaches exhibit limitations in
terms of usability and adaptivity which may hinder their application to these
new computing architectures. We propose an adaptive simulation mechanism, based
on the multi-agent system paradigm, to partially address some of those
limitations. While it is unlikely that a single approach will work well on both
settings above, we argue that the proposed adaptive mechanism has useful
features which make it attractive both in a multi-core processor and in a Cloud
system. These features include the ability to reduce communication costs by
migrating simulation components, and the support for adding (or removing) nodes
to the execution architecture at runtime. We will also show that, with the help
of an additional support layer, parallel and distributed simulations can be
executed on top of unreliable resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6477</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6477</id><created>2014-07-24</created><authors><author><keyname>HosseinNia</keyname><forenames>S. Hassan</forenames></author><author><keyname>Tejado</keyname><forenames>Ines</forenames></author><author><keyname>Vinagre</keyname><forenames>Blas M.</forenames></author></authors><title>Hybrid Systems and Control With Fractional Dynamics (I): Modeling and
  Analysis</title><categories>cs.SY nlin.AO</categories><comments>2014 International Conference on Fractional Differentiation and its
  Application, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  No mixed research of hybrid and fractional-order systems into a cohesive and
multifaceted whole can be found in the literature. This paper focuses on such a
synergistic approach of the theories of both branches, which is believed to
give additional flexibility and help to the system designer. It is part I of
two companion papers and introduces the fundamentals of fractional-order hybrid
systems, in particular, modeling and stability analysis of two kinds of such
systems, i.e., fractional-order switching and reset control systems. Some
examples are given to illustrate the applicability and effectiveness of the
developed theory. Part II will focus on fractional-order hybrid control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6480</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6480</id><created>2014-07-24</created><authors><author><keyname>HosseinNia</keyname><forenames>S. Hassan</forenames></author><author><keyname>Tejado</keyname><forenames>Ines</forenames></author><author><keyname>Vinagre</keyname><forenames>Blas M.</forenames></author></authors><title>Hybrid Systems and Control With Fractional Dynamics (II): Control</title><categories>cs.SY nlin.AO</categories><comments>2014 International Conference on Fractional Differentiation and its
  Application, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  No mixed research of hybrid and fractional-order systems into a cohesive and
multifaceted whole can be found in the literature. This paper focuses on such a
synergistic approach of the theories of both branches, which is believed to
give additional flexibility and help the system designer. It is part II of two
companion papers and focuses on fractional-order hybrid control. Specifically,
two types of such techniques are reviewed, including robust control of
switching systems and different strategies of reset control. Simulations and
experimental results are given to show the effectiveness of the proposed
strategies. Part I will introduce the fundamentals of fractional-order hybrid
systems, in particular, modelling and stability of two kinds of such systems,
i.e., fractional-order switching and reset control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6481</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6481</id><created>2014-07-24</created><updated>2015-05-04</updated><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Interference Management in 5G Reverse TDD HetNets with Wireless
  Backhaul: A Large System Analysis</title><categories>cs.IT math.IT</categories><comments>14 pages, 12 figures. To appear IEEE J. Select. Areas Commun. --
  Special Issue on HetNets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyzes a heterogeneous network (HetNet), which comprises a macro
base station (BS) equipped with a large number of antennas and an overlaid
dense tier of small cell access points (SCAs) using a wireless backhaul for
data traffic. The static and low mobility user equipment terminals (UEs) are
associated with the SCAs while those with medium-to-high mobility are served by
the macro BS. A reverse time division duplexing (TDD) protocol is used by the
two tiers, which allows the BS to locally estimate both the intra-tier and
inter-tier channels. This knowledge is then used at the BS either in the uplink
(UL) or in the downlink (DL) to simultaneously serve the macro UEs (MUEs) and
to provide the wireless backhaul to SCAs. A geographical separation of
co-channel SCAs is proposed to limit the interference coming from the UL
signals of MUEs. A concatenated linear precoding technique employing either
zero-forcing (ZF) or regularized ZF is used at the BS to simultaneously serve
MUEs and SCAs in DL while nulling interference toward those SCAs in UL. We
evaluate and characterize the performance of the system through the power
consumption of UL and DL transmissions under the assumption that target rates
must be satisfied and imperfect channel state information is available for
MUEs. The analysis is conducted in the asymptotic regime where the number of BS
antennas and the network size (MUEs and SCAs) grow large with fixed ratios.
Results from large system analysis are used to provide concise formulae for the
asymptotic UL and DL transmit powers and precoding vectors under the above
assumptions. Numerical results are used to validate the analysis in different
settings and to make comparisons with alternative network architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6482</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6482</id><created>2014-07-24</created><updated>2014-09-29</updated><authors><author><keyname>Das</keyname><forenames>Niva</forenames></author><author><keyname>Sarkar</keyname><forenames>Tanmoy</forenames></author></authors><title>Securing Cloud from Cloud Drain</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, in the world of communication, connected systems is growing at a rapid
pace. To accommodate this growth the need for computational power and storage
is also increasing at a similar rate. Companies are investing a large amount of
resources in buying, maintaining and ensuring availability of the system to
their customers. To mitigate these issues, cloud computing is playing a major
role.The underlying concept of cloud computing dates back to the 50's but the
term entering into widespread usage can be traced to 2006 when Amazon.com
announced the Elastic Compute Cloud.In this paper, we will discuss about cloud
security approaches. We have used the term Cloud-Drain to define data leakage
in case of security compromise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6486</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6486</id><created>2014-07-24</created><updated>2015-03-30</updated><authors><author><keyname>Minion</keyname><forenames>Michael</forenames></author><author><keyname>Speck</keyname><forenames>Robert</forenames></author><author><keyname>Bolten</keyname><forenames>Matthias</forenames></author><author><keyname>Emmett</keyname><forenames>Matthew</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author></authors><title>Interweaving PFASST and Parallel Multigrid</title><categories>math.NA cs.DC cs.NA</categories><journal-ref>SIAM Journal on Scientific Computing 37(5), pp. S244-S263, 2015</journal-ref><doi>10.1137/14097536X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel full approximation scheme in space and time (PFASST) introduced
by Emmett and Minion in 2012 is an iterative strategy for the temporal
parallelization of ODEs and discretized PDEs. As the name suggests, PFASST is
similar in spirit to a space-time FAS multigrid method performed over multiple
time-steps in parallel. However, since the original focus of PFASST has been on
the performance of the method in terms of time parallelism, the solution of any
spatial system arising from the use of implicit or semi-implicit temporal
methods within PFASST have simply been assumed to be solved to some desired
accuracy completely at each sub-step and each iteration by some unspecified
procedure. It hence is natural to investigate how iterative solvers in the
spatial dimensions can be interwoven with the PFASST iterations and whether
this strategy leads to a more efficient overall approach. This paper presents
an initial investigation on the relative performance of different strategies
for coupling PFASST iterations with multigrid methods for the implicit
treatment of diffusion terms in PDEs. In particular, we compare full accuracy
multigrid solves at each sub-step with a small fixed number of multigrid
V-cycles. This reduces the cost of each PFASST iteration at the possible
expense of a corresponding increase in the number of PFASST iterations needed
for convergence. Parallel efficiency of the resulting methods is explored
through numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6490</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6490</id><created>2014-07-24</created><updated>2014-12-12</updated><authors><author><keyname>Hu</keyname><forenames>Wuhua</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author></authors><title>Multi-hop Diffusion LMS for Energy-constrained Distributed Estimation</title><categories>math.OC cs.IT math.IT</categories><comments>14 pages, 12 figures. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multi-hop diffusion strategy for a sensor network to perform
distributed least mean-squares (LMS) estimation under local and network-wide
energy constraints. At each iteration of the strategy, each node can combine
intermediate parameter estimates from nodes other than its physical neighbors
via a multi-hop relay path. We propose a rule to select combination weights for
the multi-hop neighbors, which can balance between the transient and the
steady-state network mean-square deviations (MSDs). We study two classes of
networks: simple networks with a unique transmission path from one node to
another, and arbitrary networks utilizing diffusion consultations over at most
two hops. We propose a method to optimize each node's information neighborhood
subject to local energy budgets and a network-wide energy budget for each
diffusion iteration. This optimization requires the network topology, and the
noise and data variance profiles of each node, and is performed offline before
the diffusion process. In addition, we develop a fully distributed and adaptive
algorithm that approximately optimizes the information neighborhood of each
node with only local energy budget constraints in the case where diffusion
consultations are performed over at most a predefined number of hops. Numerical
results suggest that our proposed multi-hop diffusion strategy achieves the
same steady-state MSD as the existing one-hop adapt-then-combine diffusion
algorithm but with a lower energy budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6492</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6492</id><created>2014-07-24</created><updated>2014-08-14</updated><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Davami</keyname><forenames>Fatemeh</forenames></author><author><keyname>Shayegh</keyname><forenames>Hamid Reza</forenames></author></authors><title>Recognition of Handwritten Persian/Arabic Numerals Based on Robust
  Feature Set and K-NN Classifier</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the main author due to the Table 1
  and equation 2 errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author due to a crucial sign error in
equation 2 and some mistake in Table 1 information. please let me for changing
this information and updating this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6496</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6496</id><created>2014-07-24</created><updated>2014-09-15</updated><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Baghdadi</keyname><forenames>Mohammad</forenames></author></authors><title>Novel and Fast Algorithm for Extracting License Plate Location Based on
  Edge Analysis</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1, and some mistake in Table 1 information. please remove
  this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays in developing or developed countries, the Intelligent Transportation
System (ITS) technology has attracted so much attention to itself. License
Plate Recognition (LPR) systems have many applications in ITSs, such as the
payment of parking fee, controlling the traffic volume, traffic data
collection, etc. This paper presents a new and fast method for license plate
extraction based on edge analysis. our proposed method consist of four stage,
which are edge detection, non-useable edge and noise removing, edge analysis
and morphology-based license plate extraction. In the result part, the proposed
algorithm is applied on vehicle database and the accuracy rate reached 98%.
From the experimental results it is shown that the proposed method gives fairly
acceptable level of accuracy for practical license plate recognition system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6498</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6498</id><created>2014-07-24</created><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Azad</keyname><forenames>Babak</forenames></author><author><keyname>Shayegh</keyname><forenames>Hamid Reza</forenames></author></authors><title>Real-Time and Efficient Method for Accuracy Enhancement of Edge Based
  License Plate Recognition System</title><categories>cs.CV</categories><comments>2013 First International Conference on computer, Information
  Technology and Digital Media. arXiv admin note: substantial text overlap with
  arXiv:1407.6321</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  License Plate Recognition plays an important role on the traffic monitoring
and parking management. Administration and restriction of those transportation
tools for their better service becomes very essential. In this paper, a fast
and real time method has an appropriate application to find plates that the
plat has tilt and the picture quality is poor. In the proposed method, at the
beginning, the image is converted into binary mode with use of adaptive
threshold. And with use of edge detection and morphology operation, plate
number location has been specified and if the plat has tilt; its tilt is
removed away. Then its characters are distinguished using image processing
techniques. Finally, K Nearest Neighbour (KNN) classifier was used for
character recognition. This method has been tested on available data set that
has different images of the background, considering distance, and angel of view
so that the correct extraction rate of plate reached at 98% and character
recognition rate achieved at 99.12%. Further we tested our character
recognition stage on Persian vehicle data set and we achieved 99% correct
recognition rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6506</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6506</id><created>2014-07-24</created><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Shayegh</keyname><forenames>Hamid Reza</forenames></author></authors><title>Novel and Tuneable Method for Skin Detection Based on Hybrid Color Space
  and Color Statistical Features</title><categories>cs.CV</categories><comments>2013 First International Conference on computer, Information
  Technology and Digital Media. arXiv admin note: substantial text overlap with
  arXiv:1407.6318</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skin detection is one of the most important and primary stages in some of
image processing applications such as face detection and human tracking. So
far, many approaches are proposed to done this case. Near all of these methods
have tried to find best match intensity distribution with skin pixels based on
popular color spaces such as RGB, CMYK or YCbCr. Results show these methods
cannot provide an accurate approach for every kinds of skin. In this paper, an
approach is proposed to solve this problem using statistical features
technique. This approach is including two stages. In the first one, from pure
skin statistical features were extracted and at the second stage, the skin
pixels are detected using HSV and YCbCr color spaces. In the result part, the
proposed approach is applied on FEI database and the accuracy rate reached
99.25 + 0.2. Further proposed method is applied on complex background database
and accuracy rate obtained 95.40+0.31%. The proposed approach can be used for
all kinds of skin using train stage which is the main advantages of it. Low
noise sensitivity and low computational complexity are some of other
advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6507</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6507</id><created>2014-07-24</created><authors><author><keyname>Sakib</keyname><forenames>Kazi</forenames></author><author><keyname>Kamal</keyname><forenames>Mosaddek Hossain</forenames></author><author><keyname>Kabir</keyname><forenames>Upama</forenames></author></authors><title>A New Routing Protocol for All Optical Network</title><categories>cs.NI</categories><comments>10 Pages</comments><journal-ref>Dhaka University Journal of Science, Bangladesh, Vol. 53, No. 2,
  July 2005, pp. 1-10</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper, an efficient routing protocol for all optical network
(AOL) is proposed. The technique uses wavelength division multiplexing (WDM).
The proposed one is different from the conventional AOL protocol in
transmission of data and control over optical fiber. A set of wavelengths is
reserved to transfer control information, which is defined as control
wavelengths. Control wavelengths are routed with packet routing scheme and the
others are routed with wavelength routing scheme. In connection oriented
network only the control packets are sent with the control wavelengths, data or
messages are sent with other wavelengths. On the other hand, in datagram
network, all the packets are sent with the control wavelengths. The allocation
of wavelengths may be fully dynamic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6508</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6508</id><created>2014-07-24</created><authors><author><keyname>Carlson</keyname><forenames>Frederick R.</forenames></author></authors><title>Security Penetration Test Framework for the Diameter Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines the infrastructure required for a penetration testing
suite centered around the cellular call control protocol called Diameter. A
brief description of Diameter is given along with the basic equipment and
design requirements to conduct the testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6510</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6510</id><created>2014-07-24</created><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Shayegh</keyname><forenames>Hamid Reza</forenames></author></authors><title>New Method for Optimization of License Plate Recognition system with Use
  of Edge Detection and Connected Component</title><categories>cs.CV</categories><comments>3rd IEEE International Conference on Computer and Knowledge
  Engineering (ICCKE 2013), October 31 &amp; November 1, 2013, Ferdowsi Universit
  Mashhad</comments><doi>10.1109/ICCKE.2013.6682800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  License Plate recognition plays an important role on the traffic monitoring
and parking management systems. In this paper, a fast and real time method has
been proposed which has an appropriate application to find tilt and poor
quality plates. In the proposed method, at the beginning, the image is
converted into binary mode using adaptive threshold. Then, by using some edge
detection and morphology operations, plate number location has been specified.
Finally, if the plat has tilt, its tilt is removed away. This method has been
tested on another paper data set that has different images of the background,
considering distance, and angel of view so that the correct extraction rate of
plate reached at 98.66%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6513</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6513</id><created>2014-07-24</created><authors><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Salavati</keyname><forenames>Amir Hesam</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Amin</forenames></author></authors><title>Convolutional Neural Associative Memories: Massive Capacity with Noise
  Tolerance</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of a neural associative memory is to retrieve a set of previously
memorized patterns from their noisy versions using a network of neurons. An
ideal network should have the ability to 1) learn a set of patterns as they
arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize
the pattern retrieval capacity while maintaining the reliability in responding
to queries. The majority of work on neural associative memories has focused on
designing networks capable of memorizing any set of randomly chosen patterns at
the expense of limiting the retrieval capacity. In this paper, we show that if
we target memorizing only those patterns that have inherent redundancy (i.e.,
belong to a subspace), we can obtain all the aforementioned properties. This is
in sharp contrast with the previous work that could only improve one or two
aspects at the expense of the third. More specifically, we propose framework
based on a convolutional neural network along with an iterative algorithm that
learns the redundancy among the patterns. The resulting network has a retrieval
capacity that is exponential in the size of the network. Moreover, the
asymptotic error correction performance of our network is linear in the size of
the patterns. We then ex- tend our approach to deal with patterns lie
approximately in a subspace. This extension allows us to memorize datasets
containing natural patterns (e.g., images). Finally, we report experimental
results on both synthetic and real datasets to support our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6553</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6553</id><created>2014-07-24</created><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames></author></authors><title>On number of nonzero cells in some two-dimensional reversible
  second-order cellular automata</title><categories>math.CO cs.DM nlin.CG</categories><comments>LaTeX, 11pt, 15 pages, 7 figures, comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive equations for the number of cells with nonzero values at $n$-th
step for some two-dimensional reversible second-order cellular automata are
proved in this work. Initial configuration is a single cell with the value one
and all others zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6559</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6559</id><created>2014-07-24</created><authors><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Sach</keyname><forenames>Benjamin</forenames></author></authors><title>Cell-Probe Bounds for Online Edit Distance and Other Pattern Matching
  Problems</title><categories>cs.DS</categories><comments>32 pages, 4 figures</comments><acm-class>F.2.2; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give cell-probe bounds for the computation of edit distance, Hamming
distance, convolution and longest common subsequence in a stream. In this
model, a fixed string of $n$ symbols is given and one $\delta$-bit symbol
arrives at a time in a stream. After each symbol arrives, the distance between
the fixed string and a suffix of most recent symbols of the stream is reported.
The cell-probe model is perhaps the strongest model of computation for showing
data structure lower bounds, subsuming in particular the popular word-RAM
model.
  * We first give an $\Omega((\delta \log n)/(w+\log\log n))$ lower bound for
the time to give each output for both online Hamming distance and convolution,
where $w$ is the word size. This bound relies on a new encoding scheme and for
the first time holds even when $w$ is as small as a single bit.
  * We then consider the online edit distance and longest common subsequence
problems in the bit-probe model ($w=1$) with a constant sized input alphabet.
We give a lower bound of $\Omega(\sqrt{\log n}/(\log\log n)^{3/2})$ which
applies for both problems. This second set of results relies both on our new
encoding scheme as well as a carefully constructed hard distribution.
  * Finally, for the online edit distance problem we show that there is an
$O((\log n)^2/w)$ upper bound in the cell-probe model. This bound gives a
contrast to our new lower bound and also establishes an exponential gap between
the known cell-probe and RAM model complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6560</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6560</id><created>2014-07-24</created><updated>2014-07-29</updated><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Foshammer</keyname><forenames>Louise</forenames></author><author><keyname>Neve-Gr&#xe6;sb&#xf8;ll</keyname><forenames>Malte</forenames></author></authors><title>Combining subspace codes with classical linear error-correcting codes</title><categories>cs.IT math.IT</categories><comments>6 pages. The below paper was written in May 2014. The authors have
  come to know that there is a significant overlap with previous results by
  Vitaly Skachek, Olgica Milenkovic, and Angelia Nedic published in July 2011
  as arXiv:1107.4581. Their paper entitled &quot;Hybrid Noncoherent Network Coding&quot;
  appeared in IEEE Transactions on Information Theory, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how subspace codes can be used to simultaneously correct errors
and erasures when the network performs random linear network coding and the
edges are noisy channels. This is done by combining the subspace code with a
classical linear error-correcting code. The classical code then takes care of
the errors and the subspace codes takes care of the erasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6571</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6571</id><created>2014-07-24</created><authors><author><keyname>Laitinen</keyname><forenames>Tero</forenames></author><author><keyname>Junttila</keyname><forenames>Tommi</forenames></author><author><keyname>Niemel&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Conflict-Driven XOR-Clause Learning (extended version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern conflict-driven clause learning (CDCL) SAT solvers are very good in
solving conjunctive normal form (CNF) formulas. However, some application
problems involve lots of parity (xor) constraints which are not necessarily
efficiently handled if translated into CNF. This paper studies solving CNF
formulas augmented with xor-clauses in the DPLL(XOR) framework where a CDCL SAT
solver is coupled with a separate xor-reasoning module. New techniques for
analyzing xor-reasoning derivations are developed, allowing one to obtain
smaller CNF clausal explanations for xor-implied literals and also to derive
and learn new xor-clauses. It is proven that these new techniques allow very
short unsatisfiability proofs for some formulas whose CNF translations do not
have polynomial size resolution proofs, even when a very simple xor-reasoning
module capable only of unit propagation is applied. The efficiency of the
proposed techniques is evaluated on a set of challenging logical cryptanalysis
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6580</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6580</id><created>2014-07-21</created><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames><affiliation>Graz University of Technology, Austria</affiliation></author><author><keyname>Jacobs</keyname><forenames>Swen</forenames><affiliation>Graz University of Technology, Austria</affiliation></author><author><keyname>Khalimov</keyname><forenames>Ayrat</forenames><affiliation>Graz University of Technology, Austria</affiliation></author></authors><title>Parameterized Synthesis Case Study: AMBA AHB</title><categories>cs.LO cs.SE</categories><comments>Conference version of arXiv:1406.7608. In Proceedings SYNT 2014,
  arXiv:1407.4937</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 157, 2014, pp. 68-83</journal-ref><doi>10.4204/EPTCS.157.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the AMBA AHB case study that has been used as a benchmark for
several reactive synthesis tools. Synthesizing AMBA AHB implementations that
can serve a large number of masters is still a difficult problem. We
demonstrate how to use parameterized synthesis in token rings to obtain an
implementation for a component that serves a single master, and can be arranged
in a ring of arbitrarily many components. We describe new tricks - property
decompositional synthesis, and direct encoding of simple GR(1) - that together
with previously described optimizations allowed us to synthesize a component
model with 14 states in about 1 hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6598</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6598</id><created>2014-07-24</created><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>The basic reproduction number as a predictor for epidemic outbreaks in
  temporal networks</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0120567</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic reproduction number R0 -- the number of individuals directly
infected by an infectious person in an otherwise susceptible population -- is
arguably the most widely used estimator of how severe an epidemic outbreak can
be. This severity can be more directly measured as the fraction people infected
once the outbreak is over, {\Omega}. In traditional mathematical epidemiology
and common formulations of static network epidemiology, there is a
deterministic relationship between R0 and {\Omega}. However, if one considers
disease spreading on a temporal contact network -- where one knows when
contacts happen, not only between whom -- then larger R0 does not necessarily
imply larger {\Omega}. In this paper, we numerically investigate the
relationship between R0 and {\Omega} for a set of empirical temporal networks
of human contacts. Among 31 explanatory descriptors of temporal network
structure, we identify those that make R0 an imperfect predictor of {\Omega}.
We find that descriptors related to both temporal and topological aspects
affect the relationship between R0 and {\Omega}, but in different ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6600</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6600</id><created>2014-07-24</created><updated>2015-02-22</updated><authors><author><keyname>Ghavami</keyname><forenames>Siavash</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>On Capacity and Capacity per Unit Cost of Gaussian Multiple Access
  Channel with Peak Power Constraints</title><categories>cs.IT math.IT</categories><comments>28 pages, 5 figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the capacity and capacity per unit cost of Gaussian
multiple access-channel (GMAC) with peak power constraints. We first devise an
approach based on Blahut-Arimoto Algorithm to numerically optimize the sum rate
and quantify the corresponding input distributions. The results reveal that in
the case with identical peak power constraints, the user with higher SNR is to
have a symmetric antipodal input distribution for all values of noise variance.
Next, we analytically derive and characterize an achievable rate region for the
capacity in cases with small peak power constraints, which coincides with the
capacity in a certain scenario. The capacity per unit cost is of interest in
low power regimes and is a target performance measure in energy efficient
communications. In this work, we derive the capacity per unit cost of additive
white Gaussian channel and GMAC with peak power constraints. The results in
case of GMAC demonstrate that the capacity per unit cost is obtained using
antipodal signaling for both users and is independent of users rate ratio. We
characterize the optimized transmission strategies obtained for capacity and
capacity per unit cost with peak-power constraint in detail and specifically in
contrast to the settings with average-power constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6603</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6603</id><created>2014-07-24</created><authors><author><keyname>Alyasseri</keyname><forenames>Zaid Abdi Alkareem</forenames><affiliation>ISMAIL</affiliation></author><author><keyname>Al-Attar</keyname><forenames>Kadhim</forenames><affiliation>ISMAIL</affiliation></author><author><keyname>Nasser</keyname><forenames>Mazin</forenames><affiliation>ISMAIL</affiliation></author></authors><title>Parallelize Bubble Sort Algorithm Using OpenMP</title><categories>cs.DC</categories><comments>4 pages, 5 firgyes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sorting has been a profound area for the algorithmic researchers and many
resources are invested to suggest more works for sorting algorithms. For this
purpose, many existing sorting algorithms were observed in terms of the
efficiency of the algorithmic complexity. In this paper we implemented the
bubble sort algorithm using multithreading (OpenMP). The proposed work tested
on two standard datasets (text file) with different size . The main idea of the
proposed algorithm is distributing the elements of the input datasets into many
additional temporary sub-arrays according to a number of characters in each
word. The sizes of each of these sub-arrays are decided depending on a number
of elements with the same number of characters in the input array. We
implemented OpenMP using Intel core i7-3610QM ,(8 CPUs),using two approaches
(vectors of string and array 3D) . Finally, we get the data structure effects
on the performance of the algorithm for that we choice the second approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6621</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6621</id><created>2014-07-24</created><updated>2015-08-13</updated><authors><author><keyname>Garas</keyname><forenames>Antonios</forenames></author></authors><title>Reaction-Diffusion Processes on Interconnected Scale-Free Networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 5 figures</comments><journal-ref>Phys. Rev. E 92, 020801 (2015)</journal-ref><doi>10.1103/PhysRevE.92.020801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the two particle annihilation reaction $A+B\rightarrow \emptyset$ on
interconnected scale free networks, using different interconnecting strategies.
We explore how the mixing of particles and the process evolution are influenced
by the number of interconnecting links, by their functional properties, and by
the interconnectivity strategies in use. We show that the reaction rates on
this system are faster than what was observed in other topologies, due to the
better particle mixing which suppresses the segregation effect, inline with
previous studies performed on single scale free networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6637</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6637</id><created>2014-07-24</created><authors><author><keyname>Hermans</keyname><forenames>Michiel</forenames></author><author><keyname>Burm</keyname><forenames>Micha&#xeb;l</forenames></author><author><keyname>Dambre</keyname><forenames>Joni</forenames></author><author><keyname>Bienstman</keyname><forenames>Peter</forenames></author></authors><title>Trainable and Dynamic Computing: Error Backpropagation through Physical
  Media</title><categories>cs.NE</categories><doi>10.1038/ncomms7729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning algorithms, and more in particular neural networks, arguably
experience a revolution in terms of performance. Currently, the best systems we
have for speech recognition, computer vision and similar problems are based on
neural networks, trained using the half-century old backpropagation algorithm.
Despite the fact that neural networks are a form of analog computers, they are
still implemented digitally for reasons of convenience and availability. In
this paper we demonstrate how we can design physical linear dynamic systems
with non-linear feedback as a generic platform for dynamic, neuro-inspired
analog computing. We show that a crucial advantage of this setup is that the
error backpropagation can be performed physically as well, which greatly speeds
up the optimisation process. As we show in this paper, using one experimentally
validated and one conceptual example, such systems may be the key to providing
a relatively straightforward mechanism for constructing highly scalable, fully
dynamic analog computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6639</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6639</id><created>2014-07-24</created><updated>2015-12-09</updated><authors><author><keyname>Timm</keyname><forenames>Torsten</forenames></author></authors><title>How the Voynich Manuscript was created</title><categories>cs.CR cs.CL</categories><comments>96 pages, 17 figures, revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Voynich manuscript is a medieval book written in an unknown script. This
paper studies the relation between similarly spelled words in the Voynich
manuscript. By means of a detailed analysis of similar spelled words it was
possible to reveal the text generation method used for the Voynich manuscript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6655</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6655</id><created>2014-07-24</created><authors><author><keyname>Thomas</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LISE</affiliation></author><author><keyname>Delatour</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>LIST</affiliation></author><author><keyname>Terrier</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LIST</affiliation></author><author><keyname>Brun</keyname><forenames>Matthias</forenames></author><author><keyname>G&#xe9;rard</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Contribution \`a la mod\'elisation explicite des plates-formes
  d'ex\'ecution pour l'IDM</title><categories>cs.SE</categories><comments>23 pages, in French</comments><proxy>ccsd</proxy><journal-ref>L'Objet, logiciel, base de donn\'ees, r\'eseaux (RSTI s\'erie) 13,
  4 (2007) 9-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One foundation of the model driven engineering (MDE) is to separate the
modelling application description from its technological implementation (i.e.
platform). Some of them are dedicated to the system execution. Hence, one
promise solution of the MDE is to automate transformations from platform
independent models to platform specific models. Little work has explicitly
described platform characteristics. Yet, an explicit modelling allows taking in
account their characteristics more easily (par ex., performances,
maintainability,portability). This paper presents both an execution platform
modelling state of art and a pattern to describe execution platform modelling
framework. It intends to confirm the feasibility and the interests in
describing an execution platform metamodel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6665</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6665</id><created>2014-06-27</created><authors><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>&#xd6;zkan</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author></authors><title>A Tight Lower Bound for Decrease-Key in the Pure Heap Model</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1302.6641</comments><acm-class>E.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve the lower bound on the amortized cost of the decrease-key
operation in the pure heap model and show that any pure-heap-model heap (that
has a \bigoh{\log n} amortized-time extract-min operation) must spend
\bigom{\log\log n} amortized time on the decrease-key operation. Our result
shows that sort heaps as well as pure-heap variants of numerous other heaps
have asymptotically optimal decrease-key operations in the pure heap model. In
addition, our improved lower bound matches the lower bound of Fredman [J. ACM
46(4):473-501 (1999)] for pairing heaps [M.L. Fredman, R. Sedgewick, D.D.
Sleator, and R.E. Tarjan. Algorithmica 1(1):111-129 (1986)] and surpasses it
for pure-heap variants of numerous other heaps with augmented data such as
pointer rank-pairing heaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6684</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6684</id><created>2014-07-24</created><authors><author><keyname>Binu</keyname><forenames>V. P.</forenames></author><author><keyname>Sreekumar</keyname><forenames>A.</forenames></author></authors><title>Efficient Multi Secret Sharing with Generalized Access Structures</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1406.5596,
  arXiv:1407.3609</comments><journal-ref>International Journal of Computer Applications , Volume 90 -
  Number 12 Year of Publication: 2014</journal-ref><doi>10.5120/15769-4446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-secret sharing is an extension of secret sharing technique where
several secrets are shared between the participants, each according to a
specified access structure. The secrets can be reconstructed according to the
access structure by participants using their private shares.Each participant
has to hold a single share, additional information are made available in a
public bulletin board.The scheme is computationally efficient and also each
participant can verify the shares of the other participants and also the
reconstructed secret.The scheme does not need any secure channel also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6692</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6692</id><created>2014-07-24</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Gopi</keyname><forenames>Sivakanth</forenames></author></authors><title>2-Server PIR with sub-polynomial communication</title><categories>cs.CC cs.CR cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A 2-server Private Information Retrieval (PIR) scheme allows a user to
retrieve the $i$th bit of an $n$-bit database replicated among two servers
(which do not communicate) while not revealing any information about $i$ to
either server. In this work we construct a 1-round 2-server PIR with total
communication cost $n^{O({\sqrt{\log\log n/\log n}})}$. This improves over the
currently known 2-server protocols which require $O(n^{1/3})$ communication and
matches the communication cost of known 3-server PIR schemes. Our improvement
comes from reducing the number of servers in existing protocols, based on
Matching Vector Codes, from 3 or 4 servers to 2. This is achieved by viewing
these protocols in an algebraic way (using polynomial interpolation) and
extending them using partial derivatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6699</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6699</id><created>2014-02-10</created><authors><author><keyname>Vega-Fuentes</keyname><forenames>Eduardo</forenames></author><author><keyname>Cerezo-Sanchez</keyname><forenames>Juan Manuel</forenames></author><author><keyname>Rosario</keyname><forenames>Sonia Leon-del</forenames></author><author><keyname>Vega-Martinez</keyname><forenames>Aurelio</forenames></author></authors><title>Fuzzy inference system for integrated VVC in isolated power systems</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.1632</comments><journal-ref>International Journal of Artificial Intelligence &amp; Aplications,
  (IJAIA), January 2014, Volume 5, Number 1, pp 91-106</journal-ref><doi>10.5121/ijaia.2014.5107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fuzzy inference system for integrated volt/var control
(VVC) in distribution substations. The purpose is go forward to automation
distribution applying conservation voltage reduction (CVR) in isolated power
systems where control capabilities are limited. A fuzzy controller has been
designed. Working as an on-line tool, it has been tested under real conditions
and it has managed the operation during a whole day in a distribution
substation. Within the limits of control capabilities of the system, the
controller maintained successfully an acceptable voltage profile, power factor
values over 0,98 and it has ostensibly improved the performance given by an
optimal power flow based automation system. CVR savings during the test are
evaluated and the aim to integrate it in the VVC is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6705</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6705</id><created>2014-07-24</created><updated>2014-09-16</updated><authors><author><keyname>Azad</keyname><forenames>Reza</forenames></author><author><keyname>Shayegh</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Amiri</keyname><forenames>Hamed</forenames></author></authors><title>A Robust and Efficient Method for Improving Accuracy of License Plate
  Characters Recognition</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1 and some mistake</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  License Plate Recognition (LPR) plays an important role on the traffic
monitoring and parking management. A robust and efficient method for enhancing
accuracy of license plate characters recognition based on K Nearest Neighbours
(K-NN) classifier is presented in this paper. The system first prepares a
contour form of the extracted character, then the angle and distance feature
information about the character is extracted and finally K-NN classifier is
used to character recognition. Angle and distance features of a character have
been computed based on distribution of points on the bitmap image of character.
In K-NN method, the Euclidean distance between testing point and reference
points is calculated in order to find the k-nearest neighbours. We evaluated
our method on the available dataset that contain 1200 sample. Using 70% samples
for training, we tested our method on whole samples and obtained 99% correct
recognition rate.Further, we achieved average 99.41% accuracy using
three/strategy validation technique on 1200 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6714</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6714</id><created>2014-07-24</created><authors><author><keyname>Nushi</keyname><forenames>Besmira</forenames></author><author><keyname>Alonso</keyname><forenames>Omar</forenames></author><author><keyname>Hentschel</keyname><forenames>Martin</forenames></author><author><keyname>Kandylas</keyname><forenames>Vasileios</forenames></author></authors><title>CrowdSTAR: A Social Task Routing Framework for Online Communities</title><categories>cs.SI</categories><acm-class>H.4.m; H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online communities available on the Web have shown to be significantly
interactive and capable of collectively solving difficult tasks. Nevertheless,
it is still a challenge to decide how a task should be dispatched through the
network due to the high diversity of the communities and the dynamically
changing expertise and social availability of their members. We introduce
CrowdSTAR, a framework designed to route tasks across and within online crowds.
CrowdSTAR indexes the topic-specific expertise and social features of the crowd
contributors and then uses a routing algorithm, which suggests the best sources
to ask based on the knowledge vs. availability trade-offs. We experimented with
the proposed framework for question and answering scenarios by using two
popular social networks as crowd candidates: Twitter and Quora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6730</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6730</id><created>2014-07-24</created><updated>2014-08-04</updated><authors><author><keyname>Roditty</keyname><forenames>Liam</forenames></author><author><keyname>Tov</keyname><forenames>Roei</forenames></author></authors><title>New routing techniques and their applications</title><categories>cs.DS cs.DM</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be an undirected graph with $n$ vertices and $m$ edges. We
obtain the following new routing schemes:
  - A routing scheme for unweighted graphs that uses $\tilde
O(\frac{1}{\epsilon} n^{2/3})$ space at each vertex and $\tilde
O(1/\epsilon)$-bit headers, to route a message between any pair of vertices
$u,v\in V$ on a $(2 + \epsilon,1)$-stretch path, i.e., a path of length at most
$(2+\epsilon)\cdot d(u,v)+1$. This should be compared to the $(2,1)$-stretch
and $\tilde O(n^{5/3})$ space distance oracle of Patrascu and Roditty [FOCS'10
and SIAM J. Comput. 2014] and to the $(2,1)$-stretch routing scheme of Abraham
and Gavoille [DISC'11] that uses $\tilde O( n^{3/4})$ space at each vertex.
  - A routing scheme for weighted graphs with normalized diameter $D$, that
uses $\tilde O(\frac{1}{\epsilon} n^{1/3}\log D)$ space at each vertex and
$\tilde O(\frac{1}{\epsilon}\log D)$-bit headers, to route a message between
any pair of vertices on a $(5+\epsilon)$-stretch path. This should be compared
to the $5$-stretch and $\tilde O(n^{4/3})$ space distance oracle of Thorup and
Zwick [STOC'01 and J. ACM. 2005] and to the $7$-stretch routing scheme of
Thorup and Zwick [SPAA'01] that uses $\tilde O( n^{1/3})$ space at each vertex.
Since a $5$-stretch routing scheme must use tables of $\Omega( n^{1/3})$ space
our result is almost tight.
  - For an integer $\ell&gt;1$, a routing scheme for unweighted graphs that uses
$\tilde O(\ell\frac{1}{\epsilon} n^{\ell/(2\ell \pm 1)})$ space at each vertex
and $\tilde O(\frac{1}{\epsilon})$-bit headers, to route a message between any
pair of vertices on a $(3\pm2/\ell+\epsilon,2)$-stretch path.
  - A routing scheme for weighted graphs, that uses $\tilde
O(\frac{1}{\epsilon}n^{1/k}\log D)$ space at each vertex and $\tilde
O(\frac{1}{\epsilon}\log D)$-bit headers, to route a message between any pair
of vertices on a $(4k-7+\epsilon)$-stretch path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6731</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6731</id><created>2014-07-24</created><updated>2015-02-08</updated><authors><author><keyname>Bethanabhotla</keyname><forenames>Dilip</forenames></author><author><keyname>Bursalioglu</keyname><forenames>Ozgun</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Optimal User-Cell Association for Massive MIMO Wireless Networks</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of a very large number of antennas at each base station site
(referred to as &quot;Massive MIMO&quot;) is one of the most promising approaches to cope
with the predicted wireless data traffic explosion. In combination with Time
Division Duplex and with simple per-cell processing, it achieves large
throughput per cell, low latency, and attractive power efficiency performance.
Following the current wireless technology trend of moving to higher frequency
bands and denser small cell deployments, a large number of antennas can be
implemented within a small form factor even in small cell base stations. In a
heterogeneous network formed by large (macro) and small cell BSs, a key system
optimization problem consists of &quot;load balancing&quot;, that is, associating users
to BSs in order to avoid congested hot-spots and/or under-utilized
infrastructure. In this paper, we consider the user-BS association problem for
a massive MIMO heterogeneous network. We formulate the problem as a network
utility maximization, and provide a centralized solution in terms of the
fraction of transmission resources (time-frequency slots) over which each user
is served by a given BS. Furthermore, we show that such a solution is
physically realizable, i.e., there exists a sequence of integer scheduling
configurations realizing (by time-sharing) the optimal fractions. While this
solution is optimal, it requires centralized computation. Then, we also
consider decentralized user-centric schemes, formulated as non-cooperative
games where each user makes individual selfish association decisions based only
on its local information. We identify a class of schemes such that their Nash
equilibrium is very close to the global centralized optimum. Hence, these
user-centric algorithms are attractive not only for their simplicity and fully
decentralized implementation, but also because they operate near the system
&quot;social&quot; optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6745</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6745</id><created>2014-07-24</created><authors><author><keyname>Sar&#x131;y&#xfc;ce</keyname><forenames>Ahmet Erdem</forenames></author><author><keyname>Saule</keyname><forenames>Erik</forenames></author><author><keyname>&#xc7;ataly&#xfc;rek</keyname><forenames>&#xdc;mit V.</forenames></author></authors><title>On Distributed Graph Coloring with Iterative Recoloring</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the sets of operations that can be executed simultaneously is an
important problem appearing in many parallel applications. By modeling the
operations and their interactions as a graph, one can identify the independent
operations by solving a graph coloring problem. Many efficient sequential
algorithms are known for this NP-Complete problem, but they are typically
unsuitable when the operations and their interactions are distributed in the
memory of large parallel computers. On top of an existing distributed-memory
graph coloring algorithm, we investigate two compatible techniques in this
paper for fast and scalable distributed-memory graph coloring. First, we
introduce an improvement for the distributed post-processing operation, called
recoloring, which drastically improves the number of colors. We propose a novel
and efficient communication scheme for recoloring which enables it to scale
gracefully. Recoloring must be seeded with an existing coloring of the graph.
Our second contribution is to introduce a randomized color selection strategy
for initial coloring which quickly produces solutions of modest quality. We
extensively evaluate the impact of our new techniques on existing distributed
algorithms and show the time-quality tradeoffs. We show that combining an
initial randomized coloring with multiple recoloring iterations yields better
quality solutions with the smaller runtime at large scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6748</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6748</id><created>2014-07-24</created><authors><author><keyname>Makinde</keyname><forenames>Ayodeji S.</forenames></author><author><keyname>Nkansah-Gyekye</keyname><forenames>Yaw</forenames></author><author><keyname>Laizer</keyname><forenames>Loserian S.</forenames></author></authors><title>Enhancing the Accuracy of Biometric Feature Extraction Fusion Using
  Gabor Filter and Mahalanobis Distance Algorithm</title><categories>cs.CV</categories><comments>Focused on extraction of feature from two different modalities (face
  and fingerprint) using Gabor filter</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Biometric recognition systems have advanced significantly in the last decade
and their use in specific applications will increase in the near future. The
ability to conduct meaningful comparisons and assessments will be crucial to
successful deployment and increasing biometric adoption. The best modality used
as unimodal biometric systems are unable to fully address the problem of higher
recognition rate. Multimodal biometric systems are able to mitigate some of the
limitations encountered in unimodal biometric systems, such as
non-universality, distinctiveness, non-acceptability, noisy sensor data, spoof
attacks, and performance. More reliable recognition accuracy and performance
are achievable as different modalities were being combined together and
different algorithms or techniques were being used. The work presented in this
paper focuses on a bimodal biometric system using face and fingerprint. An
image enhancement technique (histogram equalization) is used to enhance the
face and fingerprint images. Salient features of the face and fingerprint were
extracted using the Gabor filter technique. A dimensionality reduction
technique was carried out on both images extracted features using a principal
component analysis technique. A feature level fusion algorithm (Mahalanobis
distance technique) is used to combine each unimodal feature together. The
performance of the proposed approach is validated and is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6751</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6751</id><created>2014-07-24</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Poorkasmaei</keyname><forenames>Sina</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Differential Distributed Space-Time Coding with Imperfect
  Synchronization</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Globecom, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential distributed space-time coding (D-DSTC) has been considered to
improve both diversity and data-rate in cooperative communications in the
absence of channel information. However, conventionally, it is assumed that
relays are perfectly synchronized in the symbol level. In practice, this
assumption is easily violated due to the distributed nature of the relay
networks. This paper proposes a new differential encoding and decoding process
for D-DSTC systems with two relays. The proposed method is robust against
synchronization errors and does not require any channel information at the
destination. Moreover, the maximum possible diversity and symbol-by-symbol
decoding are attained. Simulation results are provided to show the performance
of the proposed method for various synchronization errors and the fact that our
algorithm is not sensitive to synchronization error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6753</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6753</id><created>2014-07-24</created><updated>2014-10-03</updated><authors><author><keyname>de Lima</keyname><forenames>Rog&#xe9;rio H. B.</forenames></author><author><keyname>Meira</keyname><forenames>Luis A. A.</forenames></author></authors><title>Ordena\c{c}\~ao Baseada em \'Arvores de Fus\~ao</title><categories>cs.DS</categories><comments>10 pages, portuguese</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sorting is one of the most important problem in the computer science. After
more than 60 years of studies, there are still many research devoted to develop
faster sorting algorithms. This work aims to explain the Fusion Tree data
structure. Fusion Tree was responsible for the first sorting algorithm with
time $o(n \ lg n) $. -----
  O problema da ordena\c{c}\~ao \'e sem d\'uvida um dos mais estudados na
Ci\^encia da Computa\c{c}\~ao. No escopo da computa\c{c}\~ao moderna, depois de
mais de 60 anos de estudos, ainda existem muitas pesquisas que objetivam o
desenvolvimento de algoritmos que solucionem uma ordena\c{c}\~ao mais r\'apida
ou com menos recursos comparados a outros algoritmos j\'a conhecidos. H\'a
v\'arios tipos de algoritmos de ordena\c{c}\~ao, alguns mais r\'apidos, outros
mais econ\^omicos em rela\c{c}\~ao ao espa\c{c}o e outros com algumas
restri\c{c}\~oes com rela\c{c}\~ao \`a entrada de dados. O objetivo deste
trabalho \'e explicar a estrutura de dados \'Avore de Fus\~ao, respons\'avel
pelo primeiro algoritmo de ordena\c{c}\~ao com tempo inferior a $ n \lg n $,
tempo esse que criou certa confus\~ao, gerando uma errada cren\c{c}a de ser o
menor poss\'ivel para esse tipo de problema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6755</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6755</id><created>2014-07-24</created><updated>2015-05-04</updated><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Dynamic Set Intersection</title><categories>cs.DS</categories><comments>Accepted to WADS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of maintaining a family $F$ of dynamic sets subject to
insertions, deletions, and set-intersection reporting queries: given $S,S'\in
F$, report every member of $S\cap S'$ in any order. We show that in the word
RAM model, where $w$ is the word size, given a cap $d$ on the maximum size of
any set, we can support set intersection queries in $O(\frac{d}{w/\log^2 w})$
expected time, and updates in $O(\log w)$ expected time. Using this algorithm
we can list all $t$ triangles of a graph $G=(V,E)$ in
$O(m+\frac{m\alpha}{w/\log^2 w} +t)$ expected time, where $m=|E|$ and $\alpha$
is the arboricity of $G$. This improves a 30-year old triangle enumeration
algorithm of Chiba and Nishizeki running in $O(m \alpha)$ time.
  We provide an incremental data structure on $F$ that supports intersection
{\em witness} queries, where we only need to find {\em one} $e\in S\cap S'$.
Both queries and insertions take $O\paren{\sqrt \frac{N}{w/\log^2 w}}$ expected
time, where $N=\sum_{S\in F} |S|$. Finally, we provide time/space tradeoffs for
the fully dynamic set intersection reporting problem. Using $M$ words of space,
each update costs $O(\sqrt {M \log N})$ expected time, each reporting query
costs $O(\frac{N\sqrt{\log N}}{\sqrt M}\sqrt{op+1})$ expected time where $op$
is the size of the output, and each witness query costs $O(\frac{N\sqrt{\log
N}}{\sqrt M} + \log N)$ expected time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6756</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6756</id><created>2014-07-24</created><updated>2015-07-09</updated><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Higher Lower Bounds from the 3SUM Conjecture</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3SUM conjecture has proven to be a valuable tool for proving conditional
lower bounds on dynamic data structures and graph problems. This line of work
was initiated by P\v{a}tra\c{s}cu (STOC 2010) who reduced 3SUM to an offline
SetDisjointness problem. However, the reduction introduced by P\v{a}tra\c{s}cu
suffers from several inefficiencies, making it difficult to obtain tight
conditional lower bounds from the 3SUM conjecture.
  In this paper we address many of the deficiencies of P\v{a}tra\c{s}cu's
framework. We give new and efficient reductions from 3SUM to offline
SetDisjointness and offline SetIntersection (the reporting version of
SetDisjointness) which leads to polynomially higher lower bounds on several
problems. Using our reductions, we are able to show the essential optimality of
several algorithms, assuming the 3SUM conjecture.
  - Chiba and Nishizeki's $O(m\alpha)$-time algorithm (SICOMP 1985) for
enumerating all triangles in a graph with arboricity/degeneracy $\alpha$ is
essentially optimal, for any $\alpha$.
  - Bj{\o}rklund, Pagh, Williams, and Zwick's algorithm (ICALP 2014) for
listing $t$ triangles is essentially optimal (assuming the matrix
multiplication exponent is $\omega=2$).
  - Any static data structure for SetDisjointness that answers queries in
constant time must spend $\Omega(N^{2-o(1)})$ time in preprocessing, where $N$
is the size of the set system.
  These statements were unattainable via P\v{a}tra\c{s}cu's reductions.
  We also introduce several new reductions from 3SUM to pattern matching
problems and dynamic graph problems. Of particular interest are new conditional
lower bounds for dynamic versions of Maximum Cardinality Matching, which
introduce a new technique for obtaining amortized lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6757</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6757</id><created>2014-07-24</created><authors><author><keyname>Frackiewicz</keyname><forenames>Piotr</forenames></author></authors><title>Quantum signaling game</title><categories>cs.GT</categories><journal-ref>J. Phys. A: Math. Theor. 47 305301 2014</journal-ref><doi>10.1088/1751-8113/47/30/305301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a quantum approach to a signaling game; a special kind of
extensive games of incomplete information. Our model is based on quantum
schemes for games in strategic form where players perform unitary operators on
their own qubits of some fixed initial state and the payoff function is given
by a measurement on the resulting final state. We show that the quantum game
induced by our scheme coincides with a signaling game as a special case and
outputs nonclassical results in general. As an example, we consider a quantum
extension of the signaling game in which the chance move is a three-parameter
unitary operator whereas the players' actions are equivalent to classical ones.
In this case, we study the game in terms of Nash equilibria and refine the pure
Nash equilibria adapting to the quantum game the notion of a weak perfect
Bayesian equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6761</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6761</id><created>2014-07-24</created><updated>2015-03-31</updated><authors><author><keyname>Gu</keyname><forenames>Qian-Ping</forenames></author><author><keyname>Xu</keyname><forenames>Gengchun</forenames></author></authors><title>Near-Linear Time Constant-Factor Approximation Algorithm for
  Branch-Decomposition of Planar Graphs</title><categories>cs.DS</categories><comments>updated theorem 1 (changed time complexity to O(n\log^3 n)) ; updated
  theorem 3 (changed time complexity to O(nk^2), modified the proof)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm which for an input planar graph $G$ of $n$ vertices and
integer $k$, in $\min\{O(n\log^3n),O(nk^2)\}$ time either constructs a
branch-decomposition of $G$ with width at most $(2+\delta)k$, $\delta&gt;0$ is a
constant, or a $(k+1)\times \lceil{\frac{k+1}{2}}\rceil$ cylinder minor of $G$
implying ${\mathop {\rm bw}}(G)&gt;k$, ${\mathop {\rm bw}}(G)$ is the branchwidth
of $G$. This is the first $\tilde{O}(n)$ time constant-factor approximation for
branchwidth/treewidth and largest grid/cylinder minors of planar graphs and
improves the previous $\min\{O(n^{1+\epsilon}),O(nk^3)\}$ ($\epsilon&gt;0$ is a
constant) time constant-factor approximations. For a planar graph $G$ and
$k={\mathop {\rm bw}}(G)$, a branch-decomposition of width at most
$(2+\delta)k$ and a $g\times \frac{g}{2}$ cylinder/grid minor with
$g=\frac{k}{\beta}$, $\beta&gt;2$ is constant, can be computed by our algorithm in
$\min\{O(n\log^3n\log k),O(nk^2\log k)\}$ time
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6794</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6794</id><created>2014-07-25</created><authors><author><keyname>Dwivedi</keyname><forenames>Shri Prakash</forenames></author></authors><title>GCD Computation of n Integers</title><categories>cs.DS cs.SC</categories><comments>RAECS 2014</comments><doi>10.1109/RAECS.2014.6799612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greatest Common Divisor (GCD) computation is one of the most important
operation of algorithmic number theory. In this paper we present the algorithms
for GCD computation of $n$ integers. We extend the Euclid's algorithm and
binary GCD algorithm to compute the GCD of more than two integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6810</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6810</id><created>2014-07-25</created><authors><author><keyname>Elhamifar</keyname><forenames>Ehsan</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Dissimilarity-based Sparse Subset Selection</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding an informative subset of a large number of data points or models is
at the center of many problems in machine learning, computer vision, bio/health
informatics and image/signal processing. Given pairwise dissimilarities between
the elements of a `source set' and a `target set,' we consider the problem of
finding a subset of the source set, called representatives or exemplars, that
can efficiently describe the target set. We formulate the problem as a
row-sparsity regularized trace minimization problem. Since the proposed
formulation is, in general, an NP-hard problem, we consider a convex
relaxation. The solution of our proposed optimization program finds the
representatives and the probability that each element of the target set is
associated with the representatives. We analyze the solution of our proposed
optimization as a function of the regularization parameter. We show that when
the two sets jointly partition into multiple groups, the solution of our
proposed optimization program finds representatives from all groups and reveals
clustering of the sets. In addition, we show that our proposed formulation can
effectively deal with outliers. Our algorithm works with arbitrary
dissimilarities, which can be asymmetric or violate the triangle inequality. To
efficiently implement our proposed algorithm, we consider an Alternating
Direction Method of Multipliers (ADMM) framework, which results in quadratic
complexity in the problem size. We show that the ADMM implementation allows to
parallelize the algorithm, hence further reducing the computational cost.
Finally, by experiments on real-world datasets, we show that our proposed
algorithm improves the state of the art on the two problems of scene
categorization using representative images and time-series modeling and
segmentation using representative models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6812</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6812</id><created>2014-07-25</created><authors><author><keyname>Hoehndorf</keyname><forenames>Robert</forenames></author><author><keyname>Slater</keyname><forenames>Luke</forenames></author><author><keyname>Schofield</keyname><forenames>Paul N.</forenames></author><author><keyname>Gkoutos</keyname><forenames>Georgios V.</forenames></author></authors><title>Aber-OWL: a framework for ontology-based data access in biology</title><categories>cs.DB cs.IR q-bio.GN</categories><doi>10.1186/s12859-015-0456-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many ontologies have been developed in biology and these ontologies
increasingly contain large volumes of formalized knowledge commonly expressed
in the Web Ontology Language (OWL). Computational access to the knowledge
contained within these ontologies relies on the use of automated reasoning. We
have developed the Aber-OWL infrastructure that provides reasoning services for
bio-ontologies. Aber-OWL consists of an ontology repository, a set of web
services and web interfaces that enable ontology-based semantic access to
biological data and literature. Aber-OWL is freely available at
http://aber-owl.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6823</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6823</id><created>2014-07-25</created><authors><author><keyname>Palasek</keyname><forenames>Stan</forenames></author></authors><title>Measuring Prestige in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 5 figures</comments><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the locally-defined social capital metric of Palasek (2013) for
determining individuals' prestige within an online social network. From it we
derive an equivalent global measure by considering random walks over the
network itself. This result inspires a novel expression quantifying the
strategic desirability of a potential social connection. We show in silico that
ideal social neighbors tend to satisfy a &quot;big fish in a small pond&quot; criterion
and that the distribution of neighbor-desirability throughout a network is
governed by anti-homophily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6832</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6832</id><created>2014-07-25</created><authors><author><keyname>Holm</keyname><forenames>Jacob</forenames></author><author><keyname>Rotenberg</keyname><forenames>Eva</forenames></author><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Faster Fully-Dynamic Minimum Spanning Forest</title><categories>cs.DS</categories><comments>13 pages, 2 figures</comments><acm-class>E.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new data structure for the fully-dynamic minimum spanning forest
problem in simple graphs. Edge updates are supported in $O(\log^4n/\log\log n)$
amortized time per operation, improving the $O(\log^4n)$ amortized bound of
Holm et al. (STOC'98, JACM'01). We assume the Word-RAM model with standard
instructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6836</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6836</id><created>2014-07-25</created><updated>2014-11-15</updated><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Ghazi-Zahedi</keyname><forenames>Keyan</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>A Theory of Cheap Control in Embodied Systems</title><categories>cs.SY cs.RO math.PR</categories><comments>27 pages, 10 figures</comments><msc-class>68T05, 60K99</msc-class><doi>10.1371/journal.pcbi.1004427</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for designing cheap control architectures for embodied
agents. Our derivation is guided by the classical problem of universal
approximation, whereby we explore the possibility of exploiting the agent's
embodiment for a new and more efficient universal approximation of behaviors
generated by sensorimotor control. This embodied universal approximation is
compared with the classical non-embodied universal approximation. To exemplify
our approach, we present a detailed quantitative case study for policy models
defined in terms of conditional restricted Boltzmann machines. In contrast to
non-embodied universal approximation, which requires an exponential number of
parameters, in the embodied setting we are able to generate all possible
behaviors with a drastically smaller model, thus obtaining cheap universal
approximation. We test and corroborate the theory experimentally with a
six-legged walking machine. The experiments show that the sufficient controller
complexity predicted by our theory is tight, which means that the theory has
direct practical implications. Keywords: cheap design, embodiment, sensorimotor
loop, universal approximation, conditional restricted Boltzmann machine
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6845</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6845</id><created>2014-07-25</created><updated>2014-10-29</updated><authors><author><keyname>Barthe</keyname><forenames>Gilles</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Arias</keyname><forenames>Emilio Jes&#xfa;s Gallego</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Higher-Order Approximate Relational Refinement Types for Mechanism
  Design and Differential Privacy</title><categories>cs.PL cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mechanism design is the study of algorithm design in which the inputs to the
algorithm are controlled by strategic agents, who must be incentivized to
faithfully report them. Unlike typical programmatic properties, it is not
sufficient for algorithms to merely satisfy the property---incentive properties
are only useful if the strategic agents also believe this fact.
  Verification is an attractive way to convince agents that the incentive
properties actually hold, but mechanism design poses several unique challenges:
interesting properties can be sophisticated relational properties of
probabilistic computations involving expected values, and mechanisms may rely
on other probabilistic properties, like differential privacy, to achieve their
goals.
  We introduce a relational refinement type system, called $\mathsf{HOARe}^2$,
for verifying mechanism design and differential privacy. We show that
$\mathsf{HOARe}^2$ is sound w.r.t. a denotational semantics, and correctly
models $(\epsilon,\delta)$-differential privacy; moreover, we show that it
subsumes DFuzz, an existing linear dependent type system for differential
privacy. Finally, we develop an SMT-based implementation of $\mathsf{HOARe}^2$
and use it to verify challenging examples of mechanism design, including
auctions and aggregative games, and new proposed examples from differential
privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6846</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6846</id><created>2014-07-25</created><updated>2016-01-25</updated><authors><author><keyname>Dahlgaard</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author><author><keyname>Rotenberg</keyname><forenames>Eva</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>The Power of Two Choices with Simple Tabulation</title><categories>cs.DS</categories><comments>SODA'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power of two choices is a classic paradigm for load balancing when
assigning $m$ balls to $n$ bins. When placing a ball, we pick two bins
according to two hash functions $h_0$ and $h_1$, and place the ball in the
least loaded bin. Assuming fully random hash functions, when $m=O(n)$, Azar et
al.~[STOC'94] proved that the maximum load is $\lg \lg n + O(1)$ with high
probability.
  In this paper, we investigate the power of two choices when the hash
functions $h_0$ and $h_1$ are implemented with simple tabulation, which is a
very efficient hash function evaluated in constant time. Following their
analysis of Cuckoo hashing [J.ACM'12], P\v{a}tra\c{s}cu and Thorup claimed that
the expected maximum load with simple tabulation is $O(\lg\lg n)$. This did not
include any high probability guarantee, so the load balancing was not yet to be
trusted.
  Here, we show that with simple tabulation, the maximum load is $O(\lg\lg n)$
with high probability, giving the first constant time hash function with this
guarantee. We also give a concrete example where, unlike with fully random
hashing, the maximum load is not bounded by $\lg \lg n + O(1)$, or even
$(1+o(1))\lg \lg n$ with high probability. Finally, we show that the expected
maximum load is $\lg \lg n + O(1)$, just like with fully random hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6853</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6853</id><created>2014-07-25</created><authors><author><keyname>Cirik</keyname><forenames>Volkan</forenames></author><author><keyname>Yuret</keyname><forenames>Deniz</forenames></author></authors><title>Substitute Based SCODE Word Embeddings in Supervised NLP Tasks</title><categories>cs.CL</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a word embedding method in supervised tasks. It maps words on a
sphere such that words co-occurring in similar contexts lie closely. The
similarity of contexts is measured by the distribution of substitutes that can
fill them. We compared word embeddings, including more recent representations,
in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine
our framework in multilingual dependency parsing as well. The results show that
the proposed method achieves as good as or better results compared to the other
word embeddings in the tasks we investigate. It achieves state-of-the-art
results in multilingual dependency parsing. Word embeddings in 7 languages are
available for public use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6869</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6869</id><created>2014-07-25</created><authors><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Faster Separators for Shallow Minor-Free Graphs via Dynamic Approximate
  Distance Oracles</title><categories>cs.DS</categories><comments>16 pages. Full version of the paper that appeared at ICALP'14. Minor
  fixes regarding the time bounds such that these bounds hold also for
  non-sparse graphs</comments><acm-class>E.1; G.2.2</acm-class><journal-ref>Automata, Languages, and Programming, Lecture Notes in Computer
  Science Volume 8572, 2014, pp 1063-1074</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plotkin, Rao, and Smith (SODA'97) showed that any graph with $m$ edges and
$n$ vertices that excludes $K_h$ as a depth $O(\ell\log n)$-minor has a
separator of size $O(n/\ell + \ell h^2\log n)$ and that such a separator can be
found in $O(mn/\ell)$ time. A time bound of $O(m + n^{2+\epsilon}/\ell)$ for
any constant $\epsilon &gt; 0$ was later given (W., FOCS'11) which is an
improvement for non-sparse graphs. We give three new algorithms. The first has
the same separator size and running time $O(\mbox{poly}(h)\ell
m^{1+\epsilon})$. This is a significant improvement for small $h$ and $\ell$.
If $\ell = \Omega(n^{\epsilon'})$ for an arbitrarily small chosen constant
$\epsilon' &gt; 0$, we get a time bound of $O(\mbox{poly}(h)\ell n^{1+\epsilon})$.
The second algorithm achieves the same separator size (with a slightly larger
polynomial dependency on $h$) and running time $O(\mbox{poly}(h)(\sqrt\ell
n^{1+\epsilon} + n^{2+\epsilon}/\ell^{3/2}))$ when $\ell =
\Omega(n^{\epsilon'})$. Our third algorithm has running time
$O(\mbox{poly}(h)\sqrt\ell n^{1+\epsilon})$ when $\ell =
\Omega(n^{\epsilon'})$. It finds a separator of size $O(n/\ell) + \tilde
O(\mbox{poly}(h)\ell\sqrt n)$ which is no worse than previous bounds when $h$
is fixed and $\ell = \tilde O(n^{1/4})$. A main tool in obtaining our results
is a novel application of a decremental approximate distance oracle of Roditty
and Zwick.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6872</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6872</id><created>2014-07-25</created><authors><author><keyname>Ivek</keyname><forenames>Ivan</forenames></author></authors><title>Interpretable Low-Rank Document Representations with Label-Dependent
  Sparsity Patterns</title><categories>cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In context of document classification, where in a corpus of documents their
label tags are readily known, an opportunity lies in utilizing label
information to learn document representation spaces with better discriminative
properties. To this end, in this paper application of a Variational Bayesian
Supervised Nonnegative Matrix Factorization (supervised vbNMF) with
label-driven sparsity structure of coefficients is proposed for learning of
discriminative nonsubtractive latent semantic components occuring in TF-IDF
document representations. Constraints are such that the components pursued are
made to be frequently occuring in a small set of labels only, making it
possible to yield document representations with distinctive label-specific
sparse activation patterns. A simple measure of quality of this kind of
sparsity structure, dubbed inter-label sparsity, is introduced and
experimentally brought into tight connection with classification performance.
Representing a great practical convenience, inter-label sparsity is shown to be
easily controlled in supervised vbNMF by a single parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6876</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6876</id><created>2014-07-25</created><updated>2014-08-15</updated><authors><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>On Partial Wait-Freedom in Transactional Memory</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory (TM) is a convenient synchronization tool that allows
concurrent threads to declare sequences of instructions on shared data as
speculative \emph{transactions} with &quot;all-or-nothing&quot; semantics. It is known
that dynamic transactional memory cannot provide \emph{wait-free} progress in
the sense that every transaction commits in a finite number of its own steps.
In this paper, we explore the costs of providing wait-freedom to only a
\emph{subset} of transactions. Since most transactional workloads are believed
to be read-dominated, we require that read-only transactions commit in the
wait-free manner, while updating transactions are guaranteed to commit only if
they run in the absence of concurrency. We show that this kind of partial
wait-freedom, combined with attractive requirements like read invisibility or
disjoint-access parallelism, incurs considerable complexity costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6877</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6877</id><created>2014-07-25</created><authors><author><keyname>Mishra</keyname><forenames>Minati</forenames></author><author><keyname>Adhikary</keyname><forenames>M. C.</forenames></author></authors><title>An Easy yet Effective Method for Detecting Spatial Domain LSB
  Steganography</title><categories>cs.MM</categories><comments>12 pages; International Journal of Computer Science and Business
  Informatics, Dec 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digitization of image was a revolutionary step for the fields of photography
and Image processing as this made the editing of images much effortless and
easier. Image editing was not an issue until it was limited to corrective
editing procedures used to enhance the quality of an image such as, contrast
stretching, noise filtering, sharpening etc. But, it became a headache for many
fields when image editing became manipulative. Digital images have become an
easier source of tampering and forgery during last few decades. Today users and
editing specialists, equipped with easily available image editing software,
manipulate digital images with varied goals. Photo journalists often tamper
photographs to give dramatic effect to their stories. Scientists and
researchers use this trick to get theirs works published. Patients' diagnoses
are misrepresented by manipulating medical imageries. Lawyers and Politicians
use tampered images to direct the opinion of people or court to their favor.
Terrorists, anti-social groups use manipulated Stego images for secret
communication. In this paper we present an effective method for detecting
spatial domain Steganography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6878</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6878</id><created>2014-07-25</created><authors><author><keyname>Alyasseri</keyname><forenames>Zaid Abdi Alkareem</forenames></author></authors><title>Survey of Parallel Computing with MATLAB</title><categories>cs.DC</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matlab is one of the most widely used mathematical computing environments in
technical computing. It has an interactive environment which provides high
performance computing (HPC) procedures and easy to use. Parallel computing with
Matlab has been an interested area for scientists of parallel computing
researches for a number of years. Where there are many attempts to parallel
Matlab. In this paper, we present most of the past,present attempts of parallel
Matlab such as MatlabMPI, bcMPI, pMatlab, Star-P and PCT. Finally, we expect
the future attempts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6879</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6879</id><created>2014-07-25</created><authors><author><keyname>Mishra</keyname><forenames>Minati</forenames></author><author><keyname>Adhikary</keyname><forenames>M. C.</forenames></author></authors><title>Detection of Clones in Digital Images</title><categories>cs.MM</categories><comments>12 Pages, International Journal of Computer Science and Business
  Informatics, Jan 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the recent years, tampering of digital images has become a general
habit among people and professionals. As a result, establishment of image
authenticity has become a key issue in fields those make use of digital images.
Authentication of an image involves separation of original camera outputs from
their tampered or Stego counterparts. Digital image cloning being a popular
type of image tampering, in this paper we have experimentally analyzed seven
different algorithms of cloning detection such as the simple overlapped block
matching with lexicographic sorting (SOBMwLS) algorithm, block matching with
discrete cosine transformation, principal component analysis, discrete wavelet
transformation and singular value decomposition performed on the blocks (DCT,
DWT, PCA, SVD), two combination models where, DCT and DWT are combined with
singular value decomposition (DCTSVD and DWTSVD. A comparative study of all
these techniques with respect to their time complexities and robustness of
detection against various post processing operations such as cropping,
brightness and contrast adjustments are presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6885</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6885</id><created>2014-07-25</created><authors><author><keyname>Baget</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Garreau</keyname><forenames>Fabien</forenames></author><author><keyname>Mugnier</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Rocher</keyname><forenames>Swan</forenames></author></authors><title>Extending Acyclicity Notions for Existential Rules (\emph{long version})</title><categories>cs.AI</categories><comments>This report contains a revised version (July 2014) of the paper that
  will appear in the proceedings of ECAI 2014 and an appendix with proofs that
  could not be included in the paper for space restriction reasons</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existential rules have been proposed for representing ontological knowledge,
specifically in the context of Ontology-Based Query Answering. Entailment with
existential rules is undecidable. We focus in this paper on conditions that
ensure the termination of a breadth-first forward chaining algorithm known as
the chase. First, we propose a new tool that allows to extend existing
acyclicity conditions ensuring chase termination, while keeping good complexity
properties. Second, we consider the extension to existential rules with
nonmonotonic negation under stable model semantics and further extend
acyclicity results obtained in the positive case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6915</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6915</id><created>2014-07-25</created><authors><author><keyname>Tsiomenko</keyname><forenames>Rostislav</forenames></author><author><keyname>Rees</keyname><forenames>Bradley S.</forenames></author></authors><title>Accelerating Fast Fourier Transforms Using Hadoop and CUDA</title><categories>cs.DC</categories><comments>6 pages Was submitted to ICDCS 2013 but not accepted</comments><acm-class>F.2.1; E.1; D.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been considerable research into improving Fast Fourier Transform
(FFT) performance through parallelization and optimization for specialized
hardware. However, even with those advancements, processing of very large
files, over 1TB in size, still remains prohibitively slow. Analysts performing
signal processing are forced to wait hours or days for results, which results
in a disruption of their workflow and a decrease in productivity. In this paper
we present a unique approach that not only parallelizes the workload over
multi-cores, but distributes the problem over a cluster of graphics processing
unit (GPU)-equipped servers. By utilizing Hadoop and CUDA, we can take
advantage of inexpensive servers while still exceeding the processing power of
a dedicated supercomputer, as demonstrated in our result using Amazon EC2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6930</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6930</id><created>2014-07-25</created><authors><author><keyname>Meakins</keyname><forenames>Silvia</forenames></author><author><keyname>Grothkopf</keyname><forenames>Uta</forenames></author><author><keyname>Bishop</keyname><forenames>Marsha J.</forenames></author><author><keyname>Stoehr</keyname><forenames>Felix</forenames></author><author><keyname>Tatematsu</keyname><forenames>Ken</forenames></author></authors><title>Two years of ALMA bibliography - lessons learned</title><categories>astro-ph.IM cs.DL</categories><comments>7 pages; to be published in the Proceedings of SPIE, vol. 9149,
  9149-81 (2014)</comments><doi>10.1117/12.2055823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telescope bibliographies are integral parts of observing facilities. They are
used to associate the published literature with archived observational data, to
measure an observatory's scientific output through publication and citation
statistics, and to define guidelines for future observing strategies.
  The ESO and NRAO librarians as well as NAOJ jointly maintain the ALMA
(Atacama Large Millimeter/submillimeter Array) bibliography, a database of
refereed papers that use ALMA data.
  In this paper, we illustrate how relevant articles are identified, which
procedures are used to tag entries in the database and link them to the correct
observations, and how results are communicated to ALMA stakeholders and the
wider community. Efforts made to streamline the process will be explained and
evaluated, and a first analysis of ALMA papers published after two years of
observations will be given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6932</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6932</id><created>2014-07-25</created><authors><author><keyname>Sa&#xe0;-Garriga</keyname><forenames>Albert</forenames></author><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Carrabina</keyname><forenames>Jordi</forenames></author></authors><title>OMP2HMPP: HMPP Source Code Generation from Programs with Pragma
  Extensions</title><categories>cs.DC</categories><comments>Proceedings of HIP3ES Workshop, Vienna, January, 21st 2014</comments><acm-class>D.3.2; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-performance computing are based more and more in heterogeneous
architectures and GPGPUs have become one of the main integrated blocks in
these, as the recently emerged Mali GPU in embedded systems or the NVIDIA GPUs
in HPC servers. In both GPGPUs, programming could become a hurdle that can
limit their adoption, since the programmer has to learn the hardware
capabilities and the language to work with these. We present OMP2HMPP, a tool
that, automatically trans-lates a high-level C source code(OpenMP) code into
HMPP. The generated version rarely will differs from a hand-coded HMPP version,
and will provide an important speedup, near 113%, that could be later improved
by hand-coded CUDA. The generated code could be transported either to HPC
servers and to embedded GPUs, due to the commonalities between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6950</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6950</id><created>2014-07-24</created><authors><author><keyname>Fargion</keyname><forenames>Benjamin Isac</forenames></author></authors><title>How,when and how much a card deck is well shuffled?</title><categories>cs.DM</categories><comments>Italian Thesis In Engeenering Computer, 26 February 2013. 50 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The thesis consider the mixing of few (3-4) card shuffling as well as of
large (52 card) deck. The thesis is showing the limit on the shuffling to
homogeneity elaborated in short program; the thesis is in italian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6952</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6952</id><created>2014-07-24</created><authors><author><keyname>Rani</keyname><forenames>Monika</forenames></author><author><keyname>Parashar</keyname><forenames>Anubha</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Jyoti</forenames></author><author><keyname>Malviya</keyname><forenames>Anu</forenames></author></authors><title>Search Space Engine Optimize Search Using FCC_STF Algorithm in Fuzzy
  Co-Clustering</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy co-clustering can be improved if we handle two main problem first is
outlier and second curse of dimensionality .outlier problem can be reduce by
implementing page replacement algorithm like FIFO, LRU or priority algorithm in
a set of frame of web pages efficiently through a search engine. The web page
which has zero priority (outlier) can be represented in separate slot of frame.
Whereas curse of dimensionality problem can be improved by implementing FCC_STF
algorithm for web pages obtain by search engine that reduce the outlier problem
first. The algorithm FCCM and FUZZY CO-DOK are compared with FCC_STF algorithm
with merit and demerits on the bases of different fuzzifier used. FCC_STF
algorithm in which fuzzifier fused into one entity who have shown high
performance by experiment result of values (A1,B1,Vcj,A2,B2) seem to less
sensitive to local maxima and obtain optimization search space in 2-D for web
pages by plotting graph between J(fcc_stf) and Vcj.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6954</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6954</id><created>2014-07-25</created><updated>2015-08-15</updated><authors><author><keyname>Alk&#xe4;mper</keyname><forenames>Martin</forenames></author><author><keyname>Dedner</keyname><forenames>Andreas</forenames></author><author><keyname>Kl&#xf6;fkorn</keyname><forenames>Robert</forenames></author><author><keyname>Nolte</keyname><forenames>Martin</forenames></author></authors><title>The DUNE-ALUGrid Module</title><categories>cs.MS cs.DC</categories><comments>25 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the new DUNE-ALUGrid module. This module contains a
major overhaul of the sources from the ALUgrid library and the binding to the
DUNE software framework. The main changes include user defined load balancing,
parallel grid construction, and an redesign of the 2d grid which can now also
be used for parallel computations. In addition many improvements have been
introduced into the code to increase the parallel efficiency and to decrease
the memory footprint.
  The original ALUGrid library is widely used within the DUNE community due to
its good parallel performance for problems requiring local adaptivity and
dynamic load balancing. Therefore, this new model will benefit a number of DUNE
users. In addition we have added features to increase the range of problems for
which the grid manager can be used, for example, introducing a 3d tetrahedral
grid using a parallel newest vertex bisection algorithm for conforming grid
refinement. In this paper we will discuss the new features, extensions to the
DUNE interface, and explain for various examples how the code is used in
parallel environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6958</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6958</id><created>2014-07-25</created><updated>2015-09-06</updated><authors><author><keyname>Kiss</keyname><forenames>Viktor</forenames></author><author><keyname>T&#xf3;thm&#xe9;r&#xe9;sz</keyname><forenames>Lilla</forenames></author></authors><title>Chip-firing games on Eulerian digraphs and NP-hardness of computing the
  rank of a divisor on a graph</title><categories>cs.CC math.CO</categories><msc-class>05C57, 05C45, 14H55</msc-class><journal-ref>Discrete Appl. Math. 193 (2015) 48-56</journal-ref><doi>10.1016/j.dam.2015.04.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Baker and Norine introduced a graph-theoretic analogue of the Riemann-Roch
theory. A central notion in this theory is the rank of a divisor. In this paper
we prove that computing the rank of a divisor on a graph is NP-hard.
  The determination of the rank of a divisor can be translated to a question
about a chip-firing game on the same underlying graph. We prove the NP-hardness
of this question by relating chip-firing on directed and undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6965</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6965</id><created>2014-07-25</created><updated>2016-02-15</updated><authors><author><keyname>Egea-Lopez</keyname><forenames>Esteban</forenames></author><author><keyname>Pavon-Mari&#xf1;o</keyname><forenames>Pablo</forenames></author></authors><title>Distributed and Fair Beaconing Rate Adaptation for Congestion Control in
  Vehicular Networks</title><categories>cs.NI</categories><comments>Revised version, final version to appear in IEEE Transactions on
  Mobile Computing</comments><msc-class>90B18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative inter-vehicular applications rely on the exchange of broadcast
single-hop status messages among vehicles, called beacons. The aggregated load
on the wireless channel due to periodic beacons can prevent the transmission of
other types of messages, what is called channel congestion due to beaconing
activity. In this paper we approach the problem of controlling the beaconing
rate on each vehicle by modeling it as a Network Utility Maximization (NUM)
problem. This allows us to formally apply the notion of fairness of a beaconing
rate allocation in vehicular networks and to control the trade-off between
efficiency and fairness. The NUM methodology provides a rigorous framework to
design a broad family of simple and decentralized algorithms, with proved
convergence guarantees to a fair allocation solution. In this context, we focus
exclusively in beaconing rate control and propose the Fair Adaptive Beaconing
Rate for Intervehicular Communications (FABRIC) algorithm, which uses a
particular scaled gradient projection algorithm to solve the dual of the NUM
problem. The desired fairness notion in the allocation can be established with
an algorithm parameter. Simulation results validate our approach and show that
FABRIC converges to fair rate allocations in multi-hop and dynamic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6968</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6968</id><created>2014-07-23</created><authors><author><keyname>Dice</keyname><forenames>Dave</forenames></author><author><keyname>Harris</keyname><forenames>Timothy L.</forenames></author><author><keyname>Kogan</keyname><forenames>Alex</forenames></author><author><keyname>Lev</keyname><forenames>Yossi</forenames></author><author><keyname>Moir</keyname><forenames>Mark</forenames></author></authors><title>Hardware extensions to make lazy subscription safe</title><categories>cs.PL</categories><comments>6 pages, extended version of WTTM2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional Lock Elision (TLE) uses Hardware Transactional Memory (HTM) to
execute unmodified critical sections concurrently, even if they are protected
by the same lock. To ensure correctness, the transactions used to execute these
critical sections &quot;subscribe&quot; to the lock by reading it and checking that it is
available. A recent paper proposed using the tempting &quot;lazy subscription&quot;
optimization for a similar technique in a different context, namely
transactional systems that use a single global lock (SGL) to protect all
transactional data. We identify several pitfalls that show that lazy
subscription \emph{is not safe} for TLE because unmodified critical sections
executing before subscribing to the lock may behave incorrectly in a number of
subtle ways. We also show that recently proposed compiler support for modifying
transaction code to ensure subscription occurs before any incorrect behavior
could manifest is not sufficient to avoid all of the pitfalls we identify. We
further argue that extending such compiler support to avoid all pitfalls would
add substantial complexity and would usually limit the extent to which
subscription can be deferred, undermining the effectiveness of the
optimization. Hardware extensions suggested in the recent proposal also do not
address all of the pitfalls we identify. In this extended version of our WTTM
2014 paper, we describe hardware extensions that make lazy subscription safe,
both for SGL-based transactional systems and for TLE, without the need for
special compiler support. We also explain how nontransactional loads can be
exploited, if available, to further enhance the effectiveness of lazy
subscription.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6981</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6981</id><created>2014-07-25</created><updated>2014-08-25</updated><authors><author><keyname>Erlingsson</keyname><forenames>&#xda;lfar</forenames></author><author><keyname>Pihur</keyname><forenames>Vasyl</forenames></author><author><keyname>Korolova</keyname><forenames>Aleksandra</forenames></author></authors><title>RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response</title><categories>cs.CR</categories><comments>14 pages, accepted at ACM CCS 2014</comments><doi>10.1145/2660267.2660348</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a
technology for crowdsourcing statistics from end-user client software,
anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest
of client data to be studied, without permitting the possibility of looking at
individual trees. By applying randomized response in a novel manner, RAPPOR
provides the mechanisms for such collection as well as for efficient,
high-utility analysis of the collected data. In particular, RAPPOR permits
statistics to be collected on the population of client-side strings with strong
privacy guarantees for each client, and without linkability of their reports.
This paper describes and motivates RAPPOR, details its differential-privacy and
utility guarantees, discusses its practical deployment and properties in the
face of different attack models, and, finally, gives results of its application
to both synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.6989</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.6989</id><created>2014-07-25</created><updated>2016-01-04</updated><authors><author><keyname>Schweinhart</keyname><forenames>Benjamin</forenames></author><author><keyname>Mason</keyname><forenames>Jeremy</forenames></author><author><keyname>MacPherson</keyname><forenames>Robert</forenames></author></authors><title>Topological Similarity of Random Cell Complexes and Applications</title><categories>cs.CG cond-mat.mtrl-sci</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although random cell complexes occur throughout the physical sciences, there
does not appear to be a standard way to quantify their statistical similarities
and differences. The various proposals in the literature are usually motivated
by the analysis of particular physical systems and do not necessarily apply to
general situations. The central concepts in this paper---the swatch and the
cloth---provide a description of the local topology of a cell complex that is
general (any physical system that may be represented as a cell complex is
admissible) and complete (any statistical question about the local topology may
be answered from the cloth). Furthermore, this approach allows a distance to be
defined that measures the similarity of the local topology of two cell
complexes. The distance is used to identify a steady state of a model
dislocation network evolving by energy minimization, and then to rigorously
quantify the approach of the simulation to this steady state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7008</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7008</id><created>2014-07-25</created><updated>2014-12-17</updated><authors><author><keyname>De Santis</keyname><forenames>Enrico</forenames></author><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author></authors><title>Modeling and Recognition of Smart Grid Faults by a Combined Approach of
  Dissimilarity Learning and One-Class Classification</title><categories>cs.AI</categories><acm-class>I.2.6; K.3.2</acm-class><doi>10.1016/j.neucom.2015.05.112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting faults in electrical power grids is of paramount importance, either
from the electricity operator and consumer viewpoints. Modern electric power
grids (smart grids) are equipped with smart sensors that allow to gather
real-time information regarding the physical status of all the component
elements belonging to the whole infrastructure (e.g., cables and related
insulation, transformers, breakers and so on). In real-world smart grid
systems, usually, additional information that are related to the operational
status of the grid itself are collected such as meteorological information.
Designing a suitable recognition (discrimination) model of faults in a
real-world smart grid system is hence a challenging task. This follows from the
heterogeneity of the information that actually determine a typical fault
condition. The second point is that, for synthesizing a recognition model, in
practice only the conditions of observed faults are usually meaningful.
Therefore, a suitable recognition model should be synthesized by making use of
the observed fault conditions only. In this paper, we deal with the problem of
modeling and recognizing faults in a real-world smart grid system, which
supplies the entire city of Rome, Italy. Recognition of faults is addressed by
following a combined approach of multiple dissimilarity measures customization
and one-class classification techniques. We provide here an in-depth study
related to the available data and to the models synthesized by the proposed
one-class classifier. We offer also a comprehensive analysis of the fault
recognition results by exploiting a fuzzy set based reliability decision rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7010</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7010</id><created>2014-07-25</created><authors><author><keyname>Bokov</keyname><forenames>Grigoriy V.</forenames></author></authors><title>Undecidability of the problem of recognizing axiomatizations for
  implicative propositional calculi</title><categories>math.LO cs.LO</categories><comments>13 pages</comments><journal-ref>Logic Journal of the IGPL (2015) 23 (2): 341-353</journal-ref><doi>10.1093/jigpal/jzu047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider propositional calculi, which are finitely
axiomatizable extensions of intuitionistic implicational propositional calculus
together with the rules of modus ponens and substitution. We give a proof of
undecidability of the following problem for these calculi: whether a given
finite set of propositional formulas constitutes an adequate axiom system for a
fixed propositional calculus. Moreover, we prove the same for the following
restriction of this problem: whether a given finite set of theorems of a fixed
propositional calculus derives all theorems of this calculus. The proof of
these results is based on a reduction of the undecidable halting problem for
the tag systems introduced by Post.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7011</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7011</id><created>2014-07-25</created><authors><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>A Key Pre-Distribution Scheme based on Multiple Block Codes for Wireless
  Sensor Networks</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key pre-distribution scheme (KPS) based on multiple codewords of block
codes is presented for wireless sensor networks. The connectivity and security
of the proposed KPS, quantified in terms of probabilities of sharing common
keys for communications of pairs of nodes and their resilience against
colluding nodes, are analytically assessed. The analysis is applicable to both
linear and nonlinear codes and is simplified in the case of maximum distance
separable codes. It is shown that the multiplicity of codes significantly
enhances the security and connectivity of KPS at the cost of a modest increase
of the nodes storage. Numerical and simulation results are provided, which
sheds light on the effect of system parameters of the proposed KPS on its
complexity and performance. Specifically, it is shown that the probability of
resilience of secure pairs against collusion of other nodes only reduces slowly
as the number of colluding nodes increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7015</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7015</id><created>2014-07-01</created><authors><author><keyname>Chakraborty</keyname><forenames>Mithun</forenames></author><author><keyname>Das</keyname><forenames>Sanmay</forenames></author></authors><title>On Manipulation in Prediction Markets When Participants Influence
  Outcomes Directly</title><categories>cs.GT</categories><proxy>Walter Lasecki</proxy><report-no>ci-2014/76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction markets are often used as mechanisms to aggregate information
about a future event, for example, whether a candidate will win an election.
The event is typically assumed to be exogenous. In reality, participants may
influence the outcome, and therefore (1) running the prediction market could
change the incentives of participants in the process that creates the outcome
(for example, agents may want to change their vote in an election), and (2)
simple results such as the myopic incentive compatibility of proper scoring
rules no longer hold in the prediction market itself. We introduce a model of
games of this kind, where agents first trade in a prediction market and then
take an action that influences the market outcome. Our two-stage two-player
model, despite its simplicity, captures two aspects of real-world prediction
markets: (1) agents may directly influence the outcome, (2) some of the agents
instrumental in deciding the outcome may not take part in the prediction
market. We show that this game has two different types of perfect Bayesian
equilibria, which we term LPP and HPP, depending on the values of the belief
parameters: in the LPP domain, equilibrium prices reveal expected market
outcomes conditional on the participants' private information, whereas HPP
equilibria are collusive -- participants effectively coordinate in an
uninformative and untruthful way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7027</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7027</id><created>2014-07-18</created><authors><author><keyname>Liu</keyname><forenames>Peilei</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author></authors><title>Motor Learning Mechanism on the Neuron Scale</title><categories>q-bio.NC cs.NE physics.bio-ph</categories><comments>8 pages, 4 figures</comments><acm-class>I.2.6; I.2.8; I.2.9; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on existing data, we wish to put forward a biological model of motor
system on the neuron scale. Then we indicate its implications in statistics and
learning. Specifically, neuron firing frequency and synaptic strength are
probability estimates in essence. And the lateral inhibition also has
statistical implications. From the standpoint of learning, dendritic
competition through retrograde messengers is the foundation of conditional
reflex and grandmother cell coding. And they are the kernel mechanisms of motor
learning and sensory motor integration respectively. Finally, we compare motor
system with sensory system. In short, we would like to bridge the gap between
molecule evidences and computational models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7046</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7046</id><created>2014-07-25</created><updated>2014-08-14</updated><authors><author><keyname>Escardo</keyname><forenames>Martin</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author></authors><title>Bar Recursion and Products of Selection Functions</title><categories>cs.LO math.LO</categories><comments>28 pages, 1 figure</comments><msc-class>03F10, 03F35, 03F25</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how two iterated products of selection functions can both be used in
conjunction with system T to interpret, via the dialectica interpretation and
modified realizability, full classical analysis. We also show that one iterated
product is equivalent over system T to Spector's bar recursion, whereas the
other is T-equivalent to modified bar recursion. Modified bar recursion itself
is shown to arise directly from the iteration of a different binary product of
&quot;skewed&quot; selection functions. Iterations of the dependent binary products are
also considered but in all cases are shown to be T-equivalent to the iteration
of the simple products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7049</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7049</id><created>2014-07-24</created><authors><author><keyname>Guo</keyname><forenames>Qiang</forenames></author><author><keyname>Song</keyname><forenames>Wen-Jun</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author></authors><title>Ultra accurate collaborative information filtering via directed user
  similarity</title><categories>cs.IR</categories><comments>6 pages, 4 figures</comments><journal-ref>EPL, 107 18001 (2014)</journal-ref><doi>10.1209/0295-5075/107/18001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge of the collaborative filtering (CF) information filtering is
how to obtain the reliable and accurate results with the help of peers'
recommendation. Since the similarities from small-degree users to large-degree
users would be larger than the ones opposite direction, the large-degree users'
selections are recommended extensively by the traditional second-order CF
algorithms. By considering the users' similarity direction and the second-order
correlations to depress the influence of mainstream preferences, we present the
directed second-order CF (HDCF) algorithm specifically to address the challenge
of accuracy and diversity of the CF algorithm. The numerical results for two
benchmark data sets, MovieLens and Netflix, show that the accuracy of the new
algorithm outperforms the state-of-the-art CF algorithms. Comparing with the CF
algorithm based on random-walks proposed in the Ref.7, the average ranking
score could reach 0.0767 and 0.0402, which is enhanced by 27.3\% and 19.1\% for
MovieLens and Netflix respectively. In addition, the diversity, precision and
recall are also enhanced greatly. Without relying on any context-specific
information, tuning the similarity direction of CF algorithms could obtain
accurate and diverse recommendations. This work suggests that the user
similarity direction is an important factor to improve the personalized
recommendation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7061</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7061</id><created>2014-07-25</created><updated>2014-11-17</updated><authors><author><keyname>McCreesh</keyname><forenames>Ciaran</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>A Parallel Branch and Bound Algorithm for the Maximum Labelled Clique
  Problem</title><categories>cs.DS cs.DC cs.SI</categories><comments>Author-final version. Accepted to Optimization Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum labelled clique problem is a variant of the maximum clique
problem where edges in the graph are given labels, and we are not allowed to
use more than a certain number of distinct labels in a solution. We introduce a
new branch-and-bound algorithm for the problem, and explain how it may be
parallelised. We evaluate an implementation on a set of benchmark instances,
and show that it is consistently faster than previously published results,
sometimes by four or five orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7071</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7071</id><created>2014-07-25</created><updated>2015-04-16</updated><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author><author><keyname>Diaz</keyname><forenames>Fernando</forenames></author><author><keyname>Vieweg</keyname><forenames>Sarah</forenames></author></authors><title>Processing Social Media Messages in Mass Emergency: A Survey</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media platforms provide active communication channels during mass
convergence and emergency events such as disasters caused by natural hazards.
As a result, first responders, decision makers, and the public can use this
information to gain insight into the situation as it unfolds. In particular,
many social media messages communicated during emergencies convey timely,
actionable information. Processing social media messages to obtain such
information, however, involves solving multiple challenges including: handling
information overload, filtering credible information, and prioritizing
different classes of messages. These challenges can be mapped to classical
information processing operations such as filtering, classifying, ranking,
aggregating, extracting, and summarizing. We survey the state of the art
regarding computational methods to process social media messages, focusing on
their application in emergency response scenarios. We examine the
particularities of this setting, and then methodically examine a series of key
sub-problems ranging from the detection of events to the creation of actionable
and useful summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7072</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7072</id><created>2014-07-25</created><authors><author><keyname>Kim</keyname><forenames>Seungyeon</forenames></author><author><keyname>Park</keyname><forenames>Haesun</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author></authors><title>Fast Spammer Detection Using Structural Rank</title><categories>cs.IR cs.CR</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comments for a product or a news article are rapidly growing and became a
medium of measuring quality products or services. Consequently, spammers have
been emerged in this area to bias them toward their favor. In this paper, we
propose an efficient spammer detection method using structural rank of author
specific term-document matrices. The use of structural rank was found effective
and far faster than similar methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7073</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7073</id><created>2014-07-25</created><updated>2015-05-21</updated><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Yuan</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Shen</keyname><forenames>Xuehua</forenames></author></authors><title>Real-Time Bidding Benchmarking with iPinYou Dataset</title><categories>cs.GT cs.CY</categories><comments>UCL Technical Report 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being an emerging paradigm for display advertising, Real-Time Bidding (RTB)
drives the focus of the bidding strategy from context to users' interest by
computing a bid for each impression in real time. The data mining work and
particularly the bidding strategy development becomes crucial in this
performance-driven business. However, researchers in computational advertising
area have been suffering from lack of publicly available benchmark datasets,
which are essential to compare different algorithms and systems. Fortunately, a
leading Chinese advertising technology company iPinYou decided to release the
dataset used in its global RTB algorithm competition in 2013. The dataset
includes logs of ad auctions, bids, impressions, clicks, and final conversions.
These logs reflect the market environment as well as form a complete path of
users' responses from advertisers' perspective. This dataset directly supports
the experiments of some important research problems such as bid optimisation
and CTR estimation. To the best of our knowledge, this is the first publicly
available dataset on RTB display advertising. Thus, they are valuable for
reproducible research and understanding the whole RTB ecosystem. In this paper,
we first provide the detailed statistical analysis of this dataset. Then we
introduce the research problem of bid optimisation in RTB and the simple yet
comprehensive evaluation protocol. Besides, a series of benchmark experiments
are also conducted, including both click-through rate (CTR) estimation and bid
optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7083</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7083</id><created>2014-07-25</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Sampled-Data H-infinity Design of Coupling Wave Cancelers in
  Single-Frequency Full-Duplex Relay Stations</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>SICE Annual Conference 2014 (6 pages, 12 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose sampled-data H-infinity design of digital filters
that cancel the continuous-time effect of coupling waves in a single-frequency
full-duplex relay station. In this study, we model a relay station as a
continuous-time system while conventional researches treat it as a
discrete-time system. For a continuous-time model, we propose digital
feedforward and feedback cancelers based on the sampled-data control theory to
cancel coupling waves taking intersample behavior into account. Simulation
results are shown to illustrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7091</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7091</id><created>2014-07-25</created><authors><author><keyname>Barry</keyname><forenames>Andrew J.</forenames></author><author><keyname>Tedrake</keyname><forenames>Russ</forenames></author></authors><title>Pushbroom Stereo for High-Speed Navigation in Cluttered Environments</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel stereo vision algorithm that is capable of obstacle
detection on a mobile-CPU processor at 120 frames per second. Our system
performs a subset of standard block-matching stereo processing, searching only
for obstacles at a single depth. By using an onboard IMU and state-estimator,
we can recover the position of obstacles at all other depths, building and
updating a full depth-map at framerate.
  Here, we describe both the algorithm and our implementation on a high-speed,
small UAV, flying at over 20 MPH (9 m/s) close to obstacles. The system
requires no external sensing or computation and is, to the best of our
knowledge, the first high-framerate stereo detection system running onboard a
small UAV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7094</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7094</id><created>2014-07-26</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>David</forenames></author></authors><title>Crowdsourcing Dialect Characterization through Twitter</title><categories>physics.soc-ph cs.CL cs.SI stat.ML</categories><comments>10 pages, 5 figures</comments><journal-ref>PLoS One 9, E112074 (2014)</journal-ref><doi>10.1371/journal.pone.0112074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform a large-scale analysis of language diatopic variation using
geotagged microblogging datasets. By collecting all Twitter messages written in
Spanish over more than two years, we build a corpus from which a carefully
selected list of concepts allows us to characterize Spanish varieties on a
global scale. A cluster analysis proves the existence of well defined
macroregions sharing common lexical properties. Remarkably enough, we find that
Spanish language is split into two superdialects, namely, an urban speech used
across major American and Spanish citites and a diverse form that encompasses
rural areas and small towns. The latter can be further clustered into smaller
varieties with a stronger regional character.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7097</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7097</id><created>2014-07-26</created><authors><author><keyname>Song</keyname><forenames>Xuegui</forenames></author><author><keyname>Cheng</keyname><forenames>Julian</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>High SNR BER Comparison of Coherent and Differentially Coherent
  Modulation Schemes in Lognormal Fading Channels</title><categories>cs.IT math.IT</categories><comments>Manuscript accepted for publication in IEEE Communications Letters (4
  pages with 2 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using an auxiliary random variable technique, we prove that binary
differential phase-shift keying and binary phase-shift keying have the same
asymptotic bit-error rate performance in lognormal fading channels. We also
show that differential quaternary phase-shift keying is exactly 2.32 dB worse
than quaternary phase-shift keying over the lognormal fading channels in high
signal-to-noise ratio regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7098</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7098</id><created>2014-07-26</created><authors><author><keyname>Mamun</keyname><forenames>Md. Selim Al</forenames></author><author><keyname>Menville</keyname><forenames>David</forenames></author></authors><title>Quantum Cost Optimization for Reversible Sequential Circuit</title><categories>cs.ET</categories><comments>Quantum 4.12 (2013). arXiv admin note: substantial text overlap with
  arXiv:1312.7354</comments><doi>10.14569/IJACSA.2013.041203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible sequential circuits are going to be the significant memory blocks
for the forthcoming computing devices for their ultra low power consumption.
Therefore design of various types of latches has been considered a major
objective for the researchers quite a long time. In this paper we proposed
efficient design of reversible sequential circuits that are optimized in terms
of quantum cost, delay and garbage outputs. For this we proposed a new 3*3
reversible gate called SAM gate and we then design efficient sequential
circuits using SAM gate along with some of the basic reversible logic gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7101</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7101</id><created>2014-07-26</created><authors><author><keyname>Mamun</keyname><forenames>Md. Selim Al</forenames></author><author><keyname>Mandal</keyname><forenames>Indrani</forenames></author><author><keyname>Hasanuzzaman</keyname><forenames>Md.</forenames></author></authors><title>Efficient Design of Reversible Sequential Circuit</title><categories>cs.ET</categories><comments>IOSR Journal of Computer Engineering (IOSR-JCE) 5.6(2012)</comments><doi>10.9790/0661-0564247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic has come to the forefront of theoretical and applied
research today. Although many researchers are investigating techniques to
synthesize reversible combinational logic, there is little work in the area of
sequential reversible logic. Latches and flip-flops are the most significant
memory elements for the forthcoming sequential memory elements. In this paper,
we proposed two new reversible logic gates MG-1 and MG-2. We then proposed new
design techniques for latches and flip-flops with the help of the new proposed
gates. The proposed designs are better than the existing ones in terms of
number of gates, garbage outputs and delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7103</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7103</id><created>2014-07-26</created><authors><author><keyname>Murin</keyname><forenames>Yonathan</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>On Joint Source-Channel Coding for Correlated Sources Over
  Multiple-Access Relay Channels</title><categories>cs.IT math.IT</categories><comments>Accepted to TIT</comments><doi>10.1109/TIT.2014.2343626</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the transmission of correlated sources over discrete memoryless (DM)
multiple-access-relay channels (MARCs), in which both the relay and the
destination have access to side information arbitrarily correlated with the
sources. As the optimal transmission scheme is an open problem, in this work we
propose a new joint source-channel coding scheme based on a novel combination
of the correlation preserving mapping (CPM) technique with Slepian-Wolf (SW)
source coding, and obtain the corresponding sufficient conditions. The proposed
coding scheme is based on the decode-and-forward strategy, and utilizes CPM for
encoding information simultaneously to the relay and the destination, whereas
the cooperation information from the relay is encoded via SW source coding. It
is shown that there are cases in which the new scheme strictly outperforms the
schemes available in the literature. This is the first instance of a
source-channel code that uses CPM for encoding information to two different
nodes (relay and destination). In addition to sufficient conditions, we present
three different sets of single-letter necessary conditions for reliable
transmission of correlated sources over DM MARCs. The newly derived conditions
are shown to be at least as tight as the previously known necessary conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7125</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7125</id><created>2014-07-26</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Asish</forenames></author><author><keyname>Bhabak</keyname><forenames>Puspal</forenames></author></authors><title>A 3-factor approximation algorithm for a Minimum Acyclic Agreement
  Forest on k rooted, binary phylogenetic trees</title><categories>cs.DS</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phylogenetic trees are leaf-labelled trees, where the leaves correspond to
extant species (taxa), and the internal vertices represent ancestral species.
The evolutionary history of a set of species can be explained by more than one
phylogenetic tree, giving rise to the problem of comparing phylogenetic trees
for similarity. Various distance metrics, like the subtree prune-and-regraft
(SPR), tree bisection reconnection (TBR) and nearest neighbour interchange
(NNI) have been proposed to capture this similarity. The distance between two
phylogenetic trees can also be measured by the size of a Maximum Agreement
Forest (MAF) on these trees, as it has been shown that the rooted subtree
prune-and-regraft distance is 1 less than the size of a MAF. Since computing a
MAF of minimum size is an NP-hard problem, approximation algorithms are of
interest. Recently, it has been shown that the MAF on k(&gt;=2) trees can be
approximated to within a factor of 8. In this paper, we improve this ratio to
3. For certain species, however, the evolutionary history is not completely
tree-like. Due to reticulate evolution two gene trees, though related, appear
different, making a phylogenetic network a more appropriate representation of
reticulate evolution. A phylogenetic network contains hybrid nodes for the
species evolved from two parents. The number of such nodes is its hybridization
number. It has been shown that this number is 1 less than the size of a Maximum
Acyclic Agreement Forest (MAAF). We show that the MAAF for k(&gt;= 2) phylogenetic
trees can be approximated to within a factor of 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7126</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7126</id><created>2014-07-26</created><authors><author><keyname>Gupte</keyname><forenames>Neelima</forenames></author><author><keyname>Kachhvah</keyname><forenames>Ajay Deep</forenames></author></authors><title>Transmission of packets on a hierarchical network: Avalanches,
  statistics and explosive percolation</title><categories>cs.SI physics.soc-ph</categories><comments>10 Pages, 11 Figures, Published in (Chapter 17) International
  Conference on Theory and Application in Nonlinear Dynamics (ICAND 2012),
  Understanding Complex Systems</comments><journal-ref>ICAND2012, Understanding Complex Systems, Springer Int. Publ.
  Switzerland pp. 193-202 (2014)</journal-ref><doi>10.1007/978-3-319-02925-2_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss transport on load bearing branching hierarchical networks which
can model diverse systems which can serve as models of river networks, computer
networks, respiratory networks and granular media. We study avalanche
transmissions and directed percolation on these networks, and on the V lattice,
i.e., the strongest realization of the lattice. We find that typical
realizations of the lattice show multimodal distributions for the avalanche
transmissions, and a second order transition for directed percolation. On the
other hand, the V lattice shows power - law behavior for avalanche
transmissions, and a first order (explosive) transition to percolation. The V
lattice is thus the critical case of hierarchical networks. We note that small
perturbations to the V lattice destroy the power-law behavior of the
distributions, and the first order nature of the percolation. We discuss the
implications of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7131</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7131</id><created>2014-07-26</created><updated>2014-09-16</updated><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author><author><keyname>Jermann</keyname><forenames>Patrick</forenames></author><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Dillenbourg</keyname><forenames>Pierre</forenames></author></authors><title>Your click decides your fate: Inferring Information Processing and
  Attrition Behavior from MOOC Video Clickstream Interactions</title><categories>cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we explore video lecture interaction in Massive Open Online
Courses (MOOCs), which is central to student learning experience on these
educational platforms. As a research contribution, we operationalize video
lecture clickstreams of students into cognitively plausible higher level
behaviors, and construct a quantitative information processing index, which can
aid instructors to better understand MOOC hurdles and reason about
unsatisfactory learning outcomes. Our results illustrate how such a metric
inspired by cognitive psychology can help answer critical questions regarding
students' engagement, their future click interactions and participation
trajectories that lead to in-video &amp; course dropouts. Implications for research
and practice are discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7133</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7133</id><created>2014-07-26</created><updated>2014-08-30</updated><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author></authors><title>Who negatively influences me? Formalizing diffusion dynamics of negative
  exposure leading to student attrition in MOOCs</title><categories>cs.SI cs.CY</categories><comments>LTI Student Research Symposium 2014, School of Computer Science,
  Carnegie Mellon University, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we explain the underlying interaction mechanisms which govern
students' influence on each other in Massive Open Online Courses (MOOCs).
Specifically, we outline different ways in which students can be negatively
exposed to their peers on MOOC forums and discuss a simple formulation of
learning network diffusion, which formalizes the essence of how such an
influence spreads and can potentially lead to student attrition over time. We
also view the limitations of our student modeling in the light of real world
MOOC behavior and consequently suggest ways of extending the diffusion model to
handle more complex assumptions. Such an understanding is very beneficial for
MOOC designers and instructors to create a conducive learning environment that
supports students' growth and increases their engagement in the course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7134</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7134</id><created>2014-07-26</created><authors><author><keyname>Khambekar</keyname><forenames>Nilesh</forenames></author><author><keyname>Spooner</keyname><forenames>Chad M.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Vipin</forenames></author></authors><title>Maximizing Spectrum Availability and Exploitation: How to Maximize
  Spectrum Sharing Benefits to the Incumbents?</title><categories>cs.CY</categories><comments>30 pages. To be submitted to a journal. arXiv admin note: text
  overlap with arXiv:1405.2216</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant portion of the radio frequency spectrum remains underutilized
due to exclusive and static allocation of spectrum. Provisioning secondary
access to the underutilized spectrum could be beneficial to the incumbents if
they could gain significant value out of the fallow spectrum while ensuring
protection of their primary services. From an incumbent perspective, the
spectrum sharing approach needs to be non-harmful as well as efficient. In
order to make spectrum sharing efficient, it is necessary to maximize the
spectrum available for secondary access as well as maximize its exploitation.
We examine the impact of the conservative assumptions that lead to lesser
availability of spectrum for secondary access. The problem of joint scheduling
and spectrum-access footprint allocation is at the heart of maximizing the
exploitation. This problem is NP-hard and we present a suboptimal approach
based on the minimal spectrum consumption cost of a spectrum-access request. In
order to improve the spectrum sharing potential, we investigate the impact of
the various design choices for a spectrum access mechanism. The experiments
demonstrate the significance of the active role of the incumbents, the benefits
of fine granular spectrum access, and the need for transceiver standards for
accomplishing efficient usage of the spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7136</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7136</id><created>2014-07-26</created><authors><author><keyname>Lukyanchuk</keyname><forenames>Alexandra</forenames></author><author><keyname>Rybakov</keyname><forenames>Vladimir</forenames></author></authors><title>Linear Intransitive Temporal Logic of Knowledge LTK_r, Decision
  Algorithms, Inference Rules</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our paper investigates the linear logic of knowledge and time LTK_r with
reflexive intransitive time relation. The logic is defined semantically, -- as
the set of formulas which are true at special frames with intransitive and
reflexive time binary relation. The LTK_r -frames are linear chains of clusters
connected by a reflexive intransitive relation $R_T$. Elements inside a cluster
are connected by several equivalence relations imitating the knowledge of
different agents. We study the decidability problem for formulas and inference
rules. Decidability for formulas follows from decidability w.r.t. admissible
inference rules.To study admissibility, we introduce some special constructive
Kripke models useful for description of admissibility of inference rules. With
a special technique of definable valuations we find an algorithm determining
admissible inference rules in LTK_r. That is, we show that the logic LTK_r is
decidable and decidable with respect to admissibility of inference rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7138</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7138</id><created>2014-07-26</created><updated>2015-03-02</updated><authors><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author></authors><title>Data granulation by the principles of uncertainty</title><categories>cs.AI</categories><comments>16 pages, 9 figures, 52 references</comments><doi>10.1016/j.patrec.2015.04.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researches in granular modeling produced a variety of mathematical models,
such as intervals, (higher-order) fuzzy sets, rough sets, and shadowed sets,
which are all suitable to characterize the so-called information granules.
Modeling of the input data uncertainty is recognized as a crucial aspect in
information granulation. Moreover, the uncertainty is a well-studied concept in
many mathematical settings, such as those of probability theory, fuzzy set
theory, and possibility theory. This fact suggests that an appropriate
quantification of the uncertainty expressed by the information granule model
could be used to define an invariant property, to be exploited in practical
situations of information granulation. In this perspective, a procedure of
information granulation is effective if the uncertainty conveyed by the
synthesized information granule is in a monotonically increasing relation with
the uncertainty of the input data. In this paper, we present a data granulation
framework that elaborates over the principles of uncertainty introduced by
Klir. Being the uncertainty a mesoscopic descriptor of systems and data, it is
possible to apply such principles regardless of the input data type and the
specific mathematical setting adopted for the information granules. The
proposed framework is conceived (i) to offer a guideline for the synthesis of
information granules and (ii) to build a groundwork to compare and
quantitatively judge over different data granulation procedures. To provide a
suitable case study, we introduce a new data granulation technique based on the
minimum sum of distances, which is designed to generate type-2 fuzzy sets. We
analyze the procedure by performing different experiments on two distinct data
types: feature vectors and labeled graphs. Results show that the uncertainty of
the input data is suitably conveyed by the generated type-2 fuzzy set models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7143</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7143</id><created>2014-07-26</created><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author></authors><title>&quot;Your click decides your fate&quot;: Leveraging clickstream patterns from
  MOOC videos to infer students' information processing &amp; attrition behavior</title><categories>cs.HC cs.CY</categories><comments>Undergraduate (B.Tech, Computer Science) Thesis Report, 2014, Vellore
  Institute of Technology, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an expansive and ubiquitously available gold mine of educational data,
Massive Open Online courses (MOOCs) have become the an important foci of
learning analytics research. The hope is that this new surge of development
will bring the vision of equitable access to lifelong learning opportunities
within practical reach. MOOCs offer many valuable learning experiences to
students, from video lectures, readings, assignments and exams, to
opportunities to connect and collaborate with others through threaded
discussion forums and other Web 2.0 technologies. Nevertheless, despite all
this potential, MOOCs have so far failed to produce evidence that this
potential is being realized in the current instantiation of MOOCs. In this
work, we primarily explore video lecture interaction in Massive Open Online
Courses (MOOCs), which is central to student learning experience on these
educational platforms. As a research contribution, we operationalize video
lecture clickstreams of students into behavioral actions, and construct a
quantitative information processing index, that can aid instructors to better
understand MOOC hurdles and reason about unsatisfactory learning outcomes. Our
results illuminate the effectiveness of developing such a metric inspired by
cognitive psychology, towards answering critical questions regarding students'
engagement, their future click interactions and participation trajectories that
lead to in-video dropouts. We leverage recurring click behaviors to
differentiate distinct video watching profiles for students in MOOCs.
Additionally, we discuss about prediction of complete course dropouts,
incorporating diverse perspectives from statistics and machine learning, to
offer a more nuanced view into how the second generation of MOOCs be benefited,
if course instructors were to better comprehend factors that lead to student
attrition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7146</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7146</id><created>2014-07-26</created><updated>2015-05-28</updated><authors><author><keyname>O'Neill</keyname><forenames>Mark</forenames></author><author><keyname>Ruoti</keyname><forenames>Scott</forenames></author><author><keyname>Seamons</keyname><forenames>Kent</forenames></author><author><keyname>Zappala</keyname><forenames>Daniel</forenames></author></authors><title>TLS Proxies: Friend or Foe?</title><categories>cs.CR cs.NI</categories><acm-class>E.3; C.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of TLS proxies to intercept encrypted traffic is controversial since
the same mechanism can be used for both benevolent purposes, such as protecting
against malware, and for malicious purposes, such as identity theft or
warrantless government surveillance. To understand the prevalence and uses of
these proxies, we build a TLS proxy measurement tool and deploy it via Google
AdWords campaigns. We generate 15.2 million certificate tests across two
large-scale measurement studies. We find that 1 in 250 TLS connections are
TLS-proxied. The majority of these proxies appear to be benevolent, however we
identify over 3,600 cases where eight malware products are using this
technology nefariously. We also find numerous instances of negligent,
duplicitous, and suspicious behavior, some of which degrade security for users
without their knowledge. Distinguishing these types of practices is challenging
in practice, indicating a need for transparency and user awareness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7150</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7150</id><created>2014-07-26</created><authors><author><keyname>Kailkhura</keyname><forenames>Bhavya</forenames></author><author><keyname>Vempaty</keyname><forenames>Aditya</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Distributed Inference in Tree Networks using Coding Theory</title><categories>cs.IT math.IT stat.AP</categories><comments>16 pages, 7 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of distributed inference in tree based
networks. In the framework considered in this paper, distributed nodes make a
1-bit local decision regarding a phenomenon before sending it to the fusion
center (FC) via intermediate nodes. We propose the use of coding theory based
techniques to solve this distributed inference problem in such structures. Data
is progressively compressed as it moves towards the FC. The FC makes the global
inference after receiving data from intermediate nodes. Data fusion at nodes as
well as at the FC is implemented via error correcting codes. In this context,
we analyze the performance for a given code matrix and also design the optimal
code matrices at every level of the tree. We address the problems of
distributed classification and distributed estimation separately and develop
schemes to perform these tasks in tree networks. The proposed schemes are of
practical significance due to their simple structure. We study the asymptotic
inference performance of our schemes for two different classes of tree
networks: fixed height tree networks, and fixed degree tree networks. We show
that the proposed schemes are asymptotically optimal under certain conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7152</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7152</id><created>2014-07-26</created><authors><author><keyname>Vempaty</keyname><forenames>Aditya</forenames></author><author><keyname>He</keyname><forenames>Hao</forenames></author><author><keyname>Chen</keyname><forenames>Biao</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>On Quantizer Design for Distributed Bayesian Estimation in Sensor
  Networks</title><categories>cs.IT math.IT stat.ME</categories><comments>15 pages, 3 figures, submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2014.2350964</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed estimation under the Bayesian
criterion and explore the design of optimal quantizers in such a system. We
show that, for a conditionally unbiased and efficient estimator at the fusion
center and when local observations have identical distributions, it is optimal
to partition the local sensors into groups, with all sensors within a group
using the same quantization rule. When all the sensors use identical number of
decision regions, use of identical quantizers at the sensors is optimal. When
the network is constrained by the capacity of the wireless multiple access
channel over which the sensors transmit their quantized observations, we show
that binary quantizers at the local sensors are optimal under certain
conditions. Based on these observations, we address the location parameter
estimation problem and present our optimal quantizer design approach. We also
derive the performance limit for distributed location parameter estimation
under the Bayesian criterion and find the conditions when the widely used
threshold quantizer achieves this limit. We corroborate this result using
simulations. We then relax the assumption of conditionally independent
observations and derive the optimality conditions of quantizers for
conditionally dependent observations. Using counter-examples, we also show that
the previous results do not hold in this setting of dependent observations and,
therefore, identical quantizers are not optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7155</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7155</id><created>2014-07-26</created><updated>2014-09-02</updated><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author><author><keyname>Rajasingh</keyname><forenames>Indra</forenames></author></authors><title>Towards Investigating Substructures and Role Recognition in Goal
  Oriented Online Communities</title><categories>cs.SI</categories><comments>Extended journal version of IEEE paper accepted in &quot;4th IEEE
  International Advance Computing Conference (IACC), India&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply social network analytic methods to unveil the
structural dynamics of a popular open source goal oriented IRC community,
Ubuntu. The primary objective is to track the development of this ever growing
community over time using a social network lens and examine the dynamically
changing participation patterns of people. Specifically, our research seeks out
to investigate answers to the following question: How can the communication
dynamics help us in delineating important substructures in the IRC network?
This gives an insight into how open source learning communities function
internally and what drives the exhibited IRC behavior. By application of a
consistent set of social network metrics, we discern factors that affect
people's embeddedness in the overall IRC network, their structural influence
and importance as discussion initiators or responders. Deciphering these
informal connections are crucial for the development of novel strategies to
improve communication and foster collaboration between people conversing in the
IRC channel, there by stimulating knowledge flow in the network. Our approach
reveals a novel network skeleton, that more closely resembles the behavior of
participants interacting online. We highlight bottlenecks to effective
knowledge dissemination in the IRC, so that focused attention could be provided
to communities with peculiar behavioral patterns. Additionally, we explore
interesting research directions in augmenting the study of communication
dynamics in the IRC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7156</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7156</id><created>2014-07-26</created><updated>2014-11-18</updated><authors><author><keyname>Aravind</keyname><forenames>N. R.</forenames></author><author><keyname>Sandeep</keyname><forenames>R. B.</forenames></author><author><keyname>Sivadasan</keyname><forenames>Naveen</forenames></author></authors><title>On Polynomial Kernelization of $\mathcal{H}$-free Edge Deletion</title><categories>cs.DS</categories><comments>12 pages. IPEC 2014 accepted paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a set of graphs $\mathcal{H}$, the \textsc{$\mathcal{H}$-free Edge
Deletion} problem asks to find whether there exist at most $k$ edges in the
input graph whose deletion results in a graph without any induced copy of
$H\in\mathcal{H}$. In \cite{cai1996fixed}, it is shown that the problem is
fixed-parameter tractable if $\mathcal{H}$ is of finite cardinality. However,
it is proved in \cite{cai2013incompressibility} that if $\mathcal{H}$ is a
singleton set containing $H$, for a large class of $H$, there exists no
polynomial kernel unless $coNP\subseteq NP/poly$. In this paper, we present a
polynomial kernel for this problem for any fixed finite set $\mathcal{H}$ of
connected graphs and when the input graphs are of bounded degree. We note that
there are \textsc{$\mathcal{H}$-free Edge Deletion} problems which remain
NP-complete even for the bounded degree input graphs, for example
\textsc{Triangle-free Edge Deletion}\cite{brugmann2009generating} and
\textsc{Custer Edge Deletion($P_3$-free Edge
Deletion)}\cite{komusiewicz2011alternative}. When $\mathcal{H}$ contains
$K_{1,s}$, we obtain a stronger result - a polynomial kernel for $K_t$-free
input graphs (for any fixed $t&gt; 2$). We note that for $s&gt;9$, there is an
incompressibility result for \textsc{$K_{1,s}$-free Edge Deletion} for general
graphs \cite{cai2012polynomial}. Our result provides first polynomial kernels
for \textsc{Claw-free Edge Deletion} and \textsc{Line Edge Deletion} for
$K_t$-free input graphs which are NP-complete even for $K_4$-free
graphs\cite{yannakakis1981edge} and were raised as open problems in
\cite{cai2013incompressibility,open2013worker}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7159</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7159</id><created>2014-07-26</created><authors><author><keyname>Riechers</keyname><forenames>P. M.</forenames></author><author><keyname>Varn</keyname><forenames>D. P.</forenames></author><author><keyname>Crutchfield</keyname><forenames>J. P.</forenames></author></authors><title>Pairwise Correlations in Layered Close-Packed Structures</title><categories>cond-mat.mtrl-sci cs.LG</categories><comments>21 pages, 21 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/cfem_prb.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a description of the stacking statistics of layered close-packed
structures in the form of a hidden Markov model, we develop analytical
expressions for the pairwise correlation functions between the layers. These
may be calculated analytically as explicit functions of model parameters or the
expressions may be used as a fast, accurate, and efficient way to obtain
numerical values. We present several examples, finding agreement with previous
work as well as deriving new relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7161</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7161</id><created>2014-07-26</created><updated>2014-08-03</updated><authors><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Soca&#x142;a</keyname><forenames>Arkadiusz</forenames></author></authors><title>Assigning channels via the meet-in-the-middle approach</title><categories>cs.DS</categories><comments>SWAT 2014: 282-293</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the Channel Assignment problem. By applying the
meet-in-the-middle approach we get an algorithm for the $\ell$-bounded Channel
Assignment (when the edge weights are bounded by $\ell$) running in time
$O^*((2\sqrt{\ell+1})^n)$. This is the first algorithm which breaks the
$(O(\ell))^n$ barrier. We extend this algorithm to the counting variant, at the
cost of slightly higher polynomial factor.
  A major open problem asks whether Channel Assignment admits a $O(c^n)$-time
algorithm, for a constant $c$ independent of $\ell$. We consider a similar
question for Generalized T-Coloring, a CSP problem that generalizes \CA. We
show that Generalized T-Coloring does not admit a
$2^{2^{o\left(\sqrt{n}\right)}} {\rm poly}(r)$-time algorithm, where $r$ is the
size of the instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7162</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7162</id><created>2014-07-26</created><authors><author><keyname>Socala</keyname><forenames>Arkadiusz</forenames></author></authors><title>Tight lower bound for the channel assignment problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the Channel Assignment problem. A major open
problem asks whether Channel Assignment admits an $O(c^n)$-time algorithm, for
a constant $c$ independent of the weights on the edges. We answer this question
in the negative i.e. we show that there is no $2^{o(n\log n)}$-time algorithm
solving Channel Assignment unless the Exponential Time Hypothesis fails. Note
that the currently best known algorithm works in time $O^*(n!) = 2^{O(n\log
n)}$ so our lower bound is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7165</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7165</id><created>2014-07-26</created><authors><author><keyname>Bowsher</keyname><forenames>Clive G.</forenames></author><author><keyname>Voliotis</keyname><forenames>Margaritis</forenames></author></authors><title>Mutual Information and Conditional Mean Prediction Error</title><categories>cs.IT math.IT math.PR math.ST physics.bio-ph physics.data-an stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutual information is fundamentally important for measuring statistical
dependence between variables and for quantifying information transfer by
signaling and communication mechanisms. It can, however, be challenging to
evaluate for physical models of such mechanisms and to estimate reliably from
data. Furthermore, its relationship to better known statistical procedures is
still poorly understood. Here we explore new connections between mutual
information and regression-based dependence measures, $\nu^{-1}$, that utilise
the determinant of the second-moment matrix of the conditional mean prediction
error. We examine convergence properties as $\nu\rightarrow0$ and establish
sharp lower bounds on mutual information and capacity of the form
$\mathrm{log}(\nu^{-1/2})$. The bounds are tighter than lower bounds based on
the Pearson correlation and ones derived using average mean square-error rate
distortion arguments. Furthermore, their estimation is feasible using
techniques from nonparametric regression. As an illustration we provide
bootstrap confidence intervals for the lower bounds which, through use of a
composite estimator, substantially improve upon inference about mutual
information based on $k$-nearest neighbour estimators alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7169</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7169</id><created>2014-07-26</created><authors><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author></authors><title>Principles and Parameters: a coding theory perspective</title><categories>cs.CL cs.IT math.IT</categories><comments>11 pages, LaTeX</comments><msc-class>91F20, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach to Longobardi's parametric comparison method (PCM) via
the theory of error-correcting codes. One associates to a collection of
languages to be analyzed with the PCM a binary (or ternary) code with one code
words for each language in the family and each word consisting of the binary
values of the syntactic parameters of the language, with the ternary case
allowing for an additional parameter state that takes into account phenomena of
entailment of parameters. The code parameters of the resulting code can be
compared with some classical bounds in coding theory: the asymptotic bound, the
Gilbert-Varshamov bound, etc. The position of the code parameters with respect
to some of these bounds provides quantitative information on the variability of
syntactic parameters within and across historical-linguistic families. While
computations carried out for languages belonging to the same family yield codes
below the GV curve, comparisons across different historical families can give
examples of isolated codes lying above the asymptotic bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7170</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7170</id><created>2014-07-26</created><authors><author><keyname>Leshem</keyname><forenames>Amir</forenames></author><author><keyname>Hamdi</keyname><forenames>Maziyar</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author></authors><title>Boundary value problems in consensus networks</title><categories>cs.SI cs.IT math.IT physics.soc-ph</categories><comments>Submitted for publication, Feb. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the effect of boundary value conditions on consensus
networks. Consider a network where some nodes keep their estimates constant
while other nodes average their estimates with that of their neighbors. We
analyze such networks and show that in contrast to standard consensus networks,
the network estimate converges to a general harmonic function on the graph.
Furthermore, the final value depends only on the value at the boundary nodes.
This has important implications in consensus networks -- for example, we show
that consensus networks are extremely sensitive to the existence of a single
malicious node or consistent errors in a single node. We also discuss
applications of this result in social and sensor networks. We investigate the
existence of boundary nodes in human social networks via an experimental study
involving human subjects. Finally, the paper is concluded with the numerical
studies of the boundary value problems in consensus networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7180</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7180</id><created>2014-07-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Defining Relative Likelihood in Partially-Ordered Preferential
  Structures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twelfth Conference on Uncertainty in
  Artificial Intelligence (UAI1996)</comments><proxy>auai</proxy><report-no>UAI-P-1996-PG-299-306</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting with a likelihood or preference order on worlds, we extend it to a
likelihood ordering on sets of worlds in a natural way, and examine the
resulting logic. Lewis (1973) earlier considered such a notion of relative
likelihood in the context of studying counterfactuals, but he assumed a total
preference order on worlds. Complications arise when examining partial orders
that are not present for total orders. There are subtleties involving the exact
approach to lifting the order on worlds to an order on sets of worlds. In
addition, the axiomatization of the logic of relative likelihood in the case of
partial orders gives insight into the connection between relative likelihood
and default reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7182</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7182</id><created>2014-07-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Conditional Plausibility Measures and Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-247-255</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that the
technology of Bayesian networks can be applied to algebraic conditional
plausibility measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7183</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7183</id><created>2014-07-27</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Updating Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-187-196</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As examples such as the Monty Hall puzzle show, applying conditioning to
update a probability distribution on a ``naive space', which does not take into
account the protocol used, can often lead to counterintuitive results. Here we
examine why. A criterion known as CAR (coarsening at random) in the statistical
literature characterizes when ``naive' conditioning in a naive space works. We
show that the CAR condition holds rather infrequently. We then consider more
generalized notions of update such as Jeffrey conditioning and minimizing
relative entropy (MRE). We give a generalization of the CAR condition that
characterizes when Jeffrey conditioning leads to appropriate answers, but show
that there are no such conditions for MRE. This generalizes and interconnects
previous results obtained in the literature on CAR and MRE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7184</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7184</id><created>2014-07-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Reasoning about Expectation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-207-215</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expectation is a central notion in probability theory. The notion of
expectation also makes sense for other notions of uncertainty. We introduce a
propositional logic for reasoning about expectation, where the semantics
depends on the underlying representation of uncertainty. We give sound and
complete axiomatizations for the logic in the case that the underlying
representation is (a) probability, (b) sets of probability measures, (c) belief
functions, and (d) possibility measures. We show that this logic is more
expressive than the corresponding logic for reasoning about likelihood in the
case of sets of probability measures, but equi-expressive in the case of
probability, belief, and possibility. Finally, we show that satisfiability for
these logics is NP-complete, no harder than satisfiability for propositional
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7185</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7185</id><created>2014-07-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>A Logic for Reasoning about Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-297-304</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a logic for reasoning about evidence, that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7188</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7188</id><created>2014-07-27</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>When Ignorance is Bliss</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)</comments><proxy>auai</proxy><report-no>UAI-P-2004-PG-226-234</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is commonly-accepted wisdom that more information is better, and that
information should never be ignored. Here we argue, using both a Bayesian and a
non-Bayesian analysis, that in some situations you are better off ignoring
information if your uncertainty is represented by a set of probability
measures. These include situations in which the information is relevant for the
prediction task at hand. In the non-Bayesian analysis, we show how ignoring
information avoids dilation, the phenomenon that additional pieces of
information sometimes lead to an increase in uncertainty. In the Bayesian
analysis, we show that for small sample sizes and certain prediction tasks, the
Bayesian posterior based on a noninformative prior yields worse predictions
than simply ignoring the given information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7189</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7189</id><created>2014-07-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Evidence with Uncertain Likelihoods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-First Conference on Uncertainty
  in Artificial Intelligence (UAI2005)</comments><proxy>auai</proxy><report-no>UAI-P-2005-PG-243-250</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function {\mu}h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7190</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7190</id><created>2014-07-27</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A Game-Theoretic Analysis of Updating Sets of Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty
  in Artificial Intelligence (UAI2008)</comments><proxy>auai</proxy><report-no>UAI-P-2008-PG-240-247</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider how an agent should update her uncertainty when it is represented
by a set P of probability distributions and the agent observes that a random
variable X takes on value x, given that the agent makes decisions using the
minimax criterion, perhaps the best-studied and most commonly-used criterion in
the literature. We adopt a game-theoretic framework, where the agent plays
against a bookie, who chooses some distribution from P. We consider two
reasonable games that differ in what the bookie knows when he makes his choice.
Anomalies that have been observed before, like time inconsistency, can be
understood as arising because different games are being played, against bookies
with different information. We characterize the important special cases in
which the optimal decision rules according to the minimax criterion amount to
either conditioning or simply ignoring the information. Finally, we consider
the relationship between conditioning and calibration when uncertainty is
described by sets of probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7191</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7191</id><created>2014-07-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Rong</keyname><forenames>Nan</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>MDPs with Unawareness</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-228-235</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDPs) are widely used for modeling decision-making
problems in robotics, automated control, and economics. Traditional MDPs assume
that the decision maker (DM) knows all states and actions. However, this may
not be true in many situations of interest. We define a new framework, MDPs
with unawareness (MDPUs) to deal with the possibilities that a DM may not be
aware of all possible actions. We provide a complete characterization of when a
DM can learn to play near-optimally in an MDPU, and give an algorithm that
learns to play near-optimally when it is possible to do so, as efficiently as
possible. In particular, we characterize when a near-optimal solution can be
found in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7211</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7211</id><created>2014-07-27</created><authors><author><keyname>Pedroso</keyname><forenames>Jo&#xe3;o Pedro</forenames></author></authors><title>An evolutionary solver for linear integer programming</title><categories>cs.NE cs.AI math.OC</categories><comments>15 pages</comments><msc-class>80M50</msc-class><acm-class>G.1.6, I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce an evolutionary algorithm for the solution of
linear integer programs. The strategy is based on the separation of the
variables into the integer subset and the continuous subset; the integer
variables are fixed by the evolutionary system, and the continuous ones are
determined in function of them, by a linear program solver.
  We report results obtained for some standard benchmark problems, and compare
them with those obtained by branch-and-bound. The performance of the
evolutionary algorithm is promising. Good feasible solutions were generally
obtained, and in some of the difficult benchmark tests it outperformed
branch-and-bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7213</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7213</id><created>2014-07-27</created><updated>2015-03-02</updated><authors><author><keyname>Psillakis</keyname><forenames>Haris E.</forenames></author></authors><title>An extension of the Georgiou-Smith example: Boundedness and attractivity
  in the presence of unmodelled dynamics via nonlinear PI control</title><categories>cs.SY</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a nonlinear extension of the Georgiou-Smith system is
considered and robustness results are proved for a class of nonlinear PI
controllers with respect to fast parasitic first-order dynamics. More
specifically, for a perturbed nonlinear system with sector bounded nonlinearity
and unknown control direction, sufficient conditions for global boundedness and
attractivity have been derived. It is shown that the closed loop system is
globally bounded and attractive if (i) the unmodelled dynamics are sufficiently
fast and (ii) the PI control gain has the Nussbaum function property. For the
case of nominally unstable systems, the Nussbaum property of the control gain
appears to be crucial. A simulation study confirms the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7216</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7216</id><created>2014-07-27</created><updated>2014-09-29</updated><authors><author><keyname>Byrka</keyname><forenames>Jaroslaw</forenames><affiliation>Institute of Computer Science, University of Wroclaw, Poland</affiliation></author><author><keyname>Sornat</keyname><forenames>Krzysztof</forenames><affiliation>Institute of Computer Science, University of Wroclaw, Poland</affiliation></author></authors><title>PTAS for Minimax Approval Voting</title><categories>cs.DS cs.CC cs.GT</categories><comments>15 pages, 1 figure</comments><msc-class>68W25, 68Q17, 05C12, 91B12 (Primary) 68Q87, 68W32, 90C05, 90C47
  (Secondary)</msc-class><acm-class>G.1.6; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Approval Voting systems where each voter decides on a subset to
candidates he/she approves. We focus on the optimization problem of finding the
committee of fixed size k minimizing the maximal Hamming distance from a vote.
In this paper we give a PTAS for this problem and hence resolve the open
question raised by Carragianis et al. [AAAI'10]. The result is obtained by
adapting the techniques developed by Li et al. [JACM'02] originally used for
the less constrained Closest String problem. The technique relies on extracting
information and structural properties of constant size subsets of votes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7223</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7223</id><created>2014-07-27</created><authors><author><keyname>Huang</keyname><forenames>Zhanpeng</forenames></author><author><keyname>Hui</keyname><forenames>Pan</forenames></author><author><keyname>Peylo</keyname><forenames>Christoph</forenames></author></authors><title>When Augmented Reality Meets Big Data</title><categories>cs.DB cs.MM cs.SI</categories><comments>8 pages, 9 figures</comments><acm-class>H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With computing and sensing woven into the fabric of everyday life, we live in
an era where we are awash in a flood of data from which we can gain rich
insights. Augmented reality (AR) is able to collect and help analyze the
growing torrent of data about user engagement metrics within our personal
mobile and wearable devices. This enables us to blend information from our
senses and the digitalized world in a myriad of ways that was not possible
before. AR and big data have a logical maturity that inevitably converge them.
The tread of harnessing AR and big data to breed new interesting applications
is starting to have a tangible presence. In this paper, we explore the
potential to capture value from the marriage between AR and big data
technologies, following with several challenges that must be addressed to fully
realize this potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7247</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7247</id><created>2014-07-27</created><authors><author><keyname>Shore</keyname><forenames>Jesse</forenames></author><author><keyname>Lubin</keyname><forenames>Benjamin</forenames></author></authors><title>Spectral goodness of fit for network models</title><categories>cs.SI physics.data-an physics.soc-ph stat.ME</categories><msc-class>91D30, 05C82,</msc-class><acm-class>G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new statistic, 'spectral goodness of fit' (SGOF) to measure
how well a network model explains the structure of an observed network. SGOF
provides an absolute measure of fit, analogous to the standard R-squared in
linear regression. Additionally, as it takes advantage of the properties of the
spectrum of the graph Laplacian, it is suitable for comparing network models of
diverse functional forms, including both fitted statistical models and
algorithmic generative models of networks. After introducing, defining, and
providing guidance for interpreting SGOF, we illustrate the properties of the
statistic with a number of examples and comparisons to existing techniques. We
show that such a spectral approach to assessing model fit fills gaps left by
earlier methods and can be widely applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7250</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7250</id><created>2014-07-27</created><authors><author><keyname>Minguillo</keyname><forenames>David</forenames></author></authors><title>Mapping R&amp;D support infrastructures: A scientometric and webometric
  study of UK science parks</title><categories>cs.CY cs.DL</categories><comments>Doctoral thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis analyses UK SPs with an informetric approach to study (1) the
role of public science and HEIs in research and development (R&amp;D) networks
associated with SPs, and (2) the web-based patterns that reflect the
configuration of R&amp;D support infrastructures associated with SPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7257</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7257</id><created>2014-07-27</created><authors><author><keyname>Lamb</keyname><forenames>Christopher C.</forenames></author><author><keyname>Heileman</keyname><forenames>Gregory L.</forenames></author></authors><title>Service Level Agreement Complexity: Processing Concerns for Standalone
  and Aggregate SLAs</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the problem of a single provider offering multiple
types of service level agreements, and the implications thereof. In doing so,
we propose a simple model for machine-readable service level agreements (SLAs)
and outline specifically how these machine-readable SLAs can be constructed and
injected into cloud infrastructures - important for next-generation cloud
systems as well as customers. We then computationally characterize the problem,
establishing the importance of both verification and solution, showing that in
the general case injecting policies into cloud infrastructure is NP-Complete,
though the problem can be made more tractable by further constraining SLA
representations and using approximation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7260</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7260</id><created>2014-07-27</created><authors><author><keyname>Sinha</keyname><forenames>Tanmay</forenames></author><author><keyname>Banka</keyname><forenames>Ankit</forenames></author><author><keyname>Kang</keyname><forenames>Dae Ki</forenames></author></authors><title>Leveraging user profile attributes for improving pedagogical accuracy of
  learning pathways</title><categories>cs.CY cs.LG</categories><comments>3rd Annual International Conference on Education and E-Learning(EeL
  2013), Singapore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, with the enormous explosion of web based learning resources,
personalization has become a critical factor for the success of services that
wish to leverage the power of Web 2.0. However, the relevance, significance and
impact of tailored content delivery in the learning domain is still
questionable. Apart from considering only interaction based features like
ratings and inferring learner preferences from them, if these services were to
incorporate innate user profile attributes which affect learning activities,
the quality of recommendations produced could be vastly improved. Recognizing
the crucial role of effective guidance in informal educational settings, we
provide a principled way of utilizing multiple sources of information from the
user profile itself for the recommendation task. We explore factors that affect
the choice of learning resources and explain in what way are they helpful to
improve the pedagogical accuracy of learning objects recommended. Through a
systematical application of machine learning techniques, we further provide a
technological solution to convert these indirectly mapped learner specific
attributes into a direct mapping with the learning resources. This mapping has
a distinct advantage of tagging learning resources to make their metadata more
informative. The results of our empirical study depict the similarity of
nominal learning attributes with respect to each other. We further succeed in
capturing the learner subset, whose preferences are most likely to be an
indication of learning resource usage. Our novel system filters learner profile
attributes to discover a tag that links them with learning resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7263</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7263</id><created>2014-07-27</created><updated>2015-11-23</updated><authors><author><keyname>Balbuena</keyname><forenames>Camino</forenames></author><author><keyname>Foucaud</keyname><forenames>Florent</forenames></author><author><keyname>Hansberg</keyname><forenames>Adriana</forenames></author></authors><title>Locating-dominating sets and identifying codes in graphs of girth at
  least 5</title><categories>math.CO cs.DM</categories><comments>20 pages, 9 figures</comments><journal-ref>The Electronic Journal of Combinatorics 22:P2.15, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locating-dominating sets and identifying codes are two closely related
notions in the area of separating systems. Roughly speaking, they consist in a
dominating set of a graph such that every vertex is uniquely identified by its
neighbourhood within the dominating set. In this paper, we study the size of a
smallest locating-dominating set or identifying code for graphs of girth at
least 5 and of given minimum degree. We use the technique of vertex-disjoint
paths to provide upper bounds on the minimum size of such sets, and construct
graphs who come close to meet these bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7267</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7267</id><created>2014-07-27</created><updated>2014-07-30</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Ashour</keyname><forenames>Mahmoud</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>On Spectrum Sharing Between Energy Harvesting Cognitive Radio Users and
  Primary Users</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the maximum secondary throughput for a rechargeable
secondary user (SU) sharing the spectrum with a primary user (PU) plugged to a
reliable power supply. The SU maintains a finite energy queue and harvests
energy from natural resources and primary radio frequency (RF) transmissions.
We propose a power allocation policy at the PU and analyze its effect on the
throughput of both the PU and SU. Furthermore, we study the impact of the
bursty arrivals at the PU on the energy harvested by the SU from RF
transmissions. Moreover, we investigate the impact of the rate of energy
harvesting from natural resources on the SU throughput. We assume fading
channels and compute exact closed-form expressions for the energy harvested by
the SU under fading. Results reveal that the proposed power allocation policy
along with the implemented RF energy harvesting at the SU enhance the
throughput of both primary and secondary links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7269</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7269</id><created>2014-07-27</created><authors><author><keyname>Cohavi</keyname><forenames>Keren</forenames></author><author><keyname>Dobzinski</keyname><forenames>Shahar</forenames></author></authors><title>Faster and Simpler Sketches of Valuation Functions</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present fast algorithms for sketching valuation functions. Let $N$
($|N|=n$) be some ground set and $v:2^N\rightarrow \mathbb R$ be a function. We
say that $\tilde v:2^N\rightarrow \mathbb R$ is an $\alpha$-sketch of $v$ if
for every set $S$ we have that $\frac {v(S)} {\alpha} \leq \tilde v(S) \leq
v(S)$ and $\tilde v$ can be described in $poly(n)$ bits.
  Goemans et al. [SODA'09] showed that if $v$ is submodular then there exists
an $\tilde O(\sqrt n)$-sketch that can be constructed using polynomially many
value queries (this is the best possible, as Balcan and Harvey [STOC'11] show
that no submodular function admit an $n^{\frac 1 3 - \epsilon}$-sketch). Based
on their work, Balcan et al. [COLT'12] and Badanidiyuru et al. [SODA'12] show
that if $v$ is subadditive then there exists an $\tilde O(\sqrt n)$-sketch that
can be constructed using polynomially many demand queries. All previous
sketches are based on complicated geometric constructions. The first step in
their constructions is proving the existence of a good sketch by finding an
ellipsoid that ``approximates'' $v$ well (this is done by applying John's
theorem to ensure the existence of an ellipsoid that is ``close'' to the
polymatroid that is associated with $v$). The second step is showing this
ellipsoid can be found efficiently, and this is done by repeatedly solving a
certain convex program to obtain better approximations of John's ellipsoid.
  In this paper, we give a much simpler, non-geometric proof for the existence
of good sketches, and utilize the proof to obtain much faster algorithms that
match the previously obtained approximation bounds. Specifically, we provide an
algorithm that finds $\tilde O(\sqrt n)$-sketch of a submodular function with
only $\tilde O(n^\frac{3}{2})$ value queries, and an algorithm that finds
$\tilde O(\sqrt n)$-sketch of a subadditive function with $O(n)$ demand and
value queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7274</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7274</id><created>2014-07-27</created><updated>2015-12-27</updated><authors><author><keyname>McAllester</keyname><forenames>David</forenames></author></authors><title>Morphoid Type Theory</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphoid type theory (MorTT) is a typed foundation for mathematics extending
classical predicate calculus under Platonic compositional semantics and
supporting the concept of isomorphism. MorTT provides a formal account of the
substitution of isomorphics, the distinction between general functions and
natural maps, and &quot;Voldemort's theorem&quot; stating that certain objects exist but
cannot be named. For example, there is no natural point on a geometric circle
--- no point on a geometric circle can be named by a well-typed expression.
Similarly it is not possible to name any particular basis for a vector space or
any particular isomorphism of a finite dimensional vector space with its dual.
Homotopy type theory (HoTT) also provides a formal account of isomorphism but
extends constructive logic rather than classical predicate calculus. MorTT's
classical approach avoids HoTT's propositions-as-types, path induction,
squashing and higher order isomorphisms. Unlike HoTT, MorTT is designed to be
compatible with Platonic mathematical thought.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7276</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7276</id><created>2014-07-27</created><authors><author><keyname>Carevic</keyname><forenames>Zeljko</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Recommender Systems using Pennant Diagrams in Digital Libraries</title><categories>cs.DL cs.HC cs.IR</categories><comments>3 pages, paper accepted for the 13th European Networked Knowledge
  Organization Systems (NKOS) Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In digital libraries recommendations can be valuable for researchers, e.g.
recommending related literature to a given context. Typically, in a scientific
context the simple presentation of related content is not sufficient. Often the
users demand a more detailed view on the connection of a document and its
specific recommendations. The aim of pennants introduced by Howard White (2007)
is to provide the user with a graph showing the relatedness / distance between
a given document and related documents. Co-citation but also co-occurrence
analysis are established methods for finding related documents to a seed. A
seed could be for instance an author, a keyword, or a publication. In this
paper we introduce a recommender system in the digital library sowiport using
pennant diagrams which can be created from co-citation and/or co-occurrence
analysis. The presentation at the NKOS workshop will present demos of pennants
in sowiport and will elaborate on practical questions in visualizing pennants
and evaluating the utility of pennants for search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7277</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7277</id><created>2014-07-27</created><authors><author><keyname>Al-Ameen</keyname><forenames>Mahdi Nasrullah</forenames><affiliation>The University of Texas at Arlington, Arlington, TX, USA</affiliation></author><author><keyname>Haque</keyname><forenames>S M Taiabul</forenames><affiliation>The University of Texas at Arlington, Arlington, TX, USA</affiliation></author><author><keyname>Wright</keyname><forenames>Matthew</forenames><affiliation>The University of Texas at Arlington, Arlington, TX, USA</affiliation></author></authors><title>Q-A: Towards the Solution of Usability-Security Tension in User
  Authentication</title><categories>cs.HC cs.CR</categories><comments>10 pages, 2 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users often choose passwords that are easy to remember but also easy to guess
by attackers. Recent studies have revealed the vulnerability of textual
passwords to shoulder surfing and keystroke loggers. It remains a critical
challenge in password research to develop an authentication scheme that
addresses these security issues, in addition to offering good memorability.
Motivated by psychology research on humans' cognitive strengths and weaknesses,
we explore the potential of cognitive questions as a way to address the major
challenges in user authentication. We design, implement, and evaluate Q-A, a
novel cognitive-question-based password system that requires a user to enter
the letter at a given position in her answer for each of six personal questions
(e.g. &quot;What is the name of your favorite childhood teacher?&quot;). In this scheme,
the user does not need to memorize new, artificial information as her
authentication secret. Our scheme offers 28 bits of theoretical password space,
which has been found sufficient to prevent online brute-force attacks. Q-A is
also robust against shoulder surfing and keystroke loggers. We conducted a
multi-session in-lab user study to evaluate the usability of Q-A; 100% of users
were able to remember their Q-A password over the span of one week, although
login times were high. We compared our scheme with random six character
passwords and found that login success rate in Q-A was significantly higher.
Based on our results, we suggest that Q-A would be most appropriate in contexts
that demand high security and where logins occur infrequently (e.g., online
bank accounts).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7279</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7279</id><created>2014-07-27</created><authors><author><keyname>Aaron</keyname><forenames>Eric</forenames></author><author><keyname>Krizanc</keyname><forenames>Danny</forenames></author><author><keyname>Meyerson</keyname><forenames>Elliot</forenames></author></authors><title>DMVP: Foremost Waypoint Coverage of Time-Varying Graphs</title><categories>cs.CC cs.DS cs.RO</categories><comments>24 pages. Full version of paper from Proceedings of WG 2014, LNCS,
  Springer-Verlag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Dynamic Map Visitation Problem (DMVP), in which a team of
agents must visit a collection of critical locations as quickly as possible, in
an environment that may change rapidly and unpredictably during the agents'
navigation. We apply recent formulations of time-varying graphs (TVGs) to DMVP,
shedding new light on the computational hierarchy $\mathcal{R} \supset
\mathcal{B} \supset \mathcal{P}$ of TVG classes by analyzing them in the
context of graph navigation. We provide hardness results for all three classes,
and for several restricted topologies, we show a separation between the classes
by showing severe inapproximability in $\mathcal{R}$, limited approximability
in $\mathcal{B}$, and tractability in $\mathcal{P}$. We also give topologies in
which DMVP in $\mathcal{R}$ is fixed parameter tractable, which may serve as a
first step toward fully characterizing the features that make DMVP difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7281</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7281</id><created>2014-07-27</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Modular Belief Updates and Confusion about Measures of Certainty in
  Artificial Intelligence Research</title><categories>cs.AI</categories><comments>Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)</comments><proxy>auai</proxy><report-no>UAI-P-1985-PG-283-286</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, there has been growing interest in the use or measures
or change in belief for reasoning with uncertainty in artificial intelligence
research. An important characteristic of several methodologies that reason with
changes in belief or belief updates, is a property that we term modularity. We
call updates that satisfy this property modular updates. Whereas probabilistic
measures of belief update - which satisfy the modularity property were first
discovered in the nineteenth century, knowledge and discussion of these
quantities remains obscure in artificial intelligence research. We define
modular updates and discuss their inappropriate use in two influential expert
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7288</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7288</id><created>2014-07-27</created><authors><author><keyname>Drewniak</keyname><forenames>Krzysztof</forenames></author><author><keyname>Helsing</keyname><forenames>Joseph</forenames></author><author><keyname>Mikler</keyname><forenames>Armin R.</forenames></author></authors><title>A Method for Reducing the Severity of Epidemics by Allocating Vaccines
  According to Centrality</title><categories>cs.SI cs.CE physics.soc-ph</categories><comments>10 pages, accepted to ACM BCB 2014</comments><acm-class>J.3; G.2.2</acm-class><doi>10.1145/2649387.2649409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One long-standing question in epidemiological research is how best to
allocate limited amounts of vaccine or similar preventative measures in order
to minimize the severity of an epidemic. Much of the literature on the problem
of vaccine allocation has focused on influenza epidemics and used mathematical
models of epidemic spread to determine the effectiveness of proposed methods.
Our work applies computational models of epidemics to the problem of
geographically allocating a limited number of vaccines within several Texas
counties. We developed a graph-based, stochastic model for epidemics that is
based on the SEIR model, and tested vaccine allocation methods based on
multiple centrality measures. This approach provides an alternative method for
addressing the vaccine allocation problem, which can be combined with more
conventional approaches to yield more effective epidemic suppression
strategies. We found that allocation methods based on in-degree and inverse
betweenness centralities tended to be the most effective at containing
epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7294</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7294</id><created>2014-07-27</created><updated>2014-11-28</updated><authors><author><keyname>Amin</keyname><forenames>Kareem</forenames></author><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Dworkin</keyname><forenames>Lili</forenames></author><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Online Learning and Profit Maximization from Revealed Preferences</title><categories>cs.DS cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning from revealed preferences in an online
setting. In our framework, each period a consumer buys an optimal bundle of
goods from a merchant according to her (linear) utility function and current
prices, subject to a budget constraint. The merchant observes only the
purchased goods, and seeks to adapt prices to optimize his profits. We give an
efficient algorithm for the merchant's problem that consists of a learning
phase in which the consumer's utility function is (perhaps partially) inferred,
followed by a price optimization step. We also consider an alternative online
learning algorithm for the setting where prices are set exogenously, but the
merchant would still like to predict the bundle that will be bought by the
consumer for purposes of inventory or supply chain management. In contrast with
most prior work on the revealed preferences problem, we demonstrate that by
making stronger assumptions on the form of utility functions, efficient
algorithms for both learning and profit maximization are possible, even in
adaptive, online settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7299</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7299</id><created>2014-07-27</created><authors><author><keyname>Langville</keyname><forenames>Amy N.</forenames></author><author><keyname>Meyer</keyname><forenames>Carl D.</forenames></author><author><keyname>Albright</keyname><forenames>Russell</forenames></author><author><keyname>Cox</keyname><forenames>James</forenames></author><author><keyname>Duling</keyname><forenames>David</forenames></author></authors><title>Algorithms, Initializations, and Convergence for the Nonnegative Matrix
  Factorization</title><categories>cs.NA cs.LG stat.ML</categories><msc-class>65F30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that good initializations can improve the speed and accuracy
of the solutions of many nonnegative matrix factorization (NMF) algorithms.
Many NMF algorithms are sensitive with respect to the initialization of W or H
or both. This is especially true of algorithms of the alternating least squares
(ALS) type, including the two new ALS algorithms that we present in this paper.
We compare the results of six initialization procedures (two standard and four
new) on our ALS algorithms. Lastly, we discuss the practical issue of choosing
an appropriate convergence criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1407.7302</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1407.7302</id><created>2014-07-27</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Exploiting Large-Scale MIMO Techniques for Physical Layer Security with
  Imperfect Channel State Information</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1302.4805</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of physical layer security in large-scale
multiple input multiple output (LS-MIMO) systems. The large number of antenna
elements in LS-MIMO system is exploited to enhance transmission security and
improve system performance, especially when the eavesdropper is closer to the
information source and has more antennas than the legitimate user. However, in
practical systems, the problem becomes challenging because the eavesdropper
channel state information (CSI) is usually unavailable without cooperation and
the legitimate CSI may be imperfect due to channel estimation error. In this
paper, we first analyze the performance of physical layer security without
eavesdropper CSI and with imperfect legitimate CSI, and then propose an
energy-efficient power allocation scheme to meet the demand for wireless
security and quality of service (QoS) simultaneously. Finally, numerical
results validate the effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="63000" completeListSize="102538">1122234|64001</resumptionToken>
</ListRecords>
</OAI-PMH>
