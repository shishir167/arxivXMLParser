<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:07:06Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|58001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3461</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3461</id><created>2014-03-13</created><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author></authors><title>Aspects of Favorable Propagation in Massive MIMO</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Favorable propagation, defined as mutual orthogonality among the
vector-valued channels to the terminals, is one of the key properties of the
radio channel that is exploited in Massive MIMO. However, there has been little
work that studies this topic in detail. In this paper, we first show that
favorable propagation offers the most desirable scenario in terms of maximizing
the sum-capacity. One useful proxy for whether propagation is favorable or not
is the channel condition number. However, this proxy is not good for the case
where the norms of the channel vectors may not be equal. For this case, to
evaluate how favorable the propagation offered by the channel is, we propose a
``distance from favorable propagation'' measure, which is the gap between the
sum-capacity and the maximum capacity obtained under favorable propagation.
Secondly, we examine how favorable the channels can be for two extreme
scenarios: i.i.d. Rayleigh fading and uniform random line-of-sight (UR-LoS).
Both environments offer (nearly) favorable propagation. Furthermore, to analyze
the UR-LoS model, we propose an urns-and-balls model. This model is simple and
explains the singular value spread characteristic of the UR-LoS model well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3462</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3462</id><created>2014-03-13</created><authors><author><keyname>Friedman</keyname><forenames>Joel</forenames></author><author><keyname>Kohler</keyname><forenames>David-Emmanuel</forenames></author></authors><title>The Relativized Second Eigenvalue Conjecture of Alon</title><categories>cs.DM math.CO</categories><msc-class>Primary: 68R10, 05C50, Secondary: 05C80, 15B52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a relativization of the Alon Second Eigenvalue Conjecture for all
$d$-regular base graphs, $B$, with $d\ge 3$: for any $\epsilon&gt;0$, we show that
a random covering map of degree $n$ to $B$ has a new eigenvalue greater than
$2\sqrt{d-1}+\epsilon$ in absolute value with probability $O(1/n)$.
Furthermore, if $B$ is a Ramanujan graph, we show that this probability is
proportional to $n^{-{\eta_{\rm \,fund}}(B)}$, where ${\eta_{\rm \,fund}}(B)$
is an integer depending on $B$, which can be computed by a finite algorithm for
any fixed $B$. For any $d$-regular graph, $B$, ${\eta_{\rm \,fund}}(B)$ is
greater than $\sqrt{d-1}$.
  Our proof introduces a number of ideas that simplify and strengthen the
methods of Friedman's proof of the original conjecture of Alon. The most
significant new idea is that of a ``certified trace,'' which is not only
greatly simplifies our trace methods, but is the reason we can obtain the
$n^{-{\eta_{\rm \,fund}}(B)}$ estimate above. This estimate represents an
improvement over Friedman's results of the original Alon conjecture for random
$d$-regular graphs, for certain values of $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3465</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3465</id><created>2014-03-13</created><updated>2015-11-09</updated><authors><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author></authors><title>A Survey of Algorithms and Analysis for Adaptive Online Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present tools for the analysis of Follow-The-Regularized-Leader (FTRL),
Dual Averaging, and Mirror Descent algorithms when the regularizer
(equivalently, prox-function or learning rate schedule) is chosen adaptively
based on the data. Adaptivity can be used to prove regret bounds that hold on
every round, and also allows for data-dependent regret bounds as in
AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive
per-coordinate learning rates). We present results from a large number of prior
works in a unified manner, using a modular and tight analysis that isolates the
key arguments in easily re-usable lemmas. This approach strengthens pre-viously
known FTRL analysis techniques to produce bounds as tight as those achieved by
potential functions or primal-dual analysis. Further, we prove a general and
exact equivalence between an arbitrary adaptive Mirror Descent algorithm and a
correspond- ing FTRL update, which allows us to analyze any Mirror Descent
algorithm in the same framework. The key to bridging the gap between Dual
Averaging and Mirror Descent algorithms lies in an analysis of the
FTRL-Proximal algorithm family. Our regret bounds are proved in the most
general form, holding for arbitrary norms and non-smooth regularizers with
time-varying weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3480</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3480</id><created>2014-03-13</created><authors><author><keyname>Liang</keyname><forenames>Fan</forenames></author><author><keyname>Feng</keyname><forenames>Chen</forenames></author><author><keyname>Lu</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author></authors><title>Performance Benefits of DataMPI: A Case Study with BigDataBench</title><categories>cs.PF cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apache Hadoop and Spark are gaining prominence in Big Data processing and
analytics. Both of them are widely deployed on Internet companies. On the other
hand, high-performance data analysis requirements are causing academical and
industrial communities to adopt state-of-the-art technologies in HPC to solve
Big Data problems. Recently, we have proposed a key-value pair based
communication library, DataMPI, which is extending MPI to support
Hadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench,
a Big Data benchmark suite, to do comprehensive studies on performance and
resource utilization characterizations of Hadoop, Spark and DataMPI. From our
experiments, we observe that the job execution time of DataMPI has up to 55%
and 39% speedups compared with those of Hadoop and Spark, respectively. Most of
the benefits come from the high-efficiency communication mechanisms in DataMPI.
We also notice that the resource (CPU, memory, disk and network I/O)
utilizations of DataMPI are also more efficient than those of the other two
frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3488</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3488</id><created>2014-03-14</created><authors><author><keyname>Jonglez</keyname><forenames>Baptiste</forenames><affiliation>PPS</affiliation></author><author><keyname>Boutier</keyname><forenames>Matthieu</forenames><affiliation>PPS</affiliation></author><author><keyname>Chroboczek</keyname><forenames>Juliusz</forenames><affiliation>PPS</affiliation></author></authors><title>A delay-based routing metric</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In overlay networks, both local and long-distance links appear as a single
hop to a routing protocol. Traditional routing metrics (based on hop count or
packet loss) fail to take the differences between such links into account. In
this paper, we study a metric based on packet delay that has been designed to
improve routing in overlay networks. We show a lightweight technique for
measuring delay asynchronously, and show how to use the data it provides for
constructing a routing metric. Using delay naively leads to persistent routing
oscillations, so our routing protocol implements a number of features to bound
the frequency of oscillations. We show that our protocol causes no oscillations
in real-world tests, and has oscillations with a period on the order of minutes
in artificially constructed worst-case setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3495</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3495</id><created>2014-03-14</created><authors><author><keyname>Wang</keyname><forenames>Shuliang</forenames></author><author><keyname>Zhao</keyname><forenames>Yiping</forenames></author></authors><title>Analyzing Large Biological Datasets with an Improved Algorithm for MIC</title><categories>cs.DB cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computational framework utilizes the traditional similarity measures for
mining the significant relationships in biological annotations is recently
proposed by Tatiana V. Karpinets et al. [2]. In this paper, an improved
approximation algorithm for MIC (maximal information coefficient) named IAMIC
is suggested to perfect this framework for discovering the hidden regularities
between biological annotations. Further, IAMIC is the enhanced algorithm for
approximating a novel similarity coefficient MIC with generality and
equitability, which makes it more appropriate for data exploration. Here it is
shown that IAMIC is also applicable for identify the associations between
biological annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3498</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3498</id><created>2014-03-14</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author></authors><title>Using Cluster Curves to Control Software Development Projects</title><categories>cs.SE</categories><comments>2 pages</comments><journal-ref>Proceedings of the 1st International Symposium on Empirical
  Software Engineering (ISESE 2002), volume 3, pages 13-14, Nara, Japan,
  October 3-4 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online interpretation and visualization of project data are gaining
increasing importance on the long road towards predictable and controllable
software project execution. This paper sketches the Sprint I controlling
approach for software development projects and gives first evaluation results.
The approach is grounded on the usage of context-oriented cluster curves and
integrated in the framework of software project control centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3502</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3502</id><created>2014-03-14</created><authors><author><keyname>Facchini</keyname><forenames>Alessandro</forenames></author><author><keyname>Michalewski</keyname><forenames>Henryk</forenames></author></authors><title>Deciding the Borel complexity of regular tree languages</title><categories>cs.LO cs.FL math.LO</categories><comments>15 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We show that it is decidable whether a given a regular tree language belongs
to the class ${\bf \Delta^0_2}$ of the Borel hierarchy, or equivalently whether
the Wadge degree of a regular tree language is countable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3515</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3515</id><created>2014-03-14</created><updated>2014-06-23</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Concept Trees: Building Dynamic Concepts from Semi-Structured Data using
  Nature-Inspired Methods</title><categories>cs.IR</categories><comments>Pre-print</comments><journal-ref>Q. Zhu, A.T Azar (eds.), Complex system modelling and control
  through intelligent soft computations, Studies in Fuzziness and Soft
  Computing, Springer-Verlag, Germany, Vol. 319, pp. 221 - 252, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method for creating structure from heterogeneous
sources, as part of an information database, or more specifically, a 'concept
base'. Structures called 'concept trees' can grow from the semi-structured
sources when consistent sequences of concepts are presented. They might be
considered to be dynamic databases, possibly a variation on the distributed
Agent-Based or Cellular Automata models, or even related to Markov models.
Semantic comparison of text is required, but the trees can be built more, from
automatic knowledge and statistical feedback. This reduced model might also be
attractive for security or privacy reasons, as not all of the potential data
gets saved. The construction process maintains the key requirement of
generality, allowing it to be used as part of a generic framework. The nature
of the method also means that some level of optimisation or normalisation of
the information will occur. This gives comparisons with databases or
knowledge-bases, but a database system would firstly model its environment or
datasets and then populate the database with instance values. The concept base
deals with a more uncertain environment and therefore cannot fully model it
beforehand. The model itself therefore evolves over time. Similar to databases,
it also needs a good indexing system, where the construction process provides
memory and indexing structures. These allow for more complex concepts to be
automatically created, stored and retrieved, possibly as part of a more
cognitive model. There are also some arguments, or more abstract ideas, for
merging physical-world laws into these automatic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3522</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3522</id><created>2014-03-14</created><updated>2014-09-12</updated><authors><author><keyname>Lorenz</keyname><forenames>Dirk A.</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>An inertial forward-backward algorithm for monotone inclusions</title><categories>cs.CV cs.NA math.NA math.OC</categories><comments>The final publication is available at http://link.springer.com</comments><doi>10.1007/s10851-014-0523-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an inertial forward backward splitting algorithm to
compute a zero of the sum of two monotone operators, with one of the two
operators being co-coercive. The algorithm is inspired by the accelerated
gradient method of Nesterov, but can be applied to a much larger class of
problems including convex-concave saddle point problems and general monotone
inclusions. We prove convergence of the algorithm in a Hilbert space setting
and show that several recently proposed first-order methods can be obtained as
special cases of the general algorithm. Numerical results show that the
proposed algorithm converges faster than existing methods, while keeping the
computational cost of each iteration basically unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3524</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3524</id><created>2014-03-14</created><authors><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author><author><keyname>Lamperski</keyname><forenames>Andrew</forenames></author></authors><title>Automata Theory Meets Barrier Certificates: Temporal Logic Verification
  of Nonlinear Systems</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider temporal logic verification of (possibly nonlinear) dynamical
systems evolving over continuous state spaces. Our approach combines
automata-based verification and the use of so-called barrier certificates.
Automata-based verification allows the decomposition the verification task into
a finite collection of simpler constraints over the continuous state space. The
satisfaction of these constraints in turn can be (potentially conservatively)
proved by appropriately constructed barrier certificates. As a result, our
approach, together with optimization-based search for barrier certificates,
allows computational verification of dynamical systems against temporal logic
properties while avoiding explicit abstractions of the dynamics as commonly
done in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3533</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3533</id><created>2014-03-14</created><updated>2015-09-25</updated><authors><author><keyname>de Beaudrap</keyname><forenames>Niel</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author></authors><title>Quantum linear network coding as one-way quantum computation</title><categories>quant-ph cs.IT math.IT</categories><comments>17 pages, 6 figures. Updated to correct an incorrect (albeit
  hilarious) reference in the arXiv version of the abstract</comments><acm-class>E.4; F.1.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Network coding is a technique to maximize communication rates within a
network, in communication protocols for simultaneous multi-party transmission
of information. Linear network codes are examples of such protocols in which
the local computations performed at the nodes in the network are limited to
linear transformations of their input data (represented as elements of a ring,
such as the integers modulo 2). The quantum linear network coding protocols of
Kobayashi et al [arXiv:0908.1457 and arXiv:1012.4583] coherently simulate
classical linear network codes, using supplemental classical communication. We
demonstrate that these protocols correspond in a natural way to
measurement-based quantum computations with graph states over over qudits
[arXiv:quant-ph/0301052, arXiv:quant-ph/0603226, and arXiv:0704.1263] having a
structure directly related to the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3547</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3547</id><created>2014-03-14</created><authors><author><keyname>Pandey</keyname><forenames>Rakesh Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Dilip</forenames></author></authors><title>Distributed Transformer Monitoring System Based On Zigbee Technology</title><categories>cs.OH</categories><comments>3 Pages, 5 Figures, International Journal of Engineering Trends and
  Technology (IJETT)-Volume4 Issue5-May (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Distributed transformer networks remote monitoring system is developed and
constructed,for monitor and record the parameters like temperature, oil level
status, of a distribution transformer.The system consists of a micro controller
based circuit,with solid-state components for handling sensors,power
back-up,real time clock and data communication module which based on ZigBee
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3551</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3551</id><created>2014-03-14</created><authors><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>St&#xf6;ckel</keyname><forenames>Morten</forenames></author></authors><title>The Input/Output Complexity of Sparse Matrix Multiplication</title><categories>cs.DS</categories><comments>Submitted to ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of multiplying sparse matrices (over a semiring)
where the number of non-zero entries is larger than main memory. In the
classical paper of Hong and Kung (STOC '81) it was shown that to compute a
product of dense $U \times U$ matrices, $\Theta \left(U^3 / (B \sqrt{M})
\right)$ I/Os are necessary and sufficient in the I/O model with internal
memory size $M$ and memory block size $B$.
  In this paper we generalize the upper and lower bounds of Hong and Kung to
the sparse case. Our bounds depend of the number $N =
\mathtt{nnz}(A)+\mathtt{nnz}(C)$ of nonzero entries in $A$ and $C$, as well as
the number $Z = \mathtt{nnz}(AC)$ of nonzero entries in $AC$.
  We show that $AC$ can be computed using $\tilde{O} \left(\tfrac{N}{B}
\min\left(\sqrt{\tfrac{Z}{M}},\tfrac{N}{M}\right) \right)$ I/Os, with high
probability. This is tight (up to polylogarithmic factors) when only semiring
operations are allowed, even for dense rectangular matrices: We show a lower
bound of $\Omega \left(\tfrac{N}{B}
\min\left(\sqrt{\tfrac{Z}{M}},\tfrac{N}{M}\right) \right)$ I/Os.
  While our lower bound uses fairly standard techniques, the upper bound makes
use of ``compressed matrix multiplication'' sketches, which is new in the
context of I/O-efficient algorithms, and a new matrix product size estimation
technique that avoids the ``no cancellation'' assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3553</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3553</id><created>2014-03-14</created><authors><author><keyname>Esposito</keyname><forenames>Flavio</forenames></author><author><keyname>Matta</keyname><forenames>Ibrahim</forenames></author></authors><title>A Decomposition-based Architecture for Distributed Virtual Network
  Embedding</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network protocols have historically been developed on an ad-hoc basis, and
cloud computing is no exception. A fundamental management protocol, not yet
standardized, that cloud providers need to run to support wide-area virtual
network services is the virtual network (VN) embedding protocol. In this paper,
we use decomposition theory to provide a unifying architecture for the VN
embedding problem. We show how our architecture subsumes existing solutions,
and how it can be used by cloud providers to design a distributed VN embedding
protocol that adapts to different scenarios, by merely instantiating different
decomposition policies. We analyze key representative tradeoffs via simulation,
and with our VN embedding testbed that uses a Linux system architecture to
reserve virtual node and link capacities. In contrast with existing VN
embedding solutions, we found that partitioning a VN request not only increases
the signaling overhead, but may decrease cloud providers' revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3559</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3559</id><created>2014-03-14</created><authors><author><keyname>Rus</keyname><forenames>Ioana</forenames></author><author><keyname>Neu</keyname><forenames>Holger</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>A Systematic Methodology for Developing Discrete Event Simulation Models
  of Software Development Processes</title><categories>cs.SE</categories><comments>7 pages</comments><journal-ref>Proceedings of the 4th International Workshop on Software Process
  Simulation and Modeling (ProSim 2003), Portland, Oregon, USA, May 3-4 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far there have been several efforts for developing software process
simulators. However, the approaches for developing the simulators seem to have
been ad-hoc and no systematic methodology exists. Since modeling and simulation
in support of software development should become more popular (and there are
signs that it does), there is a need for migrating modeling from craft to
engineering. This article proposes such a systematic method, focused on the
development of discrete simulation-based decision models, but extensible to
other modeling approaches as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3562</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3562</id><created>2014-03-14</created><updated>2015-07-23</updated><authors><author><keyname>Veroneze</keyname><forenames>Rosana</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando J.</forenames></author></authors><title>Enumerating all maximal biclusters in numerical datasets</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biclustering has proved to be a powerful data analysis technique due to its
wide success in various application domains. However, the existing literature
presents efficient solutions only for enumerating maximal biclusters with
constant values, or heuristic-based approaches which can not find all
biclusters or even support the maximality of the obtained biclusters. Here, we
present a general family of biclustering algorithms for enumerating all maximal
biclusters with (i) constant values on rows, (ii) constant values on columns,
or (iii) coherent values. Versions for perfect and for perturbed biclusters are
provided. Our algorithms have four key properties (just the algorithm for
perturbed biclusters with coherent values fails to exhibit the first property):
they are (1) efficient (take polynomial time per pattern), (2) complete (find
all maximal biclusters), (3) correct (all biclusters attend the user-defined
measure of similarity), and (4) non-redundant (all the obtained biclusters are
maximal and the same bicluster is not enumerated twice). They are based on a
generalization of an efficient formal concept analysis algorithm called
In-Close2. Experimental results point to the necessity of having efficient
enumerative biclustering algorithms and provide a valuable insight into the
scalability of our family of algorithms and its sensitivity to user-defined
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3563</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3563</id><created>2014-03-14</created><authors><author><keyname>Ramsdell</keyname><forenames>John D.</forenames></author></authors><title>Proving Security Goals With Shape Analysis Sentences</title><categories>cs.CR</categories><comments>MITRE Technical Report. arXiv admin note: substantial text overlap
  with arXiv:1204.0480</comments><report-no>MTR130488</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper that introduced shape analysis sentences presented a method for
extracting a sentence in first-order logic that completely characterizes a run
of CPSA. Logical deduction can then be used to determine if a security goal is
satisfied.
  This paper presents a method for importing shape analysis sentences into a
proof assistant on top of a detailed theory of strand spaces. The result is a
semantically rich environment in which the validity of a security goal can be
determined using shape analysis sentences and the foundation on which they are
based.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3565</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3565</id><created>2014-03-14</created><updated>2015-06-10</updated><authors><author><keyname>Bazgan</keyname><forenames>Cristina</forenames></author><author><keyname>Chopin</keyname><forenames>Morgan</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>Parameterized Inapproximability of Target Set Selection and
  Generalizations</title><categories>cs.CC</categories><journal-ref>Computability, vol. 3, no. 2, 2014</journal-ref><doi>10.3233/COM-140030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Target Set Selection problem: given a graph
and a threshold value $thr(v)$ for any vertex $v$ of the graph, find a minimum
size vertex-subset to &quot;activate&quot; s.t. all the vertices of the graph are
activated at the end of the propagation process. A vertex $v$ is activated
during the propagation process if at least $thr(v)$ of its neighbors are
activated. This problem models several practical issues like faults in
distributed networks or word-to-mouth recommendations in social networks. We
show that for any functions $f$ and $\rho$ this problem cannot be approximated
within a factor of $\rho(k)$ in $f(k) \cdot n^{O(1)}$ time, unless FPT = W[P],
even for restricted thresholds (namely constant and majority thresholds). We
also study the cardinality constraint maximization and minimization versions of
the problem for which we prove similar hardness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3568</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3568</id><created>2014-03-14</created><updated>2014-06-14</updated><authors><author><keyname>I&#xf1;iguez</keyname><forenames>Gerardo</forenames></author><author><keyname>T&#xf6;r&#xf6;k</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Modeling Social Dynamics in a Collaborative Environment</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>Revised version, to appear in EPJ Data Science; 19 pages 9 figures</comments><journal-ref>EPJ Data Science 3 (1), 7 (2014)</journal-ref><doi>10.1140/epjds/s13688-014-0007-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia is a prime example of today's value production in a collaborative
environment. Using this example, we model the emergence, persistence and
resolution of severe conflicts during collaboration by coupling opinion
formation with article editing in a bounded confidence dynamics. The complex
social behavior involved in editing articles is implemented as a minimal model
with two basic elements; (i) individuals interact directly to share information
and convince each other, and (ii) they edit a common medium to establish their
own opinions. Opinions of the editors and that represented by the article are
characterised by a scalar variable. When the pool of editors is fixed, three
regimes can be distinguished: (a) a stable mainstream article opinion is
continuously contested by editors with extremist views and there is slow
convergence towards consensus, (b) the article oscillates between editors with
extremist views, reaching consensus relatively fast at one of the extremes, and
(c) the extremist editors are converted very fast to the mainstream opinion and
the article has an erratic evolution. When editors are renewed with a certain
rate, a dynamical transition occurs between different kinds of edit wars, which
qualitatively reflect the dynamics of conflicts as observed in real Wikipedia
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3579</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3579</id><created>2014-03-14</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Anderson</keyname><forenames>James</forenames></author></authors><title>On Projection-Based Model Reduction of Biochemical Networks-- Part I:
  The Deterministic Case</title><categories>math.OC cs.SY</categories><comments>Submitted to 53rd IEEE CDC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of model reduction for dynamical system
models that describe biochemical reaction networks. Inherent in such models are
properties such as stability, positivity and network structure. Ideally these
properties should be preserved by model reduction procedures, although
traditional projection based approaches struggle to do this. We propose a
projection based model reduction algorithm which uses generalised block
diagonal Gramians to preserve structure and positivity. Two algorithms are
presented, one provides more accurate reduced order models, the second provides
easier to simulate reduced order models. The results are illustrated through
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3583</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3583</id><created>2014-03-14</created><authors><author><keyname>Wei</keyname><forenames>Lai</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Fuja</keyname><forenames>Thomas E.</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Threshold Analysis of Non-Binary Spatially-Coupled LDPC Codes with
  Windowed Decoding</title><categories>cs.IT math.IT</categories><comments>6 pages, 8 figures; submitted to 2014 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the iterative decoding threshold performance of
non-binary spatially-coupled low-density parity-check (NB-SC-LDPC) code
ensembles for both the binary erasure channel (BEC) and the binary-input
additive white Gaussian noise channel (BIAWGNC), with particular emphasis on
windowed decoding (WD). We consider both (2,4)-regular and (3,6)-regular
NB-SC-LDPC code ensembles constructed using protographs and compute their
thresholds using protograph versions of NB density evolution and NB extrinsic
information transfer analysis. For these code ensembles, we show that WD of
NB-SC-LDPC codes, which provides a significant decrease in latency and
complexity compared to decoding across the entire parity-check matrix, results
in a negligible decrease in the near-capacity performance for a sufficiently
large window size W on both the BEC and the BIAWGNC. Also, we show that
NB-SC-LDPC code ensembles exhibit gains in the WD threshold compared to the
corresponding block code ensembles decoded across the entire parity-check
matrix, and that the gains increase as the finite field size q increases.
Moreover, from the viewpoint of decoding complexity, we see that (3,6)-regular
NB-SC-LDPC codes are particularly attractive due to the fact that they achieve
near-capacity thresholds even for small q and W.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3594</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3594</id><created>2014-03-14</created><updated>2014-06-18</updated><authors><author><keyname>Kaltofen</keyname><forenames>Erich L.</forenames></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames></author></authors><title>Sparse Polynomial Interpolation Codes and their decoding beyond half the
  minimal distance</title><categories>cs.SC cs.IT math.IT</categories><comments>23 pages, 5 figures, In Proceedings of the International Symposium on
  Symbolic and Algebraic Computation 2014 (ISSAC'14)</comments><acm-class>I.1.2; G.1.1; E.4</acm-class><doi>10.1145/2608628.2608660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms performing sparse univariate polynomial interpolation
with errors in the evaluations of the polynomial. Based on the initial work by
Comer, Kaltofen and Pernet [Proc. ISSAC 2012], we define the sparse polynomial
interpolation codes and state that their minimal distance is precisely the
length divided by twice the sparsity. At ISSAC 2012, we have given a decoding
algorithm for as much as half the minimal distance and a list decoding
algorithm up to the minimal distance. Our new polynomial-time list decoding
algorithm uses sub-sequences of the received evaluations indexed by a linear
progression, allowing the decoding for a larger radius, that is, more errors in
the evaluations while returning a list of candidate sparse polynomials. We
quantify this improvement for all typically small values of number of terms and
number of errors, and provide a worst case asymptotic analysis of this
improvement. For instance, for sparsity T = 5 with up to 10 errors we can list
decode in polynomial-time from 74 values of the polynomial with unknown terms,
whereas our earlier algorithm required 2T (E + 1) = 110 evaluations. We then
propose two variations of these codes in characteristic zero, where appropriate
choices of values for the variable yield a much larger minimal distance: the
length minus twice the sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3595</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3595</id><created>2014-03-14</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Generalized prisoner's dilemma</title><categories>cs.GT</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prisoner's dilemma has been heavily studied. In classical model, each player
chooses to either &quot;Cooperate&quot; or &quot;Defect&quot;. In this paper, we generalize the
prisoner's dilemma with a new alternative which is neither defect or
cooperation. The classical model is the special case under the condition that
the third state is not taken into consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3602</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3602</id><created>2014-03-14</created><authors><author><keyname>Aina</keyname><forenames>Segun</forenames></author><author><keyname>Rahulamathavan</keyname><forenames>Yogachandran</forenames></author><author><keyname>Phan</keyname><forenames>Raphael C. -W.</forenames></author><author><keyname>Chambers</keyname><forenames>Jonathon A.</forenames></author></authors><title>Spontaneous expression classification in the encrypted domain</title><categories>cs.CV cs.CR</categories><comments>4 pages. 9th IMA International Conference on Mathematics in Signal
  Processing, Birmingham, UK, Dec. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, most facial expression analysis have been based on posed image
databases and is carried out without being able to protect the identity of the
subjects whose expressions are being recognised. In this paper, we propose and
implement a system for classifying facial expressions of images in the
encrypted domain based on a Paillier cryptosystem implementation of Fisher
Linear Discriminant Analysis and k-nearest neighbour (FLDA + kNN). We present
results of experiments carried out on a recently developed natural visible and
infrared facial expression (NVIE) database of spontaneous images. To the best
of our knowledge, this is the first system that will allow the recog-nition of
encrypted spontaneous facial expressions by a remote server on behalf of a
client.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3610</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3610</id><created>2014-03-14</created><updated>2015-09-10</updated><authors><author><keyname>Ghosh</keyname><forenames>Aritra</forenames></author><author><keyname>Manwani</keyname><forenames>Naresh</forenames></author><author><keyname>Sastry</keyname><forenames>P. S.</forenames></author></authors><title>Making Risk Minimization Tolerant to Label Noise</title><categories>cs.LG</categories><doi>10.1016/j.neucom.2014.09.081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, the training data, from which one needs to learn a
classifier, is corrupted with label noise. Many standard algorithms such as SVM
perform poorly in presence of label noise. In this paper we investigate the
robustness of risk minimization to label noise. We prove a sufficient condition
on a loss function for the risk minimization under that loss to be tolerant to
uniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss and
probit loss satisfy this condition though none of the standard convex loss
functions satisfy it. We also prove that, by choosing a sufficiently large
value of a parameter in the loss function, the sigmoid loss, ramp loss and
probit loss can be made tolerant to non-uniform label noise also if we can
assume the classes to be separable under noise-free data distribution. Through
extensive empirical studies, we show that risk minimization under the $0-1$
loss, the sigmoid loss and the ramp loss has much better robustness to label
noise when compared to the SVM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3611</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3611</id><created>2014-03-12</created><authors><author><keyname>Cohen</keyname><forenames>Ernie</forenames></author></authors><title>Modular Verification of Hybrid System Code with VCC</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a methodology for object-modular reasoning about hybrid system
code using VCC, a deductive verifier for concurrent C code. We define in VCC an
explicit time model, in which the passage of time must respect the invariants
of certain timed objects. Fields that change automatically with changes to time
are then defined as volatile fields with suitable invariants. We also define
two types of timed objects that prevent time from advancing past a given
expiration: Timers (which represent assumptions about the upper limit on the
time it takes to do something) and Deadlines (which represent assertions about
these limits). The difference between the two is that once the expiration time
of a Deadline is reached, the Deadline and time itself are permanently
deadlocked. Our methodology includes showing that all Deadlines are eventually
destroyed, proving that they do not interfere with the flow of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3616</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3616</id><created>2014-03-14</created><updated>2014-12-08</updated><authors><author><keyname>Miotto</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>Predictability of extreme events in social media</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>13 pages, 3 figures</comments><journal-ref>Miotto JM, Altmann EG (2014) Predictability of Extreme Events in
  Social Media. PLoS ONE 9(11): e111506</journal-ref><doi>10.1371/journal.pone.0111506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is part of our daily social-media experience that seemingly ordinary items
(videos, news, publications, etc.) unexpectedly gain an enormous amount of
attention. Here we investigate how unexpected these events are. We propose a
method that, given some information on the items, quantifies the predictability
of events, i.e., the potential of identifying in advance the most successful
items defined as the upper bound for the quality of any prediction based on the
same information. Applying this method to different data, ranging from views in
YouTube videos to posts in Usenet discussion groups, we invariantly find that
the predictability increases for the most extreme events. This indicates that,
despite the inherently stochastic collective dynamics of users, efficient
prediction is possible for the most extreme events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3628</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3628</id><created>2014-03-14</created><authors><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LAGRANGE</affiliation></author><author><keyname>Jrad</keyname><forenames>Nisrine</forenames><affiliation>GIPSA-lab</affiliation></author><author><keyname>Phlypo</keyname><forenames>Ronald</forenames><affiliation>GIPSA-lab</affiliation></author><author><keyname>Congedo</keyname><forenames>Marco</forenames><affiliation>GIPSA-lab</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author></authors><title>Mixed-norm Regularization for Brain Decoding</title><categories>cs.LG</categories><comments>Computational and Mathematical Methods in Medicine (2014)
  http://www.hindawi.com/journals/cmmm/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the use of mixed-norm regularization for sensor
selection in Event-Related Potential (ERP) based Brain-Computer Interfaces
(BCI). The classification problem is cast as a discriminative optimization
framework where sensor selection is induced through the use of mixed-norms.
This framework is extended to the multi-task learning situation where several
similar classification tasks related to different subjects are learned
simultaneously. In this case, multi-task learning helps in leveraging data
scarcity issue yielding to more robust classifiers. For this purpose, we have
introduced a regularizer that induces both sensor selection and classifier
similarities. The different regularization approaches are compared on three ERP
datasets showing the interest of mixed-norm regularization in terms of sensor
selection. The multi-task approaches are evaluated when a small number of
learning examples are available yielding to significant performance
improvements especially for subjects performing poorly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3649</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3649</id><created>2014-03-14</created><authors><author><keyname>Ngenzi</keyname><forenames>Alexander</forenames></author></authors><title>Applying mathematical models in cloud computing: A survey</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As more and more information on individuals and companies are placed in the
cloud, concerns are beginning to grow about just how safe an environment it is.
It is better to prevent security threats before they enter into the systems and
there is no way how this can be prevented without knowing where they come from.
The issue of resource allocation and revenue maximization is also equally
important especially when it comes to cloud security. This brings about the
necessity of different modelling techniques including but not limited; security
threat, resource allocation and revenue maximization models. This survey paper
will try to analyse security threats and risk mitigation in cloud computing. It
gives introduction of how viral attack can invade the virtual machines on the
cloud, discusses the top security threats and countermeasures by providing the
viral threat modelling in virtual machines and risk mitigation. Resource
allocation models and revenue maximization techniques are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3660</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3660</id><created>2014-03-14</created><authors><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Koupparis</keyname><forenames>Charalambos</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>A CCA secure cryptosystem using matrices over group rings</title><categories>cs.CR math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a cryptosystem based on matrices over group rings and claim that
it is secure against adaptive chosen ciphertext attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3661</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3661</id><created>2014-03-14</created><updated>2014-08-27</updated><authors><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Vidaurre</keyname><forenames>Elizabeth</forenames></author></authors><title>Publicly Verifiable Secret Sharing Using Non-Abelian Groups</title><categories>cs.CR math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his paper Stadler develops techniques for improving the security of
existing secret sharing protocols by allowing to check whether the secret
shares given out by the dealer are valid. In particular, the secret sharing is
executed over abelian groups. In this paper we develop similar methods over
non-abelian groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3665</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3665</id><created>2014-03-14</created><authors><author><keyname>Sun</keyname><forenames>Qian</forenames></author><author><keyname>Zhu</keyname><forenames>Gang</forenames></author><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Xuan</forenames></author><author><keyname>Zhong</keyname><forenames>Zhangdui</forenames></author></authors><title>A Low-Complexity Algorithm for Throughput Maximization in Wireless
  Powered Communication Networks</title><categories>cs.IT math.IT</categories><comments>in Proc.IEEE Intell.Green Build.Smart Grid (IGBSG),accepted for
  pulication. Apr. 2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper investigates a wireless powered communication network (WPCN) under
the protocol of harvest-then-transmit,where a hybrid access point with constant
power supply replenishes the passive user nodes by wireless power transfer in
the downlink,then each user node transmit independent information to the hybrid
AP in a time division multiple access (TDMA) scheme in the uplink.The
sum-throughput maximization and min-throughput maximization problems are
considered in this paper.The optimal time allocation for the sum-throughput
maximization is proposed based on the Jensen's inequality,which provides more
insight into the design of WPCNs.A low-complexity fixed-point iteration
algorithm for the min-throughput maximization problem,which promises a much
better computation complexity than the state-of-the-art algorithm.Simulation
results confirm the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3668</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3668</id><created>2014-03-14</created><updated>2014-03-18</updated><authors><author><keyname>Merin</keyname><forenames>Arthur</forenames></author></authors><title>Language Heedless of Logic - Philosophy Mindful of What? Failures of
  Distributive and Absorption Laws</title><categories>cs.CL</categories><comments>33 pages. v2 differs from v1 thus: typos corrected, a sentence
  adjusted for anaphoric reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of philosophical logic and all of philosophy of language make empirical
claims about the vernacular natural language. They presume semantics under
which `and' and `or' are related by the dually paired distributive and
absorption laws. However, at least one of each pair of laws fails in the
vernacular. `Implicature'-based auxiliary theories associated with the
programme of H.P. Grice do not prove remedial. Conceivable alternatives that
might replace the familiar logics as descriptive instruments are briefly noted:
(i) substructural logics and (ii) meaning composition in linear algebras over
the reals, occasionally constrained by norms of classical logic. Alternative
(ii) locates the problem in violations of one of the idempotent laws. Reasons
for a lack of curiosity about elementary and easily testable implications of
the received theory are considered. The concept of `reflective equilibrium' is
critically examined for its role in reconciling normative desiderata and
descriptive commitments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3678</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3678</id><created>2014-03-14</created><authors><author><keyname>Kudekar</keyname><forenames>Shrinivas</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author><author><keyname>Iyengar</keyname><forenames>Aravind</forenames></author></authors><title>The Effect of Saturation on Belief Propagation Decoding of LDPC Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT. Longer version to be submitted to IT Transactions
  in preparation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the effect of LLR saturation on belief propagation decoding of
low-density parity-check codes. Saturation occurs universally in practice and
is known to have a significant effect on error floor performance. Our focus is
on threshold analysis and stability of density evolution.
  We analyze the decoder for certain low-density parity-check code ensembles
and show that belief propagation decoding generally degrades gracefully with
saturation. Stability of density evolution is, on the other hand, rather
strongly affected by saturation and the asymptotic qualitative effect of
saturation is similar to reduction of variable node degree by one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3683</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3683</id><created>2014-03-14</created><authors><author><keyname>Damiand</keyname><forenames>Guillaume</forenames></author><author><keyname>Gonzalez-Diaz</keyname><forenames>Rocio</forenames></author><author><keyname>Peltier</keyname><forenames>Samuel</forenames></author></authors><title>Removal and Contraction Operations in $n$D Generalized Maps for
  Efficient Homology Computation</title><categories>cs.CV</categories><comments>Research report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that contraction operations preserve the homology of
$n$D generalized maps, under some conditions. Removal and contraction
operations are used to propose an efficient algorithm that compute homology
generators of $n$D generalized maps. Its principle consists in simplifying a
generalized map as much as possible by using removal and contraction
operations. We obtain a generalized map having the same homology than the
initial one, while the number of cells decreased significantly.
  Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators;
Contraction and Removal Operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3698</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3698</id><created>2014-03-14</created><authors><author><keyname>Galliani</keyname><forenames>Pietro</forenames></author></authors><title>On Strongly First-Order Dependencies</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the expressive power of first-order logic with team semantics
plus contradictory negation does not rise beyond that of first-order logic
(with respect to sentences), and that the totality atoms of arity k +1 are not
definable in terms of the totality atoms of arity k. We furthermore prove that
all first-order nullary and unary dependencies are strongly first order, in the
sense that they do not increase the expressive power of first order logic if
added to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3707</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3707</id><created>2014-03-14</created><authors><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author><author><keyname>Cole</keyname><forenames>Christopher</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author></authors><title>Learning the Latent State Space of Time-Varying Graphs</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From social networks to Internet applications, a wide variety of electronic
communication tools are producing streams of graph data; where the nodes
represent users and the edges represent the contacts between them over time.
This has led to an increased interest in mechanisms to model the dynamic
structure of time-varying graphs. In this work, we develop a framework for
learning the latent state space of a time-varying email graph. We show how the
framework can be used to find subsequences that correspond to global real-time
events in the Email graph (e.g. vacations, breaks, ...etc.). These events
impact the underlying graph process to make its characteristics non-stationary.
Within the framework, we compare two different representations of the temporal
relationships; discrete vs. probabilistic. We use the two representations as
inputs to a mixture model to learn the latent state transitions that correspond
to important changes in the Email graph structure over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3710</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3710</id><created>2014-03-14</created><authors><author><keyname>Hoque</keyname><forenames>Mohammad Ashraful</forenames></author><author><keyname>Siekkinen</keyname><forenames>Matti</forenames></author><author><keyname>Nurminen</keyname><forenames>Jukka K.</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author><author><keyname>Aalto</keyname><forenames>Mika</forenames></author></authors><title>Saving Energy in Mobile Devices for On-Demand Multimedia Streaming -- A
  Cross-Layer Approach</title><categories>cs.MM cs.IT math.IT</categories><comments>Accepted in ACM Transactions on Multimedia Computing, Communications
  and Applications (ACM TOMCCAP), November 2013</comments><acm-class>C.2.1; C.2.4; C.4</acm-class><doi>10.1145/2556942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel energy-efficient multimedia delivery system
called EStreamer. First, we study the relationship between buffer size at the
client, burst-shaped TCP-based multimedia traffic, and energy consumption of
wireless network interfaces in smartphones. Based on the study, we design and
implement EStreamer for constant bit rate and rate-adaptive streaming.
EStreamer can improve battery lifetime by 3x, 1.5x and 2x while streaming over
Wi-Fi, 3G and 4G respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3715</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3715</id><created>2014-03-14</created><authors><author><keyname>Zhang</keyname><forenames>Thomas T. C. K.</forenames></author><author><keyname>Carlsson</keyname><forenames>John Gunnar</forenames></author></authors><title>On the continuous Fermat-Weber problem for a convex polygon using
  Euclidean distance</title><categories>cs.CG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the continuous Fermat-Weber problem, where the customers are
continuously (uniformly) distributed along the boundary of a convex polygon. We
derive the closed-form expression for finding the average distance from a given
point to the continuously distributed customers along the boundary. A
Weiszfeld-type procedure is proposed for this model, which is shown to be
linearly convergent. We also derive a closed-form formula to find the average
distance for a given point to the entire convex polygon, assuming a uniform
distribution. Since the function is smooth, convex, and explicitly given, the
continuous version of the Fermat-Weber problem over a convex polygon can be
solved easily by numerical algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3724</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3724</id><created>2014-03-14</created><updated>2015-09-07</updated><authors><author><keyname>Roncal</keyname><forenames>William Gray</forenames></author><author><keyname>Pekala</keyname><forenames>Michael</forenames></author><author><keyname>Kaynig-Fittkau</keyname><forenames>Verena</forenames></author><author><keyname>Kleissas</keyname><forenames>Dean M.</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Pfister</keyname><forenames>Hanspeter</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author><author><keyname>Chevillet</keyname><forenames>Mark A.</forenames></author><author><keyname>Hager</keyname><forenames>Gregory D.</forenames></author></authors><title>VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer
  vision at Large Scale</title><categories>cs.CV cs.CE q-bio.QM</categories><comments>v4: added clarifying figures and updates for readability. v3: fixed
  metadata. 11 pp v2: Added CNN classifier, significant changes to improve
  performance and generalization</comments><journal-ref>Proceedings of the British Machine Vision Conference (BMVC), pages
  81.1-81.13. BMVA Press, September 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open challenge problem at the forefront of modern neuroscience is to
obtain a comprehensive mapping of the neural pathways that underlie human brain
function; an enhanced understanding of the wiring diagram of the brain promises
to lead to new breakthroughs in diagnosing and treating neurological disorders.
Inferring brain structure from image data, such as that obtained via electron
microscopy (EM), entails solving the problem of identifying biological
structures in large data volumes. Synapses, which are a key communication
structure in the brain, are particularly difficult to detect due to their small
size and limited contrast. Prior work in automated synapse detection has relied
upon time-intensive biological preparations (post-staining, isotropic slice
thicknesses) in order to simplify the problem.
  This paper presents VESICLE, the first known approach designed for mammalian
synapse detection in anisotropic, non-post-stained data. Our methods explicitly
leverage biological context, and the results exceed existing synapse detection
methods in terms of accuracy and scalability. We provide two different
approaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight
Random Forest approach (VESICLE-RF) to offer alternatives in the
performance-scalability space. Addressing this synapse detection challenge
enables the analysis of high-throughput imaging data soon expected to reach
petabytes of data, and provide tools for more rapid estimation of brain-graphs.
Finally, to facilitate community efforts, we developed tools for large-scale
object detection, and demonstrated this framework to find $\approx$ 50,000
synapses in 60,000 $\mu m ^3$ (220 GB on disk) of electron microscopy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3740</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3740</id><created>2014-03-14</created><authors><author><keyname>Rao</keyname><forenames>Xiongbin</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Interference Alignment with Partial CSI Feedback in MIMO Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>25 pages, 4 figures</comments><doi>10.1109/TSP.2014.2307283</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is a linear precoding strategy that can achieve
optimal capacity scaling at high SNR in interference networks. However, most
existing IA designs require full channel state information (CSI) at the
transmitters, which would lead to significant CSI signaling overhead. There are
two techniques, namely CSI quantization and CSI feedback filtering, to reduce
the CSI feedback overhead. In this paper, we consider IA processing with CSI
feedback filtering in MIMO cellular networks. We introduce a novel metric,
namely the feedback dimension, to quantify the first order CSI feedback cost
associated with the CSI feedback filtering. The CSI feedback filtering poses
several important challenges in IA processing. First, there is a hidden partial
CSI knowledge constraint in IA precoder design which cannot be handled using
conventional IA design methodology. Furthermore, existing results on the
feasibility conditions of IA cannot be applied due to the partial CSI
knowledge. Finally, it is very challenging to find out how much CSI feedback is
actually needed to support IA processing. We shall address the above challenges
and propose a new IA feasibility condition under partial CSIT knowledge in MIMO
cellular networks. Based on this, we consider the CSI feedback profile design
subject to the degrees of freedom requirements, and we derive closed-form
trade-off results between the CSI feedback cost and IA performance in MIMO
cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3741</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3741</id><created>2014-03-14</created><updated>2014-10-31</updated><authors><author><keyname>Osband</keyname><forenames>Ian</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Near-optimal Reinforcement Learning in Factored MDPs</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any reinforcement learning algorithm that applies to all Markov decision
processes (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP, where $T$
is the elapsed time and $S$ and $A$ are the cardinalities of the state and
action spaces. This implies $T = \Omega(SA)$ time to guarantee a near-optimal
policy. In many settings of practical interest, due to the curse of
dimensionality, $S$ and $A$ can be so enormous that this learning time is
unacceptable. We establish that, if the system is known to be a \emph{factored}
MDP, it is possible to achieve regret that scales polynomially in the number of
\emph{parameters} encoding the factored MDP, which may be exponentially smaller
than $S$ or $A$. We provide two algorithms that satisfy near-optimal regret
bounds in this context: posterior sampling reinforcement learning (PSRL) and an
upper confidence bound algorithm (UCRL-Factored).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3752</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3752</id><created>2014-03-15</created><authors><author><keyname>Wood</keyname><forenames>Gavin</forenames></author></authors><title>Martta: A C++ Language Workbench</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language-orientated programming promises to elevate programmer productivity
through increased abstrac- tion capabilities. Structural programming
environments provide apparatus to reduce the difficulties with syntax. The
language workbench, a conceptual combination of these two approaches, is a
comparatively novel approach to software development and has so far been
attempted only in dynamic-dispatch, run-time-compiled languages (e.g. Java).
However, it must be remembered that several fields of engineering exist, each
having their own priorities. In the video games industry, where large, complex
and diverse projects are routinely developed, efficiency is paramount and as
such C++, as a development platform, is widely used. I explore the possibility
of a language workbench capable of a gradual transition in both skills and code
from the traditional C++ development environment. This article is the design
for a language workbench. It uses novel techniques including a
context-sensitive event- driven input system and a hybrid
single/multiple-inherited class model and through a prototype implementation
demon- strates that is both concise and practical for C++. I refute the
hitherto implicit hypothesis that the language workbench paradigm is not
applicable to the C++ language, showing that C++ can be used for creating an
effective development framework usable in otherwise pure-C++ programming
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3758</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3758</id><created>2014-03-15</created><updated>2014-05-21</updated><authors><author><keyname>Besse</keyname><forenames>Philippe</forenames><affiliation>IMT</affiliation></author><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>IMT</affiliation></author><author><keyname>Loubes</keyname><forenames>Jean-Michel</forenames><affiliation>IMT</affiliation></author></authors><title>Big Data Analytics - Retour vers le Futur 3; De Statisticien \`a Data
  Scientist</title><categories>math.ST cs.DB stat.TH</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid evolution of information systems managing more and more voluminous
data has caused profound paradigm shifts in the job of statistician, becoming
successively data miner, bioinformatician and now data scientist. Without the
sake of completeness and after having illustrated these successive mutations,
this article briefly introduced the new research issues that quickly rise in
Statistics, and more generally in Mathematics, in order to integrate the
characteristics: volume, variety and velocity, of big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3759</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3759</id><created>2014-03-15</created><updated>2014-03-26</updated><authors><author><keyname>Wang</keyname><forenames>Guohui</forenames></author><author><keyname>Shen</keyname><forenames>Hao</forenames></author><author><keyname>Sun</keyname><forenames>Yang</forenames></author><author><keyname>Cavallaro</keyname><forenames>Joseph R.</forenames></author><author><keyname>Vosoughi</keyname><forenames>Aida</forenames></author><author><keyname>Guo</keyname><forenames>Yuanbin</forenames></author></authors><title>Parallel Interleaver Design for a High Throughput HSPA+/LTE
  Multi-Standard Turbo Decoder</title><categories>cs.IT cs.AR cs.DC math.IT</categories><comments>14 pages, 15 figures. Accepted for publication by IEEE Transactions
  on Circuits and Systems I: Regular Papers</comments><acm-class>C.2.1; B.7.1; E.4; C.1.4; C.1.2</acm-class><doi>10.1109/TCSI.2014.2309810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the evolving data rate requirements of emerging wireless
communication technologies, many parallel architectures have been proposed to
implement high throughput turbo decoders. However, concurrent memory
reading/writing in parallel turbo decoding architectures leads to severe memory
conflict problem, which has become a major bottleneck for high throughput turbo
decoders. In this paper, we propose a flexible and efficient VLSI architecture
to solve the memory conflict problem for highly parallel turbo decoders
targeting multi-standard 3G/4G wireless communication systems. To demonstrate
the effectiveness of the proposed parallel interleaver architecture, we
implemented an HSPA+/LTE/LTE-Advanced multi-standard turbo decoder with a 45nm
CMOS technology. The implemented turbo decoder consists of 16 Radix-4 MAP
decoder cores, and the chip core area is 2.43 mm^2. When clocked at 600 MHz,
this turbo decoder can achieve a maximum decoding throughput of 826 Mbps in the
HSPA+ mode and 1.67 Gbps in the LTE/LTE-Advanced mode, exceeding the peak data
rate requirements for both standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3772</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3772</id><created>2014-03-15</created><authors><author><keyname>Fouquer&#xe9;</keyname><forenames>Christophe</forenames></author><author><keyname>Quatrini</keyname><forenames>Myriam</forenames></author></authors><title>Ludics Characterization of Multiplicative-Additive Linear Behaviours</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ludics is a logical theory that J.-Y. Girard developed around $2000$. At
first glance, it may be considered as a Brouwer-Heyting-Kolmogorov
interpretation of Logic as a formula is denoted by the set of its proofs. More
primitively, Ludics is a theory of interaction that models (a variant of)
second-order multiplicative-additive Linear Logic. A formula is denoted by a
set of objects called a behaviour, a proof by an object that satisfies some
criteria. Our aim is to analyze the structure of behaviours in order to better
understand and refine the usual notion of formulas or types. More precisely, we
study properties that guarantee a behaviour to be recursively decomposable by
means of multiplicative-additive linear connectives and linear constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3779</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3779</id><created>2014-03-15</created><authors><author><keyname>Souiki</keyname><forenames>Sihem</forenames></author><author><keyname>Feham</keyname><forenames>Maghnia</forenames></author><author><keyname>Feham</keyname><forenames>Mohamed</forenames></author><author><keyname>Labraoui</keyname><forenames>Nabila</forenames></author></authors><title>Geographic routing protocols for underwater wireless sensor networks:a
  survey</title><categories>cs.NI</categories><comments>19 pages, IJWMN journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Underwater wireless sensor networks (UWSN), similar to the terrestrial sensor
networks, have different challenges such as limited bandwidth, low battery
power, defective underwater channels, and high variable propagation delay. A
crucial problem in UWSN is finding an efficient route between a source and a
destination. Consequently, great efforts have been made for designing efficient
protocols while considering the unique characteristics of underwater
communication. Several routing protocols are proposed for this issue and can be
classified into geographic and non-geographic routing protocols. In this paper
we focus on the geographic routing protocols. We introduce a review and
comparison of different algorithms proposed recently in the literature. We also
presented a novel taxonomy of these routing in which the protocols are
classified into three categories (greedy, restricted directional flooding and
hierarchical) according to their forwarding strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3780</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3780</id><created>2014-03-15</created><authors><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Wong</keyname><forenames>Yongkang</forenames></author><author><keyname>Hobson</keyname><forenames>Peter</forenames></author><author><keyname>Minchin</keyname><forenames>Rodney F.</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Automatic Classification of Human Epithelial Type 2 Cell Indirect
  Immunofluorescence Images using Cell Pyramid Matching</title><categories>q-bio.CB cs.CV q-bio.QM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.1262</comments><acm-class>J.3; I.4.7; I.4.9; I.5.1; I.5.4; G.3</acm-class><journal-ref>Pattern Recognition, Vol. 47, No. 7, pp. 2315-2324, 2014</journal-ref><doi>10.1016/j.patcog.2013.10.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel system for automatic classification of images
obtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelial
type 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. The
IIF protocol on HEp-2 cells has been the hallmark method to identify the
presence of ANAs, due to its high sensitivity and the large range of antigens
that can be detected. However, it suffers from numerous shortcomings, such as
being subjective as well as time and labour intensive. Computer Aided
Diagnostic (CAD) systems have been developed to address these problems, which
automatically classify a HEp-2 cell image into one of its known patterns (eg.
speckled, homogeneous). Most of the existing CAD systems use handpicked
features to represent a HEp-2 cell image, which may only work in limited
scenarios. We propose a novel automatic cell image classification method termed
Cell Pyramid Matching (CPM), which is comprised of regional histograms of
visual words coupled with the Multiple Kernel Learning framework. We present a
study of several variations of generating histograms and show the efficacy of
the system on two publicly available datasets: the ICPR HEp-2 cell
classification contest dataset and the SNPHEp-2 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3785</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3785</id><created>2014-03-15</created><authors><author><keyname>Li</keyname><forenames>Ming-Xia</forenames></author><author><keyname>Palchykov</keyname><forenames>Vasyl</forenames></author><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>Janos</forenames></author><author><keyname>Miccich&#xe8;</keyname><forenames>Salvatore</forenames></author><author><keyname>Tumminello</keyname><forenames>Michele</forenames></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames></author><author><keyname>Mantegna</keyname><forenames>Rosario N.</forenames></author></authors><title>Statistically validated mobile communication networks: Evolution of
  motifs in European and Chinese data</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages, 8 figures, 5 tables</comments><journal-ref>New J. Phys. 16 (2014) 083038</journal-ref><doi>10.1088/1367-2630/16/8/083038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data open up unprecedented opportunities to investigate complex systems
including the society. In particular, communication data serve as major sources
for computational social sciences but they have to be cleaned and filtered as
they may contain spurious information due to recording errors as well as
interactions, like commercial and marketing activities, not directly related to
the social network. The network constructed from communication data can only be
considered as a proxy for the network of social relationships. Here we apply a
systematic method, based on multiple hypothesis testing, to statistically
validate the links and then construct the corresponding Bonferroni network,
generalized to the directed case. We study two large datasets of mobile phone
records, one from Europe and the other from China. For both datasets we compare
the raw data networks with the corresponding Bonferroni networks and point out
significant differences in the structures and in the basic network measures. We
show evidence that the Bonferroni network provides a better proxy for the
network of social interactions than the original one. By using the filtered
networks we investigated the statistics and temporal evolution of small
directed 3-motifs and conclude that closed communication triads have a
formation time-scale, which is quite fast and typically intraday. We also find
that open communication triads preferentially evolve to other open triads with
a higher fraction of reciprocated calls. These stylized facts were observed for
both datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3786</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3786</id><created>2014-03-15</created><authors><author><keyname>Huleihel</keyname><forenames>Wasim</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Universal Decoding for Gaussian Intersymbol Interference Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A universal decoding procedure is proposed for the intersymbol interference
(ISI) Gaussian channels. The universality of the proposed decoder is in the
sense of being independent of the various channel parameters, and at the same
time, attaining the same random coding error exponent as the optimal
maximum-likelihood (ML) decoder, which utilizes full knowledge of these unknown
parameters. The proposed decoding rule can be regarded as a frequency domain
version of the universal maximum mutual information (MMI) decoder. Contrary to
previously suggested universal decoders for ISI channels, our proposed decoding
metric can easily be evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3788</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3788</id><created>2014-03-15</created><updated>2015-04-15</updated><authors><author><keyname>Siriteanu</keyname><forenames>Constantin</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Kuriki</keyname><forenames>Satoshi</forenames></author><author><keyname>Shin</keyname><forenames>Hyundong</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>MIMO Zero-Forcing Performance Evaluation Using the Holonomic Gradient
  Method</title><categories>cs.IT math.IT</categories><comments>This manuscript was accepted in December 2014</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 14, no. 4,
  April 2015, pp. 2322-2335</journal-ref><doi>10.1109/TWC.2014.2385075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For multiple-input multiple-output (MIMO) spatial-multiplexing transmission,
zero-forcing detection (ZF) is appealing because of its low complexity. Our
recent MIMO ZF performance analysis for Rician--Rayleigh fading, which is
relevant in heterogeneous networks, has yielded for the ZF outage probability
and ergodic capacity infinite-series expressions. Because they arose from
expanding the confluent hypergeometric function $ {_1\! F_1} (\cdot, \cdot,
\sigma) $ around 0, they do not converge numerically at realistically-high
Rician $ K $-factor values. Therefore, herein, we seek to take advantage of the
fact that $ {_1\! F_1} (\cdot, \cdot, \sigma) $ satisfies a differential
equation, i.e., it is a \textit{holonomic} function. Holonomic functions can be
computed by the \textit{holonomic gradient method} (HGM), i.e., by numerically
solving the satisfied differential equation. Thus, we first reveal that the
moment generating function (m.g.f.) and probability density function (p.d.f.)
of the ZF signal-to-noise ratio (SNR) are holonomic. Then, from the
differential equation for $ {_1\! F_1} (\cdot, \cdot, \sigma) $, we deduce
those satisfied by the SNR m.g.f. and p.d.f., and demonstrate that the HGM
helps compute the p.d.f. accurately at practically-relevant values of $ K $.
Finally, numerical integration of the SNR p.d.f. produced by HGM yields
accurate ZF outage probability and ergodic capacity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3795</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3795</id><created>2014-03-15</created><updated>2014-10-08</updated><authors><author><keyname>Jeub</keyname><forenames>Lucas G. S.</forenames></author><author><keyname>Balachandran</keyname><forenames>Prakash</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Think Locally, Act Locally: The Detection of Small, Medium-Sized, and
  Large Communities in Large Networks</title><categories>cs.SI cond-mat.dis-nn math.CO nlin.AO physics.soc-ph</categories><comments>32 pages, 19 figures (many with multiple parts); the abstract is
  abridged because of space limitations in the arXiv's abstract field</comments><doi>10.1103/PhysRevE.91.012821</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common in the study of networks to investigate meso-scale features to
try to gain an understanding of network structure and function. For example,
numerous algorithms have been developed to try to identify &quot;communities,&quot; which
are typically construed as sets of nodes with denser connections internally
than with the remainder of a network. In this paper, we adopt a complementary
perspective that &quot;communities&quot; are associated with bottlenecks of
locally-biased dynamical processes that begin at seed sets of nodes, and we
employ several different community-identification procedures (using
diffusion-based and geodesic-based dynamics) to investigate community quality
as a function of community size. Using several empirical and synthetic
networks, we identify several distinct scenarios for ``size-resolved community
structure'' that can arise in real (and realistic) networks. Depending on which
scenario holds, one may or may not be able to successfully identify ``good''
communities in a given network, the manner in which different small communities
fit together to form meso-scale network structures can be very different, and
processes such as viral propagation and information diffusion can exhibit very
different dynamics.In addition, our results suggest that, for many large
realistic networks, the output of locally-biased methods that focus on
communities that are centered around a given seed node might have better
conceptual grounding and greater practical utility than the output of global
community-detection methods. They also illustrate subtler structural properties
that are important to consider in the development of better benchmark networks
to test methods for community detection.
  [Note: Because of space limitations in the arXiv's abstract field, this is an
abridged version of the paper's abstract.]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3807</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3807</id><created>2014-03-15</created><updated>2014-08-28</updated><authors><author><keyname>Hao</keyname><forenames>Bibo</forenames></author><author><keyname>Li</keyname><forenames>Lin</forenames></author><author><keyname>Gao</keyname><forenames>Rui</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Sensing Subjective Well-being from Social Media</title><categories>cs.AI cs.CY</categories><comments>12 pages, 1 figures, 2 tables, 10th International Conference, AMT
  2014, Warsaw, Poland, August 11-14, 2014. Proceedings</comments><doi>10.1007/978-3-319-09912-5_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subjective Well-being(SWB), which refers to how people experience the quality
of their lives, is of great use to public policy-makers as well as economic,
sociological research, etc. Traditionally, the measurement of SWB relies on
time-consuming and costly self-report questionnaires. Nowadays, people are
motivated to share their experiences and feelings on social media, so we
propose to sense SWB from the vast user generated data on social media. By
utilizing 1785 users' social media data with SWB labels, we train machine
learning models that are able to &quot;sense&quot; individual SWB from users' social
media. Our model, which attains the state-by-art prediction accuracy, can then
be used to identify SWB of large population of social media users in time with
very low cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3818</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3818</id><created>2014-03-15</created><authors><author><keyname>Boujelben</keyname><forenames>Maissa</forenames></author><author><keyname>Benrejeb</keyname><forenames>Sonia</forenames></author><author><keyname>Tabbane</keyname><forenames>Sami</forenames></author></authors><title>Interference Coordination Schemes for Wireless Mobile Advanced Systems:
  A Survey</title><categories>cs.NI</categories><comments>11 pages, 14 figures. e-ISSN: 2278-2834,p- ISSN: 2278-8735.Volume 9,
  Issue 1, Ver. I (Feb. 2014), PP 00-00</comments><msc-class>94-02</msc-class><doi>10.9790/2834-09168090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile communication networks have witnessed a perpetual evolution since
their launching as voice only networks. In the few past years, the focus was
addressed to high data rate networks that offer high quality of service. The
lately released LTE-Advanced network was the first to completely fulfill 4G
requirements. However, performance gains remain limited due to severe
interference levels. This paper is a survey upon the evolution of interference
mitigation solutions from Release 8 to Release 11. This problem was addressed
since earlier releases by coordinating transmission/reception among different
cells. Many enhancements were then carried out by each subsequent
specification. The originality of this work resides on the comparison between
the different schemes and the perspective of developing a new interference
coordination method available for 4G and beyond systems with small cells
deployment. Several similarities are noted and differences on performance
impact are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3829</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3829</id><created>2014-03-15</created><authors><author><keyname>Wang</keyname><forenames>Zixuan</forenames></author><author><keyname>Di</keyname><forenames>Wei</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Anurag</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author></authors><title>Geometric VLAD for Large Scale Image Search</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a novel compact image descriptor for large scale image search. Our
proposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector of
Locally Aggregated Descriptors) that incorporates weak geometry information
into the VLAD framework. The proposed geometry cues are derived as a membership
function over keypoint angles which contain evident and informative information
but yet often discarded. A principled technique for learning the membership
function by clustering angles is also presented. Further, to address the
overhead of iterative codebook training over real-time datasets, a novel
codebook adaptation strategy is outlined. Finally, we demonstrate the efficacy
of proposed gVLAD based retrieval framework where we achieve more than 15%
improvement in mAP over existing benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3841</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3841</id><created>2014-03-15</created><authors><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author></authors><title>Doubles and Negatives are Positive (in Self-Assembly)</title><categories>cs.ET cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the abstract Tile Assembly Model (aTAM), the phenomenon of cooperation
occurs when the attachment of a new tile to a growing assembly requires it to
bind to more than one tile already in the assembly. Often referred to as
``temperature-2'' systems, those which employ cooperation are known to be quite
powerful (i.e. they are computationally universal and can build an enormous
variety of shapes and structures). Conversely, aTAM systems which do not
enforce cooperative behavior, a.k.a. ``temperature-1'' systems, are conjectured
to be relatively very weak, likely to be unable to perform complex computations
or algorithmically direct the process of self-assembly. Nonetheless, a variety
of models based on slight modifications to the aTAM have been developed in
which temperature-1 systems are in fact capable of Turing universal computation
through a restricted notion of cooperation. Despite that power, though, several
of those models have previously been proven to be unable to perform or simulate
the stronger form of cooperation exhibited by temperature-2 aTAM systems.
  In this paper, we first prove that another model in which temperature-1
systems are computationally universal, namely the restricted glue TAM (rgTAM)
in which tiles are allowed to have edges which exhibit repulsive forces, is
also unable to simulate the strongly cooperative behavior of the temperature-2
aTAM. We then show that by combining the properties of two such models, the
Dupled Tile Assembly Model (DTAM) and the rgTAM into the DrgTAM, we derive a
model which is actually more powerful at temperature-1 than the aTAM at
temperature-2. Specifically, the DrgTAM, at temperature-1, can simulate any
aTAM system of any temperature, and it also contains systems which cannot be
simulated by any system in the aTAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3864</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3864</id><created>2014-03-15</created><authors><author><keyname>Marzen</keyname><forenames>Sarah</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Information Anatomy of Stochastic Equilibria</title><categories>cond-mat.stat-mech cs.IT math.DS math.IT nlin.CD</categories><comments>35 pages, 3 figures, 1 table;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/iase.htm</comments><doi>10.3390/e16094713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic nonlinear dynamical system generates information, as measured by
its entropy rate. Some---the ephemeral information---is dissipated and
some---the bound information---is actively stored and so affects future
behavior. We derive analytic expressions for the ephemeral and bound
informations in the limit of small-time discretization for two classical
systems that exhibit dynamical equilibria: first-order Langevin equations (i)
where the drift is the gradient of a potential function and the diffusion
matrix is invertible and (ii) with a linear drift term (Ornstein-Uhlenbeck) but
a noninvertible diffusion matrix. In both cases, the bound information is
sensitive only to the drift, while the ephemeral information is sensitive only
to the diffusion matrix and not to the drift. Notably, this information anatomy
changes discontinuously as any of the diffusion coefficients vanishes,
indicating that it is very sensitive to the noise structure. We then calculate
the information anatomy of the stochastic cusp catastrophe and of particles
diffusing in a heat bath in the overdamped limit, both examples of stochastic
gradient descent on a potential landscape. Finally, we use our methods to
calculate and compare approximations for the so-called time-local predictive
information for adaptive agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3867</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3867</id><created>2014-03-15</created><updated>2015-02-21</updated><authors><author><keyname>Valculescu</keyname><forenames>Claudiu</forenames></author><author><keyname>de Zeeuw</keyname><forenames>Frank</forenames></author></authors><title>Distinct values of bilinear forms on algebraic curves</title><categories>math.MG cs.CG math.CO</categories><comments>v3: Minor corrections, and title change</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $B$ be a bilinear form on pairs of points in the complex plane, of the
form $B(p,q) = p^TMq$, for an invertible $2\times2$ complex matrix $M$. We
prove that any finite set $S$ contained in an irreducible algebraic curve $C$
of degree $d$ in $\mathbb{C}^2$ determines at least $c_d|S|^{4/3}$ distinct
values of $B$, unless the curve $C$ has an exceptional form. This strengthens a
result of Charalambides in several ways. The proof is based on that of Pach and
De Zeeuw, who proved a similar statement for the Euclidean distance function in
the real plane. Our main motivation for this paper is that for bilinear forms,
this approach becomes more natural, and should better lend itself to
understanding and generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3881</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3881</id><created>2014-03-16</created><updated>2015-06-07</updated><authors><author><keyname>Etesami</keyname><forenames>Seyed Rasoul</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Complexity of Equilibrium in Diffusion Games on Social Networks</title><categories>cs.GT cs.CC cs.DC cs.DM cs.MA</categories><comments>A shorter version of this paper has been appeared in 2014 American
  Control Conference (ACC2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the competitive diffusion game, and study the
existence of its pure-strategy Nash equilibrium when defined over general
undirected networks. We first determine the set of pure-strategy Nash
equilibria for two special but well-known classes of networks, namely the
lattice and the hypercube. Characterizing the utility of the players in terms
of graphical distances of their initial seed placements to other nodes in the
network, we show that in general networks the decision process on the existence
of pure-strategy Nash equilibrium is an NP-hard problem. Following this, we
provide some necessary conditions for a given profile to be a Nash equilibrium.
Furthermore, we study players' utilities in the competitive diffusion game over
Erdos-Renyi random graphs and show that as the size of the network grows, the
utilities of the players are highly concentrated around their expectation, and
are bounded below by some threshold based on the parameters of the network.
Finally, we obtain a lower bound for the maximum social welfare of the game
with two players, and study sub-modularity of the players' utilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3885</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3885</id><created>2014-03-16</created><updated>2015-11-06</updated><authors><author><keyname>Panageas</keyname><forenames>Ioannis</forenames></author><author><keyname>Piliouras</keyname><forenames>Georgios</forenames></author></authors><title>Approximating the Geometry of Dynamics in Potential Games: Point-wise
  Convergence, Regions of Attraction, Average Case Performance Analysis and
  System Invariants</title><categories>cs.GT math.DS</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What does it mean to fully understand the behavior of a network of competing
agents? The golden standard typically is the behavior of learning dynamics in
potential games, where many evolutionary dynamics, e.g., replicator, are known
to converge to sets of equilibria. Even in such classic settings many critical
questions remain unanswered. Inspired by topological and geometric
considerations, we devise novel yardsticks and techniques that allow for much
more detailed analysis of game dynamics and network computation more generally.
We address issues such as:
  Point-wise convergence: Does the system actually equilibrate even in the
presence of continuums of equilibria? Average case analysis: How does the
system behave given a uniformly random initial condition (or more generally an
initial condition chosen from a given distribution)? How does this &quot;average
case&quot; performance compare against standard measures of efficiency such as worst
case (price of anarchy) and best case analysis (price of stability)? Computing
regions of attraction: Given point-wise convergence can we compute the region
of asymptotic stability of each equilibrium (e.g., estimate its volume,
geometry)? System invariants: A system invariant is a function defined over the
system state space such that it remains constant along every system trajectory.
In some sense, this is a notion that is orthogonal to the classic game
theoretic concept of a potential function, which always strictly
increases/decreases along system trajectories. Do dynamics in a potential game
exhibit (besides the standard potential function) non-trivial invariant
functions as well? If so, how many? How do these functions look like?
  All the above issues prove to be tightly coupled to each other and open
possibilities for a deeper understanding of many settings of networked,
decentralized computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3891</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3891</id><created>2014-03-16</created><updated>2015-05-01</updated><authors><author><keyname>Kim</keyname><forenames>Dong Min</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Exploiting Regional Differences: A Spatially Adaptive Random Access</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages, 10 figures</comments><doi>10.1109/TWC.2015.2419633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the potential for improvement of the simple random
access scheme by utilizing local information such as the received
signal-to-interference-plus-noise-ratio (SINR). We propose a spatially adaptive
random access (SARA) scheme in which the transmitters in the network utilize
different transmit probabilities depending on the local situation. In our
proposed scheme, the transmit probability is adaptively updated by the ratio of
the received SINR and the target SINR. We investigate the performance of the
spatially adaptive random access scheme. For the comparison, we derive an
optimal transmit probability of ALOHA random access scheme in which all
transmitters use the same transmit probability. We illustrate the performance
of the spatially adaptive random access scheme through simulations. We show
that the performance of the proposed scheme surpasses that of the optimal ALOHA
random access scheme and is comparable with the CSMA/CA scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3894</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3894</id><created>2014-03-16</created><updated>2014-04-10</updated><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author><author><keyname>Moriyama</keyname><forenames>Sonoko</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>A lower bound on opaque sets</title><categories>cs.CG</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that the total length of any set of countably many rectifiable
curves, whose union meets all straight lines that intersect the unit square U,
is at least 2.00002. This is the first improvement on the lower bound of 2
established by Jones in 1964. A similar bound is proved for all convex sets U
other than a triangle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3905</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3905</id><created>2014-03-16</created><authors><author><keyname>Bungiu</keyname><forenames>Francisc</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Hershberger</keyname><forenames>John</forenames></author><author><keyname>Huang</keyname><forenames>Kan</forenames></author><author><keyname>Kr&#xf6;ller</keyname><forenames>Alexander</forenames></author></authors><title>Efficient Computation of Visibility Polygons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining visibility in planar polygons and arrangements is an important
subroutine for many algorithms in computational geometry. In this paper, we
report on new implementations, and corresponding experimental evaluations, for
two established and one novel algorithm for computing visibility polygons.
These algorithms will be released to the public shortly, as a new package for
the Computational Geometry Algorithms Library (CGAL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3907</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3907</id><created>2014-03-16</created><updated>2014-11-26</updated><authors><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Khonji</keyname><forenames>Majid</forenames></author></authors><title>Truthful Mechanisms for Combinatorial AC Electric Power Allocation</title><categories>cs.GT cs.DS</categories><comments>Appears in: Proceedings of the 13th International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2014). With an updated
  abstract and a correction in the proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional studies of combinatorial auctions often only consider linear
constraints (by which the demands for certain goods are limited by the
corresponding supplies). The rise of smart grid presents a new class of
auctions, characterized by quadratic constraints. Yu and Chau [AAMAS 13']
introduced the complex-demand knapsack problem, in which the demands are
complex-valued and the capacity of supplies is described by the magnitude of
total complex-valued demand. This naturally captures the power constraints in
AC electric systems. In this paper, we provide a more complete study and
generalize the problem to the multi-minded version, beyond the previously known
1/2-approximation algorithm for only a subclass of the problem. More precisely,
we give a truthful PTAS for the case phi in [0,pi/2-delta], and a truthful
FPTAS, which fully optimizes the objective function but violates the capacity
constraint by at most (1+epsilon), for the case phi in (pi/2,pi-delta], where
phi is the maximum angle between any two complex-valued demands and
epsilon,delta&gt;0 are arbitrarily small constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3909</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3909</id><created>2014-03-16</created><authors><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author><author><keyname>Duffield</keyname><forenames>Nick</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author><author><keyname>Kompella</keyname><forenames>Ramana</forenames></author></authors><title>Graph Sample and Hold: A Framework for Big-Graph Analytics</title><categories>cs.SI cs.DB physics.soc-ph stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling is a standard approach in big-graph analytics; the goal is to
efficiently estimate the graph properties by consulting a sample of the whole
population. A perfect sample is assumed to mirror every property of the whole
population. Unfortunately, such a perfect sample is hard to collect in complex
populations such as graphs (e.g. web graphs, social networks etc), where an
underlying network connects the units of the population. Therefore, a good
sample will be representative in the sense that graph properties of interest
can be estimated with a known degree of accuracy. While previous work focused
particularly on sampling schemes used to estimate certain graph properties
(e.g. triangle count), much less is known for the case when we need to estimate
various graph properties with the same sampling scheme. In this paper, we
propose a generic stream sampling framework for big-graph analytics, called
Graph Sample and Hold (gSH). To begin, the proposed framework samples from
massive graphs sequentially in a single pass, one edge at a time, while
maintaining a small state. We then show how to produce unbiased estimators for
various graph properties from the sample. Given that the graph analysis
algorithms will run on a sample instead of the whole population, the runtime
complexity of these algorithm is kept under control. Moreover, given that the
estimators of graph properties are unbiased, the approximation error is kept
under control. Finally, we show the performance of the proposed framework (gSH)
on various types of graphs, such as social graphs, among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3928</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3928</id><created>2014-03-16</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author></authors><title>A Bootstrap Theory: the SEMAT Kernel Itself as Runnable Software</title><categories>cs.SE</categories><comments>8 pages; 2 figures; Preprint of paper accepted for GTSE'2014
  Workshop, within ICSE'2014 Conference</comments><acm-class>D.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SEMAT kernel is a thoroughly thought generic framework for Software
Engineering system development in practice. But one should be able to test its
characteristics by means of a no less generic theory matching the SEMAT kernel.
This paper claims that such a matching theory is attainable and describes its
main principles. The conceptual starting point is the robustness of the Kernel
alphas to variations in the nature of the software system, viz. to software
automation, distribution and self-evolution. From these and from observed
Kernel properties follows the proposed bootstrap principle: a software system
theory should itself be a runnable software. Thus, the kernel alphas can be
viewed as a top-level ontology, indeed the Essence of Software Engineering.
Among the interesting consequences of this bootstrap theory, the observable
system characteristics can now be formally tested. For instance, one can check
the system completeness, viz. that software system modules fulfill each one of
the system requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3931</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3931</id><created>2014-03-16</created><authors><author><keyname>Zhang</keyname><forenames>Hongzhong</forenames></author><author><keyname>Hadjiliadis</keyname><forenames>Olympia</forenames></author><author><keyname>Sch&#xe4;fer</keyname><forenames>Tobias</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Quickest detection in coupled systems</title><categories>math.OC cs.IT math.IT math.ST stat.TH</categories><comments>29 pages. SIAM Journal on Control and Optimization, forthcoming</comments><report-no>SICON2014</report-no><msc-class>62L10, 62L15, 62C20, 60G40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the problem of quickest detection of signals in a coupled
system of $N$ sensors, which receive continuous sequential observations from
the environment. It is assumed that the signals, which are modeled by general
It\^{o} processes, are coupled across sensors, but that their onset times may
differ from sensor to sensor. Two main cases are considered; in the first one
signal strengths are the same across sensors while in the second one they
differ by a constant. The objective is the optimal detection of the first time
at which any sensor in the system receives a signal. The problem is formulated
as a stochastic optimization problem in which an extended minimal
Kullback-Leibler divergence criterion is used as a measure of detection delay,
with a constraint on the mean time to the first false alarm. The case in which
the sensors employ cumulative sum (CUSUM) strategies is considered, and it is
proved that the minimum of $N$ CUSUMs is asymptotically optimal as the mean
time to the first false alarm increases without bound. In particular, in the
case of equal signal strengths across sensors, it is seen that the difference
in detection delay of the $N$-CUSUM stopping rule and the unknown optimal
stopping scheme tends to a constant related to the number of sensors as the
mean time to the first false alarm increases without bound. Alternatively, in
the case of unequal signal strengths, it is seen that this difference tends to
zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3948</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3948</id><created>2014-03-16</created><authors><author><keyname>Al-Maolegi</keyname><forenames>Mohammed</forenames></author><author><keyname>Arkok</keyname><forenames>Bassam</forenames></author></authors><title>An Improved Apriori Algorithm for Association Rules</title><categories>cs.DB</categories><comments>9 pages, 3 figures, 8 tables, journal</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are several mining algorithms of association rules. One of the most
popular algorithms is Apriori that is used to extract frequent itemsets from
large database and getting the association rule for discovering the knowledge.
Based on this algorithm, this paper indicates the limitation of the original
Apriori algorithm of wasting time for scanning the whole database searching on
the frequent itemsets, and presents an improvement on Apriori by reducing that
wasted time depending on scanning only some transactions. The paper shows by
experimental results with several groups of transactions, and with several
values of minimum support that applied on the original Apriori and our
implemented improved Apriori that our improved Apriori reduces the time
consumed by 67.38% in comparison with the original Apriori, and makes the
Apriori algorithm more efficient and less time consuming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3964</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3964</id><created>2014-03-16</created><authors><author><keyname>Niitsuma</keyname><forenames>Hirotaka</forenames></author></authors><title>Image processing using miniKanren</title><categories>cs.CV cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integral image is one of the most efficient optimization technique for
image processing. However an integral image is only a special case of delayed
stream or memoization. This research discusses generalizing concept of integral
image optimization technique, and how to generate an integral image optimized
program code automatically from abstracted image processing algorithm. In oder
to abstruct algorithms, we forces to miniKanren.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3969</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3969</id><created>2014-03-16</created><authors><author><keyname>Savani</keyname><forenames>Rahul</forenames></author><author><keyname>von Stengel</keyname><forenames>Bernhard</forenames></author></authors><title>Game Theory Explorer - Software for the Applied Game Theorist</title><categories>cs.GT</categories><msc-class>91-08</msc-class><acm-class>G.1.10; G.4</acm-class><journal-ref>Computational Management Science 12:1, 5-33 (2015)</journal-ref><doi>10.1007/s10287-014-0206-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the &quot;Game Theory Explorer&quot; software tool to create and
analyze games as models of strategic interaction. A game in extensive or
strategic form is created and nicely displayed with a graphical user interface
in a web browser. State-of-the-art algorithms then compute all Nash equilibria
of the game after a mouseclick. In tutorial fashion, we present how the program
is used, and the ideas behind its main algorithms. We report on experiences
with the architecture of the software and its development as an open-source
project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3972</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3972</id><created>2014-03-16</created><updated>2016-02-14</updated><authors><author><keyname>Vorel</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Subset Synchronization and Careful Synchronization of Binary Finite
  Automata</title><categories>cs.FL</categories><comments>An extended version of the paper &quot;Subset Synchronization of
  Transitive Automata&quot; presented at AFL 2014</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strongly exponential lower bound that applies both to the subset
synchronization threshold for binary deterministic automata and to the careful
synchronization threshold for binary partial automata. In the later form, the
result finishes the research initiated by Martyugin (2013). Moreover, we show
that both the thresholds remain strongly exponential even if restricted to
strongly connected binary automata. In addition, we apply our methods to
computational complexity. Existence of a subset reset word is known to be
PSPACE-complete; we show that this holds even under the restriction to strongly
connected binary automata. The results apply also to the corresponding
thresholds in two more general settings: D1- and D3-directable nondeterministic
automata and composition sequences over finite domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3973</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3973</id><created>2014-03-16</created><authors><author><keyname>Mayne</keyname><forenames>Richard</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards slime mould electrical logic gates with optical coupling</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physarum polycephalum is a macroscopic single celled plasmodial slime mould.
We employ plasmodial phototactic responses to construct laboratory prototypes
of NOT and NAND logical gates with electrical inputs/outputs and optical
coupling; the slime mould plays dual roles of computing device and electrical
conductor. Slime mould logical gates are fault tolerant and resettable. The
results presented here advance our understanding of how biological computing
substrates may be manipulated to implement logical operations and demonstrate
the feasibility of integrating living substrates into silicon hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3978</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3978</id><created>2014-03-16</created><updated>2014-03-18</updated><authors><author><keyname>Liu</keyname><forenames>Haijing</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Long</keyname><forenames>Wei</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author></authors><title>A Novel Scheme for Downlink Opportunistic Interference Alignment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a downlink codebook-based opportunistic interference
alignment (OIA) in a three-cell MIMO system. A codebook composed of multiple
transmit vector sets is utilized to improve the multiuser selection diversity.
The sum rate increases as the size of the codebook grows. In addition, during
the user selection, effective channel gain and alignment metric are combined to
generate a novel criterion, which improves the system performance, especially
at low SNR. Furthermore, a threshold-based feedback approach is introduced to
reduce the feedback load in the proposed scheme. Both the analytical results
and simulations show that the proposed scheme provides significant improvement
in terms of sum rates with no feedback load growth and slight increase of
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3991</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3991</id><created>2014-03-16</created><updated>2015-01-01</updated><authors><author><keyname>Yang</keyname><forenames>Gang</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Throughput Optimization for Massive MIMO Systems Powered by Wireless
  Energy Transfer</title><categories>cs.IT math.IT</categories><comments>15 double-column pages, 6 figures, 1 table, to appear in IEEE JSAC in
  February 2015, special issue on wireless communications powered by energy
  harvesting and wireless energy transfer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a wireless-energy-transfer (WET) enabled massive
multiple-input-multiple-output (MIMO) system (MM) consisting of a hybrid
data-and-energy access point (H-AP) and multiple single-antenna users. In the
WET-MM system, the H-AP is equipped with a large number $M$ of antennas and
functions like a conventional AP in receiving data from users, but additionally
supplies wireless power to the users. We consider frame-based transmissions.
Each frame is divided into three phases: the uplink channel estimation (CE)
phase, the downlink WET phase, as well as the uplink wireless information
transmission (WIT) phase. Firstly, users use a fraction of the previously
harvested energy to send pilots, while the H-AP estimates the uplink channels
and obtains the downlink channels by exploiting channel reciprocity. Next, the
H-AP utilizes the channel estimates just obtained to transfer wireless energy
to all users in the downlink via energy beamforming. Finally, the users use a
portion of the harvested energy to send data to the H-AP simultaneously in the
uplink (reserving some harvested energy for sending pilots in the next frame).
To optimize the throughput and ensure rate fairness, we consider the problem of
maximizing the minimum rate among all users. In the large-$M$ regime, we obtain
the asymptotically optimal solutions and some interesting insights for the
optimal design of WET-MM system. We define a metric, namely, the massive MIMO
degree-of-rate-gain (MM-DoRG), as the asymptotic UL rate normalized by
$\log(M)$. We show that the proposed WET-MM system is optimal in terms of
MM-DoRG, i.e., it achieves the same MM-DoRG as the case with ideal CE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3992</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3992</id><created>2014-03-17</created><authors><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Pribavkina</keyname><forenames>Elena V.</forenames></author></authors><title>Reset thresholds of automata with two cycle lengths</title><categories>cs.FL</categories><comments>11 pages, 5 figures, submitted to CIAA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several series of synchronizing automata with multiple parameters,
generalizing previously known results. Let p and q be two arbitrary co-prime
positive integers, q &gt; p. We describe reset thresholds of the colorings of
primitive digraphs with exactly one cycle of length p and one cycle of length
q. Also, we study reset thresholds of the colorings of primitive digraphs with
exactly one cycle of length q and two cycles of length p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3993</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3993</id><created>2014-03-17</created><authors><author><keyname>Khairnar</keyname><forenames>Mrs. Vaishali D.</forenames></author><author><keyname>Pradhan</keyname><forenames>Dr. S. N.</forenames></author></authors><title>V2V communication survey wireless technology</title><categories>cs.NI</categories><comments>04 Pages, 02 figures</comments><journal-ref>Int.J.Computer Technology &amp; Applications,Vol 3 (1), 370-373
  Jan-Feb 2012 ISSN:2229-6093</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the specific application of wireless
communication,automotive Wireless Communication also called as Vehicle to
Vehicle Communication.It explains the technology used for automotive Wireless
Communication along with the various automotive applications relying on
wireless communication. Automotive Wireless Communication gives drivers a sixth
sense to know whats going on around them to help avoid accidents and improve
traffic flow.The paper also describes VANETS (vehicular adhoc networks) and
real world test network implementation.Finally,the paper is summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3996</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3996</id><created>2014-03-17</created><authors><author><keyname>Kashyap</keyname><forenames>Vineeth</forenames></author><author><keyname>Dewey</keyname><forenames>Kyle</forenames></author><author><keyname>Kuefner</keyname><forenames>Ethan A.</forenames></author><author><keyname>Wagner</keyname><forenames>John</forenames></author><author><keyname>Gibbons</keyname><forenames>Kevin</forenames></author><author><keyname>Sarracino</keyname><forenames>John</forenames></author><author><keyname>Wiedermann</keyname><forenames>Ben</forenames></author><author><keyname>Hardekopf</keyname><forenames>Ben</forenames></author></authors><title>JSAI: Designing a Sound, Configurable, and Efficient Static Analyzer for
  JavaScript</title><categories>cs.PL</categories><acm-class>F.3.2; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe JSAI, an abstract interpreter for JavaScript. JSAI uses novel
abstract domains to compute a reduced product of type inference, pointer
analysis, string analysis, integer and boolean constant propagation, and
control-flow analysis. In addition, JSAI allows for analysis control-flow
sensitivity (i.e., context-, path-, and heap-sensitivity) to be modularly
configured without requiring any changes to the analysis implementation. JSAI
is designed to be provably sound with respect to a specific concrete semantics
for JavaScript, which has been extensively tested against existing
production-quality JavaScript implementations.
  We provide a comprehensive evaluation of JSAI's performance and precision
using an extensive benchmark suite. This benchmark suite includes real-world
JavaScript applications, machine-generated JavaScript code via Emscripten, and
browser addons. We use JSAI's configurability to evaluate a large number of
analysis sensitivities (some well-known, some novel) and observe some
surprising results. We believe that JSAI's configurability and its formal
specifications position it as a useful research platform to experiment on novel
sensitivities, abstract domains, and client analyses for JavaScript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4010</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4010</id><created>2014-03-17</created><updated>2015-01-12</updated><authors><author><keyname>Wei</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Renquan</forenames></author><author><keyname>Niu</keyname><forenames>Baolong</forenames></author><author><keyname>Guo</keyname><forenames>Binghui</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author></authors><title>Organization mechanism and counting algorithm on Vertex-Cover solutions</title><categories>cs.CC</categories><comments>17 pages, 6 figures</comments><doi>10.1088/1742-5468/2015/04/P04002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting the solution number of combinational optimization problems is an
important topic in the study of computational complexity, especially on the
#P-complete complexity class. In this paper, we first investigate some
organizations of Vertex-Cover unfrozen subgraphs by the underlying connectivity
and connected components of unfrozen vertices. Then, a Vertex-Cover Solution
Number Counting Algorithm is proposed and its complexity analysis is provided,
the results of which fit very well with the simulations and have better
performance than those by 1-RSB in a neighborhood of c = e for random graphs.
Base on the algorithm, variation and fluctuation on the solution number
statistics are studied to reveal the evolution mechanism of the solution
numbers. Besides, marginal probability distributions on the solution space are
investigated on both random graph and scale-free graph to illustrate different
evolution characteristics of their solution spaces. Thus, doing solution number
counting based on graph expression of solution space should be an alternative
and meaningful way to study the hardness of NP-complete and #P-complete
problems, and appropriate algorithm design can help to achieve better
approximations of solving combinational optimization problems and the
corresponding counting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4011</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4011</id><created>2014-03-17</created><updated>2014-11-11</updated><authors><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author></authors><title>Whose Opinion to follow in Multihypothesis Social Learning? A Large
  Deviations Perspective</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Journal of Selected Topics in Signal Processing</comments><doi>10.1109/JSTSP.2014.2365757</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multihypothesis social learning problem in which an agent has
access to a set of private observations and chooses an opinion from a set of
experts to incorporate into its final decision. To model individual biases, we
allow the agent and experts to have general loss functions and possibly
different decision spaces. We characterize the loss exponents of both the agent
and experts, and provide an asymptotically optimal method for the agent to
choose the best expert to follow. We show that up to asymptotic equivalence,
the worst loss exponent for the agent is achieved when it adopts the 0-1 loss
function, which assigns a loss of 0 if the true hypothesis is declared and a
loss of 1 otherwise. We introduce the concept of hypothesis-loss neutrality,
and show that if the agent adopts a particular policy that is hypothesis-loss
neutral, then it ignores all experts whose decision spaces are smaller than its
own. On the other hand, if experts have the same decision space as the agent,
then choosing an expert with the same loss function as itself is not
necessarily optimal for the agent, which is somewhat counter-intuitive. We
derive sufficient conditions for when it is optimal for the agent with 0-1 loss
function to choose an expert with the same loss function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4015</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4015</id><created>2014-03-17</created><authors><author><keyname>Caullery</keyname><forenames>Florian</forenames></author><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author><author><keyname>Zhou</keyname><forenames>Yue</forenames></author></authors><title>Exceptional planar polynomials</title><categories>math.NT cs.IT math.CO math.IT</categories><msc-class>11T06, 51E20, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planar functions are special functions from a finite field to itself that
give rise to finite projective planes and other combinatorial objects. We
consider polynomials over a finite field $K$ that induce planar functions on
infinitely many extensions of $K$; we call such polynomials exceptional planar.
Exceptional planar monomials have been recently classified. In this paper we
establish a partial classification of exceptional planar polynomials. This
includes results for the classical planar functions on finite fields of odd
characteristic and for the recently proposed planar functions on finite fields
of characteristic two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4017</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4017</id><created>2014-03-17</created><authors><author><keyname>Yang</keyname><forenames>Longqi</forenames></author><author><keyname>Wang</keyname><forenames>Yibing</forenames></author><author><keyname>Pan</keyname><forenames>Zhisong</forenames></author><author><keyname>Hu</keyname><forenames>Guyu</forenames></author></authors><title>Multi-task Feature Selection based Anomaly Detection</title><categories>stat.ML cs.LG</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network anomaly detection is still a vibrant research area. As the fast
growth of network bandwidth and the tremendous traffic on the network, there
arises an extremely challengeable question: How to efficiently and accurately
detect the anomaly on multiple traffic? In multi-task learning, the traffic
consisting of flows at different time periods is considered as a task. Multiple
tasks at different time periods performed simultaneously to detect anomalies.
In this paper, we apply the multi-task feature selection in network anomaly
detection area which provides a powerful method to gather information from
multiple traffic and detect anomalies on it simultaneously. In particular, the
multi-task feature selection includes the well-known l1-norm based feature
selection as a special case given only one task. Moreover, we show that the
multi-task feature selection is more accurate by utilizing more information
simultaneously than the l1-norm based method. At the evaluation stage, we
preprocess the raw data trace from trans-Pacific backbone link between Japan
and the United States, label with anomaly communities, and generate a
248-feature dataset. We show empirically that the multi-task feature selection
outperforms independent l1-norm based feature selection on real traffic
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4023</identifier>
 <datestamp>2014-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4023</id><created>2014-03-17</created><updated>2014-06-25</updated><authors><author><keyname>Budden</keyname><forenames>David</forenames></author><author><keyname>Wang</keyname><forenames>Peter</forenames></author><author><keyname>Obst</keyname><forenames>Oliver</forenames></author><author><keyname>Prokopenko</keyname><forenames>Mikhail</forenames></author></authors><title>Simulation leagues: Analysis of competition formats</title><categories>cs.MA cs.AI cs.RO</categories><comments>12 pages, 2 figures, presented at RoboCup 2014 symposium, Brazil</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The selection of an appropriate competition format is critical for both the
success and credibility of any competition, both real and simulated. In this
paper, the automated parallelism offered by the RoboCupSoccer 2D simulation
league is leveraged to conduct a 28,000 game round-robin between the top 8
teams from RoboCup 2012 and 2013. A proposed new competition format is found to
reduce variation from the resultant statistically significant team performance
rankings by 75% and 67%, when compared to the actual competition results from
RoboCup 2012 and 2013 respectively. These results are statistically validated
by generating 10,000 random tournaments for each of the three considered
formats and comparing the respective distributions of ranking discrepancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4024</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4024</id><created>2014-03-17</created><updated>2014-05-14</updated><authors><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Biondi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Corre</keyname><forenames>Kevin</forenames></author><author><keyname>Jegourel</keyname><forenames>Cyrille</forenames></author><author><keyname>Kongsh&#xf8;j</keyname><forenames>Simon</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author></authors><title>Measuring Global Similarity between Texts</title><categories>cs.CL</categories><comments>Submitted to SLSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new similarity measure between texts which, contrary to the
current state-of-the-art approaches, takes a global view of the texts to be
compared. We have implemented a tool to compute our textual distance and
conducted experiments on several corpuses of texts. The experiments show that
our methods can reliably identify different global types of texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4026</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4026</id><created>2014-03-17</created><authors><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Chen</keyname><forenames>Jingwei</forenames></author><author><keyname>Wu</keyname><forenames>Wenyuan</forenames></author></authors><title>A Short Note on Zero-error Computation for Algebraic Numbers by IPSLQ</title><categories>cs.SC</categories><comments>4 pages</comments><journal-ref>ACM Communications in Computer Algebra, 47(3), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PSLQ algorithm is one of the most popular algorithm for finding
nontrivial integer relations for several real numbers. In the present work, we
present an incremental version of PSLQ. For some applications needing to call
PSLQ many times, such as finding the minimal polynomial of an algebraic number
without knowing the degree, the incremental PSLQ algorithm is more efficient
than PSLQ, both theoretically and practically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4045</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4045</id><created>2014-03-17</created><authors><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Goal-oriented Data Visualization with Software Project Control Centers</title><categories>cs.SE</categories><comments>11 pages. arXiv admin note: substantial text overlap with
  arXiv:1401.1906</comments><journal-ref>Proceedings of the DASMA Software Metric Congress (MetriKon 2005):
  Magdeburger Schriften zum Empirischen Software Engineering, pages 65-75,
  Kaiserslautern, Germany, November 14-16 2005. Shaker Verlag</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many software development organizations still lack support for obtaining
intellectual control over their software development processes and for
determining the performance of their processes and the quality of the produced
products. Systematic support for detecting and reacting to critical project
states in order to achieve planned goals is usually missing. One means to
institutionalize measurement on the basis of explicit models is the development
and establishment of a so-called Software Project Control Center (SPCC) for
systematic quality assurance and management support. An SPCC is comparable to a
control room, which is a well known term in the mechanical production domain.
Its tasks include collecting, in- terpreting, and visualizing measurement data
in order to provide context-, purpose-, and role-oriented information for all
stakeholders (e.g., project managers, quality assurance manager, developers)
during the execution of a software development project. The article will
present an overview of SPCC concepts, a concrete instantiation that supports
goal-oriented data visualization (G-SPCC approach), and experiences from
practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4047</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4047</id><created>2014-03-17</created><updated>2014-07-16</updated><authors><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author></authors><title>A Queueing Characterization of Information Transmission over Block
  Fading Rayleigh Channels in the Low SNR</title><categories>cs.IT math.IT</categories><comments>29 pages, 11 figures. More details on the proof of Theorem 1 and
  proposition 1 can be found in &quot;Queueing analysis for block fading Rayleigh
  channels in the low SNR regime &quot;, IEEE WCSP 2013.It has been published by
  IEEE Trans. on Veh. Technol. in Feb. 2014</comments><doi>10.1109/TVT.2014.2307585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike the AWGN (additive white gaussian noise) channel, fading channels
suffer from random channel gains besides the additive Gaussian noise. As a
result, the instantaneous channel capacity varies randomly along time, which
makes it insufficient to characterize the transmission capability of a fading
channel using data rate only. In this paper, the transmission capability of a
buffer-aided block Rayleigh fading channel is examined by a constant rate input
data stream, and reflected by several parameters such as the average queue
length, stationary queue length distribution, packet delay and overflow
probability. Both infinite-buffer model and finite-buffer model are considered.
Taking advantage of the memoryless property of the service provided by the
channel in each block in the the low SNR (signal-to-noise ratio) regime, the
information transmission over the channel is formulated as a \textit{discrete
time discrete state} $D/G/1$ queueing problem. The obtained results show that
block fading channels are unable to support a data rate close to their ergodic
capacity, no matter how long the buffer is, even seen from the application
layer. For the finite-buffer model, the overflow probability is derived with
explicit expression, and is shown to decrease exponentially when buffer size is
increased, even when the buffer size is very small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4053</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4053</id><created>2014-03-17</created><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author></authors><title>Using the Business Process Model and Notation for Modeling Enterprise
  Integration Patterns</title><categories>cs.SE</categories><comments>30 pages, Supplementary Material for Contribution to European
  Conference on Modelling Foundations and Applications (ECMFA), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise Integration Patterns (EIP) are a collection of widely used
stencils for integrating enterprise applications and business processes. These
patterns represent a &quot;de-facto&quot; standard reference for design decisions when
integrating enterprise applications.
  For each of these patterns we present the integration semantics (model) and
the conceptual translation (syntax) to the Business Process Model and Notation
(BPMN), which is a &quot;de-facto&quot; standard for modelling business process semantics
and their runtime behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4060</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4060</id><created>2014-03-17</created><updated>2014-04-09</updated><authors><author><keyname>Galindo</keyname><forenames>Carlos</forenames></author><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author></authors><title>Quantum codes from affine variety codes and their subfield-subcodes</title><categories>cs.IT math.IT</categories><msc-class>94B, 11T71, 94A24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use affine variety codes and their subfield-subcodes for obtaining quantum
stabilizer codes via the CSS code construction. With this procedure, we get
codes with good parameters and a code whose parameters exceed the CSS quantum
Gilbert-Varshamov bound given by Feng and Ma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4064</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4064</id><created>2014-03-17</created><updated>2014-09-17</updated><authors><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author><author><keyname>Radi&#x10d;ek</keyname><forenames>Ivan</forenames></author><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author></authors><title>Feedback Generation for Performance Problems in Introductory Programming
  Assignments</title><categories>cs.PL</categories><comments>Tech report/extended version of FSE 2014 paper</comments><acm-class>D.2.5; I.2.2</acm-class><doi>10.1145/2635868.2635912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing feedback on programming assignments manually is a tedious, error
prone, and time-consuming task. In this paper, we motivate and address the
problem of generating feedback on performance aspects in introductory
programming assignments. We studied a large number of functionally correct
student solutions to introductory programming assignments and observed: (1)
There are different algorithmic strategies, with varying levels of efficiency,
for solving a given problem. These different strategies merit different
feedback. (2) The same algorithmic strategy can be implemented in countless
different ways, which are not relevant for reporting feedback on the student
program.
  We propose a light-weight programming language extension that allows a
teacher to define an algorithmic strategy by specifying certain key values that
should occur during the execution of an implementation. We describe a dynamic
analysis based approach to test whether a student's program matches a teacher's
specification. Our experimental results illustrate the effectiveness of both
our specification language and our dynamic analysis. On one of our benchmarks
consisting of 2316 functionally correct implementations to 3 programming
problems, we identified 16 strategies that we were able to describe using our
specification language (in 95 minutes after inspecting 66, i.e., around 3%,
implementations). Our dynamic analysis correctly matched each implementation
with its corresponding specification, thereby automatically producing the
intended feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4074</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4074</id><created>2014-03-12</created><authors><author><keyname>Cohen</keyname><forenames>Ernie</forenames></author></authors><title>Hypothesis Elimination in Kleene Semirings</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Kleene semiring is an algebraic structure satisfying the axioms of Kleene
algebra, minus the annihilation axioms (x.0 = 0 = 0.x). We show that Kleene
semirings (like Kleene algebras) admit the efficient elimination of various
kinds of equational hypotheses, in particular Hoare formulas (t=0, where t is
an arbitrary term). Our method is purely proof-theoretic, and can be used to
eliminate Horn hypotheses in any suitable Horn-equational theory. Moreover, it
gives a simple condition under which hypotheses eliminations can be combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4075</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4075</id><created>2014-03-17</created><updated>2014-12-06</updated><authors><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Distributed Load Balancing in Heterogeneous Systems</title><categories>cs.GT cs.DC</categories><comments>Removed due to a technical issue in one of the results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed load balancing in heterogenous
parallel server systems, where the service rate achieved by a user at a server
depends on both the user and the server. Such heterogeneity typically arises in
wireless networks (e.g., servers may represent frequency bands, and the service
rate of a user varies across bands). Users select servers in a distributed
manner. They initially attach to an arbitrary server. However, at random
instants of time, they may probe the load at a new server and migrate there to
improve their service rate. We analyze the system dynamics under the natural
Random Local Search (RLS) migration scheme, introduced in \cite{sig10}. Under
this scheme, when a user has the opportunity to switch servers, she does it
only if this improves her service rate. The dynamics under RLS may be
interpreted as those generated by strategic players updating their strategy in
a load balancing game. In closed systems, where the user population is fixed,
we show that this game has pure Nash Equilibriums (NEs), and we analyze their
efficiency. We further prove that when the user population grows large, pure
NEs get closer to a Proportionally Fair (PF) allocation of users to servers,
and we characterize the gap between equilibriums and this ideal allocation
depending on user population. Under the RLS algorithm, the system converges to
pure NEs: we study the time it takes for the system to reach the PF allocation
within a certain margin. In open systems, where users randomly enter the system
and leave upon service completion, we establish that the RLS algorithm
stabilizes the system whenever this it at all possible, i.e., it is
throughput-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4077</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4077</id><created>2014-03-17</created><updated>2015-05-21</updated><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Behavior, Organization, Substance: Three Gestalts of General Systems
  Theory</title><categories>cs.OH</categories><comments>Paper appeared in the Proc. of the IEEE Conference &quot;Wiener in the
  21st Century&quot;. Companion paper of http://arxiv.org/abs/1403.0339</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term gestalt, when used in the context of general systems theory, assumes
the value of &quot;systemic touchstone&quot;, namely a figure of reference used to
categorize the properties or qualities of a set of systems. Typical gestalts
used in biology are those based on anatomical or physiological characteristics,
which correspond respectively to architectural and organizational design
choices in natural and artificial systems. In this paper we discuss three
gestalts of general systems theory: behavior, organization, and substance,
which refer respectively to the works of Wiener, Boulding, and Leibniz. Our
major focus here is the system introduced by the latter. Through a discussion
of some of the elements of the Leibnitian System, and by means of several novel
interpretations of those elements in terms of today's computer science, we
highlight the debt that contemporary research still has with this Giant among
the giant scholars of the past.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4094</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4094</id><created>2014-03-17</created><updated>2014-04-02</updated><authors><author><keyname>Mimram</keyname><forenames>Samuel</forenames><affiliation>CEA LIST</affiliation></author></authors><title>Towards 3-Dimensional Rewriting Theory</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (April 4,
  2014) lmcs:750</journal-ref><doi>10.2168/LMCS-10(2:1)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String rewriting systems have proved very useful to study monoids. In good
cases, they give finite presentations of monoids, allowing computations on
those and their manipulation by a computer. Even better, when the presentation
is confluent and terminating, they provide one with a notion of canonical
representative of the elements of the presented monoid. Polygraphs are a
higher-dimensional generalization of this notion of presentation, from the
setting of monoids to the much more general setting of n-categories. One of the
main purposes of this article is to give a progressive introduction to the
notion of higher-dimensional rewriting system provided by polygraphs, and
describe its links with classical rewriting theory, string and term rewriting
systems in particular. After introducing the general setting, we will be
interested in proving local confluence for polygraphs presenting 2-categories
and introduce a framework in which a finite 3-dimensional rewriting system
admits a finite number of critical pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4099</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4099</id><created>2014-03-17</created><updated>2015-08-02</updated><authors><author><keyname>Hendricks</keyname><forenames>Dieter</forenames></author><author><keyname>Wilcox</keyname><forenames>Diane</forenames></author><author><keyname>Gebbie</keyname><forenames>Tim</forenames></author></authors><title>High-speed detection of emergent market clustering via an unsupervised
  parallel genetic algorithm</title><categories>q-fin.CP cs.DC cs.NE</categories><comments>10 pages, 5 figures, 4 tables, More thorough discussion of
  implementation</comments><journal-ref>S Afr J Sci. 2016;112(1/2), Art. #2014-0340, 9 pages</journal-ref><doi>10.17159/sajs.2016/20140340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We implement a master-slave parallel genetic algorithm (PGA) with a bespoke
log-likelihood fitness function to identify emergent clusters within price
evolutions. We use graphics processing units (GPUs) to implement a PGA and
visualise the results using disjoint minimal spanning trees (MSTs). We
demonstrate that our GPU PGA, implemented on a commercially available general
purpose GPU, is able to recover stock clusters in sub-second speed, based on a
subset of stocks in the South African market. This represents a pragmatic
choice for low-cost, scalable parallel computing and is significantly faster
than a prototype serial implementation in an optimised C-based
fourth-generation programming language, although the results are not directly
comparable due to compiler differences. Combined with fast online intraday
correlation matrix estimation from high frequency data for cluster
identification, the proposed implementation offers cost-effective,
near-real-time risk assessment for financial practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4106</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4106</id><created>2014-03-17</created><updated>2014-07-15</updated><authors><author><keyname>Tomasello</keyname><forenames>Mario Vincenzo</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>The role of endogenous and exogenous mechanisms in the formation of R&amp;D
  networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>12 pages, 10 figures</comments><journal-ref>Tomasello, M.V., Perra, N., Tessone, C.J., Karsai, M. &amp;
  Schweitzer, F. The role of endogenous and exogenous mechanisms in the
  formation of R&amp;D networks. Sci. Rep. 4, 5679 (2014)</journal-ref><doi>10.1038/srep05679</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an agent-based model of strategic link formation in Research and
Development (R&amp;D) networks. Empirical evidence has shown that the growth of
these networks is driven by mechanisms which are both endogenous to the system
(that is, depending on existing alliances patterns) and exogenous (that is,
driven by an exploratory search for newcomer firms). Extant research to date
has not investigated both mechanisms simultaneously in a comparative manner. To
overcome this limitation, we develop a general modeling framework to shed light
on the relative importance of these two mechanisms. We test our model against a
comprehensive dataset, listing cross-country and cross-sectoral R&amp;D alliances
from 1984 to 2009. Our results show that by fitting only three macroscopic
properties of the network topology, this framework is able to reproduce a
number of micro-level measures, including the distributions of degree, local
clustering, path length and component size, and the emergence of network
clusters. Furthermore, by estimating the link probabilities towards newcomers
and established firms from the data, we find that endogenous mechanisms are
predominant over the exogenous ones in the network formation, thus quantifying
the importance of existing structures in selecting partner firms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4109</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4109</id><created>2014-03-17</created><updated>2014-12-19</updated><authors><author><keyname>Etesami</keyname><forenames>Seyed Rasoul</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Convergence Time for Unbiased Quantized Consensus Over Static and
  Dynamic Networks</title><categories>cs.SY cs.SI</categories><comments>The paper is accepted in IEEE Transactions on Automatic Control and
  will appear soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the question of expected time to convergence is addressed for
unbiased quantized consensus on undirected connected graphs, and some strong
results are obtained. The paper first provides a tight expression for the
expected convergence time of the unbiased quantized consensus over general but
fixed networks. It is shown that the maximum expected convergence time lies
within a constant factor of the maximum hitting time of an appropriate lazy
random walk, using the theory of harmonic functions for reversible Markov
chains. Following this, and using electric resistance analogy of the reversible
Markov chains, the paper provides a tight upper bound for the expected
convergence time to consensus based on the parameters of the network. Moreover,
the paper identifies a precise order of the maximum expected convergence time
for some simple graphs such as line graph and cycle. Finally, the results are
extended to bound the expected convergence time of the underlying dynamics in
time-varying networks. Modeling such dynamics as the evolution of a time
inhomogeneous Markov chain, the paper derives a tight upper bound for expected
convergence time of the dynamics using the spectral representation of the
networks. This upper bound is significantly better than earlier results for the
quantized consensus problem over time-varying graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4118</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4118</id><created>2014-03-17</created><updated>2014-04-28</updated><authors><author><keyname>Helmling</keyname><forenames>Michael</forenames></author><author><keyname>Rosnes</keyname><forenames>Eirik</forenames></author><author><keyname>Ruzika</keyname><forenames>Stefan</forenames></author><author><keyname>Scholl</keyname><forenames>Stefan</forenames></author></authors><title>Efficient Maximum-Likelihood Decoding of Linear Block Codes on Binary
  Memoryless Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to 2014 International Symposium on Information Theory. 5
  Pages. Accepted</comments><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider efficient maximum-likelihood decoding of linear
block codes for small-to-moderate block lengths. The presented approach is a
branch-and-bound algorithm using the cutting-plane approach of Zhang and Siegel
(IEEE Trans. Inf. Theory, 2012) for obtaining lower bounds. We have compared
our proposed algorithm to the state-of-the-art commercial integer program
solver CPLEX, and for all considered codes our approach is faster for both low
and high signal-to-noise ratios. For instance, for the benchmark (155,64)
Tanner code our algorithm is more than 11 times as fast as CPLEX for an SNR of
1.0 dB on the additive white Gaussian noise channel. By a small modification,
our algorithm can be used to calculate the minimum distance, which we have
again verified to be much faster than using the CPLEX solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4143</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4143</id><created>2014-03-17</created><updated>2014-06-16</updated><authors><author><keyname>Kim</keyname><forenames>Joonmo</forenames></author></authors><title>P is not equal to NP by Modus Tollens</title><categories>cs.CC</categories><comments>7 pages. Caution: this solution should not be reported to be correct
  while this caution stands. Actually, it is quite unlikely that this simple
  solution is correct. I am sharing this e-print to get comments on the
  probable mistake that I could not have found yet. If you have a conceptual
  grasp on Cooks Theory and some related glossaries then there will be no
  difficulty in reading this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An artificially designed Turing Machine algorithm $\mathbf{M}_{}^{o}$
generates the instances of the satisfiability problem, and check their
satisfiability. Under the assumption $\mathcal{P}=\mathcal{NP}$, we show that
$\mathbf{M}_{}^{o}$ has a certain property, which, without the assumption,
$\mathbf{M}_{}^{o}$ does not have. This leads to $\mathcal{P}\neq\mathcal{NP}$
$ $ by modus tollens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4144</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4144</id><created>2014-03-17</created><authors><author><keyname>He</keyname><forenames>Qing</forenames></author><author><keyname>Angelakis</keyname><forenames>Vangelis</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author></authors><title>Polynomial Complexity Minimum-Time Scheduling in a Class of Wireless
  Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Control of Network Systems on Dec.
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless network with a set of transmitter-receiver pairs, or
links, that share a common channel, and address the problem of emptying finite
traffic volume from the transmitters in minimum time. This, so called,
minimum-time scheduling problem has been proved to be NP-hard in general. In
this paper, we study a class of minimum-time scheduling problems in which the
link rates have a particular structure consistent with the assumed environment
and topology. We show that global optimality can be reached in polynomial time
and derive optimality conditions. Then we consider a more general case in which
we apply the same approach and thus obtain approximation as well as lower and
upper bounds to the optimal solution. Simulation results confirm and validate
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4152</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4152</id><created>2014-03-17</created><authors><author><keyname>Rahimi</keyname><forenames>Reza</forenames></author></authors><title>Quick Safari Through Software Design</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a short tutorial about different software design methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4153</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4153</id><created>2014-03-17</created><updated>2014-10-20</updated><authors><author><keyname>Cavallo</keyname><forenames>Bren</forenames></author><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author></authors><title>A family of polycyclic groups over which the uniform conjugacy problem
  is NP-complete</title><categories>math.GR cs.CC cs.CR</categories><doi>10.1142/S0218196714500234</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the conjugacy problem in polycyclic groups. Our main
result is that we construct polycyclic groups $G_n$ whose conjugacy problem is
at least as hard as the subset sum problem with $n$ indeterminates. As such,
the conjugacy problem over the groups $G_n$ is NP-complete where the parameters
of the problem are taken in terms of $n$ and the length of the elements given
on input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4155</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4155</id><created>2014-03-17</created><updated>2014-11-10</updated><authors><author><keyname>Tarighati</keyname><forenames>Alla</forenames></author><author><keyname>Jalden</keyname><forenames>Joakim</forenames></author></authors><title>Bayesian Design of Tandem Networks for Distributed Detection With
  Multi-bit Sensor Decisions</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2401535</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decentralized hypothesis testing under
communication constraints in a topology where several peripheral nodes are
arranged in tandem. Each node receives an observation and transmits a message
to its successor, and the last node then decides which hypothesis is true. We
assume that the observations at different nodes are, conditioned on the true
hypothesis, independent and the channel between any two successive nodes is
considered error-free but rate-constrained. We propose a cyclic numerical
design algorithm for the design of nodes using a person-by-person methodology
with the minimum expected error probability as a design criterion, where the
number of communicated messages is not necessarily equal to the number of
hypotheses. The number of peripheral nodes in the proposed method is in
principle arbitrary and the information rate constraints are satisfied by
quantizing the input of each node. The performance of the proposed method for
different information rate constraints, in a binary hypothesis test, is
compared to the optimum rate-one solution due to Swaszek and a method proposed
by Cover, and it is shown numerically that increasing the channel rate can
significantly enhance the performance of the tandem network. Simulation results
for $M$-ary hypothesis tests also show that by increasing the channel rates the
performance of the tandem network significantly improves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4158</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4158</id><created>2014-03-17</created><authors><author><keyname>Milani</keyname><forenames>A. A.</forenames></author><author><keyname>Rahimi</keyname><forenames>Reza</forenames></author></authors><title>A Methodology for Implementation of MMS Client on Embedded Platforms</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MMS (Multimedia Messaging Service) is the next generation of messaging
services in multimedia mobile communications. MMS enables messaging with full
multimedia content including images, audios, videos, texts and data, from
client to client or e-mail. MMS is based on WAP technology, so it is technology
independent. This means that enabling messages from a GSM/GPRS network to be
sent to a TDMA or WCDMA network. In this paper a methodology for implementing
MMS client on embedded platforms especially on Wince OS is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4165</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4165</id><created>2014-03-17</created><authors><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Lam</keyname><forenames>Ha T.</forenames></author></authors><title>Heisenberg Groups as Platform for the AAG key-exchange protocol</title><categories>cs.CR math.GR</categories><comments>arXiv admin note: text overlap with arXiv:1305.0548</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Garber, Kahrobaei, and Lam studied polycyclic groups generated by number
field as platform for the AAG key-exchange protocol. In this paper, we discuss
the use of a different kind of polycyclic groups, Heisenberg groups, as a
platform group for AAG by submitting Heisenberg groups to one of AAG's major
attacks, the length-based attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4169</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4169</id><created>2014-03-17</created><authors><author><keyname>Rahimi</keyname><forenames>Reza</forenames></author><author><keyname>Hengmeechai</keyname><forenames>J</forenames></author></authors><title>Pervasive Image Computation: A Mobile Phone Application for getting
  Information of the Images</title><categories>cs.MM cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many of the information processing systems are text-based, much of
the information in the real life is generally multimedia objects, so there is a
need to define and standardize the frame works for multimedia-based information
processing systems. In this paper we consider the application of such a system
namely pervasive image computation system, in which the user uses the cellphone
for taking the picture of the objects, and he wants to get some information
about them. We have implemented two architectures, the first one, called online
architecture, which the user sends the picture to the server and server sends
the picture information directly back to him. In the second one, which is
called offline architecture, the user uploads the image in one public image
database such as Flickr and sends the ID of the image in this database to the
server. The server processes the image and adds the information of the image in
the database, and finally the user can connect to the database and download the
image information. The implementation results show that these architectures are
very flexible and could be easily extended to be used in more complicated
pervasive multimedia systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4174</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4174</id><created>2014-03-17</created><authors><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>A Receding Horizon Approach to Multi-Agent Planning from Local LTL
  Specifications</title><categories>cs.RO</categories><comments>Extended version of ACC 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of control synthesis for multi-agent systems, to achieve
complex, high-level, long-term goals that are assigned to each agent
individually. As the agents might not be capable of satisfying their respective
goals by themselves, requests for other agents' collaborations are a part of
the task descriptions. Particularly, we consider that the task specification
takes a form of a linear temporal logic formula, which may contain requirements
and constraints on the other agent's behavior. A traditional automata-based
approach to multi-agent strategy synthesis from such specifications builds on
centralized planning for the whole team and thus suffers from extreme
computational demands. In this work, we aim at reducing the computational
complexity by decomposing the strategy synthesis problem into short horizon
planning problems that are solved iteratively, upon the run of the agents. We
discuss the correctness of the solution and find assumptions, under which the
proposed iterative algorithm leads to provable eventual satisfaction of the
desired specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4175</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4175</id><created>2014-03-17</created><authors><author><keyname>Lakshminarayanan</keyname><forenames>Chandrashekar</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Approximate Dynamic Programming based on Projection onto the (min,+)
  subsemimodule</title><categories>cs.SY math.OC</categories><comments>20 pages, 6 figures (including tables), 1 algorithm, a convergence
  proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new Approximate Dynamic Programming (ADP) method for infinite
horizon discounted reward Markov Decision Processes (MDP) based on projection
onto a subsemimodule. We approximate the value function in terms of a
$(\min,+)$ linear combination of a set of basis functions whose $(\min,+)$
linear span constitutes a subsemimodule. The projection operator is closely
related to the Fenchel transform. Our approximate solution obeys the $(\min,+)$
Projected Bellman Equation (MPPBE) which is different from the conventional
Projected Bellman Equation (PBE). We show that the approximation error is
bounded in its $L_\infty$-norm. We develop a Min-Plus Approximate Dynamic
Programming (MPADP) algorithm to compute the solution to the MPPBE. We also
present the proof of convergence of the MPADP algorithm and apply it to two
problems, a grid-world problem in the discrete domain and mountain car in the
continuous domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4179</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4179</id><created>2014-03-17</created><authors><author><keyname>Lakshminarayanan</keyname><forenames>Chandrashekar</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Approximate dynamic programming with $(\min,+)$ linear function
  approximation for Markov decision processes</title><categories>cs.SY math.OC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Decision Processes (MDP) is an useful framework to cast optimal
sequential decision making problems. Given any MDP the aim is to find the
optimal action selection mechanism i.e., the optimal policy. Typically, the
optimal policy ($u^*$) is obtained by substituting the optimal value-function
($J^*$) in the Bellman equation. Alternately $u^*$ is also obtained by learning
the optimal state-action value function $Q^*$ known as the $Q$ value-function.
However, it is difficult to compute the exact values of $J^*$ or $Q^*$ for MDPs
with large number of states. Approximate Dynamic Programming (ADP) methods
address this difficulty by computing lower dimensional approximations of
$J^*$/$Q^*$. Most ADP methods employ linear function approximation (LFA), i.e.,
the approximate solution lies in a subspace spanned by a family of pre-selected
basis functions. The approximation is obtain via a linear least squares
projection of higher dimensional quantities and the $L_2$ norm plays an
important role in convergence and error analysis. In this paper, we discuss ADP
methods for MDPs based on LFAs in $(\min,+)$ algebra. Here the approximate
solution is a $(\min,+)$ linear combination of a set of basis functions whose
span constitutes a subsemimodule. Approximation is obtained via a projection
operator onto the subsemimodule which is different from linear least squares
projection used in ADP methods based on conventional LFAs. MDPs are not
$(\min,+)$ linear systems, nevertheless, we show that the monotonicity property
of the projection operator helps us to establish the convergence of our ADP
schemes. We also discuss future directions in ADP methods for MDPs based on the
$(\min,+)$ LFAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4182</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4182</id><created>2014-03-17</created><authors><author><keyname>Fanaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Schmid</keyname><forenames>Natalia A.</forenames></author></authors><title>Effects of Spatial Randomness on Locating a Point Source with
  Distributed Sensors</title><categories>cs.IT math.IT</categories><comments>7 Pages, 5 Figures, To appear at the 2014 IEEE International
  Conference on Communications (ICC'14) Workshop on Advances in Network
  Localization and Navigation (ANLN), Invited Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most studies that consider the problem of estimating the location of a point
source in wireless sensor networks assume that the source location is estimated
by a set of spatially distributed sensors, whose locations are fixed. Motivated
by the fact that the observation quality and performance of the localization
algorithm depend on the location of the sensors, which could be randomly
distributed, this paper investigates the performance of a recently proposed
energy-based source-localization algorithm under the assumption that the
sensors are positioned according to a uniform clustering process. Practical
considerations such as the existence and size of the exclusion zones around
each sensor and the source will be studied. By introducing a novel performance
measure called the estimation outage, it will be shown how parameters related
to the network geometry such as the distance between the source and the closest
sensor to it as well as the number of sensors within a region surrounding the
source affect the localization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4202</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4202</id><created>2014-03-17</created><updated>2014-04-17</updated><authors><author><keyname>de Ara&#xfa;jo</keyname><forenames>Anderson</forenames></author></authors><title>Semantic information and artificial intelligence</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a computational system to be intelligent, it should be able to perform,
at least, basic deductions. Nonetheless, since deductions are, in some sense,
equivalent to tautologies, it seems that they do not provide new information.
The present article proposes a measure the degree of semantic informativity of
valid deductions in a dynamic setting. Concepts of coherency and relevancy,
displayed in terms of insertions and deletions on databases, are used to define
semantic informativity. In this way, the article shows that a solution to the
problem about the informativity of deductions provides a heuristic principle to
improve the deductive power of computational systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4224</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4224</id><created>2014-03-17</created><updated>2014-09-19</updated><authors><author><keyname>Rabusseau</keyname><forenames>Guillaume</forenames></author><author><keyname>Denis</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Learning Negative Mixture Models by Tensor Decompositions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the problem of estimating the parameters of negative
mixture models, i.e. mixture models that possibly involve negative weights. The
contributions of this paper are as follows. (i) We show that every rational
probability distributions on strings, a representation which occurs naturally
in spectral learning, can be computed by a negative mixture of at most two
probabilistic automata (or HMMs). (ii) We propose a method to estimate the
parameters of negative mixture models having a specific tensor structure in
their low order observable moments. Building upon a recent paper on tensor
decompositions for learning latent variable models, we extend this work to the
broader setting of tensors having a symmetric decomposition with positive and
negative weights. We introduce a generalization of the tensor power method for
complex valued tensors, and establish theoretical convergence guarantees. (iii)
We show how our approach applies to negative Gaussian mixture models, for which
we provide some experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4232</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4232</id><created>2014-03-17</created><authors><author><keyname>Chakravorty</keyname><forenames>Tanushri</forenames></author><author><keyname>Bilodeau</keyname><forenames>Guillaume-Alexandre</forenames></author><author><keyname>Granger</keyname><forenames>Eric</forenames></author></authors><title>Automatic Image Registration in Infrared-Visible Videos using Polygon
  Vertices</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an automatic method is proposed to perform image registration
in visible and infrared pair of video sequences for multiple targets. In
multimodal image analysis like image fusion systems, color and IR sensors are
placed close to each other and capture a same scene simultaneously, but the
videos are not properly aligned by default because of different fields of view,
image capturing information, working principle and other camera specifications.
Because the scenes are usually not planar, alignment needs to be performed
continuously by extracting relevant common information. In this paper, we
approximate the shape of the targets by polygons and use affine transformation
for aligning the two video sequences. After background subtraction, keypoints
on the contour of the foreground blobs are detected using DCE (Discrete Curve
Evolution)technique. These keypoints are then described by the local shape at
each point of the obtained polygon. The keypoints are matched based on the
convexity of polygon's vertices and Euclidean distance between them. Only good
matches for each local shape polygon in a frame, are kept. To achieve a global
affine transformation that maximises the overlapping of infrared and visible
foreground pixels, the matched keypoints of each local shape polygon are stored
temporally in a buffer for a few number of frames. The matrix is evaluated at
each frame using the temporal buffer and the best matrix is selected, based on
an overlapping ratio criterion. Our experimental results demonstrate that this
method can provide highly accurate registered images and that we outperform a
previous related method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4238</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4238</id><created>2014-03-17</created><authors><author><keyname>Wang</keyname><forenames>Guohui</forenames></author><author><keyname>Xiong</keyname><forenames>Yingen</forenames></author><author><keyname>Yun</keyname><forenames>Jay</forenames></author><author><keyname>Cavallaro</keyname><forenames>Joseph R.</forenames></author></authors><title>Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU
  Co-Processing</title><categories>cs.DC cs.CV cs.MS</categories><comments>15 pages, 15 figures. Submitted and accepted for publication in
  Journal of Signal Processing Systems, 2014</comments><acm-class>C.1.4; H.5.1; I.5.4; C.1.2; D.1.3</acm-class><doi>10.1007/s11265-014-0878-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an OpenCL-based heterogeneous implementation of a
computer vision algorithm -- image inpainting-based object removal algorithm --
on mobile devices. To take advantage of the computation power of the mobile
processor, the algorithm workflow is partitioned between the CPU and the GPU
based on the profiling results on mobile devices, so that the
computationally-intensive kernels are accelerated by the mobile GPGPU
(general-purpose computing using graphics processing units). By exploring the
implementation trade-offs and utilizing the proposed optimization strategies at
different levels including algorithm optimization, parallelism optimization,
and memory access optimization, we significantly speed up the algorithm with
the CPU-GPU heterogeneous implementation, while preserving the quality of the
output images. Experimental results show that heterogeneous computing based on
GPGPU co-processing can significantly speed up the computer vision algorithms
and makes them practical on real-world mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4267</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4267</id><created>2014-03-17</created><updated>2014-03-19</updated><authors><author><keyname>Bilen</keyname><forenames>Cagdas</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Puy</keyname><forenames>Gilles</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author></authors><title>Balancing Sparsity and Rank Constraints in Quadratic Basis Pursuit</title><categories>cs.NA cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the methods that simultaneously enforce sparsity and low-rank
structure in a matrix as often employed for sparse phase retrieval problems or
phase calibration problems in compressive sensing. We propose a new approach
for analyzing the trade off between the sparsity and low rank constraints in
these approaches which not only helps to provide guidelines to adjust the
weights between the aforementioned constraints, but also enables new simulation
strategies for evaluating performance. We then provide simulation results for
phase retrieval and phase calibration cases both to demonstrate the consistency
of the proposed method with other approaches and to evaluate the change of
performance with different weights for the sparsity and low rank structure
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4289</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4289</id><created>2014-03-17</created><authors><author><keyname>Steiner</keyname><forenames>Thomas</forenames></author></authors><title>Telling Breaking News Stories from Wikipedia with Social Multimedia: A
  Case Study of the 2014 Winter Olympics</title><categories>cs.SI cs.CY cs.IR</categories><comments>Proceedings of the 1st International Workshop on Social Multimedia
  and Storytelling (SoMuS), co-located with the 4th International Conference on
  Multimedia Retrieval (ICMR '14), Glasgow, Scotland, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ability to watch Wikipedia and Wikidata edits in realtime, the
online encyclopedia and the knowledge base have become increasingly used
targets of research for the detection of breaking news events. In this paper,
we present a case study of the 2014 Winter Olympics, where we tell the story of
breaking news events in the context of the Olympics with the help of social
multimedia stemming from multiple social network sites. Therefore, we have
extended the application Wikipedia Live Monitor-a tool for the detection of
breaking news events-with the capability of automatically creating media
galleries that illustrate events. Athletes winning an Olympic competition, a
new country leading the medal table, or simply the Olympics themselves are all
events newsworthy enough for people to concurrently edit Wikipedia and
Wikidata-around the world in many languages. The Olympics being an event of
common interest, an even bigger majority of people share the event in a
multitude of languages on global social network sites, which makes the event an
ideal subject of study. With this work, we connect the world of Wikipedia and
Wikidata with the world of social network sites, in order to convey the spirit
of the 2014 Winter Olympics, to tell the story of victory and defeat, and
always following the Olympic motto Citius, Altius, Fortius. The proposed
system-generalized for all sort of breaking news stories-has been put in
production in form of the Twitter bot @mediagalleries, available and archived
at https://twitter.com/mediagalleries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4303</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4303</id><created>2014-03-17</created><authors><author><keyname>Li</keyname><forenames>Yongli</forenames></author><author><keyname>Luo</keyname><forenames>Peng</forenames></author><author><keyname>Wu</keyname><forenames>Chong</forenames></author></authors><title>A new network node similarity measure method and its applications</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Network node similarity measure has been paid particular attention in the
field of statistical physics. In this paper, we utilize the concept of
information and information loss to measure the node similarity. The whole
model is based on this idea that if two nodes are more similar than the others,
then the information loss of seeing them as the same is less. The present new
method has low algorithm complexity so that it can save much time and energy to
deal with the large scale real-world network. We illustrate the availability of
this approach based on two artificial examples and computer-generated networks
by comparing its accuracy with the other selected approaches. The above tests
demonstrate that the new method can provide more reasonable results consistent
with our human common judgment. The new similarity measure method is also
applied to predict the network evolution and predict unknown nodes'
attributions in the two application examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4311</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4311</id><created>2014-03-17</created><authors><author><keyname>Zhou</keyname><forenames>Heng</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>The lower bound of the PCM quantization error in high dimension</title><categories>math.NA cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we investigate the performance of the PCM scheme with linear
quantization rule for quantizing unit-norm tight frame expansions for ${\mathbb
R}^d$ without the White Noise Hypothesis. In \cite{WX}, Wang and Xu showed that
for asymptotically equidistributed unit-norm tight frame the PCM quantization
error has an upper bound ${\mathcal O}(\delta^{(d+1)/2})$ and they conjecture
the upper bound is sharp. In this note, we confirm the conjecture with
employing the asymptotic estimate of the Bessel functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4321</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4321</id><created>2014-03-17</created><authors><author><keyname>Minsky</keyname><forenames>Naftaly</forenames></author></authors><title>Dependable Management of Untrusted Distributed Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional approach to the online management of distributed
systems---represented by such standards as SNMP for network management, and
WSDM for systems based on service oriented computing (SOC)---relies on the
components of the managed system to cooperate in the management process, by
providing the managers with the means to monitor their state and activities,
and to control their behavior. Unfortunately, the trust thus placed in the
cooperation of the managed components is unwarranted for many types of
systems---such as systems based on SOA---making the conventional management of
such systems unreliable and insecure.
  This paper introduces a radically new approach to the management of
distributed systems, called governance-based management (GBM), which is based
on a middleware that can govern the exchange of messages between system
components. GBM has a substantial ability to manage distributed systems, in a
reliable and secure manner, even without any trustworthy cooperation of the
managed components.
  And it can fully incorporate the conventional management techniques wherever
such cooperation can be trusted. GBM also supports a reflexive mode of
management, which manages the management process itself, making it safer.
However, GBM is still a work in progress, as it raises several open problems
that needs to be addressed before this management technique can be put to
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4333</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4333</id><created>2014-03-17</created><authors><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Williamson</keyname><forenames>Adam R.</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Increasing Flash Memory Lifetime by Dynamic Voltage Allocation for
  Constant Mutual Information</title><categories>cs.IT math.IT</categories><comments>5 pages. 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The read channel in Flash memory systems degrades over time because the
Fowler-Nordheim tunneling used to apply charge to the floating gate eventually
compromises the integrity of the cell because of tunnel oxide degradation.
While degradation is commonly measured in the number of program/erase cycles
experienced by a cell, the degradation is proportional to the number of
electrons forced into the floating gate and later released by the erasing
process. By managing the amount of charge written to the floating gate to
maintain a constant read-channel mutual information, Flash lifetime can be
extended. This paper proposes an overall system approach based on information
theory to extend the lifetime of a flash memory device. Using the instantaneous
storage capacity of a noisy flash memory channel, our approach allocates the
read voltage of flash cell dynamically as it wears out gradually over time. A
practical estimation of the instantaneous capacity is also proposed based on
soft information via multiple reads of the memory cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4334</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4334</id><created>2014-03-17</created><updated>2014-06-11</updated><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author></authors><title>Bregman Divergences for Infinite Dimensional Covariance Matrices</title><categories>cs.CV</categories><comments>IEEE Conference on Computer Vision and Pattern Recognition (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach to computing and comparing Covariance Descriptors
(CovDs) in infinite-dimensional spaces. CovDs have become increasingly popular
to address classification problems in computer vision. While CovDs offer some
robustness to measurement variations, they also throw away part of the
information contained in the original data by only retaining the second-order
statistics over the measurements. Here, we propose to overcome this limitation
by first mapping the original data to a high-dimensional Hilbert space, and
only then compute the CovDs. We show that several Bregman divergences can be
computed between the resulting CovDs in Hilbert space via the use of kernels.
We then exploit these divergences for classification purposes. Our experiments
demonstrate the benefits of our approach on several tasks, such as material and
texture recognition, person re-identification, and action recognition from
motion capture data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4342</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4342</id><created>2014-03-18</created><updated>2014-05-11</updated><authors><author><keyname>Kwon</keyname><forenames>Taesoo</forenames></author><author><keyname>Choi</keyname><forenames>Ji-Woong</forenames></author></authors><title>Spatial Performance Analysis and Design Principles for Wireless Peer
  Discovery</title><categories>cs.IT cs.NI math.IT</categories><comments>12 pages (double columns), 10 figures, 1 table, to appear in the IEEE
  Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless peer-to-peer networks that serve various proximity-based
applications, peer discovery is the key to identifying other peers with which a
peer can communicate and an understanding of its performance is fundamental to
the design of an efficient discovery operation. This paper analyzes the
performance of wireless peer discovery through comprehensively considering the
wireless channel, spatial distribution of peers, and discovery operation
parameters. The average numbers of successfully discovered peers are expressed
in closed forms for two widely used channel models, i.e., the interference
limited Nakagami-m fading model and the Rayleigh fading model with nonzero
noise, when peers are spatially distributed according to a homogeneous Poisson
point process. These insightful expressions lead to the design principles for
the key operation parameters including the transmission probability, required
amount of wireless resources, level of modulation and coding scheme (MCS), and
transmit power. Furthermore, the impact of shadowing on the spatial performance
and suggested design principles is evaluated using mathematical analysis and
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4344</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4344</id><created>2014-03-18</created><authors><author><keyname>Li</keyname><forenames>Yangjia</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Debugging Quantum Processes Using Monitoring Measurements</title><categories>quant-ph cs.SE</categories><comments>7 pages. Comments are welcome</comments><doi>10.1103/PhysRevA.89.042338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since observation on a quantum system may cause the system state collapse, it
is usually hard to find a way to monitor a quantum process, which is a quantum
system that continuously evolves. We propose a protocol that can debug a
quantum process by monitoring, but not disturb the evolution of the system.
This protocol consists of an error detector and a debugging strategy. The
detector is a projection operator that is orthogonal to the anticipated system
state at a sequence of time points, and the strategy is used to specify these
time points. As an example, we show how to debug the computational process of
quantum search using this protocol. By applying the Skolem--Mahler--Lech
theorem in algebraic number theory, we find an algorithm to construct all of
the debugging protocols for quantum processes of time independent Hamiltonians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4346</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4346</id><created>2014-03-18</created><authors><author><keyname>Kwon</keyname><forenames>Taesoo</forenames></author></authors><title>Spatial Topology Adjustment for Minimizing Multicell Network Power
  Consumption</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the deployment of base stations (BSs) becomes increasingly dense in
order to accommodate the growth in traffic demand, these BSs may be
under-utilized during most hours except peak hours. Accordingly, the
deactivation of these under-utilized BSs is regarded as the key to reducing
network power consumption; however, the remaining active BSs should increase
their transmit power in order to fill network coverage holes that result from
BS switching off. This paper investigates the optimal balance between such
beneficial and harmful effects of BS switching off in terms of minimizing the
network power consumption, through comprehensively considering the spatial BS
distribution, BS transmit power, BS power consumption behaviors, radio
propagation environments, and frequency reuse. When BSs are deployed according
to a homogeneous Poisson point process, the suboptimal and approximated design
problems are formulated as geometric programming and the solutions lead to
insightful design principles for the key design parameters including the
spatial density, transmit power, and frequency reuse of remaining active BSs.
The numerical results demonstrate that these solutions are very close to the
optimal balances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4357</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4357</id><created>2014-03-18</created><authors><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>High Speed Railway Wireless Communications: Efficiency v.s. Fairness</title><categories>cs.IT math.IT</categories><comments>16 pages, 6 figures</comments><journal-ref>IEEE Trans. Veh. Technol., vol. 62, no. 2, 2014, pp: 925-930</journal-ref><doi>10.1109/TVT.2013.2281401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High speed railways (HSRs) have been deployed widely all over the world in
recent years. Different from traditional cellular communication, its high
mobility makes it essential to implement power allocation along the time. In
the HSR case, the transmission rate depends greatly on the distance between the
base station (BS) and the train. As a result, the train receives a time varying
data rate service when passing by a BS. It is clear that the most efficient
power allocation will spend all the power when the train is nearest from the
BS, which will cause great unfairness along the time. On the other hand, the
channel inversion allocation achieves the best fairness in terms of constant
rate transmission. However, its power efficiency is much lower. Therefore, the
power efficiency and the fairness along time are two incompatible objects. For
the HSR cellular system considered in this paper, a trade-off between the two
is achieved by proposing a temporal proportional fair power allocation scheme.
Besides, near optimal closed form solution and one algorithm finding the
$\epsilon$-optimal allocation are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4362</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4362</id><created>2014-03-18</created><authors><author><keyname>Abderrahim</keyname><forenames>Mohammed El Amine</forenames></author></authors><title>Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for
  Information Retrieval System</title><categories>cs.IR cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.3955 by
  other authors</comments><journal-ref>International Journal of Computational Linguistics Research, ISSN:
  0976-416X, Volume 4, Issue 4, December, 2013, Pages 149-158</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article evaluates the performance of two techniques for query
reformulation in a system for information retrieval, namely, the concept based
and the pseudo relevance feedback reformulation. The experiments performed on a
corpus of Arabic text have allowed us to compare the contribution of these two
reformulation techniques in improving the performance of an information
retrieval system for Arabic texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4374</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4374</id><created>2014-03-18</created><authors><author><keyname>Liu</keyname><forenames>Haijing</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Hu</keyname><forenames>Anzhong</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author></authors><title>Low-Complexity Transmission Mode Selection in MU-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>ICT2014(Accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a low-complexity transmission strategy in multi-user
multiple-input multiple-output downlink systems. The adaptive strategy adjusts
the precoding methods, denoted as the transmission mode, to improve the system
sum rates while maintaining the number of simultaneously served users. Three
linear precoding transmission modes are discussed, i.e., the block
diagonalization zero-forcing, the cooperative zero-forcing (CZF), and the
cooperative matched-filter (CMF). Considering both the number of data streams
and the multiple-antenna configuration of users, we modify the common CZF and
CMF modes by allocating data streams. Then, the transmission mode is selected
between the modified ones according to the asymptotic sum rate analyses. As
instantaneous channel state information is not needed for the mode selection,
the computational complexity is significantly reduced. Numerical simulations
confirm our analyses and demonstrate that the proposed scheme achieves
substantial performance gains with very low computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4378</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4378</id><created>2014-03-18</created><authors><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Adsul</keyname><forenames>Ajay P.</forenames></author><author><keyname>Vijayan</keyname><forenames>Aparna S.</forenames></author></authors><title>Spectral Clustering with Jensen-type kernels and their multi-point
  extensions</title><categories>cs.LG</categories><comments>To appear in IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by multi-distribution divergences, which originate in information
theory, we propose a notion of `multi-point' kernels, and study their
applications. We study a class of kernels based on Jensen type divergences and
show that these can be extended to measure similarity among multiple points. We
study tensor flattening methods and develop a multi-point (kernel) spectral
clustering (MSC) method. We further emphasize on a special case of the proposed
kernels, which is a multi-point extension of the linear (dot-product) kernel
and show the existence of cubic time tensor flattening algorithm in this case.
Finally, we illustrate the usefulness of our contributions using standard data
sets and image segmentation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4405</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4405</id><created>2014-03-18</created><authors><author><keyname>Gruner</keyname><forenames>Alexander</forenames></author><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>Absorbing Set Analysis and Design of LDPC Codes from Transversal Designs
  over the AWGN Channel</title><categories>cs.IT math.CO math.IT</categories><comments>15 pages. arXiv admin note: text overlap with arXiv:1306.5111</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct low-density parity-check (LDPC) codes from
transversal designs with low error-floors over the additive white Gaussian
noise (AWGN) channel. The constructed codes are based on transversal designs
that arise from sets of mutually orthogonal Latin squares (MOLS) with cyclic
structure. For lowering the error-floors, our approach is twofold: First, we
give an exhaustive classification of so-called absorbing sets that may occur in
the factor graphs of the given codes. These purely combinatorial substructures
are known to be the main cause of decoding errors in the error-floor region
over the AWGN channel by decoding with the standard sum-product algorithm
(SPA). Second, based on this classification, we exploit the specific structure
of the presented codes to eliminate the most harmful absorbing sets and derive
powerful constraints for the proper choice of code parameters in order to
obtain codes with an optimized error-floor performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4407</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4407</id><created>2014-03-18</created><updated>2015-02-01</updated><authors><author><keyname>Ghari</keyname><forenames>Meghdad</forenames></author></authors><title>A Note on Fixed Points in Justification Logics and the Surprise Test
  Paradox</title><categories>math.LO cs.LO</categories><comments>40 pages. In version 3, Mkrtychev models for QLP are added</comments><msc-class>03B42, 03B45, 03B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we study the effect of adding fixed points to justification
logics. We introduce two extensions of justification logics: extensions by
fixed point (or diagonal) operators, and extensions by least fixed points. The
former is a justification version of Smory\`nski's Diagonalization Operator
Logic, and the latter is a justification version of Kozen's modal
$\mu$-calculus. We also introduce fixed point extensions of Fitting's
quantified logic of proofs, and formalize the Knower Paradox and the Surprise
Test Paradox in these extensions. By interpreting a surprise statement as a
statement for which there is no justification, we give a solution to the
self-reference version of the Surprise Test Paradox in quantified logic of
proofs. We also give formalizations of the Surprise Test Paradox in timed modal
epistemic logics, and in G\&quot;odel-L\&quot;ob provability logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4415</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4415</id><created>2014-03-18</created><updated>2014-03-19</updated><authors><author><keyname>Preusse</keyname><forenames>Julia</forenames></author><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Thimm</keyname><forenames>Matthias</forenames></author><author><keyname>Sizov</keyname><forenames>Sergej</forenames></author></authors><title>DecLiNe -- Models for Decay of Links in Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prediction of graph evolution is an important and challenging problem in
the analysis of networks and of the Web in particular. But while the appearance
of new links is part of virtually every model of Web growth, the disappearance
of links has received much less attention in the literature. To fill this gap,
our approach DecLiNe (an acronym for DECay of LInks in NEtworks) aims to
predict link decay in networks, based on structural analysis of corresponding
graph models. In analogy to the link prediction problem, we show that analysis
of graph structures can help to identify indicators for superfluous links under
consideration of common network models. In doing so, we introduce novel metrics
that denote the likelihood of certain links in social graphs to remain in the
network, and combine them with state-of-the-art machine learning methods for
predicting link decay. Our methods are independent of the underlying network
type, and can be applied to such diverse networks as the Web, social networks
and any other structure representable as a network, and can be easily combined
with case-specific content analysis and adopted for a variety of social network
mining, filtering and recommendation applications. In systematic evaluations
with large-scale datasets of Wikipedia we show the practical feasibility of the
proposed structure-based link decay prediction algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4423</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4423</id><created>2014-03-18</created><authors><author><keyname>Musumbu</keyname><forenames>Kaninda</forenames><affiliation>LaBRI</affiliation></author></authors><title>Algorithms Visualization Tool for Students and Lectures in Computer
  Science</title><categories>cs.CY cs.HC</categories><proxy>ccsd</proxy><journal-ref>8th International Conference on e-Learning ICEL-2013 (ICEL 2013),
  Cape Town : South Africa (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The best way to understand complex data structures or algorithm is to see
them in action. The present work presents a new tool, especially useful for
students and lecturers in computer science. It is written in Java and developed
at Bordeaux University of Sciences and Technology. Its purposes is to help
students in understanding classical algorithms by illustrating them in
different ways: graphical (animated), formal, and descriptive. We think that it
can be useful to everyone interested in algorithms, in particular to students
in computer science that want to beef up their readings and university
lecturers in their major effort to enhance the data structures and algorithms
course. The main new thing of this tool is the fact of making it possible to
the user to animate their own algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4428</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4428</id><created>2014-03-18</created><updated>2016-02-03</updated><authors><author><keyname>Soodhalter</keyname><forenames>Kirk M.</forenames></author></authors><title>Two recursive GMRES-type methods for shifted linear systems with general
  preconditioning</title><categories>math.NA cs.NA</categories><comments>25 pages, 4 figures, 6 tables</comments><msc-class>65F10, 65F50, 65F08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two minimum residual methods for solving sequences of shifted
linear systems, the right-preconditioned shifted GMRES and shifted recycled
GMRES algorithms which use a seed projection strategy often employed to solve
multiple related problems. These methods are compatible with general
preconditioning of all systems, and when restricted to right preconditioning,
require no extra applications of the operator or preconditioner. These seed
projection methods perform a minimum residual iteration for the base system
while improving the approximations for the shifted systems at little additional
cost. The iteration continues until the base system approximation is of
satisfactory quality. The method is then recursively called for the remaining
unconverged systems. We present both methods inside of a general framework
which allows these techniques to be extended to the setting of flexible
preconditioning and inexact Krylov methods. We present some analysis of such
methods and numerical experiments demonstrating the effectiveness of the
algorithms we have derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4445</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4445</id><created>2014-03-18</created><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>A really simple approximation of smallest grammar</title><categories>cs.DS cs.FL</categories><comments>Accepted for CPM 2014</comments><acm-class>E.4; F.4.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a really simple linear-time algorithm constructing a
context-free grammar of size O(g log (N/g)) for the input string, where N is
the size of the input string and g the size of the optimal grammar generating
this string. The algorithm works for arbitrary size alphabets, but the running
time is linear assuming that the alphabet Sigma of the input string can be
identified with numbers from 1,ldots, N^c for some constant c. Algorithms with
such an approximation guarantee and running time are known, however all of them
were non-trivial and their analyses were involved. The here presented algorithm
computes the LZ77 factorisation and transforms it in phases to a grammar. In
each phase it maintains an LZ77-like factorisation of the word with at most l
factors as well as additional O(l) letters, where l was the size of the
original LZ77 factorisation. In one phase in a greedy way (by a left-to-right
sweep and a help of the factorisation) we choose a set of pairs of consecutive
letters to be replaced with new symbols, i.e. nonterminals of the constructed
grammar. We choose at least 2/3 of the letters in the word and there are O(l)
many different pairs among them. Hence there are O(log N) phases, each of them
introduces O(l) nonterminals to a grammar. A more precise analysis yields a
bound O(l log(N/l)). As l \leq g, this yields the desired bound O(g log(N/g)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4452</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4452</id><created>2014-03-18</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>The Homogeneous Weight Partition and its Character-Theoretic Dual</title><categories>cs.IT math.IT math.RA</categories><msc-class>94B05, 94B99, 16L60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The values of the normalized homogeneous weight are determined for arbitrary
finite Frobenius rings and expressed in a form that is independent from a
generating character and the M\&quot;obius function on the ring. The weight
naturally induces a partition of the ring, which is invariant under left or
right multiplication by units. It is shown that the character-theoretic
left-sided dual of this partition coincides with the right-sided dual, and even
more, the left- and right-sided Krawtchouk coefficients coincide. An example is
provided showing that this is not the case for general invariant partitions if
the ring is not semisimple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4462</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4462</id><created>2014-03-17</created><authors><author><keyname>Cichocki</keyname><forenames>A.</forenames></author><author><keyname>Mandic</keyname><forenames>D.</forenames></author><author><keyname>Phan</keyname><forenames>A-H.</forenames></author><author><keyname>Caiafa</keyname><forenames>C.</forenames></author><author><keyname>Zhou</keyname><forenames>G.</forenames></author><author><keyname>Zhao</keyname><forenames>Q.</forenames></author><author><keyname>De Lathauwer</keyname><forenames>L.</forenames></author></authors><title>Tensor Decompositions for Signal Processing Applications From Two-way to
  Multiway Component Analysis</title><categories>cs.NA</categories><doi>10.1109/MSP.2013.2297439</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread use of multi-sensor technology and the emergence of big
datasets has highlighted the limitations of standard flat-view matrix models
and the necessity to move towards more versatile data analysis tools. We show
that higher-order tensors (i.e., multiway arrays) enable such a fundamental
paradigm shift towards models that are essentially polynomial and whose
uniqueness, unlike the matrix methods, is guaranteed under verymild and natural
conditions. Benefiting fromthe power ofmultilinear algebra as theirmathematical
backbone, data analysis techniques using tensor decompositions are shown to
have great flexibility in the choice of constraints that match data properties,
and to find more general latent components in the data than matrix-based
methods. A comprehensive introduction to tensor decompositions is provided from
a signal processing perspective, starting from the algebraic foundations, via
basic Canonical Polyadic and Tucker models, through to advanced cause-effect
and multi-view data analysis schemes. We show that tensor decompositions enable
natural generalizations of some commonly used signal processing paradigms, such
as canonical correlation and subspace techniques, signal separation, linear
regression, feature extraction and classification. We also cover computational
aspects, and point out how ideas from compressed sensing and scientific
computing may be used for addressing the otherwise unmanageable storage and
manipulation problems associated with big datasets. The concepts are supported
by illustrative real world case studies illuminating the benefits of the tensor
framework, as efficient and promising tools for modern signal processing, data
analysis and machine learning applications; these benefits also extend to
vector/matrix data through tensorization. Keywords: ICA, NMF, CPD, Tucker
decomposition, HOSVD, tensor networks, Tensor Train.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4467</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4467</id><created>2014-03-18</created><updated>2014-06-25</updated><authors><author><keyname>Dubot</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Collet</keyname><forenames>Christophe</forenames></author></authors><title>A hybrid formalism to parse Sign Languages</title><categories>cs.CL</categories><comments>6 pages, 6 figures, Procedings of the 6th Workshop on the
  Representation and Processing of Sign Languages: Beyond the manual channel,
  Language Resources and Evaluation Conference (LREC) Reykjavik, Iceland, 31
  May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sign Language (SL) linguistic is dependent on the expensive task of
annotating. Some automation is already available for low-level information (eg.
body part tracking) and the lexical level has shown significant progresses. The
syntactic level lacks annotated corpora as well as complete and consistent
models. This article presents a solution for the automatic annotation of SL
syntactic elements. It exposes a formalism able to represent both
constituency-based and dependency-based models. The first enable the
representation the structures one may want to annotate, the second aims at
fulfilling the holes of the first. A parser is presented and used to conduct
two experiments on the solution. One experiment is on a real corpus, the other
is on a synthetic corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4469</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4469</id><created>2014-03-15</created><authors><author><keyname>Upadhyay</keyname><forenames>Rahul R</forenames></author></authors><title>Synchronous Relaying Of Sensor Data</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have put forth a novel methodology to relay data obtained by
inbuilt sensors of smart phones in real time to remote database followed by
fetching of this data . Smart phones are becoming very common and they are
laced with a number of sensors that can not only be used in native applications
but can also be sent to external nodes to be used by third parties for
application and service development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4473</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4473</id><created>2014-03-18</created><authors><author><keyname>Dubot</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Collet</keyname><forenames>Christophe</forenames></author></authors><title>Sign Language Gibberish for syntactic parsing evaluation</title><categories>cs.CL</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sign Language (SL) automatic processing slowly progresses bottom-up. The
field has seen proposition to handle the video signal, to recognize and
synthesize sublexical and lexical units. It starts to see the development of
supra-lexical processing. But the recognition, at this level, lacks data. The
syntax of SL appears very specific as it uses massively the multiplicity of
articulators and its access to the spatial dimensions. Therefore new parsing
techniques are developed. However these need to be evaluated. The shortage on
real data restrains the corpus-based models to small sizes. We propose here a
solution to produce data-sets for the evaluation of parsers on the specific
properties of SL. The article first describes the general model used to
generates dependency grammars and the phrase generation from these lasts. It
then discusses the limits of approach. The solution shows to be of particular
interest to evaluate the scalability of the techniques on big models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4482</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4482</id><created>2014-03-18</created><authors><author><keyname>Hu</keyname><forenames>Pili</forenames></author><author><keyname>Fan</keyname><forenames>Qijiang</forenames></author><author><keyname>Lau</keyname><forenames>Wing Cheong</forenames></author></authors><title>SNSAPI: A Cross-Platform Middleware for Rapid Deployment of
  Decentralized Social Networks</title><categories>cs.SI cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the design, implementation and our year-long
maintenance experience of SNSAPI, a Python-based middleware which unifies the
interfaces and data structures of heterogeneous Social Networking Services
(SNS). Unlike most prior works, our middleware is user-oriented and requires
zero infrastructure support. It enables a user to readily conduct online social
activities in a programmable, cross-platform fashion while gradually reducing
the dependence on centralized Online Social Networks (OSN). More importantly,
as the SNSAPI middleware can be used to support decentralized social networking
services via conventional communication channels such as RSS or Email, it
enables the deployment of Decentralized Social Networks (DSN) in an
incremental, ad hoc manner. To demonstrate the viability of such type of DSNs,
we have deployed an experimental 6000-node SNSAPI-based DSN on PlanetLab and
evaluate its performance by replaying traces of online social activities
collected from a mainstream OSN. Our results show that, with only mild resource
consumption, the SNSAPI-based DSN can achieve acceptable forwarding latency
comparable to that of a centralized OSN. We also develop an analytical model to
characterize the trade-offs between resource consumption and message forwarding
delay in our DSN. Via 20 parameterized experiments on PlanetLab, we have found
that the empirical measurement results match reasonably with the performance
predicted by our analytical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4492</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4492</id><created>2014-03-18</created><authors><author><keyname>Sun</keyname><forenames>Qian</forenames></author><author><keyname>Zhu</keyname><forenames>Gang</forenames></author><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Xuan</forenames></author><author><keyname>Zhong</keyname><forenames>Zhangdui</forenames></author></authors><title>Joint Beamforming Design and Time Allocation for Wireless Powered
  Communication Networks</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures, submitted to IEEE Communications Letters</comments><journal-ref>IEEE Communications Letters, vol.18, no.10, pp.1783-1786, Oct.
  2014</journal-ref><doi>10.1109/LCOMM.2014.2347958</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a multi-input single-output (MISO) wireless powered
communication network (WPCN) under the protocol of harvest-then-transmit. The
power station (PS) with reliable power supply can replenish the passive user
nodes by wireless power transfer (WPT) in the downlink (DL), then each user
node transmits independent information to the sink by a time division multiple
access (TDMA) scheme in the uplink (UL). We consider the joint time allocation
and beamforming design to maximize the system sum-throughput. The semidefinite
relaxation (SDR) technique is applied to solve the nonconvex design problem.
The tightness of SDR approximation, thus the global optimality, is proved. This
implies that only one single energy beamformer is required at the PS. Then a
fast semiclosed form solution is proposed by exploiting the inherent structure.
Simulation results demonstrate the efficiency of the proposed algorithms from
the perspectives of time complexity and information throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4503</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4503</id><created>2014-03-18</created><updated>2016-02-06</updated><authors><author><keyname>Fowkes</keyname><forenames>Jaroslav</forenames></author><author><keyname>Ranca</keyname><forenames>Razvan</forenames></author><author><keyname>Allamanis</keyname><forenames>Miltiadis</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Autofolding for Source Code Summarization</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developers spend much of their time reading and browsing source code, raising
new opportunities for summarization methods. Indeed, modern code editors
provide code folding, which allows one to selectively hide blocks of code.
However this is impractical to use as folding decisions must be made manually
or based on simple rules. We introduce the autofolding problem, which is to
automatically create a code summary by folding less informative code regions.
We present a novel solution by formulating it as a subtree optimization
problem, leveraging a scoped topic model for code tokens. On an annotated set
of popular open source projects, we show that our summarizer outperforms
simpler baselines, yielding a 28% error reduction. Furthermore, we find through
a case study that our summarizer is strongly preferred by experienced
developers. More broadly, we hope this work will aid program comprehension by
turning code folding into a usable and valuable tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4508</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4508</id><created>2014-01-29</created><authors><author><keyname>Suciu</keyname><forenames>Lenuta</forenames></author><author><keyname>Cziple</keyname><forenames>Florentina</forenames></author><author><keyname>Anghel</keyname><forenames>Cornelia</forenames></author></authors><title>Research on Study Mechanical Vibrations with Data Acquisition Systems</title><categories>cs.CE</categories><comments>AGIR Bucuresti Press, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new study method of mechanic vibrations with the help of
the data acquisition systems. The study of vibrations with the help of data
acquisition systems allows the solving of some engineering problems connected
to the measurement of some parameters which are difficult to measure having in
view the improvement of the technical performances of the industrial equipment
or devices
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4512</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4512</id><created>2013-11-13</created><authors><author><keyname>Vieira</keyname><forenames>Vilson</forenames></author><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Sbrissa</keyname><forenames>David</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author></authors><title>A Quantitative Approach to Painting Styles</title><categories>stat.AP cs.OH</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This research extends a method previously applied to music and
philosophy,representing the evolution of art as a time-series where relations
like dialectics are measured quantitatively. For that, a corpus of paintings of
12 well-known artists from baroque and modern art is analyzed. A set of 93
features is extracted and the features which most contributed to the
classification of painters are selected. The projection space obtained provides
the basis to the analysis of measurements. This quantitative measures underlie
revealing observations about the evolution of painting styles, specially when
compared with other humanity fields already analyzed: while music evolved along
a master-apprentice tradition (high dialectics) and philosophy by opposition,
painting presents another pattern: constant increasing skewness, low opposition
between members of the same movement and opposition peaks in the transition
between movements. Differences between baroque and modern movements are also
observed in the projected &quot;painting space&quot;: while baroque paintings are
presented as an overlapped cluster, the modern paintings present minor
overlapping and are disposed more widely in the projection than the baroque
counterparts. This finding suggests that baroque painters shared aesthetics
while modern painters tend to &quot;break rules&quot; and develop their own style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4513</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4513</id><created>2013-11-13</created><authors><author><keyname>Vieira</keyname><forenames>Vilson</forenames></author><author><keyname>Fabbri</keyname><forenames>Renato</forenames></author><author><keyname>Travieso</keyname><forenames>Gonzalo</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>A quantitative approach to evolution of music and philosophy</title><categories>stat.AP cs.SD</categories><comments>arXiv admin note: substantial text overlap with arXiv:1109.4653</comments><msc-class>62A01</msc-class><journal-ref>J. Stat. Mech. (2012) P08010</journal-ref><doi>10.1088/1742-5468/2012/08/P08010</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The development of new statistical and computational methods is increasingly
making it possible to bridge the gap between hard sciences and humanities. In
this study, we propose an approach based on a quantitative evaluation of
attributes of objects in fields of humanities, from which concepts such as
dialectics and opposition are formally defined mathematically. As case studies,
we analyzed the temporal evolution of classical music and philosophy by
obtaining data for 8 features characterizing the corresponding fields for 7
well-known composers and philosophers, which were treated with multivariate
statistics and pattern recognition methods. A bootstrap method was applied to
avoid statistical bias caused by the small sample data set, with which hundreds
of artificial composers and philosophers were generated, influenced by the 7
names originally chosen. Upon defining indices for opposition, skewness and
counter-dialectics, we confirmed the intuitive analysis of historians in that
classical music evolved according to a master-apprentice tradition, while in
philosophy changes were driven by opposition. Though these case studies were
meant only to show the possibility of treating phenomena in humanities
quantitatively, including a quantitative measure of concepts such as dialectics
and opposition the results are encouraging for further application of the
approach presented here to many other areas, since it is entirely generic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4514</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4514</id><created>2014-03-18</created><updated>2014-03-31</updated><authors><author><keyname>Fonteneau</keyname><forenames>Raphael</forenames></author><author><keyname>Prashanth</keyname><forenames>L. A.</forenames></author></authors><title>Simultaneous Perturbation Algorithms for Batch Off-Policy Search</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose novel policy search algorithms in the context of off-policy, batch
mode reinforcement learning (RL) with continuous state and action spaces. Given
a batch collection of trajectories, we perform off-line policy evaluation using
an algorithm similar to that by [Fonteneau et al., 2010]. Using this
Monte-Carlo like policy evaluator, we perform policy search in a class of
parameterized policies. We propose both first order policy gradient and second
order policy Newton algorithms. All our algorithms incorporate simultaneous
perturbation estimates for the gradient as well as the Hessian of the
cost-to-go vector, since the latter is unknown and only biased estimates are
available. We demonstrate their practicality on a simple 1-dimensional
continuous state space problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4521</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4521</id><created>2014-03-18</created><updated>2014-05-20</updated><authors><author><keyname>Atwood</keyname><forenames>James</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Efficient Network Generation Under General Preferential Attachment</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>James Atwood, Bruno Ribeiro, Don Towsley, Efficient Network
  Generation Under General Preferential Attachment, SIMPLEX 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preferential attachment (PA) models of network structure are widely used due
to their explanatory power and conceptual simplicity. PA models are able to
account for the scale-free degree distributions observed in many real-world
large networks through the remarkably simple mechanism of sequentially
introducing nodes that attach preferentially to high-degree nodes. The ability
to efficiently generate instances from PA models is a key asset in
understanding both the models themselves and the real networks that they
represent. Surprisingly, little attention has been paid to the problem of
efficient instance generation. In this paper, we show that the complexity of
generating network instances from a PA model depends on the preference function
of the model, provide efficient data structures that work under any preference
function, and present empirical results from an implementation based on these
data structures. We demonstrate that, by indexing growing networks with a
simple augmented heap, we can implement a network generator which scales many
orders of magnitude beyond existing capabilities ($10^6$ -- $10^8$ nodes). We
show the utility of an efficient and general PA network generator by
investigating the consequences of varying the preference functions of an
existing model. We also provide &quot;quicknet&quot;, a freely-available open-source
implementation of the methods described in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4523</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4523</id><created>2014-03-18</created><authors><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author></authors><title>Connectivity in Dense Networks Confined within Right Prisms</title><categories>cs.NI</categories><comments>8 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1201.4013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the probability that a dense wireless network confined within a
given convex geometry is fully connected. We exploit a recently reported theory
to develop a systematic methodology for analytically characterizing the
connectivity probability when the network resides within a convex right prism,
a polyhedron that accurately models many geometries that can be found in
practice. To maximize practicality and applicability, we adopt a general
point-to-point link model based on outage probability, and present example
analytical and numerical results for a network employing $2 \times 2$
multiple-input multiple-output (MIMO) maximum ratio combining (MRC) link level
transmission confined within particular bounding geometries. Furthermore, we
provide suggestions for extending the approach detailed herein to more general
convex geometries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4539</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4539</id><created>2014-02-27</created><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Sets Have Simple Members. (repost)</title><categories>cs.CC</categories><comments>This article replaces arXiv:1107.1458 due to co-author's decision to
  withdraw</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combined Universal Probability M(D) of strings x in sets D is close to
max M({x}) over x in D: their ~logs differ by at most D's information j=I(D:H)
about the halting sequence H. Thus if all x have complexity K(x) &gt;k, D carries
&gt;i bits of information on each its x where i+j ~ k. Note that there are no ways
to generate D with significant I(D:H).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4540</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4540</id><created>2014-03-18</created><authors><author><keyname>Belanche</keyname><forenames>Llu&#xed;s</forenames></author><author><keyname>Hern&#xe1;ndez</keyname><forenames>Jer&#xf3;nimo</forenames></author></authors><title>Similarity networks for classification: a case study in the Horse Colic
  problem</title><categories>cs.LG cs.NE</categories><comments>16 pages, 1 figure Universitat Polit\`ecnica de Catalunya preprint</comments><report-no>Technical Report LSI-14-4-R</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a two-layer neural network in which the neuron model
computes a user-defined similarity function between inputs and weights. The
neuron transfer function is formed by composition of an adapted logistic
function with the mean of the partial input-weight similarities. The resulting
neuron model is capable of dealing directly with variables of potentially
different nature (continuous, fuzzy, ordinal, categorical). There is also
provision for missing values. The network is trained using a two-stage
procedure very similar to that used to train a radial basis function (RBF)
neural network. The network is compared to two types of RBF networks in a
non-trivial dataset: the Horse Colic problem, taken as a case study and
analyzed in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4554</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4554</id><created>2014-03-18</created><authors><author><keyname>Sharifi</keyname><forenames>Fazel</forenames></author><author><keyname>Amanollahi</keyname><forenames>Saba</forenames></author><author><keyname>Taherkhani</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Hashemipour</keyname><forenames>Omid</forenames></author></authors><title>A Flexible Design for Optimization of Hardware Architecture in
  Distributed Arithmetic based FIR Filters</title><categories>cs.AR</categories><journal-ref>RadioElectronics &amp; Informatics 4 (2012) 25-30</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FIR filters are used in many performance/power critical applications such as
mobile communication devices, analogue to digital converters and digital signal
processing applications. Design of appropriate FIR filters usually causes the
order of filter to be increased. Synthesis and tape-out of high-order FIR
filters with reasonable delay, area and power has become an important challenge
for hardware designers. In many cases the complexity of high-order filters
causes the constraints of the total design could not be satisfied. In this
paper, efficient hardware architecture is proposed for distributed arithmetic
(DA) based FIR filters. The architecture is based on optimized combination of
Look-up Tables (LUTs) and compressors. The optimized system level solution is
obtained from a set of dynamic programming optimization algorithms. The
experiments show the proposed design educed the delay cost between 16%-62.5% in
comparison of previous optimized structures for DA-based architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4583</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4583</id><created>2014-03-18</created><updated>2015-01-12</updated><authors><author><keyname>Padakandla</keyname><forenames>Arun</forenames></author><author><keyname>Sahebi</keyname><forenames>Aria G.</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>An Achievable rate region for the $3-$user interference channel based on
  coset codes</title><categories>cs.IT math.IT</categories><comments>New examples for which coset codes yield strictly larger achievable
  rate regions in comparison to those achievable using unstructured iid codes
  are identified. The issue of aligning interference at multiple receiver
  terminals addressed through an example. Revised submission to IEEE Trans. on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communication over a three user discrete
memoryless interference channel ($3-$IC). The current known coding techniques
for communicating over an arbitrary $3-$IC are based on message splitting,
superposition coding and binning using independent and identically distributed
(iid) random codebooks. In this work, we propose a new ensemble of codes -
partitioned coset codes (PCC) - that possess an appropriate mix of empirical
and algebraic closure properties. We develop coding techniques that exploit
algebraic closure property of PCC to enable efficient communication over
$3-$IC. We analyze the performance of the proposed coding technique to derive
an achievable rate region for the general discrete $3-$IC. Additive and
non-additive examples are identified for which the derived achievable rate
region is the capacity, and moreover, strictly larger than current known
largest achievable rate regions based on iid random codebooks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4597</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4597</id><created>2014-03-17</created><updated>2015-03-25</updated><authors><author><keyname>Nan</keyname><forenames>Zheng</forenames></author><author><keyname>Chen</keyname><forenames>Tianyi</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Ni</keyname><forenames>Wei</forenames></author></authors><title>Energy-Efficient Transmission Schedule for Delay-Limited Bursty Data
  Arrivals under Non-Ideal Circuit Power Consumption</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel approach to obtaining energy-efficient
transmission schedules for delay-limited bursty data arrivals under non-ideal
circuit power consumption. Assuming a-prior knowledge of packet arrivals,
deadlines and channel realizations, we show that the problem can be formulated
as a convex program. For both time-invariant and time-varying fading channels,
it is revealed that the optimal transmission between any two consecutive
channel or data state changing instants, termed epoch, can only take one of the
three strategies: (i) no transmission, (ii) transmission with an
energy-efficiency (EE) maximizing rate over part of the epoch, or (iii)
transmission with a rate greater than the EE-maximizing rate over the whole
epoch. Based on this specific structure, efficient algorithms are then
developed to find the optimal policies that minimize the total energy
consumption with a low computational complexity. The proposed approach can
provide the optimal benchmarks for practical schemes designed for transmissions
of delay-limited data arrivals, and can be employed to develop efficient online
scheduling schemes which require only causal knowledge of data arrivals and
deadline requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4608</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4608</id><created>2014-03-18</created><authors><author><keyname>Cheng</keyname><forenames>Justin</forenames></author><author><keyname>Adamic</keyname><forenames>Lada A.</forenames></author><author><keyname>Dow</keyname><forenames>P. Alex</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Can Cascades be Predicted?</title><categories>cs.SI physics.soc-ph stat.ML</categories><acm-class>H.2.8</acm-class><doi>10.1145/2566486.2567997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On many social networking web sites such as Facebook and Twitter, resharing
or reposting functionality allows users to share others' content with their own
friends or followers. As content is reshared from user to user, large cascades
of reshares can form. While a growing body of research has focused on analyzing
and characterizing such cascades, a recent, parallel line of work has argued
that the future trajectory of a cascade may be inherently unpredictable. In
this work, we develop a framework for addressing cascade prediction problems.
On a large sample of photo reshare cascades on Facebook, we find strong
performance in predicting whether a cascade will continue to grow in the
future. We find that the relative growth of a cascade becomes more predictable
as we observe more of its reshares, that temporal and structural features are
key predictors of cascade size, and that initially, breadth, rather than depth
in a cascade is a better indicator of larger cascades. This prediction
performance is robust in the sense that multiple distinct classes of features
all achieve similar performance. We also discover that temporal features are
predictive of a cascade's eventual shape. Observing independent cascades of the
same content, we find that while these cascades differ greatly in size, we are
still able to predict which ends up the largest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4622</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4622</id><created>2014-03-18</created><updated>2014-12-08</updated><authors><author><keyname>Kalka</keyname><forenames>Arkadius</forenames></author><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author><author><keyname>Vinokur</keyname><forenames>Gary</forenames></author></authors><title>Complete simultaneous conjugacy invariants in Garside groups</title><categories>math.GR cs.CC cs.CR</categories><comments>Major revision: High-dimensional Cycling Theorem identified and
  proved. The proof is included in the appendix. Major stylistic revisions make
  take place prior to submission</comments><msc-class>20F36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the simultaneous conjugacy problem in Garside groups, by means of an
effectively computable invariant. In the one-dimensional case, our invariant
generalizes the notion of super summit set of a conjugacy class. As part of our
solution, we identify a high-dimensional version of the cyclic sliding
operation with a provable convergence rate.
  The complexity of this solution is a small degree polynomial in the sizes of
our generalized super summit sets and the input parameters. Computer
experiments suggest that, in Artin's braid group $\BG{N}$, the size of this
invariant for dimension of order $N$ or higher, is generically close to 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4628</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4628</id><created>2014-03-18</created><authors><author><keyname>Basu</keyname><forenames>Amitabh</forenames></author><author><keyname>Hildebrand</keyname><forenames>Robert</forenames></author><author><keyname>K&#xf6;ppe</keyname><forenames>Matthias</forenames></author></authors><title>Equivariant Perturbation in Gomory and Johnson's Infinite Group Problem.
  III. Foundations for the k-Dimensional Case with Applications to k=2</title><categories>math.OC cs.DM math.CO</categories><comments>41 pages, 9 figures</comments><msc-class>90C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop foundational tools for classifying the extreme valid functions for
the k-dimensional infinite group problem. In particular, (1) we present the
general regular solution to Cauchy's additive functional equation on bounded
convex domains. This provides a k-dimensional generalization of the so-called
interval lemma, allowing us to deduce affine properties of the function from
certain additivity relations. (2) We study the discrete geometry of additivity
domains of piecewise linear functions, providing a framework for finite tests
of minimality and extremality. (3) We give a theory of non-extremality
certificates in the form of perturbation functions.
  We apply these tools in the context of minimal valid functions for the
two-dimensional infinite group problem that are piecewise linear on a standard
triangulation of the plane, under the assumption of a regularity condition
called diagonal constrainedness. We show that the extremality of a minimal
valid function is equivalent to the extremality of its restriction to a certain
finite two-dimensional group problem. This gives an algorithm for testing the
extremality of a given minimal valid function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4637</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4637</id><created>2014-03-18</created><updated>2014-04-01</updated><authors><author><keyname>Liu</keyname><forenames>Xiaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Hongwei</forenames></author></authors><title>A Maximal Concurrency and Low Latency Distributed Scheduling Protocol
  for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>final version; 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing work that schedules concurrent transmissions collision-free suffers
from low channel utilization. We propose the Optimal Node Activation Multiple
Access (ONAMA) protocol to achieve maximal channel spatial reuse through a
distributed maximal independent set (DMIS) algorithm. To overcome DMIS's
excessive delay in finding a maximal independent set, we devise a novel
technique called pipelined precomputation that decouples DMIS from data
transmission. We implement ONAMA on resource-constrained TelosB motes using
TinyOS. Extensive measurements on two testbeds independently attest to ONAMA's
superb performance compared to existing work: improving concurrency,
throughput, and delay by a factor of 3.7, 3.0, and 5.3, respectively, while
still maintaining reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4640</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4640</id><created>2014-03-18</created><updated>2014-04-16</updated><authors><author><keyname>Gillani</keyname><forenames>Nabeel</forenames></author><author><keyname>Eynon</keyname><forenames>Rebecca</forenames></author><author><keyname>Osborne</keyname><forenames>Michael</forenames></author><author><keyname>Hjorth</keyname><forenames>Isis</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen</forenames></author></authors><title>Communication Communities in MOOCs</title><categories>cs.CY cs.SI stat.ML</categories><comments>10 pages, 3 figures, 1 table. Submitted for review to UAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive Open Online Courses (MOOCs) bring together thousands of people from
different geographies and demographic backgrounds -- but to date, little is
known about how they learn or communicate. We introduce a new content-analysed
MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to
extract communities of learners based on the nature of their online forum
posts. We see that BNMF yields a superior probabilistic generative model for
online discussions when compared to other models, and that the communities it
learns are differentiated by their composite students' demographic and course
performance indicators. These findings suggest that computationally efficient
probabilistic generative modelling of MOOCs can reveal important insights for
educational researchers and practitioners and help to develop more intelligent
and responsive online learning environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4643</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4643</id><created>2014-03-18</created><updated>2016-03-07</updated><authors><author><keyname>Czekaj</keyname><forenames>L.</forenames></author><author><keyname>Horodecki</keyname><forenames>M.</forenames></author><author><keyname>Horodecki</keyname><forenames>P.</forenames></author><author><keyname>Horodecki</keyname><forenames>R.</forenames></author></authors><title>Information Content of Elementary Systems as a Physical Principle</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages + 5 pages of appendix, 4 figures, significant new content,
  numerical anlysis and new figures added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To explain conceptual gap between classical/quantum and other, hypothetical
descriptions of world, several principles has been proposed. So far, all these
principles have not involved uncertainty concept. Here we introduce an
information content principle (ICP) which represents the new - constrained
uncertainty principle. The principle, by taking into account the
encoding/decoding properties of physical system, is capable of separation both
classicality and quanta from a number of potential physical theories including
hidden variable theories. The ICP, which is satisfied by both classical and
quantum theory, states that the amount of non-redundant information which may
be extracted from a given system is bounded by a perfectly decodable
information content of the system. We show that ICP allows to discriminate
theories which do not allow for correlations stronger than Tsirelson's bound.
We show also how to apply the principle to composite systems, ruling out some
theories despite their elementary constituents behave quantumly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4655</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4655</id><created>2014-03-18</created><authors><author><keyname>Drmac</keyname><forenames>Zlatko</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author><author><keyname>Beattie</keyname><forenames>Christopher</forenames></author></authors><title>Quadrature-Based Vector Fitting: Implications For H2 System
  Approximation</title><categories>math.NA cs.SY</categories><msc-class>34C20, 41A05, 49K15, 49M05, 93A15, 93C05, 93C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector Fitting is a popular method of constructing rational approximants
designed to fit given frequency response measurements. The original method,
which we refer to as VF, is based on a least-squares fit to the measurements by
a rational function, using an iterative reallocation of the poles of the
approximant. We show that one can improve the performance of VF significantly,
by using a particular choice of frequency sampling points and properly
weighting their contribution based on quadrature rules that connect the least
squares objective with an H2 error measure. Our modified approach, designated
here as QuadVF, helps recover the original transfer function with better global
fidelity (as measured with respect to the H2 norm), than the localized least
squares approximation implicit in VF. We extend the new framework also to
incorporate derivative information, leading to rational approximants that
minimize system error with respect to a discrete Sobolev norm. We consider the
convergence behavior of both VF and QuadVF as well, and evaluate potential
numerical ill-conditioning of the underlying least-squares problems. We
investigate briefly VF in the case of noisy measurements and propose a new
formulation for the resulting approximation problem. Several numerical examples
are provided to support the theoretical discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4661</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4661</id><created>2014-03-18</created><updated>2014-07-23</updated><authors><author><keyname>Khalid</keyname><forenames>Zubair</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author></authors><title>An Optimal-Dimensionality Sampling Scheme on the Sphere with Fast
  Spherical Harmonic Transforms</title><categories>cs.IT math.IT</categories><comments>14 pages, 11 figures</comments><doi>10.1109/TSP.2014.2337278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a sampling scheme on the sphere that permits accurate computation
of the spherical harmonic transform and its inverse for signals band-limited at
$L$ using only $L^2$ samples. We obtain the optimal number of samples given by
the degrees of freedom of the signal in harmonic space. The number of samples
required in our scheme is a factor of two or four fewer than existing
techniques, which require either $2L^2$ or $4L^2$ samples. We note, however,
that we do not recover a sampling theorem on the sphere, where spherical
harmonic transforms are theoretically exact. Nevertheless, we achieve high
accuracy even for very large band-limits. For our optimal-dimensionality
sampling scheme, we develop a fast and accurate algorithm to compute the
spherical harmonic transform (and inverse), with computational complexity
comparable with existing schemes in practice. We conduct numerical experiments
to study in detail the stability, accuracy and computational complexity of the
proposed transforms. We also highlight the advantages of the proposed sampling
scheme and associated transforms in the context of potential applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4662</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4662</id><created>2014-03-18</created><updated>2014-07-26</updated><authors><author><keyname>Dobbs</keyname><forenames>Justin R.</forenames></author><author><keyname>Hencey</keyname><forenames>Brandon M.</forenames></author></authors><title>Model Predictive HVAC Control with Online Occupancy Model</title><categories>cs.SY</categories><comments>Energy and Buildings preprint, submitted July 2014, 27 pages, 13
  figures</comments><doi>10.1016/j.enbuild.2014.07.051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an occupancy-predicting control algorithm for heating,
ventilation, and air conditioning (HVAC) systems in buildings. It incorporates
the building's thermal properties, local weather predictions, and a self-tuning
stochastic occupancy model to reduce energy consumption while maintaining
occupant comfort. Contrasting with existing approaches, the occupancy model
requires no manual training and adapts to changes in occupancy patterns during
operation. A prediction-weighted cost function provides conditioning of thermal
zones before occupancy begins and reduces system output before occupancy ends.
Simulation results with real-world occupancy data demonstrate the algorithm's
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4669</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4669</id><created>2014-03-18</created><updated>2014-11-07</updated><authors><author><keyname>Guo</keyname><forenames>Jing</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author></authors><title>Performance Analysis of Arbitrarily-Shaped Underlay Cognitive Networks:
  Effects of Secondary User Activity Protocols</title><categories>cs.IT math.IT</categories><comments>submitted to possible IEEE Transactions publication</comments><journal-ref>IEEE Transactions on Communications, vol. 63, no. 2, pp. 376-389,
  Feb. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the performance of the primary and secondary users (SUs)
in an arbitrarily-shaped underlay cognitive network. In order to meet the
interference threshold requirement for a primary receiver (PU-Rx) at an
arbitrary location, we consider different SU activity protocols which limit the
number of active SUs. We propose a framework, based on the moment generating
function (MGF) of the interference due to a random SU, to analytically compute
the outage probability in the primary network, as well as the average number of
active SUs in the secondary network. We also propose a cooperation-based SU
activity protocol in the underlay cognitive network which includes the existing
threshold-based protocol as a special case. We study the average number of
active SUs for the different SU activity protocols, subject to a given outage
probability constraint at the PU and we employ it as an analytical approach to
compare the effect of different SU activity protocols on the performance of the
primary and secondary networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4670</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4670</id><created>2014-03-18</created><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Quantized Output Feedback Stabilization of Switched Linear Systems</title><categories>cs.SY math.OC</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of stabilizing a continuous-time switched
linear system by quantized output feedback. We assume that the quantized
outputs and the switching signal are available to the controller at all time.
We develop an encoding strategy by using multiple Lyapunov functions and an
average dwell time property. The encoding strategy is based on the results in
the case of a single mode, and it requires an additional adjustment of the
&quot;zoom&quot; parameter at every switching time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4677</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4677</id><created>2014-03-18</created><authors><author><keyname>Bild</keyname><forenames>David R.</forenames></author><author><keyname>Liu</keyname><forenames>Yue</forenames></author><author><keyname>Dick</keyname><forenames>Robert P.</forenames></author><author><keyname>Mao</keyname><forenames>Z. Morley</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>Performance Analysis of Location Profile Routing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose using the predictability of human motion to eliminate the overhead
of distributed location services in human-carried MANETs, dubbing the technique
location profile routing. This method outperforms the Geographic Hashing
Location Service when nodes change locations 2x more frequently than they
initiate connections (e.g., start new TCP streams), as in applications like
text- and instant-messaging. Prior characterizations of human mobility are used
to show that location profile routing achieves a 93% delivery ratio with a
1.75x first-packet latency increase relative to an oracle location service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4679</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4679</id><created>2014-03-18</created><updated>2015-12-22</updated><authors><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Justification of Logarithmic Loss via the Benefit of Side Information</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, Vol.61, Issue 10, pp 5357
  - 5365, Oct. 2015</journal-ref><doi>10.1109/TIT.2015.2462848</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a natural measure of relevance: the reduction in optimal
prediction risk in the presence of side information. For any given loss
function, this relevance measure captures the benefit of side information for
performing inference on a random variable under this loss function. When such a
measure satisfies a natural data processing property, and the random variable
of interest has alphabet size greater than two, we show that it is uniquely
characterized by the mutual information, and the corresponding loss function
coincides with logarithmic loss. In doing so, our work provides a new
characterization of mutual information, and justifies its use as a measure of
relevance. When the alphabet is binary, we characterize the only admissible
forms the measure of relevance can assume while obeying the specified data
processing property. Our results naturally extend to measuring causal influence
between stochastic processes, where we unify different causal-inference
measures in the literature as instantiations of directed information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4682</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4682</id><created>2014-03-18</created><authors><author><keyname>Zhu</keyname><forenames>Feiyun</forenames></author><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Fan</keyname><forenames>Bin</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>Structured Sparse Method for Hyperspectral Unmixing</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral Unmixing (HU) has received increasing attention in the past
decades due to its ability of unveiling information latent in hyperspectral
data. Unfortunately, most existing methods fail to take advantage of the
spatial information in data. To overcome this limitation, we propose a
Structured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) method
from the following two aspects. First, we incorporate a graph Laplacian to
encode the manifold structures embedded in the hyperspectral data space. In
this way, the highly similar neighboring pixels can be grouped together.
Second, the lasso penalty is employed in SS-NMF for the fact that pixels in the
same manifold structure are sparsely mixed by a common set of relevant bases.
These two factors act as a new structured sparse constraint. With this
constraint, our method can learn a compact space, where highly similar pixels
are grouped to share correlated sparse representations. Experiments on real
hyperspectral data sets with different noise levels demonstrate that our method
outperforms the state-of-the-art methods significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4691</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4691</id><created>2014-03-19</created><updated>2014-08-11</updated><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Quantized Feedback Stabilization of Sampled-Data Switched Linear Systems</title><categories>cs.SY</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stability analysis method for sampled-data switched linear
systems with quantization. The available information to the controller is
limited: the quantized state and switching signal at each sampling time.
Switching between sampling times can produce the mismatch of the modes between
the plant and the controller. Moreover, the coarseness of quantization makes
the trajectory wander around, not approach, the origin. Hence the trajectory
may leave the desired neighborhood if the mismatch leads to instability of the
closed-loop system. For the stability of the switched systems, we develop a
sufficient condition characterized by the total mismatch time. The relationship
between the mismatch time and the dwell time of the switching signal is also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4702</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4702</id><created>2014-03-19</created><authors><author><keyname>Parasher</keyname><forenames>Ritu</forenames></author></authors><title>Load flow analysis of radial distribution network using linear data
  structure</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distribution systems hold a very significant position in the power system
since it is the main point of link between bulk power and consumers. A planned
and effective distribution network is the key to cope up with the ever
increasing demand for domestic, industrial and commercial load. The load-flow
study of radial distribution network is of prime importance for effective
planning of load transfers. Power companies are interested in finding the most
efficient configuration for minimization of real power loses and load balancing
among distribution feeders to save energy and enhance the over all performance
of distribution system.
  This thesis presents a fast and efficient method for load-flow analysis of
radial distribution networks. The proposed method is based on linear data
structure. The order of time and space complexity is reported here. There is
significant saving in no. of steps execution. Using graph theory concept and
exploiting multi-cores architecture, the proposed method for load flow can be
further investigated for obtaining more optimized solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4711</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4711</id><created>2014-03-19</created><authors><author><keyname>Pham</keyname><forenames>Manh Tung</forenames></author><author><keyname>Seow</keyname><forenames>Kiam Tian</forenames></author></authors><title>Multiagent Conflict Resolution for a Specification Network of
  Discrete-Event Coordinating Agents</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel compositional approach to distributed
coordination module (CM) synthesis for multiple discrete-event agents in the
formal languages and automata framework. The approach is supported by two
original ideas. The first is a new formalism called the Distributed Constraint
Specification Network (DCSN) that can comprehensibly describe the networking
constraint relationships among distributed agents. The second is multiagent
conflict resolution planning, which entails generating and using AND/OR graphs
to compactly represent conflict resolution (synthesis-process) plans for a
DCSN. Together with the framework of local CM design developed in the authors'
earlier work, the systematic approach supports separately designing local and
deconflicting CM's for individual agents in accordance to a selected conflict
resolution plan. Composing the agent models and the CM's designed furnishes an
overall nonblocking coordination solution that meets the set of inter-agent
constraints specified in a given DCSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4714</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4714</id><created>2014-03-19</created><authors><author><keyname>Baruah</keyname><forenames>R. R.</forenames></author><author><keyname>Deka</keyname><forenames>V.</forenames></author><author><keyname>Bhuyan</keyname><forenames>M. P.</forenames></author></authors><title>Enhancing Dictionary Based Preprocessing For Better Text Compression</title><categories>cs.IT math.IT</categories><comments>6 pages 4 figures</comments><journal-ref>International Journal of Computer Trends and Technology volume9
  number 1, Mar 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the rapid growing of data and number of applications, there is a crucial
need of dictionary based reversible transformation techniques to increase the
efficiency of the compression algorithms and hence contribute towards the
enhancement in compression ratio. Performance analysis of compression methods
in combination with the various transformation techniques is obtained for
different text files of varying sizes. The popular block sorting lossless
Burrows Wheeler Compression Algorithm (BWCA) is implemented along with one
proposed method. For efficient compression a dictionary based transformation
algorithm is also developed. It is observed that much increase in terms of
compression ratio is attained when a source file is preprocessed with
dictionary and then applied to BWCA and the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4722</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4722</id><created>2014-03-19</created><authors><author><keyname>Banerjee</keyname><forenames>Abhik</forenames></author><author><keyname>Ghosh</keyname><forenames>Abhirup</forenames></author><author><keyname>Bharadwaj</keyname><forenames>Koustuvmoni</forenames></author><author><keyname>Saikia</keyname><forenames>Hemanta</forenames></author></authors><title>Mouse Control using a Web Camera based on Colour Detection</title><categories>cs.HC</categories><comments>10 pages, 6 figures, 1 flowchart, &quot;Published with International
  Journal of Computer Trends and Technology (IJCTT)&quot;</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V9(1):15-20,March 2014.ISSN:2231-2803 Published by Seventh Sense Research
  Group</journal-ref><doi>10.14445/22312803/IJCTT-V9P104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an approach for Human computer Interaction (HCI),
where we have tried to control the mouse cursor movement and click events of
the mouse using hand gestures. Hand gestures were acquired using a camera based
on colour detection technique. This method mainly focuses on the use of a Web
Camera to develop a virtual human computer interaction device in a cost
effective manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4735</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4735</id><created>2014-03-19</created><updated>2014-11-01</updated><authors><author><keyname>Bouyuklieva</keyname><forenames>Stefka</forenames></author><author><keyname>Willems</keyname><forenames>Wolfgang</forenames></author><author><keyname>Yankov</keyname><forenames>Nikolay</forenames></author></authors><title>On the Automorphisms of Order 15 for a Binary Self-Dual [96, 48, 20]
  Code</title><categories>cs.IT math.IT</categories><comments>Preprint version to appear in Designs, Codes and Cryptography</comments><msc-class>94B05, 94B15</msc-class><doi>10.1007/s10623-015-0043-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of binary self-dual codes invariant under the action of a
cyclic group of order $pq$ for odd primes $p\neq q$ is considered. As an
application we prove the nonexistence of an extremal self-dual $[96, 48, 20]$
code with an automorphism of order $15$ which closes a gap in `&quot;On extremal
self-dual codes of length 96&quot;, IEEE Trans. Inf. Theory, vol. 57, pp. 6820-6823,
2011'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4747</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4747</id><created>2014-03-19</created><authors><author><keyname>Cao</keyname><forenames>Yanchuang</forenames></author><author><keyname>Wen</keyname><forenames>Lihua</forenames></author><author><keyname>Xiao</keyname><forenames>Jinyou</forenames></author></authors><title>A fast directional boundary element method for high frequency acoustic
  problems in three dimensions</title><categories>cs.NA</categories><journal-ref>Engineering Analysis with Boundary Elements. 50(1):47-58. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A highly efficient fast boundary element method (BEM) for solving large-scale
engineering acoustic problems in a broad frequency range is developed and
implemented. The acoustic problems are modeled by the Burton-Miller boundary
integral equation (BIE), thus the fictitious frequency issue is completely
avoided. The BIE is discretized by using the collocation method with piecewise
constant elements. The linear systems are solved iteratively and accelerated by
using a newly developed kernel-independent wideband fast directional algorithm
(FDA) for fast summation of oscillatory kernels. In addition, the computational
efficiency of the FDA is further promoted by exploiting the low-rank features
of the translation matrices. The high accuracy and nearly linear computational
complexity of the present method are clearly demonstrated by typical examples.
An acoustic scattering problem with dimensionless wave number $kD$ (where $k$
is the wave number and $D$ is the typical length of the obstacle) up to 1000
and the degrees of freedom up to 4 million is successfully solved within 4
hours on a computer with one core and the memory usage is 24.7 GB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4749</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4749</id><created>2014-03-19</created><updated>2014-06-23</updated><authors><author><keyname>Vorel</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Roman</keyname><forenames>Adam</forenames></author></authors><title>Parameterized Complexity of Synchronization and Road Coloring</title><categories>cs.FL</categories><acm-class>F.1.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First, we close the multivariate analysis of a canonical problem concerning
short reset words (SYN), as it was started by Fernau et al. (2013). Namely, we
prove that the problem, parameterized by the number of states, does not admit a
polynomial kernel unless the polynomial hierarchy collapses. Second, we
consider a related canonical problem concerning synchronizing road colorings
(SRCP). Here we give a similar complete multivariate analysis. Namely, we show
that the problem, parameterized by the number of states, admits a polynomial
kernel and we close the previous research of restrictions to particular values
of both the alphabet size and the maximum word length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4751</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4751</id><created>2014-03-19</created><authors><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>The Deterministic Time-Linearity of Service Provided by Fading Channels</title><categories>cs.IT math.IT</categories><comments>17 pages, 5 figures</comments><journal-ref>IEEE Trans. Wireless Commun., vol. 11, no. 5, pp. 1666-1675, May,
  2012</journal-ref><doi>10.1109/TWC.2012.030812.102276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, we study the service process $S(t)$ of an independent and
identically distributed (\textit{i.i.d.}) Nakagami-$m$ fading channel, which is
defined as the amount of service provided, i.e., the integral of the
instantaneous channel capacity over time $t$. By using the Characteristic
Function (CF) approach and the infinitely divisible law, it is proved that,
other than certain generally recognized curve form {or a stochastic process},
the channel service process $S(t)$ is a deterministic linear function of time
$t$, namely, $S(t)=c_m^\ast\cdot t$ where $c_m^\ast$ is a constant determined
by the fading parameter $m$. Furthermore, we extend it to general
\textit{i.i.d.} fading channels and present an explicit form of the constant
service rate $c_p^\ast$. The obtained work provides such a new insight on the
system design of joint source/channel coding that there exists a coding scheme
such that a receiver can decode with zero error probability and zero high layer
queuing delay, if the transmitter maintains a constant data rate no more than
$c_p^\ast$. Finally, we verify our analysis through Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4759</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4759</id><created>2014-03-19</created><authors><author><keyname>Bhatti</keyname><forenames>Zeeshan</forenames></author><author><keyname>Ismaili</keyname><forenames>Imdad Ali</forenames></author><author><keyname>Shaikh</keyname><forenames>Asad Ali</forenames></author><author><keyname>Javaid</keyname><forenames>Waseem</forenames></author></authors><title>Spelling Error Trends and Patterns in Sindhi</title><categories>cs.CL</categories><comments>5 pages, 6 tables, Sindhi Languge</comments><journal-ref>Journal of Emerging Trends in Computing and Information Sciences,
  VOL. 3, NO.10 Oct, 2012, ISSN 2079-8407</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical error Correction technique is the most accurate and widely used
approach today, but for a language like Sindhi which is a low resourced
language the trained corpora's are not available, so the statistical techniques
are not possible at all. Instead a useful alternative would be to exploit
various spelling error trends in Sindhi by using a Rule based approach. For
designing such technique an essential prerequisite would be to study the
various error patterns in a language. This pa per presents various studies of
spelling error trends and their types in Sindhi Language. The research shows
that the error trends common to all languages are also encountered in Sindhi
but their do exist some error patters that are catered specifically to a Sindhi
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4762</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4762</id><created>2014-03-19</created><authors><author><keyname>Komenda</keyname><forenames>Jan</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>van Schuppen</keyname><forenames>Jan H.</forenames></author></authors><title>Maximally Permissive Coordination Supervisory Control -- Towards
  Necessary and Sufficient Conditions</title><categories>math.OC cs.FL cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we further develop the coordination control framework for
discrete-event systems with both complete and partial observation. A new weaker
sufficient condition for the computation of the supremal conditionally
controllable sublanguage is presented. This result is then used for the
computation of the supremal conditionally controllable and conditionally normal
sublanguage. The paper further generalizes the previous study by considering
general, non-prefix-closed languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4777</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4777</id><created>2014-03-19</created><authors><author><keyname>Mohino-Herranz</keyname><forenames>Inma</forenames></author><author><keyname>Gil-Pita</keyname><forenames>Roberto</forenames></author><author><keyname>Alonso-Diaz</keyname><forenames>Sagrario</forenames></author><author><keyname>Rosa-Zurera</keyname><forenames>Manuel</forenames></author></authors><title>MFCC based Enlargement of the Training Set for Emotion Recognition in
  Speech</title><categories>cs.CV</categories><comments>Signal &amp; Image Processing : An International Journal (SIPIJ)</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ),
  Vol.5, No,1. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotional state recognition through speech is being a very interesting
research topic nowadays. Using subliminal information of speech, denominated as
prosody, it is possible to recognize the emotional state of the person. One of
the main problems in the design of automatic emotion recognition systems is the
small number of available patterns. This fact makes the learning process more
difficult, due to the generalization problems that arise under these
conditions. In this work we propose a solution to this problem consisting in
enlarging the training set through the creation the new virtual patterns. In
the case of emotional speech, most of the emotional information is included in
speed and pitch variations. So, a change in the average pitch that does not
modify neither the speed nor the pitch variations does not affect the expressed
emotion. Thus, we use this prior information in order to create new patterns
applying a gender dependent pitch shift modification in the feature extraction
process of the classification system. For this purpose, we propose a frequency
scaling modification of the Mel Frequency Cepstral Coefficients, used to
classify the emotion. For this purpose, we propose a gender dependent frequency
scaling modification. This proposed process allows us to synthetically increase
the number of available patterns in the training set, thus increasing the
generalization capability of the system and reducing the test error. Results
carried out with two different classifiers with different degree of
generalization capability demonstrate the suitability of the proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4780</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4780</id><created>2014-03-19</created><authors><author><keyname>Ahmad</keyname><forenames>Musheer</forenames></author><author><keyname>Al-Sharari</keyname><forenames>Hamed D</forenames></author></authors><title>An Inter-Component Pixels Permutation Based Color Image Encryption Using
  Hyper-chaos</title><categories>cs.CR</categories><journal-ref>European Journal of Scientific Research, Vol. 116, No. 1, pp.
  115-121, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a simple and robust color image encryption algorithm based on
high-dimensional chaotic maps is proposed. The algorithm employs a 3D Arnold
transform to perform inter-component shuffling of plain-image, while a 2D
hyper-chaotic map is used to confuse the relationship between the encrypted
image and plain-image. The control parameters of Arnold transform are extracted
from pending plain-image, thereby establishing a dependency to the plain-image.
The confusion process is carried out in cipher-block chaining mode to make it
dependent to the encrypted image. This makes the algorithm able to resist the
cryptographic attacks. The performance of the proposed algorithm is analyzed
through computer simulations. The experimental analyses show that the proposed
algorithm has desirable properties of high security, robustness to
cryptographic attacks and practicability to protect multimedia color images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4781</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4781</id><created>2014-03-19</created><authors><author><keyname>Mukherjee</keyname><forenames>Subhadip</forenames></author><author><keyname>Seelamantula</keyname><forenames>Chandra Sekhar</forenames></author></authors><title>A Split-and-Merge Dictionary Learning Algorithm for Sparse
  Representation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In big data image/video analytics, we encounter the problem of learning an
overcomplete dictionary for sparse representation from a large training
dataset, which can not be processed at once because of storage and
computational constraints. To tackle the problem of dictionary learning in such
scenarios, we propose an algorithm for parallel dictionary learning. The
fundamental idea behind the algorithm is to learn a sparse representation in
two phases. In the first phase, the whole training dataset is partitioned into
small non-overlapping subsets, and a dictionary is trained independently on
each small database. In the second phase, the dictionaries are merged to form a
global dictionary. We show that the proposed algorithm is efficient in its
usage of memory and computational complexity, and performs on par with the
standard learning strategy operating on the entire data at a time. As an
application, we consider the problem of image denoising. We present a
comparative analysis of our algorithm with the standard learning techniques,
that use the entire database at a time, in terms of training and denoising
performance. We observe that the split-and-merge algorithm results in a
remarkable reduction of training time, without significantly affecting the
denoising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4782</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4782</id><created>2014-03-19</created><authors><author><keyname>Ahmad</keyname><forenames>Musheer</forenames></author><author><keyname>Alam</keyname><forenames>Bashir</forenames></author><author><keyname>Farooq</keyname><forenames>Omar</forenames></author></authors><title>Chaos Based Mixed Keystream Generation for Voice Data Encryption</title><categories>cs.CR</categories><journal-ref>International Journal on Cryptography and Information Security,
  Vol. 2, No. 1, pp. 36--45, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a high dimensional chaotic systems based mixed keystream
generator is proposed to secure the voice data. As the voice-based
communication becomes extensively vital in the application areas of military,
voice over IP, voice-conferencing, phone banking, news telecasting etc. It
greatly demands to preserve sensitive voice signals from the unauthorized
listening and illegal usage over shared/open networks. To address the need, the
designed keystream generator employed to work as a symmetric encryption
technique to protect voice bitstreams over insecure transmission channel. The
generator utilizes the features of high dimensional chaos like Lorenz and Chen
systems to generate highly unpredictable and random-like sequences. The
encryption keystream is dynamically extracted from the pre-treated chaotic
mixed sequences, which are then applied to mask the voice bitstream for
integrity protection of voice data. The experimental analyses like
auto-correlation, signal distribution, parameter residual deviation, key space
and key-sensitivity demonstrate the effectiveness of the proposed technique for
secure voice communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4789</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4789</id><created>2014-03-19</created><authors><author><keyname>Monshizadeh</keyname><forenames>Nima</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan</forenames></author></authors><title>Structure-preserving model reduction of physical network systems by
  clustering</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish a method for model order reduction of a certain
class of physical network systems. The proposed method is based on clustering
of the vertices of the underlying graph, and yields a reduced order model
within the same class. To capture the physical properties of the network, we
allow for weights associated to both the edges as well as the vertices of the
graph. We extend the notion of almost equitable partitions to this class of
graphs. Consequently, an explicit model reduction error expression in the sense
of H2-norm is provided for clustering arising from almost equitable partitions.
Finally the method is extended to second-order systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4795</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4795</id><created>2014-03-19</created><authors><author><keyname>Whiting</keyname><forenames>James Gerald Holland</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Sensory fusion in Physarum polycephalum and implementing multi-sensory
  functional computation</title><categories>cs.ET</categories><comments>18 pages, 2 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surface electrical potential and observational growth recordings were made of
a protoplasmic tube of the slime mould Physarum polycephalum in response to a
multitude of stimuli with regards to sensory fusion or multisensory
integration. Each stimulus was tested alone and in combination in order to
evaluate for the first time the effect that multiple stimuli have on the
frequency of streaming oscillation. White light caused a decrease in frequency
whilst increasing the temperature and applying a food source in the form of oat
flakes both increased the frequency. Simultaneously stimulating P. polycephalum
with light and oat flake produced no net change in frequency, while combined
light and heat stimuli showed an increase in frequency smaller than that
observed for heat alone. When the two positive stimuli, oat flakes and heat,
were combined, there was a net increase in frequency similar to the cumulative
increases caused by the individual stimuli. Boolean logic gates were derived
from the measured frequency change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4804</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4804</id><created>2014-03-19</created><authors><author><keyname>Hansson</keyname><forenames>Anders</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Distributed System Identification with ADMM</title><categories>cs.SY</categories><comments>Submitted to IEEE CDC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents identification of both network connected systems as well
as distributed systems governed by PDEs in the framework of distributed
optimization via the Alternating Direction Method of Multipliers. This approach
opens first the possibility to identify distributed models in a global manner
using all available data sequences and second the possibility for a distributed
implementation. The latter will make the application to large scale complex
systems possible. In addition to outlining a new large scale identification
method, illustrations are shown for identifying both network connected systems
and discretized PDEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4813</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4813</id><created>2014-03-19</created><updated>2014-09-15</updated><authors><author><keyname>Fu</keyname><forenames>Xianjin</forenames></author><author><keyname>Chen</keyname><forenames>Zhenbang</forenames></author><author><keyname>Zhang</keyname><forenames>Yufeng</forenames></author><author><keyname>Huang</keyname><forenames>Chun</forenames></author><author><keyname>Dong</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Ji</forenames></author></authors><title>MPISE: Symbolic Execution of MPI Programs</title><categories>cs.DC cs.PL cs.SE</categories><comments>25pages, extended version (unpublished!) of paper submitted to ictac
  2014. Version 0.2, we carry out experiments using release llvm istead of a
  debug version one, which makes mpise 10 times faster</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Message Passing Interfaces (MPI) plays an important role in parallel
computing. Many parallel applications are implemented as MPI programs. The
existing methods of bug detection for MPI programs have the shortage of
providing both input and non-determinism coverage, leading to missed bugs. In
this paper, we employ symbolic execution to ensure the input coverage, and
propose an on-the-fly schedule algorithm to reduce the interleaving
explorations for non-determinism coverage, while ensuring the soundness and
completeness. We have implemented our approach as a tool, called MPISE, which
can automatically detect the deadlock and runtime bugs in MPI programs. The
results of the experiments on benchmark programs and real world MPI programs
indicate that MPISE finds bugs effectively and efficiently. In addition, our
tool also provides diagnostic information and replay mechanism to help
understanding bugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4828</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4828</id><created>2014-03-19</created><authors><author><keyname>Zhang</keyname><forenames>Bowen</forenames></author><author><keyname>Caramanis</keyname><forenames>Michael C.</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>Control of Smart Building Dynamic Energy Service Preferences for
  Efficient Regulation Service</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and solve a stochastic dynamic programming (DP) formulation of the
optimal provision of regulation service reserves (RSR) by controlling dynamic
demand preferences in smart buildings. A major contribution over past dynamic
pricing work is that we pioneer the relaxation of static, uniformly distributed
utility of demand. In this paper we model explicitly the dynamics of energy
user preferences leading to a non-uniform and time varying probability
distribution of demand utility. More explicitly, we model active and idle duty
cycle appliances in a smart building as a closed queuing system with
price-controlled arrival rates into the active appliance queue. Focusing on
cooling appliances, we model the utility associated with the transition from
idle to active as a non-uniform time varying function. We (i) derive an
analytic characterization of the optimal policy and the differential cost
function, and (ii) prove monotonicity and convexity properties. This enables us
to propose and implement a smart assisted value iteration algorithm as well as
an approximate DP solution based on the aforementioned functional
approximations and properties. Numerical results demonstrate the validity of
the solution techniques and the computational advantages of the proposed near
optimal approximate-DP solutions for realistic, large-state-space problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4829</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4829</id><created>2014-03-19</created><authors><author><keyname>Yue</keyname><forenames>Qinggang</forenames></author><author><keyname>Ling</keyname><forenames>Zhen</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Fu</keyname><forenames>Xinwen</forenames></author><author><keyname>Zhao</keyname><forenames>Wei</forenames></author></authors><title>Blind Recognition of Touched Keys: Attack and Countermeasures</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel computer vision based attack that
discloses inputs on a touch enabled device, while the attacker cannot see any
text or popups from a video of the victim tapping on the touch screen. In the
attack, we use the optical flow algorithm to identify touching frames where the
finger touches the screen surface. We innovatively use intersections of
detected edges of the touch screen to derive the homography matrix mapping the
touch screen surface in video frames to a reference image of the virtual
keyboard. We analyze the shadow formation around the fingertip and use the
k-means clustering algorithm to identify touched points. Homography can then
map these touched points to keys of the virtual keyboard. Our work is
substantially different from existing work. We target password input and are
able to achieve a high success rate. We target scenarios like classrooms,
conferences and similar gathering places and use a webcam or smartphone camera.
In these scenes, single-lens reflex (SLR) cameras and high-end camcorders used
in related work will appear suspicious. To defeat such computer vision based
attacks, we design, implement and evaluate the Privacy Enhancing Keyboard (PEK)
where a randomized virtual keyboard is used to input sensitive information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4847</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4847</id><created>2014-03-19</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Massive MIMO Systems with Hardware-Constrained Base Stations</title><categories>cs.IT math.IT</categories><comments>Published at IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP 2014), 5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) systems are cellular networks
where the base stations (BSs) are equipped with unconventionally many antennas.
Such large antenna arrays offer huge spatial degrees-of-freedom for
transmission optimization; in particular, great signal gains, resilience to
imperfect channel knowledge, and small inter-user interference are all
achievable without extensive inter-cell coordination. The key to cost-efficient
deployment of large arrays is the use of hardware-constrained base stations
with low-cost antenna elements, as compared to today's expensive and
power-hungry BSs. Low-cost transceivers are prone to hardware imperfections,
but it has been conjectured that the excessive degrees-of-freedom of massive
MIMO would bring robustness to such imperfections. We herein prove this claim
for an uplink channel with multiplicative phase-drift, additive distortion
noise, and noise amplification. Specifically, we derive a closed-form scaling
law that shows how fast the imperfections increase with the number of antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4851</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4851</id><created>2014-03-19</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Circuit-Aware Design of Energy-Efficient Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Published at International Symposium on Communications, Control, and
  Signal Processing (ISCCSP 2014), 4 pages, 3 figures. This version corrects an
  error related to Lemma 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Densification is a key to greater throughput in cellular networks. The full
potential of coordinated multipoint (CoMP) can be realized by massive
multiple-input multiple-output (MIMO) systems, where each base station (BS) has
very many antennas. However, the improved throughput comes at the price of more
infrastructure; hardware cost and circuit power consumption scale
linearly/affinely with the number of antennas. In this paper, we show that one
can make the circuit power increase with only the square root of the number of
antennas by circuit-aware system design. To this end, we derive achievable user
rates for a system model with hardware imperfections and show how the level of
imperfections can be gradually increased while maintaining high throughput. The
connection between this scaling law and the circuit power consumption is
established for different circuits at the BS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4861</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4861</id><created>2014-03-19</created><updated>2014-11-19</updated><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>van Dijk</keyname><forenames>Thomas C.</forenames></author><author><keyname>Fink</keyname><forenames>Martin</forenames></author><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Improved Approximation Algorithms for Box Contact Representations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following geometric representation problem: Given a graph whose
vertices correspond to axis-aligned rectangles with fixed dimensions, arrange
the rectangles without overlaps in the plane such that two rectangles touch if
the graph contains an edge between them. This problem is called \textsc{Contact
Representation of Word Networks} (\textsc{Crown}) since it formalizes the
geometric problem behind drawing word clouds in which semantically related
words are close to each other. \textsc{Crown} is known to be NP-hard, and there
are approximation algorithms for certain graph classes for the optimization
version, \textsc{Max-Crown}, in which realizing each desired adjacency yields a
certain profit. We present the first $O(1)$-approximation algorithm for the
general case, when the input is a complete weighted graph, and for the
bipartite case. Since the subgraph of realized adjacencies is necessarily
planar, we also consider several planar graph classes (namely stars, trees,
outerplanar, and planar graphs), improving upon the known results. For some
graph classes, we also describe improvements in the unweighted case, where each
adjacency yields the same profit. Finally, we show that the problem is APX-hard
on bipartite graphs of bounded maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4868</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4868</id><created>2014-03-19</created><authors><author><keyname>Monshizadeh</keyname><forenames>Nima</forenames></author><author><keyname>Zhang</keyname><forenames>Shuo</forenames></author><author><keyname>Camlibel</keyname><forenames>Kanat</forenames></author></authors><title>Zero forcing sets and controllability of dynamical systems defined on
  graphs</title><categories>cs.SY</categories><doi>10.1109/TAC.2014.2308619</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, controllability of systems defined on graphs is discussed. We
consider the problem of controllability of the network for a family of matrices
carrying the structure of an underlying directed graph. A one-to-one
correspondence between the set of leaders rendering the network controllable
and zero forcing sets is established. To illustrate the proposed results,
special cases including path, cycle, and complete graphs are discussed.
Moreover, as shown for graphs with a tree structure, the proposed results of
the present paper together with the existing results on the zero forcing sets
lead to a minimal leader selection scheme in particular cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4871</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4871</id><created>2014-03-19</created><authors><author><keyname>Shackelford</keyname><forenames>Mark</forenames></author></authors><title>Evolutionary Algorithm for Drug Discovery Interim Design Report</title><categories>cs.NE cs.CE</categories><comments>7 Pages, Interim Design Document</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A software program which aims to provide an exploration capability over the
Search Space of potential drug molecules. The program explores the search space
by generating random molecules, determining their fitness and then breeding a
new generation from the fittest individuals. The search space, in theory any
combination of any elements in any order, is constrained by the use of a subset
of elements and a list of fragments, molecular parts that are known to be
useful in drug development. The resultant molecules from each generation are
stored in a searchable database, so that the user can browse through previous
generations looking for interesting molecules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4879</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4879</id><created>2014-03-19</created><authors><author><keyname>Hawes</keyname><forenames>Matthew B.</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>A Compressive Sensing Based Approach to Sparse Wideband Array Design</title><categories>cs.IT math.IT math.OC</categories><comments>4 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse wideband sensor array design for sensor location optimisation is
highly nonlinear and it is traditionally solved by genetic algorithms,
simulated annealing or other similar optimization methods. However, this is an
extremely time-consuming process and more efficient solutions are needed. In
this work, this problem is studied from the viewpoint of compressive sensing
and a formulation based on a modified $l_1$ norm is derived. As there are
multiple coefficients associated with each sensor, the key is to make sure that
these coefficients are simultaneously minimized in order to discard the
corresponding sensor locations. Design examples are provided to verify the
effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4880</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4880</id><created>2014-03-19</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Two Puzzles About Computation</title><categories>cs.LO</categories><comments>5 pages</comments><journal-ref>In Alan Turing: his work and impact, ed. S.B. Cooper and J. van
  Leeuwen, pages 53-57, Elsevier 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this note is to raise two different questions, which are
rarely if ever considered, and to which, it seems, we lack convincing,
systematic answers. These questions can be posed as:
  - Why do we compute?
  - What do we compute?
  The point is not so much that we have no answers to these puzzles, as that we
have no established body of theory which gives satisfying, systematic answers,
as part of a broader understanding. By raising these questions, we hope to
stimulate some thinking in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4881</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4881</id><created>2014-03-19</created><authors><author><keyname>Treiber</keyname><forenames>Martin</forenames></author><author><keyname>Kanagaraj</keyname><forenames>Venkatesan</forenames></author></authors><title>Comparing Numerical Integration Schemes for Time-Continuous
  Car-Following Models</title><categories>cs.CE physics.soc-ph</categories><comments>Submitted to Transportation Research Part B: Methodological</comments><journal-ref>Physica A: Statistical Mechanics and its Applications 419C,
  pp.183-195 (2015)</journal-ref><doi>10.1016/j.physa.2014.09.061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When simulating trajectories by integrating time-continuous car-following
models, standard integration schemes such as the forth-order Runge-Kutta method
(RK4) are rarely used while the simple Euler's method is popular among
researchers. We compare four explicit methods: Euler's method, ballistic
update, Heun's method (trapezoidal rule), and the standard forth-order RK4. As
performance metrics, we plot the global discretization error as a function of
the numerical complexity. We tested the methods on several time-continuous
car-following models in several multi-vehicle simulation scenarios with and
without discontinuities such as stops or a discontinuous behavior of an
external leader. We find that the theoretical advantage of RK4 (consistency
order~4) only plays a role if both the acceleration function of the model and
the external data of the simulation scenario are sufficiently often
differentiable. Otherwise, we obtain lower (and often fractional) consistency
orders. Although, to our knowledge, Heun's method has never been used for
integrating car-following models, it turns out to be the best scheme for many
practical situations. The ballistic update always prevails Euler's method
although both are of first order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4887</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4887</id><created>2014-03-19</created><authors><author><keyname>Warren</keyname><forenames>Andrew</forenames></author><author><keyname>Setubal</keyname><forenames>Joao</forenames></author></authors><title>Using Entropy Estimates for DAG-Based Ontologies</title><categories>cs.CL</categories><comments>in ISMB Bio-Ontologies, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Entropy measurements on hierarchical structures have been used in
methods for information retrieval and natural language modeling. Here we
explore its application to semantic similarity. By finding shared ontology
terms, semantic similarity can be established between annotated genes. A common
procedure for establishing semantic similarity is to calculate the
descriptiveness (information content) of ontology terms and use these values to
determine the similarity of annotations. Most often information content is
calculated for an ontology term by analyzing its frequency in an annotation
corpus. The inherent problems in using these values to model functional
similarity motivates our work. Summary: We present a novel calculation for
establishing the entropy of a DAG-based ontology, which can be used in an
alternative method for establishing the information content of its terms. We
also compare our IC metric to two others using semantic and sequence
similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4891</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4891</id><created>2014-03-19</created><updated>2014-08-25</updated><authors><author><keyname>Nishi</keyname><forenames>Ryosuke</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Dynamics of social balance under temporal interaction</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 3 figures, 1 table</comments><journal-ref>Europhysics Letters 107, 48003 (2014)</journal-ref><doi>10.1209/0295-5075/107/48003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real social contacts are often intermittent such that a link between a pair
of nodes in a social network is only temporarily used. Effects of such temporal
networks on social dynamics have been investigated for several phenomenological
models such as epidemic spreading, linear diffusion processes, and nonlinear
oscillations. Here, we numerically investigate nonlinear social balance
dynamics in such a situation. Social balance is a classical psychological
theory, which dictates that a triad is balanced if the three agents are mutual
friends or if the two of them are the friends of each other and hostile to the
other agent. We show that the social balance dynamics is slowed down on the
temporal complete graph as compared to the corresponding static complete graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4910</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4910</id><created>2014-03-19</created><updated>2015-05-13</updated><authors><author><keyname>Kanvar</keyname><forenames>Vini</forenames></author><author><keyname>Khedker</keyname><forenames>Uday P.</forenames></author></authors><title>Heap Abstractions for Static Analysis</title><categories>cs.PL</categories><comments>49 pages, 20 figures</comments><acm-class>A.1; F.3.1; F.3.2; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heap data is potentially unbounded and seemingly arbitrary. As a consequence,
unlike stack and static memory, heap memory cannot be abstracted directly in
terms of a fixed set of source variable names appearing in the program being
analysed. This makes it an interesting topic of study and there is an abundance
of literature employing heap abstractions. Although most studies have addressed
similar concerns, their formulations and formalisms often seem dissimilar and
some times even unrelated. Thus, the insights gained in one description of heap
abstraction may not directly carry over to some other description. This survey
is a result of our quest for a unifying theme in the existing descriptions of
heap abstractions. In particular, our interest lies in the abstractions and not
in the algorithms that construct them.
  In our search of a unified theme, we view a heap abstraction as consisting of
two features: a heap model to represent the heap memory and a summarization
technique for bounding the heap representation. We classify the models as
storeless, store based, and hybrid. We describe various summarization
techniques based on k-limiting, allocation sites, patterns, variables, other
generic instrumentation predicates, and higher-order logics. This approach
allows us to compare the insights of a large number of seemingly dissimilar
heap abstractions and also paves way for creating new abstractions by
mix-and-match of models and summarization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4928</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4928</id><created>2014-03-19</created><authors><author><keyname>Bethard</keyname><forenames>Steven</forenames></author><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Pustejovsky</keyname><forenames>James</forenames></author><author><keyname>Verhagen</keyname><forenames>Marc</forenames></author></authors><title>Clinical TempEval</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the Clinical TempEval task which is currently in preparation for
the SemEval-2015 evaluation exercise. This task involves identifying and
describing events, times and the relations between them in clinical text. Six
discrete subtasks are included, focusing on recognising mentions of times and
events, describing those mentions for both entity types, identifying the
relation between an event and the document creation time, and identifying
narrative container relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4958</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4958</id><created>2014-03-18</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Desel</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>On Negotiation as Concurrency Primitive II: Deterministic Cyclic
  Negotiations</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue our study of negotations, a concurrency model with multiparty
negotiation as primitive. In a previous paper (arXiv:13072145) we have provided
a correct and complete set of reduction rules for sound, acyclic, and (weakly)
deterministic negotiations. In this paper we extend this result to all
deterministic negotiations, including cyclic ones. We also show that this set
of rules allows one to decide soundness and to summarize negotiations in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4988</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4988</id><created>2014-03-19</created><authors><author><keyname>Minsky</keyname><forenames>Naftaly</forenames></author></authors><title>Bracing Heterogeneous Distributed Systems via Built-in Frameworks</title><categories>cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel architecture of distributed systems--called
framed distributed system, or FDS--that braces a given system via a built-in
virtual framework that controls the flow of messages between system components
and between them and their environment, while being oblivious of the code of
the communicating components. This control is carried out in a decentralized,
and thus scalable, manner. The FDS architecture is expected to have a
significant impact on the dependability and security of distributed systems,
and on the whole life cycle of such systems. Although this architecture has
been designed specifically for SOA-like heterogeneous and open systems--whose
components may be written in different languages, may run on different
platforms, and may be designed, constructed, and even maintained under
different administrative domains--it should be useful for distributed systems
in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4991</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4991</id><created>2014-03-19</created><authors><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Kononov</keyname><forenames>Alexander</forenames></author><author><keyname>Letsios</keyname><forenames>Dimitrios</forenames></author><author><keyname>Lucarelli</keyname><forenames>Giorgio</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author></authors><title>Energy Efficient Scheduling and Routing via Randomized Rounding</title><categories>cs.DS</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a unifying framework based on configuration linear programs and
randomized rounding, for different energy optimization problems in the dynamic
speed-scaling setting. We apply our framework to various scheduling and routing
problems in heterogeneous computing and networking environments. We first
consider the energy minimization problem of scheduling a set of jobs on a set
of parallel speed scalable processors in a fully heterogeneous setting. For
both the preemptive-non-migratory and the preemptive-migratory variants, our
approach allows us to obtain solutions of almost the same quality as for the
homogeneous environment. By exploiting the result for the
preemptive-non-migratory variant, we are able to improve the best known
approximation ratio for the single processor non-preemptive problem.
Furthermore, we show that our approach allows to obtain a constant-factor
approximation algorithm for the power-aware preemptive job shop scheduling
problem. Finally, we consider the min-power routing problem where we are given
a network modeled by an undirected graph and a set of uniform demands that have
to be routed on integral routes from their sources to their destinations so
that the energy consumption is minimized. We improve the best known
approximation ratio for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.4997</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.4997</id><created>2014-03-19</created><authors><author><keyname>de Melo</keyname><forenames>Pedro O. S. Vaz</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author><author><keyname>Assun&#xe7;&#xe3;o</keyname><forenames>Renato</forenames></author><author><keyname>Alves</keyname><forenames>Rodrigo</forenames></author><author><keyname>Loureiro</keyname><forenames>Antonio A. F.</forenames></author></authors><title>Universal and Distinct Properties of Communication Dynamics: How to
  Generate Realistic Inter-event Times</title><categories>cs.SI physics.soc-ph</categories><acm-class>H.2.8; G.3</acm-class><doi>10.1145/2700399</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of information systems, means of communications are
becoming cheaper, faster and more available. Today, millions of people carrying
smart-phones or tablets are able to communicate at practically any time and
anywhere they want. Among others, they can access their e-mails, comment on
weblogs, watch and post comments on videos, make phone calls or text messages
almost ubiquitously. Given this scenario, in this paper we tackle a fundamental
aspect of this new era of communication: how the time intervals between
communication events behave for different technologies and means of
communications? Are there universal patterns for the inter-event time
distribution (IED)? In which ways inter-event times behave differently among
particular technologies? To answer these questions, we analyze eight different
datasets from real and modern communication data and we found four well defined
patterns that are seen in all the eight datasets. Moreover, we propose the use
of the Self-Feeding Process (SFP) to generate inter-event times between
communications. The SFP is extremely parsimonious point process that requires
at most two parameters and is able to generate inter-event times with all the
universal properties we observed in the data. We show the potential application
of SFP by proposing a framework to generate a synthetic dataset containing
realistic communication events of any one of the analyzed means of
communications (e.g. phone calls, e-mails, comments on blogs) and an algorithm
to detect anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5001</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5001</id><created>2014-03-19</created><updated>2014-08-06</updated><authors><author><keyname>Samanthula</keyname><forenames>Bharath K.</forenames></author><author><keyname>Elmehdwi</keyname><forenames>Yousef</forenames></author><author><keyname>Jiang</keyname><forenames>Wei</forenames></author></authors><title>k-Nearest Neighbor Classification over Semantically Secure Encrypted
  Relational Data</title><categories>cs.CR</categories><comments>29 pages, 2 figures, 3 tables arXiv admin note: substantial text
  overlap with arXiv:1307.4824</comments><acm-class>D.4.6; E.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Mining has wide applications in many areas such as banking, medicine,
scientific research and among government agencies. Classification is one of the
commonly used tasks in data mining applications. For the past decade, due to
the rise of various privacy issues, many theoretical and practical solutions to
the classification problem have been proposed under different security models.
However, with the recent popularity of cloud computing, users now have the
opportunity to outsource their data, in encrypted form, as well as the data
mining tasks to the cloud. Since the data on the cloud is in encrypted form,
existing privacy preserving classification techniques are not applicable. In
this paper, we focus on solving the classification problem over encrypted data.
In particular, we propose a secure k-NN classifier over encrypted data in the
cloud. The proposed k-NN protocol protects the confidentiality of the data,
user's input query, and data access patterns. To the best of our knowledge, our
work is the first to develop a secure k-NN classifier over encrypted data under
the semi-honest model. Also, we empirically analyze the efficiency of our
solution through various experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5006</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5006</id><created>2014-03-19</created><authors><author><keyname>Yan</keyname><forenames>Ning</forenames></author><author><keyname>Asudeh</keyname><forenames>Abolfazl</forenames></author><author><keyname>Li</keyname><forenames>Chengkai</forenames></author></authors><title>Generating Preview Tables for Entity Graphs</title><categories>cs.DB cs.IR</categories><comments>10 pages, 7 figures, 6 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Users and developers are tapping into big, complex entity graphs for numerous
applications. It is challenging to select entity graphs for a particular need,
given abundant datasets from many sources and the oftentimes scarce information
available for them. We propose methods to automatically produce preview tables
for entity graphs, for compact presentation of important entity types and
relationships. The preview tables assist users in attaining a quick and rough
preview of the data. They can be shown in a limited display space for a user to
browse and explore, before she decides to spend time and resources to fetch and
investigate the complete dataset. We formulate several optimization problems
that look for previews with the highest scores according to intuitive goodness
measures, under various constraints on preview size and distance between
preview tables. The optimization problem under distance constraint is NP-hard.
We design a dynamic-programming algorithm and an Apriori-style algorithm for
finding optimal previews. The experiments and user studies on Freebase
demonstrated both the scoring measures' accuracy and the discovery algorithms'
efficiency
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5007</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5007</id><created>2014-03-19</created><authors><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author><author><keyname>Kozat</keyname><forenames>Ulas C.</forenames></author></authors><title>On Throughput-Delay Optimal Access to Storage Clouds via Load Adaptive
  Coding and Chunking</title><categories>cs.NI cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1307.8083</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent literature including our past work provide analysis and solutions for
using (i) erasure coding, (ii) parallelism, or (iii) variable slicing/chunking
(i.e., dividing an object of a specific size into a variable number of smaller
chunks) in speeding the I/O performance of storage clouds. However, a
comprehensive approach that considers all three dimensions together to achieve
the best throughput-delay trade-off curve had been lacking. This paper presents
the first set of solutions that can pick the best combination of coding rate
and object chunking/slicing options as the load dynamically changes. Our
specific contributions are as follows: (1) We establish via measurement that
combining variable coding rate and chunking is mostly feasible over a popular
public cloud. (2) We relate the delay optimal values for chunking level and
code rate to the queue backlogs via an approximate queueing analysis. (3) Based
on this analysis, we propose TOFEC that adapts the chunking level and coding
rate against the queue backlogs. Our trace-driven simulation results show that
TOFEC's adaptation mechanism converges to an appropriate code that provides the
optimal throughput-delay trade-off without reducing system capacity. Compared
to a non-adaptive strategy optimized for throughput, TOFEC delivers $2.5\times$
lower latency under light workloads; compared to a non-adaptive strategy
optimized for latency, TOFEC can scale to support over $3\times$ as many
requests. (4) We propose a simpler greedy solution that performs on a par with
TOFEC in average delay performance, but exhibits significantly more performance
variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5010</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5010</id><created>2014-03-19</created><authors><author><keyname>Xing</keyname><forenames>Kexing</forenames></author><author><keyname>Zuo</keyname><forenames>Decheng</forenames></author><author><keyname>Zhou</keyname><forenames>Haiying</forenames></author><author><keyname>Kun-Mean</keyname><forenames>Hou</forenames></author></authors><title>Task &amp; Resource Self-adaptive Embedded Real-time Operating System
  Microkernel for Wireless Sensor Nodes</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are used in many application fields, such as
military, healthcare, environment surveillance, etc. The WSN OS based on
event-driven model doesn't support real-time and multi-task application types
and the OSs based on thread-driven model consume much energy because of
frequent context switch. Due to the high-dense and large-scale deployment of
sensor nodes, it is very difficult to collect sensor nodes to update their
software. Furthermore, the sensor nodes are vulnerable to security attacks
because of the characteristics of broadcast communication and unattended
application. This paper presents a task and resource self-adaptive embedded
real-time microkernel, which proposes hybrid programming model and offers a
two-level scheduling strategy to support real-time multi-task correspondingly.
A communication scheme, which takes the &quot;tuple&quot; space and &quot;IN/OUT&quot; primitives
from &quot;LINDA&quot;, is proposed to support some collaborative and distributed tasks.
In addition, this kernel implements a run-time over-the-air updating mechanism
and provides a security policy to avoid the attacks and ensure the reliable
operation of nodes. The performance evaluation is proposed and the experiential
results show this kernel is task-oriented and resource-aware and can be used
for the applications of event-driven and real-time multi-task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5012</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5012</id><created>2014-03-19</created><updated>2014-04-17</updated><authors><author><keyname>Li</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Meilin</forenames></author></authors><title>A Non-Cooperative Game Model for Reliability-Based Task Scheduling in
  Cloud Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a newly emerging distributed system which is evolved from
Grid computing. Task scheduling is the core research of cloud computing which
studies how to allocate the tasks among the physical nodes, so that the tasks
can get a balanced allocation or each task's execution cost decreases to the
minimum, or the overall system performance is optimal. Unlike task scheduling
based on time or cost before, aiming at the special reliability requirements in
cloud computing, we propose a non-cooperative game model for reliability-based
task scheduling approach. This model takes the steady-state availability that
computing nodes provide as the target, takes the task slicing strategy of the
schedulers as the game strategy, then finds the Nash equilibrium solution. And
also, we design a task scheduling algorithm based on this model. The
experiments can be seen that our task scheduling algorithm is better than the
so-called balanced scheduling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5020</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5020</id><created>2014-03-19</created><updated>2014-09-17</updated><authors><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author></authors><title>State-space solution to a minimum-entropy $\mathcal{H}_\infty$-optimal
  control problem with a nested information constraint</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-space formulas are derived for the minimum-entropy $\mathcal{H}_\infty$
controller when the plant and controller are constrained to be
block-lower-triangular. Such a controller exists if and only if: the
corresponding unstructured problem has a solution, a certain pair of coupled
algebraic Riccati equations admits a mutually stabilizing fixed point, and a
pair of spectral radius conditions is met. The controller's observer-based
structure is also discussed, and a simple numerical approach for solving the
coupled Riccati equations is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5022</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5022</id><created>2014-03-19</created><updated>2014-11-18</updated><authors><author><keyname>Williams</keyname><forenames>Jason L.</forenames></author></authors><title>An efficient, variational approximation of the best fitting
  multi-Bernoulli filter</title><categories>cs.SY</categories><comments>Accepted, IEEE Transactions on Signal Processing,
  http://dx.doi.org/10.1109/TSP.2014.2370946</comments><journal-ref>IEEE Transactions on Signal Processing, vol 63, no 1, pp 258-273,
  January 2015</journal-ref><doi>10.1109/TSP.2014.2370946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint probabilistic data association (JPDA) filter is a popular tracking
methodology for problems involving well-spaced targets, but it is rarely
applied in problems with closely-spaced targets due to its complexity in these
cases, and due to the well-known phenomenon of coalescence. This paper
addresses these difficulties using random finite sets (RFSs) and variational
inference, deriving a highly tractable, approximate method for obtaining the
multi-Bernoulli distribution that minimises the set Kullback-Leibler (KL)
divergence from the true posterior, working within the RFS framework to
incorporate uncertainty in target existence. The derivation is interpreted as
an application of expectation-maximisation (EM), where the missing data is the
correspondence of Bernoulli components (i.e., tracks) under each data
association hypothesis. The missing data is shown to play an identical role to
the selection of an ordered distribution in the same ordered family in the set
JPDA algorithm. Subsequently, a special case of the proposed method is utilised
to provide an efficient approximation of the minimum mean optimal sub-pattern
assignment estimator. The performance of the proposed methods is demonstrated
in challenging scenarios in which up to twenty targets come into close
proximity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5029</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5029</id><created>2014-03-19</created><updated>2015-09-15</updated><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Chang</keyname><forenames>Jae-Woong</forenames></author><author><keyname>Lin</keyname><forenames>Lilong</forenames></author><author><keyname>Minn</keyname><forenames>Kay</forenames></author><author><keyname>Wu</keyname><forenames>Baolin</forenames></author><author><keyname>Chien</keyname><forenames>Jeremy</forenames></author><author><keyname>Yong</keyname><forenames>Jeongsik</forenames></author><author><keyname>Zheng</keyname><forenames>Hui</forenames></author><author><keyname>Kuang</keyname><forenames>Rui</forenames></author></authors><title>Network-based Isoform Quantification with RNA-Seq Data for Cancer
  Transcriptome Analysis</title><categories>cs.CE cs.AI cs.LG</categories><doi>10.1371/journal.pcbi.1004465</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  High-throughput mRNA sequencing (RNA-Seq) is widely used for transcript
quantification of gene isoforms. Since RNA-Seq data alone is often not
sufficient to accurately identify the read origins from the isoforms for
quantification, we propose to explore protein domain-domain interactions as
prior knowledge for integrative analysis with RNA-seq data. We introduce a
Network-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) to
integrate protein domain-domain interaction network with short read alignments
for transcript abundance estimation. Based on our observation that the
abundances of the neighboring isoforms by domain-domain interactions in the
network are positively correlated, Net-RSTQ models the expression of the
neighboring transcripts as Dirichlet priors on the likelihood of the observed
read alignments against the transcripts in one gene. The transcript abundances
of all the genes are then jointly estimated with alternating optimization of
multiple EM problems. In simulation Net-RSTQ effectively improved isoform
transcript quantifications when isoform co-expressions correlate with their
interactions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, an
ovarian cancer cell line, and a breast cancer cell line also showed that
Net-RSTQ estimated more consistent isoform proportions with RNA-Seq data. In
the experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), the
transcript abundances estimated by Net-RSTQ are more informative for patient
sample classification of ovarian cancer, breast cancer and lung cancer. All
experimental results collectively support that Net-RSTQ is a promising approach
for isoform quantification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5045</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5045</id><created>2014-03-20</created><updated>2014-06-16</updated><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author><author><keyname>Eydgahi</keyname><forenames>Hoda</forenames></author><author><keyname>Eriksson</keyname><forenames>Brian</forenames></author></authors><title>Matroid Bandits: Fast Combinatorial Optimization with Learning</title><categories>cs.LG cs.AI cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A matroid is a notion of independence in combinatorial optimization which is
closely related to computational efficiency. In particular, it is well known
that the maximum of a constrained modular function can be found greedily if and
only if the constraints are associated with a matroid. In this paper, we bring
together the ideas of bandits and matroids, and propose a new class of
combinatorial bandits, matroid bandits. The objective in these problems is to
learn how to maximize a modular function on a matroid. This function is
stochastic and initially unknown. We propose a practical algorithm for solving
our problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,
gap-dependent and gap-free, on its regret. Both bounds are sublinear in time
and at most linear in all other quantities of interest. The gap-dependent upper
bound is tight and we prove a matching lower bound on a partition matroid
bandit. Finally, we evaluate our method on three real-world problems and show
that it is practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5049</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5049</id><created>2014-03-20</created><updated>2014-11-13</updated><authors><author><keyname>Mishra</keyname><forenames>Neha</forenames></author></authors><title>Hybrid Virtual network Embedding Algorithm with K-core Decomposition
  using Path Splitting</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network virtualization is an efficient approach of solving the ossification
problem of the Internet. It has become a promising way of supporting lots of
heterogeneous network onto substrate physical network. A major challenge in
network virtualization is how to map multiple virtual networks onto specific
nodes and links in the shared substrate network, known as virtual network
embedding problem. Due to its NPhardness, many heuristic approaches have been
proposed. In this thesis, we propose hybrid VN embedding algorithms that map
multiple VN requests with node and link constraints with K-core decomposition
using path splitting. Based on network pruning, a virtual network is decomposed
to core network and edge network. The mapping process is divided into two
phases: core network mapping and edge network mapping. Path splitting enables
better resource utilization by allowing the substrate to accept more VN
requests. It splits the available bandwidth of the path into small bandwidth to
satisfy the resource constraints. The proposed algorithm improves the
performance of the algorithm by splitting the path into small bandwidth. Due to
path splitting proposed algorithm will accept many virtual network requests
that will increase the revenue and acceptance ratio of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5058</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5058</id><created>2014-03-20</created><authors><author><keyname>Jucovschi</keyname><forenames>Constantin</forenames></author></authors><title>Towards an Interaction-based Integration of MKM Services into End-User
  Applications</title><categories>cs.HC</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Semantic Alliance (SAlly) Framework, first presented at MKM 2012, allows
integration of Mathematical Knowledge Management services into typical
applications and end-user workflows. From an architecture allowing invasion of
spreadsheet programs, it grew into a middle-ware connecting spreadsheet, CAD,
text and image processing environments with MKM services. The architecture
presented in the original paper proved to be quite resilient as it is still
used today with only minor changes.
  This paper explores extensibility challenges we have encountered in the
process of developing new services and maintaining the plugins invading
end-user applications. After an analysis of the underlying problems, I present
an augmented version of the SAlly architecture that addresses these issues and
opens new opportunities for document type agnostic MKM services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5071</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5071</id><created>2014-03-20</created><authors><author><keyname>Gurciullo</keyname><forenames>Stefano</forenames></author></authors><title>Organised crime infiltration in the legitimate private economy - An
  empirical network analysis approach</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>27 pages, 6, figures, International Crime and Intelligence Analysis
  Conference - 2012, Manchester</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is estimated that Italian Mafias registered 135 billion euros in profits
only in 2010. Part of this huge amount of money, coming mostly from the drugs,
prostitution and arms illicit markets, is often used to invest into legitimate
private economies. As a consequence, the affected economies destabilise, become
entrenched with violent forms of competition and are bound to stagnation.
Nonetheless, few are the attempts to uncover the patterns followed by criminal
organisations in their business ventures. The reason lays mostly in the poor
availability of data on criminal activity, or in the highly risky task of
gather it.
  This paper partially fills this gap thanks to access to information about the
Sicilian Mafia in a city. More specifically, it tries to analyse the nature and
extent of criminal infiltration into the legitimate private economy of the
case-study using network techniques. The research demonstrates that sectors
with a high degree of centrality and comprising fewer firms are the most
vulnerable to this kind of security threat. It also shows that centrality is
also the key criterion that makes a firm sensitive to infiltration, provided it
belongs to a susceptible economic sector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5081</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5081</id><created>2014-03-20</created><updated>2014-03-21</updated><authors><author><keyname>Schneider</keyname><forenames>Sven</forenames></author><author><keyname>Nestmann</keyname><forenames>Uwe</forenames></author></authors><title>Enforcing Operational Properties including Blockfreeness for
  Deterministic Pushdown Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which modifies a deterministic pushdown automaton
(DPDA) such that (i) the marked language is preserved, (ii) lifelocks are
removed, (iii) deadlocks are removed, (iv) all states and edges are accessible,
and (v) operational blockfreeness is established (i.e., coaccessibility in the
sense that every initial derivation can be continued to a marking
configuration). This problem can be trivially solved for deterministic finite
automata (DFA) but is not solvable for standard petri net classes. The
algorithm is required for an operational extension of the supervisory control
problem (SCP) to the situation where the specification in modeled by a DPDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5089</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5089</id><created>2014-03-20</created><updated>2015-06-22</updated><authors><author><keyname>Prasad</keyname><forenames>Ranga</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>On the Gaussian Many-to-One X Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory; Revised and
  updated version of the original draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Gaussian many-to-one X channel, which is a special case of
general multiuser X channel, is studied. In the Gaussian many-to-one X channel,
communication links exist between all transmitters and one of the receivers,
along with a communication link between each transmitter and its corresponding
receiver. As per the X channel assumption, transmission of messages is allowed
on all the links of the channel. This communication model is different from the
corresponding many-to-one interference channel (IC). Transmission strategies
which involve using Gaussian codebooks and treating interference from a subset
of transmitters as noise are formulated for the above channel. Sum-rate is used
as the criterion of optimality for evaluating the strategies. Initially, a $3
\times 3$ many-to-one X channel is considered and three transmission strategies
are analyzed. The first two strategies are shown to achieve sum-rate capacity
under certain channel conditions. For the third strategy, a sum-rate outer
bound is derived and the gap between the outer bound and the achieved rate is
characterized. These results are later extended to the $K \times K$ case. Next,
a region in which the many-to-one X channel can be operated as a many-to-one IC
without loss of sum-rate is identified. Further, in the above region, it is
shown that using Gaussian codebooks and treating interference as noise achieves
a rate point that is within $K/2 -1$ bits from the sum-rate capacity.
Subsequently, some implications of the above results to the Gaussian
many-to-one IC are discussed. Transmission strategies for the many-to-one IC
are formulated and channel conditions under which the strategies achieve
sum-rate capacity are obtained. A region where the sum-rate capacity can be
characterized to within $K/2-1$ bits is also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5107</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5107</id><created>2014-03-20</created><updated>2015-04-04</updated><authors><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Social and Natural Sciences Differ in Their Research Strategies, Adapted
  to Work for Different Knowledge Landscapes</title><categories>physics.soc-ph cs.DL</categories><comments>Formerly called: Simulations suggest that social and natural sciences
  differ in their research strategies adapted to work for different knowledge
  landscapes</comments><journal-ref>PLoS ONE 9(11): e113901. (2014)</journal-ref><doi>10.1371/journal.pone.0113901</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do different fields of knowledge require different research strategies? A
numerical model exploring different virtual knowledge landscapes, revealed two
diverging optimal search strategies. Trend following is maximized when the
popularity of new discoveries determine the number of individuals researching
it. This strategy works best when many researchers explore few large areas of
knowledge. In contrast, individuals or small groups of researchers are better
in discovering small bits of information in dispersed knowledge landscapes.
Bibliometric data of scientific publications showed a continuous bipolar
distribution of these strategies, ranging from natural sciences, with highly
cited publications in journals containing a large number of articles, to the
social sciences, with rarely cited publications in many journals containing a
small number of articles. The natural sciences seem to adapt their research
strategies to landscapes with large concentrated knowledge clusters, whereas
social sciences seem to have adapted to search in landscapes with many small
isolated knowledge clusters. Similar bipolar distributions were obtained when
comparing levels of insularity estimated by indicators of international
collaboration and levels of country-self citations: researchers in academic
areas with many journals such as social sciences, arts and humanities, were the
most isolated, and that was true in different regions of the world. The work
shows that quantitative measures estimating differences between academic
disciplines improve our understanding of different research strategies,
eventually helping interdisciplinary research and may be also help improve
science policies worldwide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5111</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5111</id><created>2014-03-20</created><updated>2014-04-03</updated><authors><author><keyname>Wotzlaw</keyname><forenames>Andreas</forenames></author></authors><title>On Solving the Maximum $k$-club Problem</title><categories>cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simple undirected graph $G$, the maximum $k$-club problem is to find
a maximum-cardinality subset of nodes inducing a subgraph of diameter at most
$k$ in $G$. This NP-hard generalization of clique, originally introduced to
model low diameter clusters in social networks, is of interest in network-based
data mining and clustering applications. We give two MAX-SAT formulations of
the problem and show that two exact methods resulting from our encodings
outperform significantly the state-of-the-art exact methods when evaluated both
on sparse and dense random graphs as well as on diverse real-life graphs from
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5115</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5115</id><created>2014-03-20</created><authors><author><keyname>Louche</keyname><forenames>Ugo</forenames><affiliation>LIF</affiliation></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames><affiliation>LIF</affiliation></author></authors><title>Unconfused Ultraconservative Multiclass Algorithms</title><categories>cs.LG</categories><comments>ACML, Australia (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of learning linear classifiers from noisy datasets in a
multiclass setting. The two-class version of this problem was studied a few
years ago by, e.g. Bylander (1994) and Blum et al. (1996): in these
contributions, the proposed approaches to fight the noise revolve around a
Perceptron learning scheme fed with peculiar examples computed through a
weighted average of points from the noisy training set. We propose to build
upon these approaches and we introduce a new algorithm called UMA (for
Unconfused Multiclass additive Algorithm) which may be seen as a generalization
to the multiclass setting of the previous approaches. In order to characterize
the noise we use the confusion matrix as a multiclass extension of the
classification noise studied in the aforementioned literature. Theoretically
well-founded, UMA furthermore displays very good empirical noise robustness, as
evidenced by numerical simulations conducted on both synthetic and real data.
Keywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5118</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5118</id><created>2014-03-20</created><authors><author><keyname>Lovelace</keyname><forenames>Robin</forenames></author><author><keyname>Malleson</keyname><forenames>Nick</forenames></author><author><keyname>Harland</keyname><forenames>Kirk</forenames></author><author><keyname>Birkin</keyname><forenames>Mark</forenames></author></authors><title>Geotagged tweets to inform a spatial interaction model: a case study of
  museums</title><categories>stat.ME cs.CY cs.SI</categories><comments>A concise version of this article was submitted to GISRUK2014
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the potential of volunteered geographical information
from social media for informing geographical models of behavior, based on a
case study of museums in Yorkshire, UK. A spatial interaction model of visitors
to 15 museums from 179 administrative zones is constructed to test this
potential. The main input dataset comprises geo-tagged messages harvested using
the Twitter Streaming Application Programming Interface (API), filtered,
analyzed and aggregated to allow direct comparison with the model's output.
Comparison between model output and tweet information allowed the calibration
of model parameters to optimize the fit between flows to museums inferred from
tweets and flow matrices generated by the spatial interaction model. We
conclude that volunteered geographic information from social media sites have
great potential for informing geographical models of behavior, especially if
the volume of geo-tagged social media messages continues to increase. However,
we caution that volunteered geographical information from social media has some
major limitations so should be used only as a supplement to more consistent
data sources or when official datasets are unavailable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5128</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5128</id><created>2014-03-20</created><authors><author><keyname>Pandey</keyname><forenames>Parul</forenames></author><author><keyname>Tripathi</keyname><forenames>Mahshwari</forenames></author></authors><title>A Novel Quorum Protocol</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the traditional mechanisms used in distributed systems for maintaining
the consistency of replicated data is voting.
  A problem involved in voting mechanisms is the size of the Quorums needed on
each access to the data. In this paper, we present a novel and efficient
distributed algorithm for managing replicated data. We impose a logical wheel
structure on the set of copies of an object. The protocol ensures minimum read
quorum size of one, by reading one copy of an object while guaranteeing
fault-tolerance of write operations.Wheel structure has a wider application
area as it can be imposed in a network with any number of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5142</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5142</id><created>2014-03-20</created><updated>2014-10-28</updated><authors><author><keyname>Shchekotykhin</keyname><forenames>Kostyantyn</forenames></author></authors><title>Interactive Debugging of ASP Programs</title><categories>cs.AI</categories><comments>Published in Proceedings of the 15th International Workshop on
  Non-Monotonic Reasoning (NMR 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broad application of answer set programming (ASP) for declarative problem
solving requires the development of tools supporting the coding process.
Program debugging is one of the crucial activities within this process.
Recently suggested ASP debugging approaches allow efficient computation of
possible explanations of a fault. However, even for a small program a debugger
might return a large number of possible explanations and selection of the
correct one must be done manually. In this paper we present an interactive
query-based ASP debugging method which extends previous approaches and finds a
preferred explanation by means of observations. The system queries a programmer
whether a set of ground atoms must be true in all (cautiously) or some
(bravely) answer sets of the program. Since some queries can be more
informative than the others, we discuss query selection strategies which, given
user's preferences for an explanation, can find the best query. That is, the
query an answer of which reduces the overall number of queries required for the
identification of a preferred explanation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5156</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5156</id><created>2014-03-20</created><updated>2014-07-31</updated><authors><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Cortes</keyname><forenames>Jesus M.</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author></authors><title>Synergy and redundancy in the Granger causal analysis of dynamical
  networks</title><categories>q-bio.QM cs.IT math.IT physics.data-an</categories><doi>10.1088/1367-2630/16/10/105003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze by means of Granger causality the effect of synergy and redundancy
in the inference (from time series data) of the information flow between
subsystems of a complex network. Whilst we show that fully conditioned Granger
causality is not affected by synergy, the pairwise analysis fails to put in
evidence synergetic effects.
  In cases when the number of samples is low, thus making the fully conditioned
approach unfeasible, we show that partially conditioned Granger causality is an
effective approach if the set of conditioning variables is properly chosen. We
consider here two different strategies (based either on informational content
for the candidate driver or on selecting the variables with highest pairwise
influences) for partially conditioned Granger causality and show that depending
on the data structure either one or the other might be valid. On the other
hand, we observe that fully conditioned approaches do not work well in presence
of redundancy, thus suggesting the strategy of separating the pairwise links in
two subsets: those corresponding to indirect connections of the fully
conditioned Granger causality (which should thus be excluded) and links that
can be ascribed to redundancy effects and, together with the results from the
fully connected approach, provide a better description of the causality pattern
in presence of redundancy. We finally apply these methods to two different real
datasets. First, analyzing electrophysiological data from an epileptic brain,
we show that synergetic effects are dominant just before seizure occurrences.
Second, our analysis applied to gene expression time series from HeLa culture
shows that the underlying regulatory networks are characterized by both
redundancy and synergy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5162</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5162</id><created>2014-03-20</created><authors><author><keyname>Busseniers</keyname><forenames>Evo</forenames></author></authors><title>General Centrality in a hypergraph</title><categories>cs.SI math.CO physics.soc-ph</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to present a centrality measurement for the nodes
of a hypergraph, by using existing literature which extends eigenvector
centrality from a graph to a hypergraph, and literature which give a general
centrality measurement for a graph. We will use this measurement to say more
about the number of communications in a hypergraph, to implement a learning
mechanism, and to construct certain networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5169</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5169</id><created>2014-03-20</created><authors><author><keyname>Li</keyname><forenames>Yunpeng</forenames></author><author><keyname>Li</keyname><forenames>Ya</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Defuzzify firstly or finally: Dose it matter in fuzzy DEMATEL under
  uncertain environment?</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision-Making Trial and Evaluation Laboratory (DEMATEL) method is widely
used in many real applications. With the desirable property of efficient
handling with the uncertain information in decision making, the fuzzy DEMATEL
is heavily studied. Recently, Dytczak and Ginda suggested to defuzzify the
fuzzy numbers firstly and then use the classical DEMATEL to obtain the final
result. In this short paper, we show that it is not reasonable in some
situations. The results of defuzzification at the first step are not coincide
with the results of defuzzification at the final step.It seems that the
alternative is to defuzzification in the final step in fuzzy DEMATEL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5170</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5170</id><created>2014-03-20</created><authors><author><keyname>Komenda</keyname><forenames>Jan</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Decentralized Supervisory Control with Communicating Supervisors Based
  on Top-Down Coordination Control</title><categories>math.OC cs.CY cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach to decentralized supervisory control
of large automata with communicating supervisors. We first generalize the
recently developed top-down architecture of multilevel coordination control
with a hierarchical structure of groups of subsystems, their respective
coordinators and supervisors. Namely, in the case where the equivalent
conditions for achieving a specification language fail to be satisfied, we
propose sufficient conditions for a distributed computation of the supremal
achievable sublanguage. We then apply the obtained constructive results of
multilevel coordination control to decentralized supervisory control with
communication, where local supervisors of subsystems within a group communicate
with each other via the coordinator of the group. Our approach is illustrated
by an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5171</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5171</id><created>2014-03-20</created><updated>2014-05-22</updated><authors><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Distributed Approximation Algorithms for Weighted Shortest Paths</title><categories>cs.DS</categories><comments>Full version of STOC 2014</comments><acm-class>C.2.4; F.2.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed network is modeled by a graph having $n$ nodes (processors) and
diameter $D$. We study the time complexity of approximating {\em weighted}
(undirected) shortest paths on distributed networks with a $O(\log n)$ {\em
bandwidth restriction} on edges (the standard synchronous \congest model). The
question whether approximation algorithms help speed up the shortest paths
(more precisely distance computation) was raised since at least 2004 by Elkin
(SIGACT News 2004). The unweighted case of this problem is well-understood
while its weighted counterpart is fundamental problem in the area of
distributed approximation algorithms and remains widely open. We present new
algorithms for computing both single-source shortest paths (\sssp) and
all-pairs shortest paths (\apsp) in the weighted case.
  Our main result is an algorithm for \sssp. Previous results are the classic
$O(n)$-time Bellman-Ford algorithm and an $\tilde O(n^{1/2+1/2k}+D)$-time
$(8k\lceil \log (k+1) \rceil -1)$-approximation algorithm, for any integer
$k\geq 1$, which follows from the result of Lenzen and Patt-Shamir (STOC 2013).
(Note that Lenzen and Patt-Shamir in fact solve a harder problem, and we use
$\tilde O(\cdot)$ to hide the $O(\poly\log n)$ term.) We present an $\tilde
O(n^{1/2}D^{1/4}+D)$-time $(1+o(1))$-approximation algorithm for \sssp. This
algorithm is {\em sublinear-time} as long as $D$ is sublinear, thus yielding a
sublinear-time algorithm with almost optimal solution. When $D$ is small, our
running time matches the lower bound of $\tilde \Omega(n^{1/2}+D)$ by Das Sarma
et al. (SICOMP 2012), which holds even when $D=\Theta(\log n)$, up to a
$\poly\log n$ factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5172</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5172</id><created>2014-03-20</created><authors><author><keyname>Bessa</keyname><forenames>Iury</forenames></author><author><keyname>Abreu</keyname><forenames>Renato</forenames></author><author><keyname>Filho</keyname><forenames>Jo&#xe3;o Edgar</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas</forenames></author></authors><title>SMT-Based Bounded Model Checking of Fixed-Point Digital Controllers</title><categories>cs.SY cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital controllers have several advantages with respect to their flexibility
and design's simplicity. However, they are subject to problems that are not
faced by analog controllers. In particular, these problems are related to the
finite word-length implementation that might lead to overflows, limit cycles,
and time constraints in fixed-point processors. This paper proposes a new
method to detect design's errors in digital controllers using a state-of-the
art bounded model checker based on satisfiability modulo theories. The
experiments with digital controllers for a ball and beam plant demonstrate that
the proposed method can be very effective in finding errors in digital
controllers than other existing approaches based on traditional simulations
tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5180</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5180</id><created>2014-03-20</created><authors><author><keyname>Pauwels</keyname><forenames>Edouard</forenames><affiliation>LAAS</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS, CTU/FEE</affiliation></author><author><keyname>Lasserre</keyname><forenames>Jean-Bernard Bernard</forenames><affiliation>LAAS</affiliation></author></authors><title>Inverse optimal control with polynomial optimization</title><categories>math.OC cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of optimal control, we consider the inverse problem of
Lagrangian identification given system dynamics and optimal trajectories. Many
of its theoretical and practical aspects are still open. Potential applications
are very broad as a reliable solution to the problem would provide a powerful
modeling tool in many areas of experimental science. We propose to use the
Hamilton-Jacobi-Bellman sufficient optimality conditions for the direct problem
as a tool for analyzing the inverse problem and propose a general method that
attempts at solving it numerically with techniques of polynomial optimization
and linear matrix inequalities. The relevance of the method is illustrated
based on simulations on academic examples under various settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5190</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5190</id><created>2014-03-20</created><updated>2014-05-05</updated><authors><author><keyname>Marsault</keyname><forenames>Victor</forenames></author><author><keyname>Sakarovitch</keyname><forenames>Jacques</forenames></author></authors><title>Rhythmic generation of infinite trees and languages</title><categories>cs.FL</categories><comments>21 pages, early version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work builds on the notion of breadth-first signature of infinite trees
and (prefix-closed) languages introduced by the authors in a previous work. We
focus here on periodic signatures, a case coming from the study of rational
base numeration systems; the language of integer representations in
base~$\frac{p}{q}$ has a purely periodic signature whose period is derived from
the Christoffel word of slope~$\frac{p}{q}$. Conversely, we characterise
languages whose signature are purely periodic as representations of integers in
such number systems with non-canonical alphabets of digits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5195</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5195</id><created>2014-03-20</created><authors><author><keyname>Barczyk</keyname><forenames>Martin</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>Deschaud</keyname><forenames>Jean-Emmanuel</forenames></author><author><keyname>Goulette</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Experimental Implementation of an Invariant Extended Kalman Filter-based
  Scan Matching SLAM</title><categories>cs.SY cs.RO</categories><journal-ref>Proceedings of the 2014 American Control Conference, Portland, OR,
  June 2014, pp. 4121-4126</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an application of the Invariant Extended Kalman Filter (IEKF)
design methodology to the scan matching SLAM problem. We review the theoretical
foundations of the IEKF and its practical interest of guaranteeing robustness
to poor state estimates, then implement the filter on a wheeled robot hardware
platform. The proposed design is successfully validated in experimental
testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5199</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5199</id><created>2014-03-20</created><authors><author><keyname>Chirkova</keyname><forenames>Rada</forenames></author><author><keyname>Yu</keyname><forenames>Ting</forenames></author></authors><title>Obtaining Information about Queries behind Views and Dependencies</title><categories>cs.DB cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problems of finding and determining certain query answers and
of determining containment between queries; each problem is formulated in
presence of materialized views and dependencies under the closed-world
assumption. We show a tight relationship between the problems in this setting.
Further, we introduce algorithms for solving each problem for those inputs
where all the queries and views are conjunctive, and the dependencies are
embedded weakly acyclic. We also determine the complexity of each problem under
the security-relevant complexity measure introduced by Zhang and Mendelzon in
2005. The problems studied in this paper are fundamental in ensuring correct
specification of database access-control policies, in particular in case of
fine-grained access control. Our approaches can also be applied in the areas of
inference control, secure data publishing, and database auditing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5204</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5204</id><created>2014-03-20</created><updated>2016-02-12</updated><authors><author><keyname>Wang</keyname><forenames>Hanlei</forenames></author></authors><title>Adaptive Control of Robot Manipulators With Uncertain Kinematics and
  Dynamics</title><categories>cs.SY cs.RO math.OC</categories><comments>18 pages, 7 figures, revised for improving the presentation and
  adding some contents and references based on the reviewers' and AE's comments
  from IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the adaptive control problem for robot
manipulators with both the uncertain kinematics and dynamics. We propose two
adaptive control schemes to realize the objective of task-space trajectory
tracking irrespective of the uncertain kinematics and dynamics. The proposed
controllers have the desirable separation property, and we also show that the
first adaptive controller with appropriate modifications can yield improved
performance, without the expense of conservative gain choice. The performance
of the proposed controllers is shown by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5206</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5206</id><created>2014-03-20</created><updated>2014-07-30</updated><authors><author><keyname>Chang</keyname><forenames>Yi</forenames></author><author><keyname>Tang</keyname><forenames>Lei</forenames></author><author><keyname>Inagaki</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>What is Tumblr: A Statistical Overview and Comparison</title><categories>cs.SI physics.soc-ph</categories><comments>This work is reported by MIT Technology Review at
  http://www.technologyreview.com/view/525966/the-anatomy-of-a-forgotten-social-network/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tumblr, as one of the most popular microblogging platforms, has gained
momentum recently. It is reported to have 166.4 millions of users and 73.4
billions of posts by January 2014. While many articles about Tumblr have been
published in major press, there is not much scholar work so far. In this paper,
we provide some pioneer analysis on Tumblr from a variety of aspects. We study
the social network structure among Tumblr users, analyze its user generated
content, and describe reblogging patterns to analyze its user behavior. We aim
to provide a comprehensive statistical overview of Tumblr and compare it with
other popular social services, including blogosphere, Twitter and Facebook, in
answering a couple of key questions: What is Tumblr? How is Tumblr different
from other social media networks? In short, we find Tumblr has more rich
content than other microblogging platforms, and it contains hybrid
characteristics of social networking, traditional blogosphere, and social
media. This work serves as an early snapshot of Tumblr that later work can
leverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5248</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5248</id><created>2014-03-20</created><updated>2014-04-08</updated><authors><author><keyname>Feghali</keyname><forenames>Carl</forenames></author><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Haiko</forenames></author></authors><title>NP-hardness results for partitioning graphs into disjoint cliques and a
  triangle-free subgraph</title><categories>cs.DM cs.CC</categories><comments>10 pages, 4 figures. The results in this paper can now be found,
  including further results, in our submission entitled &quot;On the Complexity of
  Partitioning a Graph into Disjoint Cliques and a Triangle-free Subgraph&quot;,
  arXiv:1403.5961</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the computational complexity of deciding whether the
vertices of a graph can be partitioned into a disjoint union of cliques and a
triangle-free subgraph. This problem is known to be $\NP$-complete on arbitrary
graphs. We show that this problem remains $\NP$-complete even when restricted
to planar graphs and perfect graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5250</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5250</id><created>2014-03-19</created><authors><author><keyname>Chhinkaniwala</keyname><forenames>Hitesh</forenames></author><author><keyname>Garg</keyname><forenames>Sanjay</forenames></author></authors><title>Privacy Gain Based Multi-Iterative k-Anonymization to Protect
  Respondents Privacy</title><categories>stat.ME cs.CR</categories><comments>8 pages</comments><journal-ref>IFRSA International Journal Of Computing, Vol 3, issue 2, April
  2013, pp. 85-92</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Huge volume of data from domain specific applications such as medical,
financial, telephone, shopping records and individuals are regularly generated.
Sharing of these data is proved to be beneficial for data mining application.
Since data mining often involves data that contains personally identifiable
information and therefore releasing such data may result in privacy breaches.
On one hand such data is an important asset to business decision making by
analyzing it. On the other hand data privacy concerns may prevent data owners
from sharing information for data analysis. In order to share data while
preserving privacy, data owner must come up with a solution which achieves the
dual goal of privacy preservation as well as accuracy of data mining task
mainly clustering and classification. Privacy Preserving Data Publishing (PPDP)
is a study of eliminating privacy threats like linkage attack while preserving
data utility by anonymizing data set before publishing. Proposed work is an
extension to k-anonymization where Privacy Gain (PrGain) has been computed for
selective anonymization for set of tuples. Classification and clustering
characteristics of original data and anonymized data using proposed algorithm
have been evaluated in terms of information loss, execution time, and privacy
achieved. Algorithm has been processed against standard data sets and analysis
shows that values for sensitive attributes are being preserved with minimal
information loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5287</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5287</id><created>2014-03-20</created><authors><author><keyname>Christiano</keyname><forenames>Paul</forenames></author></authors><title>Online Local Learning via Semidefinite Programming</title><categories>cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many online learning problems we are interested in predicting local
information about some universe of items. For example, we may want to know
whether two items are in the same cluster rather than computing an assignment
of items to clusters; we may want to know which of two teams will win a game
rather than computing a ranking of teams. Although finding the optimal
clustering or ranking is typically intractable, it may be possible to predict
the relationships between items as well as if you could solve the global
optimization problem exactly.
  Formally, we consider an online learning problem in which a learner
repeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarial
payoff depending on those labels. The learner's goal is to receive a payoff
nearly as good as the best fixed labeling of the items. We show that a simple
algorithm based on semidefinite programming can obtain asymptotically optimal
regret in the case where the number of possible labels is O(1), resolving an
open problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technical
contribution is a novel use and analysis of the log determinant regularizer,
exploiting the observation that log det(A + I) upper bounds the entropy of any
distribution with covariance matrix A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5290</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5290</id><created>2014-03-19</created><authors><author><keyname>Pucci</keyname><forenames>Daniele</forenames></author><author><keyname>Hamel</keyname><forenames>Tarek</forenames></author><author><keyname>Morin</keyname><forenames>Pascal</forenames></author><author><keyname>Samson</keyname><forenames>Claude</forenames></author></authors><title>Nonlinear Feedback Control of Axisymmetric Aerial Vehicles</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of simple aerodynamic models for the feedback control
of aerial vehicles with large flight envelopes. Thrust-propelled vehicles with
a body shape symmetric with respect to the thrust axis are considered. Upon a
condition on the aerodynamic characteristics of the vehicle, we show that the
equilibrium orientation can be explicitly determined as a function of the
desired flight velocity. This allows for the adaptation of previously proposed
control design approaches based on the thrust direction control paradigm.
Simulation results conducted by using measured aerodynamic characteristics of
quasi-axisymmetric bodies illustrate the soundness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5299</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5299</id><created>2014-03-20</created><authors><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Pietri</keyname><forenames>Riccardo</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>Privacy in Sensor-Driven Human Data Collection: A Guide for
  Practitioners</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the amount of information collected about human beings has
increased dramatically. This development has been partially driven by
individuals posting and storing data about themselves and friends using online
social networks or collecting their data for self-tracking purposes
(quantified-self movement). Across the sciences, researchers conduct studies
collecting data with an unprecedented resolution and scale. Using computational
power combined with mathematical models, such rich datasets can be mined to
infer underlying patterns, thereby providing insights into human nature. Much
of the data collected is sensitive. It is private in the sense that most
individuals would feel uncomfortable sharing their collected personal data
publicly. For this reason, the need for solutions to ensure the privacy of the
individuals generating data has grown alongside the data collection efforts.
Out of all the massive data collection efforts, this paper focuses on efforts
directly instrumenting human behavior, and notes that -- in many cases -- the
privacy of participants is not sufficiently addressed. For example, study
purposes are often not explicit, informed consent is ill-defined, and security
and sharing protocols are only partially disclosed. This paper provides a
survey of the work related to addressing privacy issues in research studies
that collect detailed sensor data on human behavior. Reflections on the key
problems and recommendations for future work are included. We hope the overview
of the privacy-related practices in massive data collection studies can be used
as a frame of reference for practitioners in the field. Although focused on
data collection in an academic context, we believe that many of the challenges
and solutions we identify are also relevant and useful for other domains where
massive data collection takes place, including businesses and governments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5315</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5315</id><created>2014-03-20</created><authors><author><keyname>Mehmetoglu</keyname><forenames>Mustafa</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>A Deterministic Annealing Optimization Approach for Witsenhausen's and
  Related Decentralized Control Settings</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>submitted to CDC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of mapping optimization in decentralized
control problems. A global optimization algorithm is proposed based on the
ideas of ``deterministic annealing&quot; - a powerful non-convex optimization
framework derived from information theoretic principles with analogies to
statistical physics. The key idea is to randomize the mappings and control the
Shannon entropy of the system during optimization. The entropy constraint is
gradually relaxed in a deterministic annealing process while tracking the
minimum, to obtain the ultimate deterministic mappings. Deterministic annealing
has been successfully employed in several problems including clustering, vector
quantization, regression, as well as the Witsenhausen's counterexample in our
recent work[1]. We extend our method to a more involved setting, a variation of
Witsenhausen's counterexample, where there is a side channel between the two
controllers. The problem can be viewed as a two stage cancellation problem. We
demonstrate that there exist complex strategies that can exploit the side
channel efficiently, obtaining significant gains over the best affine and known
non-linear strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5326</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5326</id><created>2014-03-20</created><updated>2015-05-12</updated><authors><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author><author><keyname>Brychkov</keyname><forenames>Yury A.</forenames></author><author><keyname>Freear</keyname><forenames>Steven</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Analytic Expressions and Bounds for Special Functions and Applications
  in Communication Theory</title><categories>cs.IT math.IT</categories><comments>63 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the derivation of novel analytic expressions and
bounds for a family of special functions that are useful in wireless
communication theory. These functions are the well-known Nuttall
$Q{-}$function, the incomplete Toronto function, the Rice $Ie$-function and the
incomplete Lipschitz-Hankel integrals.
  Capitalizing on the offered results, useful identities are additionally
derived between the above functions and the Humbert, $\Phi_{1}$, function as
well as for specific cases of the Kamp${\it \acute{e}}$ de F${\it
\acute{e}}$riet function. These functions can be considered useful mathematical
tools that can be employed in applications relating to the analytic performance
evaluation of modern wireless communication systems such as cognitive radio,
cooperative and free-space optical communications as well as radar, diversity
and multi-antenna systems. As an example, new closed-form expressions are
derived for the outage probability over non-linear generalized fading channels,
namely, $\alpha{-}\eta{-}\mu$, $\alpha{-}\lambda{-}\mu$ and
$\alpha{-}\kappa{-}\mu$ as well as for specific cases of the $\eta{-}\mu$ and
$\lambda{-}\mu$ fading channels. Furthermore, simple expressions are presented
for the channel capacity for the truncated channel inversion with fixed rate
and the corresponding optimum cut-off signal-to-noise ratio for single-and
multi-antenna communication systems over Rician fading channels. The accuracy
and validity of the derived expressions is justified through extensive
comparisons with respective numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5330</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5330</id><created>2014-03-20</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author></authors><title>Differential Dual-Hop Relaying over Time-Varying Rayleigh-Fading
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies dual-hop amplify-and-forward relaying over time-varying
Rayleigh fading channels with differential M-PSK modulation and non-coherent
detection. For the case of &quot;two-symbol&quot; detection, a first order time-series
model is utilized to characterize the time-varying nature of the cascaded
channel. Based on this model, an exact bit error rate (BER) expression is
derived and confirmed with simulation results. The obtained expression shows
that the BER is related to the auto-correlation of the cascaded channel and an
irreducible error floor exists at high transmit power. To overcome the error
floor experienced with fast-fading, a nearly optimal multiple-symbol
differential sphere detection (MSDSD) is also developed. The error performance
of MSDSD is illustrated with simulation results under different fading
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5331</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5331</id><created>2014-03-20</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author></authors><title>Differential Amplify-and-Forward Relaying in Time-Varying Rayleigh
  Fading Channels</title><categories>cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the performance of differential amplify-and-forward
(D-AF) relaying over time-varying Rayleigh fading channels. Using the
auto-regressive time-series model to characterize the time-varying nature of
the wireless channels, new weights for the maximum ratio combining (MRC) of the
received signals at the destination are proposed. Expression for the pair-wise
error probability (PEP) is provided and used to obtain an approximation of the
total average bit error probability (BEP). The obtained BEP approximation
clearly shows how the system performance depends on the auto-correlation of the
direct and the cascaded channels and an irreducible error floor exists at high
signal-to-noise ratio (SNR). Simulation results also demonstrate that, for
fast-fading channels, the new MRC weights lead to a better performance when
compared to the classical combining scheme. Our analysis is verified with
simulation results in different fading scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5337</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5337</id><created>2014-03-20</created><updated>2015-03-18</updated><authors><author><keyname>Aminfar</keyname><forenames>Amirhossein</forenames></author><author><keyname>Ambikasaran</keyname><forenames>Sivaram</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>A Fast Block Low-Rank Dense Solver with Applications to Finite-Element
  Matrices</title><categories>cs.NA math.NA</categories><doi>10.1016/j.jcp.2015.10.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a fast solver for the dense &quot;frontal&quot; matrices that
arise from the multifrontal sparse elimination process of 3D elliptic PDEs. The
solver relies on the fact that these matrices can be efficiently represented as
a hierarchically off-diagonal low-rank (HODLR) matrix. To construct the
low-rank approximation of the off-diagonal blocks, we propose a new
pseudo-skeleton scheme, the boundary distance low-rank approximation, that
picks rows and columns based on the location of their corresponding vertices in
the sparse matrix graph. We compare this new low-rank approximation method to
the adaptive cross approximation (ACA) algorithm and show that it achieves
betters speedup specially for unstructured meshes. Using the HODLR direct
solver as a preconditioner (with a low tolerance) to the GMRES iterative
scheme, we can reach machine accuracy much faster than a conventional LU
solver. Numerical benchmarks are provided for frontal matrices arising from 3D
finite element problems corresponding to a wide range of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5341</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5341</id><created>2014-03-20</created><updated>2015-06-08</updated><authors><author><keyname>Russo</keyname><forenames>Daniel</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>An Information-Theoretic Analysis of Thompson Sampling</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an information-theoretic analysis of Thompson sampling that
applies across a broad range of online optimization problems in which a
decision-maker must learn from partial feedback. This analysis inherits the
simplicity and elegance of information theory and leads to regret bounds that
scale with the entropy of the optimal-action distribution. This strengthens
preexisting results and yields new insight into how information improves
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5345</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5345</id><created>2014-03-20</created><authors><author><keyname>Zhang</keyname><forenames>Xiaoge</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Yang</keyname><forenames>Hai</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A Physarum-Inspired Approach to Optimal Supply Chain Network Design at
  Minimum Total Cost with Demand Satisfaction</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A supply chain is a system which moves products from a supplier to customers.
The supply chains are ubiquitous. They play a key role in all economic
activities. Inspired by biological principles of nutrients' distribution in
protoplasmic networks of slime mould Physarum polycephalum we propose a novel
algorithm for a supply chain design. The algorithm handles the supply networks
where capacity investments and product flows are variables. The networks are
constrained by a need to satisfy product demands. Two features of the slime
mould are adopted in our algorithm. The first is the continuity of a flux
during the iterative process, which is used in real-time update of the costs
associated with the supply links. The second feature is adaptivity. The supply
chain can converge to an equilibrium state when costs are changed. Practicality
and flexibility of our algorithm is illustrated on numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5346</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5346</id><created>2014-03-20</created><authors><author><keyname>Bampasidou</keyname><forenames>Maria</forenames></author><author><keyname>Gentimis</keyname><forenames>Thanos</forenames></author></authors><title>Modeling Collaborations with Persistent Homology</title><categories>math.AT cs.SI physics.soc-ph</categories><comments>11 pages, 5 figures</comments><msc-class>55N99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a model based on persistent homology that describes
interactions between mathematicians in terms of collaborations. Some ideas from
classical data analysis are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5348</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5348</id><created>2014-03-20</created><updated>2014-09-13</updated><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Coherent-Classical Estimation versus Purely-Classical Estimation for
  Linear Quantum Systems</title><categories>quant-ph cs.SY math.OC</categories><comments>7 pages, 5 figures. Minor corrections. Accepted, 2014 Conference on
  Decision and Control</comments><doi>10.1109/CDC.2014.7040294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a coherent-classical estimation scheme for a class of linear
quantum systems. It comprises an estimator that is a mixed quantum-classical
system without involving coherent feedback. The estimator yields a classical
estimate of a variable for the quantum plant. We demonstrate that for a passive
plant that can be characterized by annihilation operators only, such
coherent-classical estimation provides no improvement over purely-classical
estimation. An example is also given which shows that if the plant is not
assumed to be an annihilation operator only quantum system, it is possible to
get better estimates with such coherent-classical estimation compared with
purely-classical estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5350</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5350</id><created>2014-03-20</created><authors><author><keyname>Bonichon</keyname><forenames>Nicolas</forenames></author><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Perkovi&#x107;</keyname><forenames>Ljubomir</forenames></author><author><keyname>Xia</keyname><forenames>Ge</forenames></author></authors><title>There are Plane Spanners of Maximum Degree 4</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let E be the complete Euclidean graph on a set of points embedded in the
plane. Given a constant t &gt;= 1, a spanning subgraph G of E is said to be a
t-spanner, or simply a spanner, if for any pair of vertices u,v in E the
distance between u and v in G is at most t times their distance in E. A spanner
is plane if its edges do not cross.
  This paper considers the question: &quot;What is the smallest maximum degree that
can always be achieved for a plane spanner of E?&quot; Without the planarity
constraint, it is known that the answer is 3 which is thus the best known lower
bound on the degree of any plane spanner. With the planarity requirement, the
best known upper bound on the maximum degree is 6, the last in a long sequence
of results improving the upper bound. In this paper we show that the complete
Euclidean graph always contains a plane spanner of maximum degree at most 4 and
make a big step toward closing the question. Our construction leads to an
efficient algorithm for obtaining the spanner from Chew's L1-Delaunay
triangulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5352</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5352</id><created>2014-03-20</created><updated>2014-03-29</updated><authors><author><keyname>Hu</keyname><forenames>Anzhong</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Zhang</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author></authors><title>An ESPRIT-Based Approach for 2-D Localization of Incoherently
  Distributed Sources in Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 8 figures, 1 table, published in IEEE Journal of Selected
  Topics in Signal Processing</comments><journal-ref>IEEE Journal of Selected Topics in Signal Processing -- Special
  Issue on Signal Processing for Large-Scale MIMO Communications, Vol. 8, No.
  5, pp. 996 - 1011, October 2014</journal-ref><doi>10.1109/JSTSP.2014.2313409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an approach of estimating signal parameters via rotational
invariance technique (ESPRIT) is proposed for two-dimensional (2-D)
localization of incoherently distributed (ID) sources in large-scale/massive
multiple-input multiple-output (MIMO) systems. The traditional ESPRIT-based
methods are valid only for one-dimensional (1-D) localization of the ID
sources. By contrast, in the proposed approach the signal subspace is
constructed for estimating the nominal azimuth and elevation
direction-of-arrivals and the angular spreads. The proposed estimator enjoys
closed-form expressions and hence it bypasses the searching over the entire
feasible field. Therefore, it imposes significantly lower computational
complexity than the conventional 2-D estimation approaches. Our analysis shows
that the estimation performance of the proposed approach improves when the
large-scale/massive MIMO systems are employed. The approximate Cram\'{e}r-Rao
bound of the proposed estimator for the 2-D localization is also derived.
Numerical results demonstrate that albeit the proposed estimation method is
comparable with the traditional 2-D estimators in terms of performance, it
benefits from a remarkably lower computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5355</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5355</id><created>2014-03-20</created><updated>2014-04-01</updated><authors><author><keyname>Savvidy</keyname><forenames>Konstantin G.</forenames></author></authors><title>The MIXMAX random number generator</title><categories>hep-lat cs.MS nlin.CD</categories><comments>15 pages, 3 Figures</comments><report-no>NITS-PHY-2014003</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we give a practical solution to the problem of determining the
maximal period of matrix generators of pseudo-random numbers which are based on
an integer-valued unimodular matrix of size NxN known as MIXMAX and arithmetic
defined on a Galois field GF[p] with large prime modulus p. The existing theory
of Galois finite fields is adapted to the present case, and necessary and
sufficient condition to attain the maximum period is formulated. Three
efficient algorithms are presented. First, allowing to compute the
multiplication by the MIXMAX matrix with O(N) operations. Second, to
recursively compute the characteristic polynomial with O(N^2) operations, and
third, to apply skips of large number of steps S to the sequence in O(N^2
log(S)) operations. It is demonstrated that the dynamical properties of this
generator dramatically improve with the size of the matrix N, as compared to
the classes of generators based on sparse matrices and/or sparse characteristic
polynomials. Finally, we present the implementation details of the generator
and the results of rigorous statistical testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5361</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5361</id><created>2014-03-21</created><authors><author><keyname>Corbetta</keyname><forenames>Alessandro</forenames></author><author><keyname>Muntean</keyname><forenames>Adrian</forenames></author><author><keyname>Toschi</keyname><forenames>Federico</forenames></author><author><keyname>Vafayi</keyname><forenames>Kiamars</forenames></author></authors><title>Parameter Estimation of Social Forces in Crowd Dynamics Models via a
  Probabilistic Method</title><categories>physics.data-an cs.SI math.PR math.ST physics.soc-ph stat.TH</categories><comments>20 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on a specific crowd dynamics situation, including real life
experiments and measurements, our paper targets a twofold aim: (1) we present a
Bayesian probabilistic method to estimate the value and the uncertainty (in the
form of a probability density function) of parameters in crowd dynamic models
from the experimental data; and (2) we introduce a fitness measure for the
models to classify a couple of model structures (forces) according to their
fitness to the experimental data, preparing the stage for a more general
model-selection and validation strategy inspired by probabilistic data
analysis. Finally, we review the essential aspects of our experimental setup
and measurement technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5364</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5364</id><created>2014-03-21</created><updated>2014-09-25</updated><authors><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques E.</forenames></author></authors><title>Control Contraction Metrics, Robust Control and Observer Duality</title><categories>math.OC cs.SY</categories><comments>Conference submission and working paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problems of stabilization, robust control, and
observer design for nonlinear systems. We build upon recently a proposed method
based on contraction theory and convex optimization, extending the class of
systems to which it is applicable. We prove converse results for mechanical
systems and feedback-linearizable systems. Next we consider robust control, and
give a simple construction of a controller guaranteeing an L2-gain condition,
and discuss connections to nonlinear H-infinity control. Finally, we discuss a
&quot;duality&quot; result between nonlinear stabilization problems and observer
construction, in the process constructing globally stable reduced-order
observers for a class of nonlinear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5370</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5370</id><created>2014-03-21</created><authors><author><keyname>Dubois</keyname><forenames>Mathieu</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Emmanuelle</keyname><forenames>Frenoux</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Tarroux</keyname><forenames>Philippe</forenames><affiliation>LIMSI</affiliation></author></authors><title>Using n-grams models for visual semantic place recognition</title><categories>stat.ML cs.CV cs.LG</categories><comments>VISAPP (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a new method for visual place
recognition. Our system combines global image characterization and visual
words, which allows to use efficient Bayesian filtering methods to integrate
several images. More precisely, we extend the classical HMM model with
techniques inspired by the field of Natural Language Processing. This paper
presents our system and the Bayesian filtering algorithm. The performance of
our system and the influence of the main parameters are evaluated on a standard
database. The discussion highlights the interest of using such models and
proposes improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5374</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5374</id><created>2014-03-21</created><authors><author><keyname>Tang</keyname><forenames>Justin Z.</forenames></author><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author></authors><title>Transverse Contraction Criteria for Stability of Nonlinear Hybrid Limit
  Cycles</title><categories>math.OC cs.RO cs.SY</categories><comments>Conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive differential conditions guaranteeing the orbital
stability of nonlinear hybrid limit cycles. These conditions are represented as
a series of pointwise linear matrix inequalities (LMI), enabling the search for
stability certificates via convex optimization tools such as sum-of-squares
programming. Unlike traditional Lyapunov-based methods, the transverse
contraction framework developed in this paper enables proof of stability for
hybrid systems, without prior knowledge of the exact location of the stable
limit cycle in state space. This methodology is illustrated on a dynamic
walking example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5381</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5381</id><created>2014-03-21</created><authors><author><keyname>Huang</keyname><forenames>Silu</forenames></author><author><keyname>Fu</keyname><forenames>Ada Wai-Chee</forenames></author></authors><title>({\alpha}, k)-Minimal Sorting and Skew Join in MPI and MapReduce</title><categories>cs.DB</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As computer clusters are found to be highly effective for handling massive
datasets, the design of efficient parallel algorithms for such a computing
model is of great interest. We consider ({\alpha}, k)-minimal algorithms for
such a purpose, where {\alpha} is the number of rounds in the algorithm, and k
is a bound on the deviation from perfect workload balance. We focus on new
({\alpha}, k)-minimal algorithms for sorting and skew equijoin operations for
computer clusters. To the best of our knowledge the proposed sorting and skew
join algorithms achieve the best workload balancing guarantee when compared to
previous works. Our empirical study shows that they are close to optimal in
workload balancing. In particular, our proposed sorting algorithm is around 25%
more efficient than the state-of-the-art Terasort algorithm and achieves
significantly more even workload distribution by over 50%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5384</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5384</id><created>2014-03-21</created><updated>2014-06-25</updated><authors><author><keyname>Iraji</keyname><forenames>Reza</forenames></author><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author></authors><title>NUROA: A Numerical Roadmap Algorithm</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion planning has been studied for nearly four decades now. Complete,
combinatorial motion planning approaches are theoretically well-rooted with
completeness guarantees but they are hard to implement. Sampling-based and
heuristic methods are easy to implement and quite simple to customize but they
lack completeness guarantees. Can the best of both worlds be ever achieved,
particularly for mission critical applications such as robotic surgery, space
explorations, and handling hazardous material? In this paper, we answer
affirmatively to that question. We present a new methodology, NUROA, to
numerically approximate the Canny's roadmap, which is a network of
one-dimensional algebraic curves. Our algorithm encloses the roadmap with a
chain of tiny boxes each of which contains a piece of the roadmap and whose
connectivity captures the roadmap connectivity. It starts by enclosing the
entire space with a box. In each iteration, remaining boxes are shrunk on all
sides and then split into smaller sized boxes. Those boxes that are empty are
detected in the shrink phase and removed. The algorithm terminates when all
remaining boxes are smaller than a resolution that can be either given as input
or automatically computed using root separation lower bounds. Shrink operation
is cast as a polynomial optimization with semialgebraic constraints, which is
in turn transformed into a (series of) semidefinite programs (SDP) using the
Lasserre's approach. NUROA's success is due to fast SDP solvers. NUROA
correctly captured the connectivity of multiple curves/skeletons whereas
competitors such as IBEX and Realpaver failed in some cases. Since boxes are
independent from one another, NUROA can be parallelized particularly on GPUs.
NUROA is available as an open source package at http://nuroa.sourceforge.net/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5392</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5392</id><created>2014-03-21</created><authors><author><keyname>Shah</keyname><forenames>Gita</forenames></author><author><keyname>Annappa</keyname></author><author><keyname>Shet</keyname><forenames>K. C.</forenames></author></authors><title>Design Architecture-Based on Web Server and Application Cluster in Cloud
  Environment</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud has been a computational and storage solution for many data centric
organizations. The problem today those organizations are facing from the cloud
is in data searching in an efficient manner. A framework is required to
distribute the work of searching and fetching from thousands of computers. The
data in HDFS is scattered and needs lots of time to retrieve. The major idea is
to design a web server in the map phase using the jetty web server which will
give a fast and efficient way of searching data in MapReduce paradigm. For real
time processing on Hadoop, a searchable mechanism is implemented in HDFS by
creating a multilevel index in web server with multi-level index keys. The web
server uses to handle traffic throughput. By web clustering technology we can
improve the application performance. To keep the work down, the load balancer
should automatically be able to distribute load to the newly added nodes in the
server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5398</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5398</id><created>2014-03-21</created><authors><author><keyname>Devroey</keyname><forenames>Xavier</forenames></author><author><keyname>Perrouin</keyname><forenames>Gilles</forenames></author><author><keyname>Cordy</keyname><forenames>Maxime</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Schobbens</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Heymans</keyname><forenames>Patrick</forenames></author></authors><title>State Machine Flattening: Mapping Study and Assessment</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State machine formalisms equipped with hierarchy and parallelism allow to
compactly model complex system behaviours. Such models can then be transformed
into executable code or inputs for model-based testing and verification
techniques. Generated artifacts are mostly flat descriptions of system
behaviour. \emph{Flattening} is thus an essential step of these
transformations. To assess the importance of flattening, we have defined and
applied a systematic mapping process and 30 publications were finally selected.
However, it appeared that flattening is rarely the sole focus of the
publications and that care devoted to the description and validation of
flattening techniques varies greatly. Preliminary assessment of associated tool
support indicated limited tool availability and scalability on challenging
models. We see this initial investigation as a first step towards generic
flattening techniques and scalable tool support, cornerstones of reliable
model-based behavioural development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5401</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5401</id><created>2014-03-21</created><authors><author><keyname>Renny</keyname><forenames>Renny</forenames></author><author><keyname>Chandra</keyname><forenames>Reza</forenames></author><author><keyname>Ruhama</keyname><forenames>Syamsi</forenames></author><author><keyname>Sarjono</keyname><forenames>Mochammad Wisuda</forenames></author></authors><title>Exploring Indonesian Web Based Career Center Discrepancy of Web
  Popularity and Type of Services</title><categories>cs.CY</categories><comments>Journal of Advances in Computer Science and its Applications</comments><journal-ref>UACEE International Journal of Advances in Computer Science and
  its Applications IJCSIA Volume 3 Issue 2 Publication Date 5 June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilization of the Internet in higher education focus on the learning process
or the provision of academic information for students. The subject of this
research is in the form of web-based management alumnus Career Center with
specific sub domain. Colleges that already have a Career Center only 34 of the
264 colleges as sample. Type the service the most are information jobs, while
others are still rarely available as a forum of alumni and career consultation.
Ownership Career Center contributed to the popularity of college website.
Providing services such as communication and consultation career impact on the
popularity of the Career Center website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5403</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5403</id><created>2014-03-21</created><updated>2014-10-14</updated><authors><author><keyname>Chierchia</keyname><forenames>Giovanni</forenames></author><author><keyname>Pustelnik</keyname><forenames>Nelly</forenames></author><author><keyname>Pesquet-Popescu</keyname><forenames>Beatrice</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author></authors><title>A Non-Local Structure Tensor Based Approach for Multicomponent Image
  Recovery Problems</title><categories>cs.CV cs.NA math.OC</categories><doi>10.1109/TIP.2014.2364141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Local Total Variation (NLTV) has emerged as a useful tool in variational
methods for image recovery problems. In this paper, we extend the NLTV-based
regularization to multicomponent images by taking advantage of the Structure
Tensor (ST) resulting from the gradient of a multicomponent image. The proposed
approach allows us to penalize the non-local variations, jointly for the
different components, through various $\ell_{1,p}$ matrix norms with $p \ge 1$.
To facilitate the choice of the hyper-parameters, we adopt a constrained convex
optimization approach in which we minimize the data fidelity term subject to a
constraint involving the ST-NLTV regularization. The resulting convex
optimization problem is solved with a novel epigraphical projection method.
This formulation can be efficiently implemented thanks to the flexibility
offered by recent primal-dual proximal algorithms. Experiments are carried out
for multispectral and hyperspectral images. The results demonstrate the
interest of introducing a non-local structure tensor regularization and show
that the proposed approach leads to significant improvements in terms of
convergence speed over current state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5427</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5427</id><created>2014-03-21</created><authors><author><keyname>Cerf</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>The quasispecies regime for the simple genetic algorithm with ranking
  selection</title><categories>math.PR cs.NE</categories><msc-class>60J10 (Primary) 92D15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the simple genetic algorithm with a ranking selection mechanism
(linear ranking or tournament). We denote by $\ell$ the length of the
chromosomes, by $m$ the population size, by $p_C$ the crossover probability and
by $p_M$ the mutation probability. We introduce a parameter $\sigma$, called
the selection drift, which measures the selection intensity of the fittest
chromosome. We show that the dynamics of the genetic algorithm depend in a
critical way on the parameter $$\pi \,=\,\sigma(1-p_C)(1-p_M)^\ell\,.$$ If
$\pi&lt;1$, then the genetic algorithm operates in a disordered regime: an
advantageous mutant disappears with probability larger than $1-1/m^\beta$,
where $\beta$ is a positive exponent. If $\pi&gt;1$, then the genetic algorithm
operates in a quasispecies regime: an advantageous mutant invades a positive
fraction of the population with probability larger than a constant $p^*$ (which
does not depend on $m$). We estimate next the probability of the occurrence of
a catastrophe (the whole population falls below a fitness level which was
previously reached by a positive fraction of the population). The asymptotic
results suggest the following rules: $\pi=\sigma(1-p_C)(1-p_M)^\ell$ should be
slightly larger than $1$; $p_M$ should be of order $1/\ell$; $m$ should be
larger than $\ell\ln\ell$; the running time should be of exponential order in
$m$. The first condition requires that $ \ell p_M +p_C&lt; \ln\sigma$. These
conclusions must be taken with great care: they come from an asymptotic regime,
and it is a formidable task to understand the relevance of this regime for a
real-world problem. At least, we hope that these conclusions provide
interesting guidelines for the practical implementation of the simple genetic
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5432</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5432</id><created>2014-03-21</created><authors><author><keyname>Kl&#xe4;s</keyname><forenames>Michael</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author></authors><title>Comprehensive Landscapes for Software-related Quality Models</title><categories>cs.SE</categories><comments>6 pages. Proceedings of the Workshop
  &quot;Software-Qualit\&quot;atsmodellierung und -bewertung&quot; (SQMB '09), Kaiserslautern,
  Germany, March 3 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Managing quality (such as service availability or process adherence) during
the development, operation, and maintenance of software(-intensive) systems and
services is a challenging task. Although many organizations need to define,
control, measure, and improve various quality aspects of their devel- opment
artifacts and processes, nearly no guidance is available on how to select,
adapt, define, combine, use, and evolve quality models. Catalogs of quality
models as well as selection and tailoring processes are widely missing. One
essential reason for this tremendous lack of support is that software
development is a highly context-dependent process. Therefore, quality models
always need to be adaptable to the respective project goals and contexts. A
first step towards better support for selecting and adapting quality models can
be seen in a classification of existing quality models, especially with respect
to their suitability for different purposes and contexts. Such a classification
of quality models can be applied to provide an integrated overview of the
variety of quality models. This article presents the idea of so called
comprehensive quality model landscapes (CQMLs), which provide a classification
scheme for quality models and help to get an overview of existing quality
models and their relationships. The article describes the usage goals for such
landscapes, presents a classification scheme, presents the initial concept of
such landscapes, illustrates the concept with selected examples, and sketches
open questions and future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5462</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5462</id><created>2014-03-21</created><updated>2014-08-06</updated><authors><author><keyname>Baillieul</keyname><forenames>John</forenames></author><author><keyname>Kong</keyname><forenames>Zhaodan</forenames></author></authors><title>Saliency Based Control in Random Feature Networks</title><categories>cs.SY</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to rapidly focus attention and react to salient environmental
features enables animals to move agiley through their habitats. To replicate
this kind of high-performance control of movement in synthetic systems, we
propose a new approach to feedback control that bases control actions on
randomly perceived features. Connections will be made with recent work
incorporating communication protocols into networked control systems. The
concepts of {\em random channel controllability} and {\em random channel
observability} for LTI control systems are introduced and studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5464</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5464</id><created>2014-03-20</created><updated>2015-09-25</updated><authors><author><keyname>Vaccon</keyname><forenames>Tristan</forenames><affiliation>IRMAR</affiliation></author></authors><title>Matrix-F5 algorithms over finite-precision complete discrete valuation
  fields</title><categories>cs.SC math.AC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(f\_1,\dots, f\_s) \in \mathbb{Q}\_p [X\_1,\dots, X\_n]^s$ be a sequence
of homogeneous polynomials with $p$-adic coefficients. Such system may happen,
for example, in arithmetic geometry. Yet, since $\mathbb{Q}\_p$ is not an
effective field, classical algorithm does not apply.We provide a definition for
an approximate Gr{\&quot;o}bner basis with respect to a monomial order $w.$ We
design a strategy to compute such a basis, when precision is enough and under
the assumption that the input sequence is regular and the ideals $\langle
f\_1,\dots,f\_i \rangle$ are weakly-$w$-ideals. The conjecture of Moreno-Socias
states that for the grevlex ordering, such sequences are generic.Two variants
of that strategy are available, depending on whether one lean more on precision
or time-complexity. For the analysis of these algorithms, we study the loss of
precision of the Gauss row-echelon algorithm, and apply it to an adapted
Matrix-F5 algorithm. Numerical examples are provided.Moreover, the fact that
under such hypotheses, Gr{\&quot;o}bner bases can be computed stably has many
applications. Firstly, the mapping sending $(f\_1,\dots,f\_s)$ to the reduced
Gr{\&quot;o}bner basis of the ideal they span is differentiable, and its
differential can be given explicitly. Secondly, these hypotheses allows to
perform lifting on the Grobner bases, from $\mathbb{Z}/p^k \mathbb{Z}$ to
$\mathbb{Z}/p^{k+k'} \mathbb{Z}$ or $\mathbb{Z}.$ Finally, asking for the same
hypotheses on the highest-degree homogeneous components of the entry
polynomials allows to extend our strategy to the affine case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5468</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5468</id><created>2014-03-21</created><authors><author><keyname>Shu</keyname><forenames>Jian-Jun</forenames></author><author><keyname>Wang</keyname><forenames>Qi-Wen</forenames></author></authors><title>Beyond Parrondo's paradox</title><categories>cs.GT</categories><journal-ref>Scientific Reports, Vol. 4, No. 4244, pp. 1-9, 2014</journal-ref><doi>10.1038/srep04244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Parrondo's paradox is a counterintuitive phenomenon where
individually-losing strategies can be combined in producing a winning
expectation. In this paper, the issues surrounding the Parrondo's paradox are
investigated. The focus is lying on testifying whether the same paradoxical
effect can be reproduced by using a simple capital dependent game. The
paradoxical effect generated by the Parrondo's paradox can be explained by
placing all the parameters in one probability space. Based on this framework,
it is able to generate other possible paradoxical effects by manipulating the
parameters in the probability space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5473</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5473</id><created>2014-03-10</created><authors><author><keyname>Gharbia</keyname><forenames>Reham</forenames></author><author><keyname>Azar</keyname><forenames>Ahmad Taher</forenames></author><author><keyname>Baz</keyname><forenames>Ali El</forenames></author><author><keyname>Hassanien</keyname><forenames>Aboul Ella</forenames></author></authors><title>Image Fusion Techniques in Remote Sensing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote sensing image fusion is an effective way to use a large volume of data
from multisensor images. Most earth satellites such as SPOT, Landsat 7, IKONOS
and QuickBird provide both panchromatic (Pan) images at a higher spatial
resolution and multispectral (MS) images at a lower spatial resolution and many
remote sensing applications require both high spatial and high spectral
resolutions, especially for GIS based applications. An effective image fusion
technique can produce such remotely sensed images. Image fusion is the
combination of two or more different images to form a new image by using a
certain algorithm to obtain more and better information about an object or a
study area than. The image fusion is performed at three different processing
levels which are pixel level, feature level and decision level according to the
stage at which the fusion takes place. There are many image fusion methods that
can be used to produce high resolution multispectral images from a high
resolution pan image and low resolution multispectral images. This paper
explores the major remote sensing data fusion techniques at pixel level and
reviews the concept, principals, limitations and advantages for each technique.
This paper focused on traditional techniques like intensity hue-saturation-
(HIS), Brovey, principal component analysis (PCA) and Wavelet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5475</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5475</id><created>2014-03-03</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>K.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>P.</forenames></author></authors><title>An Efficient Method for Face Recognition System In Various Assorted
  Conditions</title><categories>cs.CV</categories><comments>9 figures and 5 pages. arXiv admin note: substantial text overlap
  with arXiv:1401.6108</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the beginning stage, face verification is done using easy method of
geometric algorithm models, but the verification route has now developed into a
scientific progress of complicated geometric representation and identical
procedure. In recent years the technologies have boosted face recognition
system into the healthy focus. Researchers currently undergoing strong research
on finding face recognition system for wider area information taken under
hysterical elucidation dissimilarity. The proposed face recognition system
consists of a narrative expositionindiscreet preprocessing method, a hybrid
Fourier-based facial feature extraction and a score fusion scheme. We have
verified the face recognition in different lightening conditions (day or night)
and at different locations (indoor or outdoor). Preprocessing, Image detection,
Feature- extraction and Face recognition are the methods used for face
verification system. This paper focuses mainly on the issue of toughness to
lighting variations. The proposed system has obtained an average of 88.1%
verification rate on Two-Dimensional images under different lightening
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5479</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5479</id><created>2014-03-21</created><updated>2014-09-11</updated><authors><author><keyname>Olmos</keyname><forenames>Felipe</forenames></author><author><keyname>Kauffmann</keyname><forenames>Bruno</forenames></author><author><keyname>Simonian</keyname><forenames>Alain</forenames></author><author><keyname>Carlinet</keyname><forenames>Yannick</forenames></author></authors><title>Catalog Dynamics: Impact of Content Publishing and Perishing on the
  Performance of a LRU Cache</title><categories>cs.NI cs.PF math.PR</categories><comments>13 Pages, 9 figures. Full version of the article submitted to the ITC
  2014 conference. Small corrections in the appendix from the previous version</comments><msc-class>60, 62P30</msc-class><acm-class>C.2; G.3; C.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet heavily relies on Content Distribution Networks and transparent
caches to cope with the ever-increasing traffic demand of users. Content,
however, is essentially versatile: once published at a given time, its
popularity vanishes over time. All requests for a given document are then
concentrated between the publishing time and an effective perishing time.
  In this paper, we propose a new model for the arrival of content requests,
which takes into account the dynamical nature of the content catalog. Based on
two large traffic traces collected on the Orange network, we use the
semi-experimental method and determine invariants of the content request
process. This allows us to define a simple mathematical model for content
requests; by extending the so-called &quot;Che approximation&quot;, we then compute the
performance of a LRU cache fed with such a request process, expressed by its
hit ratio. We numerically validate the good accuracy of our model by comparison
to trace-based simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5488</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5488</id><created>2014-03-21</created><authors><author><keyname>Leke</keyname><forenames>Collins</forenames></author><author><keyname>Twala</keyname><forenames>Bhekisipho</forenames></author><author><keyname>Marwala</keyname><forenames>T.</forenames></author></authors><title>Missing Data Prediction and Classification: The Use of Auto-Associative
  Neural Networks and Optimization Algorithms</title><categories>cs.NE cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents methods which are aimed at finding approximations to
missing data in a dataset by using optimization algorithms to optimize the
network parameters after which prediction and classification tasks can be
performed. The optimization methods that are considered are genetic algorithm
(GA), simulated annealing (SA), particle swarm optimization (PSO), random
forest (RF) and negative selection (NS) and these methods are individually used
in combination with auto-associative neural networks (AANN) for missing data
estimation and the results obtained are compared. The methods suggested use the
optimization algorithms to minimize an error function derived from training the
auto-associative neural network during which the interrelationships between the
inputs and the outputs are obtained and stored in the weights connecting the
different layers of the network. The error function is expressed as the square
of the difference between the actual observations and predicted values from an
auto-associative neural network. In the event of missing data, all the values
of the actual observations are not known hence, the error function is
decomposed to depend on the known and unknown variable values. Multi-layer
perceptron (MLP) neural network is employed to train the neural networks using
the scaled conjugate gradient (SCG) method. Prediction accuracy is determined
by mean squared error (MSE), root mean squared error (RMSE), mean absolute
error (MAE), and correlation coefficient (r) computations. Accuracy in
classification is obtained by plotting ROC curves and calculating the areas
under these. Analysis of results depicts that the approach using RF with AANN
produces the most accurate predictions and classifications while on the other
end of the scale is the approach which entails using NS with AANN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5508</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5508</id><created>2014-03-21</created><authors><author><keyname>Costantini</keyname><forenames>Stefania</forenames></author></authors><title>Towards Active Logic Programming</title><categories>cs.AI</categories><comments>This work was presented at the 2nd International Workshop on
  Component-based Software Development in Computational Logic (COCL 1999). In
  this paper, the DALI language was first introduced</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the new logic programming language DALI, aimed at
defining agents and agent systems. A main design objective for DALI has been
that of introducing in a declarative fashion all the essential features, while
keeping the language as close as possible to the syntax and semantics of the
plain Horn--clause language. Special atoms and rules have been introduced, for
representing: external events, to which the agent is able to respond
(reactivity); actions (reactivity and proactivity); internal events (previous
conclusions which can trigger further activity); past and present events (to be
aware of what has happened). An extended resolution is provided, so that a DALI
agent is able to answer queries like in the plain Horn--clause language, but is
also able to cope with the different kinds of events, and exhibit a (rational)
reactive and proactive behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5521</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5521</id><created>2014-03-21</created><authors><author><keyname>Formentin</keyname><forenames>Simone</forenames></author><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Zaccarian</keyname><forenames>Luca</forenames></author><author><keyname>Savaresi</keyname><forenames>Sergio M.</forenames></author></authors><title>Scenario optimization with certificates and applications to anti-windup
  design</title><categories>cs.SY math.OC</categories><comments>8 pages, 4 figures, 1 table. Submitted to 53rd IEEE Conference on
  Decision and Control (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a significant extension, called scenario with
certificates (SwC), of the so-called scenario approach for uncertain
optimization problems. This extension is motivated by the observation that in
many control problems only some of the optimization variables are used in the
design phase, while the other variables play the role of certificates. Examples
are all those control problems that can be reformulated in terms of linear
matrix inequalities involving parameter-dependent Lyapunov functions. These
control problems include static anti-windup compensator design for uncertain
linear systems with input saturation, where the goal is the minimization of the
nonlinear gain from an exogenous input to a performance output. The main
contribution of this paper is to show that randomization is a useful tool,
specifically for anti-windup design, to make the overall approach less
conservative compared to its robust counterpart. In particular, we demonstrate
that the scenario with certificates reformulation is appealing because it
provides a way to implicitly design the parameter-dependent Lyapunov functions.
Finally, to further reduce the computational cost of this one-shot approach, we
present a sequential randomized algorithm for iteratively solving this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5524</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5524</id><created>2014-03-21</created><updated>2014-08-15</updated><authors><author><keyname>McLaughlin</keyname><forenames>Brendan M.</forenames></author><author><keyname>Ballance</keyname><forenames>Connor P.</forenames></author></authors><title>Petascale computations for Large-scale Atomic and Molecular collisions</title><categories>cs.DC physics.atom-ph physics.comp-ph</categories><comments>14 pages, 5 figures, 3 tables, Chapter in: Workshop on Sustained
  Simulated Performance 2013, Published by Springer, 2014, edited by Michael
  Resch, Yevgeniya Kovalenko, Eric Focht, Wolfgang Bez and Hiroaki Kobaysahi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Petaflop architectures are currently being utilized efficiently to perform
large scale computations in Atomic, Molecular and Optical Collisions. We solve
the Schroedinger or Dirac equation for the appropriate collision problem using
the R-matrix or R-matrix with pseudo-states approach. We briefly outline the
parallel methodology used and implemented for the current suite of Breit-Pauli
and DARC codes. Various examples are shown of our theoretical results compared
with those obtained from Synchrotron Radiation facilities and from Satellite
observations. We also indicate future directions and implementation of the
R-matrix codes on emerging GPU architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5543</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5543</id><created>2014-03-21</created><authors><author><keyname>Vergne</keyname><forenames>Ana&#xef;s</forenames><affiliation>LTCI</affiliation></author><author><keyname>Flint</keyname><forenames>Ian</forenames><affiliation>LTCI</affiliation></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames><affiliation>LTCI</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>LTCI</affiliation></author></authors><title>Disaster Recovery in Wireless Networks: A Homology-Based Algorithm</title><categories>math.PR cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.1664</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm for the recovery of wireless networks
after a disaster. Considering a damaged wireless network, presenting coverage
holes or/and many disconnected components, we propose a disaster recovery
algorithm which repairs the network. It provides the list of locations where to
put new nodes in order to patch the coverage holes and mend the disconnected
components. In order to do this we first consider the simplicial complex
representation of the network, then the algorithm adds supplementary vertices
in excessive number, and afterwards runs a reduction algorithm in order to
reach an optimal result. One of the novelty of this work resides in the
proposed method for the addition of vertices. We use a determinantal point
process: the Ginibre point process which has inherent repulsion between
vertices, and has never been simulated before for wireless networks
representation. We compare both the determinantal point process addition method
with other vertices addition methods, and the whole disaster recovery algorithm
to the greedy algorithm for the set cover problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5544</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5544</id><created>2014-03-21</created><authors><author><keyname>Lewis</keyname><forenames>Ryan H.</forenames></author></authors><title>Yet Another Graph Partitioning Problem is NP-Hard</title><categories>cs.CC cs.DS</categories><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently a large number of graph separator problems have been proven to be
\textsc{NP-Hard}. Amazingly we have found that
$\alpha$-Subgraph-Balanced-Vertex-Separator, an important variant, has been
overlooked. In this work ``Yet Another Graph Partitioning Problem is NP-Hard&quot;
we present the surprising result that
$\alpha$-Subgraph-Balanced-Vertex-Separator is $NP$-Hard. This is despite the
fact that the constraints of our new problem are harder to satisfy than the
original problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5546</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5546</id><created>2014-03-21</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Asinowski</keyname><forenames>Andrei</forenames></author><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Disjoint compatibility graph of non-crossing matchings of points in
  convex position</title><categories>math.CO cs.CG cs.DM</categories><comments>46 pages, 30 figures</comments><msc-class>05A15, 05A18, 68R05, 68R10</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X_{2k}$ be a set of $2k$ labeled points in convex position in the plane.
We consider geometric non-intersecting straight-line perfect matchings of
$X_{2k}$. Two such matchings, $M$ and $M'$, are disjoint compatible if they do
not have common edges, and no edge of $M$ crosses an edge of $M'$. Denote by
$\mathrm{DCM}_k$ the graph whose vertices correspond to such matchings, and two
vertices are adjacent if and only if the corresponding matchings are disjoint
compatible. We show that for each $k \geq 9$, the connected components of
$\mathrm{DCM}_k$ form exactly three isomorphism classes -- namely, there is a
certain number of isomorphic small components, a certain number of isomorphic
medium components, and one big component. The number and the structure of small
and medium components is determined precisely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5553</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5553</id><created>2014-03-21</created><authors><author><keyname>Khalid</keyname><forenames>Zubair</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author></authors><title>Slepian Spatial-Spectral Concentration on the Ball</title><categories>math.CA astro-ph.IM cs.IT math.IT</categories><comments>33 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and solve the Slepian spatial-spectral concentration problem on
the three-dimensional ball. Both the standard Fourier-Bessel and also the
Fourier-Laguerre spectral domains are considered since the latter exhibits a
number of practical advantages (spectral decoupling and exact computation). The
Slepian spatial and spectral concentration problems are formulated as
eigenvalue problems, the eigenfunctions of which form an orthogonal family of
concentrated functions. Equivalence between the spatial and spectral problems
is shown. The spherical Shannon number on the ball is derived, which acts as
the analog of the space-bandwidth product in the Euclidean setting, giving an
estimate of the number of concentrated eigenfunctions and thus the dimension of
the space of functions that can be concentrated in both the spatial and
spectral domains simultaneously. Various symmetries of the spatial region are
considered that reduce considerably the computational burden of recovering
eigenfunctions, either by decoupling the problem into smaller subproblems or by
affording analytic calculations. The family of concentrated eigenfunctions
forms a Slepian basis that can be used be represent concentrated signals
efficiently. We illustrate our results with numerical examples and show that
the Slepian basis indeeds permits a sparse representation of concentrated
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5556</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5556</id><created>2014-03-20</created><updated>2014-07-22</updated><authors><author><keyname>Russo</keyname><forenames>Daniel</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Learning to Optimize via Information-Directed Sampling</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1403.5341</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose information-directed sampling -- a new algorithm for online
optimization problems in which a decision-maker must balance between
exploration and exploitation while learning from partial feedback. Each action
is sampled in a manner that minimizes the ratio between squared expected
single-period regret and a measure of information gain: the mutual information
between the optimal action and the next observation.
  We establish an expected regret bound for information-directed sampling that
applies across a very general class of models and scales with the entropy of
the optimal action distribution. For the widely studied Bernoulli, Gaussian,
and linear bandit problems, we demonstrate simulation performance surpassing
popular approaches, including upper confidence bound algorithms, Thompson
sampling, and the knowledge gradient algorithm. Further, we present simple
analytic examples illustrating that, due to the way it measures information
gain, information-directed sampling can dramatically outperform upper
confidence bound algorithms and Thompson sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5571</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5571</id><created>2014-03-21</created><updated>2014-04-04</updated><authors><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>Zheng</keyname><forenames>Zhong</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author><author><keyname>Taricco</keyname><forenames>Giorgio</forenames></author></authors><title>On the Outage Capacity of Orthogonal Space-time Block Codes Over
  Multi-cluster Scattering MIMO Channels</title><categories>cs.IT math.IT</categories><comments>Added references; changes made in Section 3-A</comments><msc-class>94A05, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple cluster scattering MIMO channel is a useful model for pico-cellular
MIMO networks. In this paper, orthogonal space-time block coded transmission
over such a channel is considered, where the effective channel equals the
product of n complex Gaussian matrices. A simple and accurate closed-form
approximation to the channel outage capacity has been derived in this setting.
The result is valid for an arbitrary number of clusters n-1 of scatterers and
an arbitrary antenna configuration. Numerical results are provided to study the
relative outage performance between the multi-cluster and the Rayleigh-fading
MIMO channels for which n=1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5590</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5590</id><created>2014-03-21</created><authors><author><keyname>Strandmark</keyname><forenames>Petter</forenames></author><author><keyname>Agarwal</keyname><forenames>Sameer</forenames></author></authors><title>Continuous Optimization for Fields of Experts Denoising Works</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent papers use image denoising with a Fields of Experts prior to
benchmark discrete optimization methods. We show that a non-linear least
squares solver significantly outperforms all known discrete methods on this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5596</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5596</id><created>2014-03-21</created><authors><author><keyname>El-Shishtawy</keyname><forenames>Tarek</forenames></author><author><keyname>El-Ghannam</keyname><forenames>Fatma</forenames></author></authors><title>A Lemma Based Evaluator for Semitic Language Text Summarization Systems</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching texts in highly inflected languages such as Arabic by simple
stemming strategy is unlikely to perform well. In this paper, we present a
strategy for automatic text matching technique for for inflectional languages,
using Arabic as the test case. The system is an extension of ROUGE test in
which texts are matched on token's lemma level. The experimental results show
an enhancement of detecting similarities between different sentences having
same semantics but written in different lexical forms..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5598</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5598</id><created>2014-03-21</created><updated>2015-04-21</updated><authors><author><keyname>Wang</keyname><forenames>Pengwei</forenames></author><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author></authors><title>Adversarial Wiretap Channel with Public Discussion</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wyner's elegant model of wiretap channel exploits noise in the communication
channel to provide perfect secrecy against a computationally unlimited
eavesdropper without requiring a shared key. We consider an adversarial model
of wiretap channel proposed in [18,19] where the adversary is active: it
selects a fraction $\rho_r$ of the transmitted codeword to eavesdrop and a
fraction $\rho_w$ of the codeword to corrupt by &quot;adding&quot; adversarial error. It
was shown that this model also captures network adversaries in the setting of
1-round Secure Message Transmission [8]. It was proved that secure
communication (1-round) is possible if and only if $\rho_r + \rho_w &lt;1$.
  In this paper we show that by allowing communicants to have access to a
public discussion channel (authentic communication without secrecy) secure
communication becomes possible even if $\rho_r + \rho_w &gt;1$. We formalize the
model of \awtppd protocol and for two efficiency measures, {\em information
rate } and {\em message round complexity} derive tight bounds. We also
construct a rate optimal protocol family with minimum number of message rounds.
We show application of these results to Secure Message Transmission with Public
Discussion (SMT-PD), and in particular show a new lower bound on transmission
rate of these protocols together with a new construction of an optimal SMT-PD
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5603</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5603</id><created>2014-03-21</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author><author><keyname>Liu</keyname><forenames>Jiangchuan</forenames></author><author><keyname>Li</keyname><forenames>Haitao</forenames></author></authors><title>Forecasting Popularity of Videos using Social Media</title><categories>cs.LG cs.SI</categories><doi>10.1109/JSTSP.2014.2370942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a systematic online prediction method (Social-Forecast)
that is capable to accurately forecast the popularity of videos promoted by
social media. Social-Forecast explicitly considers the dynamically changing and
evolving propagation patterns of videos in social media when making popularity
forecasts, thereby being situation and context aware. Social-Forecast aims to
maximize the forecast reward, which is defined as a tradeoff between the
popularity prediction accuracy and the timeliness with which a prediction is
issued. The forecasting is performed online and requires no training phase or a
priori knowledge. We analytically bound the prediction performance loss of
Social-Forecast as compared to that obtained by an omniscient oracle and prove
that the bound is sublinear in the number of video arrivals, thereby
guaranteeing its short-term performance as well as its asymptotic convergence
to the optimal performance. In addition, we conduct extensive experiments using
real-world data traces collected from the videos shared in RenRen, one of the
largest online social networks in China. These experiments show that our
proposed method outperforms existing view-based approaches for popularity
prediction (which are not context-aware) by more than 30% in terms of
prediction rewards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5605</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5605</id><created>2014-03-21</created><updated>2015-05-27</updated><authors><author><keyname>He</keyname><forenames>Yuan</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Krishnan</forenames></author><author><keyname>Gafni</keyname><forenames>Eli</forenames></author></authors><title>Group Mutual Exclusion in Linear Time and Space</title><categories>cs.DC</categories><comments>A total of 21 pages including 5 figures and 3 appendices. The bounded
  shared registers algorithm in the old version has a subtle error (that has no
  easy fix) necessitating replacement. A correct, but fundamentally different,
  bounded shared registers algorithm, which has the same properties claimed in
  the old version is presented in this new version. Also, this version has an
  additional author</comments><acm-class>D.1.3; D.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two algorithms for the Group Mutual Exclusion (GME) Problem that
satisfy the properties of Mutual Exclusion, Starvation Freedom, Bounded Exit,
Concurrent Entry and First Come First Served. Both our algorithms use only
simple read and write instructions, have O(N) Shared Space complexity and O(N)
Remote Memory Reference (RMR) complexity in the Cache Coherency (CC) model. Our
first algorithm is developed by generalizing the well-known Lamport's Bakery
Algorithm for the classical mutual exclusion problem, while preserving its
simplicity and elegance. However, it uses unbounded shared registers. Our
second algorithm uses only bounded registers and is developed by generalizing
Taubenfeld's Black and White Bakery Algorithm to solve the classical mutual
exclusion problem using only bounded shared registers. We show that contrary to
common perception our algorithms are the first to achieve these properties with
these combination of complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5606</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5606</id><created>2014-03-21</created><authors><author><keyname>Valencia</keyname><forenames>Carlos E.</forenames></author><author><keyname>Vargas</keyname><forenames>Marcos C.</forenames></author></authors><title>Optimum matchings in weighted bipartite graphs</title><categories>math.CO cs.DM cs.DS</categories><comments>11 pages</comments><msc-class>Primary 05C85, Secondary 05C70, 68Q25, 90C08, 90C35</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an integer weighted bipartite graph $\{G=(U\sqcup V, E), w:E\rightarrow
\mathbb{Z}\}$ we consider the problems of finding all the edges that occur in
some minimum weight matching of maximum cardinality and enumerating all the
minimum weight perfect matchings. Moreover, we construct a subgraph $G_{cs}$ of
$G$ which depends on an $\epsilon$-optimal solution of the dual linear program
associated to the assignment problem on $\{G,w\}$ that allows us to reduced
this problems to their unweighed variants on $G_{cs}$. For instance, when $G$
has a perfect matching and we have an $\epsilon$-optimal solution of the dual
linear program associated to the assignment problem on $\{G,w\}$, we solve the
problem of finding all the edges that occur in some minimum weight perfect
matching in linear time on the number of edges. Therefore, starting from
scratch we get an algorithm that solves this problem in time
$O(\sqrt{n}m\log(nW))$, where $n=|U|\geq |V|$, $m=|E|$, and $W={\rm
max}\{|w(e)|\, :\, e\in E\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5607</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5607</id><created>2014-03-21</created><authors><author><keyname>Gelbart</keyname><forenames>Michael A.</forenames></author><author><keyname>Snoek</keyname><forenames>Jasper</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Bayesian Optimization with Unknown Constraints</title><categories>stat.ML cs.LG</categories><comments>14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on Bayesian optimization has shown its effectiveness in global
optimization of difficult black-box objective functions. Many real-world
optimization problems of interest also have constraints which are unknown a
priori. In this paper, we study Bayesian optimization for constrained problems
in the general case that noise may be present in the constraint functions, and
the objective and constraints may be evaluated independently. We provide
motivating practical examples, and present a general framework to solve such
problems. We demonstrate the effectiveness of our approach on optimizing the
performance of online latent Dirichlet allocation subject to topic sparsity
constraints, tuning a neural network given test-time memory constraints, and
optimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed
time, subject to passing standard convergence diagnostics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5614</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5614</id><created>2014-03-22</created><authors><author><keyname>Singh</keyname><forenames>Vinay</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Vandana</forenames></author></authors><title>A New Complete Class Complexity Metric</title><categories>cs.SE</categories><comments>9</comments><journal-ref>Vinay Singh, Vandana Bhattacherjee&quot;A New Complete Class Complexity
  Metric&quot;, International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 3, No. 9, pp. 1-9, 2013</journal-ref><doi>10.7321/jscse.v3.n9.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software complexity metrics is essential for minimizing the cost of software
maintenance. Package level and System level complexity cannot be measured
without class level complexity. This research addresses the class complexity
metrics. This paper studies the existing class complexity metrics and proposes
a new class complexity metric CCC (Complete class complexity metric). The CCC
metric is then analytically evaluated by Weyuker's property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5616</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5616</id><created>2014-03-22</created><authors><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Quantum-noise limited communication with low probability of detection</title><categories>cs.IT math.IT quant-ph</categories><report-no>UM-CS-2013-002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the achievability of a square root limit on the amount of
information transmitted reliably and with low probability of detection (LPD)
over the single-mode lossy bosonic channel if either the eavesdropper's
measurements or the channel itself is subject to the slightest amount of excess
noise. Specifically, Alice can transmit $\mathcal{O}(\sqrt{n})$ bits to Bob
over $n$ channel uses such that Bob's average codeword error probability is
upper-bounded by an arbitrarily small $\delta&gt;0$ while a passive eavesdropper,
Warden Willie, who is assumed to be able to collect all the transmitted photons
that do not reach Bob, has an average probability of detection error that is
lower-bounded by $1/2-\epsilon$ for an arbitrarily small $\epsilon&gt;0$. We
analyze the thermal noise and pure loss channels. The square root law holds for
the thermal noise channel even if Willie employs a quantum-optimal measurement,
while Bob is equipped with a standard coherent detection receiver. We also show
that LPD communication is not possible on the pure loss channel. However, this
result assumes Willie to possess an ideal receiver that is not subject to
excess noise. If Willie is restricted to a practical receiver with a non-zero
dark current, the square root law is achievable on the pure loss channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5617</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5617</id><created>2014-03-22</created><authors><author><keyname>Chhabra</keyname><forenames>Simrat Singh</forenames></author><author><keyname>Brundavanam</keyname><forenames>Ajit</forenames></author><author><keyname>Shannigrahi</keyname><forenames>Saswata</forenames></author></authors><title>An Alternative Explanation for the Rise and Fall of MySpace</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise and fall of online social networks has recently generated an
enormous amount of interest among people, both inside and outside of academia.
Gillette [Businessweek magazine, 2011] did a detailed analysis of MySpace,
which started losing its popularity since 2008. Recently, Cannarella and
Spechler [arXiv:1401.4208] used a model of disease spread to explain this rise
and fall of MySpace. In this paper, we give an alternative explanation for the
same. Our explanation is based on the well-known Barabasi-Albert model of
generating random scale-free networks using preferential attachment mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5618</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5618</id><created>2014-03-22</created><updated>2015-03-09</updated><authors><author><keyname>Hossein</keyname><forenames>Shahadat</forenames></author><author><keyname>Zander</keyname><forenames>Par-Ola</forenames></author><author><keyname>Kamal</keyname><forenames>Md.</forenames></author><author><keyname>Chowdhury</keyname><forenames>Linkon</forenames></author></authors><title>Belief-Rule-Based Expert Systems for Evaluation of E- Government: A Case
  Study</title><categories>cs.AI cs.CY</categories><comments>Accepted with no Changes for Wiley Expert Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Little knowledge exists on the impact and results associated with
e-government projects in many specific use domains. Therefore it is necessary
to evaluate the efficiency and effectiveness of e-government systems. Since the
development of e-government is a continuous process of improvement, it requires
continuous evaluation of the overall e-government system as well as evaluation
of its various dimensions such as determinants, characteristics and results.
E-government development is often complex with multiple stakeholders, large
user bases and complex goals. Consequently, even experts have difficulties in
evaluating these systems, especially in an integrated and comprehensive way as
well as on an aggregate level. Expert systems are a candidate solution to
evaluate such complex e-government systems. However, it is difficult for expert
systems to cope with uncertain evaluation data that are vague, inconsistent,
highly subjective or in other ways challenging to formalize. This paper
presents an approach that can handle uncertainty in e-government evaluation:
The combination of Belief Rule Base (BRB) knowledge representation and
Evidential Reasoning (ES). This approach is illustrated with a concrete
prototype, known as Belief Rule Based Expert System (BRBES) and put to use in
the local e-government of Bangladesh. The results have been compared with a
recently developed method of evaluating e-Government, and it is shown that the
results of BRBES are more accurate and reliable. BRBES can be used to identify
the factors that need to be improved to achieve the overall aim of an
e-government project. In addition, various &quot;what if&quot; scenarios can be generated
and developers and managers can get a forecast of the outcomes. In this way,
the system can be used to facilitate decision making processes under
uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5627</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5627</id><created>2014-03-22</created><authors><author><keyname>Sharma</keyname><forenames>Shubhanjali</forenames></author><author><keyname>Gupta</keyname><forenames>Garima</forenames></author><author><keyname>Laxmi</keyname><forenames>P. R.</forenames></author></authors><title>A Survey on Cloud Security Issues and Techniques</title><categories>cs.DC cs.CR</categories><comments>8 pages, 5 figures, SCNDS Ajmer</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.4, No.1, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, cloud computing is an emerging way of computing in computer science.
Cloud computing is a set of resources and services that are offered by the
network or internet. Cloud computing extends various computing techniques like
grid computing, distributed computing. Today cloud computing is used in both
industrial field and academic field. Cloud facilitates its users by providing
virtual resources via internet. As the field of cloud computing is spreading
the new techniques are developing. This increase in cloud computing environment
also increases security challenges for cloud developers. Users of cloud save
their data in the cloud hence the lack of security in cloud can lose the users
trust. In this paper we will discuss some of the cloud security issues in
various aspects like multi-tenancy, elasticity, availability etc. The paper
also discuss existing security techniques and approaches for a secure cloud.
This paper will enable researchers and professionals to know about different
security threats and models and tools proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5628</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5628</id><created>2014-03-22</created><authors><author><keyname>Singh</keyname><forenames>Dharmendra</forenames></author><author><keyname>Sinha</keyname><forenames>Rakhi</forenames></author><author><keyname>Songara</keyname><forenames>Pawan</forenames></author><author><keyname>Rathi</keyname><forenames>Dr. Rakesh</forenames></author></authors><title>Vulnerabilities and Attacks Targeting Social Networks and Industrial
  Control Systems</title><categories>cs.SI cs.CR physics.soc-ph</categories><comments>10 pages, 6 figures, SCNDS 2014 GECA AJMER</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.4, No.1, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vulnerability is a weakness, shortcoming or flaw in the system or network
infrastructure which can be used by an attacker to harm the system, disrupt its
normal operation and use it for his financial, competitive or other motives or
just for cyber escapades. In this paper, we re-examined the various types of
attacks on industrial control systems as well as on social networking users. We
have listed which all vulnerabilities were exploited for executing these
attacks and their effects on these systems and social networks. The focus will
be mainly on the vulnerabilities that are used in OSNs as the convertors which
convert the social network into antisocial network and these networks can be
further used for the network attacks on the users associated with the victim
user whereby creating a consecutive chain of attacks on increasing number of
social networking users. Another type of attack, Stuxnet Attack which was
originally designed to attack Iran's nuclear facilities is also discussed here
which harms the system it controls by changing the code in that target system.
The Stuxnet worm is a very treacherous and hazardous means of attack and is the
first of its kind as it allows the attacker to manipulate real time equipment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5638</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5638</id><created>2014-03-22</created><authors><author><keyname>D'Amico</keyname><forenames>Antonio A.</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Convex separable problems with linear and box constraints</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures. Published at IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we focus on separable convex optimization problems with linear
and box constraints and compute the solution in closed-form as a function of
some Lagrange multipliers that can be easily computed in a finite number of
iterations. This allows us to bridge the gap between a wide family of power
allocation problems of practical interest in signal processing and
communications and their efficient implementation in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5641</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5641</id><created>2014-03-22</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author><author><keyname>Langbort</keyname><forenames>C.</forenames></author></authors><title>Control over adversarial packet-dropping communication networks
  revisited</title><categories>cs.SY math.OC</categories><comments>This paper has been accepted for presentation at the 2014 American
  Control Conference, Portland, Oregon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit a one-step control problem over an adversarial packet-dropping
link. The link is modeled as a set of binary channels controlled by a strategic
jammer whose intention is to wage a `denial of service' attack on the plant by
choosing a most damaging channel-switching strategy. The paper introduces a
class of zero-sum games between the jammer and controller as a scenario for
such attack, and derives necessary and sufficient conditions for these games to
have a nontrivial saddle-point equilibrium. At this equilibrium, the jammer's
optimal policy is to randomize in a region of the plant's state space, thus
requiring the controller to undertake a nontrivial response which is different
from what one would expect in a standard stochastic control problem over a
packet dropping channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5644</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5644</id><created>2014-03-22</created><updated>2014-06-02</updated><authors><author><keyname>Bahr</keyname><forenames>Patrick</forenames><affiliation>University of Copenhagen</affiliation></author></authors><title>Partial Order Infinitary Term Rewriting</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 3,
  2014) lmcs:751</journal-ref><doi>10.2168/LMCS-10(2:6)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative model of infinitary term rewriting. Instead of a
metric on terms, a partial order on partial terms is employed to formalise
convergence of reductions. We consider both a weak and a strong notion of
convergence and show that the metric model of convergence coincides with the
partial order model restricted to total terms. Hence, partial order convergence
constitutes a conservative extension of metric convergence, which additionally
offers a fine-grained distinction between different levels of divergence. In
the second part, we focus our investigation on strong convergence of orthogonal
systems. The main result is that the gap between the metric model and the
partial order model can be bridged by extending the term rewriting system by
additional rules. These extensions are the well-known B\&quot;ohm extensions. Based
on this result, we are able to establish that -- contrary to the metric setting
-- orthogonal systems are both infinitarily confluent and infinitarily
normalising in the partial order setting. The unique infinitary normal forms
that the partial order model admits are B\&quot;ohm trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5645</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5645</id><created>2014-03-22</created><authors><author><keyname>Veldhuizen</keyname><forenames>Todd L.</forenames></author></authors><title>Transaction Repair: Full Serializability Without Locks</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transaction Repair is a method for lock-free, scalable transaction processing
that achieves full serializability. It demonstrates parallel speedup even in
inimical scenarios where all pairs of transactions have significant read-write
conflicts. In the transaction repair approach, each transaction runs in
complete isolation in a branch of the database; when conflicts occur, we detect
and repair them. These repairs are performed efficiently in parallel, and the
net effect is that of serial processing. Within transactions, we use no locks.
This frees users from the complications and performance hazards of locks, and
from the anomalies of sub-SERIALIZABLE isolation levels. Our approach builds on
an incrementalized variant of leapfrog triejoin, a worst-case optimal algorithm
for $\exists_1$ formulae, and on well-established techniques from programming
languages: declarative languages, purely functional data structures,
incremental computation, and fixpoint equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5647</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5647</id><created>2014-03-22</created><authors><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>CUR Algorithm with Incomplete Matrix Observation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CUR matrix decomposition is a randomized algorithm that can efficiently
compute the low rank approximation for a given rectangle matrix. One limitation
with the existing CUR algorithms is that they require an access to the full
matrix A for computing U. In this work, we aim to alleviate this limitation. In
particular, we assume that besides having an access to randomly sampled d rows
and d columns from A, we only observe a subset of randomly sampled entries from
A. Our goal is to develop a low rank approximation algorithm, similar to CUR,
based on (i) randomly sampled rows and columns from A, and (ii) randomly
sampled entries from A. The proposed algorithm is able to perfectly recover the
target matrix A with only O(rn log n) number of observed entries. In addition,
instead of having to solve an optimization problem involved trace norm
regularization, the proposed algorithm only needs to solve a standard
regression problem. Finally, unlike most matrix completion theories that hold
only when the target matrix is of low rank, we show a strong guarantee for the
proposed algorithm even when the target matrix is not low rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5648</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5648</id><created>2014-03-22</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Ho</keyname><forenames>Zuleita</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Information and Energy Cooperation in Cognitive Radio Networks</title><categories>cs.IT math.IT</categories><comments>13 page, 9 figures. To appear in IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2014.2310433</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation between the primary and secondary systems can improve the
spectrum efficiency in cognitive radio networks. The key idea is that the
secondary system helps to boost the primary system's performance by relaying
and in return the primary system provides more opportunities for the secondary
system to access the spectrum. In contrast to most of existing works that only
consider information cooperation, this paper studies joint information and
energy cooperation between the two systems, i.e., the primary transmitter sends
information for relaying and feeds the secondary system with energy as well.
This is particularly useful when the secondary transmitter has good channel
quality to the primary receiver but is energy constrained. We propose and study
three schemes that enable this cooperation. Firstly, we assume there exists an
ideal backhaul between the two systems for information and energy transfer. We
then consider two wireless information and energy transfer schemes from the
primary transmitter to the secondary transmitter using power splitting and time
splitting energy harvesting techniques, respectively. For each scheme, the
optimal and zero-forcing solutions are derived. Simulation results demonstrate
promising performance gain for both systems due to the additional energy
cooperation. It is also revealed that the power splitting scheme can achieve
larger rate region than the time splitting scheme when the efficiency of the
energy transfer is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5665</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5665</id><created>2014-03-22</created><authors><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Hoffmann</keyname><forenames>Hella-Franziska</forenames></author></authors><title>Online Square-into-Square Packing</title><categories>cs.DM cs.CG</categories><comments>27 pages, 13 figures; full version of a preliminary extended abstract
  that appeared in APPROX 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1967, Moon and Moser proved a tight bound on the critical density of
squares in squares: any set of squares with a total area of at most 1/2 can be
packed into a unit square, which is tight. The proof requires full knowledge of
the set, as the algorithmic solution consists in sorting the objects by
decreasing size, and packing them greedily into shelves. Since then, the online
version of the problem has remained open; the best upper bound is still 1/2,
while the currently best lower bound is 1/3, due to Han et al. (2008). In this
paper, we present a new lower bound of 11/32, based on a dynamic shelf
allocation scheme, which may be interesting in itself. We also give results for
the closely related problem in which the size of the square container is not
fixed, but must be dynamically increased in order to ac- commodate online
sequences of objects. For this variant, we establish an upper bound of 3/7 for
the critical density, and a lower bound of 1/8. When aiming for accommodating
an online sequence of squares, this corresponds to a 2.82...- competitive
method for minimizing the required container size, and a lower bound of 1.33 .
. . for the achievable factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5683</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5683</id><created>2014-03-22</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author><author><keyname>Cloots</keyname><forenames>Rudi</forenames></author><author><keyname>Gadomski</keyname><forenames>Adam</forenames></author><author><keyname>Vitanov</keyname><forenames>Nikolay K.</forenames></author></authors><title>Ranking structures and Rank-Rank Correlations of Countries. The FIFA and
  UEFA cases</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><comments>23 pages, 8 figures, 24 references, 3 tables; to be published in Int.
  J. Mod. Phys. C</comments><journal-ref>Int. J. Mod. Phys. C 25 (11), 1450060 (2014)</journal-ref><doi>10.1142/S0129183114500600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ranking of agents competing with each other in complex systems may lead to
paradoxes according to the pre-chosen different measures. A discussion is
presented on such rank-rank, similar or not, correlations based on the case of
European countries ranked by UEFA and FIFA from different soccer competitions.
The first question to be answered is whether an empirical and simple law is
obtained for such (self-) organizations of complex sociological systems with
such different measuring schemes. It is found that the power law form is not
the best description contrary to many modern expectations. The stretched
exponential is much more adequate. Moreover, it is found that the measuring
rules lead to some inner structures, in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5686</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5686</id><created>2014-03-22</created><authors><author><keyname>Shen</keyname><forenames>Xiaohu</forenames></author><author><keyname>Shamaiah</keyname><forenames>Manohar</forenames></author><author><keyname>Vikalo</keyname><forenames>Haris</forenames></author></authors><title>Iterative Learning for Reference-Guided DNA Sequence Assembly from Short
  Reads: Algorithms and Limits of Performance</title><categories>q-bio.GN cs.CE cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2333564</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent emergence of next-generation DNA sequencing technology has enabled
acquisition of genetic information at unprecedented scales. In order to
determine the genetic blueprint of an organism, sequencing platforms typically
employ so-called shotgun sequencing strategy to oversample the target genome
with a library of relatively short overlapping reads. The order of nucleotides
in the reads is determined by processing the acquired noisy signals generated
by the sequencing instrument. Assembly of a genome from potentially erroneous
short reads is a computationally daunting task even in the scenario where a
reference genome exists. Errors and gaps in the reference, and perfect repeat
regions in the target, further render the assembly challenging and cause
inaccuracies. In this paper, we formulate the reference-guided sequence
assembly problem as the inference of the genome sequence on a bipartite graph
and solve it using a message-passing algorithm. The proposed algorithm can be
interpreted as the well-known classical belief propagation scheme under a
certain prior. Unlike existing state-of-the-art methods, the proposed algorithm
combines the information provided by the reads without needing to know
reliability of the short reads (so-called quality scores). Relation of the
message-passing algorithm to a provably convergent power iteration scheme is
discussed. To evaluate and benchmark the performance of the proposed technique,
we find an analytical expression for the probability of error of a genie-aided
maximum a posteriori (MAP) decision scheme. Results on both simulated and
experimental data demonstrate that the proposed message-passing algorithm
outperforms commonly used state-of-the-art tools, and it nearly achieves the
performance of the aforementioned MAP decision scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5693</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5693</id><created>2014-03-22</created><authors><author><keyname>Maclaurin</keyname><forenames>Dougal</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Firefly Monte Carlo: Exact MCMC with Subsets of Data</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose
tool for Bayesian inference. However, MCMC cannot be practically applied to
large data sets because of the prohibitive cost of evaluating every likelihood
term at every iteration. Here we present Firefly Monte Carlo (FlyMC) an
auxiliary variable MCMC algorithm that only queries the likelihoods of a
potentially small subset of the data at each iteration yet simulates from the
exact posterior distribution, in contrast to recent proposals that are
approximate even in the asymptotic limit. FlyMC is compatible with a wide
variety of modern MCMC algorithms, and only requires a lower bound on the
per-datum likelihood factors. In experiments, we find that FlyMC generates
samples from the posterior more than an order of magnitude faster than regular
MCMC, opening up MCMC methods to larger datasets than were previously
considered feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5698</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5698</id><created>2014-03-22</created><updated>2015-05-31</updated><authors><author><keyname>Komargodski</keyname><forenames>Ilan</forenames></author><author><keyname>Naor</keyname><forenames>Moni</forenames></author><author><keyname>Yogev</keyname><forenames>Eylon</forenames></author></authors><title>Secret-Sharing for NP</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computational secret-sharing scheme is a method that enables a dealer, that
has a secret, to distribute this secret among a set of parties such that a
&quot;qualified&quot; subset of parties can efficiently reconstruct the secret while any
&quot;unqualified&quot; subset of parties cannot efficiently learn anything about the
secret. The collection of &quot;qualified&quot; subsets is defined by a Boolean function.
  It has been a major open problem to understand which (monotone) functions can
be realized by a computational secret-sharing schemes. Yao suggested a method
for secret-sharing for any function that has a polynomial-size monotone circuit
(a class which is strictly smaller than the class of monotone functions in P).
Around 1990 Rudich raised the possibility of obtaining secret-sharing for all
monotone functions in NP: In order to reconstruct the secret a set of parties
must be &quot;qualified&quot; and provide a witness attesting to this fact.
  Recently, Garg et al. (STOC 2013) put forward the concept of witness
encryption, where the goal is to encrypt a message relative to a statement &quot;x
in L&quot; for a language L in NP such that anyone holding a witness to the
statement can decrypt the message, however, if x is not in L, then it is
computationally hard to decrypt. Garg et al. showed how to construct several
cryptographic primitives from witness encryption and gave a candidate
construction.
  One can show that computational secret-sharing implies witness encryption for
the same language. Our main result is the converse: we give a construction of a
computational secret-sharing scheme for any monotone function in NP assuming
witness encryption for NP and one-way functions. As a consequence we get a
completeness theorem for secret-sharing: computational secret-sharing scheme
for any single monotone NP-complete function implies a computational
secret-sharing scheme for every monotone function in NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5701</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5701</id><created>2014-03-22</created><authors><author><keyname>Tomas</keyname><forenames>Boris</forenames></author></authors><title>Cortex simulation system proposal using distributed computer network
  environments</title><categories>cs.AI</categories><comments>4 pages</comments><journal-ref>IJCSIS Volume 12 No. 3 2014</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In the dawn of computer science and the eve of neuroscience we participate in
rebirth of neuroscience due to new technology that allows us to deeply and
precisely explore whole new world that dwells in our brains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5702</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5702</id><created>2014-03-22</created><updated>2014-05-23</updated><authors><author><keyname>Cohen</keyname><forenames>Nathann</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author><author><keyname>Weller</keyname><forenames>Mathias</forenames></author></authors><title>A Polynomial-time Algorithm for Outerplanar Diameter Improvement</title><categories>cs.DS cs.DM math.CO</categories><comments>24 pages</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Outerplanar Diameter Improvement problem asks, given a graph $G$ and an
integer $D$, whether it is possible to add edges to $G$ in a way that the
resulting graph is outerplanar and has diameter at most $D$. We provide a
dynamic programming algorithm that solves this problem in polynomial time.
Outerplanar Diameter Improvement demonstrates several structural analogues to
the celebrated and challenging Planar Diameter Improvement problem, where the
resulting graph should, instead, be planar. The complexity status of this
latter problem is open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5711</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5711</id><created>2014-03-22</created><authors><author><keyname>Wu</keyname><forenames>Michael</forenames></author><author><keyname>Yin</keyname><forenames>Bei</forenames></author><author><keyname>Wang</keyname><forenames>Guohui</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Cavallaro</keyname><forenames>Joseph R.</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Large-Scale MIMO Detection for 3GPP LTE: Algorithms and FPGA
  Implementations</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Journal of Selected Topics in Signal Processing</comments><doi>10.1109/JSTSP.2014.2313021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale (or massive) multiple-input multiple-output (MIMO) is expected to
be one of the key technologies in next-generation multi-user cellular systems,
based on the upcoming 3GPP LTE Release 12 standard, for example. In this work,
we propose - to the best of our knowledge - the first VLSI design enabling
high-throughput data detection in single-carrier frequency-division multiple
access (SC-FDMA)-based large-scale MIMO systems. We propose a new approximate
matrix inversion algorithm relying on a Neumann series expansion, which
substantially reduces the complexity of linear data detection. We analyze the
associated error, and we compare its performance and complexity to those of an
exact linear detector. We present corresponding VLSI architectures, which
perform exact and approximate soft-output detection for large-scale MIMO
systems with various antenna/user configurations. Reference implementation
results for a Xilinx Virtex-7 XC7VX980T FPGA show that our designs are able to
achieve more than 600 Mb/s for a 128 antenna, 8 user 3GPP LTE-based large-scale
MIMO system. We finally provide a performance/complexity trade-off comparison
using the presented FPGA designs, which reveals that the detector circuit of
choice is determined by the ratio between BS antennas and users, as well as the
desired error-rate performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5712</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5712</id><created>2014-03-22</created><updated>2014-10-03</updated><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>Deficit Round-Robin-Based ISP Traffic Control Scheme Enabling Excess
  Bandwidth Allocation in Shared Access Networks</title><categories>cs.NI</categories><comments>5 pages, 6 figures, submitted to ICC 2015, Jul. 26, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In shared access shaping subscriber traffic based on token bucket by ISPs
wastes network resources when there are few active subscribers, because it
cannot allocate excess bandwidth in the long term. To address it, traffic
control schemes based on core-stateless fair queueing (CSFQ) and token bucket
meters (TBMs) have been proposed, which can allocate excess bandwidth among
active subscribers proportional to their token generation rates. Using FIFO
queue for all packets, however, degrades the short-term performance of
conformant traffic due to the presence of non-conformant packets already in the
queue. Also, the rate estimation based on exponential averaging makes it
difficult to react to rapid changes in traffic conditions. In this paper we
propose a new traffic control scheme based on deficit round-robin (DRR) and
TBMs to guarantee the quality of service of conformant packets in all time
scales while allocating excess bandwidth among active subscribers proportional
to their token generation rates, whose advantages over the CSFQ-based schemes
are demonstrated through simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5715</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5715</id><created>2014-03-22</created><updated>2014-07-25</updated><authors><author><keyname>Xu</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Stoller</keyname><forenames>Scott D.</forenames></author></authors><title>Mining Attribute-Based Access Control Policies from Logs</title><categories>cs.CR cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.2401</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute-based access control (ABAC) provides a high level of flexibility
that promotes security and information sharing. ABAC policy mining algorithms
have potential to significantly reduce the cost of migration to ABAC, by
partially automating the development of an ABAC policy from information about
the existing access-control policy and attribute data. This paper presents an
algorithm for mining ABAC policies from operation logs and attribute data. To
the best of our knowledge, it is the first algorithm for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5718</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5718</id><created>2014-03-22</created><authors><author><keyname>Wong</keyname><forenames>Yu-Shiang</forenames></author><author><keyname>Chu</keyname><forenames>Hung-Kuo</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy J.</forenames></author></authors><title>SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RGBD images with high quality annotations in the form of geometric (i.e.,
segmentation) and structural (i.e., how do the segments are mutually related in
3D) information provide valuable priors to a large number of scene and image
manipulation applications. While it is now simple to acquire RGBD images,
annotating them, automatically or manually, remains challenging especially in
cluttered noisy environments. We present SmartAnnotator, an interactive system
to facilitate annotating RGBD images. The system performs the tedious tasks of
grouping pixels, creating potential abstracted cuboids, inferring object
interactions in 3D, and comes up with various hypotheses. The user simply has
to flip through a list of suggestions for segment labels, finalize a selection,
and the system updates the remaining hypotheses. As objects are finalized, the
process speeds up with fewer ambiguities to resolve. Further, as more scenes
are annotated, the system makes better suggestions based on structural and
geometric priors learns from the previous annotation sessions. We test our
system on a large number of database scenes and report significant improvements
over naive low-level annotation tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5723</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5723</id><created>2014-03-23</created><authors><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Game-theoretic Resource Allocation Methods for Device-to-Device (D2D)
  Communication</title><categories>cs.NI</categories><comments>Accepted. IEEE Wireless Comms Mag. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication underlaying cellular networks allows
mobile devices such as smartphones and tablets to use the licensed spectrum
allocated to cellular services for direct peer-to-peer transmission. D2D
communication can use either one-hop transmission (i.e., in D2D direct
communication) or multi-hop cluster-based transmission (i.e., in D2D local area
networks). The D2D devices can compete or cooperate with each other to reuse
the radio resources in D2D networks. Therefore, resource allocation and access
for D2D communication can be treated as games. The theories behind these games
provide a variety of mathematical tools to effectively model and analyze the
individual or group behaviors of D2D users. In addition, game models can
provide distributed solutions to the resource allocation problems for D2D
communication. The aim of this article is to demonstrate the applications of
game-theoretic models to study the radio resource allocation issues in D2D
communication. The article also outlines several key open research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5729</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5729</id><created>2014-03-23</created><updated>2014-05-21</updated><authors><author><keyname>Iv&#xe1;n</keyname><forenames>Szabolcs</forenames><affiliation>University of Szeged, Hungary</affiliation></author></authors><title>Synchronizing weighted automata</title><categories>cs.FL</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 301-313</journal-ref><doi>10.4204/EPTCS.151.21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two generalizations of synchronizability to automata with
transitions weighted in an arbitrary semiring K=(K,+,*,0,1). (or equivalently,
to finite sets of matrices in K^nxn.) Let us call a matrix A
location-synchronizing if there exists a column in A consisting of nonzero
entries such that all the other columns of A are filled by zeros. If
additionally all the entries of this designated column are the same, we call A
synchronizing. Note that these notions coincide for stochastic matrices and
also in the Boolean semiring. A set M of matrices in K^nxn is called
(location-)synchronizing if M generates a matrix subsemigroup containing a
(location-)synchronizing matrix. The K-(location-)synchronizability problem is
the following: given a finite set M of nxn matrices with entries in K, is it
(location-)synchronizing?
  Both problems are PSPACE-hard for any nontrivial semiring. We give sufficient
conditions for the semiring K when the problems are PSPACE-complete and show
several undecidability results as well, e.g. synchronizability is undecidable
if 1 has infinite order in (K,+,0) or when the free semigroup on two generators
can be embedded into (K,*,1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5730</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5730</id><created>2014-03-23</created><updated>2014-07-02</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Resource Allocation for Coordinated Multipoint Networks with Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures, accepted for publication at the IEEE Globecom
  2014, Austin, TX, USA, Dec. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the resource allocation algorithm design for multiuser
coordinated multipoint (CoMP) networks with simultaneous wireless information
and power transfer (SWIPT). In particular, remote radio heads (RRHs) are
connected to a central processor (CP) via capacity-limited backhaul links to
facilitate CoMP joint transmission. Besides, the CP transfers energy to the
RRHs for more efficient network operation. The considered resource allocation
algorithm design is formulated as a non-convex optimization problem with a
minimum required signal-to-interference-plus-noise ratio (SINR) constraint at
multiple information receivers and a minimum required power transfer constraint
at the energy harvesting receivers. By optimizing the transmit beamforming
vectors at the CP and energy sharing between the CP and the RRHs, we aim at
jointly minimizing the total network transmit power and the maximum capacity
consumption per backhaul link. The resulting non-convex optimization problem is
NP-hard. In light of the intractability of the problem, we reformulate it by
replacing the non-convex objective function with its convex hull, which enables
the derivation of an efficient iterative resource allocation algorithm. In each
iteration, a non-convex optimization problem is solved by semi-definite
programming (SDP) relaxation and the proposed iterative algorithm converges to
a local optimal solution of the original problem. Simulation results illustrate
that our proposed algorithm achieves a close-to-optimal performance and
provides a significant reduction in backhaul capacity consumption compared to
full cooperation. Besides, the considered CoMP network is shown to provide
superior system performance as far as power consumption is concerned compared
to a traditional system with multiple antennas co-located.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5734</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5734</id><created>2014-03-23</created><updated>2015-12-07</updated><authors><author><keyname>Sarhan</keyname><forenames>Zahi A. M. Abu</forenames></author></authors><title>Software Agents Interaction Algorithms in Virtual Learning Environment</title><categories>cs.MA cs.CY</categories><journal-ref>The World of Computer Science and Information Technology Journal
  (WSCIT). 2014, Volume 4, Issue 2. pp. 18.25</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper highlights the multi-agent learning virtual environment and agents
communication algorithms. The researcher proposed three algorithms required
software agents interaction in virtual learning information system environment.
The first proposed algorithm is agents interaction localization algorithm, the
second one is the dynamic agents distribution algorithm (load distribution
algorithm), and the third model is Agent communication algorithm based on using
agents intermediaries. The main objectives of these algorithms are to reduce
the response time for any agents changes in virtual learning environment (VLE)
by increasing the information exchange intensity between software agents and
reduce the overall network load, and to improve the communication between
mobile agents in distributed information system to support effectiveness.
Finally the paper describe the algorithms of information exchange between
mobile agents in VLE based on the expansion of the address structure and the
use of an agent, intermediary agents, matchmaking agents, brokers and their
entrepreneurial functions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5735</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5735</id><created>2014-03-23</created><updated>2015-04-14</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Cooperative Energy Trading in CoMP Systems Powered by Smart Grids</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the energy management in the coordinated multi-point
(CoMP) systems powered by smart grids, where each base station (BS) with local
renewable energy generation is allowed to implement the two-way energy trading
with the grid. Due to the uneven renewable energy supply and communication
energy demand over distributed BSs as well as the difference in the prices for
their buying/selling energy from/to the gird, it is beneficial for the
cooperative BSs to jointly manage their energy trading with the grid and energy
consumption in CoMP based communication for reducing the total energy cost.
Specifically, we consider the downlink transmission in one CoMP cluster by
jointly optimizing the BSs' purchased/sold energy units from/to the grid and
their cooperative transmit precoding, so as to minimize the total energy cost
subject to the given quality of service (QoS) constraints for the users. First,
we obtain the optimal solution to this problem by developing an algorithm based
on techniques from convex optimization and the uplink-downlink duality. Next,
we propose a sub-optimal solution of lower complexity than the optimal
solution, where zero-forcing (ZF) based precoding is implemented at the BSs.
Finally, through extensive simulations, we show the performance gain achieved
by our proposed joint energy trading and communication cooperation schemes in
terms of energy cost reduction, as compared to conventional schemes that
separately design communication cooperation and energy trading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5753</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5753</id><created>2014-03-23</created><authors><author><keyname>Deng</keyname><forenames>Xinyang</forenames></author><author><keyname>Chan</keyname><forenames>Felix T. S.</forenames></author><author><keyname>Sadiq</keyname><forenames>Rehan</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>D-CFPR: D numbers extended consistent fuzzy preference relations</title><categories>cs.AI</categories><comments>28 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to express an expert's or a decision maker's preference for alternatives
is an open issue. Consistent fuzzy preference relation (CFPR) is with big
advantages to handle this problem due to it can be construed via a smaller
number of pairwise comparisons and satisfies additive transitivity property.
However, the CFPR is incapable of dealing with the cases involving uncertain
and incomplete information. In this paper, a D numbers extended consistent
fuzzy preference relation (D-CFPR) is proposed to overcome the weakness. The
D-CFPR extends the classical CFPR by using a new model of expressing uncertain
information called D numbers. The D-CFPR inherits the merits of classical CFPR
and can be totally reduced to the classical CFPR. This study can be integrated
into our previous study about D-AHP (D numbers extended AHP) model to provide a
systematic solution for multi-criteria decision making (MCDM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5761</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5761</id><created>2014-03-23</created><updated>2014-10-06</updated><authors><author><keyname>Sparavalo</keyname><forenames>Myroslav</forenames></author></authors><title>The Lyapunov Concept of Stability from the Standpoint of Poincare
  Approach: General Procedure of Utilization of Lyapunov Functions for
  Non-Linear Non-Autonomous Parametric Differential Inclusions</title><categories>cs.SY</categories><comments>53 pages, 4 figures, 6 graphs</comments><msc-class>34A60, 93D05, 93D20, 37C75, 37C70, 34D23, 34D35, 34D45, 57R30,
  53C12, 55R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of the research is to develop a general method of constructing
Lyapunov functions for non-linear non-autonomous differential inclusions
described by ordinary differential equations with parameters. The goal has been
attained through the following ideas and tools. First, three-point Poincare
strategy of the investigation of differential equations and manifolds has been
used. Second, the geometric-topological structure of the non-linear
non-autonomous parametric differential inclusions has been presented and
analyzed in the framework of hierarchical fiber bundles. Third, a special
canonizing transformation of the differential inclusions that allows to present
them in special canonical form, for which certain standard forms of Lyapunov
functions exist, has been found. The conditions establishing the relation
between the local asymptotical stability of two corresponding particular
integral curves of a given differential inclusion in its initial and canonical
forms are ascertained. The global asymptotical stability of the entire free
dynamical systems as some restrictions of a given parametric differential
inclusion and the whole latter one per se has been investigated in terms of the
classificational stability of the typical fiber of the meta-bundle. There have
discussed the prospects of development and modifications of the Lyapunov second
method in the light of the discovery of the new features of Lyapunov functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5768</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5768</id><created>2014-03-23</created><authors><author><keyname>Huang</keyname><forenames>Longbo</forenames></author></authors><title>Optimizing Your Online-Advertisement Asynchronously</title><categories>cs.SY cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing optimal online-ad investment strategies
for a single advertiser, who invests at multiple sponsored search sites
simultaneously, with the objective of maximizing his average revenue subject to
the advertising budget constraint. A greedy online investment scheme is
developed to achieve an average revenue that can be pushed to within
$O(\epsilon)$ of the optimal, for any $\epsilon&gt;0$, with a tradeoff that the
temporal budget violation is $O(1/\epsilon)$. Different from many existing
algorithms, our scheme allows the advertiser to \emph{asynchronously} update
his investments on each search engine site, hence applies to systems where the
timescales of action update intervals are heterogeneous for different sites. We
also quantify the impact of inaccurate estimation of the system dynamics and
show that the algorithm is robust against imperfect system knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5771</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5771</id><created>2014-03-23</created><updated>2014-03-25</updated><authors><author><keyname>Gupta</keyname><forenames>Rahul</forenames></author><author><keyname>Khirbat</keyname><forenames>Gitansh</forenames></author><author><keyname>Singh</keyname><forenames>Sanjay</forenames></author></authors><title>A Novel Method to Calculate Click Through Rate for Sponsored Search</title><categories>cs.IR</categories><comments>10 pages, 1 figure</comments><report-no>MU-MIT-ICT-2014-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sponsored search adopts generalized second price (GSP) auction mechanism
which works on the concept of pay per click which is most commonly used for the
allocation of slots in the searched page. Two main aspects associated with GSP
are the bidding amount and the click through rate (CTR). The CTR learning
algorithms currently being used works on the basic principle of (#clicks_i/
#impressions_i) under a fixed window of clicks or impressions or time. CTR are
prone to fraudulent clicks, resulting in sudden increase of CTR. The current
algorithms are unable to find the solutions to stop this, although with the use
of machine learning algorithms it can be detected that fraudulent clicks are
being generated. In our paper, we have used the concept of relative ranking
which works on the basic principle of (#clicks_i /#clicks_t). In this
algorithm, both the numerator and the denominator are linked. As #clicks_t is
higher than previous algorithms and is linked to the #clicks_i, the small
change in the clicks which occurs in the normal scenario have a very small
change in the result but in case of fraudulent clicks the number of clicks
increases or decreases rapidly which will add up with the normal clicks to
increase the denominator, thereby decreasing the CTR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5787</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5787</id><created>2014-03-23</created><updated>2014-12-27</updated><authors><author><keyname>Zhang</keyname><forenames>Pan</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Scalable detection of statistically significant communities and
  hierarchies, using message-passing for modularity</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI stat.ML</categories><journal-ref>Proceedings of National Academy of Sciences, 111, 18144 (2014)</journal-ref><doi>10.1073/pnas.1409770111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modularity is a popular measure of community structure. However, maximizing
the modularity can lead to many competing partitions, with almost the same
modularity, that are poorly correlated with each other. It can also produce
illusory &quot;communities&quot; in random graphs where none exist. We address this
problem by using the modularity as a Hamiltonian at finite temperature, and
using an efficient Belief Propagation algorithm to obtain the consensus of many
partitions with high modularity, rather than looking for a single partition
that maximizes it. We show analytically and numerically that the proposed
algorithm works all the way down to the detectability transition in networks
generated by the stochastic block model. It also performs well on real-world
networks, revealing large communities in some networks where previous work has
claimed no communities exist. Finally we show that by applying our algorithm
recursively, subdividing communities until no statistically-significant
subcommunities can be found, we can detect hierarchical structure in real-world
networks more efficiently than previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5788</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5788</id><created>2014-03-23</created><authors><author><keyname>Singh</keyname><forenames>Shubh Narayan</forenames></author><author><keyname>Krishna</keyname><forenames>K. V.</forenames></author></authors><title>$L$-Primitive Words in Submonoids</title><categories>cs.FL</categories><msc-class>68Q70, 20M35, 54H15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers a natural generalization of primitivity with respect to a
language. Given a language $L$, a nonempty word $w$ is said to be $L$-primitive
if $w$ is not a proper power of any word in $L$. After ascertaining the number
of primitive words in submonoids of a free monoid, the work proceeds to count
$L$-primitive words in submonoids of a free monoid. The work also studies the
distribution of $L$-primitive words in certain subsets of free monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5791</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5791</id><created>2014-03-23</created><updated>2014-05-05</updated><authors><author><keyname>Jaggard</keyname><forenames>Aaron D.</forenames></author><author><keyname>Lutz</keyname><forenames>Neil</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author><author><keyname>Wright</keyname><forenames>Rebecca N.</forenames></author></authors><title>Self-stabilizing uncoupled dynamics</title><categories>cs.GT cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamics in a distributed system are self-stabilizing if they are guaranteed
to reach a stable state regardless of how the system is initialized. Game
dynamics are uncoupled if each player's behavior is independent of the other
players' preferences. Recognizing an equilibrium in this setting is a
distributed computational task. Self-stabilizing uncoupled dynamics, then, have
both resilience to arbitrary initial states and distribution of knowledge. We
study these dynamics by analyzing their behavior in a bounded-recall
synchronous environment. We determine, for every &quot;size&quot; of game, the minimum
number of periods of play that stochastic (randomized) players must recall in
order for uncoupled dynamics to be self-stabilizing. We also do this for the
special case when the game is guaranteed to have unique best replies. For
deterministic players, we demonstrate two self-stabilizing uncoupled protocols.
One applies to all games and uses three steps of recall. The other uses two
steps of recall and applies to games where each player has at least four
available actions. For uncoupled deterministic players, we prove that a single
step of recall is insufficient to achieve self-stabilization, regardless of the
number of available actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5801</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5801</id><created>2014-03-23</created><updated>2015-02-27</updated><authors><author><keyname>Linn</keyname><forenames>E.</forenames></author><author><keyname>Siemon</keyname><forenames>A.</forenames></author><author><keyname>Waser</keyname><forenames>R.</forenames></author><author><keyname>Menzel</keyname><forenames>S.</forenames></author></authors><title>Applicability of Well-Established Memristive Models for Simulations of
  Resistive Switching Devices</title><categories>cs.ET</categories><comments>9 pages; accepted for IEEE TCAS-I</comments><journal-ref>IEEE Transactions on Circuits and Systems - Part I: Regular Papers
  (TCAS-I), 61, 8, pp. 2402 - 2410, (2014)</journal-ref><doi>10.1109/TCSI.2014.2332261</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly accurate and predictive models of resistive switching devices are
needed to enable future memory and logic design. Widely used is the memristive
modeling approach considering resistive switches as dynamical systems. Here we
introduce three evaluation criteria for memristor models, checking for
plausibility of the I-V characteristics, the presence of a sufficiently
non-linearity of the switching kinetics, and the feasibility of predicting the
behavior of two anti-serially connected devices correctly. We analyzed two
classes of models: the first class comprises common linear memristor models and
the second class widely used non-linear memristive models. The linear memristor
models are based on Strukovs initial memristor model extended by different
window functions, while the non-linear models include Picketts physics-based
memristor model and models derived thereof. This study reveals lacking
predictivity of the first class of models, independent of the applied window
function. Only the physics-based model is able to fulfill most of the basic
evaluation criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5802</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5802</id><created>2014-03-23</created><authors><author><keyname>Smith</keyname><forenames>Lisa F.</forenames></author><author><keyname>Arcand</keyname><forenames>Kimberly K.</forenames></author><author><keyname>Smith</keyname><forenames>Jeffrey K.</forenames></author><author><keyname>Smith</keyname><forenames>Randall K.</forenames></author><author><keyname>Bookbinder</keyname><forenames>Jay</forenames></author><author><keyname>Watzke</keyname><forenames>Megan</forenames></author></authors><title>Examining Perceptions of Astronomy Images Across Mobile Platforms</title><categories>cs.HC astro-ph.IM physics.ed-ph</categories><comments>23 pages, 1 figure; Journal of Science Communication, in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern society has led many people to become consumers of data unlike
previous generations. How this shift in the way information is communicated and
received - including in areas of science - and affects perception and
comprehension is still an open question. This study examined one aspect of this
digital age: perceptions of astronomical images and their labels, on mobile
platforms. Participants were n = 2183 respondents to an online survey, and two
focus groups (n = 12 astrophysicists; n = 11 lay public). Online participants
were randomly assigned to 1 of 12 images, and compared two label formats. Focus
groups compared mobile devices and label formats. Results indicated that the
size and quality of the images on the mobile devices affected label
comprehension and engagement. The question label format was significantly
preferred to the fun fact. Results are discussed in terms of effective science
communication using technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5804</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5804</id><created>2014-03-23</created><updated>2014-05-12</updated><authors><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author></authors><title>Sample-Optimal Fourier Sampling in Any Constant Dimension -- Part I</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for $\ell_2/\ell_2$ sparse recovery from Fourier
measurements using $O(k\log N)$ samples, matching the lower bound of
\cite{DIPW} for non-adaptive algorithms up to constant factors for any $k\leq
N^{1-\delta}$. The algorithm runs in $\tilde O(N)$ time. Our algorithm extends
to higher dimensions, leading to sample complexity of $O_d(k\log N)$, which is
optimal up to constant factors for any $d=O(1)$. These are the first sample
optimal algorithms for these problems.
  A preliminary experimental evaluation indicates that our algorithm has
empirical sampling complexity comparable to that of other recovery methods
known in the literature, while providing strong provable guarantees on the
recovery quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5805</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5805</id><created>2014-03-23</created><authors><author><keyname>Margaris</keyname><forenames>Athanasios</forenames></author><author><keyname>Souravlas</keyname><forenames>Stauros</forenames></author><author><keyname>Roumeliotis</keyname><forenames>Manos</forenames></author></authors><title>Parallel Implementations of the Jacobi Linear Algebraic Systems Solve</title><categories>cs.DC cs.NA</categories><comments>Balkan Conference on Informatics (BCI2007), Sofia, Bulgaria</comments><msc-class>65Fxx</msc-class><acm-class>D.1.3; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this research is to construct parallel implementations of
the Jacobi algorithm used for the solution of linear algebraic systems, to
measure their speedup with respect to the serial case and to compare each
other, regarding their efficiency. The programming paradigm used in this
implementation is the message passing model, while, the used MPI implementation
is the MPICH implementation of the Argonne National Laboratory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5815</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5815</id><created>2014-03-23</created><updated>2014-03-27</updated><authors><author><keyname>Alexeev</keyname><forenames>Vadim</forenames></author><author><keyname>Rozanova</keyname><forenames>Liudmila</forenames></author><author><keyname>Temerev</keyname><forenames>Alexander</forenames></author></authors><title>Heterogeneous epidemic model for assessing data dissemination in
  opportunistic networks</title><categories>cs.SI physics.soc-ph q-bio.PE</categories><acm-class>C.2.4; G.1.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate a susceptible-infected-susceptible (SIS)
epidemic model describing data dissemination in opportunistic networks with
heterogeneous setting of transmission parameters. We obtained the estimation of
the final epidemic size assuming that amount of data transferred between
network nodes possesses a Pareto distribution, implying scale-free properties.
In this context, more heterogeneity in susceptibility means the less severe
epidemic progression, and, on the contrary, more heterogeneity in infectivity
leads to more severe epidemics -- assuming that the other parameter (either
heterogeneity or susceptibility) stays fixed. The results are general enough
and can be useful in general epidemic theory for estimating the epidemic
progression for diseases with no significant acquired immunity -- in the cases
where Pareto distribution holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5821</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5821</id><created>2014-03-23</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>If Archimedes would have known functions</title><categories>math.HO cs.DM</categories><comments>31 pages, 36 figures</comments><msc-class>26A06, 97A30, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are notes and slides from a Pecha-Kucha talk given on March 6, 2013.
The presentation tinkered with the question whether calculus on graphs could
have emerged by the time of Archimedes, if the concept of a function would have
been available 2300 years ago. The text first attempts to boil down discrete
single and multivariable calculus to one page each, then presents the slides
with additional remarks and finally includes 40 &quot;calculus problems&quot; in a
discrete or so-called 'quantum calculus' setting. We also added some sample
Mathematica code, gave a short overview over the emergence of the function
concept in calculus and included comments on the development of calculus
textbooks over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5824</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5824</id><created>2014-03-23</created><authors><author><keyname>Zhu</keyname><forenames>Guanghua</forenames></author><author><keyname>Davis</keyname><forenames>Linda M.</forenames></author><author><keyname>Chan</keyname><forenames>Terence</forenames></author></authors><title>Energy-Throughput Trade-offs in a Wireless Sensor Network with Mobile
  Relay</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the trade-offs between energy and throughput for
links in a wireless sensor network. Our application of interest is one in which
a number of low-powered sensors need to wirelessly communicate their
measurements to a communications sink, or destination node, for communication
to a central processor. We focus on one particular sensor source, and consider
the case where the distance to the destination is beyond the peak power of the
source. A relay node is required. Transmission energy of the sensor and the
relay can be adjusted to minimize the total energy for a given throughput of
the connection from sensor source to destination. We introduce a bounded random
walk model for movement of the relay between the sensor and destination nodes,
and characterize the total transmission energy and throughput performance using
Markov steady state analysis. Based on the trade-offs between total energy and
throughput we propose a new time-sharing protocol to exploit the movement of
the relay to reduce the total energy. We demonstrate the effectiveness of
time-sharing for minimizing the total energy consumption while achieving the
throughput requirement. We then show that the time-sharing scheme is more
energy efficient than the popular sleep mode scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5828</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5828</id><created>2014-03-23</created><authors><author><keyname>Qin</keyname><forenames>Peng</forenames></author><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Huang</keyname><forenames>Benxiong</forenames></author><author><keyname>Xu</keyname><forenames>Guan</forenames></author><author><keyname>Wu</keyname><forenames>Kui</forenames></author></authors><title>A Survey on Network Tomography with Network Coding</title><categories>cs.NI cs.PF</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The overhead of internal network monitoring motivates techniques of network
tomography. Network coding (NC) presents a new opportunity for network
tomography as NC introduces topology-dependent correlation that can be further
exploited in topology estimation. Compared with traditional methods, network
tomography with NC has many advantages such as the improvement of tomography
accuracy and the reduction of complexity in choosing monitoring paths. In this
paper we first introduce the problem of tomography with NC and then propose the
taxonomy criteria to classify various methods. We also present existing
solutions and future trend. We expect that our comprehensive review on network
tomography with NC can serve as a good reference for researchers and
practitioners working in the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5830</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5830</id><created>2014-03-23</created><authors><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author></authors><title>Bejeweled, Candy Crush and other Match-Three Games are (NP-)Hard</title><categories>cs.CC</categories><comments>21 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The twentieth century has seen the rise of a new type of video games targeted
at a mass audience of &quot;casual&quot; gamers. Many of these games require the player
to swap items in order to form matches of three and are collectively known as
\emph{tile-matching match-three games}. Among these, the most influential one
is arguably \emph{Bejeweled} in which the matched items (gems) pop and the
above gems fall in their place. Bejeweled has been ported to many different
platforms and influenced an incredible number of similar games. Very recently
one of them, named \emph{Candy Crush Saga} enjoyed a huge popularity and
quickly went viral on social networks. We generalize this kind of games by only
parameterizing the size of the board, while all the other elements (such as the
rules or the number of gems) remain unchanged. Then, we prove that answering
many natural questions regarding such games is actually \NP-Hard. These
questions include determining if the player can reach a certain score, play for
a certain number of turns, and others. We also
\href{http://candycrush.isnphard.com}{provide} a playable web-based
implementation of our reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5843</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5843</id><created>2014-03-23</created><updated>2015-04-16</updated><authors><author><keyname>McClurg</keyname><forenames>Jedidiah</forenames></author><author><keyname>Hojjat</keyname><forenames>Hossein</forenames></author><author><keyname>Cerny</keyname><forenames>Pavol</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author></authors><title>Efficient Synthesis of Network Updates</title><categories>cs.PL</categories><acm-class>D.2.4; F.3.1; F.4.1; C.2.3</acm-class><doi>10.1145/2737924.2737980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-defined networking (SDN) is revolutionizing the networking industry,
but current SDN programming platforms do not provide automated mechanisms for
updating global configurations on the fly. Implementing updates by hand is
challenging for SDN programmers because networks are distributed systems with
hundreds or thousands of interacting nodes. Even if initial and final
configurations are correct, naively updating individual nodes can lead to
incorrect transient behaviors, including loops, black holes, and access control
violations. This paper presents an approach for automatically synthesizing
updates that are guaranteed to preserve specified properties. We formalize
network updates as a distributed programming problem and develop a synthesis
algorithm based on counterexample-guided search and incremental model checking.
We describe a prototype implementation, and present results from experiments on
real-world topologies and properties demonstrating that our tool scales to
updates involving over one-thousand nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5858</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5858</id><created>2014-03-24</created><authors><author><keyname>Khavasi</keyname><forenames>Ali A.</forenames></author><author><keyname>Aajami</keyname><forenames>Mojtaba</forenames></author><author><keyname>Park</keyname><forenames>Hae-Ryeon</forenames></author><author><keyname>Suk</keyname><forenames>Jung-Bong</forenames></author></authors><title>Utility Max-Min Fair Link Adaptation in IEEE 802.11ac Downlink
  Multi-User</title><categories>cs.NI</categories><comments>Has been accepted in IEEE Communications Letters</comments><doi>10.1109/LCOMM.2014.040214.140482</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this letter, we propose a novel model and corresponding algorithms to
address the optimal utility max-min fair link adaptation in Downlink Multi-User
(DL-MU) feature of the emerging IEEE 802.11ac WLAN standard. Herein, we first
propose a simple yet accurate model to formulate the max-min fair link
adaptation problem. Furthermore, this model guarantees the minimum utility gain
of each receiver according to its requirements. In the second step, we show
that the optimal solution of the proposed model can be obtained in polynomial
time, and then the solution algorithms are proposed and analyzed. The
simulation results demonstrate the significant achievement of the proposed
utility-aware link adaptation approach in terms of max-min fairness and utility
gain compared to utility-oblivious schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5864</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5864</id><created>2014-03-24</created><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Shen</keyname><forenames>Yao</forenames></author></authors><title>Mapping parcel-level urban areas for a large geographical area</title><categories>cs.OH</categories><comments>21 pages, 9 figures, 3 tables</comments><doi>10.1080/00045608.2015.1095062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a vital indicator for measuring urban development, urban areas are
expected to be identified explicitly and conveniently with widely available
dataset thereby benefiting the planning decisions and relevant urban studies.
Existing approaches to identify urban areas normally based on mid-resolution
sensing dataset, socioeconomic information (e.g. population density) generally
associate with low-resolution in space, e.g. cells with several square
kilometers or even larger towns/wards. Yet, few of them pay attention to
defining urban areas with micro data in a fine-scaled manner with large extend
scale by incorporating the morphological and functional characteristics. This
paper investigates an automated framework to delineate urban areas in the
parcel level, using increasingly available ordnance surveys for generating all
parcels (or geo-units) and ubiquitous points of interest (POIs) for inferring
density of each parcel. A vector cellular automata model was adopted for
identifying urban parcels from all generated parcels, taking into account
density, neighborhood condition, and other spatial variables of each parcel. We
applied this approach for mapping urban areas of all 654 Chinese cities and
compared them with those interpreted from mid-resolution remote sensing images
and inferred by population density and road intersections. Our proposed
framework is proved to be more straight-forward, time-saving and fine-scaled,
compared with other existing ones, and reclaim the need for consistency,
efficiency and availability in defining urban areas with well-consideration of
omnipresent spatial and functional factors across cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5865</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5865</id><created>2014-03-24</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>P.</forenames></author></authors><title>Step and Search Control Method to Track the Maximum Power in Wind Energy
  Conversion Systems A Study</title><categories>cs.SY</categories><comments>7 pages and 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple step and search control strategy for extracting maximum output power
from grid connected Variable Speed Wind Energy Conversion System (VSWECS) is
implemented in this work. This system consists of a variable speed wind turbine
coupled to a Permanent Magnet Synchronous Generator (PMSG) through a gear box,
a DC-DC boost converter and a hysteresis current controlled Voltage Source
Converter (VSC). The Maximum Power Point Tracking (MPPT) extracts maximum power
from the wind turbine from cut-into rated wind velocity by sensing only by DC
link power. This system can be connected to a micro-grid. Also it can be used
for supplying an isolated local load by means of converting the output of
Permanent Magnet Synchronous Generator (PMSG) to DC and then convert to AC by
means of hysteresis current controlled Voltage Source Converter (VSI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5869</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5869</id><created>2014-03-24</created><authors><author><keyname>Rahman</keyname><forenames>Akhlaqur</forenames></author><author><keyname>Tasnim</keyname><forenames>Sumaira</forenames></author></authors><title>Block Motion Based Dynamic Texture Analysis: A Review</title><categories>cs.CV</categories><comments>Published with International Journal of Computer Trends and
  Technology (IJCTT)</comments><journal-ref>Akhlaqur Rahman , Sumaira Tasnim . &quot;Block Motion Based Dynamic
  Texture Analysis: A Review&quot;. International Journal of Computer Trends and
  Technology (IJCTT) V8(2):76-78, February 2014</journal-ref><doi>10.14445/22312803/IJCTT-V8P114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic texture refers to image sequences of non-rigid objects that exhibit
some regularity in their movement. Videos of smoke, fire etc. fall under the
category of dynamic texture. Researchers have investigated different ways to
analyze dynamic textures since early nineties. Both appearance based (image
intensities) and motion based approaches are investigated. Motion based
approaches turn out to be more effective. A group of researchers have
investigated ways to utilize the motion vectors readily available with the
blocks in video codes like MGEG/H26X. In this paper we provide a review of the
dynamic texture analysis methods using block motion. Research into dynamic
texture analysis using block motion includes recognition, motion computation,
segmentation, and synthesis. We provide a comprehensive review of these
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5871</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5871</id><created>2014-03-24</created><authors><author><keyname>Liu</keyname><forenames>Yue</forenames></author><author><keyname>Bild</keyname><forenames>David R.</forenames></author><author><keyname>Dick</keyname><forenames>Robert P.</forenames></author><author><keyname>Mao</keyname><forenames>Z. Morley</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>The Mason Test: A Defense Against Sybil Attacks in Wireless Networks
  Without Trusted Authorities</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks are vulnerable to Sybil attacks, in which a malicious node
poses as many identities in order to gain disproportionate influence. Many
defenses based on spatial variability of wireless channels exist, but depend
either on detailed, multi-tap channel estimation - something not exposed on
commodity 802.11 devices - or valid RSSI observations from multiple trusted
sources, e.g., corporate access points - something not directly available in ad
hoc and delay-tolerant networks with potentially malicious neighbors. We extend
these techniques to be practical for wireless ad hoc networks of commodity
802.11 devices. Specifically, we propose two efficient methods for separating
the valid RSSI observations of behaving nodes from those falsified by malicious
participants. Further, we note that prior signalprint methods are easily
defeated by mobile attackers and develop an appropriate challenge-response
defense. Finally, we present the Mason test, the first implementation of these
techniques for ad hoc and delay-tolerant networks of commodity 802.11 devices.
We illustrate its performance in several real-world scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5874</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5874</id><created>2014-03-24</created><authors><author><keyname>Huleihel</keyname><forenames>Wasim</forenames><affiliation>Shitz</affiliation></author><author><keyname>Merhav</keyname><forenames>Neri</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On Compressive Sensing in Coding Problems: A Rigorous Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take an information theoretic perspective on a classical sparse-sampling
noisy linear model and present an analytical expression for the mutual
information, which plays central role in a variety of communications/processing
problems. Such an expression was addressed previously either by bounds, by
simulations and by the (non-rigorous) replica method. The expression of the
mutual information is based on techniques used in [1], addressing the minimum
mean square error (MMSE) analysis. Using these expressions, we study
specifically a variety of sparse linear communications models which include
coding in different settings, accounting also for multiple access channels and
different wiretap problems. For those, we provide single-letter expressions and
derive achievable rates, capturing the communications/processing features of
these timely models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5877</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5877</id><created>2014-03-24</created><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>Non-uniform Feature Sampling for Decision Tree Ensembles</title><categories>stat.ML cs.IT cs.LG math.IT stat.AP</categories><comments>7 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effectiveness of non-uniform randomized feature selection in
decision tree classification. We experimentally evaluate two feature selection
methodologies, based on information extracted from the provided dataset: $(i)$
\emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection.
Experimental evaluation of the proposed feature selection techniques indicate
that such approaches might be more effective compared to naive uniform feature
selection and moreover having comparable performance to the random forest
algorithm [3]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5882</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5882</id><created>2014-03-24</created><authors><author><keyname>de Graaf</keyname><forenames>Maurits</forenames></author><author><keyname>Manthey</keyname><forenames>Bodo</forenames></author></authors><title>Probabilistic Analysis of Power Assignments</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem for wireless ad hoc networks is the assignment of
suitable transmission powers to the wireless devices such that the resulting
communication graph is connected. The goal is to minimize the total transmit
power in order to maximize the life-time of the network. Our aim is a
probabilistic analysis of this power assignment problem. We prove complete
convergence for arbitrary combinations of the dimension d and the
distance-power gradient p. Furthermore, we prove that the expected
approximation ratio of the simple spanning tree heuristic is strictly less than
its worst-case ratio of 2.
  Our main technical novelties are two-fold: First, we find a way to deal with
the unbounded degree that the communication network induced by the optimal
power assignment can have. Minimum spanning trees and traveling salesman tours,
for which strong concentration results are known in Euclidean space, have
bounded degree, which is heavily exploited in their analysis. Second, we apply
a recent generalization of Azuma-Hoeffding's inequality to prove complete
convergence for the case p&gt;=d for both power assignments and minimum spanning
trees (MSTs). As far as we are aware, complete convergence for $p &gt; d$ has not
been proved yet for any Euclidean functional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5906</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5906</id><created>2014-03-24</created><updated>2016-01-28</updated><authors><author><keyname>Doan</keyname><forenames>Xuan Vinh</forenames></author><author><keyname>Nguyen</keyname><forenames>Tri-Dung</forenames></author></authors><title>Robust Newsvendor Games with Ambiguity in Demand Distributions</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate newsvendor games whose payoff function is uncertain due to
ambiguity in demand distributions. We discuss the concept of stability under
uncertainty and introduce solution concepts for robust cooperative games which
could be applied to these newsvendor games. Properties and numerical schemes
for finding core solutions of robust newsvendor games are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5912</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5912</id><created>2014-03-24</created><authors><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Marchi</keyname><forenames>Erik</forenames></author><author><keyname>Baron-Cohen</keyname><forenames>Simon</forenames></author><author><keyname>O'Reilly</keyname><forenames>Helen</forenames></author><author><keyname>Pigat</keyname><forenames>Delia</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Daves</keyname><forenames>Ian</forenames></author></authors><title>The state of play of ASC-Inclusion: An Integrated Internet-Based
  Environment for Social Inclusion of Children with Autism Spectrum Conditions</title><categories>cs.HC cs.CV cs.CY</categories><report-no>IDGEI/2014/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals with Autism Spectrum Conditions (ASC) have marked difficulties
using verbal and non-verbal communication for social interaction. The running
ASC-Inclusion project aims to help children with ASC by allowing them to learn
how emotions can be expressed and recognised via playing games in a virtual
world. The platform includes analysis of users' gestures, facial, and vocal
expressions using standard microphone and web-cam or a depth sensor, training
through games, text communication with peers, animation, video and audio clips.
We present the state of play in realising such a serious game platform and
provide results for the different modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5919</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5919</id><created>2014-03-24</created><authors><author><keyname>Freedman</keyname><forenames>Daniel</forenames></author><author><keyname>Krupka</keyname><forenames>Eyal</forenames></author><author><keyname>Smolin</keyname><forenames>Yoni</forenames></author><author><keyname>Leichter</keyname><forenames>Ido</forenames></author><author><keyname>Schmidt</keyname><forenames>Mirko</forenames></author></authors><title>SRA: Fast Removal of General Multipath for ToF Sensors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major issue with Time of Flight sensors is the presence of multipath
interference. We present Sparse Reflections Analysis (SRA), an algorithm for
removing this interference which has two main advantages. First, it allows for
very general forms of multipath, including interference with three or more
paths, diffuse multipath resulting from Lambertian surfaces, and combinations
thereof. SRA removes this general multipath with robust techniques based on
$L_1$ optimization. Second, due to a novel dimension reduction, we are able to
produce a very fast version of SRA, which is able to run at frame rate.
Experimental results on both synthetic data with ground truth, as well as real
images of challenging scenes, validate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5928</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5928</id><created>2014-03-24</created><updated>2014-04-02</updated><authors><author><keyname>Dai</keyname><forenames>Liang</forenames></author></authors><title>Viewing the Welch bound inequality from the kernel trick viewpoint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This brief note views to the Welch bound inequality using the idea of the
kernel trick from the machine learning research area. From this angle, some
novel insights of the inequality are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5933</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5933</id><created>2014-03-24</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein
  Coding and Promoter Region Prediction</title><categories>cs.CE cs.LG</categories><comments>7 Pages</comments><report-no>Pokkuluri Kiran Sree, et al. (2014) AIS-INMACA: A Novel Integrated
  MACA Based Clonal Classifier for Protein Coding and Promoter Region
  Prediction. J Bioinfo Comp Genom 1: 1-7</report-no><journal-ref>Journal of Bioinformatics and Comparative Genomics,2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the problems in bioinformatics are now the challenges in computing.
This paper aims at building a classifier based on Multiple Attractor Cellular
Automata (MACA) which uses fuzzy logic. It is strengthened with an artificial
Immune System Technique (AIS), Clonal algorithm for identifying a protein
coding and promoter region in a given DNA sequence. The proposed classifier is
named as AIS-INMACA introduces a novel concept to combine CA with artificial
immune system to produce a better classifier which can address major problems
in bioinformatics. This will be the first integrated algorithm which can
predict both promoter and protein coding regions. To obtain good fitness rules
the basic concept of Clonal selection algorithm was used. The proposed
classifier can handle DNA sequences of lengths 54,108,162,252,354. This
classifier gives the exact boundaries of both protein and promoter regions with
an average accuracy of 89.6%. This classifier was tested with 97,000 data
components which were taken from Fickett &amp; Toung, MPromDb, and other sequences
from a renowned medical university. This proposed classifier can handle huge
data sets and can find protein and promoter regions even in mixed and
overlapped DNA sequences. This work also aims at identifying the logicality
between the major problems in bioinformatics and tries to obtaining a common
frame work for addressing major problems in bioinformatics like protein
structure prediction, RNA structure prediction, predicting the splicing pattern
of any primary transcript and analysis of information content in DNA, RNA,
protein sequences and structure. This work will attract more researchers
towards application of CA as a potential pattern classifier to many important
problems in bioinformatics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5946</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5946</id><created>2014-03-24</created><updated>2014-05-19</updated><authors><author><keyname>Kelly</keyname><forenames>Jack</forenames></author><author><keyname>Knottenbelt</keyname><forenames>William</forenames></author></authors><title>Metadata for Energy Disaggregation</title><categories>cs.DB</categories><comments>To appear in The 2nd IEEE International Workshop on Consumer Devices
  and Systems (CDS 2014) in V\&quot;aster{\aa}s, Sweden</comments><acm-class>H.3</acm-class><doi>10.1109/COMPSACW.2014.97</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy disaggregation is the process of estimating the energy consumed by
individual electrical appliances given only a time series of the whole-home
power demand. Energy disaggregation researchers require datasets of the power
demand from individual appliances and the whole-home power demand. Multiple
such datasets have been released over the last few years but provide metadata
in a disparate array of formats including CSV files and plain-text README
files. At best, the lack of a standard metadata schema makes it unnecessarily
time-consuming to write software to process multiple datasets and, at worse,
the lack of a standard means that crucial information is simply absent from
some datasets. We propose a metadata schema for representing appliances,
meters, buildings, datasets, prior knowledge about appliances and appliance
models. The schema is relational and provides a simple but powerful inheritance
mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5952</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5952</id><created>2014-03-21</created><authors><author><keyname>Tavares</keyname><forenames>Andr&#xe9;</forenames><affiliation>UFMG</affiliation></author><author><keyname>Boissinot</keyname><forenames>Benoit</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>LLP</affiliation></author><author><keyname>Rastello</keyname><forenames>Fabrice</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes</affiliation></author></authors><title>Parameterized Construction of Program Representations for Sparse
  Dataflow Analyses</title><categories>cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-flow analyses usually associate information with control flow regions.
Informally, if these regions are too small, like a point between two
consecutive statements, we call the analysis dense. On the other hand, if these
regions include many such points, then we call it sparse. This paper presents a
systematic method to build program representations that support sparse
analyses. To pave the way to this framework we clarify the bibliography about
well-known intermediate program representations. We show that our approach, up
to parameter choice, subsumes many of these representations, such as the SSA,
SSI and e-SSA forms. In particular, our algorithms are faster, simpler and more
frugal than the previous techniques used to construct SSI - Static Single
Information - form programs. We produce intermediate representations isomorphic
to Choi et al.'s Sparse Evaluation Graphs (SEG) for the family of data-flow
problems that can be partitioned per variables. However, contrary to SEGs, we
can handle - sparsely - problems that are not in this family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5961</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5961</id><created>2014-03-24</created><updated>2015-01-04</updated><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>Feghali</keyname><forenames>Carl</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Haiko</forenames></author></authors><title>Partitioning a Graph into Disjoint Cliques and a Triangle-free Graph</title><categories>cs.CC cs.DM math.CO</categories><comments>20 pages</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G = (V, E)$ is \emph{partitionable} if there exists a partition
$\{A, B\}$ of $V$ such that $A$ induces a disjoint union of cliques and $B$
induces a triangle-free graph. In this paper we investigate the computational
complexity of deciding whether a graph is partitionable. The problem is known
to be $\NP$-complete on arbitrary graphs. Here it is proved that if a graph $G$
is bull-free, planar, perfect, $K_4$-free or does not contain certain holes
then deciding whether $G$ is partitionable is $\NP$-complete. This answers an
open question posed by Thomass{\'e}, Trotignon and Vu\v{s}kovi{\'c}. In
contrast a finite list of forbidden induced subgraphs is given for
partitionable cographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5969</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5969</id><created>2014-03-24</created><authors><author><keyname>Wang</keyname><forenames>Yang</forenames></author></authors><title>Random Matrices and Erasure Robust Frames</title><categories>cs.IT math.IT</categories><msc-class>42C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data erasure can often occur in communication. Guarding against erasures
involves redundancy in data representation. Mathematically this may be achieved
by redundancy through the use of frames. One way to measure the robustness of a
frame against erasures is to examine the worst case condition number of the
frame with a certain number of vectors erased from the frame. The term {\em
numerically erasure-robust frames (NERFs)} was introduced in \cite{FicMix12} to
give a more precise characterization of erasure robustness of frames. In the
paper the authors established that random frames whose entries are drawn
independently from the standard normal distribution can be robust against up to
approximately 15\% erasures, and asked whether there exist frames that are
robust against erasures of more than 50\%. In this paper we show that with very
high probability random frames are, independent of the dimension, robust
against any amount of erasures as long as the number of remaining vectors is at
least $1+\delta$ times the dimension for some $\delta_0&gt;0$. This is the best
possible result, and it also implies that the proportion of erasures can
arbitrarily close to 1 while still maintaining robustness. Our result depends
crucially on a new estimate for the smallest singular value of a rectangular
random matrix with independent standard normal entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5970</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5970</id><created>2014-03-24</created><authors><author><keyname>Malarz</keyname><forenames>Krzysztof</forenames></author><author><keyname>Ku&#x142;akowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Mental ability and common sense in an artificial society</title><categories>physics.soc-ph cs.SI</categories><comments>3 pages, 3 figures, for Europhysics News</comments><journal-ref>Europhysics News 45 (4), 21 (2014)</journal-ref><doi>10.1051/epn/2014402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We read newspapers and watch TV every day. There are many issues and many
controversies. Since media are free, we can hear arguments from every possible
side. How do we decide what is wrong or right? The first condition to accept a
message is to understand it; messages that are too sophisticated are ignored.
So it seems reasonable to assume that our understanding depends on our ability
and our current knowledge. Here we show that the consequences of this statement
are surprising and funny.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5971</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5971</id><created>2014-03-24</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Anderson</keyname><forenames>James</forenames></author></authors><title>On Projection-Based Model Reduction of Biochemical Networks-- Part II:
  The Stochastic Case</title><categories>math.OC cs.SY q-bio.QM</categories><comments>Submitted to the 53rd CDC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of model order reduction of stochastic
biochemical networks. In particular, we reduce the order of (the number of
equations in) the Linear Noise Approximation of the Chemical Master Equation,
which is often used to describe biochemical networks. In contrast to other
biochemical network reduction methods, the presented one is projection-based.
Projection-based methods are powerful tools, but the cost of their use is the
loss of physical interpretation of the nodes in the network. In order alleviate
this drawback, we employ structured projectors, which means that some nodes in
the network will keep their physical interpretation. For many models in
engineering, finding structured projectors is not always feasible; however, in
the context of biochemical networks it is much more likely as the networks are
often (almost) monotonic. To summarise, the method can serve as a trade-off
between approximation quality and physical interpretation, which is illustrated
on numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5976</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5976</id><created>2014-03-21</created><authors><author><keyname>Kahanwal</keyname><forenames>Brijender</forenames></author></authors><title>File System Design Approaches</title><categories>cs.OS</categories><comments>5 pages, 6 figures</comments><journal-ref>International Journal of Advances in Engineering Sciences,2014,
  Vol. 4(1), PP 16-20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the file system development design approaches are discussed.
The selection of the file system design approach is done according to the needs
of the developers what are the needed requirements and specifications for the
new design. It allowed us to identify where our proposal fitted in with
relation to current and past file system development. Our experience with file
system development is limited so the research served to identify the different
techniques that can be used. The variety of file systems encountered show what
an active area of research file system development is. The file systems may be
from one of the two fundamental categories. In one category, the file system is
developed in user space and runs as a user process. Another file system may be
developed in the kernel space and runs as a privileged process. Another one is
the mixed approach in which we can take the advantages of both aforesaid
approaches. Each development option has its own pros and cons. In this article,
these design approaches are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5986</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5986</id><created>2014-03-24</created><updated>2015-02-03</updated><authors><author><keyname>Du</keyname><forenames>Guang-Xun</forenames></author><author><keyname>Quan</keyname><forenames>Quan</forenames></author><author><keyname>Yang</keyname><forenames>Binxian</forenames></author><author><keyname>Cai</keyname><forenames>Kai-Yuan</forenames></author></authors><title>Controllability Analysis for Multirotor Helicopter Rotor Degradation and
  Failure</title><categories>cs.SY cs.RO</categories><comments>21 pages, 4 figures</comments><journal-ref>AIAA Journal of Guidance, Control, and Dynamics, 2015, 38(5):
  978-984</journal-ref><doi>10.2514/1.G000731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the controllability analysis problem for a class of
multirotor systems subject to rotor failure/wear. It is shown that classical
controllability theories of linear systems are not sufficient to test the
controllability of the considered multirotors. Owing to this, an easy-to-use
measurement index is introduced to assess the available control authority.
Based on it, a new necessary and sufficient condition for the controllability
of multirotors is derived. Furthermore, a controllability test procedure is
approached. The proposed controllability test method is applied to a class of
hexacopters with different rotor configurations and different rotor efficiency
parameters to show its effectiveness. The analysis results show that
hexacopters with different rotor configurations have different fault-tolerant
capabilities. It is therefore necessary to test the controllability of the
multirotors before any fault-tolerant control strategies are employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5996</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5996</id><created>2014-03-24</created><updated>2014-07-25</updated><authors><author><keyname>Dell'Amico</keyname><forenames>Matteo</forenames></author><author><keyname>Carra</keyname><forenames>Damiano</forenames></author><author><keyname>Pastorelli</keyname><forenames>Mario</forenames></author><author><keyname>Michiardi</keyname><forenames>Pietro</forenames></author></authors><title>Revisiting Size-Based Scheduling with Estimated Job Sizes</title><categories>cs.DS</categories><comments>To be published in the proceedings of IEEE MASCOTS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study size-based schedulers, and focus on the impact of inaccurate job
size information on response time and fairness. Our intent is to revisit
previous results, which allude to performance degradation for even small errors
on job size estimates, thus limiting the applicability of size-based
schedulers.
  We show that scheduling performance is tightly connected to workload
characteristics: in the absence of large skew in the job size distribution,
even extremely imprecise estimates suffice to outperform size-oblivious
disciplines. Instead, when job sizes are heavily skewed, known size-based
disciplines suffer.
  In this context, we show -- for the first time -- the dichotomy of
over-estimation versus under-estimation. The former is, in general, less
problematic than the latter, as its effects are localized to individual jobs.
Instead, under-estimation leads to severe problems that may affect a large
number of jobs.
  We present an approach to mitigate these problems: our technique requires no
complex modifications to original scheduling policies and performs very well.
To support our claim, we proceed with a simulation-based evaluation that covers
an unprecedented large parameter space, which takes into account a variety of
synthetic and real workloads.
  As a consequence, we show that size-based scheduling is practical and
outperforms alternatives in a wide array of use-cases, even in presence of
inaccurate size information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.5997</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.5997</id><created>2014-03-24</created><updated>2014-06-10</updated><authors><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author><author><keyname>Swart</keyname><forenames>Albert</forenames></author></authors><title>Bayesian calibration for forensic evidence reporting</title><categories>stat.ML cs.LG</categories><comments>accepted for Interspeech 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a Bayesian solution for the problem in forensic speaker
recognition, where there may be very little background material for estimating
score calibration parameters. We work within the Bayesian paradigm of evidence
reporting and develop a principled probabilistic treatment of the problem,
which results in a Bayesian likelihood-ratio as the vehicle for reporting
weight of evidence. We show in contrast, that reporting a likelihood-ratio
distribution does not solve this problem. Our solution is experimentally
exercised on a simulated forensic scenario, using NIST SRE'12 scores, which
demonstrates a clear advantage for the proposed method compared to the
traditional plugin calibration recipe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6002</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6002</id><created>2014-03-24</created><authors><author><keyname>G.</keyname><forenames>Narkhede Sachin</forenames></author><author><keyname>Khairnar</keyname><forenames>Vaishali</forenames></author><author><keyname>Kadu</keyname><forenames>Sujata</forenames></author></authors><title>Brain Tumor Detection Based On Mathematical Analysis and Symmetry
  Information</title><categories>cs.CV</categories><comments>05 Pages,02 figures</comments><journal-ref>Int.Journal of Engineering Research and Applications ISSN
  2248-9622, Vol.4,Issue 2 Version 1,February 2014,pp.231-235</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation some of the challenging issues on brain magnetic resonance
image tumor segmentation caused by the weak correlation between magnetic
resonance imaging intensity and anatomical meaning.With the objective of
utilizing more meaningful information to improve brain tumor segmentation,an
approach which employs bilateral symmetry information as an additional feature
for segmentation is proposed.This is motivated by potential performance
improvement in the general automatic brain tumor segmentation systems which are
important for many medical and scientific applications.Brain Magnetic Resonance
Imaging segmentation is a complex problem in the field of medical imaging
despite various presented methods.MR image of human brain can be divided into
several sub-regions especially soft tissues such as gray matter,white matter
and cerebra spinal fluid.Although edge information is the main clue in image
segmentation,it cannot get a better result in analysis the content of images
without combining other information.Our goal is to detect the position and
boundary of tumors automatically.Experiments were conducted on real
pictures,and the results show that the algorithm is flexible and convenient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6006</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6006</id><created>2014-03-24</created><authors><author><keyname>Cao</keyname><forenames>Hong</forenames></author></authors><title>A Tablet Based Learning Environment</title><categories>cs.CY cs.HC</categories><comments>34 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Pen computing tools such as Tablet PC, Tablet Monitors and its various
supporting software tool offer another dimension to enhance our today's
digitally integrated and connected classroom learning environment. This paper
first reviews the various state-of-the-art pen-computing hardware and software
that have been applied in the classroom setting to introduce student-centric
learning, collaboration and making annotations and designing classroom
activities easier. We then propose a new classroom environment which is fully
equipped with Tablet devices and the supporting software tools for the goals of
1) easy electronic ink annotations with least constraints; 2) enhanced active
learning with timely feedback; 3) enhanced student collaborations and 4)
lecture recording. The classroom has been put into practical teaching and
learning environment as a pilot project in our higher learning environment.
After overcoming the initial learning curves, the environment received positive
feedbacks from the teaching faculties as well as from the students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6013</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6013</id><created>2014-03-24</created><authors><author><keyname>Khairnar</keyname><forenames>Vaishali D.</forenames></author><author><keyname>Pradhan</keyname><forenames>Dr. S. N.</forenames></author></authors><title>Simulation Based: Study and Analysis of Routing Protocol in Vehicular
  Ad-hoc Network Environment</title><categories>cs.NI</categories><comments>09 pages,18 figures</comments><journal-ref>Int.J.Computer Technology &amp; Applications,Vol 3(2),617-625 ISSN
  2229-6093,March - April 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Vehicular Ad hoc Network consists of vehicles which communicate with each
other and exchange data via wireless communication links available between the
vehicles which are in communication ranges of vehicles to improve the road
safety in city.The communication between vehicles is used to provide road
safety, comfort and entertainment.The performance of communication depends on
how better routing takes place in the network.Routing data between the source
and destination vehicle depends on the routing protocols being used in
vehicular adhoc network.In this simulation based study we investigated about
different ad hoc routing protocols for vehicular adhoc network.The main goal of
our study was to identify which ad hoc routing protocol has better performance
in highly mobile environment of vehicular adhoc network.We have measured the
performance of routing protocols using 802.11p in vehicular adhoc network in
which we considered the scenario of city i.e. Route between Nerul and Vashi
where we have taken 1200 different types of vehicles and checked their
performance.Routing protocols are selected after the literature review.The
selected protocols are then evaluated through simulation under 802.11p in terms
of performance metrics i.e PDR &amp; E2E delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6022</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6022</id><created>2014-03-24</created><authors><author><keyname>Souto</keyname><forenames>A.</forenames></author><author><keyname>Mateus</keyname><forenames>P.</forenames></author><author><keyname>Ad&#xe3;o</keyname><forenames>P.</forenames></author><author><keyname>Paunkovi&#x107;</keyname><forenames>N.</forenames></author></authors><title>Oblivious transfer based on quantum state computational
  distinguishability</title><categories>quant-ph cs.CR math-ph math.MP</categories><doi>10.1103/PhysRevA.91.042306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oblivious transfer protocol is a basic building block in cryptography and is
used to transfer information from a sender to a receiver in such a way that, at
the end of the protocol, the sender does not know if the receiver got the
message or not.
  Since Shor's quantum algorithm appeared, the security of most of classical
cryptographic schemes has been compromised, as they rely on the fact that
factoring is unfeasible. To overcome this, quantum mechanics has been used
intensively in the past decades, and alternatives resistant to quantum attacks
have been developed in order to fulfill the (potential) lack of security of a
significant number of classical schemes.
  In this paper, we present a quantum computationally secure protocol for
oblivious transfer between two parties, under the assumption of quantum
hardness of state distinguishability. The protocol is feasible, in the sense
that it is implementable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6023</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6023</id><created>2014-03-24</created><authors><author><keyname>Marujo</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Gershman</keyname><forenames>Anatole</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime</forenames></author><author><keyname>Neto</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author></authors><title>Ensemble Detection of Single &amp; Multiple Events at Sentence-Level</title><categories>cs.CL cs.LG</categories><comments>Preliminary version of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event classification at sentence level is an important Information Extraction
task with applications in several NLP, IR, and personalization systems.
Multi-label binary relevance (BR) are the state-of-art methods. In this work,
we explored new multi-label methods known for capturing relations between event
types. These new methods, such as the ensemble Chain of Classifiers, improve
the F1 on average across the 6 labels by 2.8% over the Binary Relevance. The
low occurrence of multi-label sentences motivated the reduction of the hard
imbalanced multi-label classification problem with low number of occurrences of
multiple labels per instance to an more tractable imbalanced multiclass problem
with better results (+ 4.6%). We report the results of adding new features,
such as sentiment strength, rhetorical signals, domain-id (source-id and date),
and key-phrases in both single-label and multi-label event classification
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6025</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6025</id><created>2014-03-24</created><updated>2015-02-05</updated><authors><author><keyname>Bertin</keyname><forenames>Emmanuel</forenames></author><author><keyname>Pillay</keyname><forenames>Ruven</forenames></author><author><keyname>Marmo</keyname><forenames>Chiara</forenames></author></authors><title>Web-Based Visualization of Very Large Scientific Astronomy Imagery</title><categories>astro-ph.IM cs.CE cs.MM</categories><comments>Published in Astronomy &amp; Computing. IIPImage server available from
  http://iipimage.sourceforge.net . Visiomatic code and demos available from
  http://www.visiomatic.org/</comments><msc-class>85-08</msc-class><acm-class>J.2; I.4.9</acm-class><journal-ref>Astronomy and Computing, vol. 10, pp. 43-53, Apr. 2015</journal-ref><doi>10.1016/j.ascom.2014.12.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visualizing and navigating through large astronomy images from a remote
location with current astronomy display tools can be a frustrating experience
in terms of speed and ergonomics, especially on mobile devices. In this paper,
we present a high performance, versatile and robust client-server system for
remote visualization and analysis of extremely large scientific images.
Applications of this work include survey image quality control, interactive
data query and exploration, citizen science, as well as public outreach. The
proposed software is entirely open source and is designed to be generic and
applicable to a variety of datasets. It provides access to floating point data
at terabyte scales, with the ability to precisely adjust image settings in
real-time. The proposed clients are light-weight, platform-independent web
applications built on standard HTML5 web technologies and compatible with both
touch and mouse-based devices. We put the system to the test and assess the
performance of the system and show that a single server can comfortably handle
more than a hundred simultaneous users accessing full precision 32 bit
astronomy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6032</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6032</id><created>2014-03-24</created><updated>2014-03-25</updated><authors><author><keyname>Bacci</keyname><forenames>Giorgio</forenames></author><author><keyname>Bacci</keyname><forenames>Giovanni</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Mardare</keyname><forenames>Radu</forenames></author></authors><title>Topologies of Stochastic Markov Models: Computational Aspects</title><categories>cs.FL</categories><comments>extended version with proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose two behavioral distances that support approximate
reasoning on Stochastic Markov Models (SMMs), that are continuous-time
stochastic transition systems where the residence time on each state is
described by a generic probability measure on the positive real line. In
particular, we study the problem of measuring the behavioral dissimilarity of
two SMMs against linear real-time specifications expressed as Metric Temporal
Logic (MTL) formulas or Deterministic Timed-Automata (DTA). The most natural
choice for such a distance is the one that measures the maximal difference that
can be observed comparing two SMMs with respect to their probability of
satisfying an arbitrary specification. We show that computing this metric is
NP-hard. In addition, we show that any algorithm that approximates the distance
within a certain absolute error, depending on the size of the SMMs, is NP-hard.
Nevertheless, we introduce an alternative distance, based on the Kantorovich
metric, that is an over-approximation of the former and we show that, under
mild assumptions on the residence time distributions, it can be computed in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6036</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6036</id><created>2014-03-24</created><authors><author><keyname>Nampally</keyname><forenames>Arun</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>C. R.</forenames></author></authors><title>Adaptive MCMC-Based Inference in Probabilistic Logic Programs</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Logic Programming (PLP) languages enable programmers to specify
systems that combine logical models with statistical knowledge. The inference
problem, to determine the probability of query answers in PLP, is intractable
in general, thereby motivating the need for approximate techniques. In this
paper, we present a technique for approximate inference of conditional
probabilities for PLP queries. It is an Adaptive Markov Chain Monte Carlo
(MCMC) technique, where the distribution from which samples are drawn is
modified as the Markov Chain is explored. In particular, the distribution is
progressively modified to increase the likelihood that a generated sample is
consistent with evidence. In our context, each sample is uniquely characterized
by the outcomes of a set of random variables. Inspired by reinforcement
learning, our technique propagates rewards to random variable/outcome pairs
used in a sample based on whether the sample was consistent or not. The
cumulative rewards of each outcome is used to derive a new &quot;adapted
distribution&quot; for each random variable. For a sequence of samples, the
distributions are progressively adapted after each sample. For a query with
&quot;Markovian evaluation structure&quot;, we show that the adapted distribution of
samples converges to the query's conditional probability distribution. For
Markovian queries, we present a modified adaptation process that can be used in
adaptive MCMC as well as adaptive independent sampling. We empirically evaluate
the effectiveness of the adaptive sampling methods for queries with and without
Markovian evaluation structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6046</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6046</id><created>2014-03-24</created><authors><author><keyname>Zhao</keyname><forenames>Changhong</forenames></author><author><keyname>Low</keyname><forenames>Steven</forenames></author></authors><title>Decentralized Primary Frequency Control in Power Networks</title><categories>cs.SY math.OC</categories><comments>7 pages, 2 figures. Submitted to CDC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We augment existing generator-side primary frequency control with load-side
control that are local, ubiquitous, and continuous. The mechanisms on both the
generator and the load sides are decentralized in that their control decisions
are functions of locally measurable frequency deviations. These local
algorithms interact over the network through nonlinear power flows. We design
the local frequency feedback control so that any equilibrium point of the
closed-loop system is the solution to an optimization problem that minimizes
the total generation cost and user disutility subject to power balance across
entire network. With Lyapunov method we derive a sufficient condition ensuring
an equilibrium point of the closed-loop system is asymptotically stable.
Simulation demonstrates improvement in both the transient and steady-state
performance over the traditional control only on the generators, even when the
total control capacity remains the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6048</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6048</id><created>2014-03-24</created><authors><author><keyname>Kramer</keyname><forenames>Simon</forenames></author></authors><title>Computer-Aided Discovery and Categorisation of Personality Axioms</title><categories>cs.CE cs.CY cs.LO</categories><comments>related to arXiv:1403.2000</comments><journal-ref>IfCoLog Journal of Logics and their Applications, 1(2), 2014,
  Pages 107-133</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a computer-algebraic, order-theoretic framework based on
intuitionistic logic for the computer-aided discovery of personality axioms
from personality-test data and their mathematical categorisation into formal
personality theories in the spirit of F.~Klein's Erlanger Programm for
geometrical theories. As a result, formal personality theories can be
automatically generated, diagrammatically visualised, and mathematically
characterised in terms of categories of invariant-preserving transformations in
the sense of Klein and category theory. Our personality theories and categories
are induced by implicational invariants that are ground instances of
intuitionistic implication, which we postulate as axioms. In our mindset, the
essence of personality, and thus mental health and illness, is its invariance.
The truth of these axioms is algorithmically extracted from histories of
partially-ordered, symbolic data of observed behaviour. The personality-test
data and the personality theories are related by a Galois-connection in our
framework. As data format, we adopt the format of the symbolic values generated
by the Szondi-test, a personality test based on L.~Szondi's unifying,
depth-psychological theory of fate analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6060</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6060</id><created>2014-03-24</created><authors><author><keyname>Sorokin</keyname><forenames>Alexey</forenames></author></authors><title>Monoid automata for displacement context-free languages</title><categories>cs.FL</categories><comments>Revised version for ESSLLI Student Session 2013 selected papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2007 Kambites presented an algebraic interpretation of
Chomsky-Schutzenberger theorem for context-free languages. We give an
interpretation of the corresponding theorem for the class of displacement
context-free languages which are equivalent to well-nested multiple
context-free languages. We also obtain a characterization of k-displacement
context-free languages in terms of monoid automata and show how such automata
can be simulated on two stacks. We introduce the simultaneous two-stack
automata and compare different variants of its definition. All the definitions
considered are shown to be equivalent basing on the geometric interpretation of
memory operations of these automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6067</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6067</id><created>2014-03-24</created><authors><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author><author><keyname>Gao</keyname><forenames>Huiji</forenames></author></authors><title>Why Do You Spread This Message? Understanding Users Sentiment in Social
  Media Campaigns</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter has been increasingly used for spreading messages about campaigns.
Such campaigns try to gain followers through their Twitter accounts, influence
the followers and spread messages through them. In this paper, we explore the
relationship between followers sentiment towards the campaign topic and their
rate of retweeting of messages generated by the campaign. Our analysis with
followers of multiple social-media campaigns found statistical significant
correlations between such sentiment and retweeting rate. Based on our analysis,
we have conducted an online intervention study among the followers of different
social-media campaigns. Our study shows that targeting followers based on their
sentiment towards the campaign can give higher retweet rate than a number of
other baseline approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6085</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6085</id><created>2014-03-24</created><updated>2014-10-03</updated><authors><author><keyname>Mitsch</keyname><forenames>Stefan</forenames></author><author><keyname>Passmore</keyname><forenames>Grant Olney</forenames></author><author><keyname>Platzer</keyname><forenames>Andre</forenames></author></authors><title>Collaborative Verification-Driven Engineering of Hybrid Systems</title><categories>cs.LO cs.SE</categories><msc-class>97M50 (Primary) 34K34, 68T15 (Secondary)</msc-class><journal-ref>Math. Comput. Sci. 8(1), 71-97, 2014</journal-ref><doi>10.1007/s11786-014-0176-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems with both discrete and continuous dynamics are an important
model for real-world cyber-physical systems. The key challenge is to ensure
their correct functioning w.r.t. safety requirements. Promising techniques to
ensure safety seem to be model-driven engineering to develop hybrid systems in
a well-defined and traceable manner, and formal verification to prove their
correctness. Their combination forms the vision of verification-driven
engineering. Often, hybrid systems are rather complex in that they require
expertise from many domains (e.g., robotics, control systems, computer science,
software engineering, and mechanical engineering). Moreover, despite the
remarkable progress in automating formal verification of hybrid systems, the
construction of proofs of complex systems often requires nontrivial human
guidance, since hybrid systems verification tools solve undecidable problems.
It is, thus, not uncommon for development and verification teams to consist of
many players with diverse expertise. This paper introduces a
verification-driven engineering toolset that extends our previous work on
hybrid and arithmetic verification with tools for (i) graphical (UML) and
textual modeling of hybrid systems, (ii) exchanging and comparing models and
proofs, and (iii) managing verification tasks. This toolset makes it easier to
tackle large-scale verification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6089</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6089</id><created>2014-03-24</created><updated>2014-04-10</updated><authors><author><keyname>Majkic</keyname><forenames>Zoran</forenames></author></authors><title>Intensional RDB for Big Data Interoperability</title><categories>cs.DB</categories><comments>30 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1103.0967, arXiv:1403.0017</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new family of Intensional RDBs (IRDBs), introduced in [1], extends the
traditional RDBs with the Big Data and flexible and 'Open schema' features,
able to preserve the user-defined relational database schemas and all
preexisting user's applications containing the SQL statements for a deployment
of such a relational data. The standard RDB data is parsed into an internal
vector key/value relation, so that we obtain a column representation of data
used in Big Data applications, covering the key/value and column-based Big Data
applications as well, into a unifying RDB framework. Such an IRDB architecture
is adequate for the massive migrations from the existing slow RDBMSs into this
new family of fast IRDBMSs by offering a Big Data and new flexible schema
features as well. Here we present the interoperability features of the IRDBs by
permitting the queries also over the internal vector relations created by
parsing of each federated database in a given Multidatabase system. We show
that the SchemaLog with the second-order syntax and ad hoc Logic Programming
and its querying fragment can be embedded into the standard SQL IRDBMSs, so
that we obtain a full interoperabilty features of IRDBs by using only the
standard relational SQL for querying both data and meta-data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6090</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6090</id><created>2014-03-24</created><authors><author><keyname>Gholami</keyname><forenames>Mohammad</forenames></author><author><keyname>Raeisi</keyname><forenames>Ghaffar</forenames></author></authors><title>Column Weight Two and Three LDPC Codes with High Rates and Large Girths</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the concept of the {\it broken diagonal pair} in the
chess-like square board is used to define some well-structured block designs
whose incidence matrices can be considered as the parity-check matrices of some
high rate cycle codes with girth 12. The structure of the proposed parity-check
matrices significantly reduces the complexity of encoding and decoding.
Interestingly, the constructed regular cycle codes with row-weights $t$, $3\leq
t \leq 20$, $t\neq 7, 15, 16$, have the best lengths among the known regular
girth-12 cycle codes. In addition, the proposed cycle codes can be easily
extended to some high rate column weight-3 LDPC codes with girth 6. Simulation
results show that the constructed codes achieve excellent performances,
specially the constructed column weight 3 LDPC codes outperform LDPC codes
based on Steiner triple systems (STS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6102</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6102</id><created>2014-03-24</created><updated>2015-02-27</updated><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Seshadreesan</keyname><forenames>Kaushik P.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Renyi generalizations of the conditional quantum mutual information</title><categories>quant-ph cond-mat.stat-mech cs.IT hep-th math-ph math.IT math.MP</categories><comments>v6: 53 pages, final published version</comments><journal-ref>Journal of Mathematical Physics vol. 56, no. 2, article no.
  022205, February 2015</journal-ref><doi>10.1063/1.4908102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conditional quantum mutual information $I(A;B|C)$ of a tripartite state
$\rho_{ABC}$ is an information quantity which lies at the center of many
problems in quantum information theory. Three of its main properties are that
it is non-negative for any tripartite state, that it decreases under local
operations applied to systems $A$ and $B$, and that it obeys the duality
relation $I(A;B|C)=I(A;B|D)$ for a four-party pure state on systems $ABCD$. The
conditional mutual information also underlies the squashed entanglement, an
entanglement measure that satisfies all of the axioms desired for an
entanglement measure. As such, it has been an open question to find R\'enyi
generalizations of the conditional mutual information, that would allow for a
deeper understanding of the original quantity and find applications beyond the
traditional memoryless setting of quantum information theory. The present paper
addresses this question, by defining different $\alpha$-R\'enyi generalizations
$I_{\alpha}(A;B|C)$ of the conditional mutual information, some of which we can
prove converge to the conditional mutual information in the limit
$\alpha\rightarrow1$. Furthermore, we prove that many of these generalizations
satisfy non-negativity, duality, and monotonicity with respect to local
operations on one of the systems $A$ or $B$ (with it being left as an open
question to prove that monotoniticity holds with respect to local operations on
both systems). The quantities defined here should find applications in quantum
information theory and perhaps even in other areas of physics, but we leave
this for future work. We also state a conjecture regarding the monotonicity of
the R\'enyi conditional mutual informations defined here with respect to the
R\'enyi parameter $\alpha$. We prove that this conjecture is true in some
special cases and when $\alpha$ is in a neighborhood of one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6106</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6106</id><created>2014-03-24</created><updated>2014-06-16</updated><authors><author><keyname>Carro</keyname><forenames>Adri&#xe1;n</forenames></author><author><keyname>Vazquez</keyname><forenames>Federico</forenames></author><author><keyname>Toral</keyname><forenames>Ra&#xfa;l</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author></authors><title>Fragmentation transition in a coevolving network with link-state
  dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 13 figures</comments><journal-ref>Phys. Rev. E 89, 062802 (2014)</journal-ref><doi>10.1103/PhysRevE.89.062802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a network model that couples the dynamics of link states with the
evolution of the network topology. The state of each link, either A or B, is
updated according to the majority rule or zero-temperature Glauber dynamics, in
which links adopt the state of the majority of their neighboring links in the
network. Additionally, a link that is in a local minority is rewired to a
randomly chosen node. While large systems evolving under the majority rule
alone always fall into disordered topological traps composed by frustrated
links, any amount of rewiring is able to drive the network to complete order,
by relinking frustrated links and so releasing the system from traps. However,
depending on the relative rate of the majority rule and the rewiring processes,
the system evolves towards different ordered absorbing configurations: either a
one-component network with all links in the same state or a network fragmented
in two components with opposite states. For low rewiring rates and finite size
networks there is a domain of bistability between fragmented and non-fragmented
final states. Finite size scaling indicates that fragmentation is the only
possible scenario for large systems and any nonzero rate of rewiring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6135</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6135</id><created>2014-03-24</created><authors><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Differentially Private Convex Optimization with Piecewise Affine
  Objectives</title><categories>math.OC cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a recently proposed notion of privacy that provides
strong privacy guarantees without any assumptions on the adversary. The paper
studies the problem of computing a differentially private solution to convex
optimization problems whose objective function is piecewise affine. Such
problem is motivated by applications in which the affine functions that define
the objective function contain sensitive user information. We propose several
privacy preserving mechanisms and provide analysis on the trade-offs between
optimality and the level of privacy for these mechanisms. Numerical experiments
are also presented to evaluate their performance in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6143</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6143</id><created>2014-03-24</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Exact correct-decoding exponent of the wiretap channel decoder</title><categories>cs.IT math.IT</categories><comments>21 pages; submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security level of the achievability scheme for Wyner's wiretap channel
model is examined from the perspective of the probability of correct decoding,
$P_c$, at the wiretap channel decoder. In particular, for finite-alphabet
memoryless channels, the exact random coding exponent of $P_c$ is derived as a
function of the total coding rate $R_1$ and the rate of each sub-code $R_2$.
Two different representations are given for this function and its basic
properties are provided. We also characterize the region of pairs of rates
$(R_1,R_2)$ of full security in the sense of the random coding exponent of
$P_c$, in other words, the region where the exponent of this achievability
scheme is the same as that of blind guessing at the eavesdropper side. Finally,
an analogous derivation of the correct-decoding exponent is outlined for the
case of the Gaussian channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6150</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6150</id><created>2014-03-24</created><updated>2015-03-05</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Optimal Design of Energy-Efficient Multi-User MIMO Systems: Is Massive
  MIMO the Answer?</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications, 16 pages,
  14 figures, 2 tables. The results can be reproduced using the following
  Matlab code: https://github.com/emilbjornson/is-massive-MIMO-the-answer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assume that a multi-user multiple-input multiple-output (MIMO) system is
designed from scratch to uniformly cover a given area with maximal energy
efficiency (EE). What are the optimal number of antennas, active users, and
transmit power? The aim of this paper is to answer this fundamental question.
We consider jointly the uplink and downlink with different processing schemes
at the base station and propose a new realistic power consumption model that
reveals how the above parameters affect the EE. Closed-form expressions for the
EE-optimal value of each parameter, when the other two are fixed, are provided
for zero-forcing (ZF) processing in single-cell scenarios. These expressions
prove how the parameters interact. For example, in sharp contrast to common
belief, the transmit power is found to increase (not to decrease) with the
number of antennas. This implies that energy-efficient systems can operate in
high signal-to-noise ratio regimes in which interference-suppressing signal
processing is mandatory. Numerical and analytical results show that the maximal
EE is achieved by a massive MIMO setup wherein hundreds of antennas are
deployed to serve a relatively large number of users using ZF processing. The
numerical results show the same behavior under imperfect channel state
information and in symmetric multi-cell scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6164</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6164</id><created>2014-03-24</created><authors><author><keyname>Ding</keyname><forenames>Z.</forenames></author><author><keyname>Krikidis</keyname><forenames>I.</forenames></author><author><keyname>Sharif</keyname><forenames>B.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Wireless Information and Power Transfer in Cooperative Networks with
  Spatially Random Relays</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. Wireless Communications 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the application of wireless information and power transfer to
cooperative networks is investigated, where the relays in the network are
randomly located and based on the decode-forward strategy. For the scenario
with one source-destination pair, three different strategies for using the
available relays are studied, and their impact on the outage probability and
diversity gain is characterized by applying stochastic geometry. By using the
assumptions that the path loss exponent is two and that the relay-destination
distances are much larger than the source-relay distances, closed form
analytical results can be developed to demonstrate that the use of energy
harvesting relays can achieve the same diversity gain as the case with
conventional self-powered relays. For the scenario with multiple sources, the
relays can be viewed as a type of scarce resource, where the sources compete
with each other to get help from the relays. Such a competition is modeled as a
coalition formation game, and two distributed game theoretic algorithms are
developed based on different payoff functions. Simulation results are provided
to confirm the accuracy of the developed analytical results and facilitate a
better performance comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6165</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6165</id><created>2014-03-24</created><updated>2015-10-23</updated><authors><author><keyname>Schwerdtfeger</keyname><forenames>Konrad W.</forenames></author></authors><title>The Connectivity of Boolean Satisfiability: No-Constants and Quantified
  Variants</title><categories>cs.CC cs.LO</categories><comments>superseded by chapter 3 of arXiv:1510.06700</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Boolean satisfiability problems, the structure of the solution space is
characterized by the solution graph, where the vertices are the solutions, and
two solutions are connected iff they differ in exactly one variable. Motivated
by research on heuristics and the satisfiability threshold, Gopalan et al. in
2006 studied connectivity properties of the solution graph and related
complexity issues for constraint satisfaction problems in Schaefer's framework.
They found dichotomies for the diameter of connected components and for the
complexity of the st-connectivity question, and conjectured a trichotomy for
the connectivity question that we recently were able to prove.
  While Gopalan et al. considered CNF(S)-formulas with constants, we here look
at two important variants: CNF(S)-formulas without constants, and partially
quantified formulas. For the diameter and the st-connectivity question, we
prove dichotomies analogous to those of Gopalan et al. in these settings. While
we cannot give a complete classification for the connectivity problem yet, we
identify fragments where it is in P, where it is coNP-complete, and where it is
PSPACE-complete, in analogy to Gopalan et al.'s trichotomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6167</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6167</id><created>2014-03-24</created><updated>2015-09-28</updated><authors><author><keyname>Patel</keyname><forenames>Utkarsh R.</forenames></author><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>MoM-SO: a Complete Method for Computing the Impedance of Cable Systems
  Including Skin, Proximity, and Ground Return Effects</title><categories>cs.CE</categories><comments>This paper has now been published in the IEEE Trans. on Power
  Delivery in Oct. 2015, vol. 30, no. 5, pp. 2110-2118. DOI:
  10.1109/TPWRD.2014.2378594</comments><journal-ref>IEEE Trans. on Power Delivery in Oct. 2015, vol. 30, no. 5, pp.
  2110-2118</journal-ref><doi>10.1109/TPWRD.2014.2378594</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of accurate and broadband models for underground and
submarine cable systems is of paramount importance for the correct prediction
of electromagnetic transients in power grids. Recently, we proposed the MoM-SO
method for extracting the series impedance of power cables while accounting for
skin and proximity effect in the conductors. In this paper, we extend the
method to include ground return effects and to handle cables placed inside a
tunnel. Numerical tests show that the proposed method is more accurate than
widely-used analytic formulas, and is much faster than existing proximity-aware
approaches like finite elements. For a three-phase cable system in a tunnel,
the proposed method requires only 0.3 seconds of CPU time per frequency point,
against the 8.3 minutes taken by finite elements, for a speed up beyond 1000 X.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6172</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6172</id><created>2014-03-24</created><updated>2014-06-05</updated><authors><author><keyname>Endrullis</keyname><forenames>Joerg</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames><affiliation>Utrecht University</affiliation></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>Klop</keyname><forenames>Jan Willem</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>van Oostrom</keyname><forenames>Vincent</forenames><affiliation>Utrecht University</affiliation></author></authors><title>Infinitary Term Rewriting for Weakly Orthogonal Systems: Properties and
  Counterexamples</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (June 8,
  2014) lmcs:752</journal-ref><doi>10.2168/LMCS-10(2:7)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present some contributions to the theory of infinitary rewriting for
weakly orthogonal term rewrite systems, in which critical pairs may occur
provided they are trivial. We show that the infinitary unique normal form
property fails by an example of a weakly orthogonal TRS with two collapsing
rules. By translating this example, we show that this property also fails for
the infinitary lambda-beta-eta-calculus. As positive results we obtain the
following: Infinitary confluence, and hence the infinitary unique normal forms
property, holds for weakly orthogonal TRSs that do not contain collapsing
rules. To this end we refine the compression lemma. Furthermore, we establish
the triangle and diamond properties for infinitary multi-steps (complete
developments) in weakly orthogonal TRSs, by refining an earlier
cluster-analysis for the finite case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6173</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6173</id><created>2014-03-24</created><authors><author><keyname>Senina</keyname><forenames>Anna</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Qiu</keyname><forenames>Wei</forenames></author><author><keyname>Friedrich</keyname><forenames>Annemarie</forenames></author><author><keyname>Amin</keyname><forenames>Sikandar</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author><author><keyname>Pinkal</keyname><forenames>Manfred</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Coherent Multi-Sentence Video Description with Variable Level of Detail</title><categories>cs.CV cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans can easily describe what they see in a coherent way and at varying
level of detail. However, existing approaches for automatic video description
are mainly focused on single sentence generation and produce descriptions at a
fixed level of detail. In this paper, we address both of these limitations: for
a variable level of detail we produce coherent multi-sentence descriptions of
complex videos. We follow a two-step approach where we first learn to predict a
semantic representation (SR) from video and then generate natural language
descriptions from the SR. To produce consistent multi-sentence descriptions, we
model across-sentence consistency at the level of the SR by enforcing a
consistent topic. We also contribute both to the visual recognition of objects
proposing a hand-centric approach as well as to the robust generation of
sentences using a word lattice. Human judges rate our multi-sentence
descriptions as more readable, correct, and relevant than related work. To
understand the difference between more detailed and shorter descriptions, we
collect and analyze a video description corpus of three levels of detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6183</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6183</id><created>2014-03-24</created><authors><author><keyname>Avanaki</keyname><forenames>Ali R. N.</forenames></author><author><keyname>Espig</keyname><forenames>Kathryn S.</forenames></author><author><keyname>Maidment</keyname><forenames>Andrew D. A.</forenames></author><author><keyname>Marchessoux</keyname><forenames>Cedric</forenames></author><author><keyname>Bakic</keyname><forenames>Predrag R.</forenames></author><author><keyname>Kimpe</keyname><forenames>Tom R. L.</forenames></author></authors><title>Development and evaluation of a 3D model observer with nonlinear
  spatiotemporal contrast sensitivity</title><categories>cs.CV</categories><doi>10.1117/12.2043793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate improvements to our 3D model observer with the goal of better
matching human observer performance as a function of viewing distance,
effective contrast, maximum luminance, and browsing speed. Two nonlinear
methods of applying the human contrast sensitivity function (CSF) to a 3D model
observer are proposed, namely the Probability Map (PM) and Monte Carlo (MC)
methods. In the PM method, the visibility probability for each frequency
component of the image stack, p, is calculated taking into account Barten's
spatiotemporal CSF, the component modulation, and the human psychometric
function. The probability p is considered to be equal to the perceived
amplitude of the frequency component and thus can be used by a traditional
model observer (e.g., LG-msCHO) in the space-time domain. In the MC method,
each component is randomly kept with probability p or discarded with 1-p. The
amplitude of the retained components is normalized to unity. The methods were
tested using DBT stacks of an anthropomorphic breast phantom processed in a
comprehensive simulation pipeline. Our experiments indicate that both the PM
and MC methods yield results that match human observer performance better than
the linear filtering method as a function of viewing distance, effective
contrast, maximum luminance, and browsing speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6184</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6184</id><created>2014-03-24</created><authors><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>The Density of Fan-Planar Graphs</title><categories>cs.DM cs.CG math.CO</categories><comments>19 pages</comments><msc-class>68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A topological drawing of a graph is fan-planar if for each edge e the edges
crossing e have a common endpoint on the same side of e, and a fan-planar graph
is a graph admitting such a drawing. Equivalently, this can be formulated by
two forbidden patterns, one of which is the configuration where e is crossed by
two independent edges and the other where e is crossed by incident edges with
the common endpoint on different sides of e. In particular every edge of a
fan-planar graph is crossed only by the edges of a star. A topological drawing
is simple if any two edges have at most one point in common.
  The class of fan-planar graphs is a natural variant of other classes defined
by forbidden intersection patterns in a topological drawing of the graph. So
every 1-planar graph is also fan-planar, and every fan-planar graph is also
quasiplanar, where both inclusions are strict. Fan-planar graphs also fit
perfectly in a recent series of work on nearly-planar graphs from the area of
graph drawing and combinatorial embeddings.
  For topologically defined graph classes, one of the most fundamental
questions asks for the maximum number of edges in any such graph with n
vertices. We prove that every n-vertex graph without loops and parallel edges
that admits a simple fan-planar drawig has at most 5n-10 edges and that this
bound is tight for every n &gt;= 20.
  Furthermore we discuss possible extensions and generalizations of these new
concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6188</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6188</id><created>2014-03-24</created><updated>2014-05-15</updated><authors><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Brief Announcement: Almost-Tight Approximation Distributed Algorithm for
  Minimum Cut</title><categories>cs.DS</categories><comments>To appear as a brief announcement at PODC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short paper, we present an improved algorithm for approximating the
minimum cut on distributed (CONGEST) networks. Let $\lambda$ be the minimum
cut. Our algorithm can compute $\lambda$ exactly in
$\tilde{O}((\sqrt{n}+D)\poly(\lambda))$ time, where $n$ is the number of nodes
(processors) in the network, $D$ is the network diameter, and $\tilde{O}$ hides
$\poly\log n$. By a standard reduction, we can convert this algorithm into a
$(1+\epsilon)$-approximation $\tilde{O}((\sqrt{n}+D)/\poly(\epsilon))$-time
algorithm. The latter result improves over the previous
$(2+\epsilon)$-approximation $\tilde{O}((\sqrt{n}+D)/\poly(\epsilon))$-time
algorithm of Ghaffari and Kuhn [DISC 2013]. Due to the lower bound of
$\tilde{\Omega}(\sqrt{n}+D)$ by Das Sarma et al. [SICOMP 2013], this running
time is {\em tight} up to a $\poly\log n$ factor. Our algorithm is an extremely
simple combination of Thorup's tree packing theorem [Combinatorica 2007],
Kutten and Peleg's tree partitioning algorithm [J. Algorithms 1998], and
Karger's dynamic programming [JACM 2000].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6192</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6192</id><created>2014-03-24</created><updated>2014-08-18</updated><authors><author><keyname>Xie</keyname><forenames>Yixuan</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author></authors><title>Quantum Synchronizable Codes From Quadratic Residue Codes and Their
  Supercodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum synchronizable codes are quantum error-correcting codes designed to
correct the effects of both quantum noise and block synchronization errors.
While it is known that quantum synchronizable codes can be constructed from
cyclic codes that satisfy special properties, only a few classes of cyclic
codes have been proved to give promising quantum synchronizable codes. In this
paper, using quadratic residue codes and their supercodes, we give a simple
construction for quantum synchronizable codes whose synchronization
capabilities attain the upper bound. The method is applicable to cyclic codes
of prime length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6199</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6199</id><created>2014-03-24</created><updated>2014-05-30</updated><authors><author><keyname>Weng</keyname><forenames>Lilian</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Predicting Successful Memes using Network and Community Structure</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>10 pages, 6 figures, 2 tables. Proceedings of 8th AAAI Intl. Conf. on
  Weblogs and social media (ICWSM 2014)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We investigate the predictability of successful memes using their early
spreading patterns in the underlying social networks. We propose and analyze a
comprehensive set of features and develop an accurate model to predict future
popularity of a meme given its early spreading patterns. Our paper provides the
first comprehensive comparison of existing predictive frameworks. We categorize
our features into three groups: influence of early adopters, community
concentration, and characteristics of adoption time series. We find that
features based on community structure are the most powerful predictors of
future success. We also find that early popularity of a meme is not a good
predictor of its future popularity, contrary to common belief. Our methods
outperform other approaches, particularly in the task of detecting very popular
or unpopular memes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6207</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6207</id><created>2014-03-24</created><authors><author><keyname>Krishnaswamy</keyname><forenames>Ravishankar</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author><author><keyname>Pruhs</keyname><forenames>Kirk</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author></authors><title>Cluster Before You Hallucinate: Approximating Node-Capacitated Network
  Design and Energy Efficient Routing</title><categories>cs.DS</categories><comments>22 pages (full version of STOC 2014 paper)</comments><msc-class>68W25, 05C21, 05C85</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider circuit routing with an objective of minimizing energy, in a
network of routers that are speed scalable and that may be shutdown when idle.
We consider both multicast routing and unicast routing. It is known that this
energy minimization problem can be reduced to a capacitated flow network design
problem, where vertices have a common capacity but arbitrary costs, and the
goal is to choose a minimum cost collection of vertices whose induced subgraph
will support the specified flow requirements. For the multicast (single-sink)
capacitated design problem we give a polynomial-time algorithm that is
O(log^3n)-approximate with O(log^4 n) congestion. This translates back to a
O(log ^(4{\alpha}+3) n)-approximation for the multicast energy-minimization
routing problem, where {\alpha} is the polynomial exponent in the dynamic power
used by a router. For the unicast (multicommodity) capacitated design problem
we give a polynomial-time algorithm that is O(log^5 n)-approximate with
O(log^12 n) congestion, which translates back to a O(log^(12{\alpha}+5)
n)-approximation for the unicast energy-minimization routing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6213</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6213</id><created>2014-03-24</created><authors><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author><author><keyname>Xiao</keyname><forenames>Di</forenames></author><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Li</keyname><forenames>Ming</forenames></author></authors><title>Embedding Cryptographic Features in Compressive Sensing</title><categories>cs.CR cs.IT math.IT</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) has been widely studied and applied in many fields.
Recently, the way to perform secure compressive sensing (SCS) has become a
topic of growing interest. The existing works on SCS usually take the sensing
matrix as a key and the resultant security level is not evaluated in depth.
They can only be considered as a preliminary exploration on SCS, but a concrete
and operable encipher model is not given yet. In this paper, we are going to
investigate SCS in a systematic way. The relationship between CS and
symmetric-key cipher indicates some possible encryption models. To this end, we
propose the two-level protection models (TLPM) for SCS which are developed from
measurements taking and something else, respectively. It is believed that these
models will provide a new point of view and stimulate further research in both
CS and cryptography. Specifically, an efficient and secure encryption scheme
for parallel compressive sensing (PCS) is designed by embedding a two-layer
protection in PCS using chaos. The first layer is undertaken by random
permutation on a two-dimensional signal, which is proved to be an acceptable
permutation with overwhelming probability. The other layer is to sample the
permuted signal column by column with the same chaotic measurement matrix,
which satisfies the restricted isometry property of PCS with overwhelming
probability. Both the random permutation and the measurement matrix are
constructed under the control of a chaotic system. Simulation results show that
unlike the general joint compression and encryption schemes in which encryption
always leads to the same or a lower compression ratio, the proposed approach of
embedding encryption in PCS actually improves the compression performance.
Besides, the proposed approach possesses high transmission robustness against
additive Gaussian white noise and cropping attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6214</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6214</id><created>2014-03-24</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Analysis of Linear Quantum Optical Networks</title><categories>quant-ph cs.SY</categories><comments>A shortened version is to appear in the Proceedings of the 19th IFAC
  World Congress, Cape Town, August, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the analysis of linear quantum optical networks.
It provides a systematic approach to the construction a model for a given
quantum network in terms of a system of quantum stochastic differential
equations. This corresponds to a classical state space model. The linear
quantum optical networks under consideration consist of interconnections
between optical cavities, optical squeezers, and beamsplitters. These models
can then be used in the design of quantum feedback control systems for these
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6225</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6225</id><created>2014-03-24</created><updated>2014-03-27</updated><authors><author><keyname>Tudor</keyname><forenames>Sebastian F.</forenames></author><author><keyname>Oara</keyname><forenames>Cristian</forenames></author><author><keyname>Sabau</keyname><forenames>Serban</forenames></author></authors><title>H-infinity control problem for general discrete-time systems</title><categories>math.OC cs.SY</categories><comments>7 pages, 1 figure, submitted to IEEE CDC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the suboptimal H-infinity control problem for a general
discrete-time system (whose transfer function matrix is allowed to be improper
or polynomial). The parametrization of output feedback controllers is given in
a realization-based setting, involves two generalized algebraic Riccati
equations, and features the same elegant simplicity of the standard (proper)
case. A relevant real-life numerical example proves the effectiveness of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6230</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6230</id><created>2014-03-25</created><updated>2014-06-30</updated><authors><author><keyname>Sorokin</keyname><forenames>Alexey</forenames></author></authors><title>Pumping lemma and Ogden lemma for displacement context-free grammars</title><categories>cs.FL</categories><comments>Shortened version accepted to DLT 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pumping lemma and Ogden lemma offer a powerful method to prove that a
particular language is not context-free. In 2008 Kanazawa proved an analogue of
pumping lemma for well-nested multiple-context free languages. However, the
statement of lemma is too weak for practical usage. We prove a stronger variant
of pumping lemma and an analogue of Ogden lemma for this language family. We
also use these statements to prove that some natural context-sensitive
languages cannot be generated by tree-adjoining grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6237</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6237</id><created>2014-03-25</created><updated>2014-03-30</updated><authors><author><keyname>Nguyen</keyname><forenames>Thi-Minh-Tam</forenames></author><author><keyname>Vu</keyname><forenames>Viet-Trung</forenames></author><author><keyname>Doan</keyname><forenames>The-Vinh</forenames></author><author><keyname>Tran</keyname><forenames>Duc-Khanh</forenames></author></authors><title>Resolution in Linguistic First Order Logic based on Linear Symmetrical
  Hedge Algebra</title><categories>cs.LO</categories><comments>IPMU 2014 Full paper (14 pages)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper focuses on resolution in linguistic first order logic with truth
value taken from linear symmetrical hedge algebra. We build the basic
components of linguistic first order logic, including syntax and semantics. We
present a resolution principle for our logic to resolve on two clauses having
contradictory linguistic truth values. Since linguistic information is
uncertain, inference in our linguistic logic is approximate. Therefore, we
introduce the concept of reliability in order to capture the natural
approximation of the resolution inference rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6241</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6241</id><created>2014-03-25</created><updated>2014-06-06</updated><authors><author><keyname>Cucker</keyname><forenames>Felipe</forenames></author></authors><title>A Theory of Complexity, Condition and Roundoff</title><categories>cs.CC math.NA</categories><comments>46 pages, 3 figures</comments><msc-class>68Q05, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a theory of complexity for numerical computations that takes into
account the condition of the input data and allows for roundoff in the
computations. We follow the lines of the theory developed by Blum, Shub, and
Smale for computations over R (which in turn followed those of the classical,
discrete, complexity theory as laid down by Cook, Karp, and Levin among
others). In particular, we focus on complexity classes of decision problems and
paramount among them, on appropriate versions of the classes P, NP and EXP of
polynomial, nondeterministic polynomial, and exponential time, respectively. We
prove some basic relationships between these complexity classes and exhibit
natural NP-complete problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6246</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6246</id><created>2014-03-25</created><authors><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>Balancing Scalability and Uniformity in SAT Witness Generator</title><categories>cs.LO</categories><comments>This is a full version of DAC 2014 paper</comments><doi>10.1145/2593069.2593097</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained-random simulation is the predominant approach used in the
industry for functional verification of complex digital designs. The
effectiveness of this approach depends on two key factors: the quality of
constraints used to generate test vectors, and the randomness of solutions
generated from a given set of constraints. In this paper, we focus on the
second problem, and present an algorithm that significantly improves the
state-of-the-art of (almost-)uniform generation of solutions of large Boolean
constraints. Our algorithm provides strong theoretical guarantees on the
uniformity of generated solutions and scales to problems involving hundreds of
thousands of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6248</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6248</id><created>2014-03-25</created><authors><author><keyname>Qiao</keyname><forenames>Qifeng</forenames></author><author><keyname>Beling</keyname><forenames>Peter A.</forenames></author></authors><title>Classroom Video Assessment and Retrieval via Multiple Instance Learning</title><categories>cs.IR cs.CY cs.LG</categories><journal-ref>The 14th International Conference on Artificial Intelligence in
  Education 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multiple instance learning approach to content-based retrieval
of classroom video for the purpose of supporting human assessing the learning
environment. The key element of our approach is a mapping between the semantic
concepts of the assessment system and features of the video that can be
measured using techniques from the fields of computer vision and speech
analysis. We report on a formative experiment in content-based video retrieval
involving trained experts in the Classroom Assessment Scoring System, a widely
used framework for assessment and improvement of learning environments. The
results of this experiment suggest that our approach has potential application
to productivity enhancement in assessment and to broader retrieval tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6251</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6251</id><created>2014-03-25</created><updated>2014-07-11</updated><authors><author><keyname>Mignot</keyname><forenames>Ludovic</forenames><affiliation>LITIS Laboratory Normandie University, University of Rouen France</affiliation></author><author><keyname>Sebti</keyname><forenames>Nadia Ouali</forenames><affiliation>LITIS Laboratory Normandie University, University of Rouen France</affiliation></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames><affiliation>LITIS Laboratory Normandie University, University of Rouen France</affiliation></author></authors><title>K-Position, Follow, Equation and K-C-Continuation Tree Automata
  Constructions</title><categories>cs.FL</categories><proxy>EPTCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exist several methods of computing an automaton recognizing the
language denoted by a given regular expression: In the case of words, the
position automaton P due to Glushkov, the c-continuation automaton C due to
Champarnaud and Ziadi, the follow automaton F due to Ilie and Yu and the
equation automaton E due to Antimirov. It has been shown that P and C are
isomorphic and that E (resp. F) is a quotient of C (resp. of P). In this paper,
we define from a given regular tree expression the k-position tree automaton P
and the follow tree automaton F . Using the definition of the equation tree
automaton E of Kuske and Meinecke and our previously defined k-C-continuation
tree automaton C, we show that the previous morphic relations are still valid
on tree expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6260</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6260</id><created>2014-03-25</created><authors><author><keyname>Ashrafuzzaman</keyname><forenames>M.</forenames></author><author><keyname>Rahman</keyname><forenames>M. M .</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>Capturing and Recognizing Objects Appearance Employing Eigenspace</title><categories>cs.CV</categories><journal-ref>Procs. Of the 5th International Conference on Computer &amp;
  Information Technology (ICCIT 2002), pp. 434-436, Dhaka, Bangladesh, December
  27-28, (2002)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method of capturing objects appearances from its
environment and it also describes how to recognize unknown appearances creating
an eigenspace. This representation and recognition can be done automatically
taking objects various appearances by using robotic vision from a defined
environment. This technique also allows extracting objects from some sort of
complicated scenes. In this case, some of object appearances are taken with
defined occlusions and eigenspaces are created by accepting both of
non-occluded and occluded appearances together. Eigenspace is constructed
successfully every times when a new object appears, and various appearances
accumulated gradually. A sequence of appearances is generated from its
accumulated shapes, which is used for recognition of the unknown objects
appearances. Various objects environments are shown in the experiment to
capture objects appearances and experimental results show effectiveness of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6270</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6270</id><created>2014-03-25</created><authors><author><keyname>Guerrieri</keyname><forenames>Alessio</forenames></author><author><keyname>Montresor</keyname><forenames>Alberto</forenames></author></authors><title>Distributed Edge Partitioning for Graph Processing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of larger and larger graph datasets, growing exponentially
over the years, has created several new algorithmic challenges to be addressed.
Sequential approaches have become unfeasible, while interest on parallel and
distributed algorithms has greatly increased.
  Appropriately partitioning the graph as a preprocessing step can improve the
degree of parallelism of its analysis. A number of heuristic algorithms have
been developed to solve this problem, but many of them subdivide the graph on
its vertex set, thus obtaining a vertex-partitioned graph.
  Aim of this paper is to explore a completely different approach based on edge
partitioning, in which edges, rather than vertices, are partitioned into
disjoint subsets. Contribution of this paper is twofold: first, we introduce a
graph processing framework based on edge partitioning, that is flexible enough
to be applied to several different graph problems. Second, we show the
feasibility of these ideas by presenting a distributed edge partitioning
algorithm called d-fep.
  Our framework is thoroughly evaluated, using both simulations and an Hadoop
implementation running on the Amazon EC2 cloud. The experiments show that d-fep
is efficient, scalable and obtains consistently good partitions. The resulting
edge-partitioned graph can be exploited to obtain more efficient
implementations of graph analysis algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6272</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6272</id><created>2014-03-25</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>On the Excess Bandwidth Allocation in ISP Traffic Control for Shared
  Access Networks</title><categories>cs.NI</categories><journal-ref>(Early Access) IEEE Communications Letters, vol. PP, no. 99, pp.
  1-4, Mar., 2014</journal-ref><doi>10.1109/LCOMM.2014.020414.132542</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current practice of shaping subscriber traffic based on token bucket by
Internet service provider (ISP) allows short-term fluctuations in its shaped
rate and thereby enables a subscriber to transmit traffic at a higher rate than
a negotiated long-term average. The traffic shaping, however, results in
significant waste of network resources, especially when there are only a few
active subscribers, because it cannot allocate excess bandwidth to active
subscribers in the long term. In this letter we investigate the long-term
aspect of resource sharing in ISP traffic control for shared access networks.
We discuss major requirements for the excess bandwidth allocation in shared
access networks and propose ISP traffic control schemes based on core-stateless
fair queueing (CSFQ) and token bucket meters. Simulation results demonstrate
that the proposed schemes allocate excess bandwidth among active subscribers in
a fair and efficient way, while not compromising the service contracts
specified by token bucket for conformant subscribers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6274</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6274</id><created>2014-03-25</created><updated>2014-04-29</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Arguments for Nested Patterns in Neural Ensembles</title><categories>cs.NE q-bio.NC</categories><comments>Preprint</comments><journal-ref>Proceedings of the Science and Information Conference (SAI'14),
  August 27-29, 2014, pp. 488 - 492</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a relatively simple way of allowing a brain model to
self-organise its concept patterns through nested structures. Time is a key
element and a simulator would be able to show how patterns may form and then
fire in sequence, as part of a search or thought process. It uses a very simple
equation to show how the inhibitors in particular, can switch off certain
areas, to allow other areas to become the prominent ones and thereby define the
current brain state. This allows for a small amount of control over what
appears to be a chaotic structure inside of the brain. It is attractive because
it is still mostly mechanical and therefore can be added as an automatic
process, or the modelling of that. The paper also describes how the nested
pattern structure can be used as a basic counting mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6275</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6275</id><created>2014-03-25</created><authors><author><keyname>Vineet</keyname><forenames>Vibhav</forenames></author><author><keyname>Warrell</keyname><forenames>Jonathan</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>A Tiered Move-making Algorithm for General Non-submodular Pairwise
  Energies</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of problems in computer vision can be modelled as energy
minimization problems in a Markov Random Field (MRF) or Conditional Random
Field (CRF) framework. Graph-cuts based $\alpha$-expansion is a standard
move-making method to minimize the energy functions with sub-modular pairwise
terms. However, certain problems require more complex pairwise terms where the
$\alpha$-expansion method is generally not applicable.
  In this paper, we propose an iterative {\em tiered move making algorithm}
which is able to handle general pairwise terms. Each move to the next
configuration is based on the current labeling and an optimal tiered move,
where each tiered move requires one application of the dynamic programming
based tiered labeling method introduced in Felzenszwalb et. al.
\cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimum
for any general pairwise potential, and we give a theoretical analysis of the
properties of the algorithm, characterizing the situations in which we can
expect good performance. We first evaluate our method on an object-class
segmentation problem using the Pascal VOC-11 segmentation dataset where we
learn general pairwise terms. Further we evaluate the algorithm on many other
benchmark labeling problems such as stereo, image segmentation, image stitching
and image denoising. Our method consistently gets better accuracy and energy
values than alpha-expansion, loopy belief propagation (LBP), quadratic
pseudo-boolean optimization (QPBO), and is competitive with TRWS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6288</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6288</id><created>2014-03-25</created><authors><author><keyname>Dehghan</keyname><forenames>Ali</forenames></author><author><keyname>Sadeghi</keyname><forenames>Mohammad-Reza</forenames></author><author><keyname>Ahadi</keyname><forenames>Arash</forenames></author></authors><title>The Complexity of the Sigma Chromatic Number of Cubic Graphs</title><categories>math.CO cs.CC</categories><comments>12 pages, 4 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\it sigma chromatic number} of a graph $G$, denoted by $\sigma(G)$, is
the minimum number $k$ that the vertices can be partitioned into $k$ disjoint
sets $V_1, \ldots, V_k$ such that for every two adjacent vertices $u$ and $v$
there is an index $i$ that $u$ and $v$ have different numbers of neighbors in
$V_i$.
  We show that, it is $ \mathbf{NP} $-complete to decide for a given 3-regular
graph $G$, whether $ \sigma(G)=2$. Also, we prove that for every $k\geq 3$, it
is {\bf NP}-complete to decide whether $\sigma(G)= k$ for a given graph $G$.
Furthermore, for planar $3$-regular graphs with $\sigma=2$, we show that the
problem of minimizing the size of a set is $ \mathbf{NP} $-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6290</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6290</id><created>2014-03-25</created><updated>2014-09-01</updated><authors><author><keyname>Hu</keyname><forenames>Zhenfang</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Yueming</forenames></author><author><keyname>Wu</keyname><forenames>Zhaohui</forenames></author></authors><title>Spectral Sparse Representation for Clustering: Evolved from PCA,
  K-means, Laplacian Eigenmap, and Ratio Cut</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality reduction methods, e.g. PCA and Laplacian eigenmap (LE), and
cluster analysis methods, e.g. K-means and ratio cut (Rcut), are two kinds of
widely-used unsupervised data analysis tools. The former concerns high
representation fidelity, while the latter semantics. Some preliminary relations
between these methods have been established in the literature, but they are not
yet integrated into a unified framework. In this paper, we show that under an
ideal condition, the four methods: PCA, K-means, LE, and Rcut, are unified
together; and when the ideal condition is relaxed, the unification evolves to a
new sparse representation method, called spectral sparse representation (SSR).
It achieves the same representation fidelity as PCA/LE, and is able to reveal
the cluster structure of data as K-means/Rcut. SSR is inherently related to
cluster analysis, and the sparse codes can be directly used to do clustering.
An efficient algorithm NSCrt is developed to solve the sparse codes of SSR. It
is observed that NSCrt is able to effectively recover the underlying solutions.
As a direct application of SSR, a new clustering algorithm Scut is devised. It
reaches the start-of-the-art performance among spectral clustering methods.
Compared with K-means based clustering methods, Scut does not depend on
initialization and avoids getting trapped in local minima; and the solutions
are comparable to the optimal ones of K-means run many times. Extensive
experiments using data sets of different nature demonstrate the properties and
strengths of SSR, NSCrt, and Scut.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6303</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6303</id><created>2014-03-25</created><updated>2015-09-21</updated><authors><author><keyname>Quaas</keyname><forenames>Karin</forenames><affiliation>University of Leipzig</affiliation></author></authors><title>Verification for Timed Automata extended with Unbounded Discrete Data
  Structures</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:20) 2015</journal-ref><doi>10.2168/LMCS-11(3:20)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study decidability of verification problems for timed automata extended
with unbounded discrete data structures. More detailed, we extend timed
automata with a pushdown stack. In this way, we obtain a strong model that may
for instance be used to model real-time programs with procedure calls. It is
long known that the reachability problem for this model is decidable. The goal
of this paper is to identify subclasses of timed pushdown automata for which
the language inclusion problem and related problems are decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6305</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6305</id><created>2014-03-25</created><authors><author><keyname>Sbai</keyname><forenames>Hanae</forenames></author><author><keyname>Fredj</keyname><forenames>Mounia</forenames></author><author><keyname>Kjiri</keyname><forenames>Laila</forenames></author></authors><title>A Pattern based methodology for evolution management in business process
  reuse</title><categories>cs.SE</categories><comments>10 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 1, No 1, January 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, there are Process-Aware Information Systems (PAIS) with a set of
business process models which vary over time to meet the new requirements. In a
competitive environment, the key challenge of enterprises is to reduce the cost
and time of process design and application development. For this purpose,
research on reuse in business process management have introduced the concept of
configurable process models which attempts to manage business process
variability, by integrating a set of process variants in a single model. In
this context, many research works were interested in creating and elaborating
configurable process models. However, this has become insufficient since the
configurable process model should itself evolve to add new variations. In turn,
this requires a comprehensive support for managing the evolution of
configurable process models. In this paper, we present a complete pattern based
methodology for managing the evolution of configurable process models in terms
of activities, data and resources. Our objective is to propose a process
patterns system for guiding designers in modeling and evolving configurable
process models. Furthermore, our process patterns system will be used for an
automated support so as to manage the evolution of configurable process models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6315</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6315</id><created>2014-03-25</created><updated>2014-04-27</updated><authors><author><keyname>Kotnis</keyname><forenames>Bhushan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Cost Effective Rumor Containment in Social Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of rumors through social media and online social networks can not
only disrupt the daily lives of citizens but also result in loss of life and
property. A rumor spreads when individuals, who are unable decide the
authenticity of the information, mistake the rumor as genuine information and
pass it on to their acquaintances. We propose a solution where a set of
individuals (based on their degree) in the social network are trained and
provided resources to help them distinguish a rumor from genuine information.
By formulating an optimization problem we calculate the optimum set of
individuals, who must undergo training, and the quality of training that
minimizes the expected training cost and ensures an upper bound on the size of
the rumor outbreak. Our primary contribution is that although the optimization
problem turns out to be non convex, we show that the problem is equivalent to
solving a set of linear programs. This result also allows us to solve the
problem of minimizing the size of rumor outbreak for a given cost budget. The
optimum solution displays an interesting pattern which can be implemented as a
heuristic. These results can prove to be very useful for social planners and
law enforcement agencies for preventing dangerous rumors and misinformation
epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6318</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6318</id><created>2014-03-25</created><authors><author><keyname>Tracey</keyname><forenames>Brian H.</forenames></author><author><keyname>Miller</keyname><forenames>Eric L.</forenames></author></authors><title>Stabilizing dual-energy X-ray computed tomography reconstructions using
  patch-based regularization</title><categories>cs.CV physics.med-ph</categories><doi>10.1088/0266-5611/31/10/105004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen growing interest in exploiting dual- and multi-energy
measurements in computed tomography (CT) in order to characterize material
properties as well as object shape. Material characterization is performed by
decomposing the scene into constitutive basis functions, such as Compton
scatter and photoelectric absorption functions. While well motivated
physically, the joint recovery of the spatial distribution of photoelectric and
Compton properties is severely complicated by the fact that the data are
several orders of magnitude more sensitive to Compton scatter coefficients than
to photoelectric absorption, so small errors in Compton estimates can create
large artifacts in the photoelectric estimate. To address these issues, we
propose a model-based iterative approach which uses patch-based regularization
terms to stabilize inversion of photoelectric coefficients, and solve the
resulting problem though use of computationally attractive Alternating
Direction Method of Multipliers (ADMM) solution techniques. Using simulations
and experimental data acquired on a commercial scanner, we demonstrate that the
proposed processing can lead to more stable material property estimates which
should aid materials characterization in future dual- and multi-energy CT
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6322</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6322</id><created>2014-03-25</created><authors><author><keyname>Martinez</keyname><forenames>Matias</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Weimer</keyname><forenames>Westley</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe, INRIA Lille - Nord Europe</affiliation></author></authors><title>Do the Fix Ingredients Already Exist? An Empirical Inquiry into the
  Redundancy Assumptions of Program Repair Approaches</title><categories>cs.SE</categories><comments>ICSE - 36th IEEE International Conference on Software Engineering
  (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much initial research on automatic program repair has focused on experimental
results to probe their potential to find patches and reduce development effort.
Relatively less effort has been put into understanding the hows and whys of
such approaches. For example, a critical assumption of the GenProg technique is
that certain bugs can be fixed by copying and re-arranging existing code. In
other words, GenProg assumes that the fix ingredients already exist elsewhere
in the code. In this paper, we formalize these assumptions around the concept
of ''temporal redundancy''. A temporally redundant commit is only composed of
what has already existed in previous commits. Our experiments show that a large
proportion of commits that add existing code are temporally redundant. This
validates the fundamental redundancy assumption of GenProg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6330</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6330</id><created>2014-03-25</created><authors><author><keyname>Herrmann</keyname><forenames>Sebastian</forenames></author><author><keyname>Grahl</keyname><forenames>J&#xf6;rn</forenames></author><author><keyname>Rothlauf</keyname><forenames>Franz</forenames></author></authors><title>Problem Complexity in Parallel Problem Solving</title><categories>cs.SI physics.soc-ph</categories><comments>Proceedings of the International Workshop on Modeling, Analysis and
  Management of Social Networks and their Applications, Bamberg, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works examine the relationship between the communication structure and
the performance of a group in a problem solving task. Some conclude that
inefficient communication networks with long paths outperform efficient
networks on the long run. Others find no influence of the network topology on
group performance. We contribute to this discussion by examining the role of
problem complexity. In particular, we study whether and how the complexity of
the problem at hand moderates the influence of the communication network on
group performance. Results obtained from multi-agent modelling suggest that
problem complexity indeed has an influence. We observe an influence of the
network only for problems of moderate difficulty. For easier or harder
problems, the influence of network topology becomes weaker or irrelevant, which
offers a possible explanation for inconsistencies in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6331</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6331</id><created>2014-03-25</created><updated>2014-12-04</updated><authors><author><keyname>Drange</keyname><forenames>P&#xe5;l Gr&#xf8;n&#xe5;s</forenames></author><author><keyname>Dregi</keyname><forenames>Markus Sortland</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author></authors><title>On the Computational Complexity of Vertex Integrity and Component Order
  Connectivity</title><categories>cs.DS</categories><comments>A preliminary version of this paper already appeared in the
  conference proceedings of ISAAC 2014</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Weighted Vertex Integrity (wVI) problem takes as input an $n$-vertex
graph $G$, a weight function $w:V(G)\to\mathbb{N}$, and an integer $p$. The
task is to decide if there exists a set $X\subseteq V(G)$ such that the weight
of $X$ plus the weight of a heaviest component of $G-X$ is at most $p$. Among
other results, we prove that:
  (1) wVI is NP-complete on co-comparability graphs, even if each vertex has
weight $1$;
  (2) wVI can be solved in $O(p^{p+1}n)$ time;
  (3) wVI admits a kernel with at most $p^3$ vertices.
  Result (1) refutes a conjecture by Ray and Deogun and answers an open
question by Ray et al. It also complements a result by Kratsch et al., stating
that the unweighted version of the problem can be solved in polynomial time on
co-comparability graphs of bounded dimension, provided that an intersection
model of the input graph is given as part of the input.
  An instance of the Weighted Component Order Connectivity (wCOC) problem
consists of an $n$-vertex graph $G$, a weight function $w:V(G)\to \mathbb{N}$,
and two integers $k$ and $l$, and the task is to decide if there exists a set
$X\subseteq V(G)$ such that the weight of $X$ is at most $k$ and the weight of
a heaviest component of $G-X$ is at most $l$. In some sense, the wCOC problem
can be seen as a refined version of the wVI problem. We prove, among other
results, that:
  (4) wCOC can be solved in $O(\min\{k,l\}\cdot n^3)$ time on interval graphs,
while the unweighted version can be solved in $O(n^2)$ time on this graph
class;
  (5) wCOC is W[1]-hard on split graphs when parameterized by $k$ or by $l$;
  (6) wCOC can be solved in $2^{O(k\log l)} n$ time;
  (7) wCOC admits a kernel with at most $kl(k+l)+k$ vertices.
  We also show that result (6) is essentially tight by proving that wCOC cannot
be solved in $2^{o(k \log l)}n^{O(1)}$ time, unless the ETH fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6347</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6347</id><created>2014-03-25</created><updated>2014-10-29</updated><authors><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Kratsch</keyname><forenames>Dieter</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Patel</keyname><forenames>Viresh</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Finding Shortest Paths between Graph Colourings</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-colouring reconfiguration problem asks whether, for a given graph
$G$, two proper $k$-colourings $\alpha$ and $\beta$ of $G$, and a positive
integer $\ell$, there exists a sequence of at most $\ell+1$ proper
$k$-colourings of $G$ which starts with $\alpha$ and ends with $\beta$ and
where successive colourings in the sequence differ on exactly one vertex of
$G$. We give a complete picture of the parameterized complexity of the
$k$-colouring reconfiguration problem for each fixed $k$ when parameterized by
$\ell$. First we show that the $k$-colouring reconfiguration problem is
polynomial-time solvable for $k=3$, settling an open problem of Cereceda, van
den Heuvel and Johnson. Then, for all $k \geq 4$, we show that the
$k$-colouring reconfiguration problem, when parameterized by $\ell$, is
fixed-parameter tractable (addressing a question of Mouawad, Nishimura, Raman,
Simjour and Suzuki) but that it has no polynomial kernel unless the polynomial
hierarchy collapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6348</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6348</id><created>2014-03-25</created><updated>2016-02-07</updated><authors><author><keyname>Sovdat</keyname><forenames>Blaz</forenames></author></authors><title>Updating Formulas and Algorithms for Computing Entropy and Gini Index
  from Time-Changing Data Streams</title><categories>cs.AI cs.LG</categories><comments>Added directions future work; more consistent notation; fixed the
  errors in the updating algorithms for entropy; fixed an error in the
  statement of theorem 5; added two references to related work; fixed a few
  typos</comments><acm-class>I.2.6</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Despite growing interest in data stream mining the most successful
incremental learners, such as VFDT, still use periodic recomputation to update
attribute information gains and Gini indices. This note provides simple
incremental formulas and algorithms for computing entropy and Gini index from
time-changing data streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6351</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6351</id><created>2014-03-25</created><authors><author><keyname>Cortesi</keyname><forenames>Fabrizio L.</forenames></author><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Submodularity of Energy Related Controllability Metrics</title><categories>math.OC cs.SY</categories><comments>7 pages, 2 figures; submitted to the 2014 IEEE Conference on Decision
  and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantification of controllability and observability has recently received
new interest in the context of large, complex networks of dynamical systems. A
fundamental but computationally difficult problem is the placement or selection
of actuators and sensors that optimize real-valued controllability and
observability metrics of the network. We show that several classes of energy
related metrics associated with the controllability Gramian in linear dynamical
systems have a strong structural property, called submodularity. This property
allows for an approximation guarantee by using a simple greedy heuristic for
their maximization. The results are illustrated for randomly generated systems
and for placement of power electronic actuators in a model of the European
power grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6358</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6358</id><created>2014-03-21</created><authors><author><keyname>Azad</keyname><forenames>Ariful</forenames></author><author><keyname>Rajwa</keyname><forenames>Bartek</forenames></author><author><keyname>Pothen</keyname><forenames>Alex</forenames></author></authors><title>Immunophenotypes of Acute Myeloid Leukemia From Flow Cytometry Data
  Using Templates</title><categories>q-bio.QM cs.CE</categories><comments>9 pages, 5 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Motivation: We investigate whether a template-based classification pipeline
could be used to identify immunophenotypes in (and thereby classify) a
heterogeneous disease with many subtypes. The disease we consider here is Acute
Myeloid Leukemia, which is heterogeneous at the morphologic, cytogenetic and
molecular levels, with several known subtypes. The prognosis and treatment for
AML depends on the subtype.
  Results: We apply flowMatch, an algorithmic pipeline for flow cytometry data
created in earlier work, to compute templates succinctly summarizing classes of
AML and healthy samples. We develop a scoring function that accounts for
features of the AML data such as heterogeneity to identify immunophenotypes
corresponding to various AML subtypes, including APL. All of the AML samples in
the test set are classified correctly with high confidence.
  Availability: flowMatch is available at
www.bioconductor.org/packages/devel/bioc/html/flowMatch.html; programs specific
to immunophenotyping AML are at www.cs.purdue.edu/homes/aazad/software.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6361</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6361</id><created>2014-03-25</created><updated>2015-07-24</updated><authors><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Weak locking capacity of quantum channels can be much larger than
  private capacity</title><categories>quant-ph cs.IT math.IT</categories><comments>17pp; v2 has some typos corrected and modified abstract; v3 has 18pp,
  some more details in various proofs, even fewer typos and added references,
  accepted with Journal of Cryptology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that it is possible for the so-called weak locking capacity of a
quantum channel [Guha et al., PRX 4:011016, 2014] to be much larger than its
private capacity. Both reflect different ways of capturing the notion of
reliable communication via a quantum system while leaking almost no information
to an eavesdropper; the difference is that the latter imposes an intrinsically
quantum security criterion whereas the former requires only a weaker, classical
condition. The channels for which this separation is most straightforward to
establish are the complementary channels of classical-quantum (cq-)channels,
and hence a subclass of Hadamard channels. We also prove that certain symmetric
channels (related to photon number splitting) have positive weak locking
capacity in the presence of a vanishingly small pre-shared secret, whereas
their private capacity is zero.
  These findings are powerful illustrations of the difference between two
apparently natural notions of privacy in quantum systems, relevant also to
quantum key distribution (QKD): the older, naive one based on accessible
information, contrasting with the new, composable one embracing the quantum
nature of the eavesdropper's information.
  Assuming an additivity conjecture for constrained minimum output Renyi
entropies, the techniques of the first part demonstrate a single-letter formula
for the weak locking capacity of complements to cq-channels, coinciding with a
general upper bound of Guha et al. for these channels. Furthermore, still
assuming this additivity conjecture, this upper bound is given an operational
interpretation for general channels as the maximum weak locking capacity of the
channel activated by a suitable noiseless channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6367</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6367</id><created>2014-03-24</created><updated>2014-03-27</updated><authors><author><keyname>Wang</keyname><forenames>Shuling</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author></authors><title>A Framework for Hybrid Systems with Denial-of-Service Security Attack</title><categories>cs.LO cs.CR cs.SY</categories><comments>19 pages, 1 figures, the short version was accepted by FORTE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems are integrations of discrete computation and continuous
physical evolution. The physical components of such systems introduce safety
requirements, the achievement of which asks for the correct monitoring and
control from the discrete controllers. However, due to denial-of-service
security attack, the expected information from the controllers is not received
and as a consequence the physical systems may fail to behave as expected. This
paper proposes a formal framework for expressing denial-of-service security
attack in hybrid systems. As a virtue, a physical system is able to plan for
reasonable behavior in case the ideal control fails due to unreliable
communication, in such a way that the safety of the system upon
denial-of-service is still guaranteed. In the context of the modeling language,
we develop an inference system for verifying safety of hybrid systems, without
putting any assumptions on how the environments behave. Based on the inference
system, we implement an interactive theorem prover and have applied it to check
an example taken from train control system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6378</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6378</id><created>2014-03-24</created><authors><author><keyname>Bornholdt</keyname><forenames>Stefan</forenames></author><author><keyname>Sneppen</keyname><forenames>Kim</forenames></author></authors><title>Do Bitcoins make the world go round? On the dynamics of competing
  crypto-currencies</title><categories>physics.soc-ph cs.CY q-fin.GN</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoins have emerged as a possible competitor to usual currencies, but other
crypto-currencies have likewise appeared as competitors to the Bitcoin
currency. The expanding market of crypto-currencies now involves capital
equivalent to $10^{10}$ US Dollars, providing academia with an unusual
opportunity to study the emergence of value. Here we show that the Bitcoin
currency in itself is not special, but may rather be understood as the
contemporary dominating crypto-currency that may well be replaced by other
currencies. We suggest that perception of value in a social system is generated
by a voter-like dynamics, where fashions form and disperse even in the case
where information is only exchanged on a pairwise basis between agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6381</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6381</id><created>2014-03-21</created><authors><author><keyname>Sureka</keyname><forenames>K.</forenames></author><author><keyname>Srinivasagan</keyname><forenames>K. G.</forenames></author><author><keyname>Suganthi</keyname><forenames>S.</forenames></author></authors><title>An efficiency dependency parser using hybrid approach for tamil language</title><categories>cs.CL</categories><comments>5 pages,8 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Natural language processing is a prompt research area across the country.
Parsing is one of the very crucial tool in language analysis system which aims
to forecast the structural relationship among the words in a given sentence.
Many researchers have already developed so many language tools but the accuracy
is not meet out the human expectation level, thus the research is still exists.
Machine translation is one of the major application area under Natural Language
Processing. While translation between one language to another language, the
structure identification of a sentence play a key role. This paper introduces
the hybrid way to solve the identification of relationship among the given
words in a sentence. In existing system is implemented using rule based
approach, which is not suited in huge amount of data. The machine learning
approaches is suitable for handle larger amount of data and also to get better
accuracy via learning and training the system. The proposed approach takes a
Tamil sentence as an input and produce the result of a dependency relation as a
tree like structure using hybrid approach. This proposed tool is very helpful
for researchers and act as an odd-on improve the quality of existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6382</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6382</id><created>2014-03-23</created><updated>2014-05-12</updated><authors><author><keyname>Razavian</keyname><forenames>Ali Sharif</forenames></author><author><keyname>Azizpour</keyname><forenames>Hossein</forenames></author><author><keyname>Sullivan</keyname><forenames>Josephine</forenames></author><author><keyname>Carlsson</keyname><forenames>Stefan</forenames></author></authors><title>CNN Features off-the-shelf: an Astounding Baseline for Recognition</title><categories>cs.CV</categories><comments>version 3 revisions: 1)Added results using feature processing and
  data augmentation 2)Referring to most recent efforts of using CNN for
  different visual recognition tasks 3) updated text/caption</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results indicate that the generic descriptors extracted from the
convolutional neural networks are very powerful. This paper adds to the
mounting evidence that this is indeed the case. We report on a series of
experiments conducted for different recognition tasks using the publicly
available code and model of the \overfeat network which was trained to perform
object classification on ILSVRC13. We use features extracted from the \overfeat
network as a generic image representation to tackle the diverse range of
recognition tasks of object image classification, scene recognition, fine
grained recognition, attribute detection and image retrieval applied to a
diverse set of datasets. We selected these tasks and datasets as they gradually
move further away from the original task and data the \overfeat network was
trained to solve. Astonishingly, we report consistent superior results compared
to the highly tuned state-of-the-art systems in all the visual classification
tasks on various datasets. For instance retrieval it consistently outperforms
low memory footprint methods except for sculptures dataset. The results are
achieved using a linear SVM classifier (or $L2$ distance in case of retrieval)
applied to a feature representation of size 4096 extracted from a layer in the
net. The representations are further modified using simple augmentation
techniques e.g. jittering. The results strongly suggest that features obtained
from deep learning with convolutional nets should be the primary candidate in
most visual recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6386</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6386</id><created>2014-03-25</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author></authors><title>Recoloring graphs via tree decompositions</title><categories>cs.DM math.CO</categories><comments>20 pages, 8 figures, partial results already presented in
  http://arxiv.org/abs/1302.3486</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k$ be an integer. Two vertex $k$-colorings of a graph are
\emph{adjacent} if they differ on exactly one vertex. A graph is
\emph{$k$-mixing} if any proper $k$-coloring can be transformed into any other
through a sequence of adjacent proper $k$-colorings. Jerrum proved that any
graph is $k$-mixing if $k$ is at least the maximum degree plus two. We first
improve Jerrum's bound using the grundy number, which is the worst number of
colors in a greedy coloring.
  Any graph is $(tw+2)$-mixing, where $tw$ is the treewidth of the graph
(Cereceda 2006). We prove that the shortest sequence between any two
$(tw+2)$-colorings is at most quadratic (which is optimal up to a constant
factor), a problem left open in Bonamy et al. (2012).
  We also prove that given any two $(\chi(G)+1)$-colorings of a cograph (resp.
distance-hereditary graph) $G$, we can find a linear (resp. quadratic) sequence
between them. In both cases, the bounds cannot be improved by more than a
constant factor for a fixed $\chi(G)$. The graph classes are also optimal in
some sense: one of the smallest interesting superclass of distance-hereditary
graphs corresponds to comparability graphs, for which no such property holds
(even when relaxing the constraint on the length of the sequence). As for
cographs, they are equivalently the graphs with no induced $P_4$, and there
exist $P_5$-free graphs that admit no sequence between two of their
$(\chi(G)+1)$-colorings.
  All the proofs are constructivist and lead to polynomial-time recoloring
algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6392</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6392</id><created>2014-03-25</created><updated>2014-03-31</updated><authors><author><keyname>Curiel</keyname><forenames>Arturo</forenames></author><author><keyname>Collet</keyname><forenames>Christophe</forenames></author></authors><title>Implementation of an Automatic Sign Language Lexical Annotation
  Framework based on Propositional Dynamic Logic</title><categories>cs.CL</categories><comments>Accepted in the &quot;LREC 2014: 6th Workshop on the Representation and
  Processing of Signed Languages&quot;</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the implementation of an automatic Sign Language
(SL) sign annotation framework based on a formal logic, the Propositional
Dynamic Logic (PDL). Our system relies heavily on the use of a specific variant
of PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets
us describe SL signs as formulae and corpora videos as labeled transition
systems (LTSs). Here, we intend to show how a generic annotation system can be
constructed upon these underlying theoretical principles, regardless of the
tracking technologies available or the input format of corpora. With this in
mind, we generated a development framework that adapts the system to specific
use cases. Furthermore, we present some results obtained by our application
when adapted to one distinct case, 2D corpora analysis with pre-processed
tracking information. We also present some insights on how such a technology
can be used to analyze 3D real-time data, captured with a depth device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6394</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6394</id><created>2014-03-25</created><authors><author><keyname>Chiesa</keyname><forenames>Alessandro</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Bridging Utility Maximization and Regret Minimization</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We relate the strategy sets that a player ends up with after refining his own
strategies according to two very different models of rationality: namely,
utility maximization and regret minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6397</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6397</id><created>2014-03-25</created><authors><author><keyname>Rosner</keyname><forenames>Frank</forenames></author><author><keyname>Hinneburg</keyname><forenames>Alexander</forenames></author><author><keyname>R&#xf6;der</keyname><forenames>Michael</forenames></author><author><keyname>Nettling</keyname><forenames>Martin</forenames></author><author><keyname>Both</keyname><forenames>Andreas</forenames></author></authors><title>Evaluating topic coherence measures</title><categories>cs.LG cs.CL cs.IR</categories><comments>This work has been presented at the &quot;Topic Models: Computation,
  Application and Evaluation&quot; workshop at the &quot;Neural Information Processing
  Systems&quot; conference 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Topic models extract representative word sets - called topics - from word
counts in documents without requiring any semantic annotations. Topics are not
guaranteed to be well interpretable, therefore, coherence measures have been
proposed to distinguish between good and bad topics. Studies of topic coherence
so far are limited to measures that score pairs of individual words. For the
first time, we include coherence measures from scientific philosophy that score
pairs of more complex word subsets and apply them to topic scoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6409</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6409</id><created>2014-03-25</created><updated>2014-04-01</updated><authors><author><keyname>Chiesa</keyname><forenames>Alessandro</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Knightian Robustness from Regret Minimization</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider auctions in which the players have very limited knowledge about
their own valuations. Specifically, the only information that a Knightian
player $i$ has about the profile of true valuations, $\theta^*$, consists of a
set of distributions, from one of which $\theta_i^*$ has been drawn.
  We analyze the social-welfare performance of the VCG mechanism, for
unrestricted combinatorial auctions, when Knightian players that either (a)
choose a regret-minimizing strategy, or (b) resort to regret minimization only
to refine further their own sets of undominated strategies, if needed. We prove
that this performance is very good.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6410</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6410</id><created>2014-03-25</created><authors><author><keyname>Chiesa</keyname><forenames>Alessandro</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Knightian Analysis of the VCG Mechanism in Unrestricted Combinatorial
  Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider auctions in which the players have very limited knowledge about
their own valuations. Specifically, the only information that a Knightian
player $i$ has about the profile of true valuations, $\theta^*$, consists of a
set of distributions, from one of which $\theta_i^*$ has been drawn.
  The VCG mechanism guarantees very high social welfare both in single- and
multi-good auctions, so long as Knightian players do not select strategies that
are dominated. With such Knightian players, however, we prove that the VCG
mechanism guarantees very poor social welfare in unrestricted combinatorial
auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6411</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6411</id><created>2014-03-25</created><authors><author><keyname>Chiesa</keyname><forenames>Alessandro</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Knightian Robustness of Single-Parameter Domains</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider players that have very limited knowledge about their own
valuations. Specifically, the only information that a Knightian player $i$ has
about the profile of true valuations, $\theta^*$, consists of a set of
distributions, from one of which $\theta_i^*$ has been drawn.
  We prove a ``robustness'' theorem for Knightian players in single-parameter
domains: every mechanism that is weakly dominant-strategy truthful for
classical players continues to be well-behaved for Knightian players that
choose undominated strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6413</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6413</id><created>2014-03-25</created><updated>2015-04-24</updated><authors><author><keyname>Chiesa</keyname><forenames>Alessandro</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>Knightian Analysis of the Vickrey Mechanism</title><categories>cs.GT</categories><comments>To appear in Econometrica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the Vickrey mechanism for auctions of multiple identical goods
when the players have both Knightian uncertainty over their own valuations and
incomplete preferences. In this model, the Vickrey mechanism is no longer
dominant-strategy, and we prove that all dominant-strategy mechanisms are
inadequate. However, we also prove that, in undominated strategies, the social
welfare produced by the Vickrey mechanism in the worst case is not only very
good, but also essentially optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6414</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6414</id><created>2014-03-25</created><updated>2014-05-21</updated><authors><author><keyname>B&#x159;inda</keyname><forenames>Karel</forenames><affiliation>LIGM Universit&#xe9; Paris-Est Marne-la-Vall&#xe9;e</affiliation></author></authors><title>Languages of lossless seeds</title><categories>cs.DM math.CO</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 139-150</journal-ref><doi>10.4204/EPTCS.151.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several algorithms for similarity search employ seeding techniques to quickly
discard very dissimilar regions. In this paper, we study theoretical properties
of lossless seeds, i.e., spaced seeds having full sensitivity. We prove that
lossless seeds coincide with languages of certain sofic subshifts, hence they
can be recognized by finite automata. Moreover, we show that these subshifts
are fully given by the number of allowed errors k and the seed margin l. We
also show that for a fixed k, optimal seeds must asymptotically satisfy l ~
m^(k/(k+1)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6426</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6426</id><created>2014-03-25</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Fabregat-Traver</keyname><forenames>Diego</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>High Performance Solutions for Big-data GWAS</title><categories>q-bio.GN cs.CE cs.MS</categories><comments>Submitted to Parallel Computing. arXiv admin note: substantial text
  overlap with arXiv:1304.2272</comments><report-no>AICES-2013/12-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to associate complex traits with genetic polymorphisms, genome-wide
association studies process huge datasets involving tens of thousands of
individuals genotyped for millions of polymorphisms. When handling these
datasets, which exceed the main memory of contemporary computers, one faces two
distinct challenges: 1) Millions of polymorphisms and thousands of phenotypes
come at the cost of hundreds of gigabytes of data, which can only be kept in
secondary storage; 2) the relatedness of the test population is represented by
a relationship matrix, which, for large populations, can only fit in the
combined main memory of a distributed architecture. In this paper, by using
distributed resources such as Cloud or clusters, we address both challenges:
The genotype and phenotype data is streamed from secondary storage using a
double buffer- ing technique, while the relationship matrix is kept across the
main memory of a distributed memory system. With the help of these solutions,
we develop separate algorithms for studies involving only one or a multitude of
traits. We show that these algorithms sustain high-performance and allow the
analysis of enormous datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6428</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6428</id><created>2014-03-25</created><authors><author><keyname>Chicca</keyname><forenames>Elisabetta</forenames></author><author><keyname>Stefanini</keyname><forenames>Fabio</forenames></author><author><keyname>Bartolozzi</keyname><forenames>Chiara</forenames></author><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author></authors><title>Neuromorphic electronic circuits for building autonomous cognitive
  systems</title><categories>cs.ET q-bio.NC</categories><comments>Submitted to Proceedings of IEEE, spiking neural network
  implementations in full custom VLSI</comments><acm-class>A.1; B.7.1; C.1.3; I.2.6; I.5.5</acm-class><doi>10.1109/JPROC.2014.2313954</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several analog and digital brain-inspired electronic systems have been
recently proposed as dedicated solutions for fast simulations of spiking neural
networks. While these architectures are useful for exploring the computational
properties of large-scale models of the nervous system, the challenge of
building low-power compact physical artifacts that can behave intelligently in
the real-world and exhibit cognitive abilities still remains open. In this
paper we propose a set of neuromorphic engineering solutions to address this
challenge. In particular, we review neuromorphic circuits for emulating neural
and synaptic dynamics in real-time and discuss the role of biophysically
realistic temporal dynamics in hardware neural processing architectures; we
review the challenges of realizing spike-based plasticity mechanisms in real
physical systems and present examples of analog electronic circuits that
implement them; we describe the computational properties of recurrent neural
networks and show how neuromorphic Winner-Take-All circuits can implement
working-memory and decision-making mechanisms. We validate the neuromorphic
approach proposed with experimental results obtained from our own circuits and
systems, and argue how the circuits and networks presented in this work
represent a useful set of components for efficiently and elegantly implementing
neuromorphic cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6487</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6487</id><created>2014-03-23</created><authors><author><keyname>Brown</keyname><forenames>Christopher W.</forenames></author></authors><title>Model-based construction of Open Non-uniform Cylindrical Algebraic
  Decompositions</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper we introduce the notion of an Open Non-uniform Cylindrical
Algebraic Decomposition (NuCAD), and present an efficient model-based algorithm
for constructing an Open NuCAD from an input formula. A NuCAD is a
generalization of Cylindrical Algebraic Decomposition (CAD) as defined by
Collins in his seminal work from the early 1970s, and as extended in concepts
like Hong's partial CAD. A NuCAD, like a CAD, is a decomposition of
n-dimensional real space into cylindrical cells. But unlike a CAD, the cells in
a NuCAD need not be arranged cylindrically. It is in this sense that NuCADs are
not uniformly cylindrical. However, NuCADs--- like CADs --- carry a tree-like
structure that relates different cells. It is a very different tree but, as
with the CAD tree structure, it allows some operations to be performed
efficiently, for example locating the containing cell for an arbitrary input
point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6508</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6508</id><created>2014-03-25</created><updated>2014-04-10</updated><authors><author><keyname>Lin</keyname><forenames>Xiaomin</forenames></author><author><keyname>Beling</keyname><forenames>Peter A.</forenames></author><author><keyname>Cogill</keyname><forenames>Randy</forenames></author></authors><title>Multi-agent Inverse Reinforcement Learning for Zero-sum Games</title><categories>cs.GT cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a Bayesian framework for solving a class of
problems termed Multi-agent Inverse Reinforcement Learning (MIRL). Compared to
the well-known Inverse Reinforcement Learning (IRL) problem, MIRL is formalized
in the context of a stochastic game rather than a Markov decision process
(MDP). Games bring two primary challenges: First, the concept of optimality,
central to MDPs, loses its meaning and must be replaced with a more general
solution concept, such as the Nash equilibrium. Second, the non-uniqueness of
equilibria means that in MIRL, in addition to multiple reasonable solutions for
a given inversion model, there may be multiple inversion models that are all
equally sensible approaches to solving the problem. We establish a theoretical
foundation for competitive two-agent MIRL problems and propose a Bayesian
optimization algorithm to solve the problem. We focus on the case of two-person
zero-sum stochastic games, developing a generative model for the likelihood of
unknown rewards of agents given observed game play assuming that the two agents
follow a minimax bipolicy. As a numerical illustration, we apply our method in
the context of an abstract soccer game. For the soccer game, we investigate
relationships between the extent of prior information and the quality of
learned rewards. Results suggest that covariance structure is more important
than mean value in reward priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6512</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6512</id><created>2014-03-25</created><updated>2014-03-31</updated><authors><author><keyname>Turan</keyname><forenames>Gyorgy</forenames></author><author><keyname>Yaggie</keyname><forenames>Jon</forenames></author></authors><title>Non-characterizability of belief revision: an application of finite
  model theory</title><categories>math.LO cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A formal framework is given for the characterizability of a class of belief
revision operators, defined using minimization over a class of partial
preorders, by postulates. It is shown that for partial orders
characterizability implies a definability property of the class of partial
orders in monadic second-order logic. Based on a non-definability result for a
class of partial orders, an example is given of a non-characterizable class of
revision operators. This appears to be the first non-characterizability result
in belief revision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6518</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6518</id><created>2014-03-25</created><authors><author><keyname>Bonnet</keyname><forenames>Edouard</forenames></author><author><keyname>Jamain</keyname><forenames>Florian</forenames></author><author><keyname>Saffidine</keyname><forenames>Abdallah</forenames></author></authors><title>Havannah and TwixT are PSPACE-complete</title><categories>cs.CC</categories><comments>13 pages, 7 figures. Accepted in CG'13</comments><msc-class>68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous popular abstract strategy games ranging from Hex and Havannah to
Lines of Action belong to the class of connection games. Still, very few
complexity results on such games have been obtained since Hex was proved
PSPACE-complete in the early eighties. We study the complexity of two
connection games among the most widely played. Namely, we prove that Havannah
and TwixT are PSPACE-complete. The proof for Havannah involves a reduction from
Generalized Geography and is based solely on ring-threats to represent the
input graph. On the other hand, the reduction for TwixT builds up on previous
work as it is a straightforward encoding of Hex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6530</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6530</id><created>2014-03-25</created><updated>2015-03-18</updated><authors><author><keyname>A.</keyname><forenames>Prashanth L.</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author></authors><title>Variance-Constrained Actor-Critic Algorithms for Discounted and Average
  Reward MDPs</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many sequential decision-making problems we may want to manage risk by
minimizing some measure of variability in rewards in addition to maximizing a
standard criterion. Variance related risk measures are among the most common
risk-sensitive criteria in finance and operations research. However, optimizing
many such criteria is known to be a hard problem. In this paper, we consider
both discounted and average reward Markov decision processes. For each
formulation, we first define a measure of variability for a policy, which in
turn gives us a set of risk-sensitive criteria to optimize. For each of these
criteria, we derive a formula for computing its gradient. We then devise
actor-critic algorithms that operate on three timescales - a TD critic on the
fastest timescale, a policy gradient (actor) on the intermediate timescale, and
a dual ascent for Lagrange multipliers on the slowest timescale. In the
discounted setting, we point out the difficulty in estimating the gradient of
the variance of the return and incorporate simultaneous perturbation approaches
to alleviate this. The average setting, on the other hand, allows for an actor
update using compatible features to estimate the gradient of the variance. We
establish the convergence of our algorithms to locally risk-sensitive optimal
policies. Finally, we demonstrate the usefulness of our algorithms in a traffic
signal control application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6540</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6540</id><created>2014-03-25</created><updated>2014-03-27</updated><authors><author><keyname>Adcock</keyname><forenames>Ben</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author><author><keyname>Roman</keyname><forenames>Bogdan</forenames></author></authors><title>The quest for optimal sampling: Computationally efficient,
  structure-exploiting measurements for compressed sensing</title><categories>math.FA cs.IT math.IT</categories><comments>23 pages, 7 figures, to appear in Springer book &quot;Compressed Sensing
  and Its Applications&quot;, 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An intriguing phenomenon in many instances of compressed sensing is that the
reconstruction quality is governed not just by the overall sparsity of the
signal, but also on its structure. This paper is about understanding this
phenomenon, and demonstrating how it can be fruitfully exploited by the design
of suitable sampling strategies in order to outperform more standard compressed
sensing techniques based on random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6555</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6555</id><created>2014-03-25</created><authors><author><keyname>Kim</keyname><forenames>Sang Wu</forenames></author></authors><title>Modify-and-Forward for Securing Cooperative Relay Communications</title><categories>cs.IT math.IT</categories><comments>IEEE International Zurich Seminar on Communications, Feb. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed a new physical layer technique that can enhance the security of
cooperative relay communications. The proposed approach modifies the decoded
message at the relay according to the unique channel state between the relay
and the destination such that the destination can utilize the modified message
to its advantage while the eavesdropper cannot. We present a practical method
for securely sharing the modification rule between the legitimate partners and
present the secrecy outage probability in a quasi-static fading channel. It is
demonstrated that the proposed scheme can provide a significant improvement
over other schemes when the relay can successfully decode the source message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6561</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6561</id><created>2014-03-25</created><authors><author><keyname>Yue</keyname><forenames>Dian-Wu</forenames></author><author><keyname>Sun</keyname><forenames>Yichuang</forenames></author></authors><title>Transmit Power Minimization for MIMO Systems of Exponential Average BER
  with Fixed Outage Probability</title><categories>cs.IT math.IT</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a wireless system operating in MIMO fading
channels with channel state information being known at both transmitter and
receiver. By spatiotemporal subchannel selection and power control, it aims to
minimize the average transmit power (ATP) of the MIMO system while achieving an
exponential type of average bit error rate (BER) for each data stream. Under
the constraints of a given fixed individual outage probability (OP) and average
BER for each subchannel, based on a traditional upper bound and a dynamic upper
bound of Q function, two closed-form ATP expressions are derived, respectively,
and they correspond to two different power allocation schemes. Numerical
results are provided to validate the theoretical analysis, and show that the
power allocation scheme with the dynamic upper bound can achieve more power
savings than the one with the traditional upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6563</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6563</id><created>2014-03-25</created><updated>2014-04-10</updated><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>Fully Abstract Game Semantics for Actors</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1310.4306 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Along the way paved by the recent concurrent game semantics for process
algebra CCS and $\pi$-calculus, based on the basic characteristics of the actor
computational model and the very reductive semantics for actors, we establish a
fully abstract concurrent game semantics for actors by borrowing the algebraic
structure from CCS. This semantics can both be seen as an innocent presheaf
semantics, and a concurrent game semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6566</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6566</id><created>2014-03-25</created><updated>2014-08-21</updated><authors><author><keyname>Dong</keyname><forenames>Weiming</forenames></author><author><keyname>Wu</keyname><forenames>Fuzhang</forenames></author><author><keyname>Kong</keyname><forenames>Yan</forenames></author><author><keyname>Mei</keyname><forenames>Xing</forenames></author><author><keyname>Lee</keyname><forenames>Tong-Yee</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaopeng</forenames></author></authors><title>Image Retargeting by Content-Aware Synthesis</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world images usually contain vivid contents and rich textural details,
which will complicate the manipulation on them. In this paper, we design a new
framework based on content-aware synthesis to enhance content-aware image
retargeting. By detecting the textural regions in an image, the textural image
content can be synthesized rather than simply distorted or cropped. This method
enables the manipulation of textural &amp; non-textural regions with different
strategy since they have different natures. We propose to retarget the textural
regions by content-aware synthesis and non-textural regions by fast
multi-operators. To achieve practical retargeting applications for general
images, we develop an automatic and fast texture detection method that can
detect multiple disjoint textural regions. We adjust the saliency of the image
according to the features of the textural regions. To validate the proposed
method, comparisons with state-of-the-art image targeting techniques and a user
study were conducted. Convincing visual results are shown to demonstrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6600</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6600</id><created>2014-03-26</created><updated>2014-11-26</updated><authors><author><keyname>Sudholt</keyname><forenames>Dirk</forenames></author></authors><title>How Crossover Speeds Up Building-Block Assembly in Genetic Algorithms</title><categories>cs.NE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We re-investigate a fundamental question: how effective is crossover in
Genetic Algorithms in combining building blocks of good solutions? Although
this has been discussed controversially for decades, we are still lacking a
rigorous and intuitive answer. We provide such answers for royal road functions
and OneMax, where every bit is a building block. For the latter we show that
using crossover makes every ($\mu$+$\lambda$) Genetic Algorithm at least twice
as fast as the fastest evolutionary algorithm using only standard bit mutation,
up to small-order terms and for moderate $\mu$ and $\lambda$. Crossover is
beneficial because it effectively turns fitness-neutral mutations into
improvements by combining the right building blocks at a later stage. Compared
to mutation-based evolutionary algorithms, this makes multi-bit mutations more
useful. Introducing crossover changes the optimal mutation rate on OneMax from
$1/n$ to $(1+\sqrt{5})/2 \cdot 1/n \approx 1.618/n$. This holds both for
uniform crossover and $k$-point crossover. Experiments and statistical tests
confirm that our findings apply to a broad class of building-block functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6602</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6602</id><created>2014-03-26</created><updated>2014-06-13</updated><authors><author><keyname>Nebel</keyname><forenames>Markus E.</forenames></author><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author></authors><title>Pivot Sampling in Dual-Pivot Quicksort</title><categories>cs.DS math.PR</categories><comments>presented at AofA 2014 (http://www.aofa14.upmc.fr/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new dual-pivot Quicksort by Vladimir Yaroslavskiy - used in Oracle's Java
runtime library since version 7 - features intriguing asymmetries in its
behavior. They were shown to cause a basic variant of this algorithm to use
less comparisons than classic single-pivot Quicksort implementations. In this
paper, we extend the analysis to the case where the two pivots are chosen as
fixed order statistics of a random sample and give the precise leading term of
the average number of comparisons, swaps and executed Java Bytecode
instructions. It turns out that - unlike for classic Quicksort, where it is
optimal to choose the pivot as median of the sample - the asymmetries in
Yaroslavskiy's algorithm render pivots with a systematic skew more efficient
than the symmetric choice. Moreover, the optimal skew heavily depends on the
employed cost measure; most strikingly, abstract costs like the number of swaps
and comparisons yield a very different result than counting Java Bytecode
instructions, which can be assumed most closely related to actual running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6611</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6611</id><created>2014-03-26</created><authors><author><keyname>Feng</keyname><forenames>Shiguang</forenames></author><author><keyname>Zhao</keyname><forenames>Xishun</forenames></author></authors><title>More on Descriptive Complexity of Second-Order HORN Logics</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper concerns Gradel's question asked in 1992: whether all problems
which are in PTIME and closed under substructures are definable in second-order
HORN logic SO-HORN. We introduce revisions of SO-HORN and DATALOG by adding
first-order universal quantifiers over the second-order atoms in the bodies of
HORN clauses and DATALOG rules. We show that both logics are as expressive as
FO(LFP), the least fixed point logic. We also prove that FO(LFP) can not define
all of the problems that are in PTIME and closed under substructures. As a
corollary, we answer Gradel's question negatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6614</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6614</id><created>2014-03-26</created><authors><author><keyname>Ho</keyname><forenames>Kin Tat</forenames></author><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author></authors><title>QCMC: Quasi-conformal Parameterizations for Multiply-connected domains</title><categories>cs.CG cs.CV math.DG</categories><comments>26 pages, 23 figures, submitted. arXiv admin note: text overlap with
  arXiv:1402.6908, arXiv:1307.2679 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a method to compute the {\it quasi-conformal
parameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMC
computes a quasi-conformal map from a multiply-connected domain $S$ onto a
punctured disk $D_S$ associated with a given Beltrami differential. The
Beltrami differential, which measures the conformality distortion, is a
complex-valued function $\mu:S\to\mathbb{C}$ with supremum norm strictly less
than 1. Every Beltrami differential gives a conformal structure of $S$. Hence,
the conformal module of $D_S$, which are the radii and centers of the inner
circles, can be fully determined by $\mu$, up to a M\&quot;obius transformation. In
this paper, we propose an iterative algorithm to simultaneously search for the
conformal module and the optimal quasi-conformal parameterization. The key idea
is to minimize the Beltrami energy subject to the boundary constraints. The
optimal solution is our desired quasi-conformal parameterization onto a
punctured disk. The parameterization of the multiply-connected domain
simplifies numerical computations and has important applications in various
fields, such as in computer graphics and vision. Experiments have been carried
out on synthetic data together with real multiply-connected Riemann surfaces.
Results show that our proposed method can efficiently compute quasi-conformal
parameterizations of multiply-connected domains and outperforms other
state-of-the-art algorithms. Applications of the proposed parameterization
technique have also been explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6626</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6626</id><created>2014-03-26</created><authors><author><keyname>Ahmad</keyname><forenames>Musheer</forenames></author><author><keyname>Alsharari</keyname><forenames>Hamed D.</forenames></author><author><keyname>Nizam</keyname><forenames>Munazza</forenames></author></authors><title>Security Improvement of an Image Encryption Based on
  mPixel-Chaotic-Shuffle and Pixel-Chaotic-Diffusion</title><categories>cs.CR</categories><journal-ref>European Journal of Scientific Research, ISSN 1450-216X Vol. 98,
  No. 3, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to improve the security performance of a recently
proposed color image encryption algorithm which is based on multi-chaotic
systems. The existing cryptosystem employed a pixel-chaotic-shuffle mechanism
to encrypt images, in which the generation of shuffling sequences are
independent to the plain-image/cipher-image. As a result, it fails to the
chosen-plaintext and known-plaintext attacks. Moreover, the statistical
features of the cryptosystem are not up to the standard. Therefore, the
security improvements are framed to make the above attacks infeasible and
enhance the statistical features as well. It is achieved by modifying the
pixel-chaotic-shuffle mechanism and adding a new pixel-chaotic-diffusion
mechanism to it. The keys for diffusion of pixels are extracted from the same
chaotic sequences generated in the previous stage. The simulation analyses and
studies are performed to demonstrate that the updated version of cryptosystem
has better statistical features and resistant to the chosen-plaintext and
known-plaintext attacks than the existing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6632</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6632</id><created>2014-03-26</created><authors><author><keyname>Kavvadias</keyname><forenames>Nikolaos</forenames></author><author><keyname>Nikolaidis</keyname><forenames>Spiridon</forenames></author></authors><title>Design space exploration tools for the ByoRISC configurable processor
  family</title><categories>cs.AR</categories><comments>12 pages, 14 figures, 7 tables. Unpublished paper on ByoRISC, an
  extensible RISC with MIMO CIs that can outperform most mid-range VLIWs.
  Unfortunately Prof. Jorg Henkel destroyed the potential of this submission by
  using immoral tactics (neglecting his conflict of interest, changing
  reviewers accepting the paper, and requesting impossible additions for the
  average lifetime of an Earthling</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the ByoRISC (Build your own RISC) configurable
application-specific instruction-set processor (ASIP) family is presented.
ByoRISCs, as vendor-independent cores, provide extensive architectural
parameters over a baseline processor, which can be customized by
application-specific hardware extensions (ASHEs). Such extensions realize
multi-input multi-output (MIMO) custom instructions with local state and
load/store accesses to the data memory. ByoRISCs incorporate a true multi-port
register file, zero-overhead custom instruction decoding, and scalable data
forwarding mechanisms. Given these design decisions, ByoRISCs provide a unique
combination of features that allow their use as architectural testbeds and the
seamless and rapid development of new high-performance ASIPs.
  The performance characteristics of ByoRISCs, implemented as
vendor-independent cores, have been evaluated for both ASIC and FPGA
implementations, and it is proved that they provide a viable solution in
FPGA-based system-on-a-chip design. A case study of an image processing
pipeline is also presented to highlight the process of utilizing a ByoRISC
custom processor. A peak performance speedup of up to 8.5$\times$ can be
observed, whereas an average performance speedup of 4.4$\times$ on Xilinx
Virtex-4 targets is achieved. In addition, ByoRISC outperforms an experimental
VLIW architecture named VEX even in its 16-wide configuration for a number of
data-intensive application kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6636</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6636</id><created>2014-03-26</created><authors><author><keyname>Curiel</keyname><forenames>Arturo</forenames></author><author><keyname>Collet</keyname><forenames>Christophe</forenames></author></authors><title>Sign Language Lexical Recognition With Propositional Dynamic Logic</title><categories>cs.CL</categories><comments>6 pages, Proceedings of the 51st Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers), August, 2013</comments><acm-class>I.2.7</acm-class><journal-ref>In Proceedings of the 51st Annual Meeting of the Association for
  Computational Linguistics, vol. 2, pp. 328-333. 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper explores the use of Propositional Dynamic Logic (PDL) as a
suitable formal framework for describing Sign Language (SL), the language of
deaf people, in the context of natural language processing. SLs are visual,
complete, standalone languages which are just as expressive as oral languages.
Signs in SL usually correspond to sequences of highly specific body postures
interleaved with movements, which make reference to real world objects,
characters or situations. Here we propose a formal representation of SL signs,
that will help us with the analysis of automatically-collected hand tracking
data from French Sign Language (FSL) video corpora. We further show how such a
representation could help us with the design of computer aided SL verification
tools, which in turn would bring us closer to the development of an automatic
recognition system for these languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6637</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6637</id><created>2014-03-26</created><updated>2014-10-06</updated><authors><author><keyname>Korman</keyname><forenames>Simon</forenames></author><author><keyname>Litman</keyname><forenames>Roee</forenames></author><author><keyname>Avidan</keyname><forenames>Shai</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex</forenames></author></authors><title>Probably Approximately Symmetric: Fast rigid Symmetry Detection with
  Global Guarantees</title><categories>cs.CG</categories><msc-class>65D18, 68U05</msc-class><acm-class>I.3.5</acm-class><doi>10.1111/cgf.12454</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast algorithm for global rigid symmetry detection with
approximation guarantees. The algorithm is guaranteed to find the best
approximate symmetry of a given shape, to within a user-specified threshold,
with very high probability. Our method uses a carefully designed sampling of
the transformation space, where each transformation is efficiently evaluated
using a sub-linear algorithm. We prove that the density of the sampling depends
on the total variation of the shape, allowing us to derive formal bounds on the
algorithm's complexity and approximation quality. We further investigate
different volumetric shape representations (in the form of truncated distance
transforms), and in such a way control the total variation of the shape and
hence the sampling density and the runtime of the algorithm. A comprehensive
set of experiments assesses the proposed method, including an evaluation on the
eight categories of the COSEG data-set. This is the first large-scale
evaluation of any symmetry detection technique that we are aware of.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6644</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6644</id><created>2014-03-26</created><authors><author><keyname>Casey</keyname><forenames>C. Jasson</forenames></author><author><keyname>Sutton</keyname><forenames>Andrew</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author></authors><title>tinyNBI: Distilling an API from essential OpenFlow abstractions</title><categories>cs.NI cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If simplicity is a key strategy for success as a network protocol OpenFlow is
not winning. At its core OpenFlow presents a simple idea, which is a network
switch data plane abstraction along with a control protocol for manipulating
that abstraction. The result of this idea has been far from simple: a new
version released each year, five active versions, com- plex feature
dependencies, unstable version negotiation, lack of state machine definition,
etc. This complexity represents roadblocks for network, software, and hardware
engineers.
  We have distilled the core abstractions present in 5 existing versions of
OpenFlow and refactored them into a simple API called tinyNBI. Our work does
not provide high-level network abstractions (address pools, VPN maps, etc.),
instead it focuses on providing a clean low level interface that supports the
development of these higher layer abstractions. The goal of tinyNBI is to allow
configuration of all existing OpenFlow abstractions without having to deal with
the unique personalities of each version of OpenFlow or their level of support
in target switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6652</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6652</id><created>2014-03-26</created><updated>2014-06-27</updated><authors><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Al-Rfou</keyname><forenames>Rami</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>DeepWalk: Online Learning of Social Representations</title><categories>cs.SI cs.LG</categories><comments>10 pages, 5 figures, 4 tables</comments><acm-class>H.2.8; I.2.6; I.5.1</acm-class><doi>10.1145/2623330.2623732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present DeepWalk, a novel approach for learning latent representations of
vertices in a network. These latent representations encode social relations in
a continuous vector space, which is easily exploited by statistical models.
DeepWalk generalizes recent advancements in language modeling and unsupervised
feature learning (or deep learning) from sequences of words to graphs. DeepWalk
uses local information obtained from truncated random walks to learn latent
representations by treating walks as the equivalent of sentences. We
demonstrate DeepWalk's latent representations on several multi-label network
classification tasks for social networks such as BlogCatalog, Flickr, and
YouTube. Our results show that DeepWalk outperforms challenging baselines which
are allowed a global view of the network, especially in the presence of missing
information. DeepWalk's representations can provide $F_1$ scores up to 10%
higher than competing methods when labeled data is sparse. In some experiments,
DeepWalk's representations are able to outperform all baseline methods while
using 60% less training data. DeepWalk is also scalable. It is an online
learning algorithm which builds useful incremental results, and is trivially
parallelizable. These qualities make it suitable for a broad class of real
world applications such as network classification, and anomaly detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6656</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6656</id><created>2014-03-26</created><authors><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Gray</keyname><forenames>Norman</forenames></author><author><keyname>Erdmann</keyname><forenames>Chris</forenames></author><author><keyname>Biemesderfer</keyname><forenames>Chris</forenames></author><author><keyname>Frey</keyname><forenames>Katie</forenames></author><author><keyname>Soles</keyname><forenames>Justin</forenames></author></authors><title>The Unified Astronomy Thesaurus</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, 1 figure, to appear in Proceedings of Astronomical Data
  Analysis Software and Systems XXIII, which took place September 29-October 3,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Astronomy Thesaurus (UAT) is an open, interoperable and
community-supported thesaurus which unifies the existing divergent and isolated
Astronomy &amp; Astrophysics vocabularies into a single high-quality,
freely-available open thesaurus formalizing astronomical concepts and their
inter-relationships. The UAT builds upon the existing IAU Thesaurus with major
contributions from the astronomy portions of the thesauri developed by the
Institute of Physics Publishing, the American Institute of Physics, and SPIE.
We describe the effort behind the creation of the UAT and the process through
which we plan to maintain the document updated through broad community
participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6658</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6658</id><created>2014-03-17</created><authors><author><keyname>Chionis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chroni</keyname><forenames>Maria</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Stavros D.</forenames></author></authors><title>WaterRPG: A Graph-based Dynamic Watermarking Model for Software
  Protection</title><categories>cs.MM cs.CR</categories><comments>27 pages, 11 pages</comments><acm-class>D.2.2; D.4.6; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software watermarking involves embedding a unique identifier or,
equivalently, a watermark value within a software to prove owner's authenticity
and thus to prevent or discourage copyright infringement. Towards the embedding
process, several graph theoretic watermarking algorithmic techniques encode the
watermark values as graph structures and embed them in application programs.
Recently, we presented an efficient codec system for encoding a watermark
number $w$ as a reducible permutation graph $F[\pi^*]$ through the use of
self-inverting permutations $\pi^*$. In this paper, we propose a dynamic
watermarking model, which we call WaterRPG, for embedding the watermark graph
$F[\pi^*]$ into an application program $P$. The main idea behind the proposed
watermarking model is a systematic use of appropriate calls of specific
functions of the program $P$. More precisely, for a specific input $I_{key}$ of
the program $P$, our model takes the dynamic call-graph $G(P, I_{key})$ of $P$
and the watermark graph $F[\pi^*]$, and produces the watermarked program $P^*$
having the following key property: its dynamic call-graph $G(P^*, I_{key})$ is
isomorphic to the watermark graph $F[\pi^*]$. Within this idea the program
$P^*$ is produced by only altering appropriate calls of specific functions of
the input application program $P$. We have implemented our watermarking model
WaterRPG in real application programs and evaluated its functionality under
various and broadly used watermarking assessment criteria. The evaluation
results show that our model efficiently watermarks Java application programs
with respect to several watermarking metrics like data-rate, bytecode
instructions overhead, resiliency, time and space efficiency. Moreover, the
embedded watermarks withstand several software obfuscation and optimization
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6661</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6661</id><created>2014-03-26</created><authors><author><keyname>Simon</keyname><forenames>Francois</forenames></author></authors><title>One-sided asymptotically mean stationary channels</title><categories>cs.IT math.IT</categories><msc-class>94A40, 37A05</msc-class><journal-ref>Advances in applied mathematics, volume 50, issue 5, May 2013,
  pages 675-701</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an analysis of asymptotically mean stationary (AMS)
communication channels. A hierarchy based on stability properties
(stationarity, quasi-stationarity, recurrence and asymptotically mean
stationarity) of channels is identified. Stationary channels are a subclass of
quasi-stationary channels which are a subclass of recurrent AMS channels which
are a subclass of AMS channels. These classes are proved to be stable under
Markovian composition of channels (e.g., the cascade of AMS channels is an AMS
channel). Characterizations of channels of each class are given. Some
properties of the quasi-stationary mean of a channel are established. Finally,
ergodicity conditions of AMS channels are gathered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6669</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6669</id><created>2014-03-26</created><authors><author><keyname>Konomi</keyname><forenames>Shin'ichi</forenames></author></authors><title>Lost Again in Shibuya: Exploration and Awareness in a Labyrinth</title><categories>cs.HC</categories><comments>4 pages, 1 figure</comments><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing digital technologies in urban settings tend to focus narrowly on
concerns around wayfinding, safety, and consumption. In this paper, we examine
pedestrian experiences based on the data collected through field observations
as well as intensive interviews with nine pedestrians in the Shibuya area of
Tokyo, and suggest an alternative approach to blending technologies and urban
activities. Our focus is on social and cognitive aspects of pedestrians who get
lost and explore a labyrinth of sidewalks. We use the data to discuss the
activities that are often ignored or inadequately supported by existing
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6676</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6676</id><created>2014-03-26</created><authors><author><keyname>Decker</keyname><forenames>Christian</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Bitcoin Transaction Malleability and MtGox</title><categories>cs.CR cs.CE</categories><doi>10.1007/978-3-319-11212-1_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Bitcoin, transaction malleability describes the fact that the signatures
that prove the ownership of bitcoins being transferred in a transaction do not
provide any integrity guarantee for the signatures themselves. This allows an
attacker to mount a malleability attack in which it intercepts, modifies, and
rebroadcasts a transaction, causing the transaction issuer to believe that the
original transaction was not confirmed. In February 2014 MtGox, once the
largest Bitcoin exchange, closed and filed for bankruptcy claiming that
attackers used malleability attacks to drain its accounts. In this work we use
traces of the Bitcoin network for over a year preceding the filing to show
that, while the problem is real, there was no widespread use of malleability
attacks before the closure of MtGox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6678</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6678</id><created>2014-03-20</created><authors><author><keyname>Andrei</keyname><forenames>Oana</forenames></author><author><keyname>Calder</keyname><forenames>Muffy</forenames></author><author><keyname>Higgs</keyname><forenames>Matthew</forenames></author><author><keyname>Girolami</keyname><forenames>Mark</forenames></author></authors><title>Probabilistic Model Checking of DTMC Models of User Activity Patterns</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software developers cannot always anticipate how users will actually use
their software as it may vary from user to user, and even from use to use for
an individual user. In order to address questions raised by system developers
and evaluators about software usage, we define new probabilistic models that
characterise user behaviour, based on activity patterns inferred from actual
logged user traces. We encode these new models in a probabilistic model checker
and use probabilistic temporal logics to gain insight into software usage. We
motivate and illustrate our approach by application to the logged user traces
of an iOS app.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6689</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6689</id><created>2014-03-26</created><authors><author><keyname>Harrison</keyname><forenames>Amelia</forenames></author><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>On Equivalence of Infinitary Formulas under the Stable Model Semantics</title><categories>cs.LO</categories><doi>10.1017/S1471068414000088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propositional formulas that are equivalent in intuitionistic logic, or in its
extension known as the logic of here-and-there, have the same stable models. We
extend this theorem to propositional formulas with infinitely long conjunctions
and disjunctions and show how to apply this generalization to proving
properties of aggregates in answer set programming. To appear in Theory and
Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6697</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6697</id><created>2014-03-24</created><authors><author><keyname>Ravindra</keyname><forenames>S</forenames></author><author><keyname>Jagadeesha</keyname><forenames>S N</forenames></author></authors><title>Time Of Arrival Based Localization in Wireless Sensor Networks : A
  Linear Approach</title><categories>cs.NI</categories><comments>18 pages, 5 Figures, Signal &amp; Image Processing : An International
  Journal (SIPIJ)</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.4, August 2013</journal-ref><doi>10.5121/sipij.2013.4402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we aim to determine the location information of a node
deployed in Wireless Sensor Networks (WSN). We estimate the position of an
unknown source node using localization based on linear approach on a single
simulation platform. The Cramer Rao Lower Bound (CRLB) for position estimate is
derived first and the four linear approaches namely Linear Least Squares (LLS),
Subspace Approach (SA), Weighted Linear Least Squares (WLLS) and Two-step WLS
have been derived and presented. Based on the simulation study the results have
been compared. The simulation results show that the Two- step WLS approach is
having higher localization accuracy.
  Keywords: Source Localization, Time of Arrival,Linear Least Squares, Subspace
Approach, Weighted Linear Least Squares, Two-step WLS and CRLB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6703</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6703</id><created>2014-03-26</created><updated>2014-06-18</updated><authors><author><keyname>Fang</keyname><forenames>Zhaoxi</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>Towards the Asymptotic Sum Capacity of the MIMO Cellular Two-Way Relay
  Channel</title><categories>cs.IT math.IT</categories><comments>submitted to TSP. Revised April 2014</comments><doi>10.1109/TSP.2014.2332971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the transceiver and relay design for
multiple-input multiple-output (MIMO) cellular two-way relay channel (cTWRC),
where a multi-antenna base station (BS) exchanges information with multiple
multi-antenna mobile stations via a multi-antenna relay station (RS). We
propose a novel two-way relaying scheme to approach the sum capacity of the
MIMO cTWRC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6706</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6706</id><created>2014-03-26</created><authors><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author></authors><title>Beyond L2-Loss Functions for Learning Sparse Models</title><categories>stat.ML cs.CV cs.LG math.OC</categories><comments>10 pages, 6 figures</comments><acm-class>I.2.6; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating sparsity priors in learning tasks can give rise to simple, and
interpretable models for complex high dimensional data. Sparse models have
found widespread use in structure discovery, recovering data from corruptions,
and a variety of large scale unsupervised and supervised learning problems.
Assuming the availability of sufficient data, these methods infer dictionaries
for sparse representations by optimizing for high-fidelity reconstruction. In
most scenarios, the reconstruction quality is measured using the squared
Euclidean distance, and efficient algorithms have been developed for both batch
and online learning cases. However, new application domains motivate looking
beyond conventional loss functions. For example, robust loss functions such as
$\ell_1$ and Huber are useful in learning outlier-resilient models, and the
quantile loss is beneficial in discovering structures that are the
representative of a particular quantile. These new applications motivate our
work in generalizing sparse learning to a broad class of convex loss functions.
In particular, we consider the class of piecewise linear quadratic (PLQ) cost
functions that includes Huber, as well as $\ell_1$, quantile, Vapnik, hinge
loss, and smoothed variants of these penalties. We propose an algorithm to
learn dictionaries and obtain sparse codes when the data reconstruction
fidelity is measured using any smooth PLQ cost function. We provide convergence
guarantees for the proposed algorithm, and demonstrate the convergence behavior
using empirical experiments. Furthermore, we present three case studies that
require the use of PLQ cost functions: (i) robust image modeling, (ii) tag
refinement for image annotation and retrieval and (iii) computing empirical
confidence limits for subspace clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6710</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6710</id><created>2014-03-26</created><updated>2015-08-05</updated><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>The matching polytope does not admit fully-polynomial size relaxation
  schemes</title><categories>cs.CC math.CO</categories><comments>21 pages, 3 figures</comments><msc-class>52B12, 68Q17, 05C70, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The groundbreaking work of Rothvo{\ss} [arxiv:1311.2369] established that
every linear program expressing the matching polytope has an exponential number
of inequalities (formally, the matching polytope has exponential extension
complexity). We generalize this result by deriving strong bounds on the
polyhedral inapproximability of the matching polytope: for fixed $0 &lt;
\varepsilon &lt; 1$, every polyhedral $(1 + \varepsilon / n)$-approximation
requires an exponential number of inequalities, where $n$ is the number of
vertices. This is sharp given the well-known $\rho$-approximation of size
$O(\binom{n}{\rho/(\rho-1)})$ provided by the odd-sets of size up to
$\rho/(\rho-1)$. Thus matching is the first problem in $P$, whose natural
linear encoding does not admit a fully polynomial-size relaxation scheme (the
polyhedral equivalent of an FPTAS), which provides a sharp separation from the
polynomial-size relaxation scheme obtained e.g., via constant-sized odd-sets
mentioned above.
  Our approach reuses ideas from Rothvo{\ss} [arxiv:1311.2369], however the
main lower bounding technique is different. While the original proof is based
on the hyperplane separation bound (also called the rectangle corruption
bound), we employ the information-theoretic notion of common information as
introduced in Braun and Pokutta [http://eccc.hpi-web.de/report/2013/056/],
which allows to analyze perturbations of slack matrices. It turns out that the
high extension complexity for the matching polytope stem from the same source
of hardness as for the correlation polytope: a direct sum structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6713</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6713</id><created>2014-03-26</created><authors><author><keyname>O'Brien</keyname><forenames>Gear&#xf3;id</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Compensating Demand Response Participants Via Their Shapley Values</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing fair compensation mechanisms for demand response (DR) is
challenging. This paper models the problem in a game theoretic setting and
designs a payment distribution mechanism based on the Shapley Value. As exact
computation of the Shapley Value is in general intractable, we propose
estimating it using a reinforcement learning algorithm that approximates
optimal stratified sampling. We apply this algorithm to two DR programs that
utilize the Shapley Value for payments and quantify the accuracy of the
resulting estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6717</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6717</id><created>2014-03-26</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Smooth Entropy Transfer of Quantum Gravity Information Processing</title><categories>quant-ph cs.IT gr-qc hep-th math.IT</categories><comments>29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the term smooth entanglement entropy transfer, a phenomenon that
is a consequence of the causality-cancellation property of the quantum gravity
environment. The causality-cancellation of the quantum gravity space removes
the causal dependencies of the local systems. We study the physical effects of
the causality-cancellation and show that it stimulates entropy transfer between
the quantum gravity environment and the independent local systems of the
quantum gravity space. The entropy transfer reduces the entropies of the
contributing local systems and increases the entropy of the quantum gravity
environment. We discuss the space-time geometry structure of the quantum
gravity environment and the local quantum systems. We propose the space-time
geometry model of the smooth entropy transfer. We reveal on a smooth Cauchy
slice that the space-time geometry of the quantum gravity environment
dynamically adapts to the vanishing causality. We define the corresponding
Hamiltonians and the causal development of the quantum gravity environment in a
non-fixed causality structure. We prove that the Cauchy area expansion, along
with the dilation of the Rindler horizon area of the quantum gravity
environment, is a strict corollary of the causality-cancellation of the quantum
gravity environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6719</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6719</id><created>2014-03-26</created><authors><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Mata</keyname><forenames>Gadea</forenames></author><author><keyname>Rubio</keyname><forenames>Miguel Morales y Julio</forenames></author></authors><title>Procesamiento topo-geom\'etrico de im\'agenes neuronales</title><categories>cs.SC</categories><journal-ref>La Gaceta de la RSME Vol. 17 (2014), Num. 1, Pags. 109-128</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fruit of the relationship of our research group with the team coordinated by
the biologist Miguel Morales (http://spineup.es), we have applied different
topo-geometric techniques for neuronal image processing. The images, captured
with a powerful confocal microscope, allow to study the evolution of synaptic
density under the influence of various substances, with the aim of studying
neurodegenerative diseases like Alzheimer.
  In the paper we make a brief review of the techniques that appear in our
bioinformatic problems, including the calculation of ordinary and persistent
homology (for which one can use the program Kenzo for symbolic computation in
algebraic topology ) and classical problems of digital topology as skeleton
location and path tracking. We focus on some particular cases of recent
application, with which we will illustrate the previous techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6741</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6741</id><created>2014-03-26</created><authors><author><keyname>Zanko</keyname><forenames>Avi</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author><author><keyname>Zehavi</keyname><forenames>Ephraim</forenames></author></authors><title>Network coding for multicasting over Rayleigh fading multi access
  channels</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the problem of rate allocation for multicasting over slow
Rayleigh fading channels using network coding. In the proposed model, the
network is treated as a collection of Rayleigh fading multiple access channels.
In this model, rate allocation scheme that is based solely on the statistics of
the channels is presented. The rate allocation scheme is aimed at minimizing
the outage probability. An upper bound is presented for the probability of
outage in the fading multiple access channel. A suboptimal solution based on
this bound is given. A distributed primal-dual gradient algorithm is derived to
solve the rate allocation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6758</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6758</id><created>2014-03-26</created><authors><author><keyname>Eisenstat</keyname><forenames>David</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Schabanel</keyname><forenames>Nicolas</forenames></author></authors><title>Facility Location in Evolving Metrics</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the dynamics of evolving social or infrastructure networks is a
challenge in applied areas such as epidemiology, viral marketing, or urban
planning. During the past decade, data has been collected on such networks but
has yet to be fully analyzed. We propose to use information on the dynamics of
the data to find stable partitions of the network into groups. For that
purpose, we introduce a time-dependent, dynamic version of the facility
location problem, that includes a switching cost when a client's assignment
changes from one facility to another. This might provide a better
representation of an evolving network, emphasizing the abrupt change of
relationships between subjects rather than the continuous evolution of the
underlying network. We show that in realistic examples this model yields indeed
better fitting solutions than optimizing every snapshot independently. We
present an $O(\log nT)$-approximation algorithm and a matching hardness result,
where $n$ is the number of clients and $T$ the number of time steps. We also
give an other algorithms with approximation ratio $O(\log nT)$ for the variant
where one pays at each time step (leasing) for each open facility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6774</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6774</id><created>2014-03-26</created><authors><author><keyname>Berkels</keyname><forenames>Benjamin</forenames></author><author><keyname>Binev</keyname><forenames>Peter</forenames></author><author><keyname>Blom</keyname><forenames>Douglas A.</forenames></author><author><keyname>Dahmen</keyname><forenames>Wolfgang</forenames></author><author><keyname>Sharpley</keyname><forenames>Robert C.</forenames></author><author><keyname>Vogt</keyname><forenames>Thomas</forenames></author></authors><title>Optimized imaging using non-rigid registration</title><categories>cs.CV</categories><journal-ref>Ultramicroscopy 13 (2014) 46-56</journal-ref><doi>10.1016/j.ultramic.2013.11.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extraordinary improvements of modern imaging devices offer access to data
with unprecedented information content. However, widely used image processing
methodologies fall far short of exploiting the full breadth of information
offered by numerous types of scanning probe, optical, and electron
microscopies. In many applications, it is necessary to keep measurement
intensities below a desired threshold. We propose a methodology for extracting
an increased level of information by processing a series of data sets
suffering, in particular, from high degree of spatial uncertainty caused by
complex multiscale motion during the acquisition process. An important role is
played by a nonrigid pixel-wise registration method that can cope with low
signal-to-noise ratios. This is accompanied by formulating objective quality
measures which replace human intervention and visual inspection in the
processing chain. Scanning transmission electron microscopy of siliceous
zeolite material exhibits the above-mentioned obstructions and therefore serves
as orientation and a test of our procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6794</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6794</id><created>2014-03-26</created><authors><author><keyname>G&#xf3;mez-Conde</keyname><forenames>Iv&#xe1;n</forenames></author><author><keyname>Olivieri</keyname><forenames>David N.</forenames></author></authors><title>KPCA Spatio-temporal trajectory point cloud classifier for recognizing
  human actions in a CBVR system</title><categories>cs.IR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a content based video retrieval (CBVR) software system for
identifying specific locations of a human action within a full length film, and
retrieving similar video shots from a query. For this, we introduce the concept
of a trajectory point cloud for classifying unique actions, encoded in a
spatio-temporal covariant eigenspace, where each point is characterized by its
spatial location, local Frenet-Serret vector basis, time averaged curvature and
torsion and the mean osculating hyperplane. Since each action can be
distinguished by their unique trajectories within this space, the trajectory
point cloud is used to define an adaptive distance metric for classifying
queries against stored actions. Depending upon the distance to other
trajectories, the distance metric uses either large scale structure of the
trajectory point cloud, such as the mean distance between cloud centroids or
the difference in hyperplane orientation, or small structure such as the time
averaged curvature and torsion, to classify individual points in a fuzzy-KNN.
Our system can function in real-time and has an accuracy greater than 93% for
multiple action recognition within video repositories. We demonstrate the use
of our CBVR system in two situations: by locating specific frame positions of
trained actions in two full featured films, and video shot retrieval from a
database with a web search application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6807</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6807</id><created>2014-03-26</created><updated>2015-11-13</updated><authors><author><keyname>Nadendla</keyname><forenames>V. Sriram Siddhardh</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Optimal Spectrum Auction Design with Two-Dimensional Truthful
  Revelations under Uncertain Spectrum Availability</title><categories>cs.NI cs.GT cs.IT cs.SY math.IT</categories><comments>14 double-column pages, 7 figures, 2 tables, Under review in IEEE/ACM
  Transactions in Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel sealed-bid auction framework to address the
problem of dynamic spectrum allocation in cognitive radio (CR) networks. We
design an optimal auction mechanism that maximizes the moderator's expected
utility, when the spectrum is not available with certainty. We assume that the
moderator employs collaborative spectrum sensing in order to make a reliable
inference about spectrum availability. Due to the presence of a collision cost
whenever the moderator makes an erroneous inference, and a sensing cost at each
CR, we investigate feasibility conditions that guarantee a non-negative utility
at the moderator. We present tight theoretical-bounds on instantaneous network
throughput and also show that our algorithm provides maximum throughput if the
CRs have i.i.d. valuations. Since the moderator fuses CRs' sensing decisions to
obtain a global inference regarding spectrum availability, we propose a novel
strategy-proof fusion rule that encourages the CRs to simultaneously reveal
truthful sensing decisions, along with truthful valuations to the moderator.
Numerical examples are also presented to provide insights into the performance
of the proposed auction under different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6822</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6822</id><created>2014-03-26</created><authors><author><keyname>Lin</keyname><forenames>Xiaomin</forenames></author><author><keyname>Beling</keyname><forenames>Peter A.</forenames></author><author><keyname>Cogill</keyname><forenames>Randy</forenames></author></authors><title>Comparison of Multi-agent and Single-agent Inverse Learning on a
  Simulated Soccer Example</title><categories>cs.LG cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1403.6508</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the performance of Inverse Reinforcement Learning (IRL) with the
relative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Before
comparing the methods, we extend a published Bayesian IRL approach that is only
applicable to the case where the reward is only state dependent to a general
one capable of tackling the case where the reward depends on both state and
action. Comparison between IRL and MIRL is made in the context of an abstract
soccer game, using both a game model in which the reward depends only on state
and one in which it depends on both state and action. Results suggest that the
IRL approach performs much worse than the MIRL approach. We speculate that the
underperformance of IRL is because it fails to capture equilibrium information
in the manner possible in MIRL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6823</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6823</id><created>2014-03-26</created><authors><author><keyname>Schubert</keyname><forenames>Theresa</forenames></author></authors><title>ChromaPhy - A Living Wearable Connecting Humans and Their Environment</title><categories>cs.HC</categories><comments>Extended abstracts (3 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research presents an artistic project aiming to make cyberfiction become
reality and exemplifying a current trend in art and science collaborations.
Chroma+Phy is a speculative design for a living wearable that combines the
protoplasmic structure of the amoeboid acellular organism Physarum polycephalum
and the chromatophores of the reptile Chameleon. The underpin-ning idea is that
in a future far away or close, on planet earth or in outer space, humans will
need some tools to help them in their social life and day-to-day routine.
Chroma+Phy enhances the body aiming at humans in extreme habitats for an
aggression-free and healthy life. Our approach will address actual issues of
scientific discovery for society and catalyse idea translation through art and
design experiments at frontiers of science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6838</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6838</id><created>2014-03-26</created><authors><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Gummadi</keyname><forenames>Krishna</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Quantifying Information Overload in Social Media and its Impact on
  Social Contagions</title><categories>cs.SI physics.soc-ph</categories><comments>To appear at ICSWM '14</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information overload has become an ubiquitous problem in modern society.
Social media users and microbloggers receive an endless flow of information,
often at a rate far higher than their cognitive abilities to process the
information. In this paper, we conduct a large scale quantitative study of
information overload and evaluate its impact on information dissemination in
the Twitter social media site. We model social media users as information
processing systems that queue incoming information according to some policies,
process information from the queue at some unknown rates and decide to forward
some of the incoming information to other users. We show how timestamped data
about tweets received and forwarded by users can be used to uncover key
properties of their queueing policies and estimate their information processing
rates and limits. Such an understanding of users' information processing
behaviors allows us to infer whether and to what extent users suffer from
information overload.
  Our analysis provides empirical evidence of information processing limits for
social media users and the prevalence of information overloading. The most
active and popular social media users are often the ones that are overloaded.
Moreover, we find that the rate at which users receive information impacts
their processing behavior, including how they prioritize information from
different sources, how much information they process, and how quickly they
process information. Finally, the susceptibility of a social media user to
social contagions depends crucially on the rate at which she receives
information. An exposure to a piece of information, be it an idea, a convention
or a product, is much less effective for users that receive information at
higher rates, meaning they need more exposures to adopt a particular contagion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6863</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6863</id><created>2014-03-26</created><authors><author><keyname>Veness</keyname><forenames>Joel</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Online Learning of k-CNF Boolean Functions</title><categories>cs.LG</categories><comments>20 LaTeX pages. 2 Algorithms. Some Theorems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper revisits the problem of learning a k-CNF Boolean function from
examples in the context of online learning under the logarithmic loss. In doing
so, we give a Bayesian interpretation to one of Valiant's celebrated PAC
learning algorithms, which we then build upon to derive two efficient, online,
probabilistic, supervised learning algorithms for predicting the output of an
unknown k-CNF Boolean function. We analyze the loss of our methods, and show
that the cumulative log-loss can be upper bounded, ignoring logarithmic
factors, by a polynomial function of the size of each example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6865</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6865</id><created>2014-03-26</created><authors><author><keyname>Governatori</keyname><forenames>Guido</forenames></author></authors><title>ICT Support for Regulatory Compliance of Business Processes</title><categories>cs.SE cs.CY</categories><journal-ref>29th WORLD CONTINUOUS AUDITING AND REPORTING SYMPOSIUM (29WCARS),
  NOVEMBER 21-22, 2013, BRISBANE, AUSTRALIA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an ITC (Information and Communication Technology)
approach to support regulatory compliance for business processes, and we report
on the development and evaluation of a business process compliance checker
called Regorous, based on the compliance-by-design methodology proposed by
Governatori and Sadiq
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6870</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6870</id><created>2014-03-26</created><updated>2014-04-21</updated><authors><author><keyname>McFarland</keyname><forenames>Christopher D</forenames></author></authors><title>A modified ziggurat algorithm for generating exponentially- and
  normally-distributed pseudorandom numbers</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ziggurat Algorithm is a very fast rejection sampling method for
generating PseudoRandom Numbers (PRNs) from common statistical distributions.
The algorithm divides a distribution into rectangular layers that stack on top
of each other (resembling a Ziggurat), subsuming the desired distribution.
Random values within these rectangular layers are then sampled by rejection.
This implementation splits layers into two types: those constituting the
majority that fall completely under the distribution and can be sampled
extremely fast without a rejection test, and a few additional layers that
encapsulate the fringe of the distribution and require a rejection test. This
method offers speedups of 65% for exponentially- and 82% for
normally-distributed PRNs when compared to the best available C implementations
of these generators. Even greater speedups are obtained when the algorithm is
extended to the Python and MATLAB/OCTAVE programming environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6888</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6888</id><created>2014-03-26</created><updated>2015-01-20</updated><authors><author><keyname>Marku&#x161;</keyname><forenames>Nenad</forenames></author><author><keyname>Frljak</keyname><forenames>Miroslav</forenames></author><author><keyname>Pand&#x17e;i&#x107;</keyname><forenames>Igor S.</forenames></author><author><keyname>Ahlberg</keyname><forenames>J&#xf6;rgen</forenames></author><author><keyname>Forchheimer</keyname><forenames>Robert</forenames></author></authors><title>Fast Localization of Facial Landmark Points</title><categories>cs.CV</categories><journal-ref>Proceedings of the Croatian Compter Vision Workshop, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization of salient facial landmark points, such as eye corners or the
tip of the nose, is still considered a challenging computer vision problem
despite recent efforts. This is especially evident in unconstrained
environments, i.e., in the presence of background clutter and large head pose
variations. Most methods that achieve state-of-the-art accuracy are slow, and,
thus, have limited applications. We describe a method that can accurately
estimate the positions of relevant facial landmarks in real-time even on
hardware with limited processing power, such as mobile devices. This is
achieved with a sequence of estimators based on ensembles of regression trees.
The trees use simple pixel intensity comparisons in their internal nodes and
this makes them able to process image regions very fast. We test the developed
system on several publicly available datasets and analyse its processing speed
on various devices. Experimental results show that our method has practical
value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6901</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6901</id><created>2014-03-26</created><authors><author><keyname>Soni</keyname><forenames>Sapna</forenames></author><author><keyname>Imran</keyname><forenames>Ahmed</forenames></author><author><keyname>Kopparapu</keyname><forenames>Sunil Kumar</forenames></author></authors><title>Automatic Segmentation of Broadcast News Audio using Self Similarity
  Matrix</title><categories>cs.SD cs.LG cs.MM</categories><comments>4 pages, 5 images</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generally audio news broadcast on radio is com- posed of music, commercials,
news from correspondents and recorded statements in addition to the actual news
read by the newsreader. When news transcripts are available, automatic
segmentation of audio news broadcast to time align the audio with the text
transcription to build frugal speech corpora is essential. We address the
problem of identifying segmentation in the audio news broadcast corresponding
to the news read by the newsreader so that they can be mapped to the text
transcripts. The existing techniques produce sub-optimal solutions when used to
extract newsreader read segments. In this paper, we propose a new technique
which is able to identify the acoustic change points reliably using an acoustic
Self Similarity Matrix (SSM). We describe the two pass technique in detail and
verify its performance on real audio news broadcast of All India Radio for
different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6918</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6918</id><created>2014-03-27</created><authors><author><keyname>Katyal</keyname><forenames>Mayanka</forenames></author><author><keyname>Mishra</keyname><forenames>Atul</forenames></author></authors><title>A Comparative Study of Load Balancing Algorithms in Cloud Computing
  Environment</title><categories>cs.DC</categories><comments>14 pages,International Journal of Distributed and Cloud
  Computing,PublishingIndia.com,
  http://www.publishingindia.com/IJDCC/68/a-comparative-study-of-load-balancing-algorithms-in-cloud-computing-environment/277/2041/</comments><journal-ref>International Journal of Distributed and Cloud Computing, Volume 1
  Issue 2 December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing is a new trend emerging in IT environment with huge
requirements of infrastructure and resources. Load Balancing is an important
aspect of cloud computing environment. Efficient load balancing scheme ensures
efficient resource utilization by provisioning of resources to cloud users on
demand basis in pay as you say manner. Load Balancing may even support
prioritizing users by applying appropriate scheduling criteria. This paper
presents various load balancing schemes in different cloud environment based on
requirements specified in Service Level Agreement (SLA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6919</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6919</id><created>2014-03-27</created><authors><author><keyname>R</keyname><forenames>Harikrishnan.</forenames></author><author><keyname>Hammed</keyname><forenames>Shajna S.</forenames></author><author><keyname>Malini</keyname><forenames>P.</forenames></author></authors><title>Marine Buoy Location Finding and Tracking System for Linux Supporting
  Mobiles</title><categories>cs.OH</categories><comments>3pages, 5figures</comments><journal-ref>International Journal of Engineering Trends and Technology Volume
  9 Number 8 - Mar 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Marine buoy is an important part of underwater acoustic communication system.
It is of great significance to track and locate it. It is widely used in ocean
environment three - dimensional monitoring, underwater multimedia
communication, underwater mobile carrier navigation and positioning, marine
resources detection, remote control of submarine topography mapping and
offshore oil industry, data acquisition, etc. This paper describes the
application of the monitoring service of GPRS / GPS module at Marine buoy. It
can achieve real - time location of underwater acoustic communication devices
and route tracking to avoid the loss of the device, as well as assist to
retrieve the lost device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6922</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6922</id><created>2014-03-27</created><updated>2014-10-23</updated><authors><author><keyname>Guntuboyina</keyname><forenames>Adityanand</forenames></author></authors><title>Covering numbers of $L_p$-balls of convex sets and functions</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove bounds for the covering numbers of classes of convex functions and
convex sets in Euclidean space. Previous results require the underlying convex
functions or sets to be uniformly bounded. We relax this assumption and replace
it with weaker integral constraints. Existing results can be recovered as
special cases of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6923</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6923</id><created>2014-03-27</created><authors><author><keyname>Hu</keyname><forenames>Yuan</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author></authors><title>Emergency Route Selection for D2D Cellular Communications During an
  Urban Terrorist Attack</title><categories>cs.NI</categories><comments>IEEE International Conference on Communications (ICC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-Device (D2D) communications is a technology that allows mobile
users to relay information to each other, without access to the cellular
network. In this paper, we consider how to dynamically select multi-hop routes
for D2D communications in spectrum co-existence with a fully loaded cellular
network. The modelling scenario is that of a real urban environment, when the
cellular network is congested during an unexpected event, such as a terrorist
attack. We use D2D to relay data across the urban terrain, in the presence of
conventional cellular (CC) communications.
  We consider different wireless routing algorithms, namely:
shortest-path-routing (SPR), interference-aware-routing (IAR), and
broadcast-routing (BR). In general, there is a fundamental trade-off between
D2D and CC outage performances, due to their mutual interference relationship.
For different CC outage constraints and D2D end-to-end distances, the paper
recommends different D2D routing strategies. The paper also considers the
effects of varying user density and urban building material properties on
overall D2D relaying feasibility. Over a distance of a kilometre, it was found
that the success probability of D2D communications can reach 91% for a moderate
participating user density (400 per square km) and a low wall penetration loss
(&lt;10dB).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6924</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6924</id><created>2014-03-27</created><authors><author><keyname>Qiu</keyname><forenames>Song</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author></authors><title>A Molecular Communication Link for Monitoring in Confined Environments</title><categories>cs.ET</categories><comments>IEEE International Conference on Communications (ICC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a molecular diffusion based communications link
that can reliably transport data over-the-air. We show that the system can also
reliably transport data across confined structural environments, especially in
cases where conventional electromagnetic (EM) wave based systems may fail. In
particular, this paper compares the performance of our proprietary molecular
communication test-bed with Zigbee wireless sensors in a metal pipe network
that does not act as a radio wave-guide. The paper first shows that a
molecular-based communication link's performance is determined primarily by the
delay time spread of the pulse response. The paper go on to show that
molecular-based systems can transmit more reliably in complex and confined
structural environments than conventional EM-based systems. The paper then
utilizes empirical data to find relationships between the received radio signal
strength, the molecular pulse spread, data rate (0.1 bits/s) and the structural
propagation environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6929</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6929</id><created>2014-03-27</created><updated>2014-03-28</updated><authors><author><keyname>Adhikari</keyname><forenames>Satyabrata</forenames></author><author><keyname>Kumar</keyname><forenames>Atul</forenames></author></authors><title>Upper Bound on Singlet Fraction of Two Qubit Mixed Entangled States</title><categories>quant-ph cs.IT math.IT</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the possibility of achieving the maximum possible singlet
fraction using a entangled mixed two-qubit state as a resource. For this, we
establish a tight upper bound on singlet fraction and show that the maximal
singlet fraction obtained in \cite{Verstraete} does not attain the obtained
upper bound on the singlet fraction. Interestingly, we found that the required
upper bound can in fact be achieved using local filtering operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6931</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6931</id><created>2014-03-27</created><authors><author><keyname>Lee</keyname><forenames>Gilwon</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author></authors><title>A New Approach to User Scheduling in Massive Multi-User MIMO Broadcast
  Channels</title><categories>cs.IT math.IT</categories><comments>43 pages, 9 figures, submitted to IEEE Transactions on Information
  Theory</comments><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new user-scheduling-and-beamforming method is proposed for
multi-user massive multiple-input multiple-output (massive MIMO) broadcast
channels in the context of two-stage beamforming. The key ideas of the proposed
scheduling method are 1) to use a set of orthogonal reference beams and
construct a double cone around each reference beam to select `nearly-optimal'
semi-orthogonal users based only on channel quality indicator (CQI) feedback
and 2) to apply post-user-selection beam refinement with zero-forcing
beamforming (ZFBF) based on channel state information (CSI) feedback only from
the selected users. It is proved that the proposed scheduling-and-beamforming
method is asymptotically optimal as the number of users increases. Furthermore,
the proposed scheduling-and-beamforming method almost achieves the performance
of the existing semi-orthogonal user selection with ZFBF (SUS-ZFBF) that
requires full CSI feedback from all users, with significantly reduced feedback
overhead which is even less than that required by random beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6946</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6946</id><created>2014-03-27</created><authors><author><keyname>Walker</keyname><forenames>Josiah</forenames></author><author><keyname>Houliston</keyname><forenames>Trent</forenames></author><author><keyname>Annable</keyname><forenames>Brendan</forenames></author><author><keyname>Biddulph</keyname><forenames>Alex</forenames></author><author><keyname>Dabson</keyname><forenames>Andrew</forenames></author><author><keyname>Fountain</keyname><forenames>Jake</forenames></author><author><keyname>Johnson</keyname><forenames>Taylor</forenames></author><author><keyname>Johnson</keyname><forenames>Jordan</forenames></author><author><keyname>Metcalfe</keyname><forenames>Mitchell</forenames></author><author><keyname>Sugo</keyname><forenames>Anita</forenames></author><author><keyname>Chalup</keyname><forenames>Stephan K.</forenames></author><author><keyname>King</keyname><forenames>Robert A. R.</forenames></author><author><keyname>Mendes</keyname><forenames>Alexandre</forenames></author><author><keyname>Turner</keyname><forenames>Peter</forenames></author></authors><title>The NUbots Team Description Paper 2014</title><categories>cs.RO</categories><comments>RoboCup 2014 humanoid league team description paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NUbots team, from The University of Newcastle, Australia, has had a
strong record of success in the RoboCup Standard Platform League since first
entering in 2002. The team has also competed within the RoboCup Humanoid
Kid-Size League since 2012. The 2014 team brings a renewed focus on software
architecture, modularity, and the ability to easily share code. This paper
summarizes the history of the NUbots team, describes the roles and research of
the team members, gives an overview of the NUbots' robots and software system,
and addresses relevant research projects within the the Newcastle Robotics
Laboratory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6950</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6950</id><created>2014-03-27</created><authors><author><keyname>Castro</keyname><forenames>F. M.</forenames></author><author><keyname>Marin-Jimenez</keyname><forenames>M. J.</forenames></author><author><keyname>Medina-Carnicer</keyname><forenames>R.</forenames></author></authors><title>Pyramidal Fisher Motion for Multiview Gait Recognition</title><categories>cs.CV</categories><comments>Submitted to International Conference on Pattern Recognition, ICPR,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to identify individuals by analyzing their gait.
Instead of using binary silhouettes as input data (as done in many previous
works) we propose and evaluate the use of motion descriptors based on densely
sampled short-term trajectories. We take advantage of state-of-the-art people
detectors to define custom spatial configurations of the descriptors around the
target person. Thus, obtaining a pyramidal representation of the gait motion.
The local motion features (described by the Divergence-Curl-Shear descriptor)
extracted on the different spatial areas of the person are combined into a
single high-level gait descriptor by using the Fisher Vector encoding. The
proposed approach, coined Pyramidal Fisher Motion, is experimentally validated
on the recent `AVA Multiview Gait' dataset. The results show that this new
approach achieves promising results in the problem of gait recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6952</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6952</id><created>2014-03-27</created><authors><author><keyname>B&#xfc;rger</keyname><forenames>Mathias</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Optimal pricing control in distribution networks with time-varying
  supply and demand</title><categories>math.OC cs.SY</categories><comments>Submitted to 21st International Symposium on Mathematical Theory of
  Networks and Systems (MTNS) in December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of optimal flow control in dynamic inventory
systems. A dynamic optimal distribution problem, including time-varying supply
and demand, capacity constraints on the transportation lines, and convex flow
cost functions of Legendre-type, is formalized and solved. The time-varying
optimal flow is characterized in terms of the time-varying dual variables of a
corresponding network optimization problem. A dynamic feedback controller is
proposed that regulates the flows asymptotically to the optimal flows and
achieves in addition a balancing of all storage levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6953</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6953</id><created>2014-03-27</created><authors><author><keyname>Ebadat</keyname><forenames>A.</forenames></author><author><keyname>Wahlberg</keyname><forenames>B.</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H.</forenames></author><author><keyname>Rojas</keyname><forenames>C. R.</forenames></author><author><keyname>Hagg</keyname><forenames>P.</forenames></author><author><keyname>Larsson</keyname><forenames>C. A.</forenames></author></authors><title>Applications Oriented Input Design in Time-Domain Through Cyclic Methods</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method for applications oriented input design for
linear systems under time-domain constraints on the amplitude of input and
output signals. The method guarantees a desired control performance for the
estimated model in minimum time, by imposing some lower bound on the
information matrix. The problem is formulated as a time domain optimization
problem, which is non-convex. This is addressed through an alternating method,
where we separate the problem into two steps and at each step we optimize the
cost function with respect to one of two variables. We alternate between these
two steps until convergence. A time recursive input design algorithm is
performed, which enables us to use the algorithm with control. Therefore, a
receding horizon framework is used to solve each optimization problem. Finally,
we illustrate the method with two numerical examples which show the good
ability of the proposed approach in generating an optimal input signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6955</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6955</id><created>2014-03-27</created><authors><author><keyname>Bairwa</keyname><forenames>Sheetal</forenames></author><author><keyname>Mewara</keyname><forenames>Bhawna</forenames></author><author><keyname>Gajrani</keyname><forenames>Jyoti</forenames></author></authors><title>Vulnerability Scanners-A Proactive Approach To Assess Web Application
  Security</title><categories>cs.CR</categories><comments>12 pages, 6 figures, 3 tables, SCNDS 2013 Conference. International
  Journal on Computational Sciences &amp; Applications (IJCSA)2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing concern for security in the network, many approaches are
laid out that try to protect the network from unauthorised access. New methods
have been adopted in order to find the potential discrepancies that may damage
the network. Most commonly used approach is the vulnerability assessment. By
vulnerability, we mean, the potential flaws in the system that make it prone to
the attack. Assessment of these system vulnerabilities provide a means to
identify and develop new strategies so as to protect the system from the risk
of being damaged. This paper focuses on the usage of various vulnerability
scanners and their related methodology to detect the various vulnerabilities
available in the web applications or the remote host across the network and
tries to identify new mechanisms that can be deployed to secure the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6957</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6957</id><created>2014-03-27</created><authors><author><keyname>Schmidt</keyname><forenames>Gunther</forenames></author><author><keyname>Winter</keyname><forenames>Michael</forenames></author></authors><title>Relational Mathematics Continued</title><categories>cs.DM</categories><comments>Tech. Report, Universitaet der Bundeswehr Muenchen, 45 pages</comments><msc-class>06-02, 03G15, 68RXX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is in some sense an addendum to the book Relational Mathematics by the
first-named author. It originated from work on diverse other topics during
which a lot of purely relational results with broad applicability have been
produced. These include results on domain construction with novel formulae for
existential and inverse image, a relational calculus for binary mappings, and
the development of a formally derived relational calculus of Kronecker-, strict
fork-, and strict join-operators. The many visualizations in this report make
it also a scrap- and picture book for examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6958</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6958</id><created>2014-03-27</created><authors><author><keyname>Rousseau</keyname><forenames>S.</forenames></author><author><keyname>Helbert</keyname><forenames>D.</forenames></author><author><keyname>Carr&#xe9;</keyname><forenames>P.</forenames></author><author><keyname>Blanc-Talon</keyname><forenames>J.</forenames></author></authors><title>Compressive Pattern Matching on Multispectral Data</title><categories>cs.CV</categories><comments>Published in IEEE Transactions on Geoscience and Remote Sensing</comments><doi>10.1109/TGRS.2014.2314483</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new constrained minimization problem that performs template
and pattern detection on a multispectral image in a compressive sensing
context. We use an original minimization problem from Guo and Osher that uses
$L_1$ minimization techniques to perform template detection in a multispectral
image. We first adapt this minimization problem to work with compressive
sensing data. Then we extend it to perform pattern detection using a formal
transform called the spectralization along a pattern. That extension brings out
the problem of measurement reconstruction. We introduce shifted measurements
that allow us to reconstruct all the measurement with a small overhead and we
give an optimality constraint for simple patterns. We present numerical results
showing the performances of the original minimization problem and the
compressed ones with different measurement rates and applied on remotely sensed
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6968</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6968</id><created>2014-03-27</created><updated>2014-05-09</updated><authors><author><keyname>Nikolic</keyname><forenames>Milos</forenames></author><author><keyname>ElSeidy</keyname><forenames>Mohammed</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>LINVIEW: Incremental View Maintenance for Complex Analytical Queries</title><categories>cs.DB cs.NA</categories><comments>14 pages, SIGMOD</comments><acm-class>H.2.4; G.1.3</acm-class><doi>10.1145/2588555.2610519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many analytics tasks and machine learning problems can be naturally expressed
by iterative linear algebra programs. In this paper, we study the incremental
view maintenance problem for such complex analytical queries. We develop a
framework, called LINVIEW, for capturing deltas of linear algebra programs and
understanding their computational cost. Linear algebra operations tend to cause
an avalanche effect where even very local changes to the input matrices spread
out and infect all of the intermediate results and the final view, causing
incremental view maintenance to lose its performance benefit over
re-evaluation. We develop techniques based on matrix factorizations to contain
such epidemics of change. As a consequence, our techniques make incremental
view maintenance of linear algebra practical and usually substantially cheaper
than re-evaluation. We show, both analytically and experimentally, the
usefulness of these techniques when applied to standard analytics tasks. Our
evaluation demonstrates the efficiency of LINVIEW in generating parallel
incremental programs that outperform re-evaluation techniques by more than an
order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6974</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6974</id><created>2014-03-27</created><updated>2015-01-13</updated><authors><author><keyname>Sundman</keyname><forenames>Dennis</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Design and Analysis of a Greedy Pursuit for Distributed Compressed
  Sensing</title><categories>cs.IT math.IT</categories><comments>Sent to IEEE Transaction on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a distributed compressed sensing scenario where many sensors
measure correlated sparse signals and the sensors are connected through a
network. Correlation between sparse signals is modeled by a partial common
support-set. For such a scenario, the main objective of this paper is to
develop a greedy pursuit algorithm. We develop a distributed parallel pursuit
(DIPP) algorithm based on exchange of information about estimated support-sets
at sensors. The exchange of information helps to improve estimation of the
partial common support-set, that in turn helps to gradually improve estimation
of support-sets in all sensors, leading to a better quality reconstruction
performance. We provide restricted isometry property (RIP) based theoretical
analysis on the algorithm's convergence and reconstruction performance. Under
certain theoretical requirements on the quality of information exchange over
network and RIP parameters of sensor nodes, we show that the DIPP algorithm
converges to a performance level that depends on a scaled additive measurement
noise power (convergence in theory) where the scaling coefficient is a function
of RIP parameters and information processing quality parameters. Using
simulations, we show practical reconstruction performance of DIPP vis-a-vis
amount of undersampling, signal-to-measurement-noise ratios and
network-connectivity conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6977</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6977</id><created>2014-03-27</created><updated>2014-09-27</updated><authors><author><keyname>Deng</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Wenjie</forenames></author><author><keyname>Rui</keyname><forenames>Yun</forenames></author></authors><title>Utility Maximization for Uplink MU-MIMO: Combining Spectral-Energy
  Efficiency and Fairness</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by green communications, energy efficiency (EE) has become a new
important criterion for designing wireless communication systems. However, high
EE often leads to low spectral efficiency (SE), which spurs the research on
EE-SE tradeoff. In this paper, we focus on how to maximize the utility in
physical layer for an uplink multi-user multiple-input multiple-output
(MU-MIMO) system, where we will not only consider EE-SE tradeoff in a unified
way, but also ensure user fairness. We first formulate the utility maximization
problem, but it turns out to be non-convex. By exploiting the structure of this
problem, we find a convexization procedure to convert the original non-convex
problem into an equivalent convex problem, which has the same global optima
with the original problem. Following the convexization procedure, we present a
centralized algorithm to solve the utility maximization problem, but it
requires the global information of all users. Thus we propose a primal-dual
distributed algorithm which does not need global information and just consumes
a small amount of overhead. Furthermore, we have proved that the distributed
algorithm can converge to the global optima. Finally, the numeric results show
that our approach can both capture user diversity for EE-SE tradeoff and ensure
user fairness, and they also validate the effectiveness of our primal-dual
distributed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6982</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6982</id><created>2014-03-27</created><updated>2015-01-20</updated><authors><author><keyname>Benfarah</keyname><forenames>Ahmed</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author><author><keyname>Laurenti</keyname><forenames>Nicola</forenames></author></authors><title>Parallel BCC with One Common and Two Confidential Messages and Imperfect
  CSIT</title><categories>cs.IT math.IT</categories><comments>This paper is accepted for publication in IEEE Globecom second
  Workshop on trusted Com. with Physical Layer Security (TCPLS 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a broadcast communication system over parallel sub-channels where
the transmitter sends three messages: a common message to two users, and two
confidential messages to each user which need to be kept secret from the other
user. We assume partial channel state information at the transmitter (CSIT),
stemming from noisy channel estimation. The first contribution of this paper is
the characterization of the secrecy capacity region boundary as the solution of
weighted sum-rate problems, with suitable weights. Partial CSIT is addressed by
adding a margin to the estimated channel gains. The second paper contribution
is the solution of this problem in an almost closed-form, where only two single
real parameters must be optimized, e.g., through dichotomic searches. On the
one hand, the considered problem generalizes existing literature where only two
out of the three messages are transmitted. On the other hand, the solution
finds also practical applications into the resource allocation of orthogonal
frequency division multiplexing (OFDM) systems with both secrecy and fairness
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6985</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6985</id><created>2014-03-27</created><updated>2014-10-16</updated><authors><author><keyname>Demchuk</keyname><forenames>Kostyantyn</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author></authors><title>A Fast Minimal Infrequent Itemset Mining Algorithm</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel fast algorithm for finding quasi identifiers in large datasets is
presented. Performance measurements on a broad range of datasets demonstrate
substantial reductions in run-time relative to the state of the art and the
scalability of the algorithm to realistically-sized datasets up to several
million records.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.6997</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.6997</id><created>2014-03-27</created><authors><author><keyname>Li&#x161;ka</keyname><forenames>Martin</forenames></author></authors><title>Optimizing large applications</title><categories>cs.PL</categories><comments>78 pages, diploma thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both uppermost open source compilers, GCC and LLVM, are mature enough to
link-time optimize large applications. In case of large applications, we must
take into account, except standard speed efficiency and memory consumption,
different aspects. We focus on size of the code, cold start-up time, etc.
Developers of applications often come up with ad-hoc solutions such as Elfhack
utility, start-up of an application via a pre-loading utility and dlopen;
prelinking and variety of different tools that reorder functions to fit the
order of execution. The goal of the thesis is to analyse all existing
techniques of optimization, evaluate their efficiency and design new solutions
based on the link-time optimization platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7007</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7007</id><created>2014-03-27</created><updated>2014-06-06</updated><authors><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Hierarchical Coded Caching</title><categories>cs.IT cs.NI math.IT</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching of popular content during off-peak hours is a strategy to reduce
network loads during peak hours. Recent work has shown significant benefits of
designing such caching strategies not only to deliver part of the content
locally, but also to provide coded multicasting opportunities even among users
with different demands. Exploiting both of these gains was shown to be
approximately optimal for caching systems with a single layer of caches.
  Motivated by practical scenarios, we consider in this work a hierarchical
content delivery network with two layers of caches. We propose a new caching
scheme that combines two basic approaches. The first approach provides coded
multicasting opportunities within each layer; the second approach provides
coded multicasting opportunities across multiple layers. By striking the right
balance between these two approaches, we show that the proposed scheme achieves
the optimal communication rates to within a constant multiplicative and
additive gap. We further show that there is no tension between the rates in
each of the two layers up to the aforementioned gap. Thus, both layers can
simultaneously operate at approximately the minimum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7012</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7012</id><created>2014-03-27</created><updated>2014-05-12</updated><authors><author><keyname>Torrellas</keyname><forenames>Marc</forenames></author><author><keyname>Agustin</keyname><forenames>Adrian</forenames></author><author><keyname>Vidal</keyname><forenames>Josep</forenames></author></authors><title>On the Degrees of freedom of the K-user MISO Interference Channel with
  imperfect delayed CSIT</title><categories>cs.IT math.IT</categories><comments>Draft version of the accepted manuscript at IEEE ICASSP 14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the degrees of freedom (DoF) of the K-user
multiple-input single-output (MISO) interference channel (IC) with imperfect
delayed channel state information at the transmitters (dCSIT). For this
setting, new DoF inner bonds are provided, and benchmarked with
cooperation-based outer bounds. The achievability result is based on a
precoding scheme that aligns the interfering received signals through time,
exploiting the concept of Retrospective Interference Alignment (RIA). The
proposed approach outperforms all previous known schemes. Furthermore, we study
the proposed scheme under channel estimation errors (CEE) on the reported
dCSIT, and derive a closed-form expression for the achievable DoF with
imperfect dCSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7014</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7014</id><created>2014-03-27</created><updated>2014-05-07</updated><authors><author><keyname>Emura</keyname><forenames>Keita</forenames></author><author><keyname>Kanaoka</keyname><forenames>Akira</forenames></author><author><keyname>Ohta</keyname><forenames>Satoshi</forenames></author><author><keyname>Takahashi</keyname><forenames>Takeshi</forenames></author></authors><title>Building Secure and Anonymous Communication Channel: Formal Model and
  its Prototype Implementation</title><categories>cs.NI cs.CR</categories><comments>This is a preprint version of our paper presented in SAC'14, March
  24-28, 2014, Gyeongju, Korea. ACMSAC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various techniques need to be combined to realize anonymously authenticated
communication. Cryptographic tools enable anonymous user authentication while
anonymous communication protocols hide users' IP addresses from service
providers. One simple approach for realizing anonymously authenticated
communication is their simple combination, but this gives rise to another
issue; how to build a secure channel. The current public key infrastructure
cannot be used since the user's public key identifies the user. To cope with
this issue, we propose a protocol that uses identity-based encryption for
packet encryption without sacrificing anonymity, and group signature for
anonymous user authentication. Communications in the protocol take place
through proxy entities that conceal users' IP addresses from service providers.
The underlying group signature is customized to meet our objective and improve
its efficiency. We also introduce a proof-of-concept implementation to
demonstrate the protocol's feasibility. We compare its performance to SSL
communication and demonstrate its practicality, and conclude that the protocol
realizes secure, anonymous, and authenticated communication between users and
service providers with practical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7017</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7017</id><created>2014-03-27</created><updated>2014-06-12</updated><authors><author><keyname>Torrellas</keyname><forenames>Marc</forenames></author><author><keyname>Agustin</keyname><forenames>Adrian</forenames></author><author><keyname>Vidal</keyname><forenames>Josep</forenames></author></authors><title>Retrospective Interference Alignment for the 3-user MIMO Interference
  Channel with delayed CSIT</title><categories>cs.IT math.IT</categories><comments>Draft version of the accepted manuscript at IEEE ICASSP 14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) of the 3-user multiple input multiple output
interference channel (3-user MIMO IC) are investigated where there is delayed
channel state information at the transmitters (dCSIT). We generalize the ideas
of Maleki et al. about {\it Retrospective Interference Alignment (RIA)} to be
applied to the MIMO IC, where transmitters and receivers are equipped with
$(M,N)$ antennas, respectively. We propose a two-phase transmission scheme
where the number of slots per phase and number of transmitted symbols are
optimized by solving a maximization problem. Finally, we review the existing
achievable DoF results in the literature as a function of the ratio between
transmitting and receiving antennas $\rho=M/N$. The proposed scheme improves
all other strategies when $\rho \in \left(\frac{1}{2}, \frac{31}{32} \right]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7019</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7019</id><created>2014-03-27</created><updated>2015-09-22</updated><authors><author><keyname>Trip</keyname><forenames>Sebastian</forenames></author><author><keyname>B&#xfc;rger</keyname><forenames>Mathias</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>An internal model approach to (optimal) frequency regulation in power
  grids with time-varying voltages</title><categories>cs.SY math.OC</categories><comments>16 pages. Abridged version appeared in the Proceedings of the 21st
  International Symposium on Mathematical Theory of Networks and Systems, MTNS
  2014, Groningen, the Netherlands. Submitted in December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of frequency regulation in power grids under
unknown and possible time-varying load changes, while minimizing the generation
costs. We formulate this problem as an output agreement problem for
distribution networks and address it using incremental passivity and
distributed internal-model-based controllers. Incremental passivity enables a
systematic approach to study convergence to the steady state with zero
frequency deviation and to design the controller in the presence of
time-varying voltages, whereas the internal-model principle is applied to
tackle the uncertain nature of the loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7022</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7022</id><created>2014-03-27</created><updated>2015-01-14</updated><authors><author><keyname>Liu</keyname><forenames>Jiang</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author><author><keyname>Zhao</keyname><forenames>Hengjun</forenames></author><author><keyname>Zou</keyname><forenames>Liang</forenames></author></authors><title>Abstraction of Elementary Hybrid Systems by Variable Transformation</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elementary hybrid systems (EHSs) are those hybrid systems (HSs) containing
elementary functions such as exp, ln, sin, cos, etc. EHSs are very common in
practice, especially in safety-critical domains. Due to the non-polynomial
expressions which lead to undecidable arithmetic, verification of EHSs is very
hard. Existing approaches based on partition of state space or
over-approximation of reachable sets suffer from state explosion or inflation
of numerical errors. In this paper, we propose a symbolic abstraction approach
that reduces EHSs to polynomial hybrid systems (PHSs), by replacing all
non-polynomial terms with newly introduced variables. Thus the verification of
EHSs is reduced to the one of PHSs, enabling us to apply all the
well-established verification techniques and tools for PHSs to EHSs. In this
way, it is possible to avoid the limitations of many existing methods. We
illustrate the abstraction approach and its application in safety verification
of EHSs by several real world examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7024</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7024</id><created>2014-03-27</created><authors><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Kwon</keyname><forenames>O-joung</forenames></author><author><keyname>Obdr&#x17e;&#xe1;lek</keyname><forenames>Jan</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author></authors><title>Tree-depth and Vertex-minors</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper, Kwon and Oum claim that every graph of bounded rank-width
is a pivot-minor of a graph of bounded tree-width (while the converse has been
known true already before). We study the analogous questions for &quot;depth&quot;
parameters of graphs, namely for the tree-depth and related new shrub-depth. We
show that shrub-depth is monotone under taking vertex-minors, and that every
graph class of bounded shrub-depth can be obtained via vertex-minors of graphs
of bounded tree-depth. We also consider the same questions for bipartite graphs
and pivot-minors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7044</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7044</id><created>2014-03-27</created><authors><author><keyname>Schlingloff</keyname><forenames>Holger</forenames><affiliation>Fraunhofer FOKUS, Humboldt University of Berlin, Germany</affiliation></author><author><keyname>Petrenko</keyname><forenames>Alexander K.</forenames><affiliation>Institute for System Programming of Russian Academy of Sciences, Russia</affiliation></author></authors><title>Proceedings Ninth Workshop on Model-Based Testing</title><categories>cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 141, 2014</journal-ref><doi>10.4204/EPTCS.141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Ninth Workshop on Model-Based
Testing (MBT 2014), which was held in Grenoble, France on April 6, 2014 as a
satellite workshop of the European Joint Conferences on Theory and Practice of
Software (ETAPS 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7048</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7048</id><created>2014-03-27</created><updated>2015-11-11</updated><authors><author><keyname>Bonchi</keyname><forenames>Filippo</forenames></author><author><keyname>Sobocinski</keyname><forenames>Pawel</forenames></author><author><keyname>Zanasi</keyname><forenames>Fabio</forenames></author></authors><title>Interacting Hopf Algebras</title><categories>cs.LO math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the theory IH of interacting Hopf algebras, parametrised over a
principal ideal domain R. The axioms of IH are derived using Lack's approach to
composing PROPs: they feature two Hopf algebra and two Frobenius algebra
structures on four different monoid-comonoid pairs. This construction is
instrumental in showing that IH is isomorphic to the PROP of linear relations
(i.e. subspaces) over the field of fractions of R.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7057</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7057</id><created>2014-03-27</created><authors><author><keyname>Kolesnikov</keyname><forenames>Alexander</forenames></author><author><keyname>Guillaumin</keyname><forenames>Matthieu</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Closed-Form Training of Conditional Random Fields for Large Scale Image
  Segmentation</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present LS-CRF, a new method for very efficient large-scale training of
Conditional Random Fields (CRFs). It is inspired by existing closed-form
expressions for the maximum likelihood parameters of a generative graphical
model with tree topology. LS-CRF training requires only solving a set of
independent regression problems, for which closed-form expression as well as
efficient iterative solvers are available. This makes it orders of magnitude
faster than conventional maximum likelihood learning for CRFs that require
repeated runs of probabilistic inference. At the same time, the models learned
by our method still allow for joint inference at test time. We apply LS-CRF to
the task of semantic image segmentation, showing that it is highly efficient,
even for loopy models where probabilistic inference is problematic. It allows
the training of image segmentation models from significantly larger training
sets than had been used previously. We demonstrate this on two new datasets
that form a second contribution of this paper. They consist of over 180,000
images with figure-ground segmentation annotations. Our large-scale experiments
show that the possibilities of CRF-based image segmentation are far from
exhausted, indicating, for example, that semi-supervised learning and the use
of non-linear predictors are promising directions for achieving higher
segmentation accuracy in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7058</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7058</id><created>2014-03-27</created><updated>2014-11-10</updated><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>The Sketching Complexity of Graph Cuts</title><categories>cs.DS</categories><comments>The current version differs slightly from an earlier one
  (arXiv:1403.7058v1). First, the lower bound for the number of edges in
  $(1+\epsilon)$-cut sparsifiers is improved, and now our bound is tight.
  Second, we retract our earlier claim that the sparsification algorithm can be
  performed in two passes of streaming over the graph edges</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of sketching an input graph, so that given the sketch,
one can estimate the weight of any cut in the graph within factor $1+\epsilon$.
We present lower and upper bounds on the size of a randomized sketch, focusing
on the dependence on the accuracy parameter $\epsilon&gt;0$.
  First, we prove that for every $\epsilon &gt; 1/\sqrt n$, every sketch that
succeeds (with constant probability) in estimating the weight of all cuts
$(S,\bar S)$ in an $n$-vertex graph (simultaneously), must be of size
$\Omega(n/\epsilon^2)$ bits. In the special case where the sketch is itself a
weighted graph (which may or may not be a subgraph) and the estimator is the
sum of edge weights across the cut in the sketch, i.e., a cut sparsifier, we
show the sketch must have $\Omega(n/\epsilon^2)$ edges, which is optimal.
Despite the long sequence of work on graph sparsification, no such lower bound
was known on the size of a cut sparsifier.
  We then design a randomized sketch that, given $\epsilon\in(0,1)$ and an
edge-weighted $n$-vertex graph, produces a sketch of size $\tilde
O(n/\epsilon)$ bits, from which the weight of any cut $(S,\bar S)$ can be
reported, with high probability, within factor $1+\epsilon$. The previous upper
bound is $\tilde O(n/\epsilon^2)$ bits, which follows by storing a cut
sparsifier (Bencz{\'u}r and Karger, 1996). To obtain this improvement, we
critically use both that the sketch need only be correct on each fixed cut with
high probability (rather than on all cuts), and that the estimation procedure
of the data structure can be arbitrary (rather than a weighted subgraph). We
also show a lower bound of $\Omega(n/\epsilon)$ bits for the space requirement
of any data structure achieving this guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7074</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7074</id><created>2014-02-05</created><updated>2015-07-05</updated><authors><author><keyname>Khorramzadeh</keyname><forenames>Yasamin</forenames></author><author><keyname>Youssef</keyname><forenames>Mina</forenames></author><author><keyname>Eubank</keyname><forenames>Stephen</forenames></author><author><keyname>Mowlaei</keyname><forenames>Shahir</forenames></author></authors><title>Analyzing Network Reliability Using Structural Motifs</title><categories>cs.SI cond-mat.stat-mech math.CO physics.soc-ph q-bio.PE</categories><doi>10.1103/PhysRevE.91.042814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses the reliability polynomial, introduced by Moore and Shannon
in 1956, to analyze the effect of network structure on diffusive dynamics such
as the spread of infectious disease. We exhibit a representation for the
reliability polynomial in terms of what we call {\em structural motifs} that is
well suited for reasoning about the effect of a network's structural properties
on diffusion across the network. We illustrate by deriving several general
results relating graph structure to dynamical phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7084</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7084</id><created>2014-03-26</created><updated>2014-04-02</updated><authors><author><keyname>van Leeuwen</keyname><forenames>David A.</forenames></author><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author></authors><title>Constrained speaker linking</title><categories>stat.ML cs.SD</categories><comments>Submitted to Interspeech 2014, some typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study speaker linking (a.k.a.\ partitioning) given
constraints of the distribution of speaker identities over speech recordings.
Specifically, we show that the intractable partitioning problem becomes
tractable when the constraints pre-partition the data in smaller cliques with
non-overlapping speakers. The surprisingly common case where speakers in
telephone conversations are known, but the assignment of channels to identities
is unspecified, is treated in a Bayesian way. We show that for the Dutch CGN
database, where this channel assignment task is at hand, a lightweight speaker
recognition system can quite effectively solve the channel assignment problem,
with 93% of the cliques solved. We further show that the posterior distribution
over channel assignment configurations is well calibrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7086</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7086</id><created>2014-03-26</created><updated>2014-03-31</updated><authors><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author><author><keyname>Sergeraert</keyname><forenames>Francis</forenames></author></authors><title>Defining and computing persistent Z-homology in the general case</title><categories>cs.CG cs.SC math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By general case we mean methods able to process simplicial sets and chain
complexes not of finite type. A filtration of the object to be studied is the
heart of both subjects persistent homology and spectral sequences. In this
paper we present the complete relation between them, both from theoretical and
computational points of view. One of the main contributions of this paper is
the observation that a slight modification of our previous programs computing
spectral sequences is enough to compute also persistent homology. By
inheritance from our spectral sequence programs, we obtain for free persistent
homology programs applicable to spaces not of finite type (provided they are
spaces with effective homology) and with Z-coefficients (significantly
generalizing the usual presentation of persistent homology over a field). As an
illustration, we compute some persistent homology groups (and the corresponding
integer barcodes) in the case of a Postnikov tower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7087</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7087</id><created>2014-02-19</created><authors><author><keyname>Williams</keyname><forenames>Nick</forenames></author></authors><title>Conclusions from a NAIVE Bayes Operator Predicting the Medicare 2011
  Transaction Data Set</title><categories>cs.LG cs.CY physics.data-an</categories><comments>8 Pages, 7 figures</comments><msc-class>62, 91</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduction: The United States Federal Government operates one of the worlds
largest medical insurance programs, Medicare, to ensure payment for clinical
services for the elderly, illegal aliens and those without the ability to pay
for their care directly. This paper evaluates the Medicare 2011 Transaction
Data Set which details the transfer of funds from Medicare to private and
public clinical care facilities for specific clinical services for the
operational year 2011. Methods: Data mining was conducted to establish the
relationships between reported and computed transaction values in the data set
to better understand the drivers of Medicare transactions at a programmatic
level. Results: The models averaged 88 for average model accuracy and 38 for
average Kappa during training. Some reported classes are highly independent
from the available data as their predictability remains stable regardless of
redaction of supporting and contradictory evidence. DRG or procedure type
appears to be unpredictable from the available financial transaction values.
Conclusions: Overlay hypotheses such as charges being driven by the volume
served or DRG being related to charges or payments is readily false in this
analysis despite 28 million Americans being billed through Medicare in 2011 and
the program distributing over 70 billion in this transaction set alone. It may
be impossible to predict the dependencies and data structures the payer of last
resort without data from payers of first and second resort. Political concerns
about Medicare would be better served focusing on these first and second order
payer systems as what Medicare costs is not dependent on Medicare itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7088</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7088</id><created>2014-03-26</created><authors><author><keyname>Takahashi</keyname><forenames>Takeshi</forenames></author><author><keyname>Harju</keyname><forenames>Jarmo</forenames></author><author><keyname>Kannisto</keyname><forenames>Joona</forenames></author><author><keyname>Silverajan</keyname><forenames>Bilhanan</forenames></author><author><keyname>Harju</keyname><forenames>Jarmo</forenames></author><author><keyname>Matsuo</keyname><forenames>Shin'ichiro</forenames></author></authors><title>Tailored Security: Building Nonrepudiable Security Service-Level
  Agreements</title><categories>cs.CR cs.HC</categories><comments>This is a preprint version of our article posted to IEEE Vehicular
  Technology Magazine</comments><journal-ref>IEEE Vehicular Technology Magazine, Volume 8, Issue 3, September
  2013</journal-ref><doi>10.1109/MVT.2013.2269188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security features of current digital services are mostly defined and
dictated by the service provider (SP). A user can always decline to use a
service whose terms do not fulfill the expected criteria, but in many cases,
even a simple negotiation might result in a more satisfying outcome. This
article aims at building nonrepudiable security service-level agreements
(SSLAs) between a user and an SP. The proposed mechanism provides a means to
describe security requirements and capabilities in different dimensions, from
overall targets and risks to technical specifications, and it also helps in
translating between the dimensions. A negotiation protocol and a decision
algorithm are then used to let the parties agree on the security features used
in the service. This article demonstrates the feasibility and usability of the
mechanism by describing its usage scenario and proof-of-concept implementation
and analyzes its nonrepudiability and security aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7100</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7100</id><created>2014-03-26</created><authors><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author><author><keyname>Dong</keyname><forenames>Wei-Ming</forenames></author></authors><title>A study on cost behaviors of binary classification measures in
  class-imbalanced problems</title><categories>cs.LG</categories><comments>7 pages, 5 figures, 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates into cost behaviors of binary classification measures
in a background of class-imbalanced problems. Twelve performance measures are
studied, such as F measure, G-means in terms of accuracy rates, and of recall
and precision, balance error rate (BER), Matthews correlation coefficient
(MCC), Kappa coefficient, etc. A new perspective is presented for those
measures by revealing their cost functions with respect to the class imbalance
ratio. Basically, they are described by four types of cost functions. The
functions provides a theoretical understanding why some measures are suitable
for dealing with class-imbalanced problems. Based on their cost functions, we
are able to conclude that G-means of accuracy rates and BER are suitable
measures because they show &quot;proper&quot; cost behaviors in terms of &quot;a
misclassification from a small class will cause a greater cost than that from a
large class&quot;. On the contrary, F1 measure, G-means of recall and precision, MCC
and Kappa coefficient measures do not produce such behaviors so that they are
unsuitable to serve our goal in dealing with the problems properly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7102</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7102</id><created>2014-03-19</created><updated>2014-06-17</updated><authors><author><keyname>Dyagilev</keyname><forenames>Kirill</forenames></author><author><keyname>Yom-Tov</keyname><forenames>Elad</forenames></author></authors><title>Echo chamber amplification and disagreement effects in the political
  activity of Twitter users</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks have emerged as a significant platform for political
discourse. In this paper we investigate what affects the level of participation
of users in the political discussion. Specifically, are users more likely to be
active when they are surrounded by like-minded individuals, or, alternatively,
when their environment is heterogeneous, and so their messages might be carried
to people with differing views. To answer this question, we analyzed the
activity of about 200K Twitter users who expressed explicit support for one of
the candidates of the 2012 US presidential election. We quantified the level of
political activity (PA) of users by the fraction of political tweets in their
posts, and analyzed the relationship between PA and measures of the users'
political environment. These measures were designed to assess the
likemindedness, e.g., the fraction of users with similar political views, of
their virtual and geographic environments. Our results showed that high PA is
usually obtained by users in politically balanced virtual environment. This is
in line with the disagreement theory of political science that states that a
user's PA is invigorated by the disagreement with their peers. Our results also
show that users surrounded by politically like-minded virtual peers tend to
have low PA. This observation contradicts the echo chamber amplification theory
that states that a person tends to be more politically active when surrounded
by like-minded people. Finally, we observe that the likemindedness of the
geographical environment does not affect PA. We thus conclude that PA of users
is independent of the likemindedness of their geographical environment and is
correlated with likemindedness of their virtual environment. The exact form of
correlation manifests the phenomenon of disagreement and, in a majority of
settings, contradicts the echo chamber amplification theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7105</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7105</id><created>2014-03-15</created><authors><author><keyname>Baig</keyname><forenames>Dur-e-Zehra</forenames></author></authors><title>Physiological Control of Human Heart Rate and Oxygen Consumption during
  Rhythmic Exercises</title><categories>q-bio.QM cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical exercise has significant benefits for humans in improving the health
and quality of their lives, by improving the functional performance of their
cardiovascular and respiratory systems. However, it is very important to
control the workload, e.g. the frequency of body movements, within the
capability of the individual to maximise the efficiency of the exercise. The
workload is generally represented in terms of heart rate (HR) and oxygen
consumption VO2. We focus particularly on the control of HR and VO2 using the
workload of an individual body movement, also known as the exercise rate (ER),
in this research. The first part of this report deals with the modelling and
control of HR during an unknown type of rhythmic exercise. A novel feature of
the developed system is to control HR via manipulating ER as a control input.
The relation between ER and HR is modelled using a simple autoregressive model
with unknown parameters. The parameters of the model are estimated using a
Kalman filter and an indirect adaptive H1 controller is designed. The
performance of the system is tested and validated on six subjects during rowing
and cycling exercise. The results demonstrate that the designed control system
can regulate HR to a predefined profile. The second part of this report deals
with the problem of estimating VO2 during rhythmic exercise, as the direct
measurement of VO2 is not realisable in these environments. Therefore,
non-invasive sensors are used to measure HR, RespR, and ER to estimate VO2. The
developed approach for cycling and rowing exercise predicts the percentage
change in maximum VO2 from the resting to the exercising phases, using a
Hammerstein model.. Results show that the average quality of fit in both
exercises is improved as the intensity of exercise is increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7115</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7115</id><created>2014-03-27</created><authors><author><keyname>John</keyname><forenames>Saul St.</forenames></author><author><keyname>Akella</keyname><forenames>Aditya</forenames></author></authors><title>Active Switching: Packet Steering Flow Annotations</title><categories>cs.NI</categories><msc-class>68M10</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our previous experience building systems for middlebox chain composition and
scaling in software-defined networks has revealed that existing mechanisms of
flow annotation commonly do not survive middlebox-traversals, or suffer from
extreme identifier domain limitations resulting in excessive flow table size.
In this paper, we analyze the structural artifacts resulting in these
challenges, and offer a framework for describing the behavior of middleboxes
based on actions taken on traversing packets. We then present a novel mechanism
for flow annotation that features an identifier domain significantly larger
than existing techniques, that is transparent to hosts traversed, and that
conserves flow-table resources by requiring only a small number of match rules
and actions in most switches. We evaluate said technique, showing that it
requires less per-switch state than conventional techniques. We then describe
extensions allowing implementation of this architecture within a broader class
of systems. Finally, we close with architectural suggestions for enabling
straightforward integration of middleboxes within software-defined networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7123</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7123</id><created>2014-03-27</created><updated>2014-04-07</updated><authors><author><keyname>Ju</keyname><forenames>Hyungsik</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>User Cooperation in Wireless Powered Communication Networks</title><categories>cs.IT math.IT</categories><comments>7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies user cooperation in the emerging wireless powered
communication network (WPCN) for throughput optimization. For the purpose of
exposition, we consider a two-user WPCN, in which one hybrid access point
(H-AP) broadcasts wireless energy to two distributed users in the downlink (DL)
and the users transmit their independent information using their individually
harvested energy to the H-AP in the uplink (UL) through
time-division-multiple-access (TDMA). We propose user cooperation in the WPCN
where the user which is nearer to the H-AP and has a better channel for DL
energy harvesting and UL information transmission uses part of its allocated UL
time and DL harvested energy to help to relay the far user's information to the
H-AP, in order to achieve more balanced throughput optimization. We maximize
the weighted sum-rate (WSR) of the two users by jointly optimizing the time and
power allocations in the network for both wireless energy transfer in the DL
and wireless information transmission and relaying in the UL. Simulation
results show that the proposed user cooperation scheme can effectively improve
the achievable throughput in the WPCN with desired user fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7137</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7137</id><created>2014-03-27</created><updated>2014-12-05</updated><authors><author><keyname>Attia</keyname><forenames>Ahmed</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>A Sampling Filter for Non-Gaussian Data Assimilation</title><categories>cs.CE stat.CO</categories><comments>52 pages, 24 figures, 4 tables</comments><report-no>CSTR-4/2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data assimilation combines information from models, measurements, and priors
to estimate the state of a dynamical system such as the atmosphere. The
Ensemble Kalman filter (EnKF) is a family of ensemble-based data assimilation
approaches that has gained wide popularity due its simple formulation, ease of
implementation, and good practical results. Most EnKF algorithms assume that
the underlying probability distributions are Gaussian. Although this assumption
is well accepted, it is too restrictive when applied to large nonlinear models,
nonlinear observation operators, and large levels of uncertainty. Several
approaches have been proposed in order to avoid the Gaussianity assumption. One
of the most successful strategies is the maximum likelihood ensemble filter
(MLEF) which computes a maximum a posteriori estimate of the state assuming the
posterior distribution is Gaussian. MLEF is designed to work with nonlinear and
even non-differentiable observation operators, and shows good practical
performance. However, there are limits to the degree of nonlinearity that MLEF
can handle. This paper proposes a new ensemble-based data assimilation method,
named the &quot;sampling filter&quot;, which obtains the analysis by sampling directly
from the posterior distribution. The sampling strategy is based on a Hybrid
Monte Carlo (HMC) approach that can handle non-Gaussian probability
distributions. Numerical experiments are carried out using the Lorenz-96 model
and observation operators with different levels of non-linearity and
differentiability. The proposed filter is also tested with shallow water model
on a sphere with linear observation operator. The results show that the
sampling filter can perform well even in highly nonlinear situations were EnKF
and MLEF filters diverge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7152</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7152</id><created>2014-03-27</created><authors><author><keyname>Hamidou</keyname><forenames>Mansoriya</forenames><affiliation>LITIS</affiliation></author><author><keyname>Fournier</keyname><forenames>Dominique</forenames><affiliation>LITIS</affiliation></author><author><keyname>Sanlaville</keyname><forenames>Eric</forenames><affiliation>LITIS</affiliation></author><author><keyname>Serin</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LITIS</affiliation></author></authors><title>Management of dangerous goods in container terminal with MAS model</title><categories>cs.MA</categories><comments>8 pages. The 15th International Conference on Harbor, Maritime \&amp;
  Multimodal Logistics, Ath\`enes : Greece (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a container terminal, many operations occur within the storage area:
containers import, containers export and containers shifting. All these
operations require the respect of many rules and even laws in order to
guarantee the port safety and to prevent risks, especially when hazardous
material is concerned. In this paper, we propose a hybrid architecture, using a
Cellular Automaton and a Multi-Agent System to handle the dangerous container
storage problem. It is an optimization problem since the aim is to improve the
container terminal configuration, that is, the way hazardous containers are
dispatched through the terminal to improve its security. In our model, we
consider containers as agents, in order to use a Multi-Agent System for the
decision aid software, and a Cellular Automaton for modelling the terminal
itself. To validate our approach many tests have been performed and the results
show the relevance of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7162</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7162</id><created>2014-03-27</created><authors><author><keyname>Singh</keyname><forenames>Gagandeep</forenames></author><author><keyname>Jain</keyname><forenames>Vishal</forenames></author></authors><title>Information Retrieval (IR) through Semantic Web (SW): An Overview</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large amount of data is present on the web. It contains huge number of web
pages and to find suitable information from them is very cumbersome task. There
is need to organize data in formal manner so that user can easily access and
use them. To retrieve information from documents, we have many Information
Retrieval (IR) techniques. Current IR techniques are not so advanced that they
can be able to exploit semantic knowledge within documents and give precise
results. IR technology is major factor responsible for handling annotations in
Semantic Web (SW) languages and in the present paper knowledgeable
representation languages used for retrieving information are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7164</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7164</id><created>2014-03-27</created><updated>2015-03-11</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Tight Bounds for Symmetric Divergence Measures and a Refined Bound for
  Lossless Source Coding</title><categories>cs.IT math.IT math.PR</categories><comments>Appears in the IEEE Trans. on Information Theory, February 2015.
  arXiv admin note: substantial text overlap with arXiv:1502.06428</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tight bounds for several symmetric divergence measures are derived in terms
of the total variation distance. It is shown that each of these bounds is
attained by a pair of 2 or 3-element probability distributions. An application
of these bounds for lossless source coding is provided, refining and improving
a certain bound by Csisz\'{a}r. Another application of these bounds has been
recently introduced by Yardi. et al. for channel-code detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7170</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7170</id><created>2014-03-27</created><updated>2014-10-20</updated><authors><author><keyname>Cyranka</keyname><forenames>Jacek</forenames></author><author><keyname>Zgliczy&#x144;ski</keyname><forenames>Piotr</forenames></author></authors><title>Existence of globally attracting solutions for one-dimensional viscous
  Burgers equation with nonautonomous forcing - a computer assisted proof</title><categories>math.DS cs.NA math.AP</categories><comments>38 pages, 1 figure</comments><msc-class>65M99, 35B40, 35B41, 37B55, 65G40</msc-class><doi>10.1137/14096699X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of globally attracting solutions of the viscous
Burgers equation with periodic boundary conditions on the line for some
particular choices of viscosity and non-autonomous forcing. The attract- ing
solution is periodic if the forcing is periodic. The method is general and can
be applied to other similar partial differential equations. The proof is
computer assisted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7175</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7175</id><created>2014-03-27</created><updated>2014-07-21</updated><authors><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author></authors><title>Low-Rank and Low-Order Decompositions for Local System Identification</title><categories>math.OC cs.SY</categories><comments>Working note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As distributed systems increase in size, the need for scalable algorithms
becomes more and more important. We argue that in the context of system
identification, an essential building block of any scalable algorithm is the
ability to estimate local dynamics within a large interconnected system. We
show that in what we term the &quot;full interconnection measurement&quot; setting, this
task is easily solved using existing system identification methods. We also
propose a promising heuristic for the &quot;hidden interconnection measurement&quot;
case, in which contributions to local measurements from both local and global
dynamics need to be separated. Inspired by the machine learning literature, and
in particular by convex approaches to rank minimization and matrix
decomposition, we exploit the fact that the transfer function of the local
dynamics is low-order, but full-rank, while the transfer function of the global
dynamics is high-order, but low-rank, to formulate this separation task as a
nuclear norm minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7178</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7178</id><created>2014-03-27</created><authors><author><keyname>Liu</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Zhifang</forenames></author></authors><title>Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm:
  A different perspective</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of optimal layout of an offshore wind farm
to minimize the wake effect impacts. Considering the specific requirements of
concerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)
which introduces location swaps to replace random crossovers in conventional
GAs. That way the total number of turbines in the resulting layout will be
effectively kept to the initially specified value. We experiment the proposed
AGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typical
offshore wind distribution setting respectively. Numerical results verify the
effectiveness of our proposed algorithm which achieves a much faster
convergence compared to conventional GA algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7181</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7181</id><created>2014-03-27</created><updated>2014-06-30</updated><authors><author><keyname>Armas-Cervantes</keyname><forenames>Abel</forenames></author><author><keyname>Baldan</keyname><forenames>Paolo</forenames></author><author><keyname>Garcia-Ba&#xf1;uelos</keyname><forenames>Luciano</forenames></author></authors><title>Reduction of Event Structures under History Preserving Bisimulation</title><categories>cs.LO</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event structures represent concurrent processes in terms of events and
dependencies between events modelling behavioural relations like causality and
conflict. Since the introduction of prime event structures, many variants of
event structures have been proposed with different behavioural relations and,
hence, with differences in their expressive power. One of the possible benefits
of using a more expressive event structure is that of having a more compact
representation for the same behaviour when considering the number of events
used in a prime event structure. Therefore, this article addresses the problem
of reducing the size of an event structure while preserving behaviour under a
well-known notion of equivalence, namely history preserving bisimulation. In
particular, we investigate this problem on two generalisations of the prime
event structures. The first one, known as asymmetric event structure, relies on
a asymmetric form of the conflict relation. The second one, known as flow event
structure, supports a form of disjunctive causality. More specifically, we
describe the conditions under which a set of events in an event structure can
be folded into a single event while preserving the original behaviour. The
successive application of this folding operation leads to a minimal size event
structure. However, the order on which the folding operation is applied may
lead to different minimal size event structures. The latter has a negative
implication on the potential use of a minimal size event structure as a
canonical representation for behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7192</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7192</id><created>2014-03-27</created><authors><author><keyname>Azarfar</keyname><forenames>Arash</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sans&#xf2;</keyname><forenames>Brunilde</forenames></author></authors><title>Delay Analysis of Multichannel Opportunistic Spectrum Access MAC
  Protocols</title><categories>cs.NI</categories><comments>1 table, 2 algorithms and 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide in this paper a comprehensive delay and queueing analysis for two
baseline medium access control (MAC) protocols for multi-user cognitive radio
(CR) networks and investigate the impact of different network parameters, such
as packet size, Aloha-type medium access probability and number of channels on
the system performance. In addition to an accurate Markov chain, which follows
the queue status of all users, several lower complexity queueing theory
approximations are provided. Accuracy and performance of the proposed
analytical approximations are verified with extensive simulations. It is
observed that for CR networks using an Aloha-type access to the control
channel, a buffering MAC protocol, where in case of interruption the CR user
waits for the primary user to vacate the channel before resuming the
transmission, outperforms a switching MAC protocol, where the CR user vacates
the channel in case of appearance of primary users and then compete again to
gain access to a new channel. The reason is that the delay bottleneck for both
protocols is the time required to successfully access the control channel,
which occurs more frequently for the switching MAC protocol. We also propose a
user clustering approach, where users are divided into clusters with a separate
control channel per cluster, and observe that it can significantly improve the
performance by reducing the number of competing users per control channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7209</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7209</id><created>2014-03-27</created><authors><author><keyname>Reguly</keyname><forenames>Istv&#xe1;n Z.</forenames></author><author><keyname>Mudalige</keyname><forenames>Gihan R.</forenames></author><author><keyname>Bertolli</keyname><forenames>Carlo</forenames></author><author><keyname>Giles</keyname><forenames>Michael B.</forenames></author><author><keyname>Betts</keyname><forenames>Adam</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author><author><keyname>Radford</keyname><forenames>David</forenames></author></authors><title>Acceleration of a Full-scale Industrial CFD Application with OP2</title><categories>cs.CE cs.PF</categories><comments>Submitted to ACM Transactions on Parallel Computing</comments><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hydra is a full-scale industrial CFD application used for the design of
turbomachinery at Rolls Royce plc. It consists of over 300 parallel loops with
a code base exceeding 50K lines and is capable of performing complex
simulations over highly detailed unstructured mesh geometries. Unlike simpler
structured-mesh applications, which feature high speed-ups when accelerated by
modern processor architectures, such as multi-core and many-core processor
systems, Hydra presents major challenges in data organization and movement that
need to be overcome for continued high performance on emerging platforms. We
present research in achieving this goal through the OP2 domain-specific
high-level framework. OP2 targets the domain of unstructured mesh problems and
follows the design of an active library using source-to-source translation and
compilation to generate multiple parallel implementations from a single
high-level application source for execution on a range of back-end hardware
platforms. We chart the conversion of Hydra from its original hand-tuned
production version to one that utilizes OP2, and map out the key difficulties
encountered in the process. To our knowledge this research presents the first
application of such a high-level framework to a full scale production code.
Specifically we show (1) how different parallel implementations can be achieved
with an active library framework, even for a highly complicated industrial
application such as Hydra, and (2) how different optimizations targeting
contrasting parallel architectures can be applied to the whole application,
seamlessly, reducing developer effort and increasing code longevity.
Performance results demonstrate that not only the same runtime performance as
that of the hand-tuned original production code could be achieved, but it can
be significantly improved on conventional processor systems. Additionally, we
achieve further...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7232</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7232</id><created>2014-03-27</created><authors><author><keyname>Hamidi-Sepehr</keyname><forenames>Fatemeh</forenames></author><author><keyname>Chamberland</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>On the Performance of Short Block Codes over Finite-State Channels in
  the Rare-Transition Regime</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the mobile application landscape expands, wireless networks are tasked
with supporting different connection profiles, including real-time traffic and
delay-sensitive communications. Among many ensuing engineering challenges is
the need to better understand the fundamental limits of forward error
correction in non-asymptotic regimes. This article characterizes the
performance of random block codes over finite-state channels and evaluates
their queueing performance under maximum-likelihood decoding. In particular,
classical results from information theory are revisited in the context of
channels with rare transitions, and bounds on the probabilities of decoding
failure are derived for random codes. This creates an analysis framework where
channel dependencies within and across codewords are preserved. Such results
are subsequently integrated into a queueing problem formulation. For instance,
it is shown that, for random coding on the Gilbert-Elliott channel, the
performance analysis based on upper bounds on error probability provides very
good estimates of system performance and optimum code parameters. Overall, this
study offers new insights about the impact of channel correlation on the
performance of delay-aware, point-to-point communication links. It also
provides novel guidelines on how to select code rates and block lengths for
real-time traffic over wireless communication infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7238</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7238</id><created>2014-03-27</created><authors><author><keyname>Otachi</keyname><forenames>Yota</forenames></author><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author></authors><title>Reduction Techniques for Graph Isomorphism in the Context of Width
  Parameters</title><categories>cs.DM math.CO</categories><comments>23 pages, 4 figures</comments><msc-class>05C60, 05C85, 68R10,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of the graph isomorphism problem when
parameterized by width parameters related to tree decompositions. We apply the
following technique to obtain fixed-parameter tractability for such parameters.
We first compute an isomorphism invariant set of potential bags for a
decomposition and then apply a restricted version of the Weisfeiler-Lehman
algorithm to solve isomorphism. With this we show fixed-parameter tractability
for several parameters and provide a unified explanation for various
isomorphism results concerned with parameters related to tree decompositions.
As a possibly first step towards intractability results for parameterized graph
isomorphism we develop an fpt Turing-reduction from strong tree width to the a
priori unrelated parameter maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7239</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7239</id><created>2014-03-27</created><updated>2014-12-18</updated><authors><author><keyname>Poorkasmaei</keyname><forenames>Sina</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Asynchronous Orthogonal Differential Decoding for Multiple Access
  Channels</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose several differential decoding schemes for asynchronous multi-user
MIMO systems based on orthogonal space-time block codes (OSTBCs) where neither
the transmitters nor the receiver has knowledge of the channel. First, we
derive novel low complexity differential decoders by performing interference
cancelation in time and employing different decoding methods. The decoding
complexity of these schemes grows linearly with the number of users. We then
present additional differential decoding schemes that perform significantly
better than our low complexity decoders and outperform the existing synchronous
differential schemes but require higher decoding complexity compared to our low
complexity decoders. The proposed schemes work for any square OSTBC, any
constant amplitude constellation, any number of users, and any number of
receive antennas. Furthermore, we analyze the diversity of the proposed schemes
and derive conditions under which our schemes provide full diversity. For the
cases of two and four transmit antennas, we provide examples of PSK
constellations to achieve full diversity. Simulation results show that our
differential schemes provide good performance. To the best of our knowledge,
the proposed differential detection schemes are the first differential schemes
for asynchronous multi-user systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7242</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7242</id><created>2014-03-27</created><authors><author><keyname>Kooti</keyname><forenames>Farshad</forenames></author><author><keyname>Hodas</keyname><forenames>Nathan O.</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Network Weirdness: Exploring the Origins of Network Paradoxes</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Accepted to ICWSM 2014</comments><acm-class>J.4; G.3; G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have many counter-intuitive properties, including the
&quot;friendship paradox&quot; that states, on average, your friends have more friends
than you do. Recently, a variety of other paradoxes were demonstrated in online
social networks. This paper explores the origins of these network paradoxes.
Specifically, we ask whether they arise from mathematical properties of the
networks or whether they have a behavioral origin. We show that sampling from
heavy-tailed distributions always gives rise to a paradox in the mean, but not
the median. We propose a strong form of network paradoxes, based on utilizing
the median, and validate it empirically using data from two online social
networks. Specifically, we show that for any user the majority of user's
friends and followers have more friends, followers, etc. than the user, and
that this cannot be explained by statistical properties of sampling. Next, we
explore the behavioral origins of the paradoxes by using the shuffle test to
remove correlations between node degrees and attributes. We find that paradoxes
for the mean persist in the shuffled network, but not for the median. We
demonstrate that strong paradoxes arise due to the assortativity of user
attributes, including degree, and correlation between degree and attribute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7243</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7243</id><created>2014-03-27</created><authors><author><keyname>Gingl</keyname><forenames>Zoltan</forenames></author><author><keyname>Mingesz</keyname><forenames>Robert</forenames></author></authors><title>Noise properties in the ideal Kirchhoff-Law-Johnson-Noise secure
  communication system</title><categories>cs.CR</categories><journal-ref>PLoS ONE (2014) 9(4): e96109</journal-ref><doi>10.1371/journal.pone.0096109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we determine the noise properties needed for unconditional
security for the ideal Kirchhoff-Law-Johnson-Noise (KLJN) secure key
distribution system using simple statistical analysis. It has already been
shown using physical laws that resistors and Johnson-like noise sources provide
unconditional security. However real implementations use artificial noise
generators, therefore it is a question if other kind of noise sources and
resistor values could be used as well. We answer this question and in the same
time we provide a theoretical basis to analyze real systems as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7248</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7248</id><created>2014-03-27</created><authors><author><keyname>Ahmeti</keyname><forenames>Albin</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Polleres</keyname><forenames>Axel</forenames></author></authors><title>Updating RDFS ABoxes and TBoxes in SPARQL</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Updates in RDF stores have recently been standardised in the SPARQL 1.1
Update specification. However, computing answers entailed by ontologies in
triple stores is usually treated orthogonal to updates. Even the W3C's recent
SPARQL 1.1 Update language and SPARQL 1.1 Entailment Regimes specifications
explicitly exclude a standard behaviour how SPARQL endpoints should treat
entailment regimes other than simple entailment in the context of updates. In
this paper, we take a first step to close this gap. We define a fragment of
SPARQL basic graph patterns corresponding to (the RDFS fragment of) DL-Lite and
the corresponding SPARQL update language, dealing with updates both of ABox and
of TBox statements. We discuss possible semantics along with potential
strategies for implementing them. We treat both, (i) materialised RDF stores,
which store all entailed triples explicitly, and (ii) reduced RDF Stores, that
is, redundancy-free RDF stores that do not store any RDF triples (corresponding
to DL-Lite ABox statements) entailed by others already.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7257</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7257</id><created>2014-03-27</created><authors><author><keyname>van der Meer</keyname><forenames>A. P.</forenames><affiliation>Nspyre</affiliation></author><author><keyname>Kherrazi</keyname><forenames>R.</forenames><affiliation>Nspyre</affiliation></author><author><keyname>Hamilton</keyname><forenames>M.</forenames><affiliation>Nspyre</affiliation></author></authors><title>Using Formal Specifications to Support Model Based Testing ASDSpec: A
  Tool Combining the Best of Two Techniques</title><categories>cs.SE</categories><comments>In Proceedings MBT 2014, arXiv:1403.7044</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 141, 2014, pp. 1-13</journal-ref><doi>10.4204/EPTCS.141.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal methods and testing are two important approaches that assist in the
development of high quality software. For long time these approaches have been
seen as competitors and there was very little interaction between the two
communities. In recent years a new consensus has developed in which they are
seen as more complementary. In this report we present an approach based on the
ASD(Analytical Software Design) suite by Verum and the Microsoft Spec Explorer
Model Based Testing(MBT) tool. ASD is a model-based design approach that can
produce verified software components that can be combined into complete
systems. However, ASD cannot verify existing components, nor complex component
interactions involving data transfers. We have developed a tool that allows us
to convert ASD models to Spec Explorer, allowing us to do more complete
verification of software systems using dynamic testing at little additional
cost and effort. We demonstrate this by applying our approach to an
industrial-size case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7258</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7258</id><created>2014-03-27</created><authors><author><keyname>Colombo</keyname><forenames>Christian</forenames><affiliation>PEST Research Lab, Department of Computer Science, University of Malta</affiliation></author><author><keyname>Micallef</keyname><forenames>Mark</forenames><affiliation>PEST Research Lab, Department of Computer Science, University of Malta</affiliation></author><author><keyname>Scerri</keyname><forenames>Mark</forenames><affiliation>PEST Research Lab, Department of Computer Science, University of Malta</affiliation></author></authors><title>Verifying Web Applications: From Business Level Specifications to
  Automated Model-Based Testing</title><categories>cs.SE</categories><comments>In Proceedings MBT 2014, arXiv:1403.7044</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 141, 2014, pp. 14-28</journal-ref><doi>10.4204/EPTCS.141.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of reasons preventing a wider uptake of model-based testing in the
industry is the difficulty which is encountered by developers when trying to
think in terms of properties rather than linear specifications. A disparity has
traditionally been perceived between the language spoken by customers who
specify the system and the language required to construct models of that
system. The dynamic nature of the specifications for commercial systems further
aggravates this problem in that models would need to be rechecked after every
specification change. In this paper, we propose an approach for converting
specifications written in the commonly-used quasi-natural language Gherkin into
models for use with a model-based testing tool. We have instantiated this
approach using QuickCheck and demonstrate its applicability via a case study on
the eHealth system, the national health portal for Maltese residents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7259</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7259</id><created>2014-03-27</created><authors><author><keyname>Castillos</keyname><forenames>Kalou Cabrera</forenames><affiliation>LAAS-CNRS</affiliation></author><author><keyname>Dadeau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>FEMTO-ST Institute/INRIA CASSIS Project</affiliation></author><author><keyname>Julliand</keyname><forenames>Jacques</forenames><affiliation>FEMTO-ST Institute</affiliation></author></authors><title>Coverage Criteria for Model-Based Testing using Property Patterns</title><categories>cs.SE</categories><comments>In Proceedings MBT 2014, arXiv:1403.7044</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 141, 2014, pp. 29-43</journal-ref><doi>10.4204/EPTCS.141.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a model-based testing approach aiming at generating
test cases from a UML/OCL model and a given test property. The property is
expressed using a dedicated formalism based on patterns, and automatically
translated into an automaton. We propose new automata coverage criteria that
are tailored to the property automata we consider. These criteria are based on
the coverage of a relevant subset of the transitions related to the original
property, aiming at producing test cases that illustrate the dynamics of the
system described in the property. In addition, we propose to produce test cases
that can ensure the robustness of the system w.r.t. the property, by mutating
the property automaton, in order to try to provoke events that would be
forbidden by the property. This approach has been implemented into a large tool
set and it has been experimented on realistic case studies, in the context of
industrial research projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7260</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7260</id><created>2014-03-27</created><authors><author><keyname>Beohar</keyname><forenames>Harsh</forenames><affiliation>Center for Research on Embedded Systems Halmstad University, Sweden</affiliation></author><author><keyname>Mousavi</keyname><forenames>Mohammad Reza</forenames><affiliation>Center for Research on Embedded Systems Halmstad University, Sweden</affiliation></author></authors><title>Spinal Test Suites for Software Product Lines</title><categories>cs.SE cs.LO</categories><comments>In Proceedings MBT 2014, arXiv:1403.7044</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 141, 2014, pp. 44-55</journal-ref><doi>10.4204/EPTCS.141.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge in testing software product lines is efficiency. In
particular, testing a product line should take less effort than testing each
and every product individually. We address this issue in the context of
input-output conformance testing, which is a formal theory of model-based
testing. We extend the notion of conformance testing on input-output featured
transition systems with the novel concept of spinal test suites. We show how
this concept dispenses with retesting the common behavior among different, but
similar, products of a software product line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7261</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7261</id><created>2014-03-27</created><authors><author><keyname>Simao</keyname><forenames>Adenilso</forenames><affiliation>S&#xe3;o Paulo University</affiliation></author><author><keyname>Petrenko</keyname><forenames>Alexandre</forenames><affiliation>Centre de recherche informatique de Montreal</affiliation></author></authors><title>Generating Complete and Finite Test Suite for ioco: Is It Possible?</title><categories>cs.SE</categories><comments>In Proceedings MBT 2014, arXiv:1403.7044</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 141, 2014, pp. 56-70</journal-ref><doi>10.4204/EPTCS.141.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing from Input/Output Transition Systems has been intensely investigated.
The conformance between the implementation and the specification is often
determined by the so-called ioco-relation. However, generating tests for ioco
is usually hindered by the problem of conflicts between inputs and outputs.
Moreover, the generation is mainly based on nondeterministic methods, which may
deliver complete test suites but require an unbounded number of executions. In
this paper, we investigate whether it is possible to construct a finite test
suite which is complete in a predefined fault domain for the classical ioco
relation even in the presence of input/output conflicts. We demonstrate that it
is possible under certain assumptions about the specification and
implementation, by proposing a method for complete test generation, based on a
traditional method developed for FSM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7264</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7264</id><created>2014-03-27</created><authors><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author></authors><title>Proceedings Second Workshop on Synthesis</title><categories>cs.LO cs.SY</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 142, 2014</journal-ref><doi>10.4204/EPTCS.142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software synthesis is rapidly developing into an important research area with
vast potential for practical application. The SYNT Workshop on Synthesis aims
to bringing together researchers interested in synthesis to present both
ongoing and mature work on all aspects of automated synthesis and its
applications.
  The second iteration of SYNT took place in Saint Petersburg, Russia, and was
co-located with the 25th International Conference on Computer Aided
Verification. The workshop included eleven presentations covering the full
scope of the emerging synthesis community, as well as a discussion lead by Swen
Jacobs on the organization of two new synthesis competitions focusing on
reactive synthesis and syntax-guided functional synthesis respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7272</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7272</id><created>2014-03-27</created><authors><author><keyname>Iwata</keyname><forenames>Satoru</forenames></author><author><keyname>Kamiyama</keyname><forenames>Naoyuki</forenames></author><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Kijima</keyname><forenames>Shuji</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author></authors><title>Extended Formulations for Sparsity Matroids</title><categories>math.CO cs.DM math.OC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the existence of a polynomial-size extended formulation for the base
polytope of a $(k,\ell)$-sparsity matroid. For an undirected graph $G=(V,E)$,
the size of the formulation is $O(|V||E|)$ when $k \geq \ell$ and $O(|V|^2
|E|)$ when $k \leq \ell$. To this end, we employ the technique developed by
Faenza et al. recently that uses a randomized communication protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7286</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7286</id><created>2014-03-28</created><authors><author><keyname>Bose</keyname><forenames>Subhonmesh</forenames></author><author><keyname>Cai</keyname><forenames>Desmond</forenames></author><author><keyname>Low</keyname><forenames>Steven</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>The Role of a Market Maker in Networked Cournot Competition</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the role of a market maker (or market operator) in a transmission
constrained electricity market. We model the market as a one-shot networked
Cournot competition where generators supply quantity bids and load serving
entities provide downward sloping inverse demand functions. This mimics the
operation of a spot market in a deregulated market structure. In this paper, we
focus on possible mechanisms employed by the market maker to balance demand and
supply. In particular, we consider three candidate objective functions that the
market maker optimizes - social welfare, residual social welfare, and consumer
surplus. We characterize the existence of Generalized Nash Equilibrium (GNE) in
this setting and demonstrate that market outcomes at equilibrium can be very
different under the candidate objective functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7291</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7291</id><created>2014-03-28</created><authors><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Swarnalatha</forenames></author><author><keyname>Ambrose</keyname><forenames>Angelo</forenames></author></authors><title>Instruction-set Selection for Multi-application based ASIP Design: An
  Instruction-level Study</title><categories>cs.AR</categories><journal-ref>Information and Automation for Sustainability (ICIAfS), 2012 IEEE
  6th International Conference on, 27-29 Sept 2012, pp 141-146, Beijing</journal-ref><doi>10.1109/ICIAFS.2012.6419895</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficiency in embedded systems is paramount to achieve high performance while
consuming less area and power. Processors in embedded systems have to be
designed carefully to achieve such design constraints. Application Specific
Instruction set Processors (ASIPs) exploit the nature of applications to design
an optimal instruction set. Despite being not general to execute any
application, ASIPs are highly preferred in the embedded systems industry where
the devices are produced to satisfy a certain type of application domain/s
(either intra-domain or inter-domain). Typically, ASIPs are designed from a
base-processor and functionalities are added for applications. This paper
studies the multi-application ASIPs and their instruction sets, extensively
analysing the instructions for inter-domain and intra-domain designs. Metrics
analysed are the reusable instructions and the extra cost to add a certain
application. A wide range of applications from various application benchmarks
(MiBench, MediaBench and SPEC2006) and domains are analysed for two different
architectures (ARM-Thumb and PISA). Our study shows that the intra-domain
applications contain larger number of common instructions, whereas the
inter-domain applications have very less common instructions, regardless of the
architecture (and therefore the ISA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7292</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7292</id><created>2014-03-28</created><authors><author><keyname>Gupta</keyname><forenames>Arti</forenames></author><author><keyname>Deotale</keyname><forenames>Prof. N. T</forenames></author></authors><title>A Mining Method to Create Knowledge Map by Analysing the Data Resource</title><categories>cs.AI</categories><comments>6 pages,5 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and
  Technology(IJETT),V9(9),430-435 March 2014</journal-ref><doi>10.14445/22315381/IJETT-V9P282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental step in measuring the robustness of a system is the synthesis
of the so called Process Map.This is generally based on the user raw data
material.Process Maps are of fundamental importance towards the understanding
of the nature of a system in that they indicate which variables are causally
related and which are particularly important.This paper represent the system
Map or business structure map to understand business criteria studying the
various aspects of the company.The business structure map or knowledge map or
Process map are used to increase the growth of the company by giving some
useful measures according to the business criteria.This paper also deals with
the different company strategy to reduce the risk factors.Process Map is
helpful for building such knowledge successfully.Making decisions from such map
in a highly complex situation requires more knowledge and resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7293</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7293</id><created>2014-03-28</created><authors><author><keyname>Jayasinghe</keyname><forenames>D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Elkaduwe</keyname><forenames>D.</forenames></author></authors><title>Constant time encryption as a countermeasure against remote cache timing
  attacks</title><categories>cs.CR</categories><journal-ref>Information and Automation for Sustainability (ICIAfS), 2012 IEEE
  6th International Conference on, 27-29 Sept 2012, pp 129-134, Beijing</journal-ref><doi>10.1109/ICIAFS.2012.6419893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rijndael was standardized in 2001 by National Institute of Standard and
Technology as the Advanced Encryption Standard (AES). AES is still being used
to encrypt financial, military and even government confidential data. In 2005,
Bernstein illustrated a remote cache timing attack on AES using the
client-server architecture and therefore proved a side channel in its software
implementation. Over the years, a number of countermeasures have been proposed
against cache timing attacks both using hardware and software. Although the
software based countermeasures are flexible and easy to deploy, most of such
countermeasures are vulnerable to statistical analysis. In this paper, we
propose a novel software based countermeasure against cache timing attacks,
known as constant time encryption, which we believe is secure against
statistical analysis. The countermeasure we proposed performs rescheduling of
instructions such that the encryption rounds will consume constant time
independent of the cache hits and misses. Through experiments, we prove that
our countermeasure is secure against Bernstein's cache timing attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7294</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7294</id><created>2014-03-28</created><authors><author><keyname>Herath</keyname><forenames>D.</forenames></author><author><keyname>Lakmali</keyname><forenames>C.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Accelerating string matching for bio-computing applications on
  multi-core CPUs</title><categories>cs.DC</categories><journal-ref>Industrial and Information Systems (ICIIS), 2012 7th IEEE
  International Conference on, 6-9 Aug. 2012, pp. 1-6, Chennai</journal-ref><doi>10.1109/ICIInfS.2012.6304784</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Huge amount of data in the form of strings are being handled in bio-computing
applications and searching algorithms are quite frequently used in them. Many
methods utilizing on both software and hardware are being proposed to
accelerate processing of such data. The typical hardware-based acceleration
techniques either require special hardware such as general purpose graphics
processing units (GPGPUs) or need building a new hardware such as an FPGA based
design. On the other hard, software-based acceleration techniques are easier
since they only require some changes in the software code or the software
architecture. Typical software-based techniques make use of computers connected
over a network, also known as a network grid to accelerate the processing. In
this paper, we test the hypothesis that multi-core architectures should provide
better performance in this kind of computation, but still it would depend on
the algorithm selected as well as the programming model being utilized. We
present the acceleration of a string-searching algorithm on a multi-core CPU
via a POSIX thread based implementation. Our implementation on an 8-core
processor (that supports 16-threads) resulted in 9x throughput improvement
compared to a single thread implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7295</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7295</id><created>2014-03-28</created><authors><author><keyname>Barnes</keyname><forenames>A.</forenames></author><author><keyname>Fernando</keyname><forenames>R.</forenames></author><author><keyname>Mettananda</keyname><forenames>K.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Improving the throughput of the AES algorithm with multicore processors</title><categories>cs.DC cs.CR</categories><journal-ref>Industrial and Information Systems (ICIIS), 2012 7th IEEE
  International Conference on, 6-9 Aug. 2012, Chennai</journal-ref><doi>10.1109/ICIInfS.2012.6304791</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AES, Advanced Encryption Standard, can be considered the most widely used
modern symmetric key encryption standard. To encrypt/decrypt a file using the
AES algorithm, the file must undergo a set of complex computational steps.
Therefore a software implementation of AES algorithm would be slow and consume
large amount of time to complete. The immense increase of both stored and
transferred data in the recent years had made this problem even more daunting
when the need to encrypt/decrypt such data arises. As a solution to this
problem, in this paper, we present an extensive study of enhancing the
throughput of AES encryption algorithm by utilizing the state of the art
multicore architectures. We take a sequential program that implements the AES
algorithm and convert the same to run on multicore architectures with minimum
effort. We implement two different parallel programmes, one with the fork
system call in Linux and the other with the pthreads, the POSIX standard for
threads. Later, we ran both the versions of the parallel programs on different
multicore architectures and compared and analysed the throughputs between the
implementations and among different architectures. The pthreads implementation
outperformed in all the experiments we conducted and the best throughput
obtained is around 7Gbps on a 32-core processor (the largest number of cores we
had) with the pthreads implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7296</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7296</id><created>2014-03-28</created><authors><author><keyname>Vidanagamachchi</keyname><forenames>S. M.</forenames></author><author><keyname>Dewasurendra</keyname><forenames>S. D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Niranjan</keyname><forenames>M.</forenames></author></authors><title>Tile optimization for area in FPGA based hardware acceleration of
  peptide identification</title><categories>cs.CE</categories><journal-ref>Industrial and Information Systems (ICIIS), 2011 6th IEEE
  International Conference on, 16-19 Aug. 2011, pp. 140 - 145, Kandy</journal-ref><doi>10.1109/ICIINFS.2011.6038056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in life sciences over the last few decades have lead to the
generation of a huge amount of biological data. Computing research has become a
vital part in driving biological discovery where analysis and categorization of
biological data are involved. String matching algorithms can be applied for
protein/gene sequence matching and with the phenomenal increase in the size of
string databases to be analyzed, software implementations of these algorithms
seems to have hit a hard limit and hardware acceleration is increasingly being
sought. Several hardware platforms such as Field Programmable Gate Arrays
(FPGA), Graphics Processing Units (GPU) and Chip Multi Processors (CMP) are
being explored as hardware platforms. In this paper, we give a comprehensive
overview of the literature on hardware acceleration of string matching
algorithms, we take an FPGA hardware exploration and expedite the design time
by a design automation technique. Further, our design automation is also
optimized for better hardware utilization through optimizing the number of
peptides that can be represented in an FPGA tile. The results indicate
significant improvements in design time and hardware utilization which are
reported in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7297</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7297</id><created>2014-03-28</created><authors><author><keyname>Alawatugoda</keyname><forenames>Janaka</forenames></author><author><keyname>Jayasinghe</keyname><forenames>Darshana</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan</forenames></author></authors><title>Countermeasures against Bernstein's remote cache timing attack</title><categories>cs.CR</categories><journal-ref>Industrial and Information Systems (ICIIS), 2011 6th IEEE
  International Conference on, 16-19 Aug. 2011, pp. 43-48, Kandy</journal-ref><doi>10.1109/ICIINFS.2011.6038038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cache timing attack is a type of side channel attack where the leaking timing
information due to the cache behaviour of a crypto system is used by an
attacker to break the system. Advanced Encryption Standard (AES) was considered
a secure encryption standard until 2005 when Daniel Bernstein claimed that the
software implementation of AES is vulnerable to cache timing attack. Bernstein
demonstrated a remote cache timing attack on a software implementation of AES.
The original AES implementation can methodically be altered to prevent the
cache timing attack by hiding the natural cache-timing pattern during the
encryption while preserving its semantics. The alternations while preventing
the attack should not make the implementation very slow. In this paper, we
report outcomes of our experiments on designing and implementing a number of
possible countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7299</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7299</id><created>2014-03-28</created><authors><author><keyname>Nawinne</keyname><forenames>I. B.</forenames></author><author><keyname>Wickramasinghe</keyname><forenames>M. S.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>S.</forenames></author></authors><title>Heterogeneous processor pipeline for a product cipher application</title><categories>cs.AR</categories><journal-ref>Industrial and Information Systems (ICIIS), 2011 6th IEEE
  International Conference on, 16-19 Aug 2011, pp. 32 - 37, Kandy</journal-ref><doi>10.1109/ICIINFS.2011.6038036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processing data received as a stream is a task commonly performed by modern
embedded devices, in a wide range of applications such as multimedia
(encoding/decoding/ playing media), networking (switching and routing), digital
security, scientific data processing, etc. Such processing normally tends to be
calculation intensive and therefore requiring significant processing power.
Therefore, hardware acceleration methods to increase the performance of such
applications constitute an important area of study. In this paper, we present
an evaluation of one such method to process streaming data, namely
multi-processor pipeline architecture. The hardware is based on a
Multiple-Processor System on Chip (MPSoC), using a data encryption algorithm as
a case study. The algorithm is partitioned on a coarse grained level and mapped
on to an MPSoC with five processor cores in a pipeline, using specifically
configured Xtensa LX3 cores. The system is then selectively optimized by
strengthening and pruning the resources of each processor core. The optimized
system is evaluated and compared against an optimal single-processor System on
Chip (SoC) for the same application. The multiple-processor pipeline system for
data encryption algorithms used was observed to provide significant speed ups,
up to 4.45 times that of the single-processor system, which is close to the
ideal speed up from a five-stage pipeline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7308</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7308</id><created>2014-03-28</created><authors><author><keyname>Robnik-&#x160;ikonja</keyname><forenames>Marko</forenames></author></authors><title>Data generator based on RBF network</title><categories>stat.ML cs.AI cs.LG</categories><msc-class>62-07, 62H30, 97N80, 65C10</msc-class><acm-class>I.2.6; I.5.2; I.6.5; G.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are plenty of problems where the data available is scarce and
expensive. We propose a generator of semi-artificial data with similar
properties to the original data which enables development and testing of
different data mining algorithms and optimization of their parameters. The
generated data allow a large scale experimentation and simulations without
danger of overfitting. The proposed generator is based on RBF networks which
learn sets of Gaussian kernels. Learned Gaussian kernels can be used in a
generative mode to generate the data from the same distributions. To asses
quality of the generated data we developed several workflows and used them to
evaluate the statistical properties of the generated data, structural
similarity, and predictive similarity using supervised and unsupervised
learning techniques. To determine usability of the proposed generator we
conducted a large scale evaluation using 51 UCI data sets. The results show a
considerable similarity between the original and generated data and indicate
that the method can be useful in several development and simulation scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7311</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7311</id><created>2014-03-28</created><authors><author><keyname>Khan</keyname><forenames>Akbar</forenames></author><author><keyname>L</keyname><forenames>Pratap Reddy</forenames></author></authors><title>Performance Evaluation of Raster Based Shape Vectors in Object
  Recognition</title><categories>cs.CV</categories><comments>11pages,12 figures</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V9(8),378-388 March 2014. ISSN:2231-5381. www.ijettjournal.org.
  published by seventh sense research group</journal-ref><doi>10.14445/22315381/IJETT-V9P274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object recognition is still an impediment in the field of computer vision and
multimedia retrieval.Defining an object model is a critical task. Shape
information of an object play a critical role in the process of object
recognition. Extraction of boundary information of an object from the
multimedia data and classifying this information with associated objects is the
primary step towards object recognition. Rasters play an important role while
computing object boundary. The trade-off lies with the dimensionality of the
object versus computational cost while achieving maximum efficiency. In this
treatise an attempt is made to evaluate the performance of circular and spiral
raster models in terms of average retrieval efficiency and computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7315</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7315</id><created>2014-03-28</created><authors><author><keyname>Li</keyname><forenames>Yitong</forenames></author><author><keyname>Shi</keyname><forenames>Chuan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Chen</keyname><forenames>Qing</forenames></author></authors><title>HRank: A Path based Ranking Framework in Heterogeneous Information
  Network</title><categories>cs.IR</categories><comments>12 pages, 11 figures</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there is a surge of interests on heterogeneous information network
analysis. As a newly emerging network model, heterogeneous information networks
have many unique features (e.g., complex structure and rich semantics) and a
number of interesting data mining tasks have been exploited in this kind of
networks, such as similarity measure, clustering, and classification. Although
evaluating the importance of objects has been well studied in homogeneous
networks, it is not yet exploited in heterogeneous networks. In this paper, we
study the ranking problem in heterogeneous networks and propose the HRank
framework to evaluate the importance of multiple types of objects and meta
paths. Since the importance of objects depends upon the meta paths in
heterogeneous networks, HRank develops a path based random walk process.
Moreover, a constrained meta path is proposed to subtly capture the rich
semantics in heterogeneous networks. Furthermore, HRank can simultaneously
determine the importance of objects and meta paths through applying the tensor
analysis. Extensive experiments on three real datasets show that HRank can
effectively evaluate the importance of objects and paths together. Moreover,
the constrained meta path shows its potential on mining subtle semantics by
obtaining more accurate ranking results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7317</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7317</id><created>2014-03-28</created><updated>2014-05-15</updated><authors><author><keyname>Altieri</keyname><forenames>Andres</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia G.</forenames></author></authors><title>On the Outage Probability of the Full-Duplex Interference-Limited Relay
  Channel</title><categories>cs.IT math.IT</categories><comments>30 pages, 4 figures. Final version. To appear in IEEE JSAC Special
  Issue on Full-duplex Wireless Communications and Networks, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance, in terms of the asymptotic error
probability, of a user which communicates with a destination with the aid of a
full-duplex in-band relay. We consider that the network is
interference-limited, and interfering users are distributed as a Poisson point
process. In this case, the asymptotic error probability is upper bounded by the
outage probability (OP). We investigate the outage behavior for well-known
cooperative schemes, namely, decode-and-forward (DF) and compress-and-forward
(CF) considering fading and path loss. For DF we determine the exact OP and
develop upper bounds which are tight in typical operating conditions. Also, we
find the correlation coefficient between source and relay signals which
minimizes the OP when the density of interferers is small. For CF, the
achievable rates are determined by the spatial correlation of the
interferences, and a straightforward analysis isn't possible. To handle this
issue, we show the rate with correlated noises is at most one bit worse than
with uncorrelated noises, and thus find an upper bound on the performance of
CF. These results are useful to evaluate the performance and to optimize
relaying schemes in the context of full-duplex wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7321</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7321</id><created>2014-03-28</created><authors><author><keyname>Valmadre</keyname><forenames>Jack</forenames></author><author><keyname>Sridharan</keyname><forenames>Sridha</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Learning detectors quickly using structured covariance matrices</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision is increasingly becoming interested in the rapid estimation
of object detectors. Canonical hard negative mining strategies are slow as they
require multiple passes of the large negative training set. Recent work has
demonstrated that if the distribution of negative examples is assumed to be
stationary, then Linear Discriminant Analysis (LDA) can learn comparable
detectors without ever revisiting the negative set. Even with this insight,
however, the time to learn a single object detector can still be on the order
of tens of seconds on a modern desktop computer. This paper proposes to
leverage the resulting structured covariance matrix to obtain detectors with
identical performance in orders of magnitude less time and memory. We elucidate
an important connection to the correlation filter literature, demonstrating
that these can also be trained without ever revisiting the negative set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7322</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7322</id><created>2014-03-28</created><updated>2014-03-31</updated><authors><author><keyname>Jiao</keyname><forenames>Chunxu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhu</keyname><forenames>Liangliang</forenames></author></authors><title>Exploiting Delay Correlation for Multi-Antenna-Assisted High Speed Train
  Communications</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In High Speed Train Communications (HSTC), the most challenging issue is
coping with the extremely fast fading channel. Compared with its static
counterpart, channel estimation on the move consumes excessive energy and
spectrum to achieve similar performance. To address this issue, we exploit the
delay correlation inherent in the linear spatial-temporal structure of
multi-antenna array, based on which the rapid fading channel may be
approximated by a virtual slow-fading channel. Subsequently, error probability
and spectral efficiency are re-examined for this staticized channel. In
particular, we formulate the quantitative tradeoff between the two metrics of
interest, by adjusting the pilot percentage in each frame. Numerical results
verify the good performance of the proposed scheme and elucidate the tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7335</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7335</id><created>2014-03-28</created><authors><author><keyname>Tang</keyname><forenames>Duyu</forenames></author><author><keyname>Qin</keyname><forenames>Bing</forenames></author><author><keyname>Liu</keyname><forenames>Ting</forenames></author><author><keyname>Shi</keyname><forenames>Qiuhui</forenames></author></authors><title>Emotion Analysis Platform on Chinese Microblog</title><categories>cs.CL cs.CY cs.IR</categories><comments>11 pages, 6 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Weibo, as the largest social media service in China, has billions of messages
generated every day. The huge number of messages contain rich sentimental
information. In order to analyze the emotional changes in accordance with time
and space, this paper presents an Emotion Analysis Platform (EAP), which
explores the emotional distribution of each province, so that can monitor the
global pulse of each province in China. The massive data of Weibo and the
real-time requirements make the building of EAP challenging. In order to solve
the above problems, emoticons, emotion lexicon and emotion-shifting rules are
adopted in EAP to analyze the emotion of each tweet. In order to verify the
effectiveness of the platform, case study on the Sichuan earthquake is done,
and the analysis result of the platform accords with the fact. In order to
analyze from quantity, we manually annotate a test set and conduct experiment
on it. The experimental results show that the macro-Precision of EAP reaches
80% and the EAP works effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7343</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7343</id><created>2014-03-26</created><updated>2014-09-02</updated><authors><author><keyname>Lachish</keyname><forenames>Oded</forenames></author></authors><title>O(log log rank) Competitive-Ratio for the Matroid Secretary Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the \textit{Matroid Secretary Problem} (MSP), the elements of the ground
set of a Matroid are revealed on-line one by one, each together with its value.
An algorithm for the MSP is \textit{Matroid-Unknown} if, at every stage of its
execution: (i) it only knows the elements that have been revealed so far and
their values, and (ii) it has access to an oracle for testing whether or not
any subset of the elements that have been revealed so far is an independent
set. An algorithm is \textit{Known-Cardinality} if, in addition to (i) and
(ii), it also initially knows the cardinality of the ground set of the Matroid.
We present here a Known-Cardinality and \textit{Order-Oblivious} algorithm
that, with constant probability, selects an independent set of elements, whose
value is at least the optimal value divided by $O(\log{\log{\rho}})$, where
$\rho$ is the rank of the Matroid; that is, the algorithm has a
\textit{competitive-ratio} of $O(\log{\log{\rho}})$. The best previous results
for a Known-Cardinality algorithm are a competitive-ratio of $O(\log{\rho})$,
by Babaioff \textit{et al.} (2007), and a competitive-ratio of
$O(\sqrt{\log{\rho}})$, by Chakraborty and Lachish (2012). In many non-trivial
cases the algorithm we present has a competitive-ratio that is better than the
$O(\log{\log{\rho}})$. The cases in which it fails to do so are easily
characterized. Understanding these cases may lead to improved algorithms for
the problem or, conversely, to non-trivial lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7344</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7344</id><created>2014-03-27</created><authors><author><keyname>Dash</keyname><forenames>Kailash Chandra</forenames></author><author><keyname>Mishra</keyname><forenames>Umakant</forenames></author></authors><title>Critical Considerations for Developing MIS for NGOs</title><categories>cs.CY</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Information Systems and Information Technology (IS &amp; IT) has become
a major driving force for many of the current day organizations, the NGOs have
not been able to utilize the benefits up to a satisfactory level. Most
organizations use standard office tools to manage huge amount for field data
and never feel the need for a central repository of data. While many people
argue that an NGO should not spend too much money on information management, it
is a fact that organizing the information requires more of a mindset and an
organized behavior than a huge financial investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7347</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7347</id><created>2014-03-28</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Axiomatization of Finite Algebras</title><categories>cs.LO cs.FL</categories><comments>14 pages, 5 figures, author address given in header is meanwhile
  outdated</comments><msc-class>68Q45, 68T15</msc-class><acm-class>F.4.1; F.4.3; I.2.3</acm-class><journal-ref>M. Jarke, J. Koehler, G. Lakemeyer (eds.), Proceedings of the 25th
  Annual German Conference on Artificial Intelligence, Springer Lecture Notes
  in Artificial Intelligence (LNAI), Vol.2479, ISBN 3-540-44185-9, p.222-234,
  Sep 2002</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the set of all formulas in n variables valid in a finite class A
of finite algebras is always a regular tree language, and compute a finite
axiom set for A. We give a rational reconstruction of Barzdins' liquid flow
algorithm (Barzdin+Barzdin, 1991). We show a sufficient condition for the
existence of a class A of prototype algebras for a given theory T. Such a set
allows us to prove T |= p simply by testing whether p holds in A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7350</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7350</id><created>2014-03-28</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Slime mould electronic oscillators</title><categories>cs.ET physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct electronic oscillator from acellular slime mould Physarum
polycephalum. The slime mould oscillator is made of two electrodes connected by
a protoplasmic tube of the living slime mould. A protoplasmic tube has an
average resistance of 3~MOhm. The tube's resistance is changing over time due
to peristaltic contractile activity of the tube. The resistance of the
protoplasmic tube oscillates with average period of 73~sec and average
amplitude of 0.6~MOhm. We present experimental laboratory results on dynamics
of Physarum oscillator under direct current voltage up to 15~V and speculate
that slime mould P. polycephalum can be employed as a living electrical
oscillator in biological and hybrid circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7353</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7353</id><created>2014-03-28</created><authors><author><keyname>Weller</keyname><forenames>Daniel</forenames></author></authors><title>Exposition: Synthesis via Functional Interpretation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this short paper is to give a practical introduction to functional
interpretation of proofs for computer scientists interested in synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7365</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7365</id><created>2014-03-28</created><authors><author><keyname>Estrela</keyname><forenames>Vania Vieira</forenames></author><author><keyname>Bassani</keyname><forenames>Marcos Henrique da Silva</forenames></author></authors><title>Expectation-Maximization Technique and Spatial-Adaptation Applied to
  Pel-Recursive Motion Estimation</title><categories>cs.CV</categories><comments>6 pages, pp. 204-209, Proceedings of the 8th World Multi-Conference
  on Systemics, Cybernetics and Informatics, Volume XVI, Organized by the
  International Institute of Informatics and Systemics, International
  Federation of Systems Research: IFSR, Edited by Nagib Callaos, Maria Sanchez,
  and Juan M. Pineda, TIB/UB Hannover, ISSN 12615810X, July 18-21, 2004,
  Orlando, Florida, USA</comments><journal-ref>ISSN 12615810X, 2004</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Pel-recursive motion estimation isa well-established approach. However, in
the presence of noise, it becomes an ill-posed problem that requires
regularization. In this paper, motion vectors are estimated in an iterative
fashion by means of the Expectation-Maximization (EM) algorithm and a Gaussian
data model. Our proposed algorithm also utilizes the local image properties of
the scene to improve the motion vector estimates following a spatially adaptive
approach. Numerical experiments are presented that demonstrate the merits of
our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7371</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7371</id><created>2014-03-28</created><authors><author><keyname>Krylov</keyname><forenames>Vladimir</forenames></author><author><keyname>Kravtsov</keyname><forenames>Kirill</forenames></author></authors><title>DDoS Attack and Interception Resistance IP Fast Hopping Based Protocol</title><categories>cs.NI cs.CR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denial-of-Service attacks continue to be a serious problem for the Internet
community despite the fact that a large number of defense approaches has been
proposed by the research community. In this paper we introduce IP Fast Hopping,
easily deployable and effective network-layer architecture against DDoS
attacks. Our approach also provides an easy way for clients to hide content and
destination server of theirs communication sessions. We describe a method of
dynamic server IP address change and all modules necessary to implement the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7373</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7373</id><created>2014-03-28</created><authors><author><keyname>Pel&#xe1;nek</keyname><forenames>Radek</forenames></author></authors><title>Difficulty Rating of Sudoku Puzzles: An Overview and Evaluation</title><categories>cs.AI</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we predict the difficulty of a Sudoku puzzle? We give an overview of
difficulty rating metrics and evaluate them on extensive dataset on human
problem solving (more then 1700 Sudoku puzzles, hundreds of solvers). The best
results are obtained using a computational model of human solving activity.
Using the model we show that there are two sources of the problem difficulty:
complexity of individual steps (logic operations) and structure of dependency
among steps. We also describe metrics based on analysis of solutions under
relaxed constraints -- a novel approach inspired by phase transition phenomenon
in the graph coloring problem. In our discussion we focus not just on the
performance of individual metrics on the Sudoku puzzle, but also on their
generalizability and applicability to other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7374</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7374</id><created>2014-03-27</created><authors><author><keyname>Qiu</keyname><forenames>Song</forenames></author></authors><title>Molecular Communication Systems Design for Future City</title><categories>cs.ET cs.NI</categories><comments>PhD report. arXiv admin note: text overlap with arXiv:1310.0070 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An area of interest in the modern age is the human migration from rural areas
to cities. Cities are characterized by a dense concentration of buildings and
key infrastructures. However, what has been lacking is a pervasive sensor
technology that can monitor the performance of these structures. Partly, this
is due to the fact that the information collected from sensors cannot be easily
transported from the embedded location to an external data hub. Examples of
health monitoring in structures include monitoring corrosion, fracture stress,
and material delamination. The scenario examples include: pipelines, tunnel
networks, and some industrial and medical areas such as hospitals (minimise
electromagnetic interference), and turbines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7380</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7380</id><created>2014-03-28</created><authors><author><keyname>Kavvadias</keyname><forenames>Nikolaos</forenames></author></authors><title>Generating and evaluating application-specific hardware extensions</title><categories>cs.AR</categories><comments>11 pages, 15 figures, 5 tables. An unpublished journal paper
  presenting the YARDstick custom instruction generation environment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern platform-based design involves the application-specific extension of
embedded processors to fit customer requirements. To accomplish this task, the
possibilities offered by recent custom/extensible processors for tuning their
instruction set and microarchitecture to the applications of interest have to
be exploited. A significant factor often determining the success of this
process is the utomation available in application analysis and custom
instruction generation.
  In this paper we present YARDstick, a design automation tool for custom
processor development flows that focuses on generating and evaluating
application-specific hardware extensions. YARDstick is a building block for
ASIP development, integrating application analysis, custom instruction
generation and selection with user-defined compiler intermediate
representations. In a YARDstick-enabled environment, practical issues in
traditional ASIP design are confronted efficiently; the exploration
infrastructure is liberated from compiler and simulator idiosyncrasies, since
the ASIP designer is empowered with the freedom of specifying the target
architectures of choice and adding new implementations of analyses and custom
instruction generation/selection methods. To illustrate the capabilities of the
YARDstick approach, we present interesting exploration scenarios: quantifying
the effect of machine-dependent compiler optimizations and the selection of the
target architecture in terms of operation set and memory model on custom
instruction generation/selection under different input/output constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7394</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7394</id><created>2014-03-28</created><authors><author><keyname>Rose</keyname><forenames>Dillon Mark</forenames></author><author><keyname>Rouly</keyname><forenames>Jean Michel</forenames></author><author><keyname>Haber</keyname><forenames>Rana</forenames></author><author><keyname>Mijatovic</keyname><forenames>Nenad</forenames></author><author><keyname>Peter</keyname><forenames>Adrian M.</forenames></author></authors><title>Parallel Hierarchical Affinity Propagation with MapReduce</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accelerated evolution and explosion of the Internet and social media is
generating voluminous quantities of data (on zettabyte scales). Paramount
amongst the desires to manipulate and extract actionable intelligence from vast
big data volumes is the need for scalable, performance-conscious analytics
algorithms. To directly address this need, we propose a novel MapReduce
implementation of the exemplar-based clustering algorithm known as Affinity
Propagation. Our parallelization strategy extends to the multilevel
Hierarchical Affinity Propagation algorithm and enables tiered aggregation of
unstructured data with minimal free parameters, in principle requiring only a
similarity measure between data points. We detail the linear run-time
complexity of our approach, overcoming the limiting quadratic complexity of the
original algorithm. Experimental validation of our clustering methodology on a
variety of synthetic and real data sets (e.g. images and point data)
demonstrates our competitiveness against other state-of-the-art MapReduce
clustering techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7400</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7400</id><created>2014-03-28</created><updated>2014-04-15</updated><authors><author><keyname>Tufekci</keyname><forenames>Zeynep</forenames></author></authors><title>Big Questions for Social Media Big Data: Representativeness, Validity
  and Other Methodological Pitfalls</title><categories>cs.SI physics.soc-ph</categories><comments>Tufekci, Zeynep. (2014). Big Questions for Social Media Big Data:
  Representativeness, Validity and Other Methodological Pitfalls. In ICWSM '14:
  Proceedings of the 8th International AAAI Conference on Weblogs and Social
  Media, 2014. [forthcoming]</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Large-scale databases of human activity in social media have captured
scientific and policy attention, producing a flood of research and discussion.
This paper considers methodological and conceptual challenges for this emergent
field, with special attention to the validity and representativeness of social
media big data analyses. Persistent issues include the over-emphasis of a
single platform, Twitter, sampling biases arising from selection by hashtags,
and vague and unrepresentative sampling frames. The socio-cultural complexity
of user behavior aimed at algorithmic invisibility (such as subtweeting,
mock-retweeting, use of &quot;screen captures&quot; for text, etc.) further complicate
interpretation of big data social media. Other challenges include accounting
for field effects, i.e. broadly consequential events that do not diffuse only
through the network under study but affect the whole society. The application
of network methods from other fields to the study of human social activity may
not always be appropriate. The paper concludes with a call to action on
practical steps to improve our analytic capacity in this promising,
rapidly-growing field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7421</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7421</id><created>2014-03-21</created><authors><author><keyname>Saket</keyname><forenames>Bahador</forenames></author><author><keyname>Simonetto</keyname><forenames>Paolo</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author></authors><title>Group-Level Graph Visualization Taxonomy</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task taxonomies for graph and network visualizations focus on tasks commonly
encountered when analyzing graph connectivity and topology. However, in many
application fields such as the social sciences (social networks), biology
(protein interaction models), software engineering (program call graphs),
connectivity and topology information is intertwined with group, clustering,
and hierarchical information. Several recent visualization techniques, such as
BubbleSets, LineSets and GMap, make explicit use of grouping and clustering,
but evaluating such visualization has been difficult due to the lack of
standardized group-level tasks. With this in mind, our goal is to define a new
set of tasks that assess group-level comprehension. We propose several types of
group-level tasks and provide several examples of each type. Finally, we
characterize some of the proposed tasks using the multi-level typology of
abstract visualization tasks. We believe that adding group-level tasks to the
task taxonomy for graph visualization would make the taxonomy more useful for
the recent graph visualization techniques. It would help evaluators define and
categorize new tasks, and it would help generalize individual results collected
in controlled experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7426</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7426</id><created>2014-03-28</created><authors><author><keyname>Georgievski</keyname><forenames>Ilche</forenames></author><author><keyname>Aiello</keyname><forenames>Marco</forenames></author></authors><title>An Overview of Hierarchical Task Network Planning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchies are the most common structure used to understand the world
better. In galaxies, for instance, multiple-star systems are organised in a
hierarchical system. Then, governmental and company organisations are
structured using a hierarchy, while the Internet, which is used on a daily
basis, has a space of domain names arranged hierarchically. Since Artificial
Intelligence (AI) planning portrays information about the world and reasons to
solve some of world's problems, Hierarchical Task Network (HTN) planning has
been introduced almost 40 years ago to represent and deal with hierarchies. Its
requirement for rich domain knowledge to characterise the world enables HTN
planning to be very useful, but also to perform well. However, the history of
almost 40 years obfuscates the current understanding of HTN planning in terms
of accomplishments, planning models, similarities and differences among
hierarchical planners, and its current and objective image. On top of these
issues, attention attracts the ability of hierarchical planning to truly cope
with the requirements of applications from the real world. We propose a
framework-based approach to remedy this situation. First, we provide a basis
for defining different formal models of hierarchical planning, and define two
models that comprise a large portion of HTN planners. Second, we provide a set
of concepts that helps to interpret HTN planners from the aspect of their
search space. Then, we analyse and compare the planners based on a variety of
properties organised in five segments, namely domain authoring, expressiveness,
competence, performance and applicability. Furthermore, we select Web service
composition as a real-world and current application, and classify and compare
the approaches that employ HTN planning to solve the problem of service
composition. Finally, we conclude with our findings and present directions for
future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7428</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7428</id><created>2014-03-28</created><authors><author><keyname>Ianovski</keyname><forenames>Egor</forenames></author></authors><title>DValue for Boolean games is EXP-complete</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the following problem is EXP-complete: given a rational v and a
two player, zero-sum Boolean game G determine whether the value of G is at
least v. The proof is via a translation of the proof of the same result for
Boolean circuit games in Feigenbaum et al. (1995).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7429</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7429</id><created>2014-03-28</created><authors><author><keyname>Pan</keyname><forenames>Wei</forenames></author><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author></authors><title>Distributed Reconstruction of Nonlinear Networks: An ADMM Approach</title><categories>math.OC cs.DC cs.LG cs.SY</categories><comments>To appear in the Preprints of 19th IFAC World Congress 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a distributed algorithm for the reconstruction of
large-scale nonlinear networks. In particular, we focus on the identification
from time-series data of the nonlinear functional forms and associated
parameters of large-scale nonlinear networks. Recently, a nonlinear network
reconstruction problem was formulated as a nonconvex optimisation problem based
on the combination of a marginal likelihood maximisation procedure with
sparsity inducing priors. Using a convex-concave procedure (CCCP), an iterative
reweighted lasso algorithm was derived to solve the initial nonconvex
optimisation problem. By exploiting the structure of the objective function of
this reweighted lasso algorithm, a distributed algorithm can be designed. To
this end, we apply the alternating direction method of multipliers (ADMM) to
decompose the original problem into several subproblems. To illustrate the
effectiveness of the proposed methods, we use our approach to identify a
network of interconnected Kuramoto oscillators with different network sizes
(500~100,000 nodes).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7455</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7455</id><created>2014-03-28</created><authors><author><keyname>Mathur</keyname><forenames>Shruti</forenames></author><author><keyname>Saxena</keyname><forenames>Varun Prakash</forenames></author></authors><title>Hybrid Approach to English-Hindi Name Entity Transliteration</title><categories>cs.CL</categories><comments>Proceedings of IEEE Students' Conference on Electrical, Electronics
  and Computer Sciences 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine translation (MT) research in Indian languages is still in its
infancy. Not much work has been done in proper transliteration of name entities
in this domain. In this paper we address this issue. We have used English-Hindi
language pair for our experiments and have used a hybrid approach. At first we
have processed English words using a rule based approach which extracts
individual phonemes from the words and then we have applied statistical
approach which converts the English into its equivalent Hindi phoneme and in
turn the corresponding Hindi word. Through this approach we have attained
83.40% accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7458</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7458</id><created>2014-03-28</created><updated>2015-10-20</updated><authors><author><keyname>Bock</keyname><forenames>Nicolas</forenames></author><author><keyname>Challacombe</keyname><forenames>Matt</forenames></author><author><keyname>Kal&#xe9;</keyname><forenames>Laxmikant V.</forenames></author></authors><title>Solvers for $\mathcal{O} (N)$ Electronic Structure in the Strong Scaling
  Limit</title><categories>cs.NA</categories><comments>Presented at the 12th Annual Workshop on Charm++ and its Applications</comments><report-no>LA-UR-14-22050</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hybrid OpenMP/Charm++ framework for solving the $\mathcal{O}
(N)$ Self-Consistent-Field eigenvalue problem with parallelism in the strong
scaling regime, $P\gg{N}$, where $P$ is the number of cores, and $N$ a measure
of system size, i.e. the number of matrix rows/columns, basis functions, atoms,
molecules, etc. This result is achieved with a nested approach to Spectral
Projection and the Sparse Approximate Matrix Multiply [Bock and Challacombe,
SIAM J.~Sci.~Comput. 35 C72, 2013], and involves a recursive, task-parallel
algorithm, often employed by generalized $N$-Body solvers, to occlusion and
culling of negligible products in the case of matrices with decay. Employing
classic technologies associated with generalized $N$-Body solvers, including
over-decomposition, recursive task parallelism, orderings that preserve
locality, and persistence-based load balancing, we obtain scaling beyond
hundreds of cores per molecule for small water clusters ([H${}_2$O]${}_N$, $N
\in \{ 30, 90, 150 \}$, $P/N \approx \{ 819, 273, 164 \}$) and find support for
an increasingly strong scalability with increasing system size $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7465</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7465</id><created>2014-03-28</created><authors><author><keyname>Mathur</keyname><forenames>Iti</forenames></author><author><keyname>Joshi</keyname><forenames>Nisheeth</forenames></author><author><keyname>Darbari</keyname><forenames>Hemant</forenames></author><author><keyname>Kumar</keyname><forenames>Ajai</forenames></author></authors><title>Shiva: A Framework for Graph Based Ontology Matching</title><categories>cs.AI</categories><journal-ref>International Journal of Computer Applications 89(11):30-34, March
  2014</journal-ref><doi>10.5120/15678-4435</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since long, corporations are looking for knowledge sources which can provide
structured description of data and can focus on meaning and shared
understanding. Structures which can facilitate open world assumptions and can
be flexible enough to incorporate and recognize more than one name for an
entity. A source whose major purpose is to facilitate human communication and
interoperability. Clearly, databases fail to provide these features and
ontologies have emerged as an alternative choice, but corporations working on
same domain tend to make different ontologies. The problem occurs when they
want to share their data/knowledge. Thus we need tools to merge ontologies into
one. This task is termed as ontology matching. This is an emerging area and
still we have to go a long way in having an ideal matcher which can produce
good results. In this paper we have shown a framework to matching ontologies
using graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7471</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7471</id><created>2014-03-28</created><updated>2014-06-12</updated><authors><author><keyname>Campbell</keyname><forenames>Trevor</forenames></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author></authors><title>Approximate Decentralized Bayesian Inference</title><categories>cs.LG</categories><comments>This paper was presented at UAI 2014. Please use the following BibTeX
  citation: @inproceedings{Campbell14_UAI, Author = {Trevor Campbell and
  Jonathan P. How}, Title = {Approximate Decentralized Bayesian Inference},
  Booktitle = {Uncertainty in Artificial Intelligence (UAI)}, Year = {2014}}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approximate method for performing Bayesian inference
in models with conditional independence over a decentralized network of
learning agents. The method first employs variational inference on each
individual learning agent to generate a local approximate posterior, the agents
transmit their local posteriors to other agents in the network, and finally
each agent combines its set of received local posteriors. The key insight in
this work is that, for many Bayesian models, approximate inference schemes
destroy symmetry and dependencies in the model that are crucial to the correct
application of Bayes' rule when combining the local posteriors. The proposed
method addresses this issue by including an additional optimization step in the
combination procedure that accounts for these broken dependencies. Experiments
on synthetic and real data demonstrate that the decentralized method provides
advantages in computational performance and predictive test likelihood over
previous batch and distributed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7481</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7481</id><created>2014-03-28</created><authors><author><keyname>Danek</keyname><forenames>Agnieszka</forenames></author><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>Indexing large genome collections on a PC</title><categories>cs.CE q-bio.GN q-bio.QM</categories><doi>10.1371/journal.pone.0109384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: The availability of thousands of invidual genomes of one species
should boost rapid progress in personalized medicine or understanding of the
interaction between genotype and phenotype, to name a few applications. A key
operation useful in such analyses is aligning sequencing reads against a
collection of genomes, which is costly with the use of existing algorithms due
to their large memory requirements.
  Results: We present MuGI, Multiple Genome Index, which reports all
occurrences of a given pattern, in exact and approximate matching model,
against a collection of thousand(s) genomes. Its unique feature is the small
index size fitting in a standard computer with 16--32\,GB, or even 8\,GB, of
RAM, for the 1000GP collection of 1092 diploid human genomes. The solution is
also fast. For example, the exact matching queries are handled in average time
of 39\,$\mu$s and with up to 3 mismatches in 373\,$\mu$s on the test PC with
the index size of 13.4\,GB. For a smaller index, occupying 7.4\,GB in memory,
the respective times grow to 76\,$\mu$s and 917\,$\mu$s.
  Availability: Software and Suuplementary material:
\url{http://sun.aei.polsl.pl/mugi}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7495</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7495</id><created>2014-03-28</created><authors><author><keyname>Gastineau</keyname><forenames>Nicolas</forenames><affiliation>Le2i, LIRIS</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>S-Packing Colorings of Cubic Graphs</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a non-decreasing sequence $S=(s_1,s_2, \ldots, s_k)$ of positive
integers, an {\em $S$-packing coloring} of a graph $G$ is a mapping $c$ from
$V(G)$ to $\{s_1,s_2, \ldots, s_k\}$ such that any two vertices with color
$s_i$ are at mutual distance greater than $s_i$, $1\le i\le k$. This paper
studies $S$-packing colorings of (sub)cubic graphs. We prove that subcubic
graphs are $(1,2,2,2,2,2,2)$-packing colorable and $(1,1,2,2,3)$-packing
colorable. For subdivisions of subcubic graphs we derive sharper bounds, and we
provide an example of a cubic graph of order $38$ which is not
$(1,2,\ldots,12)$-packing colorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7519</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7519</id><created>2014-03-28</created><authors><author><keyname>Kalaitzis</keyname><forenames>Christos</forenames></author><author><keyname>Mcadry</keyname><forenames>Aleksander</forenames></author><author><keyname>Newman</keyname><forenames>Alantha</forenames></author><author><keyname>Pol&#xe1;&#x10d;ek</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>On the Configuration LP for Maximum Budgeted Allocation</title><categories>cs.DS</categories><comments>29 pages, 4 figures. To appear in the 17th Conference on Integer
  Programming and Combinatorial Optimization (IPCO), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Maximum Budgeted Allocation problem, i.e., the problem of
selling a set of $m$ indivisible goods to $n$ players, each with a separate
budget, such that we maximize the collected revenue. Since the natural
assignment LP is known to have an integrality gap of $\frac{3}{4}$, which
matches the best known approximation algorithms, our main focus is to improve
our understanding of the stronger configuration LP relaxation. In this
direction, we prove that the integrality gap of the configuration LP is
strictly better than $\frac{3}{4}$, and provide corresponding polynomial time
roundings, in the following restrictions of the problem: (i) the Restricted
Budgeted Allocation problem, in which all the players have the same budget and
every item has the same value for any player it can be sold to, and (ii) the
graph MBA problem, in which an item can be assigned to at most 2 players.
Finally, we improve the best known upper bound on the integrality gap for the
general case from $\frac{5}{6}$ to $2\sqrt{2}-2\approx 0.828$ and also prove
hardness of approximation results for both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7532</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7532</id><created>2014-03-28</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mahmoud H.</forenames></author><author><keyname>Tawfik</keyname><forenames>Hazim</forenames></author></authors><title>Opportunistic Spectrum Sharing using Dumb Basis Patterns: The
  Line-of-Sight Interference Scenario</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a spectrum-sharing system with non-severely faded mutual
interference links, where both the secondary-to-primary and
primary-to-secondary channels have a Line-of-Sight (LoS) component. Based on a
Rician model for the LoS channels, we show, analytically and numerically, that
LoS interference hinders the achievable secondary user capacity. This is caused
by the poor dynamic range of the interference channels fluctuations when a
dominant LoS component exists. In order to improve the capacity of such system,
we propose the usage of an Electronically Steerable Parasitic Array Radiator
(ESPAR) antenna at the secondary terminals. An ESPAR antenna requires a single
RF chain and has a reconfigurable radiation pattern that is controlled by
assigning arbitrary weights to M orthonormal basis radiation patterns. By
viewing these orthonormal patterns as multiple virtual dumb antennas, we
randomly vary their weights over time creating artificial channel fluctuations
that can perfectly eliminate the undesired impact of LoS interference. Because
the proposed scheme uses a single RF chain, it is well suited for compact and
low cost mobile terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7536</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7536</id><created>2014-03-28</created><updated>2015-02-12</updated><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>A Mirage of Market Allocation</title><categories>cs.GT</categories><report-no>Hebrew University of Jerusalem Center for the Study of Rationality
  discussion paper 663</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can noncooperative behaviour of merchants lead to a market split that prima
facie seems anticompetitive? We introduce a model in which service providers,
with ISPs being the main example, aim at optimizing the number of customers
using their services, while customers aim at choosing service providers with
low customer load (high bandwidth per subscriber, for ISPs). Each service
provider chooses between a variety of levels of service (latencies, for ISPs),
and as long as it does not lose customers, aims at minimizing its level of
service; the minimum level of service required to satisfy a customer varies
across customers. We consider a two-stage competition: in the first stage,
service providers select their levels of service; in the second stage,
customers choose between service providers. In the two-stage game, we show that
the competition among service providers possesses a unique Nash equilibrium,
which is moreover super-strong; we also show that sequential better-response
dynamics of service providers reach this equilibrium, with best-response
dynamics doing so surprisingly fast. If service providers choose their levels
of service according to this equilibrium, then the unique Nash equilibrium
among customers in the second phase is a split of the market between the
service providers, based on the customers' minimum acceptable quality of
service; moreover, each service provider's chosen level of service is the
lowest acceptable by the entirety of its market slice, seemingly making no
attempt to attract other customers. Our results show that this prima facie
market allocation (collusive split of the market) arises as the unique and
highly robust outcome of noncooperative, even myopic, service-provider
behaviour. These results are applicable to a wide variety of scenarios, from
explaining phenomena observable in food markets, to shedding a surprising light
on aspects of location theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7540</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7540</id><created>2014-03-28</created><updated>2014-12-19</updated><authors><author><keyname>Lehtonen</keyname><forenames>Erkko</forenames></author><author><keyname>Marichal</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Teheux</keyname><forenames>Bruno</forenames></author></authors><title>Associative string functions</title><categories>math.GR cs.DM math.RA</categories><msc-class>20M05, 20M32, 39B72, 68R99</msc-class><journal-ref>Asian-European Journal of Mathematics 7 (4) (2014) 1450059 (18
  pages)</journal-ref><doi>10.1142/S1793557114500594</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of associativity for string functions, where a
string function is a unary operation on the set of strings over a given
alphabet. We discuss this new property and describe certain classes of
associative string functions. We also characterize the recently introduced
preassociative functions as compositions of associative string functions with
injective unary maps. Finally, we provide descriptions of the classes of
associative and preassociative functions which depend only on the length of the
input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7543</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7543</id><created>2014-03-28</created><authors><author><keyname>Lorenz</keyname><forenames>Dirk A.</forenames></author><author><keyname>Wenger</keyname><forenames>Stephan</forenames></author><author><keyname>Sch&#xf6;pfer</keyname><forenames>Frank</forenames></author><author><keyname>Magnor</keyname><forenames>Marcus</forenames></author></authors><title>A sparse Kaczmarz solver and a linearized Bregman method for online
  compressed sensing</title><categories>math.OC cs.CV cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithmic framework to compute sparse or minimal-TV solutions of linear
systems is proposed. The framework includes both the Kaczmarz method and the
linearized Bregman method as special cases and also several new methods such as
a sparse Kaczmarz solver. The algorithmic framework has a variety of
applications and is especially useful for problems in which the linear
measurements are slow and expensive to obtain. We present examples for online
compressed sensing, TV tomographic reconstruction and radio interferometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7550</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7550</id><created>2014-03-28</created><updated>2014-07-07</updated><authors><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>DimmWitted: A Study of Main-Memory Statistical Analytics</title><categories>cs.DB cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform the first study of the tradeoff space of access methods and
replication to support statistical analytics using first-order methods executed
in the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical
analytics systems differ from conventional SQL-analytics in the amount and
types of memory incoherence they can tolerate. Our goal is to understand
tradeoffs in accessing the data in row- or column-order and at what granularity
one should share the model and data for a statistical task. We study this new
tradeoff space, and discover there are tradeoffs between hardware and
statistical efficiency. We argue that our tradeoff study may provide valuable
information for designers of analytics engines: for each system we consider,
our prototype engine can run at least one popular task at least 100x faster. We
conduct our study across five architectures using popular models including
SVMs, logistic regression, Gibbs sampling, and neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7570</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7570</id><created>2014-03-28</created><updated>2014-10-27</updated><authors><author><keyname>Shen</keyname><forenames>Bin</forenames></author></authors><title>Universal Knowledge Discovery from Big Data: Towards a Paradigm Shift
  from 'Knowledge Discovery' to 'Wisdom Discovery'</title><categories>cs.CY</categories><comments>Significance: First, UKD is highlighted as a golden key to the
  &quot;treasure-trove&quot; hidden in big data. Second, it answers how to integrate the
  fourth paradigm with the previous three paradigms (i.e., experimental
  research, theoretical research and simulations) for scientific discovery.
  Third, an interdisciplinary research paradigm is proposed. Thus, a paradigm
  shift will be triggered in data mining</comments><msc-class>68T01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many people hold a vision that big data will provide big insights and have a
big impact in the future, and big-data-assisted scientific discovery is seen as
an emerging and promising scientific paradigm. However, how to turn big data
into deep insights with tremendous value still remains obscure. To meet the
challenge, universal knowledge discovery from big data (UKD) is proposed. The
new concept focuses on discovering universal knowledge, which exists in the
statistical analyses of big data and provides valuable insights into big data.
Universal knowledge comes in different forms, e.g., universal patterns, rules,
correlations, models and mechanisms. To accelerate big data assisted universal
knowledge discovery, a unified research paradigm should be built based on
techniques and paradigms from related research domains, especially big data
mining and complex systems science. Therefore, I propose an iBEST@SEE
methodology. This study lays a solid foundation for the future development of
universal knowledge discovery, and offers a pathway to the discovery of
&quot;treasure-trove&quot; hidden in big data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7579</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7579</id><created>2014-03-28</created><authors><author><keyname>L&#xf6;we</keyname><forenames>Benedikt</forenames></author><author><keyname>Winskel</keyname><forenames>Glynn</forenames></author></authors><title>Proceedings 8th International Workshop on Developments in Computational
  Models</title><categories>cs.LO cs.DC cs.DS cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014</journal-ref><doi>10.4204/EPTCS.143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the workshop series Developments in Computational Models (DCM) is
to bring together researchers who are currently developing new computational
models or new features for traditional computational models, in order to foster
their interaction, to provide a forum for presenting new ideas and work in
progress, and to enable newcomers to learn about current activities in this
area. The eighth workshop in the series, DCM 2012, was part of the celebrations
of the Turing Centenary and was held as a satellite event of the Turing
centenary conference Computability in Europe 2012 (CiE 2012) in Cambridge. It
took place at Corpus Christi College in Cambridge on Sunday, 17 June 2013.
  This electronic proceedings volume includes one of the keynote papers as well
as revised versions of papers accepted for presentation by the programme
committee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7588</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7588</id><created>2014-03-29</created><authors><author><keyname>Mu</keyname><forenames>Cun</forenames></author><author><keyname>Zhang</keyname><forenames>Yuqian</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Goldfarb</keyname><forenames>Donald</forenames></author></authors><title>Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods</title><categories>math.OC cs.CV cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering matrices from compressive and grossly corrupted observations is a
fundamental problem in robust statistics, with rich applications in computer
vision and machine learning. In theory, under certain conditions, this problem
can be solved in polynomial time via a natural convex relaxation, known as
Compressive Principal Component Pursuit (CPCP). However, all existing provable
algorithms for CPCP suffer from superlinear per-iteration cost, which severely
limits their applicability to large scale problems. In this paper, we propose
provable, scalable and efficient methods to solve CPCP with (essentially)
linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe
and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to
update the low-rank component with rank-one SVD and exploit the proximal step
for the sparse term. Convergence results and implementation details are also
discussed. We demonstrate the scalability of the proposed approach with
promising numerical experiments on visual data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7591</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7591</id><created>2014-03-29</created><authors><author><keyname>Cui</keyname><forenames>Yin</forenames></author><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Chen</keyname><forenames>Jiawei</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Building A Large Concept Bank for Representing Events in Video</title><categories>cs.MM cs.CV cs.IR</categories><comments>25 pages, 9 figures</comments><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept-based video representation has proven to be effective in complex
event detection. However, existing methods either manually design concepts or
directly adopt concept libraries not specifically designed for events. In this
paper, we propose to build Concept Bank, the largest concept library consisting
of 4,876 concepts specifically designed to cover 631 real-world events. To
construct the Concept Bank, we first gather a comprehensive event collection
from WikiHow, a collaborative writing project that aims to build the world's
largest manual for any possible How-To event. For each event, we then search
Flickr and discover relevant concepts from the tags of the returned images. We
train a Multiple Kernel Linear SVM for each discovered concept as a concept
detector in Concept Bank. We organize the concepts into a five-layer tree
structure, in which the higher-level nodes correspond to the event categories
while the leaf nodes are the event-specific concepts discovered for each event.
Based on such tree ontology, we develop a semantic matching method to select
relevant concepts for each textual event query, and then apply the
corresponding concept detectors to generate concept-based video
representations. We use TRECVID Multimedia Event Detection 2013 and Columbia
Consumer Video open source event definitions and videos as our test sets and
show very promising results on two video event detection tasks: event modeling
over concept space and zero-shot event retrieval. To the best of our knowledge,
this is the largest concept library covering the largest number of real-world
events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7595</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7595</id><created>2014-03-29</created><authors><author><keyname>Nie</keyname><forenames>Da-Cheng</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Zhou</keyname><forenames>Jun-lin</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Kui</forenames></author></authors><title>Information Filtering on Coupled Social Networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0101675</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, based on the coupled social networks (CSN), we propose a
hybrid algorithm to nonlinearly integrate both social and behavior information
of online users. Filtering algorithm based on the coupled social networks,
which considers the effects of both social influence and personalized
preference. Experimental results on two real datasets, \emph{Epinions} and
\emph{Friendfeed}, show that hybrid pattern can not only provide more accurate
recommendations, but also can enlarge the recommendation coverage while
adopting global metric. Further empirical analyses demonstrate that the mutual
reinforcement and rich-club phenomenon can also be found in coupled social
networks where the identical individuals occupy the core position of the online
system. This work may shed some light on the in-depth understanding structure
and function of coupled social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7598</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7598</id><created>2014-03-29</created><updated>2014-10-31</updated><authors><author><keyname>Xu</keyname><forenames>Haitao</forenames></author><author><keyname>Zhou</keyname><forenames>Xianwei</forenames></author></authors><title>A Non-cooperative Differential Game Model for Frequency Reuse based
  Channel Allocation in Satellite Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, channel resource allocation problem for LEO mobile satellite
systems is investigated and a new dynamic channel resource allocation scheme is
proposed based on differential game. Optimal channel resource allocated to each
satellite beams are formulated as Nash equilibrium. It is proved that optimal
channel resource allocation can be achieved and the differential game based
scheme is applicable and acceptable. Numerical results shows that system
performance can be improved based on the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7605</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7605</id><created>2014-03-29</created><updated>2015-10-05</updated><authors><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Cascading to Equilibrium: Hydraulic Computation of Equilibria in
  Resource Selection Games</title><categories>cs.GT</categories><comments>Hebrew University of Jerusalem Center for the Study of Rationality
  discussion paper 673</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing intuition from a (physical) hydraulic system, we present a novel
framework, constructively showing the existence of a strong Nash equilibrium in
resource selection games (i.e., asymmetric singleton congestion games) with
nonatomic players, the coincidence of strong equilibria and Nash equilibria in
such games, and the uniqueness of the cost of each given resource across all
Nash equilibria. Our proofs allow for explicit calculation of Nash equilibrium
and for explicit and direct calculation of the resulting (unique) costs of
resources, and do not hinge on any fixed-point theorem, on the Minimax theorem
or any equivalent result, on linear programming, or on the existence of a
potential (though our analysis does provide powerful insights into the
potential, via a natural concrete physical interpretation). A generalization of
resource selection games, called resource selection games with I.D.-dependent
weighting, is defined, and the results are extended to this family, showing the
existence of strong equilibria, and showing that while resource costs are no
longer unique across Nash equilibria in games of this family, they are
nonetheless unique across all strong Nash equilibria, drawing a novel
fundamental connection between group deviation and I.D.-congestion. A natural
application of the resulting machinery to a large class of
constraint-satisfaction problems is also described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7625</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7625</id><created>2014-03-29</created><updated>2014-05-31</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Testing Top Monotonicity</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>J.4; I.2.11; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Top monotonicity is a relaxation of various well-known domain restrictions
such as single-peaked and single-crossing for which negative impossibility
results are circumvented and for which the median-voter theorem still holds. We
examine the problem of testing top monotonicity and present a characterization
of top monotonicity with respect to non-betweenness constraints. We then extend
the definition of top monotonicity to partial orders and show that testing top
monotonicity of partial orders is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7645</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7645</id><created>2014-03-29</created><authors><author><keyname>Karl</keyname><forenames>Andrew T.</forenames></author><author><keyname>Eubank</keyname><forenames>Randy</forenames></author><author><keyname>Milovanovic</keyname><forenames>Jelena</forenames></author><author><keyname>Reiser</keyname><forenames>Mark</forenames></author><author><keyname>Young</keyname><forenames>Dennis</forenames></author></authors><title>Using RngStreams for Parallel Random Number Generation in C++ and R</title><categories>cs.MS stat.CO</categories><comments>This paper has been accepted by Computational Statistics and is
  currently in press</comments><journal-ref>Computational Statistics, 2014, 29:1301-1320</journal-ref><doi>10.1007/s00180-014-0492-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The RngStreams software package provides one viable solution to the problem
of creating independent random number streams for simulations in parallel
processing environments. Techniques are presented for effectively using
RngStreams with C++ programs that are parallelized via OpenMP or MPI. Ways to
access the backbone generator from RngStreams in R through the parallel and
rstream packages are also described. The ideas in the paper are illustrated
with both a simple running example and a Monte Carlo integration application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7654</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7654</id><created>2014-03-29</created><authors><author><keyname>Georgiev</keyname><forenames>Petko</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Where Businesses Thrive: Predicting the Impact of the Olympic Games on
  Local Retailers through Location-based Services Data</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Olympic Games are an important sporting event with notable consequences
for the general economic landscape of the host city. Traditional economic
assessments focus on the aggregated impact of the event on the national income,
but fail to provide micro-scale insights on why local businesses will benefit
from the increased activity during the Games. In this paper we provide a novel
approach to modeling the impact of the Olympic Games on local retailers by
analyzing a dataset mined from a large location-based social service,
Foursquare. We hypothesize that the spatial positioning of businesses as well
as the mobility trends of visitors are primary indicators of whether retailers
will rise their popularity during the event. To confirm this we formulate a
retail winners prediction task in the context of which we evaluate a set of
geographic and mobility metrics. We find that the proximity to stadiums, the
diversity of activity in the neighborhood, the nearby area sociability, as well
as the probability of customer flows from and to event places such as stadiums
and parks are all vital factors. Through supervised learning techniques we
demonstrate that the success of businesses hinges on a combination of both
geographic and mobility factors. Our results suggest that location-based social
networks, where crowdsourced information about the dynamic interaction of users
with urban spaces becomes publicly available, present an alternative medium to
assess the economic impact of large scale events in a city.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7657</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7657</id><created>2014-03-29</created><authors><author><keyname>Georgiev</keyname><forenames>Petko</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>The Call of the Crowd: Event Participation in Location-based Social
  Services</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the social and behavioral forces behind event participation is
not only interesting from the viewpoint of social science, but also has
important applications in the design of personalized event recommender systems.
This paper takes advantage of data from a widely used location-based social
network, Foursquare, to analyze event patterns in three metropolitan cities. We
put forward several hypotheses on the motivating factors of user participation
and confirm that social aspects play a major role in determining the likelihood
of a user to participate in an event. While an explicit social filtering signal
accounting for whether friends are attending dominates the factors, the
popularity of an event proves to also be a strong attractor. Further, we
capture an implicit social signal by performing random walks in a high
dimensional graph that encodes the place type preferences of friends and that
proves especially suited to identify relevant niche events for users. Our
findings on the extent to which the various temporal, spatial and social
aspects underlie users' event preferences lead us to further hypothesize that a
combination of factors better models users' event interests. We verify this
through a supervised learning framework. We show that for one in three users in
London and one in five users in New York and Chicago it identifies the exact
event the user would attend among the pool of suggestions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7663</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7663</id><created>2014-03-29</created><updated>2015-05-03</updated><authors><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author><author><keyname>Gleeson</keyname><forenames>James P.</forenames></author></authors><title>Dynamical Systems on Networks: A Tutorial</title><categories>nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>39 pages, 1 figure, submitted, more examples and discussion than
  original version, some reorganization and also more pointers to interesting
  directions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a tutorial for the study of dynamical systems on networks. We focus
especially on &quot;simple&quot; situations that are tractable analytically, because they
can be very insightful and provide useful springboards for the study of more
complicated scenarios. We briefly motivate why examining dynamical systems on
networks is interesting and important, and we then give several fascinating
examples and discuss some theoretical results. We also briefly discuss
dynamical systems on dynamical (i.e., time-dependent) networks, overview
software implementations, and give an outlook on the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7679</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7679</id><created>2014-03-29</created><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Bidigare</keyname><forenames>Patrick</forenames></author></authors><title>Coded Distributed Diversity: A Novel Distributed Reception Technique for
  Wireless Communication Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures, submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2015.2389766</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a distributed reception scenario where a
transmitter broadcasts a signal to multiple geographically separated receive
nodes over fading channels, and each node forwards a few bits representing a
processed version of the received signal to a fusion center. The fusion center
then tries to decode the transmitted signal based on the forwarded information
from the receive nodes and possible channel state information. We show that
there is a strong connection between the problem of minimizing a symbol error
probability at the fusion center in distributed reception and channel coding in
coding theory. This connection allows us to design a unified framework for
coded distributed diversity reception. We focus linear block codes such as
simplex codes or first-order Reed-Muller codes that achieve the Griesmer bound
with equality to maximize the diversity gain. Due to its simple structure, no
complex offline optimization process is needed to design the coding structure
at the receive nodes for the proposed coded diversity technique. The proposed
technique can support a wide array of distributed reception scenarios, i.e.,
arbitrary $M$-ary symbol transmission at the transmitter and received signal
processing with multiple bits at the receive nodes. Numerical studies show that
the proposed coded diversity technique can achieve practical symbol error rates
even with moderate signal-to-noise ratio and numbers of the receive nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7681</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7681</id><created>2014-03-29</created><updated>2015-06-16</updated><authors><author><keyname>Lotfi</keyname><forenames>Mohammad Hassan</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author></authors><title>Uncertain Price Competition in a Duopoly with Heterogeneous Availability</title><categories>cs.GT</categories><comments>45 pages, Accepted for publication in IEEE Transaction on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the price competition in a duopoly with an arbitrary number of
buyers. Each seller can offer multiple units of a commodity depending on the
availability of the commodity which is random and may be different for
different sellers. Sellers seek to select a price that will be attractive to
the buyers and also fetch adequate profits. The selection will in general
depend on the number of units available with the seller and also that of its
competitor - the seller may only know the statistics of the latter. The setting
captures a secondary spectrum access network, a non-neutral Internet, or a
microgrid network in which unused spectrum bands, resources of ISPs, and excess
power units constitute the respective commodities of sale. We analyze this
price competition as a game, and identify a set of necessary and sufficient
properties for the Nash Equilibrium (NE). The properties reveal that sellers
randomize their price using probability distributions whose support sets are
mutually disjoint and in decreasing order of the number of availability. We
prove the uniqueness of a symmetric NE in a symmetric market, and explicitly
compute the price distribution in the symmetric NE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7682</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7682</id><created>2014-03-29</created><authors><author><keyname>Madhusudhanan</keyname><forenames>Prasanna</forenames><affiliation>Eugene</affiliation></author><author><keyname>Restrepo</keyname><forenames>Juan G.</forenames><affiliation>Eugene</affiliation></author><author><keyname>Youjian</keyname><affiliation>Eugene</affiliation></author><author><keyname>Liu</keyname></author><author><keyname>Brown</keyname><forenames>Timothy X</forenames></author></authors><title>Downlink Analysis for a Heterogeneous Cellular Network</title><categories>cs.IT math.IT</categories><comments>13 pages, 3 figures, 1 table, to be submitted to Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a comprehensive study of the the downlink performance in a
heterogeneous cellular network (or hetnet) is conducted. A general hetnet model
is considered consisting of an arbitrary number of open-access and
closed-access tier of base stations (BSs) arranged according to independent
homogeneous Poisson point processes. The BSs of each tier have a constant
transmission power, random fading coefficient with an arbitrary distribution
and arbitrary path-loss exponent of the power-law path-loss model. For such a
system, analytical characterizations for the coverage probability and average
rate at an arbitrary mobile-station (MS), and average per-tier load are derived
for both the max-SINR connectivity and nearest-BS connectivity models. Using
stochastic ordering, interesting properties and simplifications for the hetnet
downlink performance are derived by relating these two connectivity models to
the maximum instantaneous received power (MIRP) connectivity model and the
maximum biased received power (MBRP) connectivity models, respectively,
providing good insights about the hetnets and the downlink performance in these
complex networks. Furthermore, the results also demonstrate the effectiveness
and analytical tractability of the stochastic geometric approach to study the
hetnet performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7683</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7683</id><created>2014-03-29</created><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Vlachos</keyname><forenames>Michail</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>Approximate Matrix Multiplication with Application to Linear Embeddings</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>8 pages, International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of approximately computing the product of
two real matrices. In particular, we analyze a dimensionality-reduction-based
approximation algorithm due to Sarlos [1], introducing the notion of nuclear
rank as the ratio of the nuclear norm over the spectral norm. The presented
bound has improved dependence with respect to the approximation error (as
compared to previous approaches), whereas the subspace -- on which we project
the input matrices -- has dimensions proportional to the maximum of their
nuclear rank and it is independent of the input dimensions. In addition, we
provide an application of this result to linear low-dimensional embeddings.
Namely, we show that any Euclidean point-set with bounded nuclear rank is
amenable to projection onto number of dimensions that is independent of the
input dimensionality, while achieving additive error guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7685</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7685</id><created>2014-03-29</created><authors><author><keyname>Ayala-Rinc&#xf3;n</keyname><forenames>Mauricio</forenames></author><author><keyname>Bonelli</keyname><forenames>Eduardo</forenames></author><author><keyname>Mackie</keyname><forenames>Ian</forenames></author></authors><title>Proceedings 9th International Workshop on Developments in Computational
  Models</title><categories>cs.LO cs.PL</categories><comments>EPTCS 144, 2014</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains a selection of the papers presented at the Ninth
International Workshop on Developments in Computational Models (DCM 2013) held
in Buenos Aires, Argentina on 26th August 2013, as a satellite event of CONCUR
2013. Several new models of computation have emerged in the last years, and
many developments of traditional computational models have been proposed with
the aim of taking into account the new demands of computer systems users and
the new capabilities of computation engines. A new computational model, or a
new feature in a traditional one, usually is reflected in a new family of
programming languages, and new paradigms of software development. The aim of
this workshop is to bring together researchers who are currently developing new
computational models or new features for traditional computational models, in
order to foster their interaction, to provide a forum for presenting new ideas
and work in progress, and to enable newcomers to learn about current activities
in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7691</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7691</id><created>2014-03-29</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Huang</keyname><forenames>Yufan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author></authors><title>Mobile Conductance in Sparse Networks and Mobility-Connectivity Tradeoff</title><categories>cs.NI cs.SI</categories><comments>Accepted to ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, our recently proposed mobile-conductance based analytical
framework is extended to the sparse settings, thus offering a unified tool for
analyzing information spreading in mobile networks. A penalty factor is
identified for information spreading in sparse networks as compared to the
connected scenario, which is then intuitively interpreted and verified by
simulations. With the analytical results obtained, the mobility-connectivity
tradeoff is quantitatively analyzed to determine how much mobility may be
exploited to make up for network connectivity deficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7692</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7692</id><created>2014-03-29</created><updated>2015-02-01</updated><authors><author><keyname>Nino</keyname><forenames>Elias D.</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>A Derivative-Free Trust Region Framework for Variational Data
  Assimilation</title><categories>cs.NA</categories><report-no>CSL-TR-5-2014</report-no><acm-class>F.2.1; I.6.8; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study develops a hybrid ensemble-variational approach for solving data
assimilation problems. The method, called TR-4D-EnKF, is based on a trust
region framework and consists of three computational steps. First an ensemble
of model runs is propagated forward in time and snapshots of the state are
stored. Next, a sequence of basis vectors is built and a low-dimensional
representation of the data assimilation system is obtained by projecting the
model state onto the space spanned by the ensemble deviations from the mean.
Finally, the low-dimensional optimization problem is solved in the
reduced-space using a trust region approach; the size of the trust region is
updated according to the relative decrease of the reduced order surrogate cost
function. The analysis state is projected back onto the full space, and the
process is repeated with the current analysis serving as a new background. A
heuristic approach based on the trust region size is proposed in order to
adjust the background error statistics from one iteration to the next.
Experimental simulations are carried out using the Lorenz and the
quasi-geostrophic models. The results show that TR-4D-EnKF is an efficient
computational approach, and is more accurate than the current state of the art
4D-EnKF implementations such as the POD-4D-EnKF and the Iterative Subspace
Minimization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7697</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7697</id><created>2014-03-29</created><authors><author><keyname>Ho</keyname><forenames>Keang-Po</forenames></author><author><keyname>Cheng</keyname><forenames>Shi</forenames></author><author><keyname>Liu</keyname><forenames>Jianhan</forenames></author></authors><title>MIMO Beamforming in Millimeter-Wave Directional Wi-Fi</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beamforming is indispensable in the operation of 60-GHz millimeter-wave
directional multi-gigabit Wi-Fi. Simple power method and its extensions enable
the transmitting and receiving antenna arrays to form a beam for single spatial
stream. To further improve the spectral efficiency in future 60-GHz directional
Wi-Fi, alternating least square (ALS) algorithm can form multiple beams between
the transmitter and receiver for multi-input-multi-output (MIMO) operations.
For both shared and split MIMO architecture, the ALS beamforming algorithm can
be operated in both frequency-flat and frequency-selective channels. In the
split architecture, MIMO beamforming approximately maximizes the capacity of
the beam-formed MIMO channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7698</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7698</id><created>2014-03-30</created><authors><author><keyname>Gumerov</keyname><forenames>Nail A.</forenames></author><author><keyname>Duraiswami</keyname><forenames>Ramani</forenames></author></authors><title>Recursive computation of spherical harmonic rotation coefficients of
  large degree</title><categories>cs.NA</categories><report-no>University of Maryland Institute for Advanced Computer Studies,
  UMIACS-TR-2014-4; Department of Computer Science CS-TR-5037</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation of the spherical harmonic rotation coefficients or elements of
Wigner's d-matrix is important in a number of quantum mechanics and
mathematical physics applications. Particularly, this is important for the Fast
Multipole Methods in three dimensions for the Helmholtz, Laplace and related
equations, if rotation-based decomposition of translation operators are used.
In these and related problems related to representation of functions on a
sphere via spherical harmonic expansions computation of the rotation
coefficients of large degree $n$ (of the order of thousands and more) may be
necessary. Existing algorithms for their computation, based on recursions, are
usually unstable, and do not extend to $n$. We develop a new recursion and
study its behavior for large degrees, via computational and asymptotic
analyses. Stability of this recursion was studied based on a novel application
of the Courant-Friedrichs-Lewy condition and the von Neumann method for
stability of finite-difference schemes for solution of PDEs. A recursive
algorithm of minimal complexity $O\left(n^{2}\right)$ for degree $n$ and
FFT-based algorithms of complexity $O\left(n^{2}\log n\right) $ suitable for
computation of rotation coefficients of large degrees are proposed, studied
numerically, and cross-validated. It is shown that the latter algorithm can be
used for $n\lesssim 10^{3}$ in double precision, while the former algorithm was
tested for large $n$ (up to $10^{4}$ in our experiments) and demonstrated
better performance and accuracy compared to the FFT-based algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7707</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7707</id><created>2014-03-30</created><authors><author><keyname>Zehavi</keyname><forenames>Ephraim</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>On the allocation of multiple divisible and non-transferable commodities</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When there is a dispute between players how to divide multiple divisible
commodities, how should it be resolved? In this paper we introduce a
multi-commodity game model. This model enables cooperation between multiple
players to bargain on sharing K commodities, when each player has a different
value for each commodity. It thus extends the sequential discrete Raiffa
solution and the Aumann bankruptcy solution to multi-commodity cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7714</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7714</id><created>2014-03-30</created><updated>2014-09-30</updated><authors><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>Asymptotically-Optimal Motion Planning using Lower Bounds on Cost</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many path-finding algorithms on graphs such as A* are sped up by using a
heuristic function that gives lower bounds on the cost to reach the goal.
Aiming to apply similar techniques to speed up sampling-based motion-planning
algorithms, we use effective lower bounds on the cost between configurations to
tightly estimate the cost-to-go. We then use these estimates in an anytime
asymptotically-optimal algorithm which we call Motion Planning using Lower
Bounds (MPLB). MPLB is based on the Fast Marching Trees (FMT*) algorithm
recently presented by Janson and Pavone. An advantage of our approach is that
in many cases (especially as the number of samples grows) the weight of
collision detection in the computation is almost negligible with respect to
nearest-neighbor calls. We prove that MPLB performs no more collision-detection
calls than an anytime version of FMT*. Additionally, we demonstrate in
simulations that for certain scenarios, the algorithmic tools presented here
enable efficiently producing low-cost paths while spending only a small
fraction of the running time on collision detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7720</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7720</id><created>2014-03-30</created><authors><author><keyname>Yu</keyname><forenames>Quan</forenames></author><author><keyname>Sung</keyname><forenames>Chi Wan</forenames></author><author><keyname>Chan</keyname><forenames>Terence H.</forenames></author></authors><title>Irregular Fractional Repetition Code Optimization for Heterogeneous
  Cloud Storage</title><categories>cs.IT math.IT</categories><comments>12 pages, 10 figures. to appear in IEEE Journal on Selected Areas in
  Communications 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a flexible irregular model for heterogeneous cloud
storage systems and investigates how the cost of repairing failed nodes can be
minimized. The fractional repetition code, originally designed for minimizing
repair bandwidth for homogeneous storage systems, is generalized to the
irregular fractional repetition code, which is adaptable to heterogeneous
environments. The code structure and the associated storage allocation can be
obtained by solving an integer linear programming problem. For moderate sized
networks, a heuristic algorithm is proposed and shown to be near-optimal by
computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7721</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7721</id><created>2014-03-30</created><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Manokaran</keyname><forenames>Rajsekar</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author></authors><title>Maximum Quadratic Assignment Problem: Reduction from Maximum Label Cover
  and LP-based Approximation Algorithm</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for every positive $\epsilon &gt; 0$, unless NP $\subset$ BPQP, it
is impossible to approximate the maximum quadratic assignment problem within a
factor better than $2^{\log^{1-\epsilon} n}$ by a reduction from the maximum
label cover problem. Our result also implies that Approximate Graph Isomorphism
is not robust and is in fact, $1 - \epsilon$ vs $\epsilon$ hard assuming the
Unique Games Conjecture.
  Then, we present an $O(\sqrt{n})$-approximation algorithm for the problem
based on rounding of the linear programming relaxation often used in the state
of the art exact algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7726</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7726</id><created>2014-03-30</created><authors><author><keyname>Madbouly</keyname><forenames>Ayman I.</forenames></author><author><keyname>Gody</keyname><forenames>Amr M.</forenames></author><author><keyname>Barakat</keyname><forenames>Tamer M.</forenames></author></authors><title>Relevant Feature Selection Model Using Data Mining for Intrusion
  Detection System</title><categories>cs.CR cs.LG</categories><comments>12 Pages, 3 figures, 5 tables, Published with &quot;International Journal
  of Engineering Trends and Technology (IJETT)&quot;. arXiv admin note: text overlap
  with arXiv:1208.5997 by other authors without attribution</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V9(10),501-512 March 2014</journal-ref><doi>10.14445/22315381/IJETT-V9P296</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network intrusions have become a significant threat in recent years as a
result of the increased demand of computer networks for critical systems.
Intrusion detection system (IDS) has been widely deployed as a defense measure
for computer networks. Features extracted from network traffic can be used as
sign to detect anomalies. However with the huge amount of network traffic,
collected data contains irrelevant and redundant features that affect the
detection rate of the IDS, consumes high amount of system resources, and
slowdown the training and testing process of the IDS. In this paper, a new
feature selection model is proposed; this model can effectively select the most
relevant features for intrusion detection. Our goal is to build a lightweight
intrusion detection system by using a reduced features set. Deleting irrelevant
and redundant features helps to build a faster training and testing process, to
have less resource consumption as well as to maintain high detection rates. The
effectiveness and the feasibility of our feature selection model were verified
by several experiments on KDD intrusion detection dataset. The experimental
results strongly showed that our model is not only able to yield high detection
rates but also to speed up the detection process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7729</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7729</id><created>2014-03-30</created><authors><author><keyname>Garofalakis</keyname><forenames>Minos</forenames></author><author><keyname>Ioannidis</keyname><forenames>Yannis</forenames></author></authors><title>Multi-Resource Parallel Query Scheduling and Optimization</title><categories>cs.DB</categories><comments>50 pages; Conference version of the paper has appeared in the
  Proceedings of the 23rd International Conference on Very Large Databases
  (VLDB'1997), Athens, Greece, August 1997</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scheduling query execution plans is a particularly complex problem in
shared-nothing parallel systems, where each site consists of a collection of
local time-shared (e.g., CPU(s) or disk(s)) and space-shared (e.g., memory)
resources and communicates with remote sites by message-passing. Earlier work
on parallel query scheduling employs either (a) one-dimensional models of
parallel task scheduling, effectively ignoring the potential benefits of
resource sharing, or (b) models of globally accessible resource units, which
are appropriate only for shared-memory architectures, since they cannot capture
the affinity of system resources to sites. In this paper, we develop a general
approach capturing the full complexity of scheduling distributed,
multi-dimensional resource units for all forms of parallelism within and across
queries and operators. We present a level-based list scheduling heuristic
algorithm for independent query tasks (i.e., physical operator pipelines) that
is provably near-optimal for given degrees of partitioned parallelism (with a
worst-case performance ratio that depends on the number of time-shared and
space-shared resources per site and the granularity of the clones). We also
propose extensions to handle blocking constraints in logical operator (e.g.,
hash-join) pipelines and bushy query plans as well as on-line task arrivals
(e.g., in a dynamic or multi-query execution environment). Experiments with our
scheduling algorithms implemented on top of a detailed simulation model verify
their effectiveness compared to existing approaches in a realistic setting.
Based on our analytical and experimental results, we revisit the open problem
of designing efficient cost models for parallel query optimization and propose
a solution that captures all the important parameters of parallel execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7735</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7735</id><created>2014-03-30</created><updated>2014-07-08</updated><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Saad</keyname><forenames>Hussien</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>Optimal Cooperative Cognitive Relaying and Spectrum Access for an Energy
  Harvesting Cognitive Radio: Reinforcement Learning Approach</title><categories>cs.NI cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a cognitive setting under the context of
cooperative communications, where the cognitive radio (CR) user is assumed to
be a self-organized relay for the network. The CR user and the PU are assumed
to be energy harvesters. The CR user cooperatively relays some of the
undelivered packets of the primary user (PU). Specifically, the CR user stores
a fraction of the undelivered primary packets in a relaying queue (buffer). It
manages the flow of the undelivered primary packets to its relaying queue using
the appropriate actions over time slots. Moreover, it has the decision of
choosing the used queue for channel accessing at idle time slots (slots where
the PU's queue is empty). It is assumed that one data packet transmission
dissipates one energy packet. The optimal policy changes according to the
primary and CR users arrival rates to the data and energy queues as well as the
channels connectivity. The CR user saves energy for the PU by taking the
responsibility of relaying the undelivered primary packets. It optimally
organizes its own energy packets to maximize its payoff as time progresses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7737</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7737</id><created>2014-03-30</created><updated>2014-04-05</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author></authors><title>Sharpened Error Bounds for Random Sampling Based $\ell_2$ Regression</title><categories>cs.LG cs.NA stat.ML</categories><comments>unpublished manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a data matrix $X \in R^{n\times d}$ and a response vector $y \in
R^{n}$, suppose $n&gt;d$, it costs $O(n d^2)$ time and $O(n d)$ space to solve the
least squares regression (LSR) problem. When $n$ and $d$ are both large,
exactly solving the LSR problem is very expensive. When $n \gg d$, one feasible
approach to speeding up LSR is to randomly embed $y$ and all columns of $X$
into a smaller subspace $R^c$; the induced LSR problem has the same number of
columns but much fewer number of rows, and it can be solved in $O(c d^2)$ time
and $O(c d)$ space.
  We discuss in this paper two random sampling based methods for solving LSR
more efficiently. Previous work showed that the leverage scores based sampling
based LSR achieves $1+\epsilon$ accuracy when $c \geq O(d \epsilon^{-2} \log
d)$. In this paper we sharpen this error bound, showing that $c = O(d \log d +
d \epsilon^{-1})$ is enough for achieving $1+\epsilon$ accuracy. We also show
that when $c \geq O(\mu d \epsilon^{-2} \log d)$, the uniform sampling based
LSR attains a $2+\epsilon$ bound with positive probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7745</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7745</id><created>2014-03-30</created><authors><author><keyname>Doberkat</keyname><forenames>Ernst-Erich</forenames></author></authors><title>Algebraic Properties of Stochastic Effectivity Functions</title><categories>cs.LO</categories><comments>Accepted for publication, J. Log. Alg. Prog</comments><report-no>SWT-Memo-196</report-no><msc-class>03B45, 68Q85, 68Q87</msc-class><acm-class>F.1.1; F.4.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effectivity functions are the basic formalism for investigating the semantics
game logic. We discuss algebraic properties of stochastic effectivity
functions, in particular the relationship to stochastic relations, morphisms
and congruences are defined, and the relationship of abstract logical
equivalence and behavioral equivalence is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7746</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7746</id><created>2014-03-30</created><authors><author><keyname>Kursa</keyname><forenames>Miron B.</forenames></author><author><keyname>Wieczorkowska</keyname><forenames>Alicja A.</forenames></author></authors><title>Multi-label Ferns for Efficient Recognition of Musical Instruments in
  Recordings</title><categories>cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce multi-label ferns, and apply this technique for
automatic classification of musical instruments in audio recordings. We compare
the performance of our proposed method to a set of binary random ferns, using
jazz recordings as input data. Our main result is obtaining much faster
classification and higher F-score. We also achieve substantial reduction of the
model size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7747</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7747</id><created>2014-03-30</created><authors><author><keyname>Knapp</keyname><forenames>Alexander</forenames></author><author><keyname>Mossakowski</keyname><forenames>Till</forenames></author><author><keyname>Roggenbach</keyname><forenames>Markus</forenames></author></authors><title>An Institutional Framework for Heterogeneous Formal Development in UML</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for formal software development with UML. In contrast
to previous approaches that equip UML with a formal semantics, we follow an
institution based heterogeneous approach. This can express suitable formal
semantics of the different UML diagram types directly, without the need to map
everything to one specific formalism (let it be first-order logic or graph
grammars). We show how different aspects of the formal development process can
be coherently formalised, ranging from requirements over design and Hoare-style
conditions on code to the implementation itself. The framework can be used to
verify consistency of different UML diagrams both horizontally (e.g.,
consistency among various requirements) as well as vertically (e.g.,
correctness of design or implementation w.r.t. the requirements).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7748</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7748</id><created>2014-03-30</created><authors><author><keyname>Payette</keyname><forenames>Sandy</forenames></author></authors><title>The State of Technology for Digital Archiving</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Windsor Study Group on Digital Archiving was commissioned to recommend
strategies, policies, and technologies necessary for ensuring the integrity and
longevity of electronic publications. The goal of this work is to inform
institutions of the challenges and opportunities faced by information stewards
in fulfilling their mission of guaranteeing a permanent and authoritative
scholarly record in the digital age. This white paper focuses specifically on
the technological dimensions of digital archiving. It provides an analysis of
the current state of technologies as well as a forecast of how digital archive
systems are likely to evolve over the next decade. The thesis of this white
paper is that technology does not present a barrier to long term digital
archiving, but instead presents an opportunity to harness the current and
future power of these technologies to create the architectural underpinnings of
a comprehensive digital archiving strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7752</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7752</id><created>2014-03-30</created><updated>2015-01-23</updated><authors><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author></authors><title>Auto-encoders: reconstruction versus compression</title><categories>cs.NE cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the similarities and differences between training an auto-encoder
to minimize the reconstruction error, and training the same auto-encoder to
compress the data via a generative model. Minimizing a codelength for the data
using an auto-encoder is equivalent to minimizing the reconstruction error plus
some correcting terms which have an interpretation as either a denoising or
contractive property of the decoding function. These terms are related but not
identical to those used in denoising or contractive auto-encoders [Vincent et
al. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fully
determines an optimal noise level for the denoising criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7755</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7755</id><created>2014-03-30</created><authors><author><keyname>Wang</keyname><forenames>Liqi</forenames></author><author><keyname>Zhu</keyname><forenames>Shixin</forenames></author></authors><title>On the Construction of Optimal Asymmetric Quantum Codes</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Constacyclic codes are important classes of linear codes that have been
applied to the construction of quantum codes. Six new families of asymmetric
quantum codes derived from constacyclic codes are constructed in this paper.
Moreover, the constructed asymmetric quantum codes are optimal and different
from the codes available in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7760</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7760</id><created>2014-03-30</created><updated>2014-10-08</updated><authors><author><keyname>Doberkat</keyname><forenames>Ernst-Erich</forenames></author></authors><title>Categories and all that -- A Tutorial</title><categories>cs.LO</categories><report-no>SWT-Memo-198</report-no><msc-class>03B45, 68Q55, 68Q85</msc-class><acm-class>F.4.1; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a short introduction to categories with some emphasis on coalgebras.
We start from introducing basic notions (categories, functors, natural
transformations), move to Kleisli tripels and monads, with a short discussion
of monads in Haskell, and continue with displaying the interplay between
algebras, adjunctions and monads. Coalgebras are discussed and applied to the
semantics of modal logics, giving a brief introduction to coalgebraic logics as
well. The development is illustrated through examples, usually taken from
applications to computer science, with a certain predilection for stochastic
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7765</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7765</id><created>2014-03-30</created><authors><author><keyname>Doberkat</keyname><forenames>Ernst-Erich</forenames></author></authors><title>Stochastic Interpretation of Game Logic</title><categories>cs.LO</categories><report-no>SWT-Memo-193</report-no><msc-class>03B45, 18C50, 68Q55, 68Q87</msc-class><acm-class>F.4.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game logic is a dynamic modal logic which models strategic two person games;
it contains propositional dynamic logic (PDL) as a fragment. We propose an
interpretation of game logic based on stochastic effectivity functions. A
definition of these functions is proposed, and some algebraic properties of
effectivity functions such as congruences are investigated. The relationship to
stochastic relations is characterized through a deduction system. Logical and
behavioral equivalence of game models is investigated. Finally the completion
of models receives some attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7766</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7766</id><created>2014-03-30</created><authors><author><keyname>Shah</keyname><forenames>Tejal</forenames></author><author><keyname>Rabhi</keyname><forenames>Fethi</forenames></author><author><keyname>Ray</keyname><forenames>Pradeep</forenames></author><author><keyname>Taylor</keyname><forenames>Kerry</forenames></author></authors><title>Enhancing Automated Decision Support across Medical and Oral Health
  Domains with Semantic Web Technologies</title><categories>cs.AI cs.IR</categories><comments>The paper has been published at the 24th Australasian Conference on
  Information Systems, 4-6 Dec 2013, Melbourne. The paper can be found at:
  http://mo.bf.rmit.edu.au/acis2013/382.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research has shown that the general health and oral health of an individual
are closely related. Accordingly, current practice of isolating the information
base of medical and oral health domains can be dangerous and detrimental to the
health of the individual. However, technical issues such as heterogeneous data
collection and storage formats, limited sharing of patient information and lack
of decision support over the shared information are the principal reasons for
the current state of affairs. To address these issues, the following research
investigates the development and application of a cross-domain ontology and
rules to build an evidence-based and reusable knowledge base consisting of the
inter-dependent conditions from the two domains. Through example implementation
of the knowledge base in Protege, we demonstrate the effectiveness of our
approach in reasoning over and providing decision support for cross-domain
patient information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7772</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7772</id><created>2014-03-30</created><authors><author><keyname>Wen</keyname><forenames>Xidao</forenames></author><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Trattner</keyname><forenames>Christoph</forenames></author><author><keyname>Parra</keyname><forenames>Denis</forenames></author></authors><title>Twitter in Academic Conferences: Usage, Networking and Participation
  over Time</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is often referred to as a backchannel for conferences. While the main
conference takes place in a physical setting, attendees and virtual attendees
socialize, introduce new ideas or broadcast information by microblogging on
Twitter. In this paper we analyze the scholars' Twitter use in 16 Computer
Science conferences over a timespan of five years. Our primary finding is that
over the years there are increasing differences with respect to conversation
use and information use in Twitter. We studied the interaction network between
users to understand whether assumptions about the structure of the
conversations hold over time and between different types of interactions, such
as retweets, replies, and mentions. While `people come and people go', we want
to understand what keeps people stay with the conference on Twitter. By casting
the problem to a classification task, we find different factors that contribute
to the continuing participation of users to the online Twitter conference
activity. These results have implications for research communities to implement
strategies for continuous and active participation among members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7773</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7773</id><created>2014-03-30</created><authors><author><keyname>Ouyang</keyname><forenames>Wenzhuo</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Atilla</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Scheduling in Time-correlated Wireless Networks with Imperfect CSI and
  Stringent Constraint</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wireless network, the efficiency of scheduling algorithms over
time-varying channels depends heavily on the accuracy of the Channel State
Information (CSI), which is usually quite ``costly'' in terms of consuming
network resources. Scheduling in such systems is also subject to stringent
constraints such as power and bandwidth, which limit the maximum number of
simultaneous transmissions. In the meanwhile, communication channels in
wireless systems typically fluctuate in a time-correlated manner. We hence
design schedulers to exploit the temporal-correlation inherent in channels with
memory and ARQ-styled feedback from the users for better channel state
knowledge, under the assumption of Markovian channels and the stringent
constraint on the maximum number of simultaneously active users. We model this
problem under the framework of a Partially Observable Markov Decision
Processes.
  In recent work, a low-complexity optimal solution was developed for this
problem under a long-term time-average resource constraint. However, in real
systems with instantaneous resource constraints, how to optimally exploit the
temporal correlation and satisfy realistic stringent constraint on the
instantaneous service remains elusive. In this work, we incorporate a stringent
constraint on the simultaneously scheduled users and propose a low-complexity
scheduling algorithm that dynamically implements user scheduling and dummy
packet broadcasting. We show that the throughput region of the optimal policy
under the long-term average resource constraint can be asymptotically achieved
in the stringent constrained scenario by the proposed algorithm, in the many
users limiting regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7774</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7774</id><created>2014-03-30</created><authors><author><keyname>Sengar</keyname><forenames>Kritika</forenames></author><author><keyname>Rani</keyname><forenames>Nishu</forenames></author><author><keyname>Singhal</keyname><forenames>Ankita</forenames></author><author><keyname>Sharma</keyname><forenames>Dolly</forenames></author><author><keyname>Verma</keyname><forenames>Seema</forenames></author><author><keyname>Singh</keyname><forenames>Tanya</forenames></author></authors><title>Study and Capacity Evaluation of SISO, MISO and MIMO RF Wireless
  Communication Systems</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 9 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless communication systems has gone from different generations from
SISO systems to MIMO systems. Bandwidth is one important constraint in wireless
communication. In wireless communication, high data transmission rates are
essential for the services like tripple play i.e. data, voice and video. At
user end the capacity determines the quality of the communication systems. This
paper aims to compare the different RF wireless communication systems like
SISO, MISO, SIMO and MIMO systems on the capacity basis and explaining the
concept as today, the wireless communication has evolved from 2G, 3G to 4G and
the companies are fighting to create networks with more and more capacity so
that data rates can be increased and customers can be benefitted more. The
ultimate goal of wireless communication systems is to create a global personal
and multimedia communication without any capacity issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7777</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7777</id><created>2014-03-30</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Some problems with two axiomatizations of discussive logic</title><categories>math.LO cs.LO</categories><comments>13 pages. Submitted to Trends in Logic XIII 2014</comments><msc-class>03B53, 03B35, 03B45, 03-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems in two axiomatizations of Ja\'skowski's discussive (or discursive)
logic D2 are considered. A recent axiomatization of D2 and completeness proof
relative to D2's intended semantics seems to be mistaken because some formulas
valid according to the intended semantics turn out to be unprovable. Although
no new axiomatization is offered, nor a repaired completeness proof given, the
shortcomings identified here may be a step toward an improved axiomatization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7783</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7783</id><created>2014-03-30</created><authors><author><keyname>Javed</keyname><forenames>Mohammed</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Extraction of Line Word Character Segments Directly from Run Length
  Compressed Printed Text Documents</title><categories>cs.CV</categories><comments>IEEE Proceedings in National Conference on Computer Vision, Pattern
  Recognition, Image Processing and Graphics (NCVPRIPG 2013)</comments><doi>10.1109/NCVPRIPG.2013.6776195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation of a text-document into lines, words and characters, which is
considered to be the crucial pre-processing stage in Optical Character
Recognition (OCR) is traditionally carried out on uncompressed documents,
although most of the documents in real life are available in compressed form,
for the reasons such as transmission and storage efficiency. However, this
implies that the compressed image should be decompressed, which indents
additional computing resources. This limitation has motivated us to take up
research in document image analysis using compressed documents. In this paper,
we think in a new way to carry out segmentation at line, word and character
level in run-length compressed printed-text-documents. We extract the
horizontal projection profile curve from the compressed file and using the
local minima points perform line segmentation. However, tracing vertical
information which leads to tracking words-characters in a run-length compressed
file is not very straight forward. Therefore, we propose a novel technique for
carrying out simultaneous word and character segmentation by popping out column
runs from each row in an intelligent sequence. The proposed algorithms have
been validated with 1101 text-lines, 1409 words and 7582 characters from a
data-set of 35 noise and skew free compressed documents of Bengali, Kannada and
English Scripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7790</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7790</id><created>2014-03-30</created><authors><author><keyname>Matni</keyname><forenames>Nikolai</forenames></author><author><keyname>Lamperski</keyname><forenames>Andrew</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author></authors><title>Optimal Two Player LQR State Feedback With Varying Delay</title><categories>math.OC cs.SY</categories><comments>Extended version of IFAC '14 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an explicit solution to a two player distributed LQR
problem in which communication between controllers occurs across a
communication link with varying delay. We extend known dynamic programming
methods to accommodate this varying delay, and show that under suitable
assumptions, the optimal control actions are linear in their information, and
that the resulting controller has piecewise linear dynamics dictated by the
current effective delay regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7791</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7791</id><created>2014-03-30</created><authors><author><keyname>Coti</keyname><forenames>Camille</forenames></author></authors><title>POSH: Paris OpenSHMEM: A High-Performance OpenSHMEM Implementation for
  Shared Memory Systems</title><categories>cs.DC</categories><comments>This is an extended version (featuring the full proofs) of a paper
  accepted at ICCS'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the design and implementation of POSH, an
Open-Source implementation of the OpenSHMEM standard. We present a model for
its communications, and prove some properties on the memory model defined in
the OpenSHMEM specification. We present some performance measurements of the
communication library featured by POSH and compare them with an existing
one-sided communication library. POSH can be downloaded from
\url{http://www.lipn.fr/~coti/POSH}. % 9 - 67
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7792</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7792</id><created>2014-03-30</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>Swarm Intelligence Based Algorithms: A Critical Analysis</title><categories>math.OC cs.NE nlin.AO</categories><comments>16 pages</comments><msc-class>78M32</msc-class><journal-ref>Evolutionary Intelligence, vol. 7, no. 1, pp. 17-28 (2014)</journal-ref><doi>10.1007/s12065-013-0102-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many optimization algorithms have been developed by drawing inspiration from
swarm intelligence (SI). These SI-based algorithms can have some advantages
over traditional algorithms. In this paper, we carry out a critical analysis of
these SI-based algorithms by analyzing their ways to mimic evolutionary
operators. We also analyze the ways of achieving exploration and exploitation
in algorithms by using mutation, crossover and selection. In addition, we also
look at algorithms using dynamic systems, self-organization and Markov chain
framework. Finally, we provide some discussions and topics for further
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7793</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7793</id><created>2014-03-30</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Huyck</keyname><forenames>Christian</forenames></author><author><keyname>Karamanoglu</keyname><forenames>Mehmet</forenames></author><author><keyname>Khan</keyname><forenames>Nawaz</forenames></author></authors><title>True Global Optimality of the Pressure Vessel Design Problem: A
  Benchmark for Bio-Inspired Optimisation Algorithms</title><categories>math.OC cs.NE nlin.AO</categories><journal-ref>X.-S. Yang et al., Int. J. Bio-Inspired Computation, vol. 5, no.
  6, pp. 329-335 (2013)</journal-ref><doi>10.4018/jdsst.2013040103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pressure vessel design problem is a well-known design benchmark for
validating bio-inspired optimization algorithms. However, its global optimality
is not clear and there has been no mathematical proof put forward. In this
paper, a detailed mathematical analysis of this problem is provided that proves
that 6059.714335048436 is the global minimum. The Lagrange multiplier method is
also used as an alternative proof and this method is extended to find the
global optimum of a cantilever beam design problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7795</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7795</id><created>2014-03-30</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Cui</keyname><forenames>Zhihua</forenames></author></authors><title>Bio-Inspired Computation: Success and Challenges of IJBIC</title><categories>math.OC cs.NE</categories><comments>11</comments><msc-class>78M32</msc-class><journal-ref>Int. J. Bio-Inspired Computation, Vol.6, No.1, pp.1-6 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now five years since the launch of the International Journal of
Bio-Inspired Computation (IJBIC). At the same time, significant new progress
has been made in the area of bio-inspired computation. This review paper
summarizes the success and achievements of IJBIC in the past five years, and
also highlights the challenges and key issues for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7802</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7802</id><created>2014-03-30</created><authors><author><keyname>Merwaday</keyname><forenames>Arvind</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sayandev</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Capacity Analysis of LTE-Advanced HetNets with Reduced Power Subframes
  and Range Expansion</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to EURASIP Journal on Wireless Communications and
  Networking (JWCN)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The time domain inter-cell interference coordination techniques specified in
LTE Rel. 10 standard improves the throughput of picocell-edge users by
protecting them from macrocell interference. On the other hand, it also
degrades the aggregate capacity in macrocell because the macro base station
(MBS) does not transmit data during certain subframes known as almost blank
subframes. The MBS data transmission using reduced power subframes was
standardized in LTE Rel. 11, which can improve the capacity in macrocell while
not causing high interference to the nearby picocells. In order to get maximum
benefit from the reduced power subframes, setting the key system parameters,
such as the amount of power reduction, carries critical importance. Using
stochastic geometry, this paper lays down a theoretical foundation for the
performance evaluation of heterogeneous networks with reduced power subframes
and range expansion bias. The analytic expressions for average capacity and 5th
percentile throughput are derived as a function of transmit powers, node
densities, and interference coordination parameters in a heterogeneous network
scenario, and are validated through Monte Carlo simulations. Joint optimization
of range expansion bias, power reduction factor, scheduling thresholds, and
duty cycle of reduced power subframes are performed to study the trade-offs
between aggregate capacity of a cell and fairness among the users. To validate
our analysis, we also compare the stochastic geometry based theoretical results
with the real MBS deployment (in the city of London) and the hexagonal-grid
model. Our analysis shows that with optimum parameter settings, the LTE Rel. 11
with reduced power subframes can provide substantially better performance than
the LTE Rel. 10 with almost blank subframes, in terms of both aggregate
capacity and fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7806</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7806</id><created>2014-03-30</created><updated>2014-10-16</updated><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>K&#xf6;tzing</keyname><forenames>Timo</forenames></author></authors><title>Unbiased Black-Box Complexities of Jump Functions</title><categories>cs.NE</categories><comments>This paper is based on results presented in the conference versions
  [GECCO 2011] and [GECCO 2014]</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the unbiased black-box complexity of jump functions with small,
medium, and large sizes of the fitness plateau surrounding the optimal
solution.
  Among other results, we show that when the jump size is $(1/2 -
\varepsilon)n$, that is, only a small constant fraction of the fitness values
is visible, then the unbiased black-box complexities for arities $3$ and higher
are of the same order as those for the simple \textsc{OneMax} function. Even
for the extreme jump function, in which all but the two fitness values $n/2$
and $n$ are blanked out, polynomial-time mutation-based (i.e., unary unbiased)
black-box optimization algorithms exist. This is quite surprising given that
for the extreme jump function almost the whole search space (all but a
$\Theta(n^{-1/2})$ fraction) is a plateau of constant fitness.
  To prove these results, we introduce new tools for the analysis of unbiased
black-box complexities, for example, selecting the new parent individual not by
comparing the fitnesses of the competing search points, but also by taking into
account the (empirical) expected fitnesses of their offspring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7811</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7811</id><created>2014-03-30</created><authors><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Rengarajan</keyname><forenames>Balaji</forenames></author><author><keyname>Widmer</keyname><forenames>Joerg</forenames></author></authors><title>Increasing Opportunistic Gain in Small Cells Through Energy-Aware User
  Cooperation</title><categories>cs.NI</categories><comments>13 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the increasing demand for wireless capacity, future networks are
likely to consist of dense layouts of small cells. The number of users in each
cell is thus reduced which results in diminished gains from opportunistic
scheduling, particularly under dynamic traffic loads. We propose an
user-initiated base station (BS)-transparent traffic spreading approach that
leverages user-user communication to increase BS scheduling flexibility. The
proposed scheme can increase opportunistic gain and improve user performance.
For a specified tradeoff between performance and power expenditure, we
characterize the optimal policy by modeling the system as a Markov decision
process and also present a heuristic algorithm that yields significant
performance gains. Our simulations show that, in the performance-centric case,
average file transfer delays are lowered by up to 20% even in homogeneous
scenarios, and up to 50% with heterogeneous users. Further, we show that the
bulk of the performance improvement can be achieved with a small increase in
power expenditure, e.g., in an energy-sensitive case, up to 78% of the
performance improvement can be typically achieved at only 20% of the power
expenditure of the performance-centric case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7827</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7827</id><created>2014-03-30</created><updated>2014-10-02</updated><authors><author><keyname>Klosik</keyname><forenames>David F.</forenames></author><author><keyname>Bornholdt</keyname><forenames>Stefan</forenames></author><author><keyname>H&#xfc;tt</keyname><forenames>Marc-Thorsten</forenames></author></authors><title>Motif-based success scores in coauthorship networks are highly sensitive
  to author name disambiguation</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>7 pages, 7 figures</comments><journal-ref>Phys. Rev. E 90, 032811 (2014)</journal-ref><doi>10.1103/PhysRevE.90.032811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the work of Krumov et al. [Eur. Phys. J. B 84, 535 (2011)] we
revisit the question whether the usage of large citation datasets allows for
the quantitative assessment of social (by means of coauthorship of
publications) influence on the progression of science. Applying a more
comprehensive and well-curated dataset containing the publications in the
journals of the American Physical Society during the whole 20th century we find
that the measure chosen in the original study, a score based on small induced
subgraphs, has to be used with caution, since the obtained results are highly
sensitive to the exact implementation of the author disambiguation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7828</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7828</id><created>2014-03-30</created><authors><author><keyname>Merry</keyname><forenames>Alexander</forenames></author></authors><title>Reasoning with !-Graphs</title><categories>cs.LO</categories><comments>DPhil (PhD) thesis; University of Oxford; 172 pages</comments><acm-class>D.1.6; F.2.2; F.4.1; F.4.2; F.4.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this thesis is to present an extension to the string graphs of
Dixon, Duncan and Kissinger that allows the finite representation of certain
infinite families of graphs and graph rewrite rules, and to demonstrate that a
logic can be built on this to allow the formalisation of inductive proofs in
the string diagrams of compact closed and traced symmetric monoidal categories.
  String diagrams provide an intuitive method for reasoning about monoidal
categories. However, this does not negate the ability for those using them to
make mistakes in proofs. To this end, there is a project (Quantomatic) to build
a proof assistant for string diagrams, at least for those based on categories
with a notion of trace. The development of string graphs has provided a
combinatorial formalisation of string diagrams, laying the foundations for this
project.
  The prevalence of commutative Frobenius algebras (CFAs) in quantum
information theory, a major application area of these diagrams, has led to the
use of variable-arity nodes as a shorthand for normalised networks of Frobenius
algebra morphisms, so-called &quot;spider notation&quot;. This notation greatly eases
reasoning with CFAs, but string graphs are inadequate to properly encode this
reasoning.
  This dissertation extends string graphs to allow for variable-arity nodes to
be represented at all, and then introduces !-box notation (and structures to
encode it) to represent string graph equations containing repeated subgraphs,
where the number of repetitions is abitrary. It then demonstrates how we can
reason directly about !-graphs, viewed as (typically infinite) families of
string graphs. Of particular note is the presentation of a form of graph-based
induction, allowing the formal encoding of proofs that previously could only be
represented as a mix of string diagrams and explanatory text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7840</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7840</id><created>2014-03-30</created><authors><author><keyname>Noyes</keyname><forenames>Andrew</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Warszawski</keyname><forenames>Todd</forenames><affiliation>Cornell University</affiliation></author><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Pavol</forenames><affiliation>University of Colorado Boulder</affiliation></author><author><keyname>Foster</keyname><forenames>Nate</forenames><affiliation>Cornell University</affiliation></author></authors><title>Toward Synthesis of Network Updates</title><categories>cs.PL cs.LO cs.NI</categories><comments>In Proceedings SYNT 2013, arXiv:1403.7264</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 142, 2014, pp. 8-23</journal-ref><doi>10.4204/EPTCS.142.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Updates to network configurations are notoriously difficult to implement
correctly. Even if the old and new configurations are correct, the update
process can introduce transient errors such as forwarding loops, dropped
packets, and access control violations. The key factor that makes updates
difficult to implement is that networks are distributed systems with hundreds
or even thousands of nodes, but updates must be rolled out one node at a time.
In networks today, the task of determining a correct sequence of updates is
usually done manually -- a tedious and error-prone process for network
operators. This paper presents a new tool for synthesizing network updates
automatically. The tool generates efficient updates that are guaranteed to
respect invariants specified by the operator. It works by navigating through
the (restricted) space of possible solutions, learning from counterexamples to
improve scalability and optimize performance. We have implemented our tool in
OCaml, and conducted experiments showing that it scales to networks with a
thousand switches and tens of switches updating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7841</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7841</id><created>2014-03-30</created><updated>2014-04-07</updated><authors><author><keyname>Andr&#xe9;</keyname><forenames>&#xc9;tienne</forenames><affiliation>Universit&#xe9; Paris 13, Sorbonne Paris Cit&#xe9;, LIPN, CNRS, Villetaneuse, France</affiliation></author><author><keyname>Frehse</keyname><forenames>Goran</forenames><affiliation>Verimag, Grenoble, France</affiliation></author></authors><title>Proceedings 1st International Workshop on Synthesis of Continuous
  Parameters</title><categories>cs.SC cs.FL cs.SY</categories><proxy>EPTCS</proxy><acm-class>F.1.1; D.4.7; F.1.2; D.2.4</acm-class><journal-ref>EPTCS 145, 2014</journal-ref><doi>10.4204/EPTCS.145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 1st International Workshop on
Synthesis of Continuous Parameters (SynCoP'14). The workshop was held in
Grenoble, France on April 6th, 2014, as a satellite event of the 17th European
Joint Conferences on Theory and Practice of Software (ETAPS'14).
  SynCoP aims at bringing together researchers working on parameter synthesis
for systems with continuous variables, where the parameters consist of a
(usually dense) set of constant values. Synthesis problems for such parameters
arise for real-time, hybrid or probabilistic systems in a large variety
application domains. A parameter could be, e.g., a delay in a real-time system,
or a reaction rate in a biological cell model. The objective of the synthesis
problem is to identify suitable parameters to achieve desired behavior, or to
verify the behavior for a given range of parameter values.
  This volume contains seven contributions: two invited talks and five regular
papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7846</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7846</id><created>2014-03-30</created><authors><author><keyname>Liu</keyname><forenames>Xiaoyi Leo</forenames></author><author><keyname>Koyuncu</keyname><forenames>Erdem</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Distributed Channel Quantization for Two-User Interference Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce conferencing-based distributed channel quantizers for two-user
interference networks where interference signals are treated as noise. Compared
with the conventional distributed quantizers where each receiver quantizes its
own channel independently, the proposed quantizers allow multiple rounds of
feedback communication in the form of conferencing between receivers. We take
the network outage probabilities of sum rate and minimum rate as performance
measures and consider quantizer design in the transmission strategies of time
sharing and interference transmission. First, we propose distributed quantizers
that achieve the optimal network outage probability of sum rate for both time
sharing and interference transmission strategies with an average feedback rate
of only two bits per channel state. Then, for the time sharing strategy, we
propose a distributed quantizer that achieves the optimal network outage
probability of minimum rate with finite average feedback rate; conventional
quantizers require infinite rate to achieve the same performance. For the
interference transmission strategy, a distributed quantizer that can approach
the optimal network outage probability of minimum rate closely is also
proposed. Numerical simulations confirm that our distributed quantizers based
on conferencing outperform the conventional ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7851</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7851</id><created>2014-03-30</created><updated>2014-04-28</updated><authors><author><keyname>Taranalli</keyname><forenames>Veeresh</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>Adaptive Linear Programming Decoding of Polar Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, to be presented at the IEEE Symposium on
  Information Theory (ISIT) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are high density parity check codes and hence the sparse factor
graph, instead of the parity check matrix, has been used to practically
represent an LP polytope for LP decoding. Although LP decoding on this polytope
has the ML-certificate property, it performs poorly over a BAWGN channel. In
this paper, we propose modifications to adaptive cut generation based LP
decoding techniques and apply the modified-adaptive LP decoder to short
blocklength polar codes over a BAWGN channel. The proposed decoder provides
significant FER performance gain compared to the previously proposed LP decoder
and its performance approaches that of ML decoding at high SNRs. We also
present an algorithm to obtain a smaller factor graph from the original sparse
factor graph of a polar code. This reduced factor graph preserves the small
check node degrees needed to represent the LP polytope in practice. We show
that the fundamental polytope of the reduced factor graph can be obtained from
the projection of the polytope represented by the original sparse factor graph
and the frozen bit information. Thus, the LP decoding time complexity is
decreased without changing the FER performance by using the reduced factor
graph representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7869</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7869</id><created>2014-03-31</created><authors><author><keyname>Benmammar</keyname><forenames>Badr</forenames><affiliation>LTT</affiliation></author></authors><title>Application des techniques d'ench\`eres dans les r\'eseaux de radio
  cognitive</title><categories>cs.NI cs.MA</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid proliferation of standards and radio services in recent years
caused the problem of spectrum scarcity. The main objective of Cognitive Radio
(CR) is to facilitate access to radio spectrum. Our contribution in this paper
is the use of auctions to solve the problem of spectrum congestion in the
context of CR, for that, we will combine the theory of auctions with
multi-agent systems. Our approach has shown that it is preferable to use the
Sealed-bid Auction with dynamic programming because this method has many
advantages over other methods.
  ---
  La prolif\'eration rapide de standards et services de radiocommunication ces
derni\`eres ann\'ees provoquent le probl\`eme de la p\'enurie du spectre. Dans
ce contexte, l'objectif principal de la Radio Cognitive (RC) est de faciliter
l'acc\`es au spectre radio. Notre contribution dans le cadre de ce papier est
l'utilisation des ench\`eres pour r\'esoudre le probl\`eme de l'encombrement du
spectre dans le cadre de la RC. Pour cela, nous avons combin\'e la th\'eorie
des ench\`eres avec les syst\`emes multi agents. Notre approche a prouv\'e
qu'il est pr\'ef\'erable d'utiliser les ench\`eres \`a Enveloppe Scell\'ee avec
programmation dynamique car cette m\'ethode a beaucoup d'avantages par rapport
aux autres m\'ethodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7870</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7870</id><created>2014-03-31</created><updated>2014-12-18</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Optimized Training Design for Wireless Energy Transfer</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures, to appear in IEEE Trans. on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio-frequency (RF) enabled wireless energy transfer (WET), as a promising
solution to provide cost-effective and reliable power supplies for
energy-constrained wireless networks, has drawn growing interests recently. To
overcome the significant propagation loss over distance, employing
multi-antennas at the energy transmitter (ET) to more efficiently direct
wireless energy to desired energy receivers (ERs), termed \emph{energy
beamforming}, is an essential technique for enabling WET. However, the
achievable gain of energy beamforming crucially depends on the available
channel state information (CSI) at the ET, which needs to be acquired
practically. In this paper, we study the design of an efficient channel
acquisition method for a point-to-point multiple-input multiple-output (MIMO)
WET system by exploiting the channel reciprocity, i.e., the ET estimates the
CSI via dedicated reverse-link training from the ER. Considering the limited
energy availability at the ER, the training strategy should be carefully
designed so that the channel can be estimated with sufficient accuracy, and yet
without consuming excessive energy at the ER. To this end, we propose to
maximize the \emph{net} harvested energy at the ER, which is the average
harvested energy offset by that used for channel training. An optimization
problem is formulated for the training design over MIMO Rician fading channels,
including the subset of ER antennas to be trained, as well as the training time
and power allocated. Closed-form solutions are obtained for some special
scenarios, based on which useful insights are drawn on when training should be
employed to improve the net transferred energy in MIMO WET systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7872</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7872</id><created>2014-03-31</created><authors><author><keyname>Gu</keyname><forenames>Chenjie</forenames></author><author><keyname>Zaheer</keyname><forenames>Manzil</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Multiple-Population Moment Estimation: Exploiting Inter-Population
  Correlation for Efficient Moment Estimation in Analog/Mixed-Signal Validation</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moment estimation is an important problem during circuit validation, in both
pre-Silicon and post-Silicon stages. From the estimated moments, the
probability of failure and parametric yield can be estimated at each circuit
configuration and corner, and these metrics are used for design optimization
and making product qualification decisions. The problem is especially difficult
if only a very small sample size is allowed for measurement or simulation, as
is the case for complex analog/mixed-signal circuits. In this paper, we propose
an efficient moment estimation method, called Multiple-Population Moment
Estimation (MPME), that significantly improves estimation accuracy under small
sample size. The key idea is to leverage the data collected under different
corners/configurations to improve the accuracy of moment estimation at each
individual corner/configuration. Mathematically, we employ the hierarchical
Bayesian framework to exploit the underlying correlation in the data. We apply
the proposed method to several datasets including post-silicon measurements of
a commercial high-speed I/O link, and demonstrate an average error reduction of
up to 2$\times$, which can be equivalently translated to significant reduction
of validation time and cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7876</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7876</id><created>2014-03-31</created><authors><author><keyname>Galoogahi</keyname><forenames>Hamed Kiani</forenames></author><author><keyname>Sim</keyname><forenames>Terence</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Correlation Filters with Limited Boundaries</title><categories>cs.CV</categories><comments>8 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation filters take advantage of specific properties in the Fourier
domain allowing them to be estimated efficiently: O(NDlogD) in the frequency
domain, versus O(D^3 + ND^2) spatially where D is signal length, and N is the
number of signals. Recent extensions to correlation filters, such as MOSSE,
have reignited interest of their use in the vision community due to their
robustness and attractive computational properties. In this paper we
demonstrate, however, that this computational efficiency comes at a cost.
Specifically, we demonstrate that only 1/D proportion of shifted examples are
unaffected by boundary effects which has a dramatic effect on
detection/tracking performance. In this paper, we propose a novel approach to
correlation filter estimation that: (i) takes advantage of inherent
computational redundancies in the frequency domain, and (ii) dramatically
reduces boundary effects. Impressive object tracking and detection results are
presented in terms of both accuracy and computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7877</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7877</id><created>2014-03-31</created><updated>2015-03-31</updated><authors><author><keyname>Jia</keyname><forenames>Kui</forenames></author><author><keyname>Chan</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Zeng</keyname><forenames>Zinan</forenames></author><author><keyname>Gao</keyname><forenames>Shenghua</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Zhang</keyname><forenames>Tianzhu</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>ROML: A Robust Feature Correspondence Approach for Matching Objects in A
  Set of Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature-based object matching is a fundamental problem for many applications
in computer vision, such as object recognition, 3D reconstruction, tracking,
and motion segmentation. In this work, we consider simultaneously matching
object instances in a set of images, where both inlier and outlier features are
extracted. The task is to identify the inlier features and establish their
consistent correspondences across the image set. This is a challenging
combinatorial problem, and the problem complexity grows exponentially with the
image number. To this end, we propose a novel framework, termed ROML, to
address this problem. ROML optimizes simultaneously a partial permutation
matrix (PPM) for each image, and feature correspondences are established by the
obtained PPMs. Two of our key contributions are summarized as follows. (1) We
formulate the problem as rank and sparsity minimization for PPM optimization,
and treat simultaneous optimization of multiple PPMs as a regularized consensus
problem in the context of distributed optimization. (2) We use the ADMM method
to solve the thus formulated ROML problem, in which a subproblem associated
with a single PPM optimization appears to be a difficult integer quadratic
program (IQP). We prove that under wildly applicable conditions, this IQP is
equivalent to a linear sum assignment problem (LSAP), which can be efficiently
solved to an exact solution. Extensive experiments on rigid/non-rigid object
matching, matching instances of a common object category, and common object
localization show the efficacy of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7879</identifier>
 <datestamp>2014-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7879</id><created>2014-03-31</created><authors><author><keyname>Xie</keyname><forenames>Wen-Jie</forenames><affiliation>ECUST</affiliation></author><author><keyname>Li</keyname><forenames>Ming-Xia</forenames><affiliation>ECUST</affiliation></author><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames><affiliation>ECUST</affiliation></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames><affiliation>ECUST</affiliation></author></authors><title>Triadic motifs in the dependence networks of virtual societies</title><categories>physics.soc-ph cs.SI</categories><comments>8 Latex pages + 5 figures</comments><journal-ref>Scientific Reports 4, 5244 (2014)</journal-ref><doi>10.1038/srep05244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In friendship networks, individuals have different numbers of friends, and
the closeness or intimacy between an individual and her friends is
heterogeneous. Using a statistical filtering method to identify relationships
about who depends on whom, we construct dependence networks (which are
directed) from weighted friendship networks of avatars in more than two hundred
virtual societies of a massively multiplayer online role-playing game (MMORPG).
We investigate the evolution of triadic motifs in dependence networks. Several
metrics show that the virtual societies evolved through a transient stage in
the first two to three weeks and reached a relatively stable stage. We find
that the unidirectional loop motif (${\rm{M}}_9$) is underrepresented and does
not appear, open motifs are also underrepresented, while other close motifs are
overrepresented. We also find that, for most motifs, the overall level
difference of the three avatars in the same motif is significantly lower than
average, whereas the sum of ranks is only slightly larger than average. Our
findings show that avatars' social status plays an important role in the
formation of triadic motifs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7883</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7883</id><created>2014-03-31</created><updated>2015-12-08</updated><authors><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Ma</keyname><forenames>Zheng</forenames></author></authors><title>Multiple-Access Relay Wiretap Channel</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Information Forensics &amp; Security.
  arXiv admin note: text overlap with arXiv:1312.6784; text overlap with
  arXiv:cs/0612044 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the effects of an additional trusted relay node
on the secrecy of multiple-access wiretap channel (MAC-WT) by considering the
model of multiple-access relay wiretap channel (MARC-WT). More specifically,
first, we investigate the discrete memoryless MARC-WT. Three inner bounds (with
respect to decode-forward (DF), noise-forward (NF) and compress-forward (CF)
strategies) on the secrecy capacity region are provided. Second, we investigate
the degraded discrete memoryless MARC-WT, and present an outer bound on the
secrecy capacity region of this degraded model. Finally, we investigate the
Gaussian MARC-WT, and find that the NF and CF strategies help to enhance
Tekin-Yener's achievable secrecy rate region of Gaussian MAC-WT. Moreover, we
find that if the noise variance of the transmitters-relay channel is smaller
than that of the transmitters-receiver channel, the DF strategy may also
enhance Tekin-Yener's achievable secrecy rate region of Gaussian MAC-WT, and it
may perform even better than the NF and CF strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7890</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7890</id><created>2014-03-31</created><authors><author><keyname>Chang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Li</keyname><forenames>Rongjian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Sparse K-Means with $\ell_{\infty}/\ell_0$ Penalty for High-Dimensional
  Data Clustering</title><categories>stat.ML cs.LG stat.ME</categories><comments>36 pages, 4 figures, Present the paper at ICSA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse clustering, which aims to find a proper partition of an extremely
high-dimensional data set with redundant noise features, has been attracted
more and more interests in recent years. The existing studies commonly solve
the problem in a framework of maximizing the weighted feature contributions
subject to a $\ell_2/\ell_1$ penalty. Nevertheless, this framework has two
serious drawbacks: One is that the solution of the framework unavoidably
involves a considerable portion of redundant noise features in many situations,
and the other is that the framework neither offers intuitive explanations on
why this framework can select relevant features nor leads to any theoretical
guarantee for feature selection consistency.
  In this article, we attempt to overcome those drawbacks through developing a
new sparse clustering framework which uses a $\ell_{\infty}/\ell_0$ penalty.
First, we introduce new concepts on optimal partitions and noise features for
the high-dimensional data clustering problems, based on which the previously
known framework can be intuitively explained in principle. Then, we apply the
suggested $\ell_{\infty}/\ell_0$ framework to formulate a new sparse k-means
model with the $\ell_{\infty}/\ell_0$ penalty ($\ell_0$-k-means for short). We
propose an efficient iterative algorithm for solving the $\ell_0$-k-means. To
deeply understand the behavior of $\ell_0$-k-means, we prove that the solution
yielded by the $\ell_0$-k-means algorithm has feature selection consistency
whenever the data matrix is generated from a high-dimensional Gaussian mixture
model. Finally, we provide experiments with both synthetic data and the Allen
Developing Mouse Brain Atlas data to support that the proposed $\ell_0$-k-means
exhibits better noise feature detection capacity over the previously known
sparse k-means with the $\ell_2/\ell_1$ penalty ($\ell_1$-k-means for short).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7899</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7899</id><created>2014-03-31</created><authors><author><keyname>van Hoek</keyname><forenames>Wilko</forenames></author><author><keyname>Shen</keyname><forenames>Wei</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Identifying User Behavior in domain-specific Repositories</title><categories>cs.DL cs.IR</categories><comments>Elpub Conference 2014, 10 pages, 5 figures</comments><msc-class>68P20</msc-class><acm-class>H.3.7; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an analysis of the user behavior of two different
domain-specific repositories. The web analytic tool etracker was used to gain a
first overall insight into the user behavior of these repositories. Moreover,
we extended our work to describe an apache web log analysis approach which
focuses on the identification of the user behavior. Therefore the user traffic
within our systems is visualized using chord diagrams. We could find that
recommendations are used frequently and users do rarely combine searching with
faceting or filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7920</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7920</id><created>2014-03-31</created><authors><author><keyname>Elia</keyname><forenames>Michele</forenames></author><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author></authors><title>Computing the dimension of ideals in group algebras, with an application
  to coding theory</title><categories>cs.IT math.IT math.RA</categories><comments>12 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing the dimension of a left/right ideal in a
group algebra F[G] of a finite group G over a field F, by relating the
dimension to the rank of an appropriate matrix, originating from a regular
right/left representation of G. In particular, the dimension of a principal
ideal is equal to the rank of the matrix representing a generator. From this
observation, we establish a bound and an efficient algorithm for the
computation of the dimension of an ideal in a group ring. Since group codes are
ideals in finite group rings, our algorithm allows to efficiently compute their
dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7922</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7922</id><created>2014-03-31</created><updated>2016-01-15</updated><authors><author><keyname>Aragona</keyname><forenames>R.</forenames></author><author><keyname>Calderini</keyname><forenames>M.</forenames></author><author><keyname>Maccauro</keyname><forenames>D.</forenames></author><author><keyname>Sala</keyname><forenames>M.</forenames></author></authors><title>On weak differential uniformity of vectorial Boolean functions as a
  cryptographic criterion</title><categories>cs.CR</categories><comments>to appear in Applicable Algebra in Engineering, Communication and
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relation among some security parameters for vectorial Boolean
functions which prevent attacks on the related block cipher. We focus our study
on a recently-introduced security criterion, called weak differential
uniformity, which prevents the existence of an undetectable trapdoor based on
imprimitive group action. We present some properties of functions with low weak
differential uniformity, especially for the case of power functions and 4-bit
S-Boxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7923</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7923</id><created>2014-03-31</created><authors><author><keyname>Friberg</keyname><forenames>Anders</forenames></author><author><keyname>Schoonderwaldt</keyname><forenames>Erwin</forenames></author><author><keyname>Hedblad</keyname><forenames>Anton</forenames></author><author><keyname>Fabiani</keyname><forenames>Marco</forenames></author><author><keyname>Elowsson</keyname><forenames>Anders</forenames></author></authors><title>Using perceptually defined music features in music information retrieval</title><categories>cs.IR cs.SD</categories><comments>submitted to the Journal of the Acoustical Society of America January
  9, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, the notion of perceptual features is introduced for describing
general music properties based on human perception. This is an attempt at
rethinking the concept of features, in order to understand the underlying human
perception mechanisms. Instead of using concepts from music theory such as
tones, pitches, and chords, a set of nine features describing overall
properties of the music was selected. They were chosen from qualitative
measures used in psychology studies and motivated from an ecological approach.
The selected perceptual features were rated in two listening experiments using
two different data sets. They were modeled both from symbolic (MIDI) and audio
data using different sets of computational features. Ratings of emotional
expression were predicted using the perceptual features. The results indicate
that (1) at least some of the perceptual features are reliable estimates; (2)
emotion ratings could be predicted by a small combination of perceptual
features with an explained variance up to 90%; (3) the perceptual features
could only to a limited extent be modeled using existing audio features. The
results also clearly indicated that a small number of dedicated features were
superior to a 'brute force' model using a large number of general audio
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7927</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7927</id><created>2014-03-31</created><authors><author><keyname>Dunster</keyname><forenames>T. M.</forenames></author><author><keyname>Gil</keyname><forenames>A.</forenames></author><author><keyname>Segura</keyname><forenames>J.</forenames></author><author><keyname>Temme</keyname><forenames>N. M.</forenames></author></authors><title>Computation of a numerically satisfactory pair of solutions of the
  differential equation for conical functions of non-negative integer orders</title><categories>math.CA cs.NA math.NA</categories><comments>To be published in Numerical Algoritms</comments><doi>10.1007/s11075-014-9857-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing satisfactory pairs of solutions of the
differential equation for Legendre functions of non-negative integer order
$\mu$ and degree $-\frac12+i\tau$, where $\tau$ is a non-negative real
parameter. Solutions of this equation are the conical functions
${\rm{P}}^{\mu}_{-\frac12+i\tau}(x)$ and ${Q}^{\mu}_{-\frac12+i\tau}(x)$,
$x&gt;-1$. An algorithm for computing a numerically satisfactory pair of solutions
is already available when $-1&lt;x&lt;1$ (see \cite{gil:2009:con},
\cite{gil:2012:cpc}).In this paper, we present a stable computational scheme
for a real valued numerically satisfactory companion of the function
${\rm{P}}^{\mu}_{-\frac12+i\tau}(x)$ for $x&gt;1$, the function
$\Re\left\{e^{-i\pi \mu} {{Q}}^{\mu}_{-\frac{1}{2}+i\tau}(x) \right\}$. The
proposed algorithm allows the computation of the function on a large parameter
domain without requiring the use of extended precision arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7928</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7928</id><created>2014-03-31</created><authors><author><keyname>Urban</keyname><forenames>J.</forenames></author><author><keyname>Pipek</keyname><forenames>J.</forenames></author><author><keyname>Hron</keyname><forenames>M.</forenames></author><author><keyname>Janky</keyname><forenames>F.</forenames></author><author><keyname>Pap&#x159;ok</keyname><forenames>R.</forenames></author><author><keyname>Peterka</keyname><forenames>M.</forenames></author><author><keyname>Duarte</keyname><forenames>A. S.</forenames></author></authors><title>Integrated Data Acquisition, Storage, Retrieval and Processing Using the
  COMPASS DataBase (CDB)</title><categories>cs.DB</categories><doi>10.1016/j.fusengdes.2014.03.032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complex data handling system for the COMPASS tokamak, operated
by IPP ASCR Prague, Czech Republic [1]. The system, called CDB (Compass
DataBase), integrates different data sources as an assortment of data
acquisition hardware and software from different vendors is used. Based on
widely available open source technologies wherever possible, CDB is vendor and
platform independent and it can be easily scaled and distributed. The data is
directly stored and retrieved using a standard NAS (Network Attached Storage),
hence independent of the particular technology; the description of the data
(the metadata) is recorded in a relational database. Database structure is
general and enables the inclusion of multi-dimensional data signals in multiple
revisions (no data is overwritten). This design is inherently distributed as
the work is off-loaded to the clients. Both NAS and database can be implemented
and optimized for fast local access as well as secure remote access. CDB is
implemented in Python language; bindings for Java, C/C++, IDL and Matlab are
provided. Independent data acquisitions systems as well as nodes managed by
FireSignal [2] are all integrated using CDB. An automated data post-processing
server is a part of CDB. Based on dependency rules, the server executes, in
parallel if possible, prescribed post-processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7933</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7933</id><created>2014-03-31</created><authors><author><keyname>Li</keyname><forenames>Ruihu</forenames></author><author><keyname>Li</keyname><forenames>Xueliang</forenames></author><author><keyname>Mao</keyname><forenames>Yaping</forenames></author><author><keyname>Wei</keyname><forenames>Meiqin</forenames></author></authors><title>Additive codes over $GF(4)$ from circulant graphs</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1402.6399</comments><msc-class>94B05, 05C50, 05C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In $2006$, Danielsen and Parker \cite{DP} proved that every self-dual
additive code over $GF(4)$ is equivalent to a graph code. So, graph is an
important tool for searching (proposed) optimum codes. In this paper, we
introduce a new method of searching (proposed) optimum additive codes from
circulant graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7939</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7939</id><created>2014-03-31</created><updated>2014-12-04</updated><authors><author><keyname>Bokowski</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Pilaud</keyname><forenames>Vincent</forenames></author></authors><title>Quasi-configurations: building blocks for point-line configurations</title><categories>cs.CG cs.DM math.CO</categories><comments>12 pages, 9 figures</comments><msc-class>52C30</msc-class><journal-ref>Ars Math. Contemp., 10(1): 99-112, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study point-line incidence structures and their properties in the
projective plane. Our motivation is the problem of the existence of $(n_4)$
configurations, still open for few remaining values of $n$. Our approach is
based on quasi-configurations: point-line incidence structures where each point
is incident to at least $3$ lines and each line is incident to at least $3$
points. We investigate the existence problem for these quasi-configurations,
with a particular attention to $3|4$-configurations where each element is $3$-
or $4$-valent. We use these quasi-configurations to construct the first
$(37_4)$ and $(43_4)$ configurations. The existence problem of finding
$(22_4)$, $(23_4)$, and $(26_4)$ configurations remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7948</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7948</id><created>2014-03-31</created><updated>2015-11-23</updated><authors><author><keyname>Alkan</keyname><forenames>Ferhat</forenames></author><author><keyname>B&#x131;y&#x131;ko&#x11f;lu</keyname><forenames>T&#xfc;rker</forenames></author><author><keyname>Demange</keyname><forenames>Marc</forenames></author><author><keyname>Erten</keyname><forenames>Cesim</forenames></author></authors><title>Constrained Alignments of a Pair of Graphs</title><categories>cs.DS cs.CE</categories><comments>18 pages, 5 figures, submitted to DAM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the constrained graph alignment problem which has applications in
biological network analysis studies. Given two input graphs $G_1, G_2$, a pair
of vertex mappings induces an edge conservation if the vertex pairs are
adjacent in their respective graphs. In general terms the goal is to provide a
one-to-one mapping between the vertices of the input graphs such that edge
conservation is maximized. However the allowed mappings are restricted. Let
$m_1$ ($m_2$) denote the number of $G_2$-vertices ($G_1$-vertices) that each
$G_1$-vertex ($G_2$-vertex) is allowed to be mapped to. All provided results
assume $m_2=1$, except for the fixed-parameter tractability result for bounded
degree graphs which applies to the more general setting of any constant $m_1,
m_2$. We present a polynomial time solution for the special case where $G_1$ is
acyclic. Relaxing the constraint on $G_1$, for the setting of $m_1=2$, we
provide several structural properties that lead to polynomial-time
approximation algorithms. We then relax the constraint on $m_1$ and consider
any positive integer constant $m_1$. We provide further structural properties
for this setting which lead to several additional approximation algorithms with
approximation ratios better than those of the previous studies. For the same
setting, we also show that the problem is fixed-parameter tractable,
parameterized only by the output size. Previously the same result was known
only for bounded degree graphs. We finally consider the more general setting
where $m_1, m_2$ can be any positive integer constant and provide an
approximation algorithm and a fixed-parameter tractability result that applies
to bounded degree graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7970</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7970</id><created>2014-03-31</created><updated>2014-11-25</updated><authors><author><keyname>Novara</keyname><forenames>Carlo</forenames></author></authors><title>Direct design of LPV feedback controllers: technical details and
  numerical examples</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper contains technical details of recent results developed by the
author, regarding the design of LPV controllers directly from experimental
data. Two numerical examples are also presented, about control of the Duffing
oscillator and control of a two-degree-of-freedom manipulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7980</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7980</id><created>2014-03-31</created><updated>2016-01-13</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Schulz</keyname><forenames>Andre</forenames></author></authors><title>Embedding Stacked Polytopes on a Polynomial-Size Grid</title><categories>cs.CG cs.DM math.CO</categories><comments>21 pages, 10 Figures</comments><msc-class>52B11, 68R10</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stacking operation adds a $d$-simplex on top of a facet of a simplicial
$d$-polytope while maintaining the convexity of the polytope. A stacked
$d$-polytope is a polytope that is obtained from a $d$-simplex and a series of
stacking operations. We show that for a fixed $d$ every stacked $d$-polytope
with $n$ vertices can be realized with nonnegative integer coordinates. The
coordinates are bounded by $O(n^{2\log(2d)})$, except for one axis, where the
coordinates are bounded by $O(n^{3\log(2d)})$. The described realization can be
computed with an easy algorithm.
  The realization of the polytopes is obtained with a lifting technique which
produces an embedding on a large grid. We establish a rounding scheme that
places the vertices on a sparser grid, while maintaining the convexity of the
embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7985</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7985</id><created>2014-03-31</created><updated>2014-07-24</updated><authors><author><keyname>Geil</keyname><forenames>Olav</forenames></author><author><keyname>Martin</keyname><forenames>Stefano</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author><author><keyname>Luo</keyname><forenames>Yuan</forenames></author></authors><title>Relative generalized Hamming weights of one-point algebraic geometric
  codes</title><categories>cs.IT math.AG math.IT</categories><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 10, pp.
  5938-5949, October 2014</journal-ref><doi>10.1109/TIT.2014.2345375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of linear ramp secret sharing schemes can be characterized by the
relative generalized Hamming weights of the involved codes. In this paper we
elaborate on the implication of these parameters and we devise a method to
estimate their value for general one-point algebraic geometric codes. As it is
demonstrated, for Hermitian codes our bound is often tight. Furthermore, for
these codes the relative generalized Hamming weights are often much larger than
the corresponding generalized Hamming weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7987</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7987</id><created>2014-03-31</created><authors><author><keyname>Nikolaev</keyname><forenames>Sergei</forenames></author></authors><title>Implementation of interaction between soft tissues and foreign bodies
  using modified voxel model</title><categories>cs.CG cs.GR</categories><comments>5 pages, 9 figures</comments><journal-ref>Journal Computer Tools in Education. 3 (2013) 28-32</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Interactive bodies collision detection and elimination is one of the most
popular task nowadays. Collisions can be detected in different ways. Collision
search using space voxelization is one of the most fast. This paper describes
improved voxel model that covers only area of collision interest and quickly
eliminates collisions. This new method can be useful in real time collision
processing of different rigid and soft bodies grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7991</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7991</id><created>2014-03-31</created><authors><author><keyname>Venero</keyname><forenames>Mirtha Lina Fern&#xe1;ndez</forenames></author><author><keyname>da Silva</keyname><forenames>Fl&#xe1;vio Soares Corr&#xea;a</forenames></author></authors><title>A general translation from nested Petri nets into PROMELA</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested Petri nets have been applied for modeling interaction protocols,
mobility, adaptive systems and interorganizational workflows. However, few
results have been reported on the use of automated tools for analyzing the
behavior of these nets. In this paper we present a general translation from
nested Petri nets into PROMELA and explain how some properties of these nets
can be studied using SPIN model checker. Besides, we discuss how to deal with
the main limitations that may influence SPIN performance when verifying
practical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.7997</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.7997</id><created>2014-03-31</created><updated>2015-08-13</updated><authors><author><keyname>Gregoriades</keyname><forenames>Vassilios</forenames></author><author><keyname>Kisp&#xe9;ter</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>A comparison of concepts from computable analysis and effective
  descriptive set theory</title><categories>cs.LO math.GN</categories><comments>accepted for publication in the special issue of &quot;Mathematical
  Structures in Computer Science&quot; dedicated to CCC 2013</comments><msc-class>03E15, 03D78</msc-class><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computable analysis and effective descriptive set theory are both concerned
with complete metric spaces, functions between them and subsets thereof in an
effective setting. The precise relationship of the various definitions used in
the two disciplines has so far been neglected, a situation this paper is meant
to remedy.
  As the role of the Cauchy completion is relevant for both effective
approaches to Polish spaces, we consider the interplay of effectivity and
completion in some more detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8002</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8002</id><created>2014-03-31</created><updated>2014-12-07</updated><authors><author><keyname>Steinerberger</keyname><forenames>Stefan</forenames></author></authors><title>A Remark on Disk Packings and Numerical Integration of Harmonic
  Functions</title><categories>math.NA cs.DM math.MG</categories><comments>to appear in Journal of Complexity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the following problem: given an open, bounded domain
$\Omega \subset \mathbb{R}^2$, what is the largest constant $\alpha =
\alpha(\Omega) &gt; 0$ such that there exist an infinite sequence of disks $B_1,
B_2, \dots, B_N, \dots \subset \mathbb{R}^2$ and a sequence $(n_i)$ with $n_i
\in \left\{1,2\right\}$ such that $$ \sup_{N \in \mathbb{N}}{N^{\alpha}\left\|
\chi_{\Omega} -
\sum_{i=1}^{N}{(-1)^{n_i}\chi_{B_i}}\right\|_{L^1(\mathbb{R}^2)}} &lt; \infty,$$
where $\chi$ denotes the characteristic function? We prove that certain
(somewhat peculiar) domains $\Omega \subset \mathbb{R}^2$ satisfy the property
with $\alpha = 0.53$. For these domains there exists a sequence of points
$(x_i)_{i=1}^{\infty}$ in $\Omega$ with weights $(a_i)_{i=1}^{\infty}$ such
that for all harmonic functions $u:\mathbb{R}^2 \rightarrow \mathbb{R}$ $$
\left|\int_{\Omega}{u(x)dx} - \sum_{i=1}^{N}{a_i u(x_i)}\right| \leq
C_{\Omega}\frac{\|u\|_{L^{\infty}(\Omega)}}{N^{0.53}},$$ where $C_{\Omega}$
depends only on $\Omega$. This gives a Quasi-Monte-Carlo method for harmonic
functions which improves on the probabilistic Monte-Carlo bound
$\|u\|_{L^{2}(\Omega)}/N^{0.5}$ \textit{without} introducing a dependence on
the total variation. We do not know which decay rates are optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8003</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8003</id><created>2014-03-31</created><authors><author><keyname>Rathke</keyname><forenames>Fabian</forenames></author><author><keyname>Schmidt</keyname><forenames>Stefan</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Probabilistic Intra-Retinal Layer Segmentation in 3-D OCT Images Using
  Global Shape Regularization</title><categories>cs.CV</categories><comments>Accepted for publication in Medical Image Analysis (MIA), Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the introduction of spectral-domain optical coherence tomography (OCT),
resulting in a significant increase in acquisition speed, the fast and accurate
segmentation of 3-D OCT scans has become evermore important. This paper
presents a novel probabilistic approach, that models the appearance of retinal
layers as well as the global shape variations of layer boundaries. Given an OCT
scan, the full posterior distribution over segmentations is approximately
inferred using a variational method enabling efficient probabilistic inference
in terms of computationally tractable model components: Segmenting a full 3-D
volume takes around a minute. Accurate segmentations demonstrate the benefit of
using global shape regularization: We segmented 35 fovea-centered 3-D volumes
with an average unsigned error of 2.46 $\pm$ 0.22 {\mu}m as well as 80 normal
and 66 glaucomatous 2-D circular scans with errors of 2.92 $\pm$ 0.53 {\mu}m
and 4.09 $\pm$ 0.98 {\mu}m respectively. Furthermore, we utilized the inferred
posterior distribution to rate the quality of the segmentation, point out
potentially erroneous regions and discriminate normal from pathological scans.
No pre- or postprocessing was required and we used the same set of parameters
for all data sets, underlining the robustness and out-of-the-box nature of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8006</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8006</id><created>2014-03-31</created><authors><author><keyname>Tousimojarad</keyname><forenames>Ashkan</forenames></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>Cache-aware Parallel Programming for Manycore Processors</title><categories>cs.DC cs.PF</categories><comments>This work was presented at the international symposium on Highly-
  Efficient Accelerators and Reconfigurable Technologies (HEART2013),
  Edinburgh, Scotland, June 13-14, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With rapidly evolving technology, multicore and manycore processors have
emerged as promising architectures to benefit from increasing transistor
numbers. The transition towards these parallel architectures makes today an
exciting time to investigate challenges in parallel computing. The TILEPro64 is
a manycore accelerator, composed of 64 tiles interconnected via multiple 8x8
mesh networks. It contains per-tile caches and supports cache-coherent shared
memory by default. In this paper we present a programming technique to take
advantages of distributed caching facilities in manycore processors. However,
unlike other work in this area, our approach does not use architecture-specific
libraries. Instead, we provide the programmer with a novel technique on how to
program future Non-Uniform Cache Architecture (NUCA) manycore systems, bearing
in mind their caching organisation. We show that our localised programming
approach can result in a significant improvement of the parallelisation
efficiency (speed-up).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8008</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8008</id><created>2014-03-31</created><authors><author><keyname>Yu</keyname><forenames>Ye</forenames></author><author><keyname>Chen</keyname><forenames>Qian</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Distributed Collaborative Monitoring in Software Defined Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Distributed and Collaborative Monitoring system, DCM, with the
following properties. First, DCM allow switches to collaboratively achieve flow
monitoring tasks and balance measurement load. Second, DCM is able to perform
per-flow monitoring, by which different groups of flows are monitored using
different actions. Third, DCM is a memory-efficient solution for switch data
plane and guarantees system scalability. DCM uses a novel two-stage Bloom
filters to represent monitoring rules using small memory space. It utilizes the
centralized SDN control to install, update, and reconstruct the two-stage Bloom
filters in the switch data plane. We study how DCM performs two representative
monitoring tasks, namely flow size counting and packet sampling, and evaluate
its performance. Experiments using real data center and ISP traffic data on
real network topologies show that DCM achieves highest measurement accuracy
among existing solutions given the same memory budget of switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8020</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8020</id><created>2014-03-31</created><authors><author><keyname>Tousimojarad</keyname><forenames>Ashkan</forenames></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>An Efficient Thread Mapping Strategy for Multiprogramming on Manycore
  Processors</title><categories>cs.DC cs.PF</categories><comments>ParCo Conference, Munich, Germany, 2013</comments><journal-ref>Parallel Computing: Accelerating Computational Science and
  Engineering (CSE) 25 (2014) 63-71</journal-ref><doi>10.3233/978-1-61499-381-0-63</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of multicore and manycore processors is set to change the
parallel computing world. Applications are shifting towards increased
parallelism in order to utilise these architectures efficiently. This leads to
a situation where every application creates its desirable number of threads,
based on its parallel nature and the system resources allowance. Task
scheduling in such a multithreaded multiprogramming environment is a
significant challenge. In task scheduling, not only the order of the execution,
but also the mapping of threads to the execution resources is of a great
importance. In this paper we state and discuss some fundamental rules based on
results obtained from selected applications of the BOTS benchmarks on the
64-core TILEPro64 processor. We demonstrate how previously efficient mapping
policies such as those of the SMP Linux scheduler become inefficient when the
number of threads and cores grows. We propose a novel, low-overhead technique,
a heuristic based on the amount of time spent by each CPU doing some useful
work, to fairly distribute the workloads amongst the cores in a
multiprogramming environment. Our novel approach could be implemented as a
pragma similar to those in the new task-based OpenMP versions, or can be
incorporated as a distributed thread mapping mechanism in future manycore
programming frameworks. We show that our thread mapping scheme can outperform
the native GNU/Linux thread scheduler in both single-programming and
multiprogramming environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8024</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8024</id><created>2014-03-31</created><updated>2014-04-17</updated><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Replica Analysis and Approximate Message Passing Decoder for
  Superposition Codes</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>5 pages, 5 figures, To be presented at the 2014 IEEE International
  Symposium on Information Theory</comments><journal-ref>Information Theory Proceedings (ISIT), 2014 IEEE International
  Symposium on, page(s) 1494 - 1498</journal-ref><doi>10.1109/ISIT.2014.6875082</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superposition codes are efficient for the Additive White Gaussian Noise
channel. We provide here a replica analysis of the performances of these codes
for large signals. We also consider a Bayesian Approximate Message Passing
decoder based on a belief-propagation approach, and discuss its performance
using the density evolution technic. Our main findings are 1) for the sizes we
can access, the message-passing decoder outperforms other decoders studied in
the literature 2) its performance is limited by a sharp phase transition and 3)
while these codes reach capacity as $B$ (a crucial parameter in the code)
increases, the performance of the message passing decoder worsen as the phase
transition goes to lower rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8028</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8028</id><created>2014-02-23</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>AlSalem</keyname><forenames>Adel I.</forenames></author></authors><title>ImNet: An Imperative Network Programming Language</title><categories>cs.PL cs.NI</categories><comments>8 pages, 8 figures, Mohamed A. El-Zawawy and Adel I. AlSalem. ImNet:
  An Imperative Network Programming Language. Proceedings of The 14th
  International Conference on Applied Computer Science, ACS 2014, Constantin
  Buzatu (Ed): Modern Computer Applications in Science and Education, pp.
  149--156</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most recent architectures of networks is Software-Defined Networks
(SDNs) using a con- troller appliance to control the set of switches on the
network. The controlling process includes installing or uninstalling
packet-processing rules on flow tables of switches. This paper presents a
high-level imperative network programming language, called ImNet, to facilitate
writing efficient, yet simple, programs executed by controller to manage
switches. ImNet is simply-structured, expressive, compositional, and
imperative. This paper also introduces an operational semantics to ImNet.
Detailed examples of programs (with their operational semantics) constructed in
ImNet are illustrated in the paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8034</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8034</id><created>2014-03-31</created><authors><author><keyname>Hristova</keyname><forenames>Desislava</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Keep Your Friends Close and Your Facebook Friends Closer: A Multiplex
  Network Approach to the Analysis of Offline and Online Social Ties</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media allow for an unprecedented amount of interaction between people
online. A fundamental aspect of human social behavior, however, is the tendency
of people to associate themselves with like-minded individuals, forming
homogeneous social circles both online and offline. In this work, we apply a
new model that allows us to distinguish between social ties of varying
strength, and to observe evidence of homophily with regards to politics, music,
health, residential sector &amp; year in college, within the online and offline
social network of 74 college students. We present a multiplex network approach
to social tie strength, here applied to mobile communication data - calls, text
messages, and co-location, allowing us to dimensionally identify relationships
by considering the number of communication channels utilized between students.
We find that strong social ties are characterized by maximal use of
communication channels, while weak ties by minimal use. We are able to identify
75% of close friendships, 90% of weaker ties, and 90% of Facebook friendships
as compared to reported ground truth. We then show that stronger ties exhibit
greater profile similarity than weaker ones. Apart from high homogeneity in
social circles with respect to political and health aspects, we observe strong
homophily driven by music, residential sector and year in college. Despite
Facebook friendship being highly dependent on residence and year, exposure to
less homogeneous content can be found in the online rather than the offline
social circles of students, most notably in political and music aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8042</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8042</id><created>2014-03-31</created><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Optimal Power Allocation for Three-phase Bidirectional DF Relaying with
  Fixed Rates</title><categories>cs.IT math.IT</categories><comments>ISWCS 2013 - The Tenth International Symposium on Wireless
  Communication Systems, August 27 - 30, 2013, Ilmenau, Germany, 5 pages, 4
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless systems that carry delay-sensitive information (such as speech
and/or video signals) typically transmit with fixed data rates, but may
occasionally suffer from transmission outages caused by the random nature of
the fading channels. If the transmitter has instantaneous channel state
information (CSI) available, it can compensate for a significant portion of
these outages by utilizing power allocation. In this paper, we consider optimal
power allocation for a conventional dual-hop bidirectional decode-and-forward
(DF) relaying system with a three-phase transmission protocol. The proposed
strategy minimizes the average power consumed by the end nodes and the relay,
subject to some maximum allowable system outage probability (OP), or
equivalently, minimizes the system OP while meeting average power constraints
at the end nodes and the relay. We show that in the proposed power allocation
scheme, the end nodes and the relay adjust their output powers to the minimum
level required to avoid outages, but will sometimes be silent, in order to
conserve power and prolong their lifetimes. For the proposed scheme, the end
nodes use the instantaneous CSI of their respective source-relay links and the
relay uses the instantaneous CSI of both links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8046</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8046</id><created>2014-03-31</created><authors><author><keyname>Buliga</keyname><forenames>Marius</forenames></author><author><keyname>Kauffman</keyname><forenames>Louis H.</forenames></author></authors><title>Chemlambda, universality and self-multiplication</title><categories>cs.AI math.GT math.LO</categories><comments>8 pages, 21 colour figures, conference paper submitted to ALIFE 14</comments><doi>10.7551/978-0-262-32621-6-ch079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present chemlambda (or the chemical concrete machine), an artificial
chemistry with the following properties: (a) is Turing complete, (b) has a
model of decentralized, distributed computing associated to it, (c) works at
the level of individual (artificial) molecules, subject of reversible, but
otherwise deterministic interactions with a small number of enzymes, (d)
encodes information in the geometrical structure of the molecules and not in
their numbers, (e) all interactions are purely local in space and time. This is
part of a larger project to create computing, artificial chemistry and
artificial life in a distributed context, using topological and graphical
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8055</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8055</id><created>2014-03-31</created><authors><author><keyname>Abou-zeid</keyname><forenames>Hatem</forenames></author><author><keyname>Hassanein</keyname><forenames>Hossam S.</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author></authors><title>Energy-Efficient Adaptive Video Transmission: Exploiting Rate
  Predictions in Wireless Networks</title><categories>cs.NI cs.MM</categories><comments>14 pages, 14 figures, accepted for publication in IEEE Transactions
  on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unprecedented growth of mobile video traffic is adding significant
pressure to the energy drain at both the network and the end user. Energy
efficient video transmission techniques are thus imperative to cope with the
challenge of satisfying user demand at sustainable costs. In this paper, we
investigate how predicted user rates can be exploited for energy efficient
video streaming with the popular HTTP-based Adaptive Streaming (AS) protocols
(e.g. DASH). To this end, we develop an energy-efficient Predictive Green
Streaming (PGS) optimization framework that leverages predictions of wireless
data rates to achieve the following objectives 1) minimize the required
transmission airtime without causing streaming interruptions, 2) minimize total
downlink Base Station (BS) power consumption for cases where BSs can be
switched off in deep sleep, and 3) enable a trade-off between AS quality and
energy consumption. Our framework is first formulated as a Mixed Integer Linear
Program (MILP) where decisions on multi-user rate allocation, video segment
quality, and BS transmit power are jointly optimized. Then, to provide an
online solution, we present a polynomial-time heuristic algorithm that
decouples the PGS problem into multiple stages. We provide a performance
analysis of the proposed methods by simulations, and numerical results
demonstrate that the PGS framework yields significant energy savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8065</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8065</id><created>2014-03-31</created><authors><author><keyname>Cui</keyname><forenames>Wenzhi</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author></authors><title>Dual-structure Data Center Multicast Using Software Defined Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data center applications use multicast as an effective method to reduce
bandwidth cost. However, traditional multicast protocols designed for IP
networks are usually bottlenecked by the limited state capacity on switches. In
this paper, we propose a scalable multicast solution on fat tree networks based
on the observation that data center multicast traffic has strong heterogeneity.
We propose to remove the multicast management logic from switches and use the
SDN controller to manage multicast groups. The proposed Dual-structure
Multicast (DuSM) determines elephant and mice groups according to their traffic
amounts and treats them separately. For each elephant group, the controller
installs multicast state to maintain multiple shared trees and the group
traffic will be balanced evenly among the trees to avoid congestion. For mice
groups, the controller applies state-free mutlicast that trades bandwidth
capacity for state capacity, such as multicast-to-unicast translation. Our
experiments using real multicast traffic data show that the number of groups
DuSM supports can be 300% of that of IP multicast. DuSM also achieves traffic
balance among links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8067</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8067</id><created>2014-03-31</created><updated>2014-04-20</updated><authors><author><keyname>Bian</keyname><forenames>Xiao</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author></authors><title>Robust Subspace Recovery via Bi-Sparsity Pursuit</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful applications of sparse models in computer vision and machine
learning imply that in many real-world applications, high dimensional data is
distributed in a union of low dimensional subspaces. Nevertheless, the
underlying structure may be affected by sparse errors and/or outliers. In this
paper, we propose a bi-sparse model as a framework to analyze this problem and
provide a novel algorithm to recover the union of subspaces in presence of
sparse corruptions. We further show the effectiveness of our method by
experiments on both synthetic data and real-world vision data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8084</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8084</id><created>2014-03-31</created><authors><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Weinsberg</keyname><forenames>Udi</forenames></author><author><keyname>Bhagat</keyname><forenames>Smriti</forenames></author><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author><author><keyname>Taft</keyname><forenames>Nina</forenames></author></authors><title>Privacy Tradeoffs in Predictive Analytics</title><categories>cs.CR cs.LG</categories><comments>Extended version of the paper appearing in SIGMETRICS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online services routinely mine user data to predict user preferences, make
recommendations, and place targeted ads. Recent research has demonstrated that
several private user attributes (such as political affiliation, sexual
orientation, and gender) can be inferred from such data. Can a
privacy-conscious user benefit from personalization while simultaneously
protecting her private attributes? We study this question in the context of a
rating prediction service based on matrix factorization. We construct a
protocol of interactions between the service and users that has remarkable
optimality properties: it is privacy-preserving, in that no inference algorithm
can succeed in inferring a user's private attribute with a probability better
than random guessing; it has maximal accuracy, in that no other
privacy-preserving protocol improves rating prediction; and, finally, it
involves a minimal disclosure, as the prediction accuracy strictly decreases
when the service reveals less information. We extensively evaluate our protocol
using several rating datasets, demonstrating that it successfully blocks the
inference of gender, age and political affiliation, while incurring less than
5% decrease in the accuracy of rating prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8086</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8086</id><created>2014-03-31</created><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Chambers</keyname><forenames>Erin Wolf</forenames></author></authors><title>Covering nearly surface-embedded graphs with a fixed number of balls</title><categories>cs.CG</categories><comments>To appear in DCG</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent result of Chepoi, Estellon and Vaxes [DCG '07] states that any
planar graph of diameter at most 2R can be covered by a constant number of
balls of size R; put another way, there are a constant-sized subset of vertices
within which every other vertex is distance half the diameter. We generalize
this result to graphs embedded on surfaces of fixed genus with a fixed number
of apices, making progress toward the conjecture that graphs excluding a fixed
minor can also be covered by a constant number of balls. To do so, we develop
two tools which may be of independent interest. The first gives a bound on the
density of graphs drawn on a surface of genus $g$ having a limit on the number
of pairwise-crossing edges. The second bounds the size of a non-contractible
cycle in terms of the Euclidean norm of the degree sequence of a graph embedded
on surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8091</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8091</id><created>2014-03-31</created><authors><author><keyname>Fon-Der-Flaass</keyname><forenames>D. G.</forenames></author></authors><title>Perfect colorings of the 12-cube that attain the bound on correlation
  immunity</title><categories>math.CO cs.DM</categories><comments>4 pages. A translation of the original paper
  http://mi.mathnet.ru/eng/semr158</comments><msc-class>05C15</msc-class><journal-ref>Siberian Electronic Mathematical Reports 4, 2007, 292-295 [in
  Russian, with English Abstract]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct perfect $2$-colorings of the $12$-hypercube that attain our
recent bound on the dimension of arbitrary correlation immune functions. We
prove that such colorings with parameters $(x,12-x,4+x,8-x$) exist if $x=0$,
$2$, $3$ and do not exist if $x=1$.
  This is a translation into English of the original paper by D. G.
Fon-Der-Flaass, &quot;Perfect colorings of the $12$-cube that attain the bound on
correlation immunity&quot;, published in Russian in Siberian Electronic Mathematical
Reports, vol. 4 (2007) 292-295.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8093</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8093</id><created>2014-03-31</created><authors><author><keyname>Viswanatha</keyname><forenames>Kumar</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>The Lossy Common Information of Correlated Sources</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two most prevalent notions of common information (CI) are due to Wyner
and Gacs-Korner and both the notions can be stated as two different
characteristic points in the lossless Gray-Wyner region. Although the
information theoretic characterizations for these two CI quantities can be
easily evaluated for random variables with infinite entropy (eg., continuous
random variables), their operational significance is applicable only to the
lossless framework. The primary objective of this paper is to generalize these
two CI notions to the lossy Gray-Wyner network, which hence extends the
theoretical foundation to general sources and distortion measures. We begin by
deriving a single letter characterization for the lossy generalization of
Wyner's CI, defined as the minimum rate on the shared branch of the Gray-Wyner
network, maintaining minimum sum transmit rate when the two decoders
reconstruct the sources subject to individual distortion constraints. To
demonstrate its use, we compute the CI of bivariate Gaussian random variables
for the entire regime of distortions. We then similarly generalize Gacs and
Korner's definition to the lossy framework. The latter half of the paper
focuses on studying the tradeoff between the total transmit rate and receive
rate in the Gray-Wyner network. We show that this tradeoff yields a contour of
points on the surface of the Gray-Wyner region, which passes through both the
Wyner and Gacs-Korner operating points, and thereby provides a unified
framework to understand the different notions of CI. We further show that this
tradeoff generalizes the two notions of CI to the excess sum transmit rate and
receive rate regimes, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8098</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8098</id><created>2014-03-31</created><updated>2014-06-10</updated><authors><author><keyname>Sim&#xf5;es</keyname><forenames>Miguel</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Almeida</keyname><forenames>Luis B.</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author></authors><title>Hyperspectral image superresolution: An edge-preserving convex
  formulation</title><categories>cs.CV physics.data-an stat.ML</categories><comments>International Conference on Image Processing (ICIP), 2014 - accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral remote sensing images (HSIs) are characterized by having a low
spatial resolution and a high spectral resolution, whereas multispectral images
(MSIs) are characterized by low spectral and high spatial resolutions. These
complementary characteristics have stimulated active research in the inference
of images with high spatial and spectral resolutions from HSI-MSI pairs.
  In this paper, we formulate this data fusion problem as the minimization of a
convex objective function containing two data-fitting terms and an
edge-preserving regularizer. The data-fitting terms are quadratic and account
for blur, different spatial resolutions, and additive noise; the regularizer, a
form of vector Total Variation, promotes aligned discontinuities across the
reconstructed hyperspectral bands.
  The optimization described above is rather hard, owing to its
non-diagonalizable linear operators, to the non-quadratic and non-smooth nature
of the regularizer, and to the very large size of the image to be inferred. We
tackle these difficulties by tailoring the Split Augmented Lagrangian Shrinkage
Algorithm (SALSA)---an instance of the Alternating Direction Method of
Multipliers (ADMM)---to this optimization problem. By using a convenient
variable splitting and by exploiting the fact that HSIs generally &quot;live&quot; in a
low-dimensional subspace, we obtain an effective algorithm that yields
state-of-the-art results, as illustrated by experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8105</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8105</id><created>2014-03-31</created><authors><author><keyname>Koerner</keyname><forenames>David</forenames></author><author><keyname>Portsmouth</keyname><forenames>Jamie</forenames></author><author><keyname>Sadlo</keyname><forenames>Filip</forenames></author><author><keyname>Ertl</keyname><forenames>Thomas</forenames></author><author><keyname>Eberhardt</keyname><forenames>Bernd</forenames></author></authors><title>Flux-Limited Diffusion for Multiple Scattering in Participating Media</title><categories>cs.GR</categories><comments>Accepted in Computer Graphics Forum</comments><doi>10.1111/cgf.12342</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the rendering of multiple scattering effects in participating media,
methods based on the diffusion approximation are an extremely efficient
alternative to Monte Carlo path tracing. However, in sufficiently transparent
regions, classical diffusion approximation suffers from non-physical radiative
fluxes which leads to a poor match to correct light transport. In particular,
this prevents the application of classical diffusion approximation to
heterogeneous media, where opaque material is embedded within transparent
regions. To address this limitation, we introduce flux-limited diffusion, a
technique from the astrophysics domain. This method provides a better
approximation to light transport than classical diffusion approximation,
particularly when applied to heterogeneous media, and hence broadens the
applicability of diffusion-based techniques. We provide an algorithm for
flux-limited diffusion, which is validated using the transport theory for a
point light source in an infinite homogeneous medium. We further demonstrate
that our implementation of flux-limited diffusion produces more accurate
renderings of multiple scattering in various heterogeneous datasets than
classical diffusion approximation, by comparing both methods to ground truth
renderings obtained via volumetric path tracing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8106</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8106</id><created>2014-03-31</created><authors><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>Recent advances on the log-rank conjecture in communication complexity</title><categories>cs.CC math.CO</categories><comments>arXiv admin note: text overlap with arXiv:1306.1877</comments><msc-class>68Q10</msc-class><acm-class>F.1.2; F.2.2</acm-class><journal-ref>Lovett, Shachar. &quot;Recent advances on the log-rank conjecture in
  communication complexity.&quot; Bulletin of EATCS 1.112 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The log-rank conjecture is one of the fundamental open problems in
communication complexity. It speculates that the deterministic communication
complexity of any two-party function is equal to the log of the rank of its
associated matrix, up to polynomial factors. Despite much research, we still
know very little about this conjecture. Recently, there has been renewed
interest in this conjecture and its relations to other fundamental problems in
complexity theory. This survey describes some of the recent progress, and hints
at potential directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8118</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8118</id><created>2014-03-28</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>E-Generalization Using Grammars</title><categories>cs.LO cs.FL</categories><comments>49 pages, 16 figures, author address given in header is meanwhile
  outdated, full version of an article in the &quot;Artificial Intelligence
  Journal&quot;, appeared as technical report in 2003</comments><msc-class>68Q32, 68Q45, 68T15</msc-class><acm-class>I.2.3; F.4.1</acm-class><journal-ref>Artificial Intelligence Journal (Elsevier), Vol.165, No.1, p.1-35,
  2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the notion of anti-unification to cover equational theories and
present a method based on regular tree grammars to compute a finite
representation of E-generalization sets. We present a framework to combine
Inductive Logic Programming and E-generalization that includes an extension of
Plotkin's lgg theorem to the equational case. We demonstrate the potential
power of E-generalization by three example applications: computation of
suggestions for auxiliary lemmas in equational inductive proofs, computation of
construction laws for given term sequences, and learning of screen editor
command sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8122</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8122</id><created>2014-03-31</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author></authors><title>Performance of Selection Combining for Differential Amplify-and-Forward
  Relaying Over Time-Varying Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selection combining (SC) at the destination for differential
amplify-and-forward (AF) relaying is attractive as it does not require channel
state information as compared to the semi maximum-ratio-combining (semi-MRC)
while delivering close performance. Performance analysis of the SC scheme was
recently reported but only for the case of slow-fading channels. This paper
provides an exact average bit-error-rate (BER) of the SC scheme over a general
case of time-varying Rayleigh fading channels and when the DBPSK modulation is
used together with the non-coherent detection at the destination. The presented
analysis is thoroughly verified with simulation results in various fading
scenarios. It is shown that the performance of the system is related to the
auto-correlation values of the channels. It is also shown that the performance
of the SC method is very close to that of the semi-MRC method and the existence
of an error floor at high signal-to-noise ratio region is inevitable in both
methods. The obtained BER analysis for the SC method can also be used to
approximate the BER performance of the MRC method, whose exact analytical
evaluation in time-varying channels appears to be difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8123</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8123</id><created>2014-03-31</created><updated>2014-05-08</updated><authors><author><keyname>Lu</keyname><forenames>Jihua</forenames></author><author><keyname>An</keyname><forenames>Jianping</forenames></author><author><keyname>Li</keyname><forenames>Xiangming</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author><author><keyname>Yang</keyname><forenames>Lei</forenames></author></authors><title>A Percolation based M2M Networking Architecture for Data Transmission
  and Routing</title><categories>cs.NI</categories><comments>15 pages, 7 figures. We propose a percolation based routing and data
  transmission method for the M2M network, which consists of routing phase and
  transmission phases. In the routing phase, probes packets are transmitted and
  flowed in the network, multiple paths are built to form a route. After that,
  the data file will be fountain encoded and the transmitted</comments><msc-class>94C30</msc-class><journal-ref>KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 6, NO.
  2, Feb 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a percolation based M2M networking architecture and its data
transmission method. The proposed network architecture can be server-free and
router-free, which allows us to operate routing efficiently with percolations
based on six degrees of separation theory in small world network modeling. The
data transmission can be divided into two phases: routing and data transmission
phases. In the routing phase, probe packets will be transmitted and forwarded
in the network thus multiple paths are selected and performed based on the
constriction of the maximum hop number. In the second phase, the information
will be encoded, say, with the fountain codes, and transmitted using the paths
generated in the first phase. In such a way, an efficient routing and data
transmission mechanism can be built, which allow us to construct a low-cost,
flexible and ubiquitous network. Such a networking architecture and data
transmission can be used in many M2M communications, such as the stub network
of internet of things, and deep space networking, and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8128</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8128</id><created>2014-03-31</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames><affiliation>Senior Member, IEEE</affiliation></author></authors><title>Performance of Differential Amplify-and-Forward Relaying in Multi-Node
  Wireless Communications</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1403.5331</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the performance of differential
amplify-and-forward (D-AF) relaying for multi-node wireless communications over
time-varying Rayleigh fading channels. A first-order auto-regressive model is
utilized to characterize the time-varying nature of the channels. Based on the
secondorder statistical properties of the wireless channels, a new set of
combining weights is proposed for signal detection at the destination.
Expression of pair-wise error probability (PEP) is provided and used to obtain
the approximated total average bit error probability (BER). It is shown that
the performance of the system is related to the auto-correlation of the direct
and cascaded channels and an irreducible error floor exists at high
signal-to-noise ratio (SNR). The new weights lead to a better performance when
compared to the conventional combining scheme. Computer simulation is carried
out in different scenarios to support the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8130</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8130</id><created>2014-03-31</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author></authors><title>Selection Combining for Differential Amplify-and-Forward Relaying Over
  Rayleigh-Fading Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes and analyses selection combining (SC) at the destination
for differential amplify-andforward (D-AF) relaying over slow Rayleigh-fading
channels. The selection combiner chooses the link with the maximum magnitude of
the decision variable to be used for non-coherent detection of the transmitted
symbols. Therefore, in contrast to the maximum ratio combining (MRC), no
channel information is needed at the destination. The exact average
bit-error-rate (BER) of the proposed SC is derived and verified with simulation
results. It is also shown that the performance of the SC method is very close
to that of the MRC method, albeit with lower complexity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8137</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8137</id><created>2014-03-31</created><authors><author><keyname>Krishnan</keyname><forenames>Rajesh</forenames></author><author><keyname>Sundaram</keyname><forenames>Ravi</forenames></author></authors><title>Secure and scalable match: overcoming the universal circuit bottleneck
  using group programs</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confidential Content-Based Publish/Subscribe (C-CBPS) is an interaction
(pub/sub) model that allows parties to exchange data while still protecting
their security and privacy interests. In this paper we advance the state of the
art in C-CBPS by showing how all predicate circuits in NC1 (logarithmic-depth,
bounded fan-in) can be securely computed by a broker while guaranteeing perfect
information-theoretic security. Previous work could handle only strictly
shallower circuits (e.g. those with depth O(\sqrt{\lg n}) [SYY99, V76]. We
present three protocols -- UGP-Match, FSGP-Match and OFSGP-Match -- all three
are based on (2-decomposable randomized encodings of) group programs and handle
circuits in NC1. UGP-Match is conceptually simple and has a clean proof of
correctness but it is inefficient and impractical. FSGP-Match uses a &quot;fixed
structure&quot; trick to achieve efficiency and scalability. And, finally,
OFSGP-Match uses hand-optimized group programs to wring greater efficiencies.
We complete our investigation with an experimental evaluation of a prototype
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8144</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8144</id><created>2014-03-31</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author></authors><title>Coding for Random Projections and Approximate Near Neighbor Search</title><categories>cs.LG cs.DB cs.DS stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical note compares two coding (quantization) schemes for random
projections in the context of sub-linear time approximate near neighbor search.
The first scheme is based on uniform quantization while the second scheme
utilizes a uniform quantization plus a uniformly random offset (which has been
popular in practice). The prior work compared the two schemes in the context of
similarity estimation and training linear classifiers, with the conclusion that
the step of random offset is not necessary and may hurt the performance
(depending on the similarity level). The task of near neighbor search is
related to similarity estimation with importance distinctions and requires own
study. In this paper, we demonstrate that in the context of near neighbor
search, the step of random offset is not needed either and may hurt the
performance (sometimes significantly so, depending on the similarity and other
parameters).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.8150</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.8150</id><created>2014-03-31</created><updated>2014-04-02</updated><authors><author><keyname>Atighehchi</keyname><forenames>Kevin</forenames></author></authors><title>On the Incremental Asymmetric Signatures</title><categories>cs.CR</categories><comments>Preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of incremental cryptography is to allow the updating of
cryptographic forms of documents undergoing modifications, more efficiently
than if we had to recompute them from scratch. This paper defines a framework
for using securely a variant of the incremental hash function designed by
Bok-Min Goi et al. The condition of use of their hash function is somehow
impractical since they assume that the blocks of the message are all distinct.
In this paper we show how we can discard this strong assumption so as to
construct the first practical incremental asymmetric signature scheme that
keeps efficient update operations. Finally, as the proposed scheme has the
defect to severely expand the signature size, we propose a solution which
drastically reduces this drawback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0002</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0002</id><created>2014-03-31</created><authors><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Heinle</keyname><forenames>Albert</forenames></author><author><keyname>Levandovskyy</keyname><forenames>Viktor</forenames></author></authors><title>Factoring Differential Operators in n Variables</title><categories>cs.SC math.AC math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new algorithm and an experimental implementation
for factoring elements in the polynomial n'th Weyl algebra, the polynomial n'th
shift algebra, and ZZ^n-graded polynomials in the n'th q-Weyl algebra.
  The most unexpected result is that this noncommutative problem of factoring
partial differential operators can be approached effectively by reducing it to
the problem of solving systems of polynomial equations over a commutative ring.
In the case where a given polynomial is ZZ^n-graded, we can reduce the problem
completely to factoring an element in a commutative multivariate polynomial
ring.
  The implementation in Singular is effective on a broad range of polynomials
and increases the ability of computer algebra systems to address this important
problem. We compare the performance and output of our algorithm with other
implementations in commodity computer algebra systems on nontrivial examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0003</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0003</id><created>2014-03-31</created><authors><author><keyname>Bongers</keyname><forenames>Rodrigo</forenames></author></authors><title>EPICS process variables in different subnetworks and different IOCs
  without the use of the CaGateway</title><categories>cs.NI</categories><comments>12 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This technical document describes the comparison of the EPICS PV gateway and
a new solution based on relay of UDP packets using the UDP-HELPER switch
feature, iptables and a C program. The solution can be applied on environments
that contain multiple sub-networks and a number of IOCs on the same host or
multiple IOCs on the same sub-network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0024</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0024</id><created>2014-03-31</created><updated>2014-10-02</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Blum</keyname><forenames>Manuel</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Human Computable Passwords</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interesting challenge for the cryptography community is to design
authentication protocols that are so simple that a human can execute them
without relying on a fully trusted computer. We propose several candidate
authentication protocols for a setting in which the human user can only receive
assistance from a semi-trusted computer --- a computer that stores information
and performs computations correctly but does not provide confidentiality. Our
schemes use a semi-trusted computer to store and display public challenges
$C_i\in[n]^k$. The human user memorizes a random secret mapping
$\sigma:[n]\rightarrow\mathbb{Z}_d$ and authenticates by computing responses
$f(\sigma(C_i))$ to a sequence of public challenges where
$f:\mathbb{Z}_d^k\rightarrow\mathbb{Z}_d$ is a function that is easy for the
human to evaluate. We prove that any statistical adversary needs to sample
$m=\tilde{\Omega}(n^{s(f)})$ challenge-response pairs to recover $\sigma$, for
a security parameter $s(f)$ that depends on two key properties of $f$. To
obtain our results, we apply the general hypercontractivity theorem to lower
bound the statistical dimension of the distribution over challenge-response
pairs induced by $f$ and $\sigma$. Our lower bounds apply to arbitrary
functions $f $ (not just to functions that are easy for a human to evaluate),
and generalize recent results of Feldman et al. As an application, we propose a
family of human computable password functions $f_{k_1,k_2}$ in which the user
needs to perform $2k_1+2k_2+1$ primitive operations (e.g., adding two digits or
remembering $\sigma(i)$), and we show that $s(f) = \min\{k_1+1, (k_2+1)/2\}$.
For these schemes, we prove that forging passwords is equivalent to recovering
the secret mapping. Thus, our human computable password schemes can maintain
strong security guarantees even after an adversary has observed the user login
to many different accounts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0027</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0027</id><created>2014-03-28</created><authors><author><keyname>Mickael</keyname><forenames>Neri</forenames></author><author><keyname>Mestivier</keyname><forenames>Denis</forenames></author></authors><title>An efficient GPU acceptance-rejection algorithm for the selection of the
  next reaction to occur for Stochastic Simulation Algorithms</title><categories>cs.CE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: The Stochastic Simulation Algorithm (SSA) has largely diffused in
the field of systems biology. This approach needs many realizations for
establishing statistical results on the system under study. It is very
computationnally demanding, and with the advent of large models this burden is
increasing. Hence parallel implementation of SSA are needed to address these
needs.
  At the very heart of the SSA is the selection of the next reaction to occur
at each time step, and to the best of our knowledge all implementations are
based on an inverse transformation method. However, this method involves a
random number of steps to select this next reaction and is poorly amenable to a
parallel implementation.
  Results: Here, we introduce a parallel acceptance-rejection algorithm to
select the K next reactions to occur. This algorithm uses a deterministic
number of steps, a property well suited to a parallel implementation. It is
simple and small, accurate and scalable. We propose a Graphics Processing Unit
(GPU) implementation and validate our algorithm with simulated propensity
distributions and the propensity distribution of a large model of yeast iron
metabolism. We show that our algorithm can handle thousands of selections of
next reaction to occur in parallel on the GPU, paving the way to massive SSA.
  Availability: We present our GPU-AR algorithm that focuses on the very heart
of the SSA. We do not embed our algorithm within a full implementation in order
to stay pedagogical and allows its rapid implementation in existing software.
We hope that it will enable stochastic modelers to implement our algorithm with
the benefits of their own optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0039</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0039</id><created>2014-03-31</created><updated>2014-05-08</updated><authors><author><keyname>Lu</keyname><forenames>Jihua</forenames></author><author><keyname>Li</keyname><forenames>Xiangming</forenames></author><author><keyname>Liu</keyname><forenames>Dan</forenames></author></authors><title>Asynchronous Transmission of Wireless Multicast System with Genetic
  Joint Antennas Selection</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 3 figures. A downlink multicast scenario with genetic
  antenna selection is presented. The sender equipped with multi-antennas
  broadcasts successive data packets in groups to several multi-antenna users
  over a common bandwidth. Select appropriate weight vectors to maximize the
  minimum SINR under a power constraint. Proposed algorithm can improve system
  capacity with lower complexity</comments><msc-class>94A05</msc-class><journal-ref>IEEE WCNC 2013 workshop on Convergence of Broadcasting and
  Broadband Communications. Shanghai, China: April , pp.1-5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal antenna selection algorithm of multicast transmission can
significantly reduce the number of antennas and can acquire lower complexity
and high performance which is close to that of exhaustive search. An
asynchronous multicast transmission mechanism based on genetic antenna
selection is proposed. The computational complexity of genetic antenna
selection algorithm remains moderate while the total number of antennas
increases comparing with optimum searching algorithm. Symbol error rate (SER)
and capacity of our mechanism are analyzed and simulated, and the simulation
results demonstrate that our proposed mechanism can achieve better SER and
sub-maximum channel capacity in wireless multicast systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0046</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0046</id><created>2014-03-31</created><authors><author><keyname>Trummer</keyname><forenames>Immanuel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Approximation Schemes for Many-Objective Query Optimization</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of multi-objective query optimization (MOQO) is to find query plans
that realize a good compromise between conflicting objectives such as
minimizing execution time and minimizing monetary fees in a Cloud scenario. A
previously proposed exhaustive MOQO algorithm needs hours to optimize even
simple TPC-H queries. This is why we propose several approximation schemes for
MOQO that generate guaranteed near-optimal plans in seconds where exhaustive
optimization takes hours.
  We integrated all MOQO algorithms into the Postgres optimizer and present
experimental results for TPC-H queries; we extended the Postgres cost model and
optimize for up to nine conflicting objectives in our experiments. The proposed
algorithms are based on a formal analysis of typical cost functions that occur
in the context of MOQO. We identify properties that hold for a broad range of
objectives and can be exploited for the design of future MOQO algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0061</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0061</id><created>2014-03-31</created><updated>2014-12-18</updated><authors><author><keyname>Maric</keyname><forenames>Ivana</forenames></author><author><keyname>Hui</keyname><forenames>Dennis</forenames></author></authors><title>Short Message Noisy Network Coding with Rate Splitting</title><categories>cs.IT math.IT</categories><comments>Presented at the 2014 Asilomar Conference on Signals, Systems and
  Computers, Nov. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short message noisy network coding with rate splitting (SNNC-RS) encoding
strategy is presented. It has been shown by Hou and Kramer that mixed
cooperative strategies in which relays in favorable positions perform
decode-and-forward (DF) and the rest of the relays perform short message noisy
network coding (SNNC) can outperform noisy network coding (NNC). Our proposed
strategy further improves the rate performance of such mixed SNNC-DF
cooperative strategy. In the proposed scheme, superposition coding is
incorporated into the SNNC encoding in order to facilitate partial interference
cancellation at DF relays, thereby increasing the overall rate. To demonstrate
gains of the proposed SNNC-RS strategy, the achievable rate is analyzed for the
discrete memoryless two-relay network with one DF relay and one SNNC-RS relay
and compared to the case without rate-splitting. The obtained rate is evaluated
in the Gaussian two-relay network and gains over the rate achieved without rate
splitting are demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0062</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0062</id><created>2014-03-31</created><authors><author><keyname>Hosseini</keyname><forenames>Maryam</forenames></author><author><keyname>Santhanam</keyname><forenames>Narayana</forenames></author></authors><title>On redundancy of memoryless sources over countable alphabets</title><categories>cs.IT math.IT</categories><comments>Journal submission</comments><msc-class>68P30, 94A15</msc-class><acm-class>H.1.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum average number of bits need to describe a random variable is its
entropy, assuming knowledge of the underlying statistics On the other hand,
universal compression supposes that the distribution of the random variable,
while unknown, belongs to a known set $\cal P$ of distributions. Such universal
descriptions for the random variable are agnostic to the identity of the
distribution in $\cal P$. But because they are not matched exactly to the
underlying distribution of the random variable, the average number of bits they
use is higher, and the excess over the entropy used is the &quot;redundancy&quot;. This
formulation is fundamental to problems not just in compression, but also
estimation and prediction and has a wide variety of applications from language
modeling to insurance.
  In this paper, we study the redundancy of universal encodings of strings
generated by independent identically distributed (iid) sampling from a set
$\cal P$ of distributions over a countable support. We first show that if
describing a single sample from $\cal P$ incurs finite redundancy, then $\cal
P$ is tight but that the converse does not always hold. If a single sample can
be described with finite worst-case-regret (a more stringent formulation than
redundancy above), then it is known that length-$n$ iid samples only incurs a
diminishing (in $n$) redundancy per symbol as $n$ increases. However, we show
it is possible that a collection $\cal P$ incurs finite redundancy, yet
description of length-$n$ iid samples incurs a constant redundancy per symbol
encoded. We then show a sufficient condition on $\cal P$ such that length-$n$
iid samples will incur diminishing redundancy per symbol encoded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0067</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0067</id><created>2014-03-31</created><authors><author><keyname>O'Malley</keyname><forenames>A. James</forenames></author><author><keyname>Onnela</keyname><forenames>Jukka-Pekka</forenames></author></authors><title>Topics in social network analysis and network science</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter introduces statistical methods used in the analysis of social
networks and in the rapidly evolving parallel-field of network science.
Although several instances of social network analysis in health services
research have appeared recently, the majority involve only the most basic
methods and thus scratch the surface of what might be accomplished.
Cutting-edge methods using relevant examples and illustrations in health
services research are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0073</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0073</id><created>2014-03-31</created><authors><author><keyname>Al-Humaimeedy</keyname><forenames>Abeer S.</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames></author></authors><title>General dynamic recovery for compensating CSP</title><categories>cs.PL</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 3-16</journal-ref><doi>10.4204/EPTCS.143.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compensation is a technique to roll-back a system to a consistent state in
case of failure. Recovery mechanisms for compensating calculi specify the order
of execution of compensation sequences. Dynamic recovery means that the order
of execution is determined at runtime. In this paper, we define an extension of
Compensating CSP, called DEcCSP, with general dynamic recovery. We provide a
formal, operational semantics for the calculus, and illustrate its expressive
power with a case study. In contrast with previous versions of Compensating
CSP, DEcCSP provides mechanisms to replace or discard compensations at runtime.
Additionally, we bring back to DEcCSP standard CSP operators that are not
available in other compensating CSP calculi, and introduce channel
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0074</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0074</id><created>2014-03-31</created><authors><author><keyname>Bartha</keyname><forenames>Mikl&#xf3;s</forenames></author></authors><title>Quantum Turing automata</title><categories>quant-ph cs.FL cs.IT math.IT</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 17-31</journal-ref><doi>10.4204/EPTCS.143.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A denotational semantics of quantum Turing machines having a quantum control
is defined in the dagger compact closed category of finite dimensional Hilbert
spaces. Using the Moore-Penrose generalized inverse, a new additive trace is
introduced on the restriction of this category to isometries, which trace is
carried over to directed quantum Turing machines as monoidal automata. The
Joyal-Street-Verity Int construction is then used to extend this structure to a
reversible bidirectional one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0075</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0075</id><created>2014-03-31</created><authors><author><keyname>Blakey</keyname><forenames>Ed</forenames><affiliation>University of Bristol</affiliation></author></authors><title>Ray tracing -- computing the incomputable?</title><categories>cs.CC</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 32-40</journal-ref><doi>10.4204/EPTCS.143.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recall from previous work a model-independent framework of computational
complexity theory. Notably for the present paper, the framework allows
formalization of the issues of precision that present themselves when one
considers physical, error-prone (especially analogue rather than digital)
computational systems. We take as a case study the ray-tracing problem, a
Turing-machine-incomputable problem that can, in apparent violation of the
Church-Turing thesis, nonetheless be said to be solved by certain optical
computers; however, we apply the framework of complexity theory so as to
formalize the intuition that the purported super-Turing power of these
computers in fact vanishes once precision is properly considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0076</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0076</id><created>2014-03-31</created><authors><author><keyname>Jiresch</keyname><forenames>Eugen</forenames><affiliation>Technische Universit&#xe4;t Wien</affiliation></author></authors><title>Towards a GPU-based implementation of interaction nets</title><categories>cs.PL cs.DC</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 41-53</journal-ref><doi>10.4204/EPTCS.143.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ingpu, a GPU-based evaluator for interaction nets that heavily
utilizes their potential for parallel evaluation. We discuss advantages and
challenges of the ongoing implementation of ingpu and compare its performance
to existing interaction nets evaluators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0077</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0077</id><created>2014-03-31</created><authors><author><keyname>Mayordomo</keyname><forenames>Elvira</forenames></author></authors><title>Effective dimension in some general metric spaces</title><categories>cs.CC cs.IT math.IT</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 67-75</journal-ref><doi>10.4204/EPTCS.143.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of effective dimension for a general metric space.
Effective dimension was defined by Lutz in (Lutz 2003) for Cantor space and has
also been extended to Euclidean space. Our extension to other metric spaces is
based on a supergale characterization of Hausdorff dimension. We present here
the concept of constructive dimension and its characterization in terms of
Kolmogorov complexity. Further research directions are indicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0078</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0078</id><created>2014-03-31</created><authors><author><keyname>Nagy</keyname><forenames>Benedek</forenames></author><author><keyname>V&#xe1;lyi</keyname><forenames>S&#xe1;ndor</forenames></author></authors><title>Computing discrete logarithm by interval-valued paradigm</title><categories>cs.DS</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 76-86</journal-ref><doi>10.4204/EPTCS.143.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interval-valued computing is a relatively new computing paradigm. It uses
finitely many interval segments over the unit interval in a computation as data
structure. The satisfiability of Quantified Boolean formulae and other hard
problems, like integer factorization, can be solved in an effective way by its
massive parallelism. The discrete logarithm problem plays an important role in
practice, there are cryptographical methods based on its computational
hardness. In this paper we show that the discrete logarithm problem is
computable by an interval-valued computing in a polynomial number of steps
(within this paradigm).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0079</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0079</id><created>2014-03-31</created><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author><author><keyname>de Brecht</keyname><forenames>Matthew</forenames></author></authors><title>Non-deterministic computation and the Jayne-Rogers Theorem</title><categories>cs.LO math.LO</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><acm-class>F.1.1</acm-class><journal-ref>EPTCS 143, 2014, pp. 87-96</journal-ref><doi>10.4204/EPTCS.143.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple proof of a computable analogue to the Jayne Rogers
Theorem from descriptive set theory. The difficulty of the proof is delegated
to a simulation result pertaining to non-deterministic type-2 machines. Thus,
we demonstrate that developments in computational models can have applications
in fields thought to be far removed from it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0080</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0080</id><created>2014-03-31</created><authors><author><keyname>Sanders</keyname><forenames>Sam</forenames></author></authors><title>Algorithm and proof as \Omega-invariance and transfer: A new model of
  computation in nonstandard analysis</title><categories>cs.LO math.LO</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 97-109</journal-ref><doi>10.4204/EPTCS.143.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new model of computation based on nonstandard analysis.
Intuitively, the role of &quot;algorithm&quot; is played by a new notion of finite
procedure, called Omega-invariance and inspired by physics, from nonstandard
analysis. Moreover, the role of 'proof' is taken up by the Transfer Principle
from nonstandard analysis. We obtain a number of results in Constructive
Reverse Mathematics to illustrate the tight correspondence to Errett Bishop's
Constructive Analysis and the associated Constructive Reverse Mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0081</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0081</id><created>2014-03-31</created><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author></authors><title>The probability of non-confluent systems</title><categories>cs.LO</categories><comments>In Proceedings DCM 2013, arXiv:1403.7685</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 144, 2014, pp. 1-15</journal-ref><doi>10.4204/EPTCS.144.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to provide a structure of probability space to the set of
execution traces on a non-confluent abstract rewrite system, by defining a
variant of a Lebesgue measure on the space of traces. Then, we show how to use
this probability space to transform a non-deterministic calculus into a
probabilistic one. We use as example Lambda+, a recently introduced calculus
defined through type isomorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0082</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0082</id><created>2014-03-31</created><authors><author><keyname>Quispe-Cruz</keyname><forenames>Marcela</forenames><affiliation>PUC-Rio</affiliation></author><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames><affiliation>PUC-Rio</affiliation></author><author><keyname>Gordeev</keyname><forenames>Lew</forenames><affiliation>Tubingen University, Ghent University, PUC-Rio</affiliation></author></authors><title>Proof-graphs for Minimal Implicational Logic</title><categories>cs.LO</categories><comments>In Proceedings DCM 2013, arXiv:1403.7685</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 144, 2014, pp. 16-29</journal-ref><doi>10.4204/EPTCS.144.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the size of propositional classical proofs can be huge.
Proof theoretical studies discovered exponential gaps between normal or cut
free proofs and their respective non-normal proofs. The aim of this work is to
study how to reduce the weight of propositional deductions. We present the
formalism of proof-graphs for purely implicational logic, which are graphs of a
specific shape that are intended to capture the logical structure of a
deduction. The advantage of this formalism is that formulas can be shared in
the reduced proof.
  In the present paper we give a precise definition of proof-graphs for the
minimal implicational logic, together with a normalization procedure for these
proof-graphs. In contrast to standard tree-like formalisms, our normalization
does not increase the number of nodes, when applied to the corresponding
minimal proof-graph representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0083</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0083</id><created>2014-03-31</created><authors><author><keyname>Arrighi</keyname><forenames>Pablo</forenames></author><author><keyname>Martiel</keyname><forenames>Simon</forenames></author><author><keyname>Wang</keyname><forenames>Zizhu</forenames></author></authors><title>Causal Dynamics of Discrete Surfaces</title><categories>cs.DM cs.CG</categories><comments>In Proceedings DCM 2013, arXiv:1403.7685</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 144, 2014, pp. 30-40</journal-ref><doi>10.4204/EPTCS.144.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalize the intuitive idea of a labelled discrete surface which evolves
in time, subject to two natural constraints: the evolution does not propagate
information too fast; and it acts everywhere the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0084</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0084</id><created>2014-03-31</created><authors><author><keyname>Compagnoni</keyname><forenames>Adriana</forenames><affiliation>Stevens Institute of Technology</affiliation></author><author><keyname>Giannini</keyname><forenames>Paola</forenames><affiliation>Universit&#xe0; del Piemonte Orientale</affiliation></author><author><keyname>Kim</keyname><forenames>Catherine</forenames><affiliation>Stevens Institute of Technology</affiliation></author><author><keyname>Milideo</keyname><forenames>Matthew</forenames><affiliation>Stevens Institute of Technology</affiliation></author><author><keyname>Sharma</keyname><forenames>Vishakha</forenames><affiliation>Stevens Institute of Technology</affiliation></author></authors><title>A Calculus of Located Entities</title><categories>cs.PL cs.CE</categories><comments>In Proceedings DCM 2013, arXiv:1403.7685</comments><proxy>EPTCS</proxy><acm-class>D.3.1;D.3.2;D.3.3;B.1.2</acm-class><journal-ref>EPTCS 144, 2014, pp. 41-56</journal-ref><doi>10.4204/EPTCS.144.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define BioScapeL, a stochastic pi-calculus in 3D-space. A novel aspect of
BioScapeL is that entities have programmable locations. The programmer can
specify a particular location where to place an entity, or a location relative
to the current location of the entity. The motivation for the extension comes
from the need to describe the evolution of populations of biochemical species
in space, while keeping a sufficiently high level description, so that
phenomena like diffusion, collision, and confinement can remain part of the
semantics of the calculus. Combined with the random diffusion movement
inherited from BioScape, programmable locations allow us to capture the
assemblies of configurations of polymers, oligomers, and complexes such as
microtubules or actin filaments.
  Further new aspects of BioScapeL include random translation and scaling.
Random translation is instrumental in describing the location of new entities
relative to the old ones. For example, when a cell secretes a hydronium ion,
the ion should be placed at a given distance from the originating cell, but in
a random direction. Additionally, scaling allows us to capture at a high level
events such as division and growth; for example, daughter cells after mitosis
have half the size of the mother cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0085</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0085</id><created>2014-03-31</created><authors><author><keyname>Restrepo</keyname><forenames>Carlos Alberto Ram&#xed;rez</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames></author><author><keyname>Aranda</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>D&#xed;az-Frias</keyname><forenames>Juan Francisco</forenames></author></authors><title>Towards Formal Interaction-Based Models of Grid Computing
  Infrastructures</title><categories>cs.PL cs.DC cs.LO</categories><comments>In Proceedings DCM 2013, arXiv:1403.7685</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.3.1; F.3.1</acm-class><journal-ref>EPTCS 144, 2014, pp. 57-72</journal-ref><doi>10.4204/EPTCS.144.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid computing (GC) systems are large-scale virtual machines, built upon a
massive pool of resources (processing time, storage, software) that often span
multiple distributed domains. Concurrent users interact with the grid by adding
new tasks; the grid is expected to assign resources to tasks in a fair,
trustworthy way. These distinctive features of GC systems make their
specification and verification a challenging issue. Although prior works have
proposed formal approaches to the specification of GC systems, a precise
account of the interaction model which underlies resource sharing has not been
yet proposed. In this paper, we describe ongoing work aimed at filling in this
gap. Our approach relies on (higher-order) process calculi: these core
languages for concurrency offer a compositional framework in which GC systems
can be precisely described and potentially reasoned about.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0086</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0086</id><created>2014-03-31</created><authors><author><keyname>Benevides</keyname><forenames>Mario</forenames><affiliation>Federal University of Rio de Janeiro</affiliation></author><author><keyname>Lima</keyname><forenames>Isaque</forenames><affiliation>Federal University of Rio de Janeiro</affiliation></author><author><keyname>Nader</keyname><forenames>Rafael</forenames><affiliation>Federal University of Rio de Janeiro</affiliation></author><author><keyname>Rougemont</keyname><forenames>Pedro</forenames><affiliation>Federal University of Rio de Janeiro</affiliation></author></authors><title>Using HMM in Strategic Games</title><categories>cs.GT cs.IR cs.LG</categories><comments>In Proceedings DCM 2013, arXiv:1403.7685</comments><proxy>EPTCS</proxy><acm-class>I.2.8</acm-class><journal-ref>EPTCS 144, 2014, pp. 73-84</journal-ref><doi>10.4204/EPTCS.144.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe an approach to resolve strategic games in which
players can assume different types along the game. Our goal is to infer which
type the opponent is adopting at each moment so that we can increase the
player's odds. To achieve that we use Markov games combined with hidden Markov
model. We discuss a hypothetical example of a tennis game whose solution can be
applied to any game with similar characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0087</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0087</id><created>2014-03-31</created><authors><author><keyname>Quaas</keyname><forenames>Karin</forenames><affiliation>University of Leipzig</affiliation></author></authors><title>MTL-Model Checking of One-Clock Parametric Timed Automata is Undecidable</title><categories>cs.LO cs.FL</categories><comments>In Proceedings SynCoP 2014, arXiv:1403.7841</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 145, 2014, pp. 5-17</journal-ref><doi>10.4204/EPTCS.145.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parametric timed automata extend timed automata (Alur and Dill, 1991) in that
they allow the specification of parametric bounds on the clock values. Since
their introduction in 1993 by Alur, Henzinger, and Vardi, it is known that the
emptiness problem for parametric timed automata with one clock is decidable,
whereas it is undecidable if the automaton uses three or more parametric
clocks. The problem is open for parametric timed automata with two parametric
clocks. Metric temporal logic, MTL for short, is a widely used specification
language for real-time systems. MTL-model checking of timed automata is
decidable, no matter how many clocks are used in the timed automaton. In this
paper, we prove that MTL-model checking for parametric timed automata is
undecidable, even if the automaton uses only one clock and one parameter and is
deterministic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0088</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0088</id><created>2014-03-31</created><authors><author><keyname>Sun</keyname><forenames>Youcheng</forenames></author><author><keyname>Lipari</keyname><forenames>Giuseppe</forenames></author><author><keyname>Andr&#xe9;</keyname><forenames>&#xc9;tienne</forenames></author><author><keyname>Fribourg</keyname><forenames>Laurent</forenames></author></authors><title>Toward Parametric Timed Interfaces for Real-Time Components</title><categories>cs.OS</categories><comments>In Proceedings SynCoP 2014, arXiv:1403.7841</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 145, 2014, pp. 49-64</journal-ref><doi>10.4204/EPTCS.145.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose here a framework to model real-time components consisting of
concurrent real-time tasks running on a single processor, using parametric
timed automata. Our framework is generic and modular, so as to be easily
adapted to different schedulers and more complex task models. We first perform
a parametric schedulability analysis of the components using the inverse
method. We show that the method unfortunately does not provide satisfactory
results when the task periods are consid- ered as parameters. After identifying
and explaining the problem, we present a solution adapting the model by making
use of the worst-case scenario in schedulability analysis. We show that the
analysis with the inverse method always converges on the modified model when
the system load is strictly less than 100%. Finally, we show how to use our
parametric analysis for the generation of timed interfaces in compositional
system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0089</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0089</id><created>2014-03-31</created><authors><author><keyname>Skelin</keyname><forenames>Mladen</forenames></author><author><keyname>Geilen</keyname><forenames>Marc</forenames></author><author><keyname>Catthoor</keyname><forenames>Francky</forenames></author><author><keyname>Hendseth</keyname><forenames>Sverre</forenames></author></authors><title>Worst-case Throughput Analysis for Parametric Rate and Parametric Actor
  Execution Time Scenario-Aware Dataflow Graphs</title><categories>cs.PL</categories><comments>In Proceedings SynCoP 2014, arXiv:1403.7841</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 145, 2014, pp. 65-79</journal-ref><doi>10.4204/EPTCS.145.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scenario-aware dataflow (SADF) is a prominent tool for modeling and analysis
of dynamic embedded dataflow applications. In SADF the application is
represented as a finite collection of synchronous dataflow (SDF) graphs, each
of which represents one possible application behaviour or scenario. A finite
state machine (FSM) specifies the possible orders of scenario occurrences. The
SADF model renders the tightest possible performance guarantees, but is limited
by its finiteness. This means that from a practical point of view, it can only
handle dynamic dataflow applications that are characterized by a reasonably
sized set of possible behaviours or scenarios. In this paper we remove this
limitation for a class of SADF graphs by means of SADF model parametrization in
terms of graph port rates and actor execution times. First, we formally define
the semantics of the model relevant for throughput analysis based on (max,+)
linear system theory and (max,+) automata. Second, by generalizing some of the
existing results, we give the algorithms for worst-case throughput analysis of
parametric rate and parametric actor execution time acyclic SADF graphs with a
fully connected, possibly infinite state transition system. Third, we
demonstrate our approach on a few realistic applications from digital signal
processing (DSP) domain mapped onto an embedded multi-processor architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0091</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0091</id><created>2014-03-31</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author></authors><title>Interestingness a Unifying Paradigm Bipolar Function Composition</title><categories>cs.IR</categories><comments>9 pages, 2 figures, 1 table; in Proceedings of KDIR 2009, 1st
  International Conference on Knowledge Discovery and Information Retrieval,
  Madeira, Portugal, pp. 196-201</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interestingness is an important criterion by which we judge knowledge
discovery. But, interestingness has escaped all attempts to capture its
intuitive meaning into a concise and comprehensive form. A unifying paradigm is
formulated by function composition. We claim that composition is bipolar, i.e.
composition of exactly two functions, whose two semantic poles are relevance
and unexpectedness. The paradigm generality is demonstrated by case studies of
new interestingness functions, examples of known functions that fit the
framework, and counter-examples for which the paradigm points out to the
lacking pole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0097</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0097</id><created>2014-03-31</created><updated>2014-05-11</updated><authors><author><keyname>Si</keyname><forenames>Hongbo</forenames></author><author><keyname>Vikalo</keyname><forenames>Haris</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Haplotype Assembly: An Information Theoretic View</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures, 1 tabel, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the haplotype assembly problem from an information
theoretic perspective. A haplotype is a sequence of nucleotide bases on a
chromosome, often conveniently represented by a binary string, that differ from
the bases in the corresponding positions on the other chromosome in a
homologous pair. Information about the order of bases in a genome is readily
inferred using short reads provided by high-throughput DNA sequencing
technologies. In this paper, the recovery of the target pair of haplotype
sequences using short reads is rephrased as a joint source-channel coding
problem. Two messages, representing haplotypes and chromosome memberships of
reads, are encoded and transmitted over a channel with erasures and errors,
where the channel model reflects salient features of high-throughput
sequencing. The focus of this paper is on the required number of reads for
reliable haplotype reconstruction, and both the necessary and sufficient
conditions are presented with order-wise optimal bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0099</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0099</id><created>2014-03-31</created><authors><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Selsam</keyname><forenames>Daniel</forenames></author><author><keyname>Perov</keyname><forenames>Yura</forenames></author></authors><title>Venture: a higher-order probabilistic programming platform with
  programmable inference</title><categories>cs.AI cs.PL stat.CO stat.ML</categories><comments>78 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Venture, an interactive virtual machine for probabilistic
programming that aims to be sufficiently expressive, extensible, and efficient
for general-purpose use. Like Church, probabilistic models and inference
problems in Venture are specified via a Turing-complete, higher-order
probabilistic language descended from Lisp. Unlike Church, Venture also
provides a compositional language for custom inference strategies built out of
scalable exact and approximate techniques. We also describe four key aspects of
Venture's implementation that build on ideas from probabilistic graphical
models. First, we describe the stochastic procedure interface (SPI) that
specifies and encapsulates primitive random variables. The SPI supports custom
control flow, higher-order probabilistic procedures, partially exchangeable
sequences and ``likelihood-free'' stochastic simulators. It also supports
external models that do inference over latent variables hidden from Venture.
Second, we describe probabilistic execution traces (PETs), which represent
execution histories of Venture programs. PETs capture conditional dependencies,
existential dependencies and exchangeable coupling. Third, we describe
partitions of execution histories called scaffolds that factor global inference
problems into coherent sub-problems. Finally, we describe a family of
stochastic regeneration algorithms for efficiently modifying PET fragments
contained within scaffolds. Stochastic regeneration linear runtime scaling in
cases where many previous approaches scaled quadratically. We show how to use
stochastic regeneration and the SPI to implement general-purpose inference
strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals
based on particle Markov chain Monte Carlo and mean-field variational inference
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0101</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0101</id><created>2014-03-31</created><authors><author><keyname>Thai</keyname><forenames>Chan Dai Truyen</forenames></author><author><keyname>Berbineau</keyname><forenames>Marion</forenames></author></authors><title>Quantization for Uplink Transmissions in Two-tier Networks with
  Femtocells</title><categories>cs.NI cs.IT math.IT</categories><comments>16 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two novel schemes to level up the sum--rate for a two-tier network
with femtocell where the backhaul uplink and downlink connecting the Base
Stations have limited capacity. The backhaul links are exploited to transport
the information in order to improve the decoding of the macrocell and femtocell
messages. In the first scheme, Quantize-and-Forward, the Femto Base Station
(FBS) quantizes what it receives and forwards it to the Macro Base Station
(MBS). Two quantization methods are considered: Elementary Quantization and
Wyner-Ziv Quantization. In the second scheme, called Decode-and-Forward with
Quantized Side Information (DFQSI) to be distinguished with the considered
conventional Decode-and-Forward (DF) scheme. The DFQSI scheme exploits the
backhaul downlink to quantize and send the information about the message in the
macrocell to the FBS to help it better decode the message, cancel it and decode
the message in the femtocell. The results show that there are interesting
scenarios in which the proposed techniques offer considerable gains in terms of
maximal sum rate and max minimal rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0103</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0103</id><created>2014-03-31</created><authors><author><keyname>Matta</keyname><forenames>John</forenames></author><author><keyname>Borwey</keyname><forenames>Jeffrey</forenames></author><author><keyname>Ercal</keyname><forenames>Gunes</forenames></author></authors><title>Comparative Resilience Notions and Vertex Attack Tolerance of Scale-Free
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>keywords: scale-free, conductance, resilience, vertex attack
  tolerance</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are concerned with an appropriate mathematical measure of resilience in
the face of targeted node attacks for arbitrary degree networks, and
subsequently comparing the resilience of different scale-free network models
with the proposed measure. We strongly motivate our resilience measure termed
\emph{vertex attack tolerance} (VAT), which is denoted mathematically as
$\tau(G) = \min_{S \subset V} \frac{|S|}{|V-S-C_{max}(V-S)|+1}$, where
$C_{max}(V-S)$ is the largest connected component in $V-S$. We attempt a
thorough comparison of VAT with several existing resilience notions:
conductance, vertex expansion, integrity, toughness, tenacity and scattering
number. Our comparisons indicate that for artbitrary degree distributions VAT
is the only measure that fully captures both the major \emph{bottlenecks} of a
network and the resulting \emph{component size distribution} upon targeted node
attacks (both captured in a manner proportional to the size of the attack set).
For the case of $d$-regular graphs, we prove that $\tau(G) \le d\Phi(G)$, where
$\Phi(G)$ is the conductance of the graph $G$. Conductance and expansion are
well-studied measures of robustness and bottlenecks in the case of regular
graphs but fail to capture resilience in the case of highly heterogeneous
degree graphs. Regarding comparison of different scale-free graph models, our
experimental results indicate that PLOD graphs with degree distributions
identical to BA graphs of the same size exhibit consistently better vertex
attack tolerance than the BA type graphs, although both graph types appear
asymptotically resilient for BA generative parameter $m = 2$. BA graphs with $m
= 1$ also appear to lack resilience, not only exhibiting very low VAT values,
but also great transparency in the identification of the vulnerable node sets,
namely the hubs, consistent with well known previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0106</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0106</id><created>2014-03-31</created><authors><author><keyname>Kumar</keyname><forenames>Shiu</forenames></author><author><keyname>Ham</keyname><forenames>Eun Sik</forenames></author><author><keyname>Lee</keyname><forenames>Seong Ro</forenames></author></authors><title>Traffic Monitoring Using M2M Communication</title><categories>cs.CV</categories><comments>2 pages, 2 figures, presented in local conference in Korea South</comments><journal-ref>General Fall Conference of Korea Information and Communications
  Society (KICS) 2012, Seoul, South Korea, 2012, pp. 233-234</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an intelligent traffic monitoring system using wireless
vision sensor network that captures and processes the real-time video image to
obtain the traffic flow rate and vehicle speeds along different urban roadways.
This system will display the traffic states on the front roadways that can
guide the drivers to select the right way and avoid potential traffic
congestions. On the other hand, it will also monitor the vehicle speeds and
store the vehicle details, for those breaking the roadway speed limits, in its
database. The real-time traffic data is processed by the Personal Computer (PC)
at the sub roadway station and the traffic flow rate data is transmitted to the
main roadway station Arduino 3G via email, where the data is extracted and
traffic flow rate displayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0117</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0117</id><created>2014-03-31</created><updated>2014-04-03</updated><authors><author><keyname>Diaz</keyname><forenames>Josep</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author></authors><title>Minimum Bisection is NP-hard on Unit Disk Graphs</title><categories>cs.CC math.CO</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that the \textsc{Min-Bisection} problem is NP-hard on
\emph{unit disk graphs}, thus solving a longstanding open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0119</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0119</id><created>2014-03-31</created><authors><author><keyname>Adsul</keyname><forenames>Bharat</forenames></author><author><keyname>Machchhar</keyname><forenames>Jinesh</forenames></author><author><keyname>Sohoni</keyname><forenames>Milind</forenames></author></authors><title>A Computational Framework for Boundary Representation of Solid Sweeps</title><categories>cs.CG cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a robust algorithmic and computational framework to
address the problem of modeling the volume obtained by sweeping a solid along a
trajectory of rigid motions. The boundary representation (simply brep) of the
input solid naturally induces a brep of the swept volume. We show that it is
locally similar to the input brep and this serves as the basis of the
framework. All the same, it admits several intricacies: (i) geometric, in terms
of parametrizations and, (ii) topological, in terms of orientations. We provide
a novel analysis for their resolution. More specifically, we prove a
non-trivial lifting theorem which allows to locally orient the output using the
orientation of the input. We illustrate the framework by providing many
examples from a pilot implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0122</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0122</id><created>2014-04-01</created><updated>2014-11-27</updated><authors><author><keyname>Roosta-Khorasani</keyname><forenames>Farbod</forenames></author><author><keyname>Sz&#xe9;kely</keyname><forenames>G&#xe1;bor J.</forenames></author><author><keyname>Ascher</keyname><forenames>Uri</forenames></author></authors><title>Assessing stochastic algorithms for large scale nonlinear least squares
  problems using extremal probabilities of linear combinations of gamma random
  variables</title><categories>math.NA cs.NA math.ST stat.AP stat.TH</categories><msc-class>65C20, 65C05, 60E05, 68W20</msc-class><journal-ref>SIAM/ASA Journal on Uncertainty Quantification. 3 (2015) 61-90</journal-ref><doi>10.1137/14096311X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers stochastic algorithms for efficiently solving a class
of large scale non-linear least squares (NLS) problems which frequently arise
in applications. We propose eight variants of a practical randomized algorithm
where the uncertainties in the major stochastic steps are quantified. Such
stochastic steps involve approximating the NLS objective function using
Monte-Carlo methods, and this is equivalent to the estimation of the trace of
corresponding symmetric positive semi-definite (SPSD) matrices. For the latter,
we prove tight necessary and sufficient conditions on the sample size (which
translates to cost) to satisfy the prescribed probabilistic accuracy. We show
that these conditions are practically computable and yield small sample sizes.
They are then incorporated in our stochastic algorithm to quantify the
uncertainty in each randomized step. The bounds we use are applications of more
general results regarding extremal tail probabilities of linear combinations of
gamma distributed random variables. We derive and prove new results concerning
the maximal and minimal tail probabilities of such linear combinations, which
can be considered independently of the rest of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0123</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0123</id><created>2014-04-01</created><authors><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author></authors><title>Downlink Interference Estimation without Feedback for Heterogeneous
  Network Interference Avoidance</title><categories>cs.NI</categories><comments>6 pages, 5 figures, IEEE International Conference on
  Telecommunications (ICT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel method for a base station (BS) to estimate
the total downlink interference power received by any given mobile receiver,
without information feedback from the user or information exchange between
neighbouring BSs. The prediction method is deterministic and can be computed
rapidly. This is achieved by first abstracting the cellular network into a
mathematical model, and then inferring the interference power received at any
location based on the power spectrum measurements taken at the observing BS.
The analysis expands the methodology to a $\mathsf{K}$-tier heterogeneous
network and demonstrates the accuracy of the technique for a variety of
sampling densities. The paper demonstrates the methodology by applying it to an
opportunistic transmission technique that avoids transmissions to channels
which are overwhelmed by interference. The simulation results show that the
proposed technique performs closely or better than existing interference
avoidance techniques that require information exchange, and yields a 30%
throughput improvement over baseline configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0127</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0127</id><created>2014-04-01</created><authors><author><keyname>Wang</keyname><forenames>Siyi</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Qiu</keyname><forenames>Song</forenames></author><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author></authors><title>Performance of Macro-Scale Molecular Communications with Sensor Cleanse
  Time</title><categories>cs.NI cs.ET</categories><comments>6 pages, 6 figures, IEEE International Conference on
  Telecommunications (ICT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a molecular diffusion based communications link
that conveys information on the macro-scale (several metres). The motivation is
to apply molecular-based communications to challenging electromagnetic
environments. We first derive a novel capture probability expression of a
finite sized receiver. The paper then introduces the concept of time-aggregated
molecular noise at the receiver as a function of the rate at which the sensor
can self-cleanse. The resulting inter-symbol-interference is expressed as a
function of the sensor cleanse time, and the performance metrics of bit error
rate, throughput and round-trip-time are derived. The results show that the
performance is very sensitive to the sensor cleanse time and the drift
velocity. The paper concludes with recommendations on the design of a real
communication link based on these findings and applies the concepts to a
test-bed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0138</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0138</id><created>2014-04-01</created><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>Efficient Algorithms and Error Analysis for the Modified Nystrom Method</title><categories>cs.LG</categories><comments>9-page paper plus appendix. In Proceedings of the 17th International
  Conference on Artificial Intelligence and Statistics (AISTATS) 2014,
  Reykjavik, Iceland. JMLR: W&amp;CP volume 33</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many kernel methods suffer from high time and space complexities and are thus
prohibitive in big-data applications. To tackle the computational challenge,
the Nystr\&quot;om method has been extensively used to reduce time and space
complexities by sacrificing some accuracy. The Nystr\&quot;om method speedups
computation by constructing an approximation of the kernel matrix using only a
few columns of the matrix. Recently, a variant of the Nystr\&quot;om method called
the modified Nystr\&quot;om method has demonstrated significant improvement over the
standard Nystr\&quot;om method in approximation accuracy, both theoretically and
empirically.
  In this paper, we propose two algorithms that make the modified Nystr\&quot;om
method practical. First, we devise a simple column selection algorithm with a
provable error bound. Our algorithm is more efficient and easier to implement
than and nearly as accurate as the state-of-the-art algorithm. Second, with the
selected columns at hand, we propose an algorithm that computes the
approximation in lower time complexity than the approach in the previous work.
Furthermore, we prove that the modified Nystr\&quot;om method is exact under certain
conditions, and we establish a lower error bound for the modified Nystr\&quot;om
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0142</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0142</id><created>2014-04-01</created><authors><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Geng</keyname><forenames>Yanhui</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Information-Theoretic Bounds for Performance of Resource-Constrained
  Communication Systems</title><categories>cs.IT cs.SY math.IT math.OC</categories><comments>Submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource-constrained systems are prevalent in communications. Such a system
is composed of many components but only some of them can be allocated with
resources such as time slots. According to the amount of information about the
system, algorithms are employed to allocate resources and the overall system
performance depends on the result of resource allocation. We do not always have
complete information, and thus, the system performance may not be satisfactory.
In this work, we propose a general model for the resource-constrained
communication systems. We draw the relationship between system information and
performance and derive the performance bounds for the optimal algorithm for the
system. This gives the expected performance corresponding to the available
information, and we can determine if we should put more efforts to collect more
accurate information before actually constructing an algorithm for the system.
Several examples of applications in communications to the model are also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0144</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0144</id><created>2014-04-01</created><authors><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Julian-Steffen</forenames></author><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>Modal Independence Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces modal independence logic MIL, a modal logic that can
explicitly talk about independence among propositional variables. Formulas of
MIL are not evaluated in worlds but in sets of worlds, so called teams. In this
vein, MIL can be seen as a variant of V\&quot;a\&quot;an\&quot;anen's modal dependence logic
MDL. We show that MIL embeds MDL and is strictly more expressive. However, on
singleton teams, MIL is shown to be not more expressive than usual modal logic,
but MIL is exponentially more succinct. Making use of a new form of
bisimulation, we extend these expressivity results to modal logics extended by
various generalized dependence atoms. We demonstrate the expressive power of
MIL by giving a specification of the anonymity requirement of the dining
cryptographers protocol in MIL. We also study complexity issues of MIL and show
that, though it is more expressive, its satisfiability and model checking
problem have the same complexity as for MDL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0158</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0158</id><created>2014-04-01</created><authors><author><keyname>Ogunduyile</keyname><forenames>O. O.</forenames></author><author><keyname>Olugbara</keyname><forenames>O. O.</forenames></author><author><keyname>Lall</keyname><forenames>M.</forenames></author></authors><title>Development of Wearable Systems for Ubiquitous Healthcare Service
  Provisioning</title><categories>cs.OH</categories><comments>6 pages, 3 figures, APCBEE Procedia 7, 2013. arXiv admin note:
  substantial text overlap with arXiv:1309.1542</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on the development of a wearable system using wireless
biomedical sensors for ubiquitous healthcare service provisioning. The
prototype system is developed to address current healthcare challenges such as
increasing cost of services, inability to access diverse services, low quality
services and increasing population of elderly as experienced globally. The
biomedical sensors proactively collect physiological data of remote patients to
recommend diagnostic services. The prototype system is designed to monitor
oxygen saturation level (SpO2), Heart Rate (HR), activity and location of the
elderly. Physiological data collected are uploaded to a Health Server (HS) via
GPRS/Internet for analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0161</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0161</id><created>2014-04-01</created><authors><author><keyname>Eder</keyname><forenames>Christian</forenames></author></authors><title>Predicting zero reductions in Gr\&quot;obner basis computations</title><categories>math.AC cs.SC</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Buchberger's initial algorithm for computing Gr\&quot;obner bases in 1965
many attempts have been taken to detect zero reductions in advance.
Buchberger's Product and Chain criteria may be known the most, especially in
the installaton of Gebauer and M\&quot;oller. A relatively new approach are
signature-based criteria which were first used in Faug\`ere's F5 algorithm in
2002. For regular input sequences these criteria are known to compute no zero
reduction at all. In this paper we give a detailed discussion on zero
reductions and the corresponding syzygies. We explain how the different methods
to predict them compare to each other and show advantages and drawbacks in
theory and practice. With this a new insight into algebraic structures
underlying Gr\&quot;obner bases and their computations might be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0163</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0163</id><created>2014-04-01</created><authors><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Garimella</keyname><forenames>Venkata Rama Kiran</forenames></author></authors><title>Gender Asymmetries in Reality and Fiction: The Bechdel Test of Social
  Media</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>To appear in Proceedings of the 8th International AAAI Conference on
  Weblogs and Social Media (ICWSM '14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subjective nature of gender inequality motivates the analysis and
comparison of data from real and fictional human interaction. We present a
computational extension of the Bechdel test: A popular tool to assess if a
movie contains a male gender bias, by looking for two female characters who
discuss about something besides a man. We provide the tools to quantify Bechdel
scores for both genders, and we measure them in movie scripts and large
datasets of dialogues between users of MySpace and Twitter. Comparing movies
and users of social media, we find that movies and Twitter conversations have a
consistent male bias, which does not appear when analyzing MySpace.
Furthermore, the narrative of Twitter is closer to the movies that do not pass
the Bechdel test than to those that pass it.
  We link the properties of movies and the users that share trailers of those
movies. Our analysis reveals some particularities of movies that pass the
Bechdel test: Their trailers are less popular, female users are more likely to
share them than male users, and users that share them tend to interact less
with male users. Based on our datasets, we define gender independence
measurements to analyze the gender biases of a society, as manifested through
digital traces of online behavior. Using the profile information of Twitter
users, we find larger gender independence for urban users in comparison to
rural ones. Additionally, the asymmetry between genders is larger for parents
and lower for students. Gender asymmetry varies across US states, increasing
with higher average income and latitude. This points to the relation between
gender inequality and social, economical, and cultural factors of a society,
and how gender roles exist in both fictional narratives and public online
dialogues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0169</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0169</id><created>2014-04-01</created><updated>2014-12-26</updated><authors><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Triangle-free geometric intersection graphs with no large independent
  sets</title><categories>math.CO cs.CG cs.DM math.MG</categories><comments>Change of the title, minor revision</comments><msc-class>05C62, 05C15</msc-class><journal-ref>Discrete Comput.Geom. 53 (2015) 221-225</journal-ref><doi>10.1007/s00454-014-9645-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that there are triangle-free intersection graphs of line
segments in the plane with arbitrarily small ratio between the maximum size of
an independent set and the total number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0173</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0173</id><created>2014-04-01</created><authors><author><keyname>Soltanalian</keyname><forenames>Mojtaba</forenames></author><author><keyname>Stoica</keyname><forenames>Petre</forenames></author></authors><title>A Recursive Method for Enumeration of Costas Arrays</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a recursive method for finding Costas arrays that
relies on a particular formation of Costas arrays from similar patterns of
smaller size. By using such an idea, the proposed algorithm is able to
dramatically reduce the computational burden (when compared to the exhaustive
search), and at the same time, still can find all possible Costas arrays of
given size. Similar to exhaustive search, the proposed method can be
conveniently implemented in parallel computing. The efficiency of the method is
discussed based on theoretical and numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0180</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0180</id><created>2014-04-01</created><updated>2014-05-14</updated><authors><author><keyname>Bellalta</keyname><forenames>B.</forenames></author><author><keyname>Zocca</keyname><forenames>A.</forenames></author><author><keyname>Cano</keyname><forenames>C.</forenames></author><author><keyname>Checco</keyname><forenames>A.</forenames></author><author><keyname>Barcelo</keyname><forenames>J.</forenames></author><author><keyname>Vinel</keyname><forenames>A.</forenames></author></authors><title>Throughput Analysis in CSMA/CA Networks using Continuous Time Markov
  Networks: A Tutorial</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book chapter introduces the use of Continuous Time Markov Networks
(CTMN) to analytically capture the operation of Carrier Sense Multiple Access
with Collision Avoidance (CSMA/CA) networks. It is of tutorial nature, and it
aims to be an introduction on this topic, providing a clear and easy-to-follow
description. To illustrate how CTMN can be used, we introduce a set of
representative and cutting-edge scenarios, such as Vehicular Ad-hoc Networks
(VANETs), Power Line Communication networks and multiple overlapping Wireless
Local Area Networks (WLANs). For each scenario, we describe the specific CTMN,
obtain its stationary distribution and compute the throughput achieved by each
node in the network. Taking the per-node throughput as reference, we discuss
how the complex interactions between nodes using CSMA/CA have an impact on
system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0191</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0191</id><created>2014-04-01</created><updated>2014-07-02</updated><authors><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Pavlidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Semendeferi</keyname><forenames>Ioanna</forenames></author></authors><title>A quantitative perspective on ethics in large team science</title><categories>physics.soc-ph cs.CY cs.DL</categories><comments>13 pages, 5 figures, 1 table. Keywords: team ethics; team management;
  team evaluation; science of science</comments><journal-ref>Science &amp; Engineering Ethics 20, 923-945 (2014)</journal-ref><doi>10.1007/s11948-014-9562-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The gradual crowding out of singleton and small team science by large team
endeavors is challenging key features of research culture. It is therefore
important for the future of scientific practice to reflect upon the individual
scientist's ethical responsibilities within teams. To facilitate this
reflection we show labor force trends in the US revealing a skewed growth in
academic ranks and increased levels of competition for promotion within the
system; we analyze teaming trends across disciplines and national borders
demonstrating why it is becoming difficult to distribute credit and to avoid
conflicts of interest; and we use more than a century of Nobel prize data to
show how science is outgrowing its old institutions of singleton awards. Of
particular concern within the large team environment is the weakening of the
mentor-mentee relation, which undermines the cultivation of virtue ethics
across scientific generations. These trends and emerging organizational
complexities call for a universal set of behavioral norms that transcend team
heterogeneity and hierarchy. To this end, our expository analysis provides a
survey of ethical issues in team settings to inform science ethics education
and science policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0195</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0195</id><created>2014-04-01</created><authors><author><keyname>Kaya</keyname><forenames>Abidin</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author></authors><title>Extension theorems for self-dual codes over rings and new binary
  self-dual codes</title><categories>cs.IT math.IT</categories><comments>under review since April 2014, 14 pages, 12 tables</comments><journal-ref>Discrete Mathematics Vol 338 Issue 2 2016</journal-ref><doi>10.1016/j.disc.2015.09.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, extension theorems are generalized to self-dual codes over
rings and as applications many new binary self-dual extremal codes are found
from self-dual codes over F_2^m+uF_2^m for m = 1, 2. The duality and distance
preserving Gray maps from F4 +uF4 to (F_2 +uF_2)^2 and (F_4)^2 are used to
obtain self-dual codes whose binary Gray images are [64,32,12]-extremal
self-dual. An F_2+uF_2-extension is used and as binary images, 178 extremal
binary self-dual codes of length 68 with new weight enumerators are obtained.
Especially the ?rst examples of codes with gamma=3 and many codes with the rare
gamma= 4, 6 parameters are obtained. In addition to these, two hundred ?fty
doubly even self dual [96,48,16]-codes with new weight enumerators are obtained
from four-circulant codes over F_4 + uF_4. New extremal doubly even binary
codes of lengths 80 and 88 are also found by the F_2+uF_2-lifts of binary four
circulant codes and a corresponding result about 3-designs is stated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0200</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0200</id><created>2014-04-01</created><authors><author><keyname>Veit</keyname><forenames>Andreas</forenames></author><author><keyname>Goebel</keyname><forenames>Christoph</forenames></author><author><keyname>Tidke</keyname><forenames>Rohit</forenames></author><author><keyname>Doblander</keyname><forenames>Christoph</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author></authors><title>Household Electricity Demand Forecasting -- Benchmarking
  State-of-the-Art Methods</title><categories>cs.LG stat.AP</categories><comments>Technical Report</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing use of renewable energy sources with variable output, such as
solar photovoltaic and wind power generation, calls for Smart Grids that
effectively manage flexible loads and energy storage. The ability to forecast
consumption at different locations in distribution systems will be a key
capability of Smart Grids. The goal of this paper is to benchmark
state-of-the-art methods for forecasting electricity demand on the household
level across different granularities and time scales in an explorative way,
thereby revealing potential shortcomings and find promising directions for
future research in this area. We apply a number of forecasting methods
including ARIMA, neural networks, and exponential smoothening using several
strategies for training data selection, in particular day type and sliding
window based strategies. We consider forecasting horizons ranging between 15
minutes and 24 hours. Our evaluation is based on two data sets containing the
power usage of individual appliances at second time granularity collected over
the course of several months. The results indicate that forecasting accuracy
varies significantly depending on the choice of forecasting methods/strategy
and the parameter configuration. Measured by the Mean Absolute Percentage Error
(MAPE), the considered state-of-the-art forecasting methods rarely beat
corresponding persistence forecasts. Overall, we observed MAPEs in the range
between 5 and &gt;100%. The average MAPE for the first data set was ~30%, while it
was ~85% for the other data set. These results show big room for improvement.
Based on the identified trends and experiences from our experiments, we
contribute a detailed discussion of promising future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0218</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0218</id><created>2014-04-01</created><updated>2014-08-26</updated><authors><author><keyname>Jung</keyname><forenames>Peter</forenames></author><author><keyname>Walk</keyname><forenames>Philipp</forenames></author></authors><title>Sparse Model Uncertainties in Compressed Sensing with Application to
  Convolutions and Sporadic Communication</title><categories>cs.IT math.IT</categories><comments>Book chapter, submitted to &quot;Compressed Sensing and its Applications&quot;,
  31 pages, revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of the compressed sensing paradigm has shown that a substantial
reduction in sampling and storage complexity can be achieved in certain linear
and non-adaptive estimation problems. It is therefore an advisable strategy for
noncoherent information retrieval in, for example, sporadic blind and
semi-blind communication and sampling problems. But, the conventional model is
not practical here since the compressible signals have to be estimated from
samples taken solely on the output of an un-calibrated system which is unknown
during measurement but often compressible. Conventionally, one has either to
operate at suboptimal sampling rates or the recovery performance substantially
suffers from the dominance of model mismatch. In this work we discuss such type
of estimation problems and we focus on bilinear inverse problems. We link this
problem to the recovery of low-rank and sparse matrices and establish stable
low-dimensional embeddings of the uncalibrated receive signals whereby
addressing also efficient communication-oriented methods like universal random
demodulation. Exemplary, we investigate in more detail sparse convolutions
serving as a basic communication channel model. In using some recent results
from additive combinatorics we show that such type of signals can be
efficiently low-rate sampled by semi-blind methods. Finally, we present a
further application of these results in the field of phase retrieval from
intensity Fourier measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0231</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0231</id><created>2014-04-01</created><authors><author><keyname>alqaralleh</keyname><forenames>Bassam A.</forenames></author><author><keyname>Almiani</keyname><forenames>Khaled</forenames></author></authors><title>Mobile Elements Scheduling for Periodic Sensor Applications</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of designing the mobile elements
tours such that the length of each tour is below a per-determined length and
the depth of the multi-hop routing trees bounded by k. The path of the mobile
element is designed to visit subset of the nodes (cache points). These cache
points store other nodes data. To address this problem, we propose two
heuristic-based solutions. Our solutions take into consideration the
distribution of the nodes during the establishment of the tour. The results of
our experiments indicate that our schemes significantly outperforms the best
comparable scheme in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0237</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0237</id><created>2014-04-01</created><updated>2016-03-04</updated><authors><author><keyname>Borri</keyname><forenames>Alessandro</forenames></author><author><keyname>Pola</keyname><forenames>Giordano</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Maria Domenica</forenames></author></authors><title>Symbolic Control Design of Nonlinear Networked Control Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked Control Systems (NCS) are distributed systems where plants,
sensors, actuators and controllers communicate over shared networks. Non-ideal
behaviors of the communication network include variable sampling/transmission
intervals and communication delays, packet losses, communication constraints
and quantization errors. NCS have been the object of intensive study in the
last few years. However, due to the inherent complexity of NCS, current
literature focuses on only a subset of these non-idealities and mostly
considers stability and stabilizability problems. Recent technology advances
indeed demand that different and more complex control objectives are
considered. In this paper we present first a general model of NCS, including
all the non-idealities of the communication network; then, we propose a
symbolic model approach to the control design with objectives expressed in
terms of non-deterministic transition systems. The presented results are based
on recent advances in symbolic control design of hybrid and continuous control
systems. An example in the context of robot motion planning with remote control
is included, showing the effectiveness of the approach taken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0255</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0255</id><created>2014-04-01</created><authors><author><keyname>Le</keyname><forenames>Sy-Quoc</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>A Case Where Interference Does Not Affect The Channel Dispersion</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1975, Carleial presented a special case of an interference channel in
which the interference does not reduce the capacity of the constituent
point-to-point Gaussian channels. In this work, we show that if the
inequalities in the conditions that Carleial stated are strict, the dispersions
are similarly unaffected. More precisely, in this work, we characterize the
second-order coding rates of the Gaussian interference channel in the strictly
very strong interference regime. In other words, we characterize the speed of
convergence of rates of optimal block codes towards a boundary point of the
(rectangular) capacity region. These second-order rates are expressed in terms
of the average probability of error and variances of some modified information
densities which coincide with the dispersion of the (single-user) Gaussian
channel. We thus conclude that the dispersions are unaffected by interference
in this channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0261</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0261</id><created>2014-04-01</created><authors><author><keyname>Alvarez</keyname><forenames>Victor</forenames></author><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Ray</keyname><forenames>Saurabh</forenames></author><author><keyname>Seidel</keyname><forenames>Raimund</forenames></author></authors><title>Counting Triangulations and other Crossing-Free Structures Approximately</title><categories>cs.CG cs.DS</categories><comments>19 pages, 2 figures. A preliminary version appeared at CCCG 2013</comments><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of counting straight-edge triangulations of a given
set $P$ of $n$ points in the plane. Until very recently it was not known
whether the exact number of triangulations of $P$ can be computed
asymptotically faster than by enumerating all triangulations. We now know that
the number of triangulations of $P$ can be computed in $O^{*}(2^{n})$ time,
which is less than the lower bound of $\Omega(2.43^{n})$ on the number of
triangulations of any point set. In this paper we address the question of
whether one can approximately count triangulations in sub-exponential time. We
present an algorithm with sub-exponential running time and sub-exponential
approximation ratio, that is, denoting by $\Lambda$ the output of our
algorithm, and by $c^{n}$ the exact number of triangulations of $P$, for some
positive constant $c$, we prove that $c^{n}\leq\Lambda\leq c^{n}\cdot
2^{o(n)}$. This is the first algorithm that in sub-exponential time computes a
$(1+o(1))$-approximation of the base of the number of triangulations, more
precisely, $c\leq\Lambda^{\frac{1}{n}}\leq(1 + o(1))c$. Our algorithm can be
adapted to approximately count other crossing-free structures on $P$, keeping
the quality of approximation and running time intact. In this paper we show how
to do this for matchings and spanning trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0265</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0265</id><created>2014-04-01</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>On Minimizing the Maximum Broadcast Decoding Delay for Instantly
  Decodable Network Coding</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of minimizing the maximum broadcast
decoding delay experienced by all the receivers of generalized instantly
decodable network coding (IDNC). Unlike the sum decoding delay, the maximum
decoding delay as a definition of delay for IDNC allows a more equitable
distribution of the delays between the different receivers and thus a better
Quality of Service (QoS). In order to solve this problem, we first derive the
expressions for the probability distributions of maximum decoding delay
increments. Given these expressions, we formulate the problem as a maximum
weight clique problem in the IDNC graph. Although this problem is known to be
NP-hard, we design a greedy algorithm to perform effective packet selection.
Through extensive simulations, we compare the sum decoding delay and the max
decoding delay experienced when applying the policies to minimize the sum
decoding delay [1] and our policy to reduce the max decoding delay. Simulations
results show that our policy gives a good agreement among all the delay aspects
in all situations and outperforms the sum decoding delay policy to effectively
minimize the sum decoding delay when the channel conditions become harsher.
They also show that our definition of delay significantly improve the number of
served receivers when they are subject to strict delay constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0267</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0267</id><created>2014-04-01</created><authors><author><keyname>Coulmont</keyname><forenames>Baptiste</forenames></author><author><keyname>Supervie</keyname><forenames>Virginie</forenames></author><author><keyname>Breban</keyname><forenames>Romulus</forenames></author></authors><title>The diffusion dynamics of choice: From durable goods markets to fashion
  first names</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 1 table, 2 figures, 4 pages of supporting information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goods, styles, ideologies are adopted by society through various mechanisms.
In particular, adoption driven by innovation is extensively studied by
marketing economics. Mathematical models are currently used to forecast the
sales of innovative goods. Inspired by the theory of diffusion processes
developed for marketing economics, we propose, for the first time, a predictive
framework for the mechanism of fashion, which we apply to first names. Analyses
of French, Dutch and US national databases validate our modelling approach for
thousands of first names, covering, on average, more than 50% of the yearly
incidence in each database. In these cases, it is thus possible to forecast how
popular the first names will become and when they will run out of fashion.
Furthermore, we uncover a clear distinction between popularity and fashion:
less popular names, typically not included in studies of fashion, may be driven
by fashion, as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0273</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0273</id><created>2014-04-01</created><updated>2015-01-12</updated><authors><author><keyname>Zhu</keyname><forenames>Jingge</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Lattice Codes for Many-to-One Interference Channels With and Without
  Cognitive Messages</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new achievable rate region is given for the Gaussian cognitive many-to-one
interference channel. The proposed novel coding scheme is based on the
compute-and-forward approach with lattice codes. Using the idea of decoding
sums of codewords, our scheme improves considerably upon the conventional
coding schemes which treat interference as noise or decode messages
simultaneously. Our strategy also extends directly to the usual many-to-one
interference channels without cognitive messages. Comparing to the usual
compute-and-forward scheme where a fixed lattice is used for the code
construction, the novel scheme employs scaled lattices and also encompasses key
ingredients of the existing schemes for the cognitive interference channel.
With this new component, our scheme achieves a larger rate region in general.
For some symmetric channel settings, new constant gap or capacity results are
established, which are independent of the number of users in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0281</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0281</id><created>2014-04-01</created><updated>2014-09-22</updated><authors><author><keyname>Dubey</keyname><forenames>Chandan</forenames></author><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author></authors><title>Sampling a Uniform Random Solution of a Quadratic Equation Modulo $p^k$</title><categories>cs.DS cs.DM math.NT math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $n$-ary integral quadratic form is a formal expression
$Q(x_1,...,x_n)=\sum_{1\leq i,j\leq n}a_{ij}x_ix_j$ in $n$-variables
$x_1,...,x_n$, where $a_{ij}=a_{ji} \in \mathbb{Z}$. We present a poly$(n,k,
\log p, \log t)$ randomized algorithm that given a quadratic form
$Q(x_1,...,x_n)$, a prime $p$, a positive integer $k$ and an integer $t$,
samples a uniform solution of $Q(x_1,...,x_n)\equiv t \bmod{p^k}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0284</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0284</id><created>2014-04-01</created><updated>2015-03-19</updated><authors><author><keyname>Kelly</keyname><forenames>Jack</forenames></author><author><keyname>Knottenbelt</keyname><forenames>William</forenames></author></authors><title>The UK-DALE dataset, domestic appliance-level electricity demand and
  whole-house demand from five UK homes</title><categories>cs.OH</categories><journal-ref>Scientific Data 2 (2015) Article number: 150007 (2015)</journal-ref><doi>10.1038/sdata.2015.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many countries are rolling out smart electricity meters. These measure a
home's total power demand. However, research into consumer behaviour suggests
that consumers are best able to improve their energy efficiency when provided
with itemised, appliance-by-appliance consumption information. Energy
disaggregation is a computational technique for estimating
appliance-by-appliance energy consumption from a whole-house meter signal.
  To conduct research on disaggregation algorithms, researchers require data
describing not just the aggregate demand per building but also the `ground
truth' demand of individual appliances. In this context, we present UK-DALE: an
open-access dataset from the UK recording Domestic Appliance-Level Electricity
at a sample rate of 16 kHz for the whole-house and at 1/6 Hz for individual
appliances. This is the first open access UK dataset at this temporal
resolution. We recorded from five houses, one of which was recorded for 655
days, the longest duration we are aware of for any energy dataset at this
sample rate. We also describe the low-cost, open-source, wireless system we
built for collecting our dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0286</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0286</id><created>2014-04-01</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Pszona</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Wear Minimization for Cuckoo Hashing: How Not to Throw a Lot of Eggs
  into One Basket</title><categories>cs.DS</categories><comments>13 pages, 1 table, 7 figures; to appear at the 13th Symposium on
  Experimental Algorithms (SEA 2014)</comments><acm-class>B.8.2; E.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study wear-leveling techniques for cuckoo hashing, showing that it is
possible to achieve a memory wear bound of $\log\log n+O(1)$ after the
insertion of $n$ items into a table of size $Cn$ for a suitable constant $C$
using cuckoo hashing. Moreover, we study our cuckoo hashing method empirically,
showing that it significantly improves on the memory wear performance for
classic cuckoo hashing and linear probing in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0296</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0296</id><created>2014-04-01</created><authors><author><keyname>Rahman</keyname><forenames>Mostafizur</forenames></author><author><keyname>Narayanan</keyname><forenames>Pritish</forenames></author><author><keyname>Moritz</keyname><forenames>Csaba Andras</forenames></author></authors><title>Metal-Gated Junctionless Nanowire Transistors</title><categories>cs.ET cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Junctionless Nanowire Field-Effect Transistors (JNFETs), where the channel
region is uniformly doped without the need for source-channel and drain-channel
junctions or lateral doping abruptness, are considered an attractive
alternative to conventional CMOS FETs. Previous theoretical and experimental
works [1][2] on JNFETs have considered polysilicon gates and silicon-dioxide
dielectric. However, with further scaling, JNFETs will suffer from deleterious
effects of doped polysilicon such as high resistance, additional capacitance
due to gate-oxide interface depletion, and incompatibility with high-k
dielectrics[3][4]. In this paper, novel metal- gated high-k JNFETs are
investigated through detailed process and device simulations. These MJNFETs are
also ideally suited for new types of nano-architectures such as N3ASICs [5]
which utilize regular nanowire arrays with limited customization. In such nano-
systems, the simplified device geometry in conjunction with a single-type FET
circuit style [6] would imply that logic arrays could be patterned out of
pre-doped SOI wafers without the need for any additional ion implantation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0297</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0297</id><created>2014-04-01</created><authors><author><keyname>Schr&#xf6;der</keyname><forenames>Matthias</forenames></author><author><keyname>Selivanov</keyname><forenames>Victor</forenames></author></authors><title>Hyperprojective Hierarchy of QCB_0-spaces</title><categories>cs.LO math.LO</categories><comments>Conference version to appear in LNCS</comments><msc-class>03D55, 03D57, 03D65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the Luzin hierarchy of qcb$_0$-spaces introduced in [ScS13] to all
countable ordinals, obtaining in this way the hyperprojective hierarchy of
qcb$_0$-spaces. We generalize all main results of [ScS13] to this larger
hierarchy. In particular, we extend the Kleene-Kreisel continuous functionals
of finite types to the continuous functionals of countable types and relate
them to the new hierarchy. We show that the category of hyperprojective
qcb$_0$-spaces has much better closure properties than the category of
projective qcb$_0$-space. As a result, there are natural examples of spaces
that are hyperprojective but not projective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0298</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0298</id><created>2014-04-01</created><authors><author><keyname>Zou</keyname><forenames>Shaofeng</forenames></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Kernel-Based Nonparametric Test for Anomaly Detection over Line
  Networks</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nonparametric problem of detecting existence of an anomalous interval
over a one dimensional line network is studied. Nodes corresponding to an
anomalous interval (if exists) receive samples generated by a distribution q,
which is different from the distribution p that generates samples for other
nodes. If anomalous interval does not exist, then all nodes receive samples
generated by p. It is assumed that the distributions p and q are arbitrary, and
are unknown. In order to detect whether an anomalous interval exists, a test is
built based on mean embeddings of distributions into a reproducing kernel
Hilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It is
shown that as the network size n goes to infinity, if the minimum length of
candidate anomalous intervals is larger than a threshold which has the order
O(log n), the proposed test is asymptotically successful, i.e., the probability
of detection error approaches zero asymptotically. An efficient algorithm to
perform the test with substantial computational complexity reduction is
proposed, and is shown to be asymptotically successful if the condition on the
minimum length of candidate anomalous interval is satisfied. Numerical results
are provided, which are consistent with the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0300</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0300</id><created>2014-04-01</created><updated>2014-08-19</updated><authors><author><keyname>Darmon</keyname><forenames>David</forenames></author><author><keyname>Omodei</keyname><forenames>Elisa</forenames></author><author><keyname>Garland</keyname><forenames>Joshua</forenames></author></authors><title>Followers Are Not Enough: A Question-Oriented Approach to Community
  Detection in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 4 figures, 1 tables</comments><doi>10.1371/journal.pone.0134860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in online social networks is typically based on the
analysis of the explicit connections between users, such as &quot;friends&quot; on
Facebook and &quot;followers&quot; on Twitter. But online users often have hundreds or
even thousands of such connections, and many of these connections do not
correspond to real friendships or more generally to accounts that users
interact with. We claim that community detection in online social networks
should be question-oriented and rely on additional information beyond the
simple structure of the network. The concept of 'community' is very general,
and different questions such as &quot;whom do we interact with?&quot; and &quot;with whom do
we share similar interests?&quot; can lead to the discovery of different social
groups. In this paper we focus on three types of communities beyond structural
communities: activity-based, topic-based, and interaction-based. We analyze a
Twitter dataset using three different weightings of the structural network
meant to highlight these three community types, and then infer the communities
associated with these weightings. We show that the communities obtained in the
three weighted cases are highly different from each other, and from the
communities obtained by considering only the unweighted structural network. Our
results confirm that asking a precise question is an unavoidable first step in
community detection in online social networks, and that different questions can
lead to different insights about the network under study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0320</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0320</id><created>2014-04-01</created><authors><author><keyname>Kundu</keyname><forenames>Abhisek</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>A Note on Randomized Element-wise Matrix Sparsification</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a matrix A \in R^{m x n}, we present a randomized algorithm that
sparsifies A by retaining some of its elements by sampling them according to a
distribution that depends on both the square and the absolute value of the
entries. We combine the ideas of [4, 1] and provide an elementary proof of the
approximation accuracy of our algorithm following [4] without the truncation
step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0321</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0321</id><created>2014-03-31</created><authors><author><keyname>Yu</keyname><forenames>Bei</forenames></author><author><keyname>Pan</keyname><forenames>David Z.</forenames></author></authors><title>Layout Decomposition for Quadruple Patterning Lithography and Beyond</title><categories>cs.DS</categories><comments>DAC'2014</comments><acm-class>B.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For next-generation technology nodes, multiple patterning lithography (MPL)
has emerged as a key solution, e.g., triple patterning lithography (TPL) for
14/11nm, and quadruple patterning lithography (QPL) for sub-10nm. In this
paper, we propose a generic and robust layout decomposition framework for QPL,
which can be further extended to handle any general K-patterning lithography
(K$&gt;$4). Our framework is based on the semidefinite programming (SDP)
formulation with novel coloring encoding. Meanwhile, we propose fast yet
effective coloring assignment and achieve significant speedup. To our best
knowledge, this is the first work on the general multiple patterning
lithography layout decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0333</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0333</id><created>2014-04-01</created><updated>2014-08-26</updated><authors><author><keyname>Lenormand</keyname><forenames>Maxime</forenames></author><author><keyname>Picornell</keyname><forenames>Miguel</forenames></author><author><keyname>Cantu-Ros</keyname><forenames>Oliva G.</forenames></author><author><keyname>Tugores</keyname><forenames>Antonia</forenames></author><author><keyname>Louail</keyname><forenames>Thomas</forenames></author><author><keyname>Herranz</keyname><forenames>Ricardo</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author><author><keyname>Frias-Martinez</keyname><forenames>Enrique</forenames></author><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author></authors><title>Cross-checking different sources of mobility information</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>11 pages, 9 figures, 1 appendix with 7 figures</comments><journal-ref>PLoS ONE 9, e105184 (2014)</journal-ref><doi>10.1371/journal.pone.0105184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasive use of new mobile devices has allowed a better characterization
in space and time of human concentrations and mobility in general. Besides its
theoretical interest, describing mobility is of great importance for a number
of practical applications ranging from the forecast of disease spreading to the
design of new spaces in urban environments. While classical data sources, such
as surveys or census, have a limited level of geographical resolution (e.g.,
districts, municipalities, counties are typically used) or are restricted to
generic workdays or weekends, the data coming from mobile devices can be
precisely located both in time and space. Most previous works have used a
single data source to study human mobility patterns. Here we perform instead a
cross-check analysis by comparing results obtained with data collected from
three different sources: Twitter, census and cell phones. The analysis is
focused on the urban areas of Barcelona and Madrid, for which data of the three
types is available. We assess the correlation between the datasets on different
aspects: the spatial distribution of people concentration, the temporal
evolution of people density and the mobility patterns of individuals. Our
results show that the three data sources are providing comparable information.
Even though the representativeness of Twitter geolocated data is lower than
that of mobile phone and census data, the correlations between the population
density profiles and mobility patterns detected by the three datasets are close
to one in a grid with cells of 2x2 and 1x1 square kilometers. This level of
correlation supports the feasibility of interchanging the three data sources at
the spatio-temporal scales considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0334</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0334</id><created>2014-04-01</created><updated>2014-04-02</updated><authors><author><keyname>Zhu</keyname><forenames>Menglong</forenames></author><author><keyname>Atanasov</keyname><forenames>Nikolay</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Active Deformable Part Models</title><categories>cs.CV cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an active approach for part-based object detection, which
optimizes the order of part filter evaluations and the time at which to stop
and make a prediction. Statistics, describing the part responses, are learned
from training data and are used to formalize the part scheduling problem as an
offline optimization. Dynamic programming is applied to obtain a policy, which
balances the number of part evaluations with the classification accuracy.
During inference, the policy is used as a look-up table to choose the part
order and the stopping time based on the observed filter responses. The method
is faster than cascade detection with deformable part models (which does not
optimize the part order) with negligible loss in accuracy when evaluated on the
PASCAL VOC 2007 and 2010 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0336</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0336</id><created>2014-04-01</created><updated>2014-06-05</updated><authors><author><keyname>Baxter</keyname><forenames>John S. H.</forenames></author><author><keyname>Rajchl</keyname><forenames>Martin</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author></authors><title>A Continuous Max-Flow Approach to General Hierarchical Multi-Labeling
  Problems</title><categories>cs.CV</categories><comments>11 pages, 1 figure, 3 algorithms -v2: Fixed typos / grammatical
  errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-region segmentation algorithms often have the onus of incorporating
complex anatomical knowledge representing spatial or geometric relationships
between objects, and general-purpose methods of addressing this knowledge in an
optimization-based manner have thus been lacking. This paper presents
Generalized Hierarchical Max-Flow (GHMF) segmentation, which captures simple
anatomical part-whole relationships in the form of an unconstrained hierarchy.
Regularization can then be applied to both parts and wholes independently,
allowing for spatial grouping and clustering of labels in a globally optimal
convex optimization framework. For the purposes of ready integration into a
variety of segmentation tasks, the hierarchies can be presented in run-time,
allowing for the segmentation problem to be readily specified and alternatives
explored without undue programming effort or recompilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0337</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0337</id><created>2014-04-01</created><updated>2014-04-16</updated><authors><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author></authors><title>The Complexity of Bounded Length Graph Recoloring</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following question: Given are two $k$-colorings $\alpha$ and
$\beta$ of a graph $G$ on $n$ vertices, and integer $\ell$. The question is
whether $\alpha$ can be modified into $\beta$, by recoloring vertices one at a
time, while maintaining a $k$-coloring throughout, and using at most $\ell$
such recoloring steps. This problem is weakly PSPACE-hard for every constant
$k\ge 4$. We show that it is also strongly NP-hard for every constant $k\ge 4$.
On the positive side, we give an $O(f(k,\ell) n^{O(1)})$ algorithm for the
problem, for some computable function $f$. Hence the problem is fixed-parameter
tractable when parameterized by $k+\ell$. Finally, we show that the problem is
W[1]-hard (but in XP) when parameterized only by $\ell$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0338</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0338</id><created>2014-04-01</created><authors><author><keyname>Lee</keyname><forenames>Sung G.</forenames></author><author><keyname>Egerstedt</keyname><forenames>Magnus</forenames></author></authors><title>Multi-Robot Control Using Time-Varying Density Functions</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to externally influencing a team of robots by
means of time-varying density functions. These density functions represent
rough references for where the robots should be located. To this end, a
continuous-time algorithm is proposed that moves the robots so as to provide
optimal coverage given the density functions as they evolve over time. The
developed algorithm represents an extension to previous coverage algorithms in
that time-varying densities are explicitly taken into account in a provable
manner. A distributed approximation to this algorithm is moreover proposed
whereby the robots only need to access information from adjacent robots.
Simulations and robotic experiments show that the proposed algorithms do indeed
exhibit the desired behaviors in practice as well as in theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0339</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0339</id><created>2014-04-01</created><updated>2015-10-10</updated><authors><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>Constructive discrepancy minimization for convex sets</title><categories>cs.DM cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical theorem of Spencer shows that any set system with n sets and n
elements admits a coloring of discrepancy O(n^1/2). Recent exciting work of
Bansal, Lovett and Meka shows that such colorings can be found in polynomial
time. In fact, the Lovett-Meka algorithm finds a half integral point in any
&quot;large enough&quot; polytope. However, their algorithm crucially relies on the facet
structure and does not apply to general convex sets.
  We show that for any symmetric convex set K with measure at least
exp(-n/500), the following algorithm finds a point y in K \cap [-1,1]^n with
Omega(n) coordinates in {-1,+1}: (1) take a random Gaussian vector x; (2)
compute the point y in K \cap [-1,1]^n that is closest to x. (3) return y.
  This provides another truly constructive proof of Spencer's theorem and the
first constructive proof of a Theorem of Gluskin and Giannopoulos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0346</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0346</id><created>2014-04-01</created><authors><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Scaling laws for molecular communication</title><categories>cs.IT math-ph math.IT math.MP q-bio.QM</categories><comments>Accepted for publication in the 2014 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate information-theoretic scaling laws, independent
from communication strategies, for point-to-point molecular communication,
where it sends/receives information-encoded molecules between nanomachines.
Since the Shannon capacity for this is still an open problem, we first derive
an asymptotic order in a single coordinate, i.e., i) scaling time with constant
number of molecules $m$ and ii) scaling molecules with constant time $t$. For a
single coordinate case, we show that the asymptotic scaling is logarithmic in
either coordinate, i.e., $\Theta(\log t)$ and $\Theta(\log m)$, respectively.
We also study asymptotic behavior of scaling in both time and molecules and
show that, if molecules and time are proportional to each other, then the
asymptotic scaling is linear, i.e., $\Theta(t)=\Theta(m)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0354</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0354</id><created>2014-04-01</created><updated>2014-06-19</updated><authors><author><keyname>Lei</keyname><forenames>Ming</forenames></author><author><keyname>Soleymani</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Performance of the Generalized Quantize-and-Forward Scheme over the
  Multiple-Access Relay Channel</title><categories>cs.IT math.IT</categories><comments>26 pages, 8 figures, submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on the half-duplex (HD) relaying based on the generalized
quantize-and-forward (GQF) scheme in the slow fading Multiple Access Relay
Channel (MARC) where the relay has no channel state information (CSI) of the
relay-to-destination link. Relay listens to the channel in the first slot of
the transmission block and cooperatively transmits to the destination in the
second slot. In order to investigate the performance of the GQF, the following
steps have been taken: 1)The GQF scheme is applied to establish the achievable
rate regions of the discrete memoryless half-duplex MARC and the corresponding
additive white Gaussian noise channel. This scheme is developed based on the
generalization of the Quantize-and-Forward (QF) scheme and single block with
two slots coding structure. 2) as the general performance measure of the slow
fading channel, the common outage probability and the expected sum rate (total
throughput) of the GQF scheme have been characterized. The numerical examples
show that when the relay has no access to the CSI of the relay-destination
link, the GQF scheme outperforms other relaying schemes, e.g., classic
compress-and-forward (CF), decode-and-forward (DF) and amplify-and-forward
(AF). 3) for a MAC channel with heterogeneous user channels and
quality-of-service (QoS) requirements, individual outage probability and total
throughput of the GQF scheme are also obtained and shown to outperform the
classic CF scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0359</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0359</id><created>2014-03-31</created><updated>2014-06-03</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Inter-rater reliability and convergent validity of F1000Prime peer
  review</title><categories>cs.DL physics.soc-ph</categories><comments>Accepted for publication in the Journal of the Association for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer review is the backbone of modern science. F1000Prime is a
post-publication peer review system of the biomedical literature (papers from
medical and biological journals). This study is concerned with the inter-rater
reliability and convergent validity of the peer recommendations formulated in
the F1000Prime peer review system. The study is based on around 100,000 papers
with recommendations from Faculty members. Even if intersubjectivity plays a
fundamental role in science, the analyses of the reliability of the F1000Prime
peer review system show a rather low level of agreement between Faculty
members. This result is in agreement with most other studies which have been
published on the journal peer review system. Logistic regression models are
used to investigate the convergent validity of the F1000Prime peer review
system. As the results show, the proportion of highly cited papers among those
selected by the Faculty members is significantly higher than expected. In
addition, better recommendation scores are also connected with better
performance of the papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0367</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0367</id><created>2014-04-01</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author><author><keyname>Mobilia</keyname><forenames>Mauro</forenames></author></authors><title>Facilitators on networks reveal the optimal interplay between
  information exchange and reciprocity</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>8 two-column pages, 6 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 89 (2014) 042802</journal-ref><doi>10.1103/PhysRevE.89.042802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reciprocity is firmly established as an important mechanism that promotes
cooperation. An efficient information exchange is likewise important,
especially on structured populations, where interactions between players are
limited. Motivated by these two facts, we explore the role of facilitators in
social dilemmas on networks. Facilitators are here mirrors to their neighbors
-- they cooperate with cooperators and defect with defectors -- but they do not
participate in the exchange of strategies. As such, in addition to introducing
direct reciprocity, they also obstruct information exchange. In well-mixed
populations, facilitators favor the replacement and invasion of defection by
cooperation as long as their number exceeds a critical value. In structured
populations, on the other hand, there exists a delicate balance between the
benefits of reciprocity and the deterioration of information exchange.
Extensive Monte Carlo simulations of social dilemmas on various interaction
networks reveal that there exists an optimal interplay between reciprocity and
information exchange, which sets in only when a small number of facilitators
occupies the main hubs of the scale-free network. The drawbacks of missing
cooperative hubs are more than compensated by reciprocity and, at the same
time, the compromised information exchange is routed via the auxiliary hubs
with only marginal losses in effectivity. These results indicate that it is not
always optimal for the main hubs to become ''leaders of the masses'', but
rather to exploit their highly connected state to promote tit-for-tat-like
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0390</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0390</id><created>2014-04-01</created><updated>2015-12-14</updated><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Further scramblings of Marsaglia's xorshift generators</title><categories>cs.DS cs.CR cs.MS</categories><comments>arXiv admin note: text overlap with arXiv:1402.6246</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  xorshift* generators are a variant of Marsaglia's xorshift generators that
eliminate linear artifacts typical of generators based on $\mathbf Z/2\mathbf
Z$-linear operations using multiplication by a suitable constant. Shortly after
high-dimensional xorshift* generators were introduced, Saito and Matsumoto
suggested a different way to eliminate linear artifacts based on addition in
$\mathbf Z/2^{32}\mathbf Z$, leading to the XSadd generator. Starting from the
observation that the lower bits of XSadd are very weak, as its reverse fails
systematically several statistical tests, we explore xorshift+, a variant of
XSadd using 64-bit operations, which leads, in small dimension, to extremely
fast high-quality generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0400</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0400</id><created>2014-04-01</created><authors><author><keyname>Zhang</keyname><forenames>Chiyuan</forenames></author><author><keyname>Evangelopoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Voinea</keyname><forenames>Stephen</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>A Deep Representation for Invariance And Music Classification</title><categories>cs.SD cs.LG stat.ML</categories><comments>5 pages, CBMM Memo No. 002, (to appear) IEEE 2014 International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014)</comments><report-no>CBMM Memo No. 002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representations in the auditory cortex might be based on mechanisms similar
to the visual ventral stream; modules for building invariance to
transformations and multiple layers for compositionality and selectivity. In
this paper we propose the use of such computational modules for extracting
invariant and discriminative audio representations. Building on a theory of
invariance in hierarchical architectures, we propose a novel, mid-level
representation for acoustical signals, using the empirical distributions of
projections on a set of templates and their transformations. Under the
assumption that, by construction, this dictionary of templates is composed from
similar classes, and samples the orbit of variance-inducing signal
transformations (such as shift and scale), the resulting signature is
theoretically guaranteed to be unique, invariant to transformations and stable
to deformations. Modules of projection and pooling can then constitute layers
of deep networks, for learning composite representations. We present the main
theoretical and computational aspects of a framework for unsupervised learning
of invariant audio representations, empirically evaluated on music genre
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0404</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0404</id><created>2014-04-01</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Syed</keyname><forenames>Zeeshan</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>EEG Spatial Decoding and Classification with Logit Shrinkage Regularized
  Directed Information Assessment (L-SODA)</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There is an increasing interest in studying the neural interaction mechanisms
behind patterns of cognitive brain activity. This paper proposes a new approach
to infer such interaction mechanisms from electroencephalographic (EEG) data
using a new estimator of directed information (DI) called logit shrinkage
optimized directed information assessment (L-SODA). Unlike previous directed
information measures applied to neural decoding, L-SODA uses shrinkage
regularization on multinomial logistic regression to deal with the high
dimensionality of multi-channel EEG signals and the small sizes of many
real-world datasets. It is designed to make few a priori assumptions and can
handle both non-linear and non-Gaussian flows among electrodes. Our L-SODA
estimator of the DI is accompanied by robust statistical confidence intervals
on the true DI that make it especially suitable for hypothesis testing on the
information flow patterns. We evaluate our work in the context of two different
problems where interaction localization is used to determine highly interactive
areas for EEG signals spatially and temporally. First, by mapping the areas
that have high DI into Brodmann area, we identify that the areas with high DI
are associated with motor-related functions. We demonstrate that L-SODA
provides better accuracy for neural decoding of EEG signals as compared to
several state-of-the-art approaches on the Brain Computer Interface (BCI) EEG
motor activity dataset. Second, the proposed L-SODA estimator is evaluated on
the CHB-MIT Scalp EEG database. We demonstrate that compared to the
state-of-the-art approaches, the proposed method provides better performance in
detecting the epileptic seizure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0408</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0408</id><created>2014-04-01</created><updated>2014-04-23</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Optimal Multiuser Transmit Beamforming: A Difficult Problem with a
  Simple Solution Structure</title><categories>cs.IT math.IT</categories><comments>Accepted for publication as lecture note in IEEE Signal Processing
  Magazine, 11 pages, 3 figures. The results can be reproduced using the
  following Matlab code: https://github.com/emilbjornson/optimal-beamforming</comments><journal-ref>IEEE Signal Processing Magazine, vol. 31, no. 4, pp. 142-148, July
  2014</journal-ref><doi>10.1109/MSP.2014.2312183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmit beamforming is a versatile technique for signal transmission from an
array of $N$ antennas to one or multiple users [1]. In wireless communications,
the goal is to increase the signal power at the intended user and reduce
interference to non-intended users. A high signal power is achieved by
transmitting the same data signal from all antennas, but with different
amplitudes and phases, such that the signal components add coherently at the
user. Low interference is accomplished by making the signal components add
destructively at non-intended users. This corresponds mathematically to
designing beamforming vectors (that describe the amplitudes and phases) to have
large inner products with the vectors describing the intended channels and
small inner products with non-intended user channels.
  While it is fairly easy to design a beamforming vector that maximizes the
signal power at the intended user, it is difficult to strike a perfect balance
between maximizing the signal power and minimizing the interference leakage. In
fact, the optimization of multiuser transmit beamforming is generally a
nondeterministic polynomial-time (NP) hard problem [2]. Nevertheless, this
lecture shows that the optimal transmit beamforming has a simple structure with
very intuitive properties and interpretations. This structure provides a
theoretical foundation for practical low-complexity beamforming schemes.
  (See this lecture note for the complete abstract/introduction)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0414</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0414</id><created>2014-04-01</created><authors><author><keyname>Mogavero</keyname><forenames>Fabio</forenames><affiliation>Universit&#xe0; degli Studi di Napoli Federico II</affiliation></author><author><keyname>Murano</keyname><forenames>Aniello</forenames><affiliation>Universit&#xe0; degli Studi di Napoli Federico II</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames><affiliation>Rice University</affiliation></author></authors><title>Proceedings 2nd International Workshop on Strategic Reasoning</title><categories>cs.GT cs.LO cs.MA</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014</journal-ref><doi>10.4204/EPTCS.146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 2nd International Workshop on
Strategic Reasoning 2014 (SR 2014), held in Grenoble (France), April 5-6, 2014.
The SR workshop aims to bring together researchers, possibly with different
backgrounds, working on various aspects of strategic reasoning in computer
science, both from a theoretical and a practical point of view. This year SR
has hosted four invited talks by Thomas A. Henzinger, Wiebe van der Hoek,
Alessio R. Lomuscio, and Wolfgang Thomas. Moreover, the workshop has hosted 14
contributed talks, all selected among the full contributions submitted, which
have been deeply evaluated, by four reviewers, according to their quality and
relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0417</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0417</id><created>2014-04-01</created><updated>2014-06-02</updated><authors><author><keyname>Allamanis</keyname><forenames>Miltiadis</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Mining Idioms from Source Code</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first method for automatically mining code idioms from a
corpus of previously written, idiomatic software projects. We take the view
that a code idiom is a syntactic fragment that recurs across projects and has a
single semantic role. Idioms may have metavariables, such as the body of a for
loop. Modern IDEs commonly provide facilities for manually defining idioms and
inserting them on demand, but this does not help programmers to write idiomatic
code in languages or using libraries with which they are unfamiliar. We present
HAGGIS, a system for mining code idioms that builds on recent advanced
techniques from statistical natural language processing, namely, nonparametric
Bayesian probabilistic tree substitution grammars. We apply HAGGIS to several
of the most popular open source projects from GitHub. We present a wide range
of evidence that the resulting idioms are semantically meaningful,
demonstrating that they do indeed recur across software projects and that they
occur more frequently in illustrative code examples collected from a Q&amp;A site.
Manual examination of the most common idioms indicate that they describe
important program concepts, including object creation, exception handling, and
resource management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0424</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0424</id><created>2014-04-01</created><updated>2014-08-25</updated><authors><author><keyname>Yin</keyname><forenames>Bei</forenames></author><author><keyname>Wu</keyname><forenames>Michael</forenames></author><author><keyname>Cavallaro</keyname><forenames>Joseph R.</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Conjugate Gradient-based Soft-Output Detection and Precoding in Massive
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>to appear at IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) promises improved spectral
efficiency, coverage, and range, compared to conventional (small-scale) MIMO
wireless systems. Unfortunately, these benefits come at the cost of
significantly increased computational complexity, especially for systems with
realistic antenna configurations. To reduce the complexity of data detection
(in the uplink) and precoding (in the downlink) in massive MIMO systems, we
propose to use conjugate gradient (CG) methods. While precoding using CG is
rather straightforward, soft-output minimum mean-square error (MMSE) detection
requires the computation of the post-equalization
signal-to-interference-and-noise-ratio (SINR). To enable CG for soft-output
detection, we propose a novel way of computing the SINR directly within the CG
algorithm at low complexity. We investigate the performance/complexity
trade-offs associated with CG-based soft-output detection and precoding, and we
compare it to exact and approximate methods. Our results reveal that the
proposed method outperforms existing algorithms for massive MIMO systems with
realistic antenna configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0425</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0425</id><created>2014-04-01</created><updated>2014-07-19</updated><authors><author><keyname>Wu</keyname><forenames>Shuhang</forenames></author><author><keyname>Wei</keyname><forenames>Shuangqing</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Vaidyanathan</keyname><forenames>Ramachandran</forenames></author><author><keyname>Yuan</keyname><forenames>Jian</forenames></author></authors><title>Partition Information and its Transmission over Boolean Multi-Access
  Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel partition reservation system to study the
partition information and its transmission over a noise-free Boolean
multi-access channel. The objective of transmission is not message restoration,
but to partition active users into distinct groups so that they can,
subsequently, transmit their messages without collision. We first calculate (by
mutual information) the amount of information needed for the partitioning
without channel effects, and then propose two different coding schemes to
obtain achievable transmission rates over the channel. The first one is the
brute force method, where the codebook design is based on centralized source
coding; the second method uses random coding where the codebook is generated
randomly and optimal Bayesian decoding is employed to reconstruct the
partition. Both methods shed light on the internal structure of the partition
problem. A novel hypergraph formulation is proposed for the random coding
scheme, which intuitively describes the information in terms of a strong
coloring of a hypergraph induced by a sequence of channel operations and
interactions between active users. An extended Fibonacci structure is found for
a simple, but non-trivial, case with two active users. A comparison between
these methods and group testing is conducted to demonstrate the uniqueness of
our problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0427</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0427</id><created>2014-04-01</created><updated>2014-07-13</updated><authors><author><keyname>Banda</keyname><forenames>Peter</forenames></author><author><keyname>Teuscher</keyname><forenames>Christof</forenames></author></authors><title>Learning Two-input Linear and Nonlinear Analog Functions with a Simple
  Chemical System</title><categories>q-bio.MN cs.LG</categories><comments>13 pages, 4 figures, 2 tables</comments><journal-ref>Lecture Notes in Computer Science, 8553, 14-26 (2014)</journal-ref><doi>10.1007/978-3-319-08123-6_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current biochemical information processing systems behave in a
predetermined manner because all features are defined during the design phase.
To make such unconventional computing systems reusable and programmable for
biomedical applications, adaptation, learning, and self-modification based on
external stimuli would be highly desirable. However, so far, it has been too
challenging to implement these in wet chemistries. In this paper we extend the
chemical perceptron, a model previously proposed by the authors, to function as
an analog instead of a binary system. The new analog asymmetric signal
perceptron learns through feedback and supports Michaelis-Menten kinetics. The
results show that our perceptron is able to learn linear and nonlinear
(quadratic) functions of two inputs. To the best of our knowledge, it is the
first simulated chemical system capable of doing so. The small number of
species and reactions and their simplicity allows for a mapping to an actual
wet implementation using DNA-strand displacement or deoxyribozymes. Our results
are an important step toward actual biochemical systems that can learn and
adapt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0431</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0431</id><created>2014-04-01</created><updated>2014-06-03</updated><authors><author><keyname>Aicher</keyname><forenames>Christopher</forenames></author><author><keyname>Jacobs</keyname><forenames>Abigail Z.</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Learning Latent Block Structure in Weighted Networks</title><categories>stat.ML cs.SI physics.data-an physics.soc-ph</categories><comments>28 Pages</comments><journal-ref>Journal of Complex Networks (2015) 3 (2): 221-248</journal-ref><doi>10.1093/comnet/cnu026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is an important task in network analysis, in which we aim
to learn a network partition that groups together vertices with similar
community-level connectivity patterns. By finding such groups of vertices with
similar structural roles, we extract a compact representation of the network's
large-scale structure, which can facilitate its scientific interpretation and
the prediction of unknown or future interactions. Popular approaches, including
the stochastic block model, assume edges are unweighted, which limits their
utility by throwing away potentially useful information. We introduce the
`weighted stochastic block model' (WSBM), which generalizes the stochastic
block model to networks with edge weights drawn from any exponential family
distribution. This model learns from both the presence and weight of edges,
allowing it to discover structure that would otherwise be hidden when weights
are discarded or thresholded. We describe a Bayesian variational algorithm for
efficiently approximating this model's posterior distribution over latent block
structures. We then evaluate the WSBM's performance on both edge-existence and
edge-weight prediction tasks for a set of real-world weighted networks. In all
cases, the WSBM performs as well or better than the best alternatives on these
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0434</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0434</id><created>2014-04-01</created><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author></authors><title>New Asymptotic Metrics for Relative Generalized Hamming Weight</title><categories>cs.IT math.CO math.IT</categories><comments>3 pages, 1 figure, IEEEtran.cls. To be presented at 2014 IEEE
  International Symposium on Information Theory, June 29-July 4, 2014 Hawai'i
  Convention Center, Honolulu, HI, USA</comments><msc-class>94B65</msc-class><journal-ref>Proc. 2014 IEEE International Symposium on Information Theory, pp.
  3142-3144</journal-ref><doi>10.1109/ISIT.2014.6875413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently shown that RGHW (relative generalized Hamming weight) exactly
expresses the security of linear ramp secret sharing scheme. In this paper we
determine the true value of the asymptotic metric for RGHW previously proposed
by Zhuang et al. in 2013. Then we propose new asymptotic metrics useful for
investigating the optimal performance of linear ramp secret sharing scheme
constructed from a pair of linear codes. We also determine the true values of
the proposed metrics in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0436</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0436</id><created>2014-04-01</created><authors><author><keyname>Buhnova</keyname><forenames>Bara</forenames><affiliation>Masaryk University, Czech Republic</affiliation></author><author><keyname>Happe</keyname><forenames>Lucia</forenames><affiliation>Karlsruhe Institute of Technology, Germany</affiliation></author><author><keyname>Kofro&#x148;</keyname><forenames>Jan</forenames><affiliation>Charles University in Prague, Czech Republic</affiliation></author></authors><title>Proceedings 11th International Workshop on Formal Engineering approaches
  to Software Components and Architectures</title><categories>cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.1.;D.2.4;D.2.7;D.2.11</acm-class><journal-ref>EPTCS 147, 2014</journal-ref><doi>10.4204/EPTCS.147</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the FESCA workshop is to bring together both young and senior
researchers from formal methods, software engineering, and industry interested
in the development and application of formal modelling approaches as well as
associated analysis and reasoning techniques with practical benefits for
component-based software engineering.
  Component-based software design has received considerable attention in
industry and academia in the past decade. In recent years, with the emergence
of new platforms (such as smartphones), new areas advocating software
correctness along with new challenges have appeared. These include development
of new methods and adapting existing ones to accommodate unique features of the
platforms, such as inherent distribution, openness, and continuous migration.
On the other hand, with the growing power of computers, more and more is
possible with respect to practical applicability of modelling and specification
methods as well as verification tools to real-life software, i.e, to scale to
more complex systems.
  FESCA aims to address the open question of how formal methods can be applied
effectively to these new contexts and challenges. The workshop is interested in
both the development and application of formal methods in component-based
development and tries to cross-fertilize their research and application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0437</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0437</id><created>2014-04-01</created><authors><author><keyname>Suderman</keyname><forenames>Robert</forenames></author><author><keyname>Lizotte</keyname><forenames>Daniel</forenames></author><author><keyname>Abukhdeir</keyname><forenames>Nasser Mohieddin</forenames></author></authors><title>Theory and Application of Shapelets to the Analysis of Surface
  Self-assembly Imaging</title><categories>cs.CV physics.data-an</categories><comments>11 pages, 8 figures, submitted to the Journal of Computational
  Physics</comments><doi>10.1103/PhysRevE.00.003300</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for quantitative analysis of local pattern strength and defects in
surface self-assembly imaging is presented and applied to images of stripe and
hexagonal ordered domains. The presented method uses &quot;shapelet&quot; functions which
were originally developed for quantitative analysis of images of galaxies
($\propto 10^{20}\mathrm{m}$). In this work, they are used instead to quantify
the presence of translational order in surface self-assembled films ($\propto
10^{-9}\mathrm{m}$) through reformulation into &quot;steerable&quot; filters. The
resulting method is both computationally efficient (with respect to the number
of filter evaluations), robust to variation in pattern feature shape, and,
unlike previous approaches, is applicable to a wide variety of pattern types.
An application of the method is presented which uses a nearest-neighbour
analysis to distinguish between uniform (defect-free) and non-uniform
(strained, defect-containing) regions within imaged self-assembled domains,
both with striped and hexagonal patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0442</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0442</id><created>2014-04-01</created><updated>2014-07-17</updated><authors><author><keyname>Carlberg</keyname><forenames>Kevin</forenames></author></authors><title>Adaptive $h$-refinement for reduced-order models</title><categories>cs.NA math.NA</categories><comments>submitted to the International Journal for Numerical Methods in
  Engineering, Special Issue on Model Reduction</comments><journal-ref>International Journal for Numerical Methods in Engineering, Vol.
  102, No. 5, p.1192-1210 (2014)</journal-ref><doi>10.1002/nme.4800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a method to adaptively refine reduced-order models \emph{a
posteriori} without requiring additional full-order-model solves. The technique
is analogous to mesh-adaptive $h$-refinement: it enriches the reduced-basis
space online by `splitting' a given basis vector into several vectors with
disjoint support. The splitting scheme is defined by a tree structure
constructed offline via recursive $k$-means clustering of the state variables
using snapshot data. The method identifies the vectors to split online using a
dual-weighted-residual approach that aims to reduce error in an output quantity
of interest. The resulting method generates a hierarchy of subspaces online
without requiring large-scale operations or full-order-model solves. Further,
it enables the reduced-order model to satisfy \emph{any prescribed error
tolerance} regardless of its original fidelity, as a completely refined
reduced-order model is mathematically equivalent to the original full-order
model. Experiments on a parameterized inviscid Burgers equation highlight the
ability of the method to capture phenomena (e.g., moving shocks) not contained
in the span of the original reduced basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0444</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0444</id><created>2014-04-01</created><authors><author><keyname>Schivo</keyname><forenames>Stefano</forenames><affiliation>Formal Methods and Tools, Faculty of EEMCS, University of Twente, Enschede, The Netherlands</affiliation></author><author><keyname>Scholma</keyname><forenames>Jetse</forenames><affiliation>Developmental BioEngineering, MIRA Institute for Biomedical Technology and Technical Medicine, University of Twente, Enschede, The Netherlands</affiliation></author><author><keyname>Karperien</keyname><forenames>Marcel</forenames><affiliation>Developmental BioEngineering, MIRA Institute for Biomedical Technology and Technical Medicine, University of Twente, Enschede, The Netherlands</affiliation></author><author><keyname>Post</keyname><forenames>Janine N.</forenames><affiliation>Developmental BioEngineering, MIRA Institute for Biomedical Technology and Technical Medicine, University of Twente, Enschede, The Netherlands</affiliation></author><author><keyname>van de Pol</keyname><forenames>Jaco</forenames><affiliation>Formal Methods and Tools, Faculty of EEMCS, University of Twente, Enschede, The Netherlands</affiliation></author><author><keyname>Langerak</keyname><forenames>Rom</forenames><affiliation>Formal Methods and Tools, Faculty of EEMCS, University of Twente, Enschede, The Netherlands</affiliation></author></authors><title>Setting Parameters for Biological Models With ANIMO</title><categories>q-bio.MN cs.CE</categories><proxy>Selena Clancy</proxy><journal-ref>EPTCS 145, 2014, pp. 35-47</journal-ref><doi>10.4204/EPTCS.145.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ANIMO (Analysis of Networks with Interactive MOdeling) is a software for
modeling biological networks, such as e.g. signaling, metabolic or gene
networks. An ANIMO model is essentially the sum of a network topology and a
number of interaction parameters. The topology describes the interactions
between biological entities in form of a graph, while the parameters determine
the speed of occurrence of such interactions. When a mismatch is observed
between the behavior of an ANIMO model and experimental data, we want to update
the model so that it explains the new data. In general, the topology of a model
can be expanded with new (known or hypothetical) nodes, and enables it to match
experimental data. However, the unrestrained addition of new parts to a model
causes two problems: models can become too complex too fast, to the point of
being intractable, and too many parts marked as &quot;hypothetical&quot; or &quot;not known&quot;
make a model unrealistic. Even if changing the topology is normally the easier
task, these problems push us to try a better parameter fit as a first step, and
resort to modifying the model topology only as a last resource. In this paper
we show the support added in ANIMO to ease the task of expanding the knowledge
on biological networks, concentrating in particular on the parameter settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0447</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0447</id><created>2014-04-01</created><updated>2015-05-28</updated><authors><author><keyname>Jacquelin</keyname><forenames>Mathias</forenames></author><author><keyname>Lin</keyname><forenames>Lin</forenames></author><author><keyname>Yang</keyname><forenames>Chao</forenames></author></authors><title>PSelInv -- A Distributed Memory Parallel Algorithm for Selected
  Inversion : the Symmetric Case</title><categories>math.NA cs.DC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an efficient parallel implementation of the selected inversion
algorithm for distributed memory computer systems, which we call
\texttt{PSelInv}. The \texttt{PSelInv} method computes selected elements of a
general sparse matrix $A$ that can be decomposed as $A = LU$, where $L$ is
lower triangular and $U$ is upper triangular. The implementation described in
this paper focuses on the case of sparse symmetric matrices. It contains an
interface that is compatible with the distributed memory parallel sparse direct
factorization \texttt{SuperLU\_DIST}. However, the underlying data structure
and design of \texttt{PSelInv} allows it to be easily combined with other
factorization routines such as \texttt{PARDISO}. We discuss general
parallelization strategies such as data and task distribution schemes. In
particular, we describe how to exploit the concurrency exposed by the
elimination tree associated with the $LU$ factorization of $A$. We demonstrate
the efficiency and accuracy of \texttt{PSelInv} by presenting a number of
numerical experiments. In particular, we show that \texttt{PSelInv} can run
efficiently on more than $4,000$ cores for a modestly sized matrix. We also
demonstrate how \texttt{PSelInv} can be used to accelerate large-scale
electronic structure calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0453</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0453</id><created>2014-04-02</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>Cellular Automata and Its Applications in Bioinformatics: A Review</title><categories>cs.CE cs.LG</categories><journal-ref>Global Perspectives on Artificial Intelligence (GPAI) Volume 2
  Issue 2, Pages 16-22 April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at providing a survey on the problems that can be easily
addressed by cellular automata in bioinformatics. Some of the authors have
proposed algorithms for addressing some problems in bioinformatics but the
application of cellular automata in bioinformatics is a virgin field in
research. None of the researchers has tried to relate the major problems in
bioinformatics and find a common solution. Extensive literature surveys were
conducted. We have considered some papers in various journals and conferences
for conduct of our research. This paper provides intuition towards relating
various problems in bioinformatics logically and tries to attain a common frame
work for addressing the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0459</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0459</id><created>2014-04-02</created><authors><author><keyname>Baba-Ahmed</keyname><forenames>Mohammed Zakarya</forenames><affiliation>LTT</affiliation></author><author><keyname>Benmammar</keyname><forenames>Badr</forenames><affiliation>LTT</affiliation></author><author><keyname>Bendimerad</keyname><forenames>Fethi Tarik</forenames><affiliation>LTT</affiliation></author></authors><title>Self-protection and self-healing in the context of cognitive radio</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Artificial Intelligence and
  Information Technology (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio (CR) operates in different fields as varied, one of these is
cognitive radio networks. In this paper, we propose a new approach used CR,
which aims to manage potential failures of computer systems and applications
through the introduction of two aspects of autonomous networks to make systems
capable of managing themselves with minimum human intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0462</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0462</id><created>2014-04-02</created><updated>2014-05-14</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Wagner</keyname><forenames>Caroline S.</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>The European Union, China, and the United States in the Top-1% and
  Top-10% Layers of Most-Frequently-Cited Publications: Competition and
  Collaborations</title><categories>cs.DL</categories><comments>Journal of Informetrics (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The percentages of shares of world publications of the European Union and its
member states, China, and the United States have been represented differently
as a result of using different databases. An analytical variant of the
Web-of-Science (of Thomson Reuters) enables us to study the dynamics in the
world publication system in terms of the field-normalized top-1% and top-10%
most-frequently-cited publications. Comparing the EU28, USA, and China at the
global level shows a top-level dynamics that is different from the analysis in
terms of shares of publications: the United States remains far more productive
in the top-1% of all papers; China drops out of the competition for elite
status; and the EU28 increased its share among the top-cited papers from
2000-2010. Some of the EU28 member states overtook the U.S. during this decade,
but a clear divide remains between EU15 (Western Europe) and the Accession
Countries. Network analysis shows that internationally co-authored top-1%
publications perform far above expectation and also above top-10% ones. In
2005, China was embedded in this top-layer of internationally co-authored
publications. These publications often involve more than a single European
nation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0466</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0466</id><created>2014-04-02</created><updated>2015-06-10</updated><authors><author><keyname>Kuang</keyname><forenames>Da</forenames></author><author><keyname>Gittens</keyname><forenames>Alex</forenames></author><author><keyname>Hamid</keyname><forenames>Raffay</forenames></author></authors><title>piCholesky: Polynomial Interpolation of Multiple Cholesky Factors for
  Efficient Approximate Cross-Validation</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dominant cost in solving least-square problems using Newton's method is
often that of factorizing the Hessian matrix over multiple values of the
regularization parameter ($\lambda$). We propose an efficient way to
interpolate the Cholesky factors of the Hessian matrix computed over a small
set of $\lambda$ values. This approximation enables us to optimally minimize
the hold-out error while incurring only a fraction of the cost compared to
exact cross-validation. We provide a formal error bound for our approximation
scheme and present solutions to a set of key implementation challenges that
allow our approach to maximally exploit the compute power of modern
architectures. We present a thorough empirical analysis over multiple datasets
to show the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0471</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0471</id><created>2014-04-02</created><authors><author><keyname>Kang</keyname><forenames>Xin</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Full-Duplex Wireless-Powered Communication Network with Energy Causality</title><categories>cs.IT math.IT</categories><comments>Energy Harvesting, Wireless Power Transfer, Full-Duplex, Optimal
  Resource Allocation, Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a wireless communication network with a
full-duplex hybrid access point (HAP) and a set of wireless users with energy
harvesting capabilities. The HAP implements the full-duplex through two
antennas: one for broadcasting wireless energy to users in the downlink and one
for receiving independent information from users via
time-division-multiple-access (TDMA) in the uplink at the same time. All users
can continuously harvest wireless power from the HAP until its transmission
slot, i.e., the energy causality constraint is modeled by assuming that energy
harvested in the future cannot be used for tranmission. Hence, latter users'
energy harvesting time is coupled with the transmission time of previous users.
Under this setup, we investigate the sum-throughput maximization (STM) problem
and the total-time minimization (TTM) problem for the proposed multi-user
full-duplex wireless-powered network. The STM problem is proved to be a convex
optimization problem. The optimal solution strategy is then obtained in
closed-form expression, which can be computed with linear complexity. It is
also shown that the sum throughput is non-decreasing with increasing of the
number of users. For the TTM problem, by exploiting the properties of the
coupling constraints, we propose a two-step algorithm to obtain an optimal
solution. Then, for each problem, two suboptimal solutions are proposed and
investigated. Finally, the effect of user scheduling on STM and TTM are
investigated through simulations. It is also shown that different user
scheduling strategies should be used for STM and TTM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0530</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0530</id><created>2014-04-01</created><authors><author><keyname>Zhang</keyname><forenames>Haixin</forenames></author><author><keyname>Wei</keyname><forenames>Daijun</forenames></author><author><keyname>Hu</keyname><forenames>Yong</forenames></author><author><keyname>Lan</keyname><forenames>Xin</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>Modelling the Self-similarity in Complex Networks Based on Coulomb's Law</title><categories>cs.SI physics.soc-ph</categories><comments>25 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, self-similarity of complex networks have attracted much attention.
Fractal dimension of complex network is an open issue. Hub repulsion plays an
important role in fractal topologies. This paper models the repulsion among the
nodes in the complex networks in calculation of the fractal dimension of the
networks. The Coulomb's law is adopted to represent the repulse between two
nodes of the network quantitatively. A new method to calculate the fractal
dimension of complex networks is proposed. The Sierpinski triangle network and
some real complex networks are investigated. The results are illustrated to
show that the new model of self-similarity of complex networks is reasonable
and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0533</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0533</id><created>2014-04-02</created><authors><author><keyname>Kappes</keyname><forenames>J&#xf6;rg H.</forenames></author><author><keyname>Andres</keyname><forenames>Bjoern</forenames></author><author><keyname>Hamprecht</keyname><forenames>Fred A.</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Nowozin</keyname><forenames>Sebastian</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author><author><keyname>Kim</keyname><forenames>Sungwoong</forenames></author><author><keyname>Kausler</keyname><forenames>Bernhard X.</forenames></author><author><keyname>Kr&#xf6;ger</keyname><forenames>Thorben</forenames></author><author><keyname>Lellmann</keyname><forenames>Jan</forenames></author><author><keyname>Komodakis</keyname><forenames>Nikos</forenames></author><author><keyname>Savchynskyy</keyname><forenames>Bogdan</forenames></author><author><keyname>Rother</keyname><forenames>Carsten</forenames></author></authors><title>A Comparative Study of Modern Inference Techniques for Structured
  Discrete Energy Minimization Problems</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Szeliski et al. published an influential study in 2006 on energy minimization
methods for Markov Random Fields (MRF). This study provided valuable insights
in choosing the best optimization technique for certain classes of problems.
While these insights remain generally useful today, the phenomenal success of
random field models means that the kinds of inference problems that have to be
solved changed significantly. Specifically, the models today often include
higher order interactions, flexible connectivity structures, large
la\-bel-spaces of different cardinalities, or learned energy tables. To reflect
these changes, we provide a modernized and enlarged study. We present an
empirical comparison of 32 state-of-the-art optimization techniques on a corpus
of 2,453 energy minimization instances from diverse applications in computer
vision. To ensure reproducibility, we evaluate all methods in the OpenGM 2
framework and report extensive results regarding runtime and solution quality.
Key insights from our study agree with the results of Szeliski et al. for the
types of models they studied. However, on new and challenging types of models
our findings disagree and suggest that polyhedral methods and integer
programming solvers are competitive in terms of runtime and solution quality
over a large range of model types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0540</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0540</id><created>2014-04-02</created><authors><author><keyname>Gou</keyname><forenames>Li</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author><author><keyname>Sadiq</keyname><forenames>Rehan</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sankaran</forenames></author></authors><title>Modeling contaminant intrusion in water distribution networks based on D
  numbers</title><categories>cs.AI cs.CE</categories><comments>20 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient modeling on uncertain information plays an important role in
estimating the risk of contaminant intrusion in water distribution networks.
Dempster-Shafer evidence theory is one of the most commonly used methods.
However, the Dempster-Shafer evidence theory has some hypotheses including the
exclusive property of the elements in the frame of discernment, which may not
be consistent with the real world. In this paper, based on a more effective
representation of uncertainty, called D numbers, a new method that allows the
elements in the frame of discernment to be non-exclusive is proposed. To
demonstrate the efficiency of the proposed method, we apply it to the water
distribution networks to estimate the risk of contaminant intrusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0542</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0542</id><created>2014-04-02</created><authors><author><keyname>Rahwan</keyname><forenames>Talal</forenames></author><author><keyname>Naroditskiy</keyname><forenames>Victor</forenames></author><author><keyname>Michalak</keyname><forenames>Tomasz</forenames></author><author><keyname>Wooldridge</keyname><forenames>Michael</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R</forenames></author></authors><title>Towards a Fair Allocation of Rewards in Multi-Level Marketing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of businesses and organisations rely on existing users
for finding new users or spreading a message. One of the widely used
&quot;refer-a-friend&quot; mechanisms offers an equal reward to both the referrer and the
invitee. This mechanism provides incentives for direct referrals and is fair to
the invitee. On the other hand, multi-level marketing and recent social
mobilisation experiments focus on mechanisms that incentivise both direct and
indirect referrals. Such mechanisms share the reward for inviting a new member
among the ancestors, usually in geometrically decreasing shares. A new member
receives nothing at the time of joining. We study fairness in multi-level
marketing mechanisms. We show how characteristic function games can be used to
model referral marketing, show how the canonical fairness concept of the
Shapley value can be applied to this setting, and establish the complexity of
finding the Shapley value in each class, and provide a comparison of the
Shapley value-based mechanism to existing referral mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0545</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0545</id><created>2014-04-02</created><updated>2014-10-28</updated><authors><author><keyname>Given-Wilson</keyname><forenames>Thomas</forenames><affiliation>INRIA, Saclay</affiliation></author></authors><title>An Intensional Concurrent Faithful Encoding of Turing Machines</title><categories>cs.FL</categories><comments>In Proceedings ICE 2014, arXiv:1410.7013</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 166, 2014, pp. 21-37</journal-ref><doi>10.4204/EPTCS.166.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The benchmark for computation is typically given as Turing computability; the
ability for a computation to be performed by a Turing Machine. Many languages
exploit (indirect) encodings of Turing Machines to demonstrate their ability to
support arbitrary computation. However, these encodings are usually by
simulating the entire Turing Machine within the language, or by encoding a
language that does an encoding or simulation itself. This second category is
typical for process calculi that show an encoding of lambda-calculus (often
with restrictions) that in turn simulates a Turing Machine. Such approaches
lead to indirect encodings of Turing Machines that are complex, unclear, and
only weakly equivalent after computation. This paper presents an approach to
encoding Turing Machines into intensional process calculi that is faithful,
reduction preserving, and structurally equivalent. The encoding is demonstrated
in a simple asymmetric concurrent pattern calculus before generalised to
simplify infinite terms, and to show encodings into Concurrent Pattern Calculus
and Psi Calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0554</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0554</id><created>2014-04-02</created><authors><author><keyname>Werbos</keyname><forenames>Paul J</forenames></author></authors><title>From ADP to the Brain: Foundations, Roadmap, Challenges and Research
  Priorities</title><categories>cs.NE</categories><comments>5p, to appear in Proc. of the Interantional Joint Conference on
  Neural Networks 2014</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper defines and discusses Mouse Level Computational Intelligence
(MLCI) as a grand challenge for the coming century. It provides a specific
roadmap to reach that target, citing relevant work and review papers and
discussing the relation to funding priorities in two NSF funding activities:
the ongoing Energy, Power and Adaptive Systems program (EPAS) and the recent
initiative in Cognitive Optimization and Prediction (COPN). It elaborates on
the first step, vector intelligence, a challenge in the development of
universal learning systems, which itself will require considerable new research
to attain. This in turn is a crucial prerequisite to true functional
understanding of how mammal brains achieve such general learning capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0564</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0564</id><created>2014-04-02</created><authors><author><keyname>Sahraei</keyname><forenames>Saeid</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael C.</forenames></author></authors><title>New Shortest Lattice Vector Problems of Polynomial Complexity</title><categories>cs.DS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shortest Lattice Vector (SLV) problem is in general hard to solve, except
for special cases (such as root lattices and lattices for which an obtuse
superbase is known). In this paper, we present a new class of SLV problems that
can be solved efficiently. Specifically, if for an $n$-dimensional lattice, a
Gram matrix is known that can be written as the difference of a diagonal matrix
and a positive semidefinite matrix of rank $k$ (for some constant $k$), we show
that the SLV problem can be reduced to a $k$-dimensional optimization problem
with countably many candidate points. Moreover, we show that the number of
candidate points is bounded by a polynomial function of the ratio of the
smallest diagonal element and the smallest eigenvalue of the Gram matrix.
Hence, as long as this ratio is upper bounded by a polynomial function of $n$,
the corresponding SLV problem can be solved in polynomial complexity. Our
investigations are motivated by the emergence of such lattices in the field of
Network Information Theory. Further applications may exist in other areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0566</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0566</id><created>2014-02-17</created><authors><author><keyname>Chadzitaskos</keyname><forenames>Goce</forenames></author><author><keyname>H&#xe1;kov&#xe1;</keyname><forenames>Lenka</forenames></author><author><keyname>Kaj&#xed;nek</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>Weyl group orbit functions in image processing</title><categories>cs.CV</categories><comments>12 pages, 5 figures</comments><journal-ref>Applied Mathematics, Vol. 5 No. 3, 2014, pp. 501-511</journal-ref><doi>10.4236/am.2014.53049.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We deal with the Fourier-like analysis of functions on discrete grids in
two-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. For
these cases we present the convolution theorem. We provide an example of
application of image processing using the $C-$ functions and the convolutions
for spatial filtering of the treated image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0570</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0570</id><created>2014-04-02</created><updated>2014-08-14</updated><authors><author><keyname>Arthan</keyname><forenames>Rob</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author></authors><title>On Affine Logic and {\L}ukasiewicz Logic</title><categories>cs.LO math.LO</categories><comments>28 pages</comments><msc-class>03B47, 03B35, 03F03, 03F52</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-valued logic of {\L}ukasiewicz is a substructural logic that has
been widely studied and has many interesting properties. It is classical, in
the sense that it admits the axiom schema of double negation, [DNE]. However,
our understanding of {\L}ukasiewicz logic can be improved by separating its
classical and intuitionistic aspects. The intuitionistic aspect of
{\L}ukasiewicz logic is captured in an axiom schema, [CWC], which asserts the
commutativity of a weak form of conjunction. This is equivalent to a very
restricted form of contraction. We show how {\L}ukasiewicz Logic can be viewed
both as an extension of classical affine logic with [CWC], or as an extension
of what we call \emph{intuitionistic} {\L}ukasiewicz logic with [DNE],
intuitionistic {\L}ukasiewicz logic being the extension of intuitionistic
affine logic by the schema [CWC]. At first glance, intuitionistic affine logic
seems very weak, but, in fact, [CWC] is surprisingly powerful, implying results
such as intuitionistic analogues of De Morgan's laws. However the proofs can be
very intricate. We present these results using derived connectives to clarify
and motivate the proofs and give several applications. We give an analysis of
the applicability to these logics of the well-known methods that use negation
to translate classical logic into intuitionistic logic. The usual proofs of
correctness for these translations make much use of contraction. Nonetheless,
we show that all the usual negative translations are already correct for
intuitionistic {\L}ukasiewicz logic, where only the limited amount of
contraction given by [CWC] is allowed. This is in contrast with affine logic
for which we show, by appeal to results on semantics proved in a companion
paper, that both the Gentzen and the Glivenko translations fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0576</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0576</id><created>2014-04-02</created><updated>2014-10-01</updated><authors><author><keyname>de Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Postoyan</keyname><forenames>Romain</forenames></author></authors><title>A Lyapunov redesign of coordination algorithms for cyberphysical systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective is to design distributed coordination strategies for a network
of agents in a cyber-physical environment. In particular, we concentrate on the
rendez-vous of agents having double-integrator dynamics with the addition of a
damping term in the velocity dynamics. We start with distributed controllers
that solve the problem in continuous-time, and we then explain how to implement
these using event-based sampling. The idea is to define a triggering rule per
edge using a clock variable which only depends on the local variables. The
triggering laws are designed to compensate for the perturbative term introduced
by the sampling, a technique that reminds of Lyapunov-based control redesign.
We first present an event-triggered solution which requires continuous
measurement of the relative position and we then explain how to convert it to a
self-triggered policy. The latter only requires the measurements of the
relative position and velocity at the last transmission instants, which is
useful to reduce both the communication and the computation costs. The
strategies guarantee the existence of a uniform minimum amount of times between
any two edge events. The analysis is carried out using an invariance principle
for hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0578</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0578</id><created>2014-04-02</created><updated>2015-07-29</updated><authors><author><keyname>Ma</keyname><forenames>Xinpei</forenames></author><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author></authors><title>Mental Disorder Recovery Correlated with Centralities and Interactions
  on an Online Social Network</title><categories>cs.SI cs.CY</categories><comments>20 pages, 5 figures, 5 tables; accepted for publication in PeerJ</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has established both a theoretical basis and strong empirical
evidence that effective social behavior plays a beneficial role in the
maintenance of physical and psychological well-being of people. To test whether
social behavior and well-being are also associated in online communities, we
studied the correlations between the recovery of patients with mental disorders
and their behaviors in online social media. As the source of the data related
to the social behavior and progress of mental recovery, we used PatientsLikeMe
(PLM), the world's first open-participation research platform for the
development of patient-centered health outcome measures. We first constructed
an online social network structure based on patient-to-patient ties among 200
patients obtained from PLM. We then characterized patients' online social
activities by measuring the numbers of &quot;posts and views&quot; and &quot;helpful marks&quot;
each patient obtained. The patients' recovery data were obtained from their
self-reported status information that was also available on PLM. We found that
some node properties (in-degree, eigenvector centrality and PageRank) and the
two online social activity measures were significantly correlated with
patients' recovery. Furthermore, we re-collected the patients' recovery data
two months after the first data collection. We found significant correlations
between the patients' social behaviors and the second recovery data, which were
collected two months apart. Our results indicated that social interactions in
online communities such as PLM were significantly associated with the current
and future recoveries of patients with mental disorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0588</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0588</id><created>2014-04-02</created><authors><author><keyname>Adjiashvili</keyname><forenames>David</forenames></author><author><keyname>Rotbart</keyname><forenames>Noy</forenames></author></authors><title>Labeling Schemes for Bounded Degree Graphs</title><categories>cs.DM math.CO</categories><msc-class>05C78, 05C85</msc-class><acm-class>G.2.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate adjacency labeling schemes for graphs of bounded degree
$\Delta = O(1)$. In particular, we present an optimal (up to an additive
constant) $\log n + O(1)$ adjacency labeling scheme for bounded degree trees.
The latter scheme is derived from a labeling scheme for bounded degree
outerplanar graphs. Our results complement a similar bound recently obtained
for bounded depth trees [Fraigniaud and Korman, SODA 10], and may provide new
insights for closing the long standing gap for adjacency in trees [Alstrup and
Rauhe, FOCS 02]. We also provide improved labeling schemes for bounded degree
planar graphs. Finally, we use combinatorial number systems and present an
improved adjacency labeling schemes for graphs of bounded degree $\Delta$ with
$(e+1)\sqrt{n} &lt; \Delta \leq n/5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0600</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0600</id><created>2014-04-02</created><updated>2014-04-07</updated><authors><author><keyname>Esteban</keyname><forenames>Oscar</forenames></author><author><keyname>Wollny</keyname><forenames>Gert</forenames></author><author><keyname>Gorthi</keyname><forenames>Subrahmanyam</forenames></author><author><keyname>Ledesma-Carbayo</keyname><forenames>Maria-J.</forenames></author><author><keyname>Thiran</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Santos</keyname><forenames>Andres</forenames></author><author><keyname>Bach-Cuadra</keyname><forenames>Meritxell</forenames></author></authors><title>MBIS: Multivariate Bayesian Image Segmentation Tool</title><categories>cs.CV</categories><msc-class>62P10, 62F15</msc-class><journal-ref>Comput. Meth. Prog. Bio. 115(2):76-94 (2014)</journal-ref><doi>10.1016/j.cmpb.2014.03.003</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present MBIS (Multivariate Bayesian Image Segmentation tool), a clustering
tool based on the mixture of multivariate normal distributions model. MBIS
supports multi-channel bias field correction based on a B-spline model. A
second methodological novelty is the inclusion of graph-cuts optimization for
the stationary anisotropic hidden Markov random field model. Along with MBIS,
we release an evaluation framework that contains three different experiments on
multi-site data. We first validate the accuracy of segmentation and the
estimated bias field for each channel. MBIS outperforms a widely used
segmentation tool in a cross-comparison evaluation. The second experiment
demonstrates the robustness of results on atlas-free segmentation of two image
sets from scan-rescan protocols on 21 healthy subjects. Multivariate
segmentation is more replicable than the monospectral counterpart on
T1-weighted images. Finally, we provide a third experiment to illustrate how
MBIS can be used in a large-scale study of tissue volume change with increasing
age in 584 healthy subjects. This last result is meaningful as multivariate
segmentation performs robustly without the need for prior knowledge
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0605</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0605</id><created>2014-04-02</created><updated>2014-04-17</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>The Complexity of the Simplex Method</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simplex method is a well-studied and widely-used pivoting method for
solving linear programs. When Dantzig originally formulated the simplex method,
he gave a natural pivot rule that pivots into the basis a variable with the
most violated reduced cost. In their seminal work, Klee and Minty showed that
this pivot rule takes exponential time in the worst case. We prove two main
results on the simplex method. Firstly, we show that it is PSPACE-complete to
find the solution that is computed by the simplex method using Dantzig's pivot
rule. Secondly, we prove that deciding whether Dantzig's rule ever chooses a
specific variable to enter the basis is PSPACE-complete. We use the known
connection between Markov decision processes (MDPs) and linear programming, and
an equivalence between Dantzig's pivot rule and a natural variant of policy
iteration for average-reward MDPs. We construct MDPs and show
PSPACE-completeness results for single-switch policy iteration, which in turn
imply our main results for the simplex method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0606</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0606</id><created>2014-04-02</created><authors><author><keyname>Frochaux</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author></authors><title>Monadic Datalog Containment on Trees</title><categories>cs.LO cs.CC cs.DB</categories><comments>This article is the full version of an article published in the
  proccedings of the 8th Alberto Mendelzon Workshop (AMW 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the query containment problem for monadic datalog on finite
unranked labeled trees can be solved in 2-fold exponential time when (a)
considering unordered trees using the axes child and descendant, and when (b)
considering ordered trees using the axes firstchild, nextsibling, child, and
descendant. When omitting the descendant-axis, we obtain that in both cases the
problem is EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0607</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0607</id><created>2014-04-02</created><authors><author><keyname>Rahman</keyname><forenames>Mostafizur</forenames></author><author><keyname>Khasanvis</keyname><forenames>Santosh</forenames></author><author><keyname>Shi</keyname><forenames>Jiajun</forenames></author><author><keyname>Li</keyname><forenames>Mingyu</forenames></author><author><keyname>Moritz</keyname><forenames>Csaba Andras</forenames></author></authors><title>Skybridge: 3-D Integrated Circuit Technology Alternative to CMOS</title><categories>cs.ET cs.AR</categories><comments>53 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous scaling of CMOS has been the major catalyst in miniaturization of
integrated circuits (ICs) and crucial for global socio-economic progress.
However, scaling to sub-20nm technologies is proving to be challenging as
MOSFETs are reaching their fundamental limits and interconnection bottleneck is
dominating IC operational power and performance. Migrating to 3-D, as a way to
advance scaling, has eluded us due to inherent customization and manufacturing
requirements in CMOS that are incompatible with 3-D organization. Partial
attempts with die-die and layer-layer stacking have their own limitations. We
propose a 3-D IC fabric technology, Skybridge[TM], which offers paradigm shift
in technology scaling as well as design. We co-architect Skybridge's core
aspects, from device to circuit style, connectivity, thermal management, and
manufacturing pathway in a 3-D fabric-centric manner, building on a uniform 3-D
template. Our extensive bottom-up simulations, accounting for detailed material
system structures, manufacturing process, device, and circuit parasitics,
carried through for several designs including a designed microprocessor, reveal
a 30-60x density, 3.5x performance per watt benefits, and 10X reduction in
interconnect lengths vs. scaled 16-nm CMOS. Fabric-level heat extraction
features are shown to successfully manage IC thermal profiles in 3-D. Skybridge
can provide continuous scaling of integrated circuits beyond CMOS in the 21st
century.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0614</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0614</id><created>2014-04-02</created><updated>2014-07-07</updated><authors><author><keyname>Vardi</keyname><forenames>Shai</forenames></author></authors><title>The secretary returns</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the online random-arrival model, an algorithm receives a sequence of n
requests that arrive in a random order. The algorithm is expected to make an
irrevocable decision with regard to each request based only on the observed
history. We consider the following natural extension of this model: each
request arrives k times, and the arrival order is a random permutation of the
kn arrivals; the algorithm is expected to make a decision regarding each
request only upon its last arrival. We focus primarily on the case when k=2,
which can also be interpreted as each request arriving at, and departing from
the system, at a random time. We examine the secretary problem: the problem of
selecting the best secretary when the secretaries are presented online
according to a random permutation. We show that when each secretary arrives
twice, we can achieve a competitive ratio of ~0.768 (compared to 1/e in the
classical secretary problem), and that it is optimal. We also show that without
any knowledge about the number of secretaries or their arrival times, we can
still hire the best secretary with probability at least 2/3, in contrast to the
impossibility of achieving a constant success probability in the classical
setting. We extend our results to the matroid secretary problem, introduced by
Babaioff et al., (2007) and show a simple algorithm that achieves a
2-approximation to the maximal weighted basis in the new model (for k=2). We
show that this approximation factor can be improved in special cases of the
matroid secretary problem; in particular, we give a 16/9-competitive algorithm
for the returning edge-weighted bipartite matching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0615</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0615</id><created>2014-04-02</created><authors><author><keyname>Rahman</keyname><forenames>Mostafizur</forenames></author><author><keyname>Khasanvis</keyname><forenames>Santosh</forenames></author><author><keyname>Moritz</keyname><forenames>Csaba Andras</forenames></author></authors><title>Nanowire Volatile RAM as an Alternative to SRAM</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maintaining benefits of CMOS technology scaling is becoming challenging due
to increased manufacturing complexities and unwanted passive power
dissipations. This is particularly challenging in SRAM, where manufacturing
precision and leakage power control are critical issues. To alleviate some of
these challenges a novel non-volatile memory alternative to SRAM was proposed
called nanowire volatile RAM (NWRAM). Due to NWRAMs regular grid based layout
and innovative circuit style, manufacturing complexity is reduced and at the
same time considerable benefits are attained in terms of performance and
leakage power reduction. In this paper, we elaborate more on NWRAM circuit
aspects and manufacturability, and quantify benefits at 16nm technology node
through simulation against state-of-the-art 6T-SRAM and gridded 8T-SRAM
designs. Our results show the 10T-NWRAM to be 2x faster and 35x better in terms
of leakage when compared to high performance gridded 8T-SRAM design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0627</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0627</id><created>2014-04-02</created><authors><author><keyname>Javed</keyname><forenames>Mohammed</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Extraction of Projection Profile, Run-Histogram and Entropy Features
  Straight from Run-Length Compressed Text-Documents</title><categories>cs.CV</categories><comments>Published by IEEE in Proceedings of ACPR-2013. arXiv admin note: text
  overlap with arXiv:1403.7783</comments><journal-ref>2013 Second IAPR Asian Conference on Pattern Recognition, Pages
  813-817</journal-ref><doi>10.1109/ACPR.2013.147</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document Image Analysis, like any Digital Image Analysis requires
identification and extraction of proper features, which are generally extracted
from uncompressed images, though in reality images are made available in
compressed form for the reasons such as transmission and storage efficiency.
However, this implies that the compressed image should be decompressed, which
indents additional computing resources. This limitation induces the motivation
to research in extracting features directly from the compressed image. In this
research, we propose to extract essential features such as projection profile,
run-histogram and entropy for text document analysis directly from run-length
compressed text-documents. The experimentation illustrates that features are
extracted directly from the compressed image without going through the stage of
decompression, because of which the computing time is reduced. The feature
values so extracted are exactly identical to those extracted from uncompressed
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0640</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0640</id><created>2014-04-02</created><authors><author><keyname>Kazakci</keyname><forenames>Akin Osman</forenames><affiliation>CGS</affiliation></author></authors><title>Conceptive Artificial Intelligence: Insights from design theory</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Design Conference DESIGN2014, Croatia (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current paper offers a perspective on what we term conceptive
intelligence - the capacity of an agent to continuously think of new object
definitions (tasks, problems, physical systems, etc.) and to look for methods
to realize them. The framework, called a Brouwer machine, is inspired by
previous research in design theory and modeling, with its roots in the
constructivist mathematics of intuitionism. The dual constructivist perspective
we describe offers the possibility to create novelty both in terms of the types
of objects and the methods for constructing objects. More generally, the
theoretical work on which Brouwer machines are based is called imaginative
constructivism. Based on the framework and the theory, we discuss many
paradigms and techniques omnipresent in AI research and their merits and
shortcomings for modeling aspects of design, as described by imaginative
constructivism. To demonstrate and explain the type of creative process
expressed by the notion of a Brouwer machine, we compare this concept with a
system using genetic algorithms for scientific law discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0644</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0644</id><created>2014-04-02</created><updated>2014-09-05</updated><authors><author><keyname>Cassaigne</keyname><forenames>Julien</forenames></author><author><keyname>Halava</keyname><forenames>Vesa</forenames></author><author><keyname>Harju</keyname><forenames>Tero</forenames></author><author><keyname>Nicolas</keyname><forenames>Francois</forenames></author></authors><title>Tighter Undecidability Bounds for Matrix Mortality, Zero-in-the-Corner
  Problems, and More</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the decidability of three well-known problems related to integer
matrix multiplication: Mortality (M), Zero in the Left-Upper Corner (Z), and
Zero in the Right-Upper Corner (R).
  Let d and k be positive integers. Define M(k, d x d) as the following special
case of the Mortality problem: given a set X of d -by-d integer matrices such
that the cardinality of X is not greater than k, decide whether the d-by-d zero
matrix belongs to X^+, where X^+ denotes the closure of X under the usual
matrix multiplication. In the same way, define the Z(k, d x d) problem as:
given an instance X of M(k, d x d) (the instances of Z(k, d x d) are the same
as those of M(k, d x d)), decide whether at least one matrix in X^+ has a zero
in the left-upper corner. Define R(k, d x d) as the variant of Z(k, d x d)
where &quot;left-upper corner&quot; is replaced with &quot;right-upper corner&quot;.
  In the paper, we prove that M(6, 3 x 3), M(4, 5 x 5), M(3, 9 x 9), M(2, 15 x
15), Z(5, 3 x 3), Z(3, 5 x 5), Z(2, 9 x 9), R(6, 3 x 3), R(5, 4 x 4), and R(3,
6 x 6) are undecidable. The previous best comparable results were the
undecidabilities of M(7, 3 x 3), M(3, 13 x 13), M(2, 21 x 21), Z(7, 3 x 3),
Z(2, 13 x 13), R(7, 3 x 3), and R(2, 10 x 10).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0649</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0649</id><created>2014-03-30</created><authors><author><keyname>Cort&#xe9;s</keyname><forenames>Juan-Carlos</forenames></author><author><keyname>Santonja</keyname><forenames>Francisco-J.</forenames></author><author><keyname>Tarazona</keyname><forenames>Ana-C.</forenames></author><author><keyname>Villanueva</keyname><forenames>Rafael-J.</forenames></author><author><keyname>Villanueva-Oller</keyname><forenames>Javier</forenames></author></authors><title>A probabilistic estimation and prediction technique for dynamic
  continuous social science models: The evolution of the attitude of the Basque
  Country population towards ETA as a case study</title><categories>cs.LG</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a computational technique to deal with uncertainty
in dynamic continuous models in Social Sciences. Considering data from surveys,
the method consists of determining the probability distribution of the survey
output and this allows to sample data and fit the model to the sampled data
using a goodness-of-fit criterion based on the chi-square-test. Taking the
fitted parameters non-rejected by the chi-square-test, substituting them into
the model and computing their outputs, we build 95% confidence intervals in
each time instant capturing uncertainty of the survey data (probabilistic
estimation). Using the same set of obtained model parameters, we also provide a
prediction over the next few years with 95% confidence intervals (probabilistic
prediction). This technique is applied to a dynamic social model describing the
evolution of the attitude of the Basque Country population towards the
revolutionary organization ETA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0653</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0653</id><created>2014-04-02</created><updated>2015-02-23</updated><authors><author><keyname>Pak</keyname><forenames>Igor</forenames></author><author><keyname>Panova</keyname><forenames>Greta</forenames></author></authors><title>On the complexity of computing Kronecker coefficients</title><categories>math.CO cs.CC math.RT</categories><comments>v3: incorporated referee's comments; accepted to Computational
  Complexity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of computing Kronecker coefficients
$g(\lambda,\mu,\nu)$. We give explicit bounds in terms of the number of parts
$\ell$ in the partitions, their largest part size $N$ and the smallest second
part $M$ of the three partitions. When $M = O(1)$, i.e. one of the partitions
is hook-like, the bounds are linear in $\log N$, but depend exponentially on
$\ell$. Moreover, similar bounds hold even when $M=e^{O(\ell)}$. By a separate
argument, we show that the positivity of Kronecker coefficients can be decided
in $O(\log N)$ time for a bounded number $\ell$ of parts and without
restriction on $M$. Related problems of computing Kronecker coefficients when
one partition is a hook, and computing characters of $S_n$ are also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0654</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0654</id><created>2014-04-02</created><authors><author><keyname>Cohen</keyname><forenames>Gil</forenames></author><author><keyname>Tal</keyname><forenames>Avishay</forenames></author></authors><title>Two Structural Results for Low Degree Polynomials and Applications</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two structural results concerning low degree polynomials over
finite fields are given. The first states that over any finite field
$\mathbb{F}$, for any polynomial $f$ on $n$ variables with degree $d \le
\log(n)/10$, there exists a subspace of $\mathbb{F}^n$ with dimension $\Omega(d
\cdot n^{1/(d-1)})$ on which $f$ is constant. This result is shown to be tight.
Stated differently, a degree $d$ polynomial cannot compute an affine disperser
for dimension smaller than $\Omega(d \cdot n^{1/(d-1)})$. Using a recursive
argument, we obtain our second structural result, showing that any degree $d$
polynomial $f$ induces a partition of $F^n$ to affine subspaces of dimension
$\Omega(n^{1/(d-1)!})$, such that $f$ is constant on each part.
  We extend both structural results to more than one polynomial. We further
prove an analog of the first structural result to sparse polynomials (with no
restriction on the degree) and to functions that are close to low degree
polynomials. We also consider the algorithmic aspect of the two structural
results.
  Our structural results have various applications, two of which are:
  * Dvir [CC 2012] introduced the notion of extractors for varieties, and gave
explicit constructions of such extractors over large fields. We show that over
any finite field, any affine extractor is also an extractor for varieties with
related parameters. Our reduction also holds for dispersers, and we conclude
that Shaltiel's affine disperser [FOCS 2011] is a disperser for varieties over
$F_2$.
  * Ben-Sasson and Kopparty [SIAM J. C 2012] proved that any degree 3 affine
disperser over a prime field is also an affine extractor with related
parameters. Using our structural results, and based on the work of Kaufman and
Lovett [FOCS 2008] and Haramaty and Shpilka [STOC 2010], we generalize this
result to any constant degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0660</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0660</id><created>2014-04-02</created><updated>2014-04-28</updated><authors><author><keyname>Hammerich</keyname><forenames>Edwin</forenames></author></authors><title>Waterfilling Theorems in the Time-Frequency Plane for the Heat Channel
  and a Related Source</title><categories>cs.IT math.IT</categories><comments>5 pages; to be presented at 2014 IEEE International Symposium on
  Information Theory, Honolulu, HI, USA, June/July 2014</comments><journal-ref>Proc. IEEE Int. Symp. Information Theory, Honolulu, HI, 2014, pp.
  2416-2420</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of the heat channel, a linear time-varying (LTV) filter with
additive white Gaussian noise (AWGN), is characterized by waterfilling in the
time-frequency plane. Similarly, the rate distortion function for a related
nonstationary source is characterized by reverse waterfilling in the
time-frequency plane. The source is formed by the white Gaussian noise response
of the same LTV filter as before. The proofs of both waterfilling theorems rely
on a specific Szego theorem for a positive definite operator associated with
the filter. An essentially self-contained proof of the Szego theorem is given.
The waterfilling theorems compare well with classical results of Gallager and
Berger. In case of the nonstationary source it is observed that the part of the
classical power spectral density (PSD) is taken by the Wigner-Ville spectrum
(WVS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0662</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0662</id><created>2014-04-02</created><authors><author><keyname>Cho</keyname><forenames>YounSun</forenames></author></authors><title>Privacy-Preserving Social Network with Multigrained and Multilevel
  Access Control</title><categories>cs.SI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I study two privacy-preserving social network graphs to dis- close the types
of relationships of connecting edges and provide flexible multigrained access
control. To create such graphs, my schemes employ the concept of secretaries
and types of relationships. It is significantly more efficient than those that
using expensive cryptographic primitives. I also show how these schemes can be
used for multigrained access control with various options. In addition, I
describe how much these schemes are resilient to infer the types of connecting
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0665</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0665</id><created>2014-03-16</created><updated>2015-07-13</updated><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>Entanglement in Quantum Process Algebra</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.2960</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explicitly model entanglement in quantum processes by treating
entanglement as a kind of parallelism. We introduce a shadow constant quantum
operation and a so-called entanglement merge into quantum process algebra qACP.
The transition rules of the shadow constant quantum operation and entanglement
merge are designed. We also do a sound and complete axiomatization modulo the
so-called quantum bisimularity for the shadow constant quantum operation and
entanglement merge. Then, this new type entanglement merge is extended into the
full qACP. The new qACP has wide use in verification for quantum protocols,
since most quantum protocols have mixtures with classical and quantum
information, and also there are many quantum protocols adopting entanglement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0672</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0672</id><created>2014-04-02</created><authors><author><keyname>Mendes</keyname><forenames>Hammurabi</forenames></author><author><keyname>Istrail</keyname><forenames>Sorin</forenames></author></authors><title>Thermodynamic Hypothesis as Social Choice: An Impossibility Theorem for
  Protein Folding</title><categories>cs.CE cs.GT</categories><comments>Submitted for peer review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein Folding is concerned with the reasons and mechanism behind a
protein's tertiary structure. The thermodynamic hypothesis of Anfinsen
postulates an universal energy function (UEF) characterizing the tertiary
structure, defined consistently across proteins, in terms of their aminoacid
sequence.
  We consider the approach of examining multiple protein structure descriptors
in the PDB (Protein Data Bank), and infer individual preferences, biases
favoring particular classes of aminoacid interactions in each of them, later
aggregating these individual preferences into a global preference. This 2-step
process would ideally expose intrinsic biases on classes of aminoacid
interactions in the UEF itself. The intuition is that any intrinsic biases in
the UEF are expressed within each protein in a specific manner consistent with
its specific aminoacid sequence, size, and fold (consistently with Anfinsen's
thermodynamic hypothesis), making a 1-step, holistic aggregation less
desirable.
  Our intention is to illustrate how some impossibility results from voting
theory would apply in this setting, being possibly applicable to other protein
folding problems as well. We consider concepts and results from voting theory
and unveil methodological difficulties for the approach mentioned above. With
our observations, we intend to highlight how key theoretical barriers, already
exposed by economists, can be relevant for the development of new methods, new
algorithms, for problems related to protein folding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0695</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0695</id><created>2014-04-02</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Karamanoglu</keyname><forenames>M.</forenames></author><author><keyname>He</keyname><forenames>Xingshi</forenames></author></authors><title>Multi-objective Flower Algorithm for Optimization</title><categories>cs.NE math.OC</categories><comments>2 figures. arXiv admin note: substantial text overlap with
  arXiv:1312.5673</comments><msc-class>90C26</msc-class><journal-ref>X. S. Yang, M. Karamanoglu, X. S. He, Multi-objective Flower
  Algorithm for Optimization, Procedia Computer Science, vol. 18, pp. 861-868
  (2013)</journal-ref><doi>10.1016/j.procs.2013.05.251</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flower pollination algorithm is a new nature-inspired algorithm, based on the
characteristics of flowering plants. In this paper, we extend this flower
algorithm to solve multi-objective optimization problems in engineering. By
using the weighted sum method with random weights, we show that the proposed
multi-objective flower algorithm can accurately find the Pareto fronts for a
set of test functions. We then solve a bi-objective disc brake design problem,
which indeed converges quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0696</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0696</id><created>2014-04-02</created><authors><author><keyname>Sioutas</keyname><forenames>S.</forenames></author><author><keyname>Sakkopoulos</keyname><forenames>E.</forenames></author><author><keyname>Panaretos</keyname><forenames>A.</forenames></author><author><keyname>Tsoumakos</keyname><forenames>D.</forenames></author><author><keyname>Gerolymatos</keyname><forenames>P.</forenames></author><author><keyname>Tzimas</keyname><forenames>G.</forenames></author><author><keyname>Manolopoulos</keyname><forenames>Y.</forenames></author></authors><title>D-P2P-Sim+:A Novel Distributed Framework for P2P Protocols Performance
  Testing</title><categories>cs.DB</categories><comments>51 pages, 37 figures, submitted to JSS (Elsevier)</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent IoT (Internet of Things) and Web 2.0 technologies, a critical
problem arises with respect to storing and processing the large amount of
collected data. In this paper we develop and evaluate distributed
infrastructures for storing and processing large amount of such data. We
present a distributed framework that supports customized deployment of a
variety of indexing engines over million-node overlays. The proposed framework
provides the appropriate integrated set of tools that allows applications
processing large amount of data, to evaluate and test the performance of
various application protocols for very large scale deployments (multi million
nodes - billions of keys). The key aim is to provide the appropriate
environment that contributes in taking decisions regarding the choice of the
protocol in storage P2P systems for a variety of big data applications. Using
lightweight and efficient collection mechanisms, our system enables real-time
registration of multiple measures, integrating support for real-life parameters
such as node failure models and recovery strategies. Experiments have been
performed at the PlanetLab network and at a typical research laboratory in
order to verify scalability and show maximum re-usability of our setup.
D-P2P-Sim+ framework is publicly available at
http://code.google.com/p/d-p2p-sim/downloads/list.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0698</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0698</id><created>2014-04-02</created><authors><author><keyname>Merelli</keyname><forenames>Emanuela</forenames></author><author><keyname>Paoletti</keyname><forenames>Nicola</forenames></author><author><keyname>Tesei</keyname><forenames>Luca</forenames></author></authors><title>Adaptability Checking in Multi-Level Complex Systems</title><categories>cs.SE</categories><comments>57 page, 10 figures, research papaer, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hierarchical model for multi-level adaptive systems is built on two basic
levels: a lower behavioural level B accounting for the actual behaviour of the
system and an upper structural level S describing the adaptation dynamics of
the system. The behavioural level is modelled as a state machine and the
structural level as a higher-order system whose states have associated logical
formulas (constraints) over observables of the behavioural level. S is used to
capture the global and stable features of B, by a defining set of allowed
behaviours. The adaptation semantics is such that the upper S level imposes
constraints on the lower B level, which has to adapt whenever it no longer can
satisfy them. In this context, we introduce weak and strong adaptabil- ity,
i.e. the ability of a system to adapt for some evolution paths or for all
possible evolutions, respectively. We provide a relational characterisation for
these two notions and we show that adaptability checking, i.e. deciding if a
system is weak or strong adaptable, can be reduced to a CTL model checking
problem. We apply the model and the theoretical results to the case study of
motion control of autonomous transport vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0703</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0703</id><created>2014-04-02</created><updated>2015-12-22</updated><authors><author><keyname>Khamis</keyname><forenames>Mahmoud Abo</forenames></author><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>Joins via Geometric Resolutions: Worst-case and Beyond</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple geometric framework for the relational join. Using this
framework, we design an algorithm that achieves the fractional hypertree-width
bound, which generalizes classical and recent worst-case algorithmic results on
computing joins. In addition, we use our framework and the same algorithm to
show a series of what are colloquially known as beyond worst-case results. The
framework allows us to prove results for data stored in Btrees,
multidimensional data structures, and even multiple indices per table. A key
idea in our framework is formalizing the inference one does with an index as a
type of geometric resolution; transforming the algorithmic problem of computing
joins to a geometric problem. Our notion of geometric resolution can be viewed
as a geometric analog of logical resolution. In addition to the geometry and
logic connections, our algorithm can also be thought of as backtracking search
with memoization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0708</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0708</id><created>2014-04-02</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Koziel</keyname><forenames>Slawomir</forenames></author><author><keyname>Leifsson</keyname><forenames>Leifur</forenames></author></authors><title>Computational Optimization, Modelling and Simulation: Recent Trends and
  Challenges</title><categories>cs.NE math.OC</categories><msc-class>90C26</msc-class><journal-ref>X. S. Yang, S. Koziel, L. Leifsson, Computational Optimization,
  Modelling and Simulation: Recent Trends and Challenges, Procedia Computer
  Science, vol. 18, pp. 855-860 (2013)</journal-ref><doi>10.1016/j.procs.2013.05.250</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling, simulation and optimization form an integrated part of modern
design practice in engineering and industry. Tremendous progress has been
observed for all three components over the last few decades. However, many
challenging issues remain unresolved, and the current trends tend to use
nature-inspired algorithms and surrogate-based techniques for modelling and
optimization. This 4th workshop on Computational Optimization, Modelling and
Simulation (COMS 2013) at ICCS 2013 will further summarize the latest
developments of optimization and modelling and their applications in science,
engineering and industry. In this review paper, we will analyse the recent
trends in modelling and optimization, and their associated challenges. We will
discuss important topics for further research, including parameter-tuning,
large-scale problems, and the gaps between theory and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0718</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0718</id><created>2014-04-02</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Dixit</keyname><forenames>Kashyap</forenames></author><author><keyname>Jha</keyname><forenames>Madhav</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Property Testing on Product Distributions: Optimal Testers for Bounded
  Derivative Properties</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary problem in property testing is to decide whether a given function
satisfies a certain property, or is far from any function satisfying it. This
crucially requires a notion of distance between functions. The most prevalent
notion is the Hamming distance over the {\em uniform} distribution on the
domain. This restriction to uniformity is more a matter of convenience than of
necessity, and it is important to investigate distances induced by more general
distributions.
  In this paper, we make significant strides in this direction. We give simple
and optimal testers for {\em bounded derivative properties} over {\em arbitrary
product distributions}. Bounded derivative properties include fundamental
properties such as monotonicity and Lipschitz continuity. Our results subsume
almost all known results (upper and lower bounds) on monotonicity and Lipschitz
testing.
  We prove an intimate connection between bounded derivative property testing
and binary search trees (BSTs). We exhibit a tester whose query complexity is
the sum of expected depths of optimal BSTs for each marginal. Furthermore, we
show this sum-of-depths is also a lower bound. A fundamental technical
contribution of this work is an {\em optimal dimension reduction theorem} for
all bounded derivative properties, which relates the distance of a function
from the property to the distance of restrictions of the function to random
lines. Such a theorem has been elusive even for monotonicity for the past 15
years, and our theorem is an exponential improvement to the previous best known
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0736</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0736</id><created>2014-04-02</created><updated>2014-06-09</updated><authors><author><keyname>Denton</keyname><forenames>Emily</forenames></author><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Exploiting Linear Structure Within Convolutional Networks for Efficient
  Evaluation</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present techniques for speeding up the test-time evaluation of large
convolutional networks, designed for object recognition tasks. These models
deliver impressive accuracy but each image evaluation requires millions of
floating point operations, making their deployment on smartphones and
Internet-scale clusters problematic. The computation is dominated by the
convolution operations in the lower layers of the model. We exploit the linear
structure present within the convolutional filters to derive approximations
that significantly reduce the required computation. Using large
state-of-the-art models, we demonstrate we demonstrate speedups of
convolutional layers on both CPU and GPU by a factor of 2x, while keeping the
accuracy within 1% of the original model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0743</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0743</id><created>2014-04-02</created><updated>2014-04-03</updated><authors><author><keyname>Irving</keyname><forenames>Geoffrey</forenames></author></authors><title>Pentago is a First Player Win: Strongly Solving a Game Using Parallel
  In-Core Retrograde Analysis</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a strong solution of the board game pentago, computed using
exhaustive parallel retrograde analysis in 4 hours on 98304 ($3 \times 2^{15}$)
threads of NERSC's Cray Edison. At $3.0 \times 10^{15}$ states, pentago is the
largest divergent game solved to date by two orders of magnitude, and the only
example of a nontrivial divergent game solved using retrograde analysis. Unlike
previous retrograde analyses, our computation was performed entirely in-core,
writing only a small portion of the results to disk; an out-of-core
implementation would have been much slower. Symmetry was used to reduce
branching factor and exploit instruction level parallelism. Despite a
theoretically embarrassingly parallel structure, asynchronous message passing
was required to fit the computation into available RAM, causing latency
problems on an older Cray machine. All code and data for the project are open
source, together with a website which combines database lookup and on-the-fly
computation to interactively explore the strong solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0751</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0751</id><created>2014-04-02</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Azizyan</keyname><forenames>Martin</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Subspace Learning from Extremely Compressed Measurements</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning the principal subspace of a large set of vectors from an
extremely small number of compressive measurements of each vector. Our
theoretical results show that even a constant number of measurements per column
suffices to approximate the principal subspace to arbitrary precision, provided
that the number of vectors is large. This result is achieved by a simple
algorithm that computes the eigenvectors of an estimate of the covariance
matrix. The main insight is to exploit an averaging effect that arises from
applying a different random projection to each vector. We provide a number of
simulations confirming our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0753</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0753</id><created>2014-04-02</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Sorkin</keyname><forenames>Gregory B.</forenames></author></authors><title>Separate, Measure and Conquer: Faster Algorithms for Max 2-CSP and
  Counting Dominating Sets</title><categories>cs.DS cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a method resulting in the improvement of several polynomial-space,
exponential-time algorithms.
  An instance of the problem Max (r,2)-CSP, or simply Max 2-CSP, is
parametrized by the domain size r (often 2), the number of variables n
(vertices in the constraint graph G), and the number of constraints m (edges in
G). When G is cubic, and omitting sub-exponential terms here for clarity, we
give an algorithm running in time r^((1/5)n) = r^((2/15)m); the previous best
was r^((1/4)n) = r^((1/6)m). By known results, this improvement for the cubic
case results in an algorithm running in time r^((9/50)m) for general instances;
the previous best was r^((19/100)m). We show that the analysis of the earlier
algorithm was tight: our improvement is in the algorithm, not just the
analysis. The new algorithm, like the old, extends to Polynomial and Ring CSP.
  We also give faster algorithms for #Dominating Set, counting the dominating
sets of every cardinality 0,...,n for a graph G of order n. For cubic graphs,
our algorithm runs in time 3^((1/6)n); the previous best was 2^((1/2)n). For
general graphs, we give an unrelated algorithm running in time 1.5183^n; the
previous best was 1.5673^n.
  The previous best algorithms for these problems all used local
transformations and were analyzed by the &quot;Measure and Conquer&quot; method. Our new
algorithms capitalize on the existence of small balanced separators for cubic
graphs - a non-local property - and the ability to tailor the local algorithms
always to &quot;pivot&quot; on a vertex in the separator. The new algorithms perform much
as the old ones until the separator is empty, at which point they gain because
the remaining vertices are split into two independent problem instances that
can be solved recursively. It is likely that such algorithms can be effective
for other problems too, and we present their design and analysis in a general
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0760</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0760</id><created>2014-04-03</created><updated>2014-04-30</updated><authors><author><keyname>Wechsler</keyname><forenames>Bertrand</forenames></author><author><keyname>Eilat</keyname><forenames>Dan</forenames></author><author><keyname>Limal</keyname><forenames>Nicolas</forenames></author></authors><title>Information Flow Decomposition in Feedback Systems: General Case Study</title><categories>cs.IT math.IT</categories><comments>10 pages, 1 figure. Seminar Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive three fundamental decompositions on relevant information quantities
in feedback systems. The feedback systems considered in this paper are only
restricted to be causal in time domain and the channels are allowed to be
subject to arbitrary distribution. These decompositions comprise the well-known
mutual information and the directed information, and indicate a law of
conservation of information flows in the closed-loop network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0766</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0766</id><created>2014-04-03</created><updated>2016-03-04</updated><authors><author><keyname>Ghosh</keyname><forenames>Mrinalkanti</forenames></author><author><keyname>Nandakumar</keyname><forenames>Satyadev</forenames></author><author><keyname>Pal</keyname><forenames>Atanu</forenames></author></authors><title>Ornstein Isomorphism and Algorithmic Randomness</title><categories>cs.IT math.IT</categories><msc-class>37A35, 03D32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1970, Donald Ornstein proved a landmark result in dynamical systems, viz.,
two Bernoulli systems with the same entropy are isomorphic except for a measure
0 set. Keane and Smorodinsky gave a finitary proof of this result. They also
indicated how one can generalize the result to mixing Markov Shifts. We adapt
their construction to show that if two computable mixing Markov systems have
the same entropy, then there is a layerwise computable isomorphism defined on
all Martin-Lof random points in the system. Since the set of Martin-Lof random
points forms a measure 1 set, it implies the classical result for such systems.
  This result uses several recent developments in computable analysis and
algorithmic randomness. Following the work by Braverman, Nandakumar, and Hoyrup
and Rojas introduced discontinuous functions into the study of algorithmic
randomness. We utilize Hoyrup and Rojas' elegant notion of layerwise computable
functions to produce the test of randomness in our result. Further, we use the
recent result of the effective Shannon-McMillan-Breiman theorem, independently
established by Hochman and Hoyrup to prove the properties of our construction.
  We show that the result cannot be improved to include all points in the
systems - only trivial computable isomorphisms exist between systems with the
same entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0774</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0774</id><created>2014-04-03</created><authors><author><keyname>Haque</keyname><forenames>Md. Enamul</forenames></author><author><keyname>Kaisan</keyname><forenames>Abdullah Al</forenames></author><author><keyname>Saniat</keyname><forenames>Mahmudur R</forenames></author><author><keyname>Rahman</keyname><forenames>Aminur</forenames></author></authors><title>GPU Accelerated Fractal Image Compression for Medical Imaging in
  Parallel Computing Platform</title><categories>cs.DC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we implemented both sequential and parallel version of fractal
image compression algorithms using CUDA (Compute Unified Device Architecture)
programming model for parallelizing the program in Graphics Processing Unit for
medical images, as they are highly similar within the image itself. There are
several improvement in the implementation of the algorithm as well. Fractal
image compression is based on the self similarity of an image, meaning an image
having similarity in majority of the regions. We take this opportunity to
implement the compression algorithm and monitor the effect of it using both
parallel and sequential implementation. Fractal compression has the property of
high compression rate and the dimensionless scheme. Compression scheme for
fractal image is of two kind, one is encoding and another is decoding. Encoding
is very much computational expensive. On the other hand decoding is less
computational. The application of fractal compression to medical images would
allow obtaining much higher compression ratios. While the fractal magnification
an inseparable feature of the fractal compression would be very useful in
presenting the reconstructed image in a highly readable form. However, like all
irreversible methods, the fractal compression is connected with the problem of
information loss, which is especially troublesome in the medical imaging. A
very time consuming encoding pro- cess, which can last even several hours, is
another bothersome drawback of the fractal compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0780</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0780</id><created>2014-04-03</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Khabbazian</keyname><forenames>Majid</forenames></author></authors><title>Randomized Broadcast in Radio Networks with Collision Detection</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized distributed algorithm that in radio networks with
collision detection broadcasts a single message in $O(D + \log^6 n)$ rounds,
with high probability. This time complexity is most interesting because of its
optimal additive dependence on the network diameter $D$. It improves over the
currently best known $O(D\log\frac{n}{D}\,+\,\log^2 n)$ algorithms, due to
Czumaj and Rytter [FOCS 2003], and Kowalski and Pelc [PODC 2003]. These
algorithms where designed for the model without collision detection and are
optimal in that model. However, as explicitly stated by Peleg in his 2007
survey on broadcast in radio networks, it had remained an open question whether
the bound can be improved with collision detection.
  We also study distributed algorithms for broadcasting $k$ messages from a
single source to all nodes. This problem is a natural and important
generalization of the single-message broadcast problem, but is in fact
considerably more challenging and less understood. We show the following
results: If the network topology is known to all nodes, then a $k$-message
broadcast can be performed in $O(D + k\log n + \log^2 n)$ rounds, with high
probability. If the topology is not known, but collision detection is
available, then a $k$-message broadcast can be performed in $O(D + k\log n +
\log^6 n)$ rounds, with high probability. The first bound is optimal and the
second is optimal modulo the additive $O(\log^6 n)$ term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0781</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0781</id><created>2014-04-03</created><authors><author><keyname>Bakeva</keyname><forenames>Verica</forenames></author><author><keyname>Popovska-Mitrovikj</keyname><forenames>Aleksandra</forenames></author><author><keyname>Dimitrova</keyname><forenames>Vesna</forenames></author></authors><title>Resistance of Statistical Attacks of Parastrophic Quasigroup
  Transformation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove an important cryptographic property of
$PE$-transformation proposed elsewhere. If $PE$-transformation is used as
encrypting function then after $n$ applications of it on arbitrary message the
distribution of $l$-tuples ($l=1,2,\dots, n$) is uniform. This property implies
the resistance of statistical kind of attack of this transformation. For
illustration of theoretical results, some experimental results are presented as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0783</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0783</id><created>2014-04-03</created><authors><author><keyname>Evrendilek</keyname><forenames>Cem</forenames></author><author><keyname>Toroslu</keyname><forenames>Ismail Hakki</forenames></author><author><keyname>Hashemi</keyname><forenames>Sasan</forenames></author></authors><title>Task Assignment in Tree-Like Hierarchical Structures</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most large organizations, such as corporations, are hierarchical
organizations. In hierarchical organizations each entity in the organization,
except the root entity, is a sub-part of another entity. In this paper we study
the task assignment problem to the entities of tree-like hierarchical
organizations. The inherent tree structure introduces an interesting and
challenging constraint to the standard assignment problem. When a task is
assigned to an entity in a hierarchical organization, the whole entity,
including its sub-entities, is responsible from the execution of that
particular task. In other words, if an entity has been assigned to a task,
neither its descendants nor its ancestors can be assigned to a task.
Sub-entities cannot be assigned as they have an ancestor already occupied.
Ancestor entities cannot be assigned since one of their sub-entities has
already been employed in an assignment. In the paper, we formally introduce
this new version of the assignment problem called Maximum Weight Tree Matching
($MWTM$), and show its NP-hardness. We also propose an effective heuristic
solution based on an iterative LP-relaxation to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0789</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0789</id><created>2014-04-03</created><updated>2014-04-17</updated><authors><author><keyname>Stiffelman</keyname><forenames>Oscar</forenames></author></authors><title>The Least Wrong Model Is Not in the Data</title><categories>cs.LG</categories><comments>added citations and acknowledgements, and replaced the ideal model
  section with a more intuitive argument</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The true process that generated data cannot be determined when multiple
explanations are possible. Prediction requires a model of the probability that
a process, chosen randomly from the set of candidate explanations, generates
some future observation. The best model includes all of the information
contained in the minimal description of the data that is not contained in the
data. It is closely related to the Halting Problem and is logarithmic in the
size of the data. Prediction is difficult because the ideal model is not
computable, and the best computable model is not &quot;findable.&quot; However, the error
from any approximation can be bounded by the size of the description using the
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0798</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0798</id><created>2014-04-03</created><authors><author><keyname>Marsault</keyname><forenames>Victor</forenames></author><author><keyname>Sakarovitch</keyname><forenames>Jacques</forenames></author></authors><title>Breadth-first serialisation of trees and rational languages</title><categories>cs.FL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here the notion of breadth-first signature and its relationship
with numeration system theory. It is the serialisation into an infinite word of
an ordered infinite tree of finite degree. We study which class of languages
corresponds to which class of words and,more specifically, using a known
construction from numeration system theory, we prove that the signature of
rational languages are substitutive sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0799</identifier>
 <datestamp>2014-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0799</id><created>2014-04-03</created><updated>2014-05-30</updated><authors><author><keyname>Gr&#xf8;nlund</keyname><forenames>Allan</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author></authors><title>Threesomes, Degenerates, and Love Triangles</title><categories>cs.DS cs.CC cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3SUM problem is to decide, given a set of $n$ real numbers, whether any
three sum to zero. It is widely conjectured that a trivial $O(n^2)$-time
algorithm is optimal and over the years the consequences of this conjecture
have been revealed. This 3SUM conjecture implies $\Omega(n^2)$ lower bounds on
numerous problems in computational geometry and a variant of the conjecture
implies strong lower bounds on triangle enumeration, dynamic graph algorithms,
and string matching data structures.
  In this paper we refute the 3SUM conjecture. We prove that the decision tree
complexity of 3SUM is $O(n^{3/2}\sqrt{\log n})$ and give two subquadratic 3SUM
algorithms, a deterministic one running in $O(n^2 / (\log n/\log\log n)^{2/3})$
time and a randomized one running in $O(n^2 (\log\log n)^2 / \log n)$ time with
high probability. Our results lead directly to improved bounds for $k$-variate
linear degeneracy testing for all odd $k\ge 3$. The problem is to decide, given
a linear function $f(x_1,\ldots,x_k) = \alpha_0 + \sum_{1\le i\le k} \alpha_i
x_i$ and a set $A \subset \mathbb{R}$, whether $0\in f(A^k)$. We show the
decision tree complexity of this problem is $O(n^{k/2}\sqrt{\log n})$.
  Finally, we give a subcubic algorithm for a generalization of the
$(\min,+)$-product over real-valued matrices and apply it to the problem of
finding zero-weight triangles in weighted graphs. We give a
depth-$O(n^{5/2}\sqrt{\log n})$ decision tree for this problem, as well as an
algorithm running in time $O(n^3 (\log\log n)^2/\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0807</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0807</id><created>2014-04-03</created><updated>2016-02-22</updated><authors><author><keyname>Anglano</keyname><forenames>Cosimo</forenames></author><author><keyname>Guazzone</keyname><forenames>Marco</forenames></author><author><keyname>Sereno</keyname><forenames>Matteo</forenames></author></authors><title>Maximizing Profit in Green Cellular Networks through Collaborative Games</title><categories>cs.GT cs.NI</categories><comments>Added publisher info and citation notice</comments><journal-ref>Computer Networks Volume 75, Part A, 2014, Pages 260-275</journal-ref><doi>10.1016/j.comnet.2014.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the problem of maximizing the profit of Network
Operators (NOs) of green cellular networks in situations where
Quality-of-Service (QoS) guarantees must be ensured to users, and Base Stations
(BSs) can be shared among different operators. We show that if NOs cooperate
among them, by mutually sharing their users and BSs, then each one of them can
improve its net profit. By using a game-theoretic framework, we study the
problem of forming stable coalitions among NOs. Furthermore, we propose a
mathematical optimization model to allocate users to a set of BSs, in order to
reduce costs and, at the same time, to meet user QoS for NOs inside the same
coalition. Based on this, we propose an algorithm, based on cooperative game
theory, that enables each operator to decide with whom to cooperate in order to
maximize its profit. This algorithms adopts a distributed approach in which
each NO autonomously makes its own decisions, and where the best solution
arises without the need to synchronize them or to resort to a trusted third
party. The effectiveness of the proposed algorithm is demonstrated through a
thorough experimental evaluation considering real-world traffic traces, and a
set of realistic scenarios. The results we obtain indicate that our algorithm
allows a population of NOs to significantly improve their profits thanks to the
combination of energy reduction and satisfaction of QoS requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0812</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0812</id><created>2014-04-03</created><authors><author><keyname>Shankar</keyname><forenames>Varun</forenames></author><author><keyname>Wright</keyname><forenames>Grady B.</forenames></author><author><keyname>Kirby</keyname><forenames>Robert M.</forenames></author><author><keyname>Fogelson</keyname><forenames>Aaron L.</forenames></author></authors><title>A Radial Basis Function (RBF)-Finite Difference (FD) Method for
  Diffusion and Reaction-Diffusion Equations on Surfaces</title><categories>math.NA cs.NA</categories><comments>29 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a method based on Radial Basis Function
(RBF)-generated Finite Differences (FD) for numerically solving diffusion and
reaction-diffusion equations (PDEs) on closed surfaces embedded in
$\mathbb{R}^d$. Our method uses a method-of-lines formulation, in which surface
derivatives that appear in the PDEs are approximated locally using RBF
interpolation. The method requires only scattered nodes representing the
surface and normal vectors at those scattered nodes. All computations use only
extrinsic coordinates, thereby avoiding coordinate distortions and
singularities. We also present an optimization procedure that allows for the
stabilization of the discrete differential operators generated by our RBF-FD
method by selecting shape parameters for each stencil that correspond to a
global target condition number. We show the convergence of our method on two
surfaces for different stencil sizes, and present applications to nonlinear
PDEs simulated both on implicit/parametric surfaces and more general surfaces
represented by point clouds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0816</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0816</id><created>2014-04-03</created><updated>2014-10-16</updated><authors><author><keyname>Arthan</keyname><forenames>Rob</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author></authors><title>On Pocrims and Hoops</title><categories>math.LO cs.LO</categories><comments>37 pages</comments><msc-class>03G25, 06D35, 03B47</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pocrims and suitable specialisations thereof are structures that provide the
natural algebraic semantics for a minimal affine logic and its extensions.
Hoops comprise a special class of pocrims that provide algebraic semantics for
what we view as an intuitionistic analogue of the classical multi-valued
{\L}ukasiewicz logic. We present some contributions to the theory of these
algebraic structures. We give a new proof that the class of hoops is a variety.
We use a new indirect method to establish several important identities in the
theory of hoops: in particular, we prove that the double negation mapping in a
hoop is a homormorphism. This leads to an investigation of algebraic analogues
of the various double negation translations that are well-known from proof
theory. We give an algebraic framework for studying the semantics of double
negation translations and use it to prove new results about the applicability
of the double negation translations due to Gentzen and Glivenko.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0818</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0818</id><created>2014-04-03</created><updated>2014-12-10</updated><authors><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Fixed-parameter tractable canonization and isomorphism test for graphs
  of bounded treewidth</title><categories>cs.DS cs.CC</categories><comments>Full version of a paper presented at FOCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a fixed-parameter tractable algorithm that, given a parameter $k$ and
two graphs $G_1,G_2$, either concludes that one of these graphs has treewidth
at least $k$, or determines whether $G_1$ and $G_2$ are isomorphic. The running
time of the algorithm on an $n$-vertex graph is $2^{O(k^5\log k)}\cdot n^5$,
and this is the first fixed-parameter algorithm for Graph Isomorphism
parameterized by treewidth.
  Our algorithm in fact solves the more general canonization problem. We namely
design a procedure working in $2^{O(k^5\log k)}\cdot n^5$ time that, for a
given graph $G$ on $n$ vertices, either concludes that the treewidth of $G$ is
at least $k$, or: * finds in an isomorphic-invariant way a graph
$\mathfrak{c}(G)$ that is isomorphic to $G$; * finds an isomorphism-invariant
construction term --- an algebraic expression that encodes $G$ together with a
tree decomposition of $G$ of width $O(k^4)$.
  Hence, the isomorphism test reduces to verifying whether the computed
isomorphic copies or the construction terms for $G_1$ and $G_2$ are equal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0823</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0823</id><created>2014-04-03</created><updated>2014-04-05</updated><authors><author><keyname>Bernini</keyname><forenames>A.</forenames></author><author><keyname>Bilotta</keyname><forenames>S.</forenames></author><author><keyname>Pinzani</keyname><forenames>R.</forenames></author><author><keyname>Sabri</keyname><forenames>A.</forenames></author><author><keyname>Vajnovszki</keyname><forenames>V.</forenames></author></authors><title>Gray code orders for $q$-ary words avoiding a given factor</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on BRGC inspired order relations we give Gray codes and a generating
algorithm for $q$-ary words avoiding a prescribed factor. These generalize an
early 2001 result and a very recent one published by some of the present
authors, and can be seen as an alternative to those of Squire published in
1996. Among the involved tools, we make use of generalized BRGC order
relations, ultimate periodicity of infinite words, and word matching
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0832</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0832</id><created>2014-04-03</created><authors><author><keyname>Kopetz</keyname><forenames>Tal</forenames><affiliation>Shitz</affiliation></author><author><keyname>Permuter</keyname><forenames>Haim</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Multiple Access Channels with Combined Cooperation and Partial Cribbing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the multiple access channel (MAC) with combined
cooperation and partial cribbing and characterize its capacity region.
Cooperation means that the two encoders send a message to one another via a
rate-limited link prior to transmission, while partial cribbing means that each
of the two encoders obtains a deterministic function of the other encoder's
output with or without delay. Prior work in this field dealt separately with
cooperation and partial cribbing. However, by combining these two methods we
can achieve significantly higher rates. Remarkably, the capacity region does
not require an additional auxiliary random variable (RV) since the purpose of
both cooperation and partial cribbing is to generate a common message between
the encoders. In the proof we combine methods of block Markov coding, backward
decoding, double rate-splitting, and joint typicality decoding. Furthermore, we
present the Gaussian MAC with combined one-sided cooperation and quantized
cribbing. For this model, we give an achievability scheme that shows how many
cooperation or quantization bits are required in order to achieve a Gaussian
MAC with full cooperation/cribbing capacity region. After establishing our main
results, we consider two cases where only one auxiliary RV is needed. The first
is a rate distortion dual setting for the MAC with a common message, a private
message and combined cooperation and cribbing. The second is a state-dependent
MAC with cooperation, where the state is known at a partially cribbing encoder
and at the decoder. However, there are cases where more than one auxiliary RV
is needed, e.g., when the cooperation and cribbing are not used for the same
purposes. We present a MAC with an action-dependent state, where the action is
based on the cooperation but not on the cribbing. Therefore, in this case more
than one auxiliary RV is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0834</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0834</id><created>2014-04-03</created><authors><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames><affiliation>Universit&#xe9; de Mons, Belgium</affiliation></author><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Belgium</affiliation></author><author><keyname>Randour</keyname><forenames>Mickael</forenames><affiliation>Universit&#xe9; de Mons, Belgium</affiliation></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Belgium</affiliation></author></authors><title>Expectations or Guarantees? I Want It All! A crossroad between games and
  MDPs</title><categories>cs.GT cs.FL cs.LO</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 1-8</journal-ref><doi>10.4204/EPTCS.146.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When reasoning about the strategic capabilities of an agent, it is important
to consider the nature of its adversaries. In the particular context of
controller synthesis for quantitative specifications, the usual problem is to
devise a strategy for a reactive system which yields some desired performance,
taking into account the possible impact of the environment of the system. There
are at least two ways to look at this environment. In the classical analysis of
two-player quantitative games, the environment is purely antagonistic and the
problem is to provide strict performance guarantees. In Markov decision
processes, the environment is seen as purely stochastic: the aim is then to
optimize the expected payoff, with no guarantee on individual outcomes.
  In this expository work, we report on recent results introducing the beyond
worst-case synthesis problem, which is to construct strategies that guarantee
some quantitative requirement in the worst-case while providing an higher
expected value against a particular stochastic model of the environment given
as input. This problem is relevant to produce system controllers that provide
nice expected performance in the everyday situation while ensuring a strict
(but relaxed) performance threshold even in the event of very bad (while
unlikely) circumstances. It has been studied for both the mean-payoff and the
shortest path quantitative measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0835</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0835</id><created>2014-04-03</created><authors><author><keyname>Godo</keyname><forenames>Llu&#xed;s</forenames><affiliation>IIIA - CSIC</affiliation></author><author><keyname>Marchioni</keyname><forenames>Enrico</forenames><affiliation>IRIT - UPS</affiliation></author></authors><title>Games for the Strategic Influence of Expectations</title><categories>cs.GT cs.LO cs.MA</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><acm-class>I.2.11, I.2.4</acm-class><journal-ref>EPTCS 146, 2014, pp. 9-15</journal-ref><doi>10.4204/EPTCS.146.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of games where each player's aim is to randomise her
strategic choices in order to affect the other players' expectations aside from
her own. The way each player intends to exert this influence is expressed
through a Boolean combination of polynomial equalities and inequalities with
rational coefficients. We offer a logical representation of these games as well
as a computational study of the existence of equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0836</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0836</id><created>2014-04-03</created><authors><author><keyname>Jamroga</keyname><forenames>Wojciech</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Melissen</keyname><forenames>Matthijs</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>Schnoor</keyname><forenames>Henning</forenames><affiliation>University of Kiel</affiliation></author></authors><title>On Defendability of Security Properties</title><categories>cs.CR cs.GT</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 17-25</journal-ref><doi>10.4204/EPTCS.146.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the security of interaction protocols when incentives of
participants are taken into account. We begin by formally defining correctness
of a protocol, given a notion of rationality and utilities of participating
agents. Based on that, we propose how to assess security when the precise
incentives are unknown. Then, the security level can be defined in terms of
defender sets, i.e., sets of participants who can effectively &quot;defend&quot; the
security property as long as they are in favor of the property.
  We present some theoretical characterizations of defendable protocols under
Nash equilibrium, first for bijective games (a standard assumption in game
theory), and then for games with non-injective outcomes that better correspond
to interaction protocols. Finally, we apply our concepts to analyze fairness in
the ASW contract-signing protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0837</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0837</id><created>2014-04-03</created><authors><author><keyname>Belardinelli</keyname><forenames>Francesco</forenames><affiliation>Universit&#xe9; d'Evry</affiliation></author></authors><title>Reasoning about Knowledge and Strategies: Epistemic Strategy Logic</title><categories>cs.LO cs.AI</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 27-33</journal-ref><doi>10.4204/EPTCS.146.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce Epistemic Strategy Logic (ESL), an extension of
Strategy Logic with modal operators for individual knowledge. This enhanced
framework allows us to represent explicitly and to reason about the knowledge
agents have of their own and other agents' strategies. We provide a semantics
to ESL in terms of epistemic concurrent game models, and consider the
corresponding model checking problem. We show that the complexity of model
checking ESL is not worse than (non-epistemic) Strategy Logic
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0838</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0838</id><created>2014-04-03</created><authors><author><keyname>Huang</keyname><forenames>Xiaowei</forenames><affiliation>University of New South Wales, Australia</affiliation></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames><affiliation>University of New South Wales, Australia</affiliation></author></authors><title>An Epistemic Strategy Logic (Extended Abstract)</title><categories>cs.LO</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 35-41</journal-ref><doi>10.4204/EPTCS.146.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an extension of temporal epistemic logic with operators
that quantify over strategies. The language also provides a natural way to
represent what agents would know were they to be aware of the strategies being
used by other agents. Some examples are presented to motivate the framework,
and relationships to several variants of alternating temporal epistemic logic
are discussed. The computational complexity of model checking the logic is also
characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0839</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0839</id><created>2014-04-03</created><authors><author><keyname>Bouyer</keyname><forenames>Patricia</forenames><affiliation>LSV -- CNRS &amp; ENS Cachan</affiliation></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames><affiliation>LSV -- CNRS &amp; ENS Cachan</affiliation></author><author><keyname>Vester</keyname><forenames>Steen</forenames><affiliation>DTU, Kgs. Lyngby</affiliation></author></authors><title>Nash Equilibria in Symmetric Games with Partial Observation</title><categories>cs.GT cs.LO</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><acm-class>D2.4</acm-class><journal-ref>EPTCS 146, 2014, pp. 49-55</journal-ref><doi>10.4204/EPTCS.146.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a model for representing large multiplayer games, which
satisfy strong symmetry properties. This model is made of multiple copies of an
arena; each player plays in his own arena, and can partially observe what the
other players do. Therefore, this game has partial information and symmetry
constraints, which make the computation of Nash equilibria difficult. We show
several undecidability results, and for bounded-memory strategies, we precisely
characterize the complexity of computing pure Nash equilibria (for qualitative
objectives) in this game model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0840</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0840</id><created>2014-04-03</created><authors><author><keyname>Guelev</keyname><forenames>Dimitar P.</forenames><affiliation>Institute of Mathematics and Informatics, Bulgarian Academy of Sciences</affiliation></author></authors><title>Refining and Delegating Strategic Ability in ATL</title><categories>cs.LO cs.GT cs.MA</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 57-63</journal-ref><doi>10.4204/EPTCS.146.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose extending Alternating-time Temporal Logic (ATL) by an operator &lt;i
refines-to G&gt; F to express that agent i can distribute its powers to a set of
sub-agents G in a way which satisfies ATL condition f on the strategic ability
of the coalitions they may form, possibly together with others agents. We prove
the decidability of model-checking of formulas whose subformulas with this
operator as the main connective have the form &lt;i_1 refines-to G_1&gt;...&lt;i_m
refines-to G_m&gt; f, with no further occurrences of this operator in f.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0841</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0841</id><created>2014-04-03</created><authors><author><keyname>Nalon</keyname><forenames>Cl&#xe1;udia</forenames><affiliation>Department of Computer Science, University of Bras&#xed;lia, Brazil</affiliation></author><author><keyname>Zhang</keyname><forenames>Lan</forenames><affiliation>Information School Capital University of Economics and Business, China</affiliation></author><author><keyname>Dixon</keyname><forenames>Clare</forenames><affiliation>Department of Computer Science, University of Liverpool, UK</affiliation></author><author><keyname>Hustadt</keyname><forenames>Ullrich</forenames><affiliation>Department of Computer Science, University of Liverpool, UK</affiliation></author></authors><title>A Resolution Prover for Coalition Logic</title><categories>cs.LO cs.AI</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><acm-class>I.2.3;F.4.1</acm-class><journal-ref>EPTCS 146, 2014, pp. 65-73</journal-ref><doi>10.4204/EPTCS.146.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a prototype tool for automated reasoning for Coalition Logic, a
non-normal modal logic that can be used for reasoning about cooperative agency.
The theorem prover CLProver is based on recent work on a resolution-based
calculus for Coalition Logic that operates on coalition problems, a normal form
for Coalition Logic. We provide an overview of coalition problems and of the
resolution-based calculus for Coalition Logic. We then give details of the
implementation of CLProver and present the results for a comparison with an
existing tableau-based solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0842</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0842</id><created>2014-04-03</created><authors><author><keyname>Jiang</keyname><forenames>Xiang</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Pauly</keyname><forenames>Arno</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Efficient Decomposition of Bimatrix Games (Extended Abstract)</title><categories>cs.GT cs.CC</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><acm-class>F.2.1;G.4</acm-class><journal-ref>EPTCS 146, 2014, pp. 75-81</journal-ref><doi>10.4204/EPTCS.146.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting the algebraic structure of the set of bimatrix games, a
divide-and-conquer algorithm for finding Nash equilibria is proposed. The
algorithm is fixed-parameter tractable with the size of the largest irreducible
component of a game as parameter. An implementation of the algorithm is shown
to yield a significant performance increase on inputs with small parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0843</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0843</id><created>2014-04-03</created><authors><author><keyname>Aminof</keyname><forenames>Benjamin</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Rubin</keyname><forenames>Sasha</forenames><affiliation>IST Austria and TU Wien</affiliation></author></authors><title>First Cycle Games</title><categories>cs.LO cs.GT</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><acm-class>F.3</acm-class><journal-ref>EPTCS 146, 2014, pp. 83-90</journal-ref><doi>10.4204/EPTCS.146.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First cycle games (FCG) are played on a finite graph by two players who push
a token along the edges until a vertex is repeated, and a simple cycle is
formed. The winner is determined by some fixed property Y of the sequence of
labels of the edges (or nodes) forming this cycle. These games are
traditionally of interest because of their connection with infinite-duration
games such as parity and mean-payoff games.
  We study the memory requirements for winning strategies of FCGs and certain
associated infinite duration games. We exhibit a simple FCG that is not
memoryless determined (this corrects a mistake in \it Memoryless determinacy of
parity and mean payoff games: a simple proof by Bj\&quot;orklund, Sandberg, Vorobyov
(2004) that claims that FCGs for which Y is closed under cyclic permutations
are memoryless determined). We show that /Theta(n)! memory (where n is the
number of nodes in the graph), which is always sufficient, may be necessary to
win some FCGs. On the other hand, we identify easy to check conditions on Y
(i.e., Y is closed under cyclic permutations, and both Y and its complement are
closed under concatenation) that are sufficient to ensure that the
corresponding FCGs and their associated infinite duration games are memoryless
determined. We demonstrate that many games considered in the literature, such
as mean-payoff, parity, energy, etc., satisfy these conditions. On the
complexity side, we show (for efficiently computable Y) that while solving FCGs
is in PSPACE, solving some families of FCGs is PSPACE-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0844</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0844</id><created>2014-04-03</created><authors><author><keyname>Aucher</keyname><forenames>Guillaume</forenames></author><author><keyname>Maubert</keyname><forenames>Bastien</forenames></author><author><keyname>Pinchinat</keyname><forenames>Sophie</forenames></author></authors><title>Automata Techniques for Epistemic Protocol Synthesis</title><categories>cs.LO</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 97-103</journal-ref><doi>10.4204/EPTCS.146.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we aim at applying automata techniques to problems studied in
Dynamic Epistemic Logic, such as epistemic planning. To do so, we first remark
that repeatedly executing ad infinitum a propositional event model from an
initial epistemic model yields a relational structure that can be finitely
represented with automata. This correspondence, together with recent results on
uniform strategies, allows us to give an alternative decidability proof of the
epistemic planning problem for propositional events, with as by-products
accurate upper-bounds on its time complexity, and the possibility to synthesize
a finite word automaton that describes the set of all solution plans. In fact,
using automata techniques enables us to solve a much more general problem, that
we introduce and call epistemic protocol synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0845</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0845</id><created>2014-04-03</created><authors><author><keyname>Bonatti</keyname><forenames>Piero A.</forenames><affiliation>D.I.E.T.I. Universit&#xe0; di Napoli Federico II</affiliation></author><author><keyname>Faella</keyname><forenames>Marco</forenames><affiliation>D.I.E.T.I. Universit&#xe0; di Napoli Federico II</affiliation></author><author><keyname>Sauro</keyname><forenames>Luigi</forenames><affiliation>D.I.E.T.I. Universit&#xe0; di Napoli Federico II</affiliation></author></authors><title>Partial Preferences for Mediated Bargaining</title><categories>cs.GT cs.MA</categories><comments>In Proceedings SR 2014, arXiv:1404.0414</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 146, 2014, pp. 105-111</journal-ref><doi>10.4204/EPTCS.146.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we generalize standard Decision Theory by assuming that two
outcomes can also be incomparable. Two motivating scenarios show how
incomparability may be helpful to represent those situations where, due to lack
of information, the decision maker would like to maintain different options
alive and defer the final decision. In particular, a new axiomatization is
given which turns out to be a weakening of the classical set of axioms used in
Decision Theory. Preliminary results show how preferences involving complex
distributions are related to judgments on single alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0846</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0846</id><created>2014-04-03</created><authors><author><keyname>Han</keyname><forenames>Fenglin</forenames></author><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Herrmann</keyname><forenames>Peter</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz</forenames></author></authors><title>Towards Verifying Safety Properties of Real-Time Probabilistic Systems</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 147, 2014, pp. 1-15</journal-ref><doi>10.4204/EPTCS.147.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using probabilities in the formal-methods-based development of
safety-critical software has quickened interests in academia and industry. We
address this area by our model-driven engineering method for reactive systems
SPACE and its tool-set Reactive Blocks that provide an extension to support the
modeling and verification of real-time behaviors. The approach facilitates the
composition of system models from reusable building blocks as well as the
verification of functional and real-time properties and the automatic
generation of Java code.
  In this paper, we describe the extension of the tool-set to enable the
modeling and verification of probabilistic real-time system behavior with the
focus on spatial properties that ensure system safety. In particular, we
incorporate descriptions of probabilistic behavior into our Reactive Blocks
models and integrate the model checker PRISM which allows to verify that a
real-time system satisfies certain safety properties with a given probability.
Moreover, we consider the spatial implication of probabilistic system
specifications by integrating the spatial verification tool BeSpaceD and give
an automatic approach to translate system specifications to the input languages
of PRISM and BeSpaceD. The approach is highlighted by an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0847</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0847</id><created>2014-04-03</created><authors><author><keyname>Stattelmann</keyname><forenames>Stefan</forenames><affiliation>ABB Corporate Research, Ladenburg, Germany</affiliation></author><author><keyname>Oriol</keyname><forenames>Manuel</forenames><affiliation>ABB Corporate Research, Baden-D&#xe4;ttwil, Switzerland</affiliation></author><author><keyname>Gamer</keyname><forenames>Thomas</forenames><affiliation>ABB Corporate Research, Ladenburg, Germany</affiliation></author></authors><title>Execution Time Analysis for Industrial Control Applications</title><categories>cs.SE cs.SY</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><acm-class>D.4.8;</acm-class><journal-ref>EPTCS 147, 2014, pp. 16-31</journal-ref><doi>10.4204/EPTCS.147.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the execution time of software components is often mandatory when
evaluating the non-functional properties of software-intensive systems. This
particularly holds for real-time embedded systems, e.g., in the context of
industrial automation. In practice it is however very hard to obtain reliable
execution time estimates which are accurate, but not overly pessimistic with
respect to the typical behavior of the software.
  This article proposes two new concepts to ease the use of execution time
analysis for industrial control applications: (1) a method based on recurring
occurrences of code sequences for automatically creating a timing model of a
given processor and (2) an interactive way to integrate execution time analysis
into the development environment, thus making timing analysis results easily
accessible for software developers. The proposed methods are validated by an
industrial case study, which shows that a significant amount of code reuse is
present in a set of representative industrial control applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0848</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0848</id><created>2014-04-03</created><authors><author><keyname>Lanoix</keyname><forenames>Arnaud</forenames><affiliation>LINA CNRS and Nantes University</affiliation></author><author><keyname>Kouchnarenko</keyname><forenames>Olga</forenames><affiliation>FEMTO-ST CNRS and University of Franche-Comt&#xe9;</affiliation></author></authors><title>Component Substitution through Dynamic Reconfigurations</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 147, 2014, pp. 32-46</journal-ref><doi>10.4204/EPTCS.147.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Component substitution has numerous practical applications and constitutes an
active research topic. This paper proposes to enrich an existing
component-based framework--a model with dynamic reconfigurations making the
system evolve--with a new reconfiguration operation which &quot;substitutes&quot;
components by other components, and to study its impact on sequences of dynamic
reconfigurations.
  Firstly, we define substitutability constraints which ensure the component
encapsulation while performing reconfigurations by component substitutions.
Then, we integrate them into a substitutability-based simulation to take these
substituting reconfigurations into account on sequences of dynamic
reconfigurations. Thirdly, as this new relation being in general undecidable
for infinite-state systems, we propose a semi-algorithm to check it on the fly.
Finally, we report on experimentations using the B tools to show the
feasibility of the developed approach, and to illustrate the paper's proposals
on an example of the HTTP server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0849</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0849</id><created>2014-04-03</created><authors><author><keyname>Colombo</keyname><forenames>Christian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Pace</keyname><forenames>Gordon J.</forenames><affiliation>University of Malta</affiliation></author></authors><title>Comprehensive Monitor-Oriented Compensation Programming</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 147, 2014, pp. 47-61</journal-ref><doi>10.4204/EPTCS.147.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compensation programming is typically used in the programming of web service
compositions whose correct implementation is crucial due to their handling of
security-critical activities such as financial transactions. While traditional
exception handling depends on the state of the system at the moment of failure,
compensation programming is significantly more challenging and dynamic because
it is dependent on the runtime execution flow - with the history of behaviour
of the system at the moment of failure affecting how to apply compensation. To
address this dynamic element, we propose the use of runtime monitors to
facilitate compensation programming, with monitors enabling the modeller to be
able to implicitly reason in terms of the runtime control flow, thus separating
the concerns of system building and compensation modelling. Our approach is
instantiated into an architecture and shown to be applicable to a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0850</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0850</id><created>2014-04-03</created><authors><author><keyname>Couto</keyname><forenames>Rui</forenames><affiliation>University of Minho</affiliation></author><author><keyname>Ribeiro</keyname><forenames>Ant&#xf3;nio Nestor</forenames><affiliation>University of Minho</affiliation></author><author><keyname>Campos</keyname><forenames>Jos&#xe9; Creissac</forenames><affiliation>University of Minho</affiliation></author></authors><title>Application of Ontologies in Identifying Requirements Patterns in Use
  Cases</title><categories>cs.SE cs.CL cs.IR</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><acm-class>D.2.1; D.2.2; D.2.10</acm-class><journal-ref>EPTCS 147, 2014, pp. 62-76</journal-ref><doi>10.4204/EPTCS.147.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use case specifications have successfully been used for requirements
description. They allow joining, in the same modeling space, the expectations
of the stakeholders as well as the needs of the software engineer and analyst
involved in the process. While use cases are not meant to describe a system's
implementation, by formalizing their description we are able to extract
implementation relevant information from them. More specifically, we are
interested in identifying requirements patterns (common requirements with
typical implementation solutions) in support for a requirements based software
development approach. In the paper we propose the transformation of Use Case
descriptions expressed in a Controlled Natural Language into an ontology
expressed in the Web Ontology Language (OWL). OWL's query engines can then be
used to identify requirements patterns expressed as queries over the ontology.
We describe a tool that we have developed to support the approach and provide
an example of usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0851</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0851</id><created>2014-04-03</created><authors><author><keyname>Di Marco</keyname><forenames>Antinisca</forenames><affiliation>University of L'Aquila</affiliation></author><author><keyname>Trubiani</keyname><forenames>Catia</forenames><affiliation>University of L'Aquila</affiliation></author></authors><title>A model-driven approach to broaden the detection of software performance
  antipatterns at runtime</title><categories>cs.SE cs.PF</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><acm-class>C.4, Performance of Systems; D.2.8, Software Engineering, Metrics,
  performance measures</acm-class><journal-ref>EPTCS 147, 2014, pp. 77-92</journal-ref><doi>10.4204/EPTCS.147.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance antipatterns document bad design patterns that have negative
influence on system performance. In our previous work we formalized such
antipatterns as logical predicates that predicate on four views: (i) the static
view that captures the software elements (e.g. classes, components) and the
static relationships among them; (ii) the dynamic view that represents the
interaction (e.g. messages) that occurs between the software entities elements
to provide the system functionalities; (iii) the deployment view that describes
the hardware elements (e.g. processing nodes) and the mapping of the software
entities onto the hardware platform; (iv) the performance view that collects
specific performance indices. In this paper we present a lightweight
infrastructure that is able to detect performance antipatterns at runtime
through monitoring. The proposed approach precalculates such predicates and
identifies antipatterns whose static, dynamic and deployment sub-predicates are
validated by the current system configuration and brings at runtime the
verification of performance sub-predicates. The proposed infrastructure
leverages model-driven techniques to generate probes for monitoring the
performance sub-predicates and detecting antipatterns at runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0852</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0852</id><created>2014-04-03</created><authors><author><keyname>Muram</keyname><forenames>Faiz UL</forenames></author><author><keyname>Tran</keyname><forenames>Huy</forenames></author><author><keyname>Zdun</keyname><forenames>Uwe</forenames></author></authors><title>Automated Mapping of UML Activity Diagrams to Formal Specifications for
  Supporting Containment Checking</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><acm-class>D.2.0; D.2.4;F.4.1;</acm-class><journal-ref>EPTCS 147, 2014, pp. 93-107</journal-ref><doi>10.4204/EPTCS.147.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business analysts and domain experts are often sketching the behaviors of a
software system using high-level models that are technology- and
platform-independent. The developers will refine and enrich these high-level
models with technical details. As a consequence, the refined models can deviate
from the original models over time, especially when the two kinds of models
evolve independently. In this context, we focus on behavior models; that is, we
aim to ensure that the refined, low-level behavior models conform to the
corresponding high-level behavior models. Based on existing formal verification
techniques, we propose containment checking as a means to assess whether the
system's behaviors described by the low-level models satisfy what has been
specified in the high-level counterparts. One of the major obstacles is how to
lessen the burden of creating formal specifications of the behavior models as
well as consistency constraints, which is a tedious and error-prone task when
done manually. Our approach presented in this paper aims at alleviating the
aforementioned challenges by considering the behavior models as verification
inputs and devising automated mappings of behavior models onto formal
properties and descriptions that can be directly used by model checkers. We
discuss various challenges in our approach and show the applicability of our
approach in illustrative scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0853</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0853</id><created>2014-04-03</created><authors><author><keyname>Hamiaz</keyname><forenames>Mounira Kezadri</forenames><affiliation>Universit&#xe9; de Toulouse, IRIT, France</affiliation></author><author><keyname>Pantel</keyname><forenames>Marc</forenames><affiliation>Universit&#xe9; de Toulouse, IRIT, France</affiliation></author><author><keyname>Combemale</keyname><forenames>Beno&#xee;t</forenames><affiliation>Universit&#xe9; de Rennes 1, IRISA, France</affiliation></author><author><keyname>Thirioux</keyname><forenames>Xavier</forenames><affiliation>Universit&#xe9; de Toulouse, IRIT, France</affiliation></author></authors><title>Correct-by-construction model composition: Application to the Invasive
  Software Composition method</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><acm-class>D.2; F.4;</acm-class><journal-ref>EPTCS 147, 2014, pp. 108-122</journal-ref><doi>10.4204/EPTCS.147.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Composition technologies improve reuse in the development of large-scale
complex systems. Safety critical systems require intensive validation and
verification activities. These activities should be compositional in order to
reduce the amount of residual verification activities that must be conducted on
the composite in addition to the ones conducted on each components. In order to
ensure the correctness of compositional verification and assess the minimality
of the residual verification, the contribution proposes to use formal
specification and verification at the composition operator level. A first
experiment was conducted in [15] using proof assistants to formalize the
generic composition technology ISC and prove that type checking was
compositional. This contribution extends our early work to handle full model
conformance and study the mandatory residual verification. It shows that ISC
operators are not fully compositional with respect to conformance and provides
the minimal preconditions on the operators mandatory to ensure compositional
conformance. The appropriate operators from ISC (especially bind) have been
implemented in the COQ4MDE framework that provides a full implementation of MOF
in the COQ proof assistant. Expected properties, respectively residual
verification, are expressed as post, respectfully pre, conditions for the
composition operators. The correctness of the compositional verification is
proven in COQ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0854</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0854</id><created>2014-04-03</created><authors><author><keyname>Bai</keyname><forenames>Wei</forenames></author><author><keyname>Tadjouddine</keyname><forenames>Emmanuel M.</forenames></author><author><keyname>Guo</keyname><forenames>Yu</forenames></author></authors><title>Enabling Automatic Certification of Online Auctions</title><categories>cs.LO cs.AI</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 147, 2014, pp. 123-132</journal-ref><doi>10.4204/EPTCS.147.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of building up trust in a network of online auctions
by software agents. This requires agents to have a deeper understanding of
auction mechanisms and be able to verify desirable properties of a given
mechanism. We have shown how these mechanisms can be formalised as semantic web
services in OWL-S, a good enough expressive machine-readable formalism enabling
software agents, to discover, invoke, and execute a web service. We have also
used abstract interpretation to translate the auction's specifications from
OWL-S, based on description logic, to COQ, based on typed lambda calculus, in
order to enable automatic verification of desirable properties of the auction
by the software agents. For this language translation, we have discussed the
syntactic transformation as well as the semantics connections between both
concrete and abstract domains. This work contributes to the implementation of
the vision of agent-mediated e-commerce systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0855</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0855</id><created>2014-04-03</created><authors><author><keyname>Santos</keyname><forenames>Luciana Brasil Rebelo dos</forenames><affiliation>Instituto Nacional de Pesquisas Espaciais - INPE</affiliation></author><author><keyname>J&#xfa;nior</keyname><forenames>Valdivino Alexandre de Santiago</forenames><affiliation>Instituto Nacional de Pesquisas Espaciais - INPE</affiliation></author><author><keyname>Vijaykumar</keyname><forenames>Nandamudi Lankalapalli</forenames><affiliation>Instituto Nacional de Pesquisas Espaciais - INPE</affiliation></author></authors><title>Transformation of UML Behavioral Diagrams to Support Software Model
  Checking</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2014, arXiv:1404.0436</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 147, 2014, pp. 133-142</journal-ref><doi>10.4204/EPTCS.147.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unified Modeling Language (UML) is currently accepted as the standard for
modeling (object-oriented) software, and its use is increasing in the aerospace
industry. Verification and Validation of complex software developed according
to UML is not trivial due to complexity of the software itself, and the several
different UML models/diagrams that can be used to model behavior and structure
of the software. This paper presents an approach to transform up to three
different UML behavioral diagrams (sequence, behavioral state machines, and
activity) into a single Transition System to support Model Checking of software
developed in accordance with UML. In our approach, properties are formalized
based on use case descriptions. The transformation is done for the NuSMV model
checker, but we see the possibility in using other model checkers, such as
SPIN. The main contribution of our work is the transformation of a non-formal
language (UML) to a formal language (language of the NuSMV model checker)
towards a greater adoption in practice of formal methods in software
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0864</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0864</id><created>2014-04-03</created><authors><author><keyname>Liu</keyname><forenames>Kangqi</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Yang</keyname><forenames>Dingcheng</forenames></author></authors><title>Generalized Signal Alignment For Arbitrary MIMO Two-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, submitted to IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the arbitrary MIMO two-way relay channels, where
there are $K$ source nodes, each equipped with $M_i$ antennas, for
$i=1,2,\cdots,K$, and one relay node, equipped with $N$ antennas. Each source
node can exchange independent messages with arbitrary other source nodes
assisted by the relay. We extend our newly-proposed transmission scheme,
generalized signal alignment (GSA) in [1], to arbitrary MIMO two-way relay
channels when $N&gt;M_i+M_j$, $\forall i \neq j$. The key idea of GSA is to cancel
the interference for each data pair in its specific subspace by two steps. This
is realized by jointly designing the precoding matrices at all source nodes and
the processing matrix at the relay node. Moreover, the aligned subspaces are
orthogonal to each other. By applying the GSA, we show that a necessary
condition on the antenna configuration to achieve the DoF upper bound $\min
\{\sum_{i=1}^K M_i, 2\sum_{i=2}^K M_i,2N\}$ is $N \geq \max\{\sum_{i=1}^K
M_i-M_s-M_t+d_{s,t}\mid \forall s,t\}$. Here, $d_{s,t}$ denotes the DoF of the
message exchanged between source node $s$ and $t$. In the special case when the
arbitrary MIMO two-way relay channel reduces to the $K$-user MIMO Y channel, we
show that our achievable region of DoF upper bound is larger than the previous
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0868</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0868</id><created>2014-04-03</created><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>He</keyname><forenames>Feidun</forenames></author><author><keyname>Dong</keyname><forenames>Hongbin</forenames></author></authors><title>A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack
  Problem</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 0-1 knapsack problem is a well-known combinatorial optimisation problem.
Approximation algorithms have been designed for solving it and they return
provably good solutions within polynomial time. On the other hand, genetic
algorithms are well suited for solving the knapsack problem and they find
reasonably good solutions quickly. A naturally arising question is whether
genetic algorithms are able to find solutions as good as approximation
algorithms do. This paper presents a novel multi-objective optimisation genetic
algorithm for solving the 0-1 knapsack problem. Experiment results show that
the new algorithm outperforms its rivals, the greedy algorithm, mixed strategy
genetic algorithm, and greedy algorithm + mixed strategy genetic algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0900</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0900</id><created>2014-04-03</created><updated>2014-04-29</updated><authors><author><keyname>Tang</keyname><forenames>Youze</forenames></author><author><keyname>Xiao</keyname><forenames>Xiaokui</forenames></author><author><keyname>Shi</keyname><forenames>Yanchen</forenames></author></authors><title>Influence Maximization: Near-Optimal Time Complexity Meets Practical
  Efficiency</title><categories>cs.SI cs.DB</categories><comments>Revised Sections 1, 2.3, and 5 to remove incorrect claims about
  reference [3]. Updated experiments accordingly. A shorter version of the
  paper will appear in SIGMOD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a social network G and a constant k, the influence maximization problem
asks for k nodes in G that (directly and indirectly) influence the largest
number of nodes under a pre-defined diffusion model. This problem finds
important applications in viral marketing, and has been extensively studied in
the literature. Existing algorithms for influence maximization, however, either
trade approximation guarantees for practical efficiency, or vice versa. In
particular, among the algorithms that achieve constant factor approximations
under the prominent independent cascade (IC) model or linear threshold (LT)
model, none can handle a million-node graph without incurring prohibitive
overheads.
  This paper presents TIM, an algorithm that aims to bridge the theory and
practice in influence maximization. On the theory side, we show that TIM runs
in O((k+\ell) (n+m) \log n / \epsilon^2) expected time and returns a
(1-1/e-\epsilon)-approximate solution with at least 1 - n^{-\ell} probability.
The time complexity of TIM is near-optimal under the IC model, as it is only a
\log n factor larger than the \Omega(m + n) lower-bound established in previous
work (for fixed k, \ell, and \epsilon). Moreover, TIM supports the triggering
model, which is a general diffusion model that includes both IC and LT as
special cases. On the practice side, TIM incorporates novel heuristics that
significantly improve its empirical efficiency without compromising its
asymptotic performance. We experimentally evaluate TIM with the largest
datasets ever tested in the literature, and show that it outperforms the
state-of-the-art solutions (with approximation guarantees) by up to four orders
of magnitude in terms of running time. In particular, when k = 50, \epsilon =
0.2, and \ell = 1, TIM requires less than one hour on a commodity machine to
process a network with 41.6 million nodes and 1.4 billion edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0904</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0904</id><created>2014-04-03</created><authors><author><keyname>Aszal&#xf3;s</keyname><forenames>L.</forenames></author><author><keyname>Hajdu</keyname><forenames>L.</forenames></author><author><keyname>Peth&#x151;</keyname><forenames>A.</forenames></author></authors><title>On a correlational clustering of integers</title><categories>math.NT cs.AI</categories><msc-class>11N36, 11Y16, 68G05, 62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation clustering is a concept of machine learning. The ultimate goal of
such a clustering is to find a partition with minimal conflicts. In this paper
we investigate a correlation clustering of integers, based upon the greatest
common divisor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0906</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0906</id><created>2014-04-03</created><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Optimal Power Control for Analog Bidirectional Relaying with Long-Term
  Relay Power Constraint</title><categories>cs.IT math.IT</categories><comments>conference IEEE Globecom 2013, Atlanta, Georgia, US</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless systems that carry delay-sensitive information (such as speech
and/or video signals) typically transmit with fixed data rates, but may
occasionally suffer from transmission outages caused by the random nature of
the fading channels. If the transmitter has instantaneous channel state
information (CSI) available, it can compensate for a significant portion of
these outages by utilizing power allocation. In a conventional dual-hop
bidirectional amplify-and-forward (AF) relaying system, the relay already has
instantaneous CSI of both links available, as this is required for relay gain
adjustment. We therefore develop an optimal power allocation strategy for the
relay, which adjusts its instantaneous output power to the minimum level
required to avoid outages, but only if the required output power is below some
cutoff level; otherwise, the relay is silent in order to conserve power and
prolong its lifetime. The proposed scheme is proven to minimize the system
outage probability, subject to an average power constraint at the relay and
fixed output powers at the end nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0933</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0933</id><created>2014-04-03</created><authors><author><keyname>Vikramkumar</keyname><affiliation>B092633</affiliation></author><author><keyname>B</keyname><forenames>Vijaykumar</forenames><affiliation>B091956</affiliation></author><author><keyname>Trilochan</keyname><affiliation>B092654</affiliation></author></authors><title>Bayes and Naive Bayes Classifier</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Bayesian Classification represents a supervised learning method as well
as a statistical method for classification. Assumes an underlying probabilistic
model and it allows us to capture uncertainty about the model in a principled
way by determining probabilities of the outcomes. This Classification is named
after Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesian
classification provides practical learning algorithms and prior knowledge and
observed data can be combined. Bayesian Classification provides a useful
perspective for understanding and evaluating many learning algorithms. It
calculates explicit probabilities for hypothesis and it is robust to noise in
input data. In statistical classification the Bayes classifier minimises the
probability of misclassification. That was a visual intuition for a simple case
of the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)Simple
Bayes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0934</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0934</id><created>2014-04-02</created><authors><author><keyname>Li</keyname><forenames>Jiyi</forenames></author></authors><title>Map Route Ranking with Weighted Distance using Environmental Factors</title><categories>cs.HC</categories><comments>2 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When users search for the routes between two places using map based services,
these services compute and provide the top candidate routes based on shortest
geometric distances or ideal time consuming. However, other real factors like
physical exertion and practical time consuming will influence user experience,
and the environmental factors like steep slope and traffic jam that result in
these real factors need to be considered. For example, when users travel on
foot or by bicycle, if there are many steep slopes on the routes, it will be
difficult or easy to be tired. In this paper, we propose an approach computing
weighted distance considering these environmental factors. We rank the
candidate route results generated by Google Map using elevation information. We
integrate the elevation information in the route results to assist users to
make decision. The solution can also be used in other scenarios that need to
consider environmental factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0948</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0948</id><created>2014-04-03</created><updated>2014-09-23</updated><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Cruz-Filipe</keyname><forenames>Luis</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>The Quest for Optimal Sorting Networks: Efficient Generation of
  Two-Layer Prefixes</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work identifying depth-optimal $n$-channel sorting networks for
$9\leq n \leq 16$ is based on exploiting symmetries of the first two layers.
However, the naive generate-and-test approach typically applied does not scale.
This paper revisits the problem of generating two-layer prefixes modulo
symmetries. An improved notion of symmetry is provided and a novel technique
based on regular languages and graph isomorphism is shown to generate the set
of non-symmetric representations. An empirical evaluation demonstrates that the
new method outperforms the generate-and-test approach by orders of magnitude
and easily scales until $n=40$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0953</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0953</id><created>2014-04-01</created><updated>2014-04-04</updated><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author><author><keyname>Heinz</keyname><forenames>Birgit</forenames></author></authors><title>Implementing Anti-Unification Modulo Equational Theory</title><categories>cs.LO</categories><comments>113 pages; 57 figures</comments><msc-class>68T15</msc-class><acm-class>I.2.6; F.4.2</acm-class><journal-ref>Technical Report &quot;Arbeitspapiere der GMD&quot;,ISSN 0723-0508,
  Vol.1006, June 1996</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an implementation of E-anti-unification as defined in Heinz
(1995), where tree-grammar descriptions of equivalence classes of terms are
used to compute generalizations modulo equational theories. We discuss several
improvements, including an efficient implementation of variable-restricted
E-anti-unification from Heinz (1995), and give some runtime figures about them.
We present applications in various areas, including lemma generation in
equational inductive proofs, intelligence tests, diverging Knuth-Bendix
completion, strengthening of induction hypotheses, and theory formation about
finite algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0956</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0956</id><created>2014-04-02</created><updated>2014-06-21</updated><authors><author><keyname>Given-Wilson</keyname><forenames>Thomas</forenames></author></authors><title>Expressiveness via Intensionality and Concurrency</title><categories>cs.LO</categories><comments>18 pages, to appear in ICTAC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation can be considered by taking into account two dimensions:
extensional versus intensional, and sequential versus concurrent. Traditionally
sequential extensional computation can be captured by the lambda-calculus.
However, recent work shows that there are more expressive intensional calculi
such as SF-calculus. Traditionally process calculi capture computation by
encoding the lambda-calculus, such as in the pi-calculus. Following this
increased expressiveness via intensionality, other recent work has shown that
concurrent pattern calculus is more expressive than pi-calculus. This paper
formalises the relative expressiveness of all four of these calculi by placing
them on a square whose edges are irreversible encodings. This square is
representative of a more general result: that expressiveness increases with
both intensionality and concurrency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0964</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0964</id><created>2014-04-03</created><updated>2014-04-03</updated><authors><author><keyname>Rhim</keyname><forenames>Joong Bum</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Distributed Hypothesis Testing with Social Learning and Symmetric Fusion</title><categories>cs.IT cs.MA cs.SI math.IT</categories><comments>10 pages, 7 figures</comments><journal-ref>IEEE Trans. on Signal Processing, vol. 62, no. 23, pp. 6298-6308,
  December 2014</journal-ref><doi>10.1109/TSP.2014.2362885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the utility of social learning in a distributed detection model with
agents sharing the same goal: a collective decision that optimizes an agreed
upon criterion. We show that social learning is helpful in some cases but is
provably futile (and thus essentially a distraction) in other cases.
Specifically, we consider Bayesian binary hypothesis testing performed by a
distributed detection and fusion system, where all decision-making agents have
binary votes that carry equal weight. Decision-making agents in the team
sequentially make local decisions based on their own private signals and all
precedent local decisions. It is shown that the optimal decision rule is not
affected by precedent local decisions when all agents observe conditionally
independent and identically distributed private signals. Perfect Bayesian
reasoning will cancel out all effects of social learning. When the agents
observe private signals with different signal-to-noise ratios, social learning
is again futile if the team decision is only approved by unanimity. Otherwise,
social learning can strictly improve the team performance. Furthermore, the
order in which agents make their decisions affects the team decision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0965</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0965</id><created>2014-04-03</created><authors><author><keyname>Monsees</keyname><forenames>Fabian</forenames></author><author><keyname>Bockelmann</keyname><forenames>Carsten</forenames></author><author><keyname>W&#xfc;bben</keyname><forenames>Dirk</forenames></author><author><keyname>Dekorsy</keyname><forenames>Armin</forenames></author></authors><title>Compressed Sensing Bayes Risk Minimization for Under-determined Systems
  via Sphere Detection</title><categories>cs.IT math.IT</categories><comments>Published at 77th IEEE Vehicular Technology Conference
  (VTC2013-Spring)</comments><doi>10.1109/VTCSpring.2013.6692486</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of Compresses Sensing is a promising physical layer
technology for the joint activity and data detection of signals. Detecting the
activity pattern correctly has severe impact on the system performance and is
therefore of major concern. In contrast to previous work, in this paper we
optimize joint activity and data detection in under-determined systems by
minimizing the Bayes-Risk for erroneous activity detection. We formulate a new
Compressed Sensing Bayes-Risk detector which directly allows to influence error
rates at the activity detection dynamically by a parameter that can be
controlled at higher layers. We derive the detector for a general linear system
and show that our detector outperforms classical Compressed Sensing approaches
by investigating an overloaded CDMA system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0967</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0967</id><created>2014-04-03</created><authors><author><keyname>Kari</keyname><forenames>Lila</forenames></author><author><keyname>Kopecki</keyname><forenames>Steffen</forenames></author><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Binary pattern tile set synthesis is NP-hard</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of algorithmic self-assembly, a long-standing unproven
conjecture has been that of the NP-hardness of binary pattern tile set
synthesis (2-PATS). The $k$-PATS problem is that of designing a tile assembly
system with the smallest number of tile types which will self-assemble an input
pattern of $k$ colors. Of both theoretical and practical significance, $k$-PATS
has been studied in a series of papers which have shown $k$-PATS to be NP-hard
for $k = 60$, $k = 29$, and then $k = 11$. In this paper, we close the
fundamental conjecture that 2-PATS is NP-hard, concluding this line of study.
  While most of our proof relies on standard mathematical proof techniques, one
crucial lemma makes use of a computer-assisted proof, which is a relatively
novel but increasingly utilized paradigm for deriving proofs for complex
mathematical problems. This tool is especially powerful for attacking
combinatorial problems, as exemplified by the proof of the four color theorem
by Appel and Haken (simplified later by Robertson, Sanders, Seymour, and
Thomas) or the recent important advance on the Erd\H{o}s discrepancy problem by
Konev and Lisitsa using computer programs. We utilize a massively parallel
algorithm and thus turn an otherwise intractable portion of our proof into a
program which requires approximately a year of computation time, bringing the
use of computer-assisted proofs to a new scale. We fully detail the algorithm
employed by our code, and make the code freely available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0969</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0969</id><created>2014-04-03</created><updated>2014-04-08</updated><authors><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>A Class of Reducible Cyclic Codes and Their Weight Distribution</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.4638</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a family of reducible cyclic codes over GF(p) whose duals have
four zeros is presented, where p is an odd prime. Furthermore, the weight
distribution of these cyclic codes is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0971</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0971</id><created>2014-03-27</created><updated>2014-04-15</updated><authors><author><keyname>Zidane</keyname><forenames>Karine</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Boucheret</keyname><forenames>Marie-Laure</forenames></author><author><keyname>Poulliat</keyname><forenames>Charly</forenames></author></authors><title>Improved channel estimation for interference cancellation in random
  access methods for satellite communications</title><categories>cs.IT math.IT</categories><comments>submitted to ASMS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of satellite communications, random access (RA) methods can
significantly increase throughput and reduce latency over the network. The
recent RA methods are based on multi-user multiple access transmission at the
same time and frequency combined with interference cancellation and iterative
decoding at the receiver. Generally, it is assumed that perfect knowledge of
the interference is available at the receiver. In practice, the interference
term has to be accurately estimated to avoid performance degradation. Several
estimation techniques have been proposed lately in the case of superimposed
signals. In this paper, we present an improved channel estimation technique
that combines estimation using an autocorrelation based method and the
Expectation-Maximization algorithm, and uses Pilot Symbol Assisted Modulation
to further improve the performance and achieve optimal interference
cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0975</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0975</id><created>2014-04-03</created><authors><author><keyname>Beccuti</keyname><forenames>Marco</forenames></author><author><keyname>Bibbona</keyname><forenames>Enrico</forenames></author><author><keyname>Horvath</keyname><forenames>Andras</forenames></author><author><keyname>Sirovich</keyname><forenames>Roberta</forenames></author><author><keyname>Angius</keyname><forenames>Alessio</forenames></author><author><keyname>Balbo</keyname><forenames>Gianfranco</forenames></author></authors><title>Analysis of Petri Net Models through Stochastic Differential Equations</title><categories>cs.PF math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known, mainly because of the work of Kurtz, that density dependent
Markov chains can be approximated by sets of ordinary differential equations
(ODEs) when their indexing parameter grows very large. This approximation
cannot capture the stochastic nature of the process and, consequently, it can
provide an erroneous view of the behavior of the Markov chain if the indexing
parameter is not sufficiently high. Important phenomena that cannot be revealed
include non-negligible variance and bi-modal population distributions. A
less-known approximation proposed by Kurtz applies stochastic differential
equations (SDEs) and provides information about the stochastic nature of the
process. In this paper we apply and extend this diffusion approximation to
study stochastic Petri nets. We identify a class of nets whose underlying
stochastic process is a density dependent Markov chain whose indexing parameter
is a multiplicative constant which identifies the population level expressed by
the initial marking and we provide means to automatically construct the
associated set of SDEs. Since the diffusion approximation of Kurtz considers
the process only up to the time when it first exits an open interval, we extend
the approximation by a machinery that mimics the behavior of the Markov chain
at the boundary and allows thus to apply the approach to a wider set of
problems. The resulting process is of the jump-diffusion type. We illustrate by
examples that the jump-diffusion approximation which extends to bounded domains
can be much more informative than that based on ODEs as it can provide accurate
quantity distributions even when they are multi-modal and even for relatively
small population levels. Moreover, we show that the method is faster than
simulating the original Markov chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0977</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0977</id><created>2014-04-03</created><authors><author><keyname>Mozes</keyname><forenames>Shay</forenames></author><author><keyname>Nussbaum</keyname><forenames>Yahav</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Faster Shortest Paths in Dense Distance Graphs, with Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to combine two techniques for efficiently computing shortest
paths in directed planar graphs. The first is the linear-time shortest-path
algorithm of Henzinger, Klein, Subramanian, and Rao [STOC'94]. The second is
Fakcharoenphol and Rao's algorithm [FOCS'01] for emulating Dijkstra's algorithm
on the dense distance graph (DDG). A DDG is defined for a decomposition of a
planar graph $G$ into regions of at most $r$ vertices each, for some parameter
$r &lt; n$. The vertex set of the DDG is the set of $\Theta(n/\sqrt r)$ vertices
of $G$ that belong to more than one region (boundary vertices). The DDG has
$\Theta(n)$ arcs, such that distances in the DDG are equal to the distances in
$G$. Fakcharoenphol and Rao's implementation of Dijkstra's algorithm on the DDG
(nicknamed FR-Dijkstra) runs in $O(n\log(n) r^{-1/2} \log r)$ time, and is a
key component in many state-of-the-art planar graph algorithms for shortest
paths, minimum cuts, and maximum flows. By combining these two techniques we
remove the $\log n$ dependency in the running time of the shortest-path
algorithm, making it $O(n r^{-1/2} \log^2r)$.
  This work is part of a research agenda that aims to develop new techniques
that would lead to faster, possibly linear-time, algorithms for problems such
as minimum-cut, maximum-flow, and shortest paths with negative arc lengths. As
immediate applications, we show how to compute maximum flow in directed
weighted planar graphs in $O(n \log p)$ time, where $p$ is the minimum number
of edges on any path from the source to the sink. We also show how to compute
any part of the DDG that corresponds to a region with $r$ vertices and $k$
boundary vertices in $O(r \log k)$ time, which is faster than has been
previously known for small values of $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0979</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0979</id><created>2014-04-03</created><updated>2015-05-20</updated><authors><author><keyname>Kasparick</keyname><forenames>Martin</forenames></author><author><keyname>Cavalcante</keyname><forenames>Renato L. G.</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author><author><keyname>Yukawa</keyname><forenames>Masahiro</forenames></author></authors><title>Kernel-Based Adaptive Online Reconstruction of Coverage Maps With Side
  Information</title><categories>cs.NI cs.LG stat.ML</categories><comments>submitted to IEEE Transactions on Vehicular Technology; revised and
  extended version with new simulation scenario</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of reconstructing coverage maps from
path-loss measurements in cellular networks. We propose and evaluate two
kernel-based adaptive online algorithms as an alternative to typical offline
methods. The proposed algorithms are application-tailored extensions of
powerful iterative methods such as the adaptive projected subgradient method
and a state-of-the-art adaptive multikernel method. Assuming that the moving
trajectories of users are available, it is shown how side information can be
incorporated in the algorithms to improve their convergence performance and the
quality of the estimation. The complexity is significantly reduced by imposing
sparsity-awareness in the sense that the algorithms exploit the compressibility
of the measurement data to reduce the amount of data which is saved and
processed. Finally, we present extensive simulations based on realistic data to
show that our algorithms provide fast, robust estimates of coverage maps in
real-world scenarios. Envisioned applications include path-loss prediction
along trajectories of mobile users as a building block for anticipatory
buffering or traffic offloading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.0981</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.0981</id><created>2014-04-03</created><authors><author><keyname>Joshi</keyname><forenames>Suman</forenames></author><author><keyname>Chauhan</keyname><forenames>Dr. Yashwant Singh</forenames></author><author><keyname>Negi</keyname><forenames>Dr. Ashish</forenames></author></authors><title>New Julia and Mandelbrot Sets for Jungck Ishikawa Iterates</title><categories>cs.GR</categories><comments>8 pages,18 tables,18 charts,18 figures Published with International
  Journal of Computer Trends and Technology (IJCTT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generation of fractals and study of the dynamics of polynomials is one of
the emerging and interesting field of research nowadays. We introduce in this
paper the dynamics of polynomials z^ n - z + c = 0 for n&gt;=2 and applied Jungck
Ishikawa Iteration to generate new Relative Superior Mandelbrot sets and
Relative Superior Julia sets. In order to solve this function by Jungck type
iterative schemes, we write it in the form of Sz = Tz, where the function T, S
are defined as Tz = z^ n + c and Sz = z. Only mathematical explanations are
derived by applying Jungck Ishikawa Iteration for polynomials in the literature
but in this paper we have generated Relative Mandelbrot sets and Relative Julia
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1006</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1006</id><created>2014-04-03</created><updated>2015-06-23</updated><authors><author><keyname>Sun</keyname><forenames>Kaiyuan</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author></authors><title>Contrasting Effects of Strong Ties on SIR and SIS Processes in Temporal
  Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1140/epjb/e2015-60568-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most real networks are characterized by connectivity patterns that evolve in
time following complex, non-Markovian, dynamics. Here we investigate the impact
of this ubiquitous feature by studying the Susceptible-Infected-Recovered (SIR)
and Susceptible-Infected-Susceptible (SIS) epidemic models on activity driven
networks with and without memory (i.e., Markovian and non-Markovian). We show
that while memory inhibits the spreading process in SIR models, where the
epidemic threshold is moved to larger values, it plays the opposite effect in
the case of the SIS, where the threshold is lowered. The heterogeneity in tie
strengths, and the frequent repetition of connections that it entails, allows
in fact less virulent SIS-like diseases to survive in tightly connected local
clusters that serve as reservoir for the virus. We validate this picture by
evaluating the threshold of both processes in a real temporal network. Our
findings confirm the important role played by non-Markovian network dynamics on
dynamical processes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1008</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1008</id><created>2014-04-03</created><updated>2015-06-23</updated><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Peng</keyname><forenames>Pan</forenames></author><author><keyname>Rossi</keyname><forenames>Alfred</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Spectral concentration and greedy k-clustering</title><categories>cs.DS</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular graph clustering method is to consider the embedding of an input
graph into R^k induced by the first k eigenvectors of its Laplacian, and to
partition the graph via geometric manipulations on the resulting metric space.
Despite the practical success of this methodology, there is limited
understanding of several heuristics that follow this framework. We provide
theoretical justification for one such natural and computationally efficient
variant.
  Our result can be summarized as follows. A partition of a graph is called
strong if each cluster has small external conductance, and large internal
conductance. We present a simple greedy spectral clustering algorithm which
returns a partition that is provably close to a suitably strong partition,
provided that such a partition exists. A recent result shows that strong
partitions exist for graphs with a sufficiently large spectral gap between the
k-th and (k+1)-th eigenvalues. Taking this together with our main theorem gives
a spectral algorithm which finds a partition close to a strong one for graphs
with large enough spectral gap. We also show how this simple greedy algorithm
can be implemented in near-linear time for any fixed k and error guarantee.
Finally, we evaluate our algorithm on some real-world and synthetic inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1009</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1009</id><created>2014-04-03</created><authors><author><keyname>Silva</keyname><forenames>Thiago H</forenames></author><author><keyname>de Melo</keyname><forenames>Pedro O S Vaz</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author><author><keyname>Loureiro</keyname><forenames>Antonio</forenames></author></authors><title>You are What you Eat (and Drink): Identifying Cultural Boundaries by
  Analyzing Food &amp; Drink Habits in Foursquare</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>10 pages, 10 figures, 1 table. Proceedings of 8th AAAI Intl. Conf. on
  Weblogs and Social Media (ICWSM 2014)</comments><acm-class>J.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Food and drink are two of the most basic needs of human beings. However, as
society evolved, food and drink became also a strong cultural aspect, being
able to describe strong differences among people. Traditional methods used to
analyze cross-cultural differences are mainly based on surveys and, for this
reason, they are very difficult to represent a significant statistical sample
at a global scale. In this paper, we propose a new methodology to identify
cultural boundaries and similarities across populations at different scales
based on the analysis of Foursquare check-ins. This approach might be useful
not only for economic purposes, but also to support existing and novel
marketing and social applications. Our methodology consists of the following
steps. First, we map food and drink related check-ins extracted from Foursquare
into users' cultural preferences. Second, we identify particular individual
preferences, such as the taste for a certain type of food or drink, e.g., pizza
or sake, as well as temporal habits, such as the time and day of the week when
an individual goes to a restaurant or a bar. Third, we show how to analyze this
information to assess the cultural distance between two countries, cities or
even areas of a city. Fourth, we apply a simple clustering technique, using
this cultural distance measure, to draw cultural boundaries across countries,
cities and regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1056</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1056</id><created>2014-04-03</created><authors><author><keyname>Dosa</keyname><forenames>Gyorgy</forenames></author><author><keyname>Epstein</keyname><forenames>Leah</forenames></author></authors><title>Online bin packing with cardinality constraints revisited</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bin packing with cardinality constraints is a bin packing problem where an
upper bound k \geq 2 on the number of items packed into each bin is given, in
addition to the standard constraint on the total size of items packed into a
bin. We study the online scenario where items are presented one by one. We
analyze it with respect to the absolute competitive ratio and prove tight
bounds of 2 for any k \geq 4. We show that First Fit also has an absolute
competitive ratio of 2 for k=4, but not for larger values of k, and we present
a complete analysis of its asymptotic competitive ratio for all values of k
\geq 5. Additionally, we study the case of small $k$ with respect to the
asymptotic competitive ratio and the absolute competitive ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1059</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1059</id><created>2014-04-03</created><authors><author><keyname>Epstein</keyname><forenames>Leah</forenames></author><author><keyname>Levin</keyname><forenames>Asaf</forenames></author></authors><title>Minimum total weighted completion time: Faster approximation schemes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study classic scheduling problems on uniformly related machines. Efficient
polynomial time approximation schemes (EPTAS's) are fast and practical
approximation schemes. New methods and techniques are essential in developing
such improved approximation schemes, and their design is a primary goal of this
research agenda. We present EPTAS's for the scheduling problem of a set of jobs
on uniformly related machines so as to minimize the total weighted completion
time, both for the case with release dates and its special case without release
dates. These problems are NP-hard in the strong sense, and therefore EPTAS's
are the best possible approximation schemes unless P=NP. Previously, only
PTAS's were known for these two problems, while an EPTAS was known only for the
special case of identical machines without release dates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1066</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1066</id><created>2014-04-03</created><authors><author><keyname>Tyree</keyname><forenames>Stephen</forenames></author><author><keyname>Gardner</keyname><forenames>Jacob R.</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Agrawal</keyname><forenames>Kunal</forenames></author><author><keyname>Tran</keyname><forenames>John</forenames></author></authors><title>Parallel Support Vector Machines in Practice</title><categories>cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we evaluate the performance of various parallel optimization
methods for Kernel Support Vector Machines on multicore CPUs and GPUs. In
particular, we provide the first comparison of algorithms with explicit and
implicit parallelization. Most existing parallel implementations for multi-core
or GPU architectures are based on explicit parallelization of Sequential
Minimal Optimization (SMO)---the programmers identified parallelizable
components and hand-parallelized them, specifically tuned for a particular
architecture. We compare these approaches with each other and with implicitly
parallelized algorithms---where the algorithm is expressed such that most of
the work is done within few iterations with large dense linear algebra
operations. These can be computed with highly-optimized libraries, that are
carefully parallelized for a large variety of parallel platforms. We highlight
the advantages and disadvantages of both approaches and compare them on various
benchmark data sets. We find an approximate implicitly parallel algorithm which
is surprisingly efficient, permits a much simpler implementation, and leads to
unprecedented speedups in SVM training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1068</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1068</id><created>2014-04-03</created><authors><author><keyname>Martin-Vega</keyname><forenames>F. Javier</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author><author><keyname>Gomez</keyname><forenames>Gerardo</forenames></author><author><keyname>Aguayo-Torres</keyname><forenames>Mari Carmen</forenames></author></authors><title>Multi-User Coverage Probability of Uplink Cellular Systems: a Stochastic
  Geometry Approach</title><categories>cs.IT math.IT</categories><comments>6 pages, 9 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the coverage probability of multi-user uplink cellular networks
with fractional power control. We use a stochastic geometry approach where the
mobile users are distributed as a Poisson Point Process (PPP), whereas the
serving base station (BS) is placed at the origin. Using conditional thinning,
we are able to calculate the coverage probability of $k$ users which are
allocated a set of orthogonal resources in the cell of interest, obtaining
analytical expressions for this probability considering their respective
distances to the serving BS. These expressions give useful insights on the
interplay between the power control policy, the interference level and the
degree of fairness among different users in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1069</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1069</id><created>2014-04-03</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Rewarding evolutionary fitness with links between populations promotes
  cooperation</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><comments>7 two-column pages, 5 figures; accepted for publication in Journal of
  Theoretical Biology</comments><journal-ref>J. Theor. Biol. 349 (2014) 50-56</journal-ref><doi>10.1016/j.jtbi.2014.01.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolution of cooperation in the prisoner's dilemma and the public goods game
is studied, where initially players belong to two independent structured
populations. Simultaneously with the strategy evolution, players whose current
utility exceeds a threshold are rewarded by an external link to a player
belonging to the other population. Yet as soon as the utility drops below the
threshold, the external link is terminated. The rewarding of current
evolutionary fitness thus introduces a time-varying interdependence between the
two populations. We show that, regardless of the details of the evolutionary
game and the interaction structure, the self-organization of fitness and reward
gives rise to distinguished players that act as strong catalysts of cooperative
behavior. However, there also exist critical utility thresholds beyond which
distinguished players are no longer able to percolate. The interdependence
between the two populations then vanishes, and cooperators are forced to rely
on traditional network reciprocity alone. We thus demonstrate that a simple
strategy-independent form of rewarding may significantly expand the scope of
cooperation on structured populations. The formation of links outside the
immediate community seems particularly applicable in human societies, where an
individual is typically member in many different social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1087</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1087</id><created>2014-04-03</created><authors><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author><author><keyname>Leal</keyname><forenames>Katia</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author><author><keyname>Thraves</keyname><forenames>Christopher</forenames></author></authors><title>Dynamic Windows Scheduling with Reallocation</title><categories>cs.DS</categories><comments>6 figures</comments><msc-class>68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Windows Scheduling problem. The problem is a restricted
version of Unit-Fractions Bin Packing, and it is also called Inventory
Replenishment in the context of Supply Chain. In brief, the problem is to
schedule the use of communication channels to clients. Each client ci is
characterized by an active cycle and a window wi. During the period of time
that any given client ci is active, there must be at least one transmission
from ci scheduled in any wi consecutive time slots, but at most one
transmission can be carried out in each channel per time slot. The goal is to
minimize the number of channels used. We extend previous online models, where
decisions are permanent, assuming that clients may be reallocated at some cost.
We assume that such cost is a constant amount paid per reallocation. That is,
we aim to minimize also the number of reallocations. We present three online
reallocation algorithms for Windows Scheduling. We evaluate experimentally
these protocols showing that, in practice, all three achieve constant amortized
reallocations with close to optimal channel usage. Our simulations also expose
interesting trade-offs between reallocations and channel usage. We introduce a
new objective function for WS with reallocations, that can be also applied to
models where reallocations are not possible. We analyze this metric for one of
the algorithms which, to the best of our knowledge, is the first online WS
protocol with theoretical guarantees that applies to scenarios where clients
may leave and the analysis is against current load rather than peak load. Using
previous results, we also observe bounds on channel usage for one of the
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1089</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1089</id><created>2014-04-03</created><updated>2014-09-21</updated><authors><author><keyname>Horowitz</keyname><forenames>Matanya B.</forenames></author><author><keyname>Damle</keyname><forenames>Anil</forenames></author><author><keyname>Burdick</keyname><forenames>Joel W.</forenames></author></authors><title>Linear Hamilton Jacobi Bellman Equations in High Dimensions</title><categories>math.OC cs.SY</categories><comments>8 pages. Accepted to CDC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hamilton Jacobi Bellman Equation (HJB) provides the globally optimal
solution to large classes of control problems. Unfortunately, this generality
comes at a price, the calculation of such solutions is typically intractible
for systems with more than moderate state space size due to the curse of
dimensionality. This work combines recent results in the structure of the HJB,
and its reduction to a linear Partial Differential Equation (PDE), with methods
based on low rank tensor representations, known as a separated representations,
to address the curse of dimensionality. The result is an algorithm to solve
optimal control problems which scales linearly with the number of states in a
system, and is applicable to systems that are nonlinear with stochastic forcing
in finite-horizon, average cost, and first-exit settings. The method is
demonstrated on inverted pendulum, VTOL aircraft, and quadcopter models, with
system dimension two, six, and twelve respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1097</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1097</id><created>2014-04-03</created><authors><author><keyname>Im</keyname><forenames>Sungjin</forenames></author><author><keyname>Kulkarni</keyname><forenames>Janardhan</forenames></author><author><keyname>Munagala</keyname><forenames>Kamesh</forenames></author></authors><title>Competitive Algorithms from Competitive Equilibria: Non-Clairvoyant
  Scheduling under Polyhedral Constraints</title><categories>cs.DS</categories><comments>Accepted for publication in STOC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study a general scheduling problem that we term the Packing
Scheduling problem. In this problem, jobs can have different arrival times and
sizes; a scheduler can process job $j$ at rate $x_j$, subject to arbitrary
packing constraints over the set of rates ($\vec{x}$) of the outstanding jobs.
The PSP framework captures a variety of scheduling problems, including the
classical problems of unrelated machines scheduling, broadcast scheduling, and
scheduling jobs of different parallelizability. It also captures scheduling
constraints arising in diverse modern environments ranging from individual
computer architectures to data centers. More concretely, PSP models
multidimensional resource requirements and parallelizability, as well as
network bandwidth requirements found in data center scheduling.
  In this paper, we design non-clairvoyant online algorithms for PSP and its
special cases -- in this setting, the scheduler is unaware of the sizes of
jobs. Our two main results are, 1) a constant competitive algorithm for
minimizing total weighted completion time for PSP and 2)a scalable algorithm
for minimizing the total flow-time on unrelated machines, which is a special
case of PSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1100</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1100</id><created>2014-04-03</created><authors><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author></authors><title>A Tutorial on Principal Component Analysis</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) is a mainstay of modern data analysis - a
black box that is widely used but (sometimes) poorly understood. The goal of
this paper is to dispel the magic behind this black box. This manuscript
focuses on building a solid intuition for how and why principal component
analysis works. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind PCA. This tutorial does not shy away
from explaining the ideas informally, nor does it shy away from the
mathematics. The hope is that by addressing both aspects, readers of all levels
will be able to gain a better understanding of PCA as well as the when, the how
and the why of applying this technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1103</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1103</id><created>2014-04-03</created><authors><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author></authors><title>A Polylogarithmic PRG for Degree $2$ Threshold Functions in the Gaussian
  Setting</title><categories>cs.CC</categories><msc-class>60G15</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a new pseudorandom generator against degree 2 polynomial threshold
functions in the Gaussian setting. We manage to achieve $\epsilon$ error with
seed length polylogarithmic in $\epsilon$ and the dimension, and exponential
improvement over previous constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1107</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1107</id><created>2014-04-03</created><updated>2014-04-17</updated><authors><author><keyname>Zhu</keyname><forenames>Junjie</forenames></author><author><keyname>Govindasamy</keyname><forenames>Siddhartan</forenames></author><author><keyname>Hwang</keyname><forenames>Jeff</forenames></author></authors><title>Performance of Multiantenna Linear MMSE Receivers in Doubly Stochastic
  Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications and under second
  round of revision. arXiv admin note: text overlap with arXiv:1210.0558</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique is presented to characterize the
Signal-to-Interference-plus-Noise Ratio (SINR) of a representative link with a
multiantenna linear Minimum-Mean-Square-Error receiver in a wireless network
with transmitting nodes distributed according to a doubly stochastic process,
which is a generalization of the Poisson point process. The cumulative
distribution function of the SINR of the representative link is derived
assuming independent Rayleigh fading between antennas. Several representative
spatial node distributions are considered, including networks with both
deterministic and random clusters, strip networks (used to model roadways,
e.g.), hard-core networks and networks with generalized path-loss models. In
addition, it is shown that if the number of antennas at the representative
receiver is increased linearly with the nominal node density, the
signal-to-interference ratio converges in distribution to a random variable
that is non-zero in general, and a positive constant in certain cases. This
result indicates that to the extent that the system assumptions hold, it is
possible to scale such networks by increasing the number of receiver antennas
linearly with the node density. The results presented here are useful in
characterizing the performance of multiantenna wireless networks in more
general network models than what is currently available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1108</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1108</id><created>2014-04-03</created><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Honghai</forenames></author><author><keyname>Zhao</keyname><forenames>Baohua</forenames></author><author><keyname>Rangarajan</keyname><forenames>Sampath</forenames></author></authors><title>A Collaborative Framework for In-network Video Caching in Mobile
  Networks</title><categories>cs.NI</categories><journal-ref>Proceedings of IEEE Conference on Sensor, Mesh and Ad Hoc
  Communications and Networks (SECON) 2013, pp. 406-414</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to explosive growth of online video content in mobile wireless networks,
in-network caching is becoming increasingly important to improve the end-user
experience and reduce the Internet access cost for mobile network operators.
However, caching is a difficult problem due to the very large number of online
videos and video requests,limited capacity of caching nodes, and limited
bandwidth of in-network links. Existing solutions that rely on static
configurations and average request arrival rates are insufficient to handle
dynamic request patterns effectively. In this paper, we propose a dynamic
collaborative video caching framework to be deployed in mobile networks. We
decompose the caching problem into a content placement subproblem and a
source-selection subproblem. We then develop SRS (System capacity Reservation
Strategy) to solve the content placement subproblem, and LinkShare, an adaptive
traffic-aware algorithm to solve the source selection subproblem. Our framework
supports congestion avoidance and allows merging multiple requests for the same
video into one request. We carry extensive simulations to validate the proposed
schemes. Simulation results show that our SRS algorithm achieves performance
within 1-3% of the optimal values and LinkShare significantly outperforms
existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1112</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1112</id><created>2014-04-03</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Negrete-Pincetic</keyname><forenames>Matias</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Varaiya</keyname><forenames>Pravin</forenames></author></authors><title>Duration-Differentiated Services in Electricity</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of renewable sources poses challenges at the operational and
economic levels of the power grid. In terms of keeping the balance between
supply and demand, the usual scheme of supply following load may not be
appropriate for large penetration levels of uncertain and intermittent
renewable supply. In this paper, we focus on an alternative scheme in which the
load follows the supply, exploiting the flexibility associated with the demand
side. We consider a model of flexible loads that are to be serviced by
zero-marginal cost renewable power together with conventional generation if
necessary. Each load demands 1 kW for a specified number of time slots within
an operational period. The flexibility of a load resides in the fact that the
service may be delivered over any slots within the operational period. Loads
therefore require flexible energy services that are differentiated by the
demanded duration. We focus on two problems associated with
durations-differentiated loads. The first problem deals with the operational
decisions that a supplier has to make to serve a given set of duration
differentiated loads. The second problem focuses on a market implementation for
duration differentiated services. We give necessary and sufficient conditions
under which the available power can service the loads, and we describe an
algorithm that constructs an appropriate allocation. In the event the available
supply is inadequate, we characterize the minimum amount of power that must be
purchased to service the loads. Next we consider a forward market where
consumers can purchase duration differentiated energy services. We first
characterize social welfare maximizing allocations in this forward market and
then show the existence of an efficient competitive equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1113</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1113</id><created>2014-04-03</created><updated>2014-07-08</updated><authors><author><keyname>Anwar</keyname><forenames>Ahmed H.</forenames></author><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Guizani</keyname><forenames>Mohsen</forenames></author></authors><title>Interference-Based Optimal Power-Efficient Access Scheme for Cognitive
  Radio Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new optimization-based access strategy of
multipacket reception (MPR) channel for multiple secondary users (SUs)
accessing the primary user (PU) spectrum opportunistically. We devise an
analytical model that realizes the multipacket access strategy of SUs that
maximizes the throughput of individual backlogged SUs subject to queue
stability of the PU. All the network receiving nodes have MPR capability. We
aim at maximizing the throughput of the individual SUs such that the PU's queue
is maintained stable. Moreover, we are interested in providing an
energy-efficient cognitive scheme. Therefore, we include energy constraints on
the PU and SU average transmitted energy to the optimization problem. Each SU
accesses the medium with certain probability that depends on the PU's activity,
i.e., active or inactive. The numerical results show the advantage in terms of
SU throughput of the proposed scheme over the conventional access scheme, where
the SUs access the channel randomly with fixed power when the PU is sensed to
be idle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1116</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1116</id><created>2014-04-03</created><authors><author><keyname>Bhandari</keyname><forenames>Ayush</forenames></author><author><keyname>Kadambi</keyname><forenames>Achuta</forenames></author><author><keyname>Whyte</keyname><forenames>Refael</forenames></author><author><keyname>Barsi</keyname><forenames>Christopher</forenames></author><author><keyname>Feigin</keyname><forenames>Micha</forenames></author><author><keyname>Dorrington</keyname><forenames>Adrian</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author></authors><title>Resolving Multi-path Interference in Time-of-Flight Imaging via
  Modulation Frequency Diversity and Sparse Regularization</title><categories>cs.CV cs.IT math.IT physics.optics</categories><comments>11 Pages, 4 figures, appeared with minor changes in Optics Letters</comments><journal-ref>Optics Letters, Vol. 39, Issue 6, pp. 1705-1708 (2014)</journal-ref><doi>10.1364/OL.39.001705</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Time-of-flight (ToF) cameras calculate depth maps by reconstructing phase
shifts of amplitude-modulated signals. For broad illumination or transparent
objects, reflections from multiple scene points can illuminate a given pixel,
giving rise to an erroneous depth map. We report here a sparsity regularized
solution that separates K-interfering components using multiple modulation
frequency measurements. The method maps ToF imaging to the general framework of
spectral estimation theory and has applications in improving depth profiles and
exploiting multiple scattering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1124</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1124</id><created>2014-04-03</created><updated>2014-04-17</updated><authors><author><keyname>Li</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Liu</keyname><forenames>Meilin</forenames></author></authors><title>A Task Allocation Schema Based on Response Time Optimization in Cloud
  Computing</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1403.5012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a newly emerging distributed computing which is evolved
from Grid computing. Task scheduling is the core research of cloud computing
which studies how to allocate the tasks among the physical nodes so that the
tasks can get a balanced allocation or each task's execution cost decreases to
the minimum or the overall system performance is optimal. Unlike the previous
task slices' sequential execution of an independent task in the model of which
the target is processing time, we build a model that targets at the response
time, in which the task slices are executed in parallel. Then we give its
solution with a method based on an improved adjusting entropy function. At
last, we design a new task scheduling algorithm. Experimental results show that
the response time of our proposed algorithm is much lower than the
game-theoretic algorithm and balanced scheduling algorithm and compared with
the balanced scheduling algorithm, game-theoretic algorithm is not necessarily
superior in parallel although its objective function value is better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1129</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1129</id><created>2014-04-03</created><updated>2014-07-25</updated><authors><author><keyname>Peng</keyname><forenames>Chengyu</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author><author><keyname>Ko</keyname><forenames>Manchor</forenames></author></authors><title>An Efficient Two-Stage Sparse Representation Method</title><categories>cs.CV</categories><comments>21 pages, 2 figures, 4 tables</comments><acm-class>G.1.6; I.4.10</acm-class><doi>10.1142/S0218001416510010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are a large number of methods for solving under-determined linear
inverse problem. Many of them have very high time complexity for large
datasets. We propose a new method called Two-Stage Sparse Representation (TSSR)
to tackle this problem. We decompose the representing space of signals into two
parts, the measurement dictionary and the sparsifying basis. The dictionary is
designed to approximate a sub-Gaussian distribution to exploit its
concentration property. We apply sparse coding to the signals on the dictionary
in the first stage, and obtain the training and testing coefficients
respectively. Then we design the basis to approach an identity matrix in the
second stage, to acquire the Restricted Isometry Property (RIP) and
universality property. The testing coefficients are encoded over the basis and
the final representing coefficients are obtained. We verify that the projection
of testing coefficients onto the basis is a good approximation of the signal
onto the representing space. Since the projection is conducted on a much
sparser space, the runtime is greatly reduced. For concrete realization, we
provide an instance for the proposed TSSR. Experiments on four biometrics
databases show that TSSR is effective and efficient, comparing with several
classical methods for solving linear inverse problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1140</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1140</id><created>2014-04-03</created><updated>2014-12-19</updated><authors><author><keyname>Amato</keyname><forenames>Christopher</forenames></author><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author></authors><title>Scalable Planning and Learning for Multiagent POMDPs: Extended Version</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online, sample-based planning algorithms for POMDPs have shown great promise
in scaling to problems with large state spaces, but they become intractable for
large action and observation spaces. This is particularly problematic in
multiagent POMDPs where the action and observation space grows exponentially
with the number of agents. To combat this intractability, we propose a novel
scalable approach based on sample-based planning and factored value functions
that exploits structure present in many multiagent settings. This approach
applies not only in the planning case, but also in the Bayesian reinforcement
learning setting. Experimental results show that we are able to provide high
quality solutions to large multiagent planning and learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1142</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1142</id><created>2014-04-03</created><updated>2014-08-01</updated><authors><author><keyname>Zhou</keyname><forenames>Yifan</forenames></author><author><keyname>Zhao</keyname><forenames>Zhifeng</forenames></author><author><keyname>Ying</keyname><forenames>Qianlan</forenames></author><author><keyname>Li</keyname><forenames>Rongpeng</forenames></author><author><keyname>Zhou</keyname><forenames>Xuan</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author></authors><title>Two-tier Spatial Modeling of Base Stations in Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poisson Point Process (PPP) has been widely adopted as an efficient model for
the spatial distribution of base stations (BSs) in cellular networks. However,
real BSs deployment are rarely completely random, due to environmental impact
on actual site planning. Particularly, for multi-tier heterogeneous cellular
networks, operators have to place different BSs according to local coverage and
capacity requirement, and the diversity of BSs' functions may result in
different spatial patterns on each networking tier. In this paper, we consider
a two-tier scenario that consists of macrocell and microcell BSs in cellular
networks. By analyzing these two tiers separately and applying both classical
statistics and network performance as evaluation metrics, we obtain accurate
spatial model of BSs deployment for each tier. Basically, we verify the
inaccuracy of using PPP in BS locations modeling for either macrocells or
microcells. Specifically, we find that the first tier with macrocell BSs is
dispersed and can be precisely modelled by Strauss point process, while Matern
cluster process captures the second tier's aggregation nature very well. These
statistical models coincide with the inherent properties of macrocell and
microcell BSs respectively, thus providing a new perspective in understanding
the relationship between spatial structure and operational functions of BSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1143</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1143</id><created>2014-04-03</created><updated>2014-06-19</updated><authors><author><keyname>Ying</keyname><forenames>Qianlan</forenames></author><author><keyname>Zhao</keyname><forenames>Zhifeng</forenames></author><author><keyname>Zhou</keyname><forenames>Yifan</forenames></author><author><keyname>Li</keyname><forenames>Rongpeng</forenames></author><author><keyname>Zhou</keyname><forenames>Xuan</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author></authors><title>Characterizing Spatial Patterns of Base Stations in Cellular Networks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The topology of base stations (BSs) in cellular networks, serving as a basis
of networking performance analysis, is considered to be obviously distinctive
with the traditional hexagonal grid or square lattice model, thus stimulating a
fundamental rethinking. Recently, stochastic geometry based models, especially
the Poisson point process (PPP), attracts an ever-increasing popularity in
modeling BS deployment of cellular networks due to its merits of tractability
and capability for capturing nonuniformity. In this study, a detailed
comparison between common stochastic models and real BS locations is performed.
Results indicate that the PPP fails to precisely characterize either urban or
rural BS deployment. Furthermore, the topology of real data in both regions are
examined and distinguished by statistical methods according to the point
interaction trends they exhibit. By comparing the corresponding real data with
aggregative point process models as well as repulsive point process models, we
verify that the capacity-centric deployment in urban areas can be modeled by
typical aggregative processes such as the Matern cluster process, while the
coverage-centric deployment in rural areas can be modeled by representative
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1144</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1144</id><created>2014-04-03</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>AIS-MACA- Z: MACA based Clonal Classifier for Splicing Site, Protein
  Coding and Promoter Region Identification in Eukaryotes</title><categories>cs.CE cs.LG</categories><comments>6,1-6 Pages, Journal of Artificial Intelligence Research &amp; Advances
  Volume 1, Issue 1,2014. arXiv admin note: text overlap with arXiv:1403.5933,
  arXiv:1404.0453</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bioinformatics incorporates information regarding biological data storage,
accessing mechanisms and presentation of characteristics within this data. Most
of the problems in bioinformatics and be addressed efficiently by computer
techniques. This paper aims at building a classifier based on Multiple
Attractor Cellular Automata (MACA) which uses fuzzy logic with version Z to
predict splicing site, protein coding and promoter region identification in
eukaryotes. It is strengthened with an artificial immune system technique
(AIS), Clonal algorithm for choosing rules of best fitness. The proposed
classifier can handle DNA sequences of lengths 54,108,162,252,354. This
classifier gives the exact boundaries of both protein and promoter regions with
an average accuracy of 90.6%. This classifier can predict the splicing site
with 97% accuracy. This classifier was tested with 1, 97,000 data components
which were taken from Fickett &amp; Toung , EPDnew, and other sequences from a
renowned medical university.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1148</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1148</id><created>2014-04-03</created><authors><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Brandt-Pearce</keyname><forenames>Maite</forenames></author></authors><title>Hadamard Coded Modulation: An Alternative to OFDM for Optical Wireless
  Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing (OFDM) is a modulation technique
susceptible to source, channel and amplifier nonlinearities because of its high
peak-to-average ratio (PAPR). The distortion gets worse by increasing the
average power of the OFDM signals since larger portion of the signals are
affected by nonlinearity. In this paper we introduce Hadamard coded modulation
(HCM) that uses the fast Walsh-Hadamard transform (FWHT) to modulate data as an
alternative technique to OFDM in direct-detection wireless optical systems.
This technique is shown to have a better performance for high average optical
power scenarios because of its small PAPR, and can be used instead of OFDM in
two scenarios: 1) in optical systems that require high average optical powers
such as visible light communications (VLC), and 2) in optical wireless systems
unconstrained by average power, for which HCM achieves lower bit error rate
(BER) compared to OFDM. The power efficiency of HCM can be improved by removing
a part of the signal's DC bias without losing any information. In this way, the
amplitude of the transmitted signal is decreased and the signals become less
susceptible to nonlinearity. Interleaving can be applied on HCM to make the
resulting signals resistent against inter-symbol interference (ISI) effects in
dispersive channels by uniformly distributing the interference over all
symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1151</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1151</id><created>2014-04-04</created><updated>2014-10-27</updated><authors><author><keyname>Kulkarni</keyname><forenames>Sadanand A.</forenames></author><author><keyname>Borde</keyname><forenames>Prashant L.</forenames></author><author><keyname>Manza</keyname><forenames>Ramesh R.</forenames></author><author><keyname>Yannawar</keyname><forenames>Pravin L.</forenames></author></authors><title>Recognition of Handwritten MODI Numerals using Hu and Zernike features</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to the paper was
  rejected by journal with a reson &quot;paper was not suitable for the journal&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handwritten automatic character recognition has attracted many researchers
all over the world to contribute automatic character recognition domain. Shape
identification and feature extraction is very important part of any character
recognition system and success of method is highly dependent on selection of
features. However feature extraction is the most important step in defining the
shape of the character as precisely and as uniquely as possible. This is indeed
the most important step and complex task as well and achieved success by using
invariance property, irrespective of position and orientation. Zernike moments
describes shape, identify rotation invariant due to its Orthogonality property.
MODI is an ancient script of India had cursive and complex representation of
characters. The work described in this paper presents efficiency of Zernike
moments over Hus moment for automatic recognition of handwritten MODI numerals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1158</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1158</id><created>2014-04-04</created><authors><author><keyname>Gaba</keyname><forenames>Siddharth</forenames></author><author><keyname>Sheridan</keyname><forenames>Patrick</forenames></author><author><keyname>Du</keyname><forenames>Chao</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author></authors><title>3D Vertical Dual-Layer Oxide Memristive Devices for Neuromorphic
  Computing</title><categories>cond-mat.other cs.ET</categories><comments>11 Pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual-layer resistive switching devices with horizontal W electrodes, vertical
Pd electrodes and WOx switching layer formed at the sidewall of the horizontal
electrodes have been fabricated and characterized. The devices exhibit
well-characterized analog switching characteristics and small mismatch in
electrical characteristics for devices formed at the two layers. The
three-dimensional (3D) vertical device structure allows higher storage density
and larger connectivity for neuromorphic computing applications. We show the
vertical devices exhibit potentiation and depression characteristics similar to
planar devices, and can be programmed independently with no crosstalk between
the layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1165</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1165</id><created>2014-04-04</created><updated>2014-10-21</updated><authors><author><keyname>Xia</keyname><forenames>YiMing</forenames></author></authors><title>A new multiresolution finite element method based on a multiresolution
  quadrilateral plate element</title><categories>math.NA cs.NA</categories><comments>17 pages</comments><journal-ref>Journal of Coupled Systems and Multiscale Dynamics 2014,Vol
  2(2),1-10</journal-ref><doi>10.1166/jcsmd.2014.1040</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A new multiresolution quadrilateral plate element is proposed and a
multiresolution finite element method is hence presented. The multiresolution
analysis (MRA) framework is formulated out of a mutually nesting displacement
subspace sequence, whose basis functions are constructed of scaling and
shifting on the element domain of basic node shape function. The basic node
shape function is constructed by extending shape function around a specific
node. The MRA endows the proposed element with the resolution level (RL) to
adjust the element node number, thus modulating structural analysis accuracy
accordingly. As a result, the traditional 4-node quadrilateral plate element
and method is a monoresolution one and also a special case of the proposed
element and method. The meshing for the monoresolution plate element model is
based on the empiricism while the RL adjusting for the multiresolution is laid
on the rigorous mathematical basis. The accuracy of a structural analysis is
actually determined by the RL, not by the mesh. The rational MRA enable the
implementation of the multiresolution element method to be more rational and
efficient than that of the conventional monoresolution plate element method or
other corresponding MRA methods such as the wavelet finite element method, the
meshfree method, and the natural element method etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1168</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1168</id><created>2014-04-04</created><authors><author><keyname>Chowdhury</keyname><forenames>Nilanjan Roy</forenames></author><author><keyname>Sukumar</keyname><forenames>Srikant</forenames></author></authors><title>Persistence based analysis of consensus protocols for dynamic graph
  networks</title><categories>cs.SY</categories><comments>This article contains 7 pages and includes 4 figures. it is accepted
  in 13th European Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the consensus problem involving agents with
time-varying singularities in the dynamics or communication in undirected graph
networks. Existing results provide control laws which guarantee asymptotic
consensus. These results are based on the analysis of a system switching
between piecewise constant and time-invariant dynamics. This work introduces a
new analysis technique relying upon classical notions of persistence of
excitation to study the convergence properties of the time-varying multi-agent
dynamics. Since the individual edge weights pass through singularities and vary
with time, the closed-loop dynamics consists of a non-autonomous linear system.
Instead of simplifying to a piecewise continuous switched system as in
literature, smooth variations in edge weights are allowed, albeit assuming an
underlying persistence condition which characterizes sufficient inter-agent
communication to reach consensus. The consensus task is converted to
edge-agreement in order to study a stabilization problem to which classical
persistence based results apply. The new technique allows precise computation
of the rate of convergence to the consensus value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1178</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1178</id><created>2014-04-04</created><authors><author><keyname>Madue&#xf1;o</keyname><forenames>Germ&#xe1;n Corrales</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Reliable Reporting for Massive M2M Communications with Periodic Resource
  Pooling</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter considers a wireless M2M communication scenario with a massive
number of M2M devices. Each device needs to send its reports within a given
deadline and with certain reliability, e. g. 99.99%. A pool of resources
available to all M2M devices is periodically available for transmission. The
number of transmissions required by an M2M device within the pool is random due
to two reasons - random number of arrived reports since the last reporting
opportunity and requests for retransmission due to random channel errors. We
show how to dimension the pool of M2M-dedicated resources in order to guarantee
the desired reliability of the report delivery within the deadline. The fact
that the pool of resources is used by a massive number of devices allows to
base the dimensioning on the central limit theorem. The results are interpreted
in the context of LTE, but they are applicable to any M2M communication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1183</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1183</id><created>2014-04-04</created><updated>2014-04-14</updated><authors><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Waldspurger</keyname><forenames>Ir&#xe8;ne</forenames></author></authors><title>Phase retrieval for the Cauchy wavelet transform</title><categories>math.FA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the phase retrieval problem in which one tries to reconstruct a
function from the modulus of its wavelet transform. We study the unicity and
stability of the reconstruction. In the case where the wavelets are Cauchy
wavelets, we prove that the modulus of the wavelet transform uniquely
determines the function up to a global phase. We show that the reconstruction
operator is continuous but not uniformly continuous. We describe how to
construct pairs of functions which are far away in $L^2$-norm but whose wavelet
transforms are very close, in modulus. The principle is to modulate the wavelet
transform of a fixed initial function by a phase which varies slowly in both
time and frequency. This construction seems to cover all the instabilities that
we observe in practice; we give a partial formal justification to this fact.
Finally, we describe an exact reconstruction algorithm and use it to
numerically confirm our analysis of the stability question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1191</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1191</id><created>2014-04-04</created><updated>2015-05-16</updated><authors><author><keyname>Abdullah</keyname><forenames>Amirali</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>A directed isoperimetric inequality with application to Bregman near
  neighbor lower bounds</title><categories>cs.CG cs.CC</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bregman divergences $D_\phi$ are a class of divergences parametrized by a
convex function $\phi$ and include well known distance functions like
$\ell_2^2$ and the Kullback-Leibler divergence. There has been extensive
research on algorithms for problems like clustering and near neighbor search
with respect to Bregman divergences, in all cases, the algorithms depend not
just on the data size $n$ and dimensionality $d$, but also on a structure
constant $\mu \ge 1$ that depends solely on $\phi$ and can grow without bound
independently.
  In this paper, we provide the first evidence that this dependence on $\mu$
might be intrinsic. We focus on the problem of approximate near neighbor search
for Bregman divergences. We show that under the cell probe model, any
non-adaptive data structure (like locality-sensitive hashing) for
$c$-approximate near-neighbor search that admits $r$ probes must use space
$\Omega(n^{1 + \frac{\mu}{c r}})$. In contrast, for LSH under $\ell_1$ the best
bound is $\Omega(n^{1+\frac{1}{cr}})$.
  Our new tool is a directed variant of the standard boolean noise operator. We
show that a generalization of the Bonami-Beckner hypercontractivity inequality
exists &quot;in expectation&quot; or upon restriction to certain subsets of the Hamming
cube, and that this is sufficient to prove the desired isoperimetric inequality
that we use in our data structure lower bound.
  We also present a structural result reducing the Hamming cube to a Bregman
cube. This structure allows us to obtain lower bounds for problems under
Bregman divergences from their $\ell_1$ analog. In particular, we get a
(weaker) lower bound for approximate near neighbor search of the form
$\Omega(n^{1 + \frac{1}{cr}})$ for an $r$-query non-adaptive data structure,
and new cell probe lower bounds for a number of other near neighbor questions
in Bregman space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1193</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1193</id><created>2014-04-04</created><authors><author><keyname>Kang</keyname><forenames>Xin</forenames></author><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Cost minimization for fading channels with energy harvesting and
  conventional energy</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate resource allocation strategies for a
point-to-point wireless communications system with hybrid energy sources
consisting of an energy harvester and a conventional energy source. In
particular, as an incentive to promote the use of renewable energy, we assume
that the renewable energy has a lower cost than the conventional energy. Then,
by assuming that the non-causal information of the energy arrivals and the
channel power gains are available, we minimize the total energy cost of such a
system over $N$ fading slots under a proposed outage constraint together with
the energy harvesting constraints. The outage constraint requires a minimum
fixed number of slots to be reliably decoded, and thus leads to a mixed-integer
programming formulation for the optimization problem. This constraint is
useful, for example, if an outer code is used to recover all the data bits.
Optimal linear time algorithms are obtained for two extreme cases, i.e., the
number of outage slot is $1$ or $N-1$. For the general case, a lower bound
based on the linear programming relaxation, and two suboptimal algorithms are
proposed. It is shown that the proposed suboptimal algorithms exhibit only a
small gap from the lower bound. We then extend the proposed algorithms to the
multi-cycle scenario in which the outage constraint is imposed for each cycle
separately. Finally, we investigate the resource allocation strategies when
only causal information on the energy arrivals and only channel statistics is
available. It is shown that the greedy energy allocation is optimal for this
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1198</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1198</id><created>2014-04-04</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Experiences in Developing Time-Critical Systems - The Case Study
  &quot;Production Cell&quot;</title><categories>cs.SE cs.LO</categories><comments>13 pages; 11 figures</comments><msc-class>68Q60</msc-class><acm-class>I.2.2</acm-class><journal-ref>T. Lindner and C. Lewerentz (eds.), Formal development of reactive
  systems - Case study production cell, Springer LNCS, Vol.891, p.297-311, 1995</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from an informal requirements description of a toy production cell
used in an intra-project competition in 1994, we give a formal specification
that is as close as possible to requirements. We use the deductive program
synthesis approach by Manna and Waldinger (1980) to obtain a verified TTL-like
circuitery to control the cell. The formal specification also covers mechanical
aspects and thus allows to reason not only about software issues but also about
issues of mechanical engineering. Besides an approach confined to first order
predicate logic with explicit, continuous time, an attempt is presented to
employ application specific user-defined logical operators to get a more
concise specification as well as proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1201</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1201</id><created>2014-04-04</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Regular Substitution Sets: A Means of Controlling E-Unification</title><categories>cs.FL cs.LO</categories><comments>64 pages; 35 figures</comments><msc-class>68Q70, 68Q42</msc-class><acm-class>F.4.3; I.2.2</acm-class><journal-ref>Jieh Hsiang (ed.), Rewriting Techniques and Applications, 6th Int.
  Conf., RTA-95, Springer LNCS 914, p.382-396, 1995</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for selecting solution constructors in narrowing is presented. The
method is based on a sort discipline that describes regular sets of ground
constructor terms as sorts. It is extended to cope with regular sets of ground
substitutions, thus allowing different sorts to be computed for terms with
different variable bindings. An algorithm for computing signatures of
equationally defined functions is given that allows potentially infinite
overloading. Applications to formal program development are sketched.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1214</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1214</id><created>2014-04-04</created><updated>2015-01-27</updated><authors><author><keyname>Bauer</keyname><forenames>Ulrich</forenames></author><author><keyname>Munk</keyname><forenames>Axel</forenames></author><author><keyname>Sieling</keyname><forenames>Hannes</forenames></author><author><keyname>Wardetzky</keyname><forenames>Max</forenames></author></authors><title>Persistence Barcodes versus Kolmogorov Signatures: Detecting Modes of
  One-Dimensional Signals</title><categories>math.ST cs.CG stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of estimating the number of modes (i.e., local
maxima) - a well known question in statistical inference - and we show how to
do so without presmoothing the data. To this end, we modify the ideas of
persistence barcodes by first relating persistence values in dimension one to
distances (with respect to the supremum norm) to the sets of functions with a
given number of modes, and subsequently working with norms different from the
supremum norm. As a particular case we investigate the Kolmogorov norm. We
argue that this modification has certain statistical advantages. We offer
confidence bands for the attendant Kolmogorov signatures, thereby allowing for
the selection of relevant signatures with a statistically controllable error.
As a result of independent interest, we show that taut strings minimize the
number of critical points for a very general class of functions. We illustrate
our results by several numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1215</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1215</id><created>2014-04-04</created><updated>2014-05-02</updated><authors><author><keyname>Goncharov</keyname><forenames>Sergey</forenames></author><author><keyname>Pattinson</keyname><forenames>Dirk</forenames></author></authors><title>Coalgebraic Weak Bisimulation from Recursive Equations over Monads</title><categories>cs.LO</categories><comments>final version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strong bisimulation for labelled transition systems is one of the most
fundamental equivalences in process algebra, and has been generalised to
numerous classes of systems that exhibit richer transition behaviour. Nearly
all of the ensuing notions are instances of the more general notion of
coalgebraic bisimulation. Weak bisimulation, however, has so far been much less
amenable to a coalgebraic treatment. Here we attempt to close this gap by
giving a coalgebraic treatment of (parametrized) weak equivalences, including
weak bisimulation. Our analysis requires that the functor defining the
transition type of the system is based on a suitable order-enriched monad,
which allows us to capture weak equivalences by least fixpoints of recursive
equations. Our notion is in agreement with existing notions of weak
bisimulations for labelled transition systems, probabilistic and weighted
systems, and simple Segala systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1225</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1225</id><created>2014-04-04</created><updated>2015-02-09</updated><authors><author><keyname>Felgenhauer</keyname><forenames>Bertram</forenames></author><author><keyname>Middeldorp</keyname><forenames>Aart</forenames></author><author><keyname>Zankl</keyname><forenames>Harald</forenames></author><author><keyname>van Oostrom</keyname><forenames>Vincent</forenames></author></authors><title>Layer Systems for Proving Confluence</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><doi>10.1145/2710017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce layer systems for proving generalizations of the modularity of
confluence for first-order rewrite systems. Layer systems specify how terms can
be divided into layers. We establish structural conditions on those systems
that imply confluence. Our abstract framework covers known results like
modularity, many-sorted persistence, layer-preservation and currying. We
present a counterexample to an extension of persistence to order-sorted
rewriting and derive new sufficient conditions for the extension to hold. All
our proofs are constructive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1227</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1227</id><created>2014-04-04</created><authors><author><keyname>Burghardt</keyname><forenames>Jochen</forenames></author></authors><title>Formale Entwicklung einer Steuerung f\&quot;ur eine Fertigungszelle mit
  SYSYFOS</title><categories>cs.LO cs.SE</categories><comments>in german; 66 pages; 29 figures</comments><msc-class>68Q60, 68N30</msc-class><acm-class>I.2.2; C.0</acm-class><journal-ref>Technical Report, GMD Arbeitspapier Vol.996, Jun 1996</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the synthesis approach of Manna and Waldinger, a formally specified and
verified control circuitery for a production cell was developped. Building an
appropriate formal language level, we could achieve a requirements
specification to the informal description. We demonstrated that the paradigm of
deductive synthesis can be applied to the development of complete verified
systems, including hardware and mechanics. We defined two domain-specific
logical operators that schematise frequent patterns in specification and proof
and hence allow a more concise and expressive presentation. In Burghardt
(1995), an english short version of this paper, without appendices, can be
found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1237</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1237</id><created>2014-04-04</created><authors><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Roumy</keyname><forenames>Aline</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Operational Rate-Distortion Performance of Single-source and Distributed
  Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2014.2316176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider correlated and distributed sources without cooperation at the
encoder. For these sources, we derive the best achievable performance in the
rate-distortion sense of any distributed compressed sensing scheme, under the
constraint of high--rate quantization. Moreover, under this model we derive a
closed--form expression of the rate gain achieved by taking into account the
correlation of the sources at the receiver and a closed--form expression of the
average performance of the oracle receiver for independent and joint
reconstruction. Finally, we show experimentally that the exploitation of the
correlation between the sources performs close to optimal and that the only
penalty is due to the missing knowledge of the sparsity support as in (non
distributed) compressed sensing. Even if the derivation is performed in the
large system regime, where signal and system parameters tend to infinity,
numerical results show that the equations match simulations for parameter
values of practical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1269</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1269</id><created>2014-03-31</created><authors><author><keyname>Zedini</keyname><forenames>Emna</forenames></author><author><keyname>Ansari</keyname><forenames>Imran Shafique</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Unified Performance Analysis of Mixed Line of Sight RF-FSO Fixed Gain
  Dual-Hop Transmission Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we carry out a unified performance analysis of a dual-hop fixed
gain relay system over asymmetric links composed of both radio-frequency (RF)
and unified free-space optics (FSO) under the effect of pointing errors. The RF
link is modeled by the Nakagami-$m$ fading channel and the FSO link by the
Gamma-Gamma fading channel subject to both types of detection techniques (i.e.
heterodyne detection and intensity modulation with direct detection (IM/DD)).
In particular, we derive new unified closed-form expressions for the cumulative
distribution function, the probability density function, the moment generation
function, and the moments of the end-to-end signal-to-noise ratio of these
systems in terms of the Meijer's G function. Based on these formulas, we offer
exact closed-form expressions for the outage probability, the higher-order
amount of fading, and the average bit-error rate of a variety of binary
modulations in terms of the Meijer's G function. Further, an exact closed-form
expression for the end-to-end ergodic capacity for the Nakagami-$m$-unified FSO
relay links is derived in terms of the bivariate G function. All the given
results are verified via Computer-based Monte-Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1270</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1270</id><created>2014-04-04</created><updated>2014-08-07</updated><authors><author><keyname>Boneva</keyname><forenames>Iovka</forenames></author><author><keyname>Gayo</keyname><forenames>Jose Emilio Labra</forenames></author><author><keyname>Hym</keyname><forenames>Samuel</forenames></author><author><keyname>Prud'hommeau</keyname><forenames>Eric G.</forenames></author><author><keyname>Solbrig</keyname><forenames>Harold</forenames></author><author><keyname>Staworko</keyname><forenames>S&#x142;awek</forenames></author></authors><title>Validating RDF with Shape Expressions</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose shape expression schema (ShEx), a novel schema formalism for
describing the topology of an RDF graph that uses regular bag expressions
(RBEs) to define constraints on the admissible neighborhood for the nodes of a
given type. We provide two alternative semantics, multi- and single-type,
depending on whether or not a node may have more than one type. We study the
expressive power of ShEx and study the complexity of the validation problem. We
show that the single-type semantics is strictly more expressive than the
multi-type semantics, single-type validation is generally intractable and
multi-type validation is feasible for a small class of RBEs. To further curb
the high computational complexity of validation, we propose a natural notion of
determinism and show that multi-type validation for the class of deterministic
schemas using single-occurrence regular bag expressions (SORBEs) is tractable.
Finally, we consider the problem of validating only a fragment of a graph with
preassigned types for some of its nodes, and argue that for deterministic ShEx
using SORBEs, multi-type validation can be performed efficiently and
single-type validation can be performed with a single pass over the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1271</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1271</id><created>2014-03-29</created><authors><author><keyname>Mamun</keyname><forenames>Md. Selim Al</forenames></author><author><keyname>Karmaker</keyname><forenames>B. K.</forenames></author></authors><title>Design of Reversible Counter</title><categories>cs.OH</categories><comments>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 5 Issue 1, 2014</comments><doi>10.14569/IJACSA.2014.050117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a research work on the design and synthesis of
sequential circuits and flip-flops that are available in digital arena; and
describes a new synthesis design of reversible counter that is optimized in
terms of quantum cost, delay and garbage outputs compared to the existing
designs. We proposed a new model of reversible T flip-flop in designing
reversible counter
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1279</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1279</id><created>2014-04-04</created><updated>2015-03-08</updated><authors><author><keyname>Tamrawi</keyname><forenames>Ahmed</forenames></author><author><keyname>Kothari</keyname><forenames>Suresh</forenames></author></authors><title>Event-Flow Graphs for Efficient Path-Sensitive Analyses</title><categories>cs.SE</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient and accurate path-sensitive analyses pose the challenges of: (a)
analyzing an exponentially-increasing number of paths in a control-flow graph
(CFG), and (b) checking feasibility of paths in a CFG. We address these
challenges by introducing an equivalence relation on the CFG paths to partition
them into equivalence classes. It is then sufficient to perform analysis on
these equivalence classes rather than on the individual paths in a CFG. This
technique has two major advantages: (a) although the number of paths in a CFG
can be exponentially large, the essential information to be analyzed is
captured by a small number of equivalence classes, and (b) checking path
feasibility becomes simpler. The key challenge is how to efficiently compute
equivalence classes of paths in a CFG without examining each path in the CFG?
In this paper, we present a linear-time algorithm to form equivalence classes
without the need for examination of each path in a CFG. The key to this
algorithm is construction of an event-flow graph (EFG), a compact derivative of
the CFG, in which each path represents an equivalence class of paths in the
corresponding CFG. EFGs are defined with respect to the set of events that are
in turn defined by the analyzed property. The equivalence classes are thus
guaranteed to preserve all the event traces in the original CFG. We present an
empirical evaluation of the Linux kernel (v3.12). The EFGs in our evaluation
are defined with respect to events of the spin safe-synchronization property.
Evaluation results show that there are many fewer EFG-based equivalence classes
compared to the corresponding number of paths in a CFG. This reduction is close
to 99% for CFGs with a large number of paths. Moreover, our controlled
experiment results show that EFGs are human comprehensible and compact compared
to their corresponding CFGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1282</identifier>
 <datestamp>2015-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1282</id><created>2014-03-22</created><updated>2015-02-11</updated><authors><author><keyname>Kim</keyname><forenames>Dongwoo</forenames></author><author><keyname>Oh</keyname><forenames>Alice</forenames></author></authors><title>Hierarchical Dirichlet Scaling Process</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the \textit{hierarchical Dirichlet scaling process} (HDSP), a
Bayesian nonparametric mixed membership model. The HDSP generalizes the
hierarchical Dirichlet process (HDP) to model the correlation structure between
metadata in the corpus and mixture components. We construct the HDSP based on
the normalized gamma representation of the Dirichlet process, and this
construction allows incorporating a scaling function that controls the
membership probabilities of the mixture components. We develop two scaling
methods to demonstrate that different modeling assumptions can be expressed in
the HDSP. We also derive the corresponding approximate posterior inference
algorithms using variational Bayes. Through experiments on datasets of
newswire, medical journal articles, conference proceedings, and product
reviews, we show that the HDSP results in a better predictive performance than
labeled LDA, partially labeled LDA, and author topic model and a better
negative review classification performance than the supervised topic model and
SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1283</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1283</id><created>2014-03-24</created><updated>2014-06-02</updated><authors><author><keyname>Shalygo</keyname><forenames>Yuri</forenames></author></authors><title>The Kinetic Basis of Self-Organized Pattern Formation</title><categories>cs.FL cs.SY</categories><comments>8 pages, submitted to the 14th International Conference on the
  Synthesis and Simulation of Living Systems (Alife 14) on 23.03.2014, accepted
  09.05.2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his seminal paper on morphogenesis (1952), Alan Turing demonstrated that
different spatio-temporal patterns can arise due to instability of the
homogeneous state in reaction-diffusion systems, but at least two species are
necessary to produce even the simplest stationary patterns. This paper is aimed
to propose a novel model of the analog (continuous state) kinetic automaton and
to show that stationary and dynamic patterns can arise in one-component
networks of kinetic automata. Possible applicability of kinetic networks to
modeling of real-world phenomena is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1286</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1286</id><created>2014-04-03</created><authors><author><keyname>Rahimian</keyname><forenames>Ardavan</forenames></author></authors><title>Steerable Antennas for Automotive Communication Systems</title><categories>cs.OH</categories><comments>MEng Dissertation, School of EECE, University of Birmingham [Rohde &amp;
  Schwarz Technology Prize Winner]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research project undertakes a comprehensive analysis of RF beamforming
techniques for design, simulation, fabrication, and measurement of Butler
Matrix and Rotman Lens beamforming networks. It is aimed to develop novel and
well-established designs for steerable antenna systems that can be used in
vehicular telematics and automotive communication systems based on microwave
and millimeter-wave techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1292</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1292</id><created>2014-03-20</created><authors><author><keyname>AL-Allaf</keyname><forenames>Omaima N. A.</forenames></author></authors><title>Review of Face Detection Systems Based Artificial Neural Networks
  Algorithms</title><categories>cs.CV cs.NE</categories><comments>16 pages, 12 figures, 1 table, IJMA Journal</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.6, No.1, February 2014</journal-ref><doi>10.5121/ijma.2013.6101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face detection is one of the most relevant applications of image processing
and biometric systems. Artificial neural networks (ANN) have been used in the
field of image processing and pattern recognition. There is lack of literature
surveys which give overview about the studies and researches related to the
using of ANN in face detection. Therefore, this research includes a general
review of face detection studies and systems which based on different ANN
approaches and algorithms. The strengths and limitations of these literature
studies and systems were included also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1295</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1295</id><created>2014-04-03</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Catanese</keyname><forenames>Salvatore</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author></authors><title>Detecting criminal organizations in mobile phone networks</title><categories>cs.SI physics.soc-ph</categories><comments>http://www.sciencedirect.com/science/article/pii/S0957417414001614.
  Expert Systems with Applications, 2014</comments><journal-ref>Volume 41, Issue 13, 1 October 2014, Pages 5733-5750</journal-ref><doi>10.1016/j.eswa.2014.03.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of criminal networks using traces from heterogeneous communication
media is acquiring increasing importance in nowadays society. The usage of
communication media such as phone calls and online social networks leaves
digital traces in the form of metadata that can be used for this type of
analysis. The goal of this work is twofold: first we provide a theoretical
framework for the problem of detecting and characterizing criminal
organizations in networks reconstructed from phone call records. Then, we
introduce an expert system to support law enforcement agencies in the task of
unveiling the underlying structure of criminal networks hidden in communication
data. This platform allows for statistical network analysis, community
detection and visual exploration of mobile phone network data. It allows
forensic investigators to deeply understand hierarchies within criminal
organizations, discovering members who play central role and provide connection
among sub-groups. Our work concludes illustrating the adoption of our
computational framework for a real-word criminal investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1298</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1298</id><created>2014-04-04</created><updated>2015-07-13</updated><authors><author><keyname>Axenovich</keyname><forenames>Maria</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Density of Range Capturing Hypergraphs</title><categories>math.CO cs.CG</categories><comments>new version with a tight result and shorter proof</comments><msc-class>05C10, 05C62, 05C65</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a finite set $X$ of points in the plane, a set $S$ in the plane, and a
positive integer $k$, we say that a $k$-element subset $Y$ of $X$ is captured
by $S$ if there is a homothetic copy $S'$ of $S$ such that $X\cap S' = Y$,
i.e., $S'$ contains exactly $k$ elements from $X$. A $k$-uniform $S$-capturing
hypergraph $H = H(X,S,k)$ has a vertex set $X$ and a hyperedge set consisting
of all $k$-element subsets of $X$ captured by $S$. In case when $k=2$ and $S$
is convex these graphs are planar graphs, known as convex distance function
Delaunay graphs.
  In this paper we prove that for any $k\geq 2$, any $X$, and any convex
compact set $S$, the number of hyperedges in $H(X,S,k)$ is at most $(2k-1)|X| -
k^2 + 1 - \sum_{i=1}^{k-1}a_i$, where $a_i$ is the number of $i$-element
subsets of $X$ that can be separated from the rest of $X$ with a straight line.
In particular, this bound is independent of $S$ and indeed the bound is tight
for all &quot;round&quot; sets $S$ and point sets $X$ in general position with respect to
$S$.
  This refines a general result of Buzaglo, Pinchasi and Rote stating that
every pseudodisc topological hypergraph with vertex set $X$ has $O(k^2|X|)$
hyperedges of size $k$ or less.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1301</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1301</id><created>2014-02-21</created><authors><author><keyname>Zahedi</keyname><forenames>Zohreh</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author><author><keyname>Wouters</keyname><forenames>Paul</forenames></author></authors><title>How well developed are altmetrics? A cross-disciplinary analysis of the
  presence of 'alternative metrics' in scientific publications</title><categories>cs.DL</categories><doi>10.1007/s11192-014-1264-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an analysis of the presence and possibilities of altmetrics for
bibliometric and performance analysis is carried out. Using the web based tool
Impact Story, we collected metrics for 20,000 random publications from the Web
of Science. We studied both the presence and distribution of altmetrics in the
set of publications, across fields, document types and over publication years,
as well as the extent to which altmetrics correlate with citation indicators.
The main result of the study is that the altmetrics source that provides the
most metrics is Mendeley, with metrics on readerships for 62.6% of all the
publications studied, other sources only provide marginal information. In terms
of relation with citations, a moderate spearman correlation (r=0.49) has been
found between Mendeley readership counts and citation indicators. Other
possibilities and limitations of these indicators are discussed and future
research lines are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1311</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1311</id><created>2014-03-25</created><updated>2016-01-31</updated><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>Comments on &quot;IEEE 1588 Clock Synchronization using Dual Slave Clocks in
  a Slave&quot;</title><categories>cs.AR cs.NI</categories><comments>2 pages, 1 figure</comments><journal-ref>IEEE Communications Letters, vol. 18, no. 6, pp. 981-982, Jun.
  2014</journal-ref><doi>10.1109/LCOMM.2014.2317738</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the above letter, Chin and Chen proposed an IEEE 1588 clock
synchronization method based on dual slave clocks, where they claim that
multiple unknown parameters --- i.e., clock offset, clock skew, and
master-to-slave delay --- can be estimated with only one-way time transfers
using more equations than usual. This comment investigates Chin and Chen's dual
clock scheme with detailed models for a master and dual slave clocks and shows
that the formulation of multi-parameter estimation is invalid, which affirms
that it is impossible to distinguish the effect of delay from that of clock
offset at a slave even with dual slave clocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1312</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1312</id><created>2014-04-04</created><updated>2014-10-21</updated><authors><author><keyname>Tunali</keyname><forenames>Nihat Engin</forenames></author><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Boutros</keyname><forenames>Joseph J.</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>Lattices over Eisenstein Integers for Compute-and-Forward</title><categories>cs.IT math.IT</categories><comments>27 pages, 6 figures, submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the use of lattice codes over Eisenstein integers
for implementing a compute-and-forward protocol in wireless networks when
channel state information is not available at the transmitter. We extend the
compute-and-forward paradigm of Nazer and Gastpar to decoding Eisenstein
integer combinations of transmitted messages at relays by proving the existence
of a sequence of pairs of nested lattices over Eisenstein integers in which the
coarse lattice is good for covering and the fine lattice can achieve the
Poltyrev limit. Using this result, we show that both the outage performance and
error-correcting performance of nested lattice codebooks over Eisenstein
integers surpasses lattice codebooks over integers considered by Nazer and
Gastpar with no additional computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1313</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1313</id><created>2014-03-31</created><authors><author><keyname>Gorbachev</keyname><forenames>V. N.</forenames></author><author><keyname>Kaynarova</keyname><forenames>E. M.</forenames></author><author><keyname>Metelev</keyname><forenames>I. K.</forenames></author><author><keyname>Yakovleva</keyname><forenames>E. S.</forenames></author></authors><title>Color to Gray and Back transformation for distributing color digital
  images</title><categories>cs.MM</categories><comments>5 pages, 3 figures, submitted to The International Conference on
  Computer Graphics and Vision, GraphiCon'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Color to Gray and Back transformation watermarking with a secrete key is
considered. Color is embedded into the bit planes of the luminosity component
of the YUV color space with the help of a block algorithm that allows using not
only the least significant bits. An application of the problem of distributing
color digital images from a data base among legitimate users is discussed. The
proposed protocol can protect original images from unauthorized copying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1314</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1314</id><created>2014-03-21</created><authors><author><keyname>Meenakshi</keyname><forenames>K.</forenames></author><author><keyname>Rao</keyname><forenames>Ch. Srinivasa</forenames></author><author><keyname>Prasad</keyname><forenames>K. Satya</forenames></author></authors><title>Robust Video Watermarking Schemes in Phase domain Using Binary Phase
  Shift Keying</title><categories>cs.MM cs.CR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a robust video watermarking scheme in Discrete Fourier
Transform (DFT) and Sequencyordered Complex Hadamard Transform (SCHT). The DFT
and SCHT coefficients are complex and consist of both magnitude and phase and
are well suited to adopt phase shift keying techniques to embed the watermark.
In the proposed schemes, the phases of DFT and SCHT coefficients are modified
to convey watermark information using binary phase shift keying in cover video.
Low amplitude block selection (LABS) is used to improve transparency, amplitude
boost to improve the resistance of watermark from signal processing and
compression attacks and spread spectrum technique is used for encrypting
watermark in order to protect it from third party. It is observed that both
algorithms showing more or less same robustness but SCHT offers high
transparency, simple implementation and less computational cost than DFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1320</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1320</id><created>2014-03-26</created><authors><author><keyname>LiKamWa</keyname><forenames>Robert</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Carroll</keyname><forenames>Aaron</forenames></author><author><keyname>Lin</keyname><forenames>Felix Xiaozhu</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author></authors><title>Draining our Glass: An Energy and Heat Characterization of Google Glass</title><categories>cs.HC</categories><report-no>Rice University ECE Technical Report 2014-03-23</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Google Glass is a mobile device designed to be worn as eyeglasses. This
form factor enables new usage possibilities, such as hands-free video chats and
instant web search. However, its shape also hampers its potential: (1) battery
size, and therefore lifetime, is limited by a need for the device to be
lightweight, and (2) high-power processing leads to significant heat, which
should be limited, due to the Glass' compact form factor and close proximity to
the user's skin. We use the Glass in a case study of the power and thermal
characteristics of optical head-mounted display devices. We share insights and
implications to limit power consumption to increase the safety and utility of
head-mounted devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1323</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1323</id><created>2014-04-04</created><updated>2014-04-08</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Migler</keyname><forenames>Theresa</forenames></author></authors><title>Lower bounds for testing digraph connectivity with one-pass streaming
  algorithms</title><categories>cs.DS cs.CC</categories><comments>Added some references to previous work, removed the part of the
  result that was already known before, and changed the label of the result
  from &quot;Theorem&quot; to &quot;Lemma&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show that three graph properties - strong connectivity,
acyclicity, and reachability from a vertex $s$ to all vertices - each require a
working memory of $\Omega (\epsilon m)$ on a graph with $m$ edges to be
determined correctly with probability greater than $(1+\epsilon)/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1328</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1328</id><created>2014-04-04</created><authors><author><keyname>Wang</keyname><forenames>Da</forenames></author><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Efficient Task Replication for Fast Response Times in Parallel
  Computation</title><categories>cs.DC</categories><comments>Extended version of the 2-page paper accepted to ACM SIGMETRICS 2014</comments><acm-class>C.4; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One typical use case of large-scale distributed computing in data centers is
to decompose a computation job into many independent tasks and run them in
parallel on different machines, sometimes known as the &quot;embarrassingly
parallel&quot; computation. For this type of computation, one challenge is that the
time to execute a task for each machine is inherently variable, and the overall
response time is constrained by the execution time of the slowest machine. To
address this issue, system designers introduce task replication, which sends
the same task to multiple machines, and obtains result from the machine that
finishes first. While task replication reduces response time, it usually
increases resource usage. In this work, we propose a theoretical framework to
analyze the trade-off between response time and resource usage. We show that,
while in general, there is a tension between response time and resource usage,
there exist scenarios where replicating tasks judiciously reduces completion
time and resource usage simultaneously. Given the execution time distribution
for machines, we investigate the conditions for a scheduling policy to achieve
optimal performance trade-off, and propose efficient algorithms to search for
optimal or near-optimal scheduling policies. Our analysis gives insights on
when and why replication helps, which can be used to guide scheduler design in
large-scale distributed computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1333</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1333</id><created>2014-04-04</created><updated>2014-05-26</updated><authors><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Snyder</keyname><forenames>John C.</forenames></author><author><keyname>Pelaschier</keyname><forenames>Isabelle M.</forenames></author><author><keyname>Huang</keyname><forenames>Jessica</forenames></author><author><keyname>Niranjan</keyname><forenames>Uma-Naresh</forenames></author><author><keyname>Duncan</keyname><forenames>Paul</forenames></author><author><keyname>Rupp</keyname><forenames>Matthias</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Burke</keyname><forenames>Kieron</forenames></author></authors><title>Understanding Machine-learned Density Functionals</title><categories>physics.chem-ph cs.LG physics.comp-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel ridge regression is used to approximate the kinetic energy of
non-interacting fermions in a one-dimensional box as a functional of their
density. The properties of different kernels and methods of cross-validation
are explored, and highly accurate energies are achieved. Accurate {\em
constrained optimal densities} are found via a modified Euler-Lagrange
constrained minimization of the total energy. A projected gradient descent
algorithm is derived using local principal component analysis. Additionally, a
sparse grid representation of the density can be used without degrading the
performance of the methods. The implications for machine-learned density
functional approximations are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1338</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1338</id><created>2014-04-04</created><authors><author><keyname>Charapitsa</keyname><forenames>Siarhei V.</forenames></author><author><keyname>Dubovskaya</keyname><forenames>Irina Ya.</forenames></author><author><keyname>Lobko</keyname><forenames>Alexander S.</forenames></author><author><keyname>Savitskaya</keyname><forenames>Tatiana A.</forenames></author><author><keyname>Sytova</keyname><forenames>Svetlana N.</forenames></author></authors><title>Structure of Belarusian educational and research web portal of nuclear
  knowledge</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>11 pages, 1 figure, submitted to Nuclear Physics and Atomic Energy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objectives and instruments to develop Belarusian educational and
research web portal of nuclear knowledge are discussed. Draft structure of
portal is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1341</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1341</id><created>2014-04-04</created><updated>2015-08-23</updated><authors><author><keyname>Haghpanah</keyname><forenames>Nima</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author></authors><title>Multi-dimensional Virtual Values and Second-degree Price Discrimination</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-dimensional screening problem of selling a product with
multiple quality levels and design virtual value functions to derive conditions
that imply optimality of only selling highest quality. A challenge of designing
virtual values for multi-dimensional agents is that a mechanism that pointwise
optimizes virtual values resulting from a general application of integration by
parts is not incentive compatible, and no general methodology is known for
selecting the right paths for integration by parts. We resolve this issue by
first uniquely solving for paths that satisfy certain necessary conditions that
the pointwise optimality of the mechanism imposes on virtual values, and then
identifying distributions that ensure the resulting virtual surplus is indeed
pointwise optimized by the mechanism. Our method of solving for virtual values
is general, and as a second application we use it to derive conditions of
optimality for selling only the grand bundle of items to an agent with additive
preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1345</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1345</id><created>2014-03-17</created><authors><author><keyname>Hong</keyname><forenames>Lilian</forenames></author></authors><title>Optimizing Relay Precoding for Wireless Coordinated Relaying</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processing of multiple communication flows in wireless systems has given rise
to a number of novel transmission techniques, notably the two-way relaying
based on wireless network coding. Recently, a related set of techniques has
emerged, termed coordinated direct and relay (CDR) transmissions, where the
constellation of traffic flows is more general than the two-way. Regardless of
the actual traffic flows, in a CDR scheme the relay has a central role in
managing the interference and boosting the overall system performance. In this
paper we investigate the novel transmission modes, based on
amplify-and-forward, that arise when the relay is equipped with multiple
antennas and can use beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1355</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1355</id><created>2014-04-04</created><authors><author><keyname>Gabielkov</keyname><forenames>Maksym</forenames><affiliation>Inria Sophia Antipolis</affiliation></author><author><keyname>Rao</keyname><forenames>Ashwin</forenames><affiliation>Inria Sophia Antipolis</affiliation></author><author><keyname>Legout</keyname><forenames>Arnaud</forenames><affiliation>Inria Sophia Antipolis</affiliation></author></authors><title>Studying Social Networks at Scale: Macroscopic Anatomy of the Twitter
  Social Graph</title><categories>cs.SI physics.soc-ph</categories><comments>ACM Sigmetrics 2014 (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is one of the largest social networks using exclusively directed
links among accounts. This makes the Twitter social graph much closer to the
social graph supporting real life communications than, for instance, Facebook.
Therefore, understanding the structure of the Twitter social graph is
interesting not only for computer scientists, but also for researchers in other
fields, such as sociologists. However, little is known about how the
information propagation in Twitter is constrained by its inner structure. In
this paper, we present an in-depth study of the macroscopic structure of the
Twitter social graph unveiling the highways on which tweets propagate, the
specific user activity associated with each component of this macroscopic
structure, and the evolution of this macroscopic structure with time for the
past 6 years. For this study, we crawled Twitter to retrieve all accounts and
all social relationships (follow links) among accounts; the crawl completed in
July 2012 with 505 million accounts interconnected by 23 billion links. Then,
we present a methodology to unveil the macroscopic structure of the Twitter
social graph. This macroscopic structure consists of 8 components defined by
their connectivity characteristics. Each component group users with a specific
usage of Twitter. For instance, we identified components gathering together
spammers, or celebrities. Finally, we present a method to approximate the
macroscopic structure of the Twitter social graph in the past, validate this
method using old datasets, and discuss the evolution of the macroscopic
structure of the Twitter social graph during the past 6 years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1356</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1356</id><created>2014-04-04</created><updated>2015-02-04</updated><authors><author><keyname>Wintenberger</keyname><forenames>Olivier</forenames><affiliation>LSTA</affiliation></author></authors><title>Optimal learning with Bernstein Online Aggregation</title><categories>stat.ML cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new recursive aggregation procedure called Bernstein Online
Aggregation (BOA). The exponential weights include an accuracy term and a
second order term that is a proxy of the quadratic variation as in Hazan and
Kale (2010). This second term stabilizes the procedure that is optimal in
different senses. We first obtain optimal regret bounds in the deterministic
context. Then, an adaptive version is the first exponential weights algorithm
that exhibits a second order bound with excess losses that appears first in
Gaillard et al. (2014). The second order bounds in the deterministic context
are extended to a general stochastic context using the cumulative predictive
risk. Such conversion provides the main result of the paper, an inequality of a
novel type comparing the procedure with any deterministic aggregation procedure
for an integrated criteria. Then we obtain an observable estimate of the excess
of risk of the BOA procedure. To assert the optimality, we consider finally the
iid case for strongly convex and Lipschitz continuous losses and we prove that
the optimal rate of aggregation of Tsybakov (2003) is achieved. The batch
version of the BOA procedure is then the first adaptive explicit algorithm that
satisfies an optimal oracle inequality with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1366</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1366</id><created>2014-04-04</created><updated>2015-10-02</updated><authors><author><keyname>Anshu</keyname><forenames>Anurag</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Priyanka</forenames></author><author><keyname>Shayeghi</keyname><forenames>Ala</forenames></author><author><keyname>Yao</keyname><forenames>Penghui</forenames></author></authors><title>New one shot quantum protocols with application to communication
  complexity</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>23 pages. Changed title, abstract and presentation of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the following quantum compression protocol:
  P : Let $\rho,\sigma$ be quantum states such that $S(\rho || \sigma) =
\text{Tr} (\rho \log \rho - \rho \log \sigma)$, the relative entropy between
$\rho$ and $\sigma$, is finite. Alice gets to know the eigen-decomposition of
$\rho$. Bob gets to know the eigen-decomposition of $\sigma$. Both Alice and
Bob know $S(\rho || \sigma)$ and an error parameter $\epsilon$. Alice and Bob
use shared entanglement and after communication of $\mathcal{O}((S(\rho ||
\sigma)+1)/\epsilon^4)$ bits from Alice to Bob, Bob ends up with a quantum
state $\tilde{\rho}$ such that $F(\rho, \tilde{\rho}) \geq 1 - 5\epsilon$,
where $F(\cdot)$ represents fidelity.
  This result can be considered as a non-commutative generalization of a result
due to Braverman and Rao [2011] where they considered the special case when
$\rho$ and $\sigma$ are classical probability distributions (or commute with
each other) and use shared randomness instead of shared entanglement. We use P
to obtain an alternate proof of a direct-sum result for entanglement assisted
quantum one-way communication complexity for all relations, which was first
shown by Jain, Radhakrishnan and Sen [2005,2008]. We also present a variant of
protocol P in which Bob has some side information about the state with Alice.
We show that in such a case, the amount of communication can be further
reduced, based on the side information that Bob has.
  Our second result provides a quantum analogue of the widely used classical
correlated-sampling protocol. For example, Holenstein [2007] used the classical
correlated-sampling protocol in his proof of a parallel-repetition theorem for
two-player one-round games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1368</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1368</id><created>2014-04-04</created><authors><author><keyname>Verma</keyname><forenames>Trivik</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Nuno A. M.</forenames></author><author><keyname>Herrmann</keyname><forenames>Hans J</forenames></author></authors><title>Revealing the structure of the world airline network</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>8 pages, 4 figures, Suupporting Info</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resilience of most critical infrastructures against failure of elements that
appear insignificant is usually taken for granted. The World Airline Network
(WAN) is an infrastructure that reduces the geographical gap between societies,
both small and large, and brings forth economic gains. With the extensive use
of a publicly maintained data set that contains information about airports and
alternative connections between these airports, we empirically reveal that the
WAN is a redundant and resilient network for long distance air travel, but
otherwise breaks down completely due to removal of short and apparently
insignificant connections. These short range connections with moderate number
of passengers and alternate flights are the connections that keep remote parts
of the world accessible. It is surprising, insofar as there exists a highly
resilient and strongly connected core consisting of a small fraction of
airports (around 2.3%) together with an extremely fragile star-like periphery.
Yet, in spite of their relevance, more than 90% of the world airports are still
interconnected upon removal of this core. With standard and unconventional
removal measures we compare both empirical and topological perceptions for the
fragmentation of the world. We identify how the WAN is organized into different
classes of clusters based on the physical proximity of airports and analyze the
consequence of this fragmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1377</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1377</id><created>2014-04-04</created><updated>2014-04-16</updated><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author><author><keyname>Lai</keyname><forenames>Ming-Jun</forenames></author><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author><author><keyname>Davulcu</keyname><forenames>Hasan</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion</title><categories>cs.LG math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an efficient and scalable low rank matrix
completion algorithm. The key idea is to extend orthogonal matching pursuit
method from the vector case to the matrix case. We further propose an economic
version of our algorithm by introducing a novel weight updating rule to reduce
the time and storage complexity. Both versions are computationally inexpensive
for each matrix pursuit iteration, and find satisfactory results in a few
iterations. Another advantage of our proposed algorithm is that it has only one
tunable parameter, which is the rank. It is easy to understand and to use by
the user. This becomes especially important in large-scale learning problems.
In addition, we rigorously show that both versions achieve a linear convergence
rate, which is significantly better than the previous known results. We also
empirically compare the proposed algorithms with several state-of-the-art
matrix completion algorithms on many real-world datasets, including the
large-scale recommendation dataset Netflix as well as the MovieLens datasets.
Numerical results show that our proposed algorithm is more efficient than
competing algorithms while achieving similar or better prediction performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1395</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1395</id><created>2014-04-04</created><authors><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Bugiel</keyname><forenames>Sven</forenames></author><author><keyname>Gerling</keyname><forenames>Sebastian</forenames></author><author><keyname>von Styp-Rekowsky</keyname><forenames>Philipp</forenames></author></authors><title>Android Security Framework: Enabling Generic and Extensible Access
  Control on Android</title><categories>cs.CR</categories><report-no>A/01/2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Android Security Framework (ASF), a generic, extensible
security framework for Android that enables the development and integration of
a wide spectrum of security models in form of code-based security modules. The
design of ASF reflects lessons learned from the literature on established
security frameworks (such as Linux Security Modules or the BSD MAC Framework)
and intertwines them with the particular requirements and challenges from the
design of Android's software stack. ASF provides a novel security API that
supports authors of Android security extensions in developing their modules.
This overcomes the current unsatisfactory situation to provide security
solutions as separate patches to the Android software stack or to embed them
into Android's mainline codebase. As a result, ASF provides different practical
benefits such as a higher degree of acceptance, adaptation, and maintenance of
security solutions than previously possible on Android. We present a
prototypical implementation of ASF and demonstrate its effectiveness and
efficiency by modularizing different security models from related work, such as
context-aware access control, inlined reference monitoring, and type
enforcement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1404</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1404</id><created>2014-04-04</created><authors><author><keyname>Gupta</keyname><forenames>Abhishek</forenames></author><author><keyname>Yuksel</keyname><forenames>Serdar</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author><author><keyname>Langbort</keyname><forenames>Cedric</forenames></author></authors><title>On the Existence of Optimal Policies for a Class of Static and
  Sequential Dynamic Teams</title><categories>cs.SY math.OC math.PR</categories><comments>38 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we identify sufficient conditions under which static teams and
a class of sequential dynamic teams admit team-optimal solutions. We first
investigate the existence of optimal solutions in static teams where the
observations of the decision makers are conditionally independent or satisfy
certain regularity conditions. Building on these findings and the static
reduction method of Witsenhausen, we then extend the analysis to sequential
dynamic teams. In particular, we show that a large class of dynamic LQG team
problems, including the vector version of the well-known Witsenhausen's
counterexample and the Gaussian relay channel problem viewed as a dynamic team,
admit team-optimal solutions. Results in this paper substantially broaden the
class of stochastic control and team problems with non-classical information
known to have optimal solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1405</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1405</id><created>2014-04-04</created><authors><author><keyname>Fazeli</keyname><forenames>Arastoo</forenames></author><author><keyname>Ajorlou</keyname><forenames>Amir</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Optimal Budget Allocation in Social Networks: Quality or Seeding</title><categories>cs.SI cs.GT math.OC physics.soc-ph</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a strategic model of marketing and product
consumption in social networks. We consider two competing firms in a market
providing two substitutable products with preset qualities. Agents choose their
consumptions following a myopic best response dynamics which results in a
local, linear update for the consumptions. At some point in time, firms receive
a limited budget which they can use to trigger a larger consumption of their
products in the network. Firms have to decide between marginally improving the
quality of their products and giving free offers to a chosen set of agents in
the network in order to better facilitate spreading their products. We derive a
simple threshold rule for the optimal allocation of the budget and describe the
resulting Nash equilibrium. It is shown that the optimal allocation of the
budget depends on the entire distribution of centralities in the network,
quality of products and the model parameters. In particular, we show that in a
graph with a higher number of agents with centralities above a certain
threshold, firms spend more budget on seeding in the optimal allocation.
Furthermore, if seeding budget is nonzero for a balanced graph, it will also be
nonzero for any other graph, and if seeding budget is zero for a star graph, it
will be zero for any other graph too. We also show that firms allocate more
budget to quality improvement when their qualities are close, in order to
distance themselves from the rival firm. However, as the gap between qualities
widens, competition in qualities becomes less effective and firms spend more
budget on seeding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1428</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1428</id><created>2014-04-04</created><updated>2014-04-15</updated><authors><author><keyname>Sun</keyname><forenames>Yao</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author><author><keyname>Wang</keyname><forenames>Dingkang</forenames></author></authors><title>An Improvement over the GVW Algorithm for Inhomogeneous Polynomial
  Systems</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GVW algorithm is a signature-based algorithm for computing Gr\&quot;obner
bases. If the input system is not homogeneous, some J-pairs with higher
signatures but lower degrees are rejected by GVW's Syzygy Criterion, instead,
GVW have to compute some J-pairs with lower signatures but higher degrees.
Consequently, degrees of polynomials appearing during the computations may
unnecessarily grow up higher and the computation become more expensive. In this
paper, a variant of the GVW algorithm, called M-GVW, is proposed and mutant
pairs are introduced to overcome inconveniences brought by inhomogeneous input
polynomials. Some techniques from linear algebra are used to improve the
efficiency. Both GVW and M-GVW have been implemented in C++ and tested by many
examples from boolean polynomial rings. The timings show M-GVW usually performs
much better than the original GVW algorithm when mutant pairs are found.
Besides, M-GVW is also compared with intrinsic Gr\&quot;obner bases functions on
Maple, Singular and Magma. Due to the efficient routines from the M4RI library,
the experimental results show that M-GVW is very efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1434</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1434</id><created>2014-04-05</created><updated>2015-04-01</updated><authors><author><keyname>Einav</keyname><forenames>Amit</forenames></author></authors><title>On the Subadditivity of the Entropy on the Sphere</title><categories>math.FA cs.IT math-ph math.IT math.MP</categories><msc-class>39B62, 52A40, 60D99, 82C40, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a refinement of a known entropic inequality on the sphere, finding
suitable conditions under which the uniform probability measure on the sphere
behaves asymptomatically like the Gaussian measure on $\mathbb{R}^N$ with
respect to the entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1436</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1436</id><created>2014-04-05</created><updated>2014-12-23</updated><authors><author><keyname>Tian</keyname><forenames>Cong</forenames></author><author><keyname>Duan</keyname><forenames>Zhenhua</forenames></author></authors><title>Buchi Determinization Made Tighter</title><categories>cs.FL</categories><comments>12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By separating the principal acceptance mechanism from the concrete acceptance
condition of a given B\&quot;{u}chi automaton with $n$ states,Schewe presented the
construction of an equivalent deterministic Rabin transition automaton with
$o((1.65n)^n)$ states via \emph{history trees}, which can be simply translated
to a standard Rabin automaton with $o((2.26n)^n)$ states. Apart from the
inherent simplicity, Schewe's construction improved Safra's construction (which
requires $12^nn^{2n}$ states). However, the price that is paid is the use of
$2^{n-1}$ Rabin pairs (instead of $n$ in Safra's construction). Further, by
introducing the \emph{later introduction record} as a record tailored for
ordered trees, deterministic automata with Parity acceptance condition is
constructed which exactly resembles Piterman's determinization with Parity
acceptance condition where the state complexity is $O((n!)^2)$ and the index
complexity is $2n$.In this paper, we improve Schewe's construction to
$2^{\lceil (n-1)/2\rceil}$ Rabin pairs with the same state complexity.
Meanwhile, we give a new determinization construction of Parity automata with
the state complexity being $o(n^2(0.69n\sqrt{n})^n)$ and index complexity being
$n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1441</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1441</id><created>2014-04-05</created><authors><author><keyname>Djehiche</keyname><forenames>Boualem</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Tempone</keyname><forenames>Raul</forenames></author></authors><title>A Stochastic Maximum Principle for Risk-Sensitive Mean-Field Type
  Control</title><categories>math.OC cs.SY math.PR q-fin.RM</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study mean-field type control problems with risk-sensitive
performance functionals. We establish a stochastic maximum principle (SMP) for
optimal control of stochastic differential equations (SDEs) of mean-field type,
in which the drift and the diffusion coefficients as well as the performance
functional depend not only on the state and the control but also on the mean of
the distribution of the state. Our result extends the risk-sensitive SMP
(without mean-field coupling) of Lim and Zhou (2005), derived for feedback (or
Markov) type optimal controls, to optimal control problems for non-Markovian
dynamics which may be time-inconsistent in the sense that the Bellman
optimality principle does not hold. In our approach to the risk-sensitive SMP,
the smoothness assumption on the value-function imposed in Lim and Zhou (2005)
need not to be satisfied. For a general action space a Peng's type SMP is
derived, specifying the necessary conditions for optimality. Two examples are
carried out to illustrate the proposed risk-sensitive mean-field type SMP under
linear stochastic dynamics with exponential quadratic cost function. Explicit
solutions are given for both mean-field free and mean-field models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1443</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1443</id><created>2014-04-05</created><authors><author><keyname>Shams</keyname><forenames>Farshad</forenames></author><author><keyname>Luise</keyname><forenames>Marco</forenames></author></authors><title>Upper-Bounding the Capacity of Relay Communications - Part II</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the capacity of peer-to-peer relay communications
wherein the transmitter are assisted by an arbitrary number of parallel relays,
i.e. there is no link and cooperation between the relays themselves. We detail
the mathematical model of different relaying strategies including cutset and
amplify and forward strategies. The cutset upper bound capacity is presented as
a reference to compare another realistic strategy. We present its outer region
capacity which is lower than that in the existing literature. We show that a
multiple parallel relayed network achieves its maximum capacity by virtue of
only one relay or by virtue of all relays together. Adding a relay may even
decrease the overall capacity or may do not change it. We exemplify various
outer region capacities of the addressed strategies with two different case
studies. The results exhibit that in low signal-to-noise ratio (SNR)
environments the cutset outperforms the amplify and forward strategy and this
is contrary in high SNR environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1448</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1448</id><created>2014-04-05</created><updated>2014-07-31</updated><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author></authors><title>Why walking the dog takes time: Frechet distance has no strongly
  subquadratic algorithms unless SETH fails</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Frechet distance is a well-studied and very popular measure of similarity
of two curves. Many variants and extensions have been studied since Alt and
Godau introduced this measure to computational geometry in 1991. Their original
algorithm to compute the Frechet distance of two polygonal curves with n
vertices has a runtime of O(n^2 log n). More than 20 years later, the state of
the art algorithms for most variants still take time more than O(n^2 / log n),
but no matching lower bounds are known, not even under reasonable complexity
theoretic assumptions.
  To obtain a conditional lower bound, in this paper we assume the Strong
Exponential Time Hypothesis or, more precisely, that there is no
O*((2-delta)^N) algorithm for CNF-SAT for any delta &gt; 0. Under this assumption
we show that the Frechet distance cannot be computed in strongly subquadratic
time, i.e., in time O(n^{2-delta}) for any delta &gt; 0. This means that finding
faster algorithms for the Frechet distance is as hard as finding faster CNF-SAT
algorithms, and the existence of a strongly subquadratic algorithm can be
considered unlikely.
  Our result holds for both the continuous and the discrete Frechet distance.
We extend the main result in various directions. Based on the same assumption
we (1) show non-existence of a strongly subquadratic 1.001-approximation, (2)
present tight lower bounds in case the numbers of vertices of the two curves
are imbalanced, and (3) examine realistic input assumptions (c-packed curves).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1449</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1449</id><created>2014-04-05</created><authors><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author></authors><title>Non-Asymptotic Mean-Field Games</title><categories>cs.GT cs.SY</categories><comments>36 pages, 2 figures. Accepted and to appear in IEEE Transactions on
  Systems, Man, and Cybernetics, Part B 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-field games have been studied under the assumption of very large number
of players. For such large systems, the basic idea consists to approximate
large games by a stylized game model with a continuum of players. The approach
has been shown to be useful in some applications. However, the stylized game
model with continuum of decision-makers is rarely observed in practice and the
approximation proposed in the asymptotic regime is meaningless for networks
with few entities. In this paper we propose a mean-field framework that is
suitable not only for large systems but also for a small world with few number
of entities. The applicability of the proposed framework is illustrated through
various examples including dynamic auction with asymmetric valuation
distributions, and spiteful bidders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1451</identifier>
 <datestamp>2014-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1451</id><created>2014-04-05</created><updated>2014-10-15</updated><authors><author><keyname>Cierny</keyname><forenames>Michal</forenames></author><author><keyname>Ding</keyname><forenames>Zhi</forenames></author><author><keyname>Wichman</keyname><forenames>Risto</forenames></author></authors><title>Higher Rank Interference Effect on Weak Beamforming or OSTBC Terminals</title><categories>cs.IT math.IT</categories><comments>Revised after first round of reviews</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User performance on a wireless network depends on whether a neighboring
cochannel interferer applies a single (spatial) stream or a multi stream
transmission. This work analyzes the impact of interference rank on a
beamforming and orthogonal space-time block coded (OSTBC) user transmission. We
generalize existing analytical results on
signal-to-interference-plus-noise-ratio (SINR) distribution and outage
probability under arbitrary number of unequal power interferers. We show that
higher rank interference causes lower outage probability, and can support
better outage threshold especially in the case of beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1462</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1462</id><created>2014-04-05</created><authors><author><keyname>S</keyname><forenames>Pallavi. V.</forenames></author><author><keyname>D</keyname><forenames>Dr. Rukmani Devi.</forenames></author></authors><title>Design of a High Speed FPGA-Based Classifier for Efficient Packet
  Classification</title><categories>cs.NI</categories><comments>6 pages, 6 figures, &quot;Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;, &quot;National Conference on Modern Electronics
  and Signal Processing (2014)- Velammal Engineering College&quot;, &quot;Recent Trends
  in Information Technology (2014)- R.M.K College of Engineering and
  Technology&quot;</comments><journal-ref>Pallavi.V.S, Dr.Rukmani Devi.D Article: Design of a High Speed
  FPGA-Based Classifier for Efficient Packet Classification. International
  Journal of Computer Trends and Technology(IJCTT) 9(3):123-128,Mar 2014</journal-ref><doi>10.14445/22312803/IJCTT-V9P126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packet classification is a vital and complicated task as the processing of
packets should be done at a specified line speed. In order to classify a packet
as belonging to a particular flow or set of flows, network nodes must perform a
search over a set of filters using multiple fields of the packet as the search
key. Hence the matching of packets should be much faster and simpler for quick
processing and classification. A hardware accelerator or a classifier has been
proposed here using a modified version of the HyperCuts packet classification
algorithm. A new pre-cutting process has been implemented to reduce the memory
size to fit in an FPGA. This classifier can classify packets with high speed
and with a power consumption factor of less than 3W. This methodology removes
the need for floating point division to be performed by replacing the region
compaction scheme of HyperCuts by pre-cutting, while classifying the packets
and concentrates on classifying the packets at the core of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1464</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1464</id><created>2014-04-05</created><authors><author><keyname>Randhawa</keyname><forenames>Sukhchandan</forenames></author><author><keyname>Verma</keyname><forenames>Anil Kumar</forenames></author></authors><title>A Review of Power Aware Routing Protocols in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>6 Pages including 5 figures and 2 tables, Presented in the National
  Conference held at Center for Information technology, Manonmaniam Sundaranar
  University, Tirunelveli, India</comments><report-no>SN203</report-no><msc-class>94A99</msc-class><acm-class>C.2.1</acm-class><journal-ref>CONFERENCE PROCEEDINGS OF THE NATIONAL CONFERENCE ON WIRELESS
  SENSOR NETWORK AND ITS APPLICATION (NCWSNA 2012), 22- 23 MARCH 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  WSNs are envisioned to consist of many small devices that can sense the
environment and communicate the data as required. The most critical requirement
for widespread sensor networks is power efficiency since battery replacement is
not viable. Many protocols are proposed to minimize the power consumption by
using complex algorithms. However, it is difficult to perform these complex
methods since an individual sensor node in sensor networks does not have high
computational capacity. On the other hand, many sensor nodes should transfer
the data packet to the sink node that collects the required data. Therefore,
the operations of the sensor nodes over the route are terminated. It is
difficult to deliver the data packet to the sink node even if some sensor nodes
are active. In this paper, an introduction of WSNs is presented with a deep
insight into the power-aware routing protocol for sensor networks. The
protocols considered are LEACH,VGA and PEGASIS. In addition, a comparison of
these protocols is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1468</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1468</id><created>2014-04-05</created><authors><author><keyname>R</keyname><forenames>Swetha.</forenames></author><author><keyname>D</keyname><forenames>Rukmani Devi.</forenames></author></authors><title>High Throughput and Less Area AMP Architecture for Audio Signal
  Restoration</title><categories>cs.SD cs.IT math.IT</categories><comments>5 pages,6 figures,&quot;Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;,&quot;National Conference on Modern Electronics and
  Signal Processing(2014)-Velammal Engineering College&quot;,&quot;Recent Trends in
  Information Technology(2014)-R.M.K College of Engineering and Technology&quot;</comments><journal-ref>Swetha.R,Rukmani Devi.D Article: High Throughput and Less Area AMP
  Architecture for Audio Signal Restoration.International Journal of Computer
  Trends and Technology (IJCTT) 9(3):107-111,Mar 2014</journal-ref><doi>10.14445/22312803/IJCTT-V9P123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio restoration is effectively achieved by using low complexity algorithm
called AMP. This algorithm has fast convergence and has lower computation
intensity making it suitable for audio recovery problems. This paper focuses on
restoring an audio signal by using VLSI architecture called AMP-M that
implements AMP algorithm. This architecture employs MAC unit with fixed bit
Wallace tree multiplier, FFT-MUX and various memory units (RAM) for audio
restoration. VLSI and FPGA implementation results shows that reduced area, high
throughput, low power is achieved making it suitable for real time audio
recovery problems. Prominent examples are Magnetic Resonance Imaging (MRI),
Radar and Wireless Communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1469</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1469</id><created>2014-04-05</created><authors><author><keyname>Randhawa</keyname><forenames>Sukhchandan</forenames></author></authors><title>Research Challenges in Wireless Sensor Network: A State of the Play</title><categories>cs.NI</categories><comments>4 Pages including 1 figures and 1 tables, Presented in the National
  Conference held at Northwest Institute Of Engineering &amp; Technology Punjab,
  India. arXiv admin note: text overlap with arXiv:1002.4680 by other authors</comments><report-no>WSN12</report-no><msc-class>94A99</msc-class><acm-class>C.2.1</acm-class><journal-ref>Conference Proceeding of National conference on convergence of
  science, engineering &amp; management in education and research, 21 - 22 March,
  2014</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A wireless sensor network (WSN) has important applications such as remote
environmental monitoring and target tracking. This has been enabled by the
availability, particularly in recent years, of sensors that are smaller,
cheaper, and intelligent. These sensors are equipped with wireless interfaces
with which they can communicate with one another to form a network. The design
of a WSN depends significantly on the application, and it must consider factors
such as the environment, the applications design objectives, cost, hardware,
and system constraints. The goal of survey is to present a comprehensive review
of the recent literature in wireless sensor network. This paper reviews the
major development and new research challenges in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1484</identifier>
 <datestamp>2014-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1484</id><created>2014-04-05</created><updated>2014-09-21</updated><authors><author><keyname>Liao</keyname><forenames>Wenjing</forenames></author><author><keyname>Fannjiang</keyname><forenames>Albert</forenames></author></authors><title>MUSIC for Single-Snapshot Spectral Estimation: Stability and
  Super-resolution</title><categories>cs.IT math.IT math.NA</categories><comments>Studies on the super-resolution of the MUSIC algorithm have been
  added in Section 4 and Section 5.4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of line spectral estimation in the continuum
of a bounded interval with one snapshot of array measurement. The
single-snapshot measurement data is turned into a Hankel data matrix which
admits the Vandermonde decomposition and is suitable for the MUSIC algorithm.
The MUSIC algorithm amounts to finding the null space (the noise space) of the
Hankel matrix, forming the noise-space correlation function and identifying the
s smallest local minima of the noise-space correlation as the frequency set.
  In the noise-free case exact reconstruction is guaranteed for any arbitrary
set of frequencies as long as the number of measurements is at least twice the
number of distinct frequencies to be recovered. In the presence of noise the
stability analysis shows that the perturbation of the noise-space correlation
is proportional to the spectral norm of the noise matrix as long as the latter
is smaller than the smallest (nonzero) singular value of the noiseless Hankel
data matrix. Under the assumption that frequencies are separated by at least
twice the Rayleigh Length (RL), the stability of the noise-space correlation is
proved by means of novel discrete Ingham inequalities which provide bounds on
nonzero singular values of the noiseless Hankel data matrix.
  The numerical performance of MUSIC is tested in comparison with other
algorithms such as BLO-OMP and SDP (TV-min). While BLO-OMP is the stablest
algorithm for frequencies separated above 4 RL, MUSIC becomes the best
performing one for frequencies separated between 2 RL and 3 RL. Also, MUSIC is
more efficient than other methods. MUSIC truly shines when the frequency
separation drops to 1 RL or below when all other methods fail. Indeed, the
resolution length of MUSIC decreases to zero as noise decreases to zero as a
power law with an exponent much smaller than an upper bound established by
Donoho.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1486</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1486</id><created>2014-04-05</created><authors><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author></authors><title>MIMO Multiway Relaying with Clustered Full Data Exchange: Signal Space
  Alignment and Degrees of Freedom</title><categories>cs.IT math.IT</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate achievable degrees of freedom (DoF) for a multiple-input
multiple-output (MIMO) multiway relay channel (mRC) with $L$ clusters and $K$
users per cluster. Each user is equipped with $M$ antennas and the relay with
$N$ antennas. We assume a new data exchange model, termed \emph{clustered full
data exchange}, i.e., each user in a cluster wants to learn the messages of all
the other users in the same cluster. Novel signal alignment techniques are
developed to systematically construct the beamforming matrices at the users and
the relay for efficient physical-layer network coding. Based on that, we derive
an achievable DoF of the MIMO mRC with an arbitrary network configuration of
$L$ and $K$, as well as with an arbitrary antenna configuration of $M$ and $N$.
We show that our proposed scheme achieves the DoF capacity when $\frac{M}{N}
\leq \frac{1}{LK-1}$ and $\frac{M}{N} \geq \frac{(K-1)L+1}{KL}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1491</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1491</id><created>2014-03-24</created><authors><author><keyname>Mitra</keyname><forenames>Jayita</forenames></author><author><keyname>Saha</keyname><forenames>Diganta</forenames></author></authors><title>An Efficient Feature Selection in Classification of Audio Files</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have focused on an efficient feature selection method in
classification of audio files. The main objective is feature selection and
extraction. We have selected a set of features for further analysis, which
represents the elements in feature vector. By extraction method we can compute
a numerical representation that can be used to characterize the audio using the
existing toolbox. In this study Gain Ratio (GR) is used as a feature selection
measure. GR is used to select splitting attribute which will separate the
tuples into different classes. The pulse clarity is considered as a subjective
measure and it is used to calculate the gain of features of audio files. The
splitting criterion is employed in the application to identify the class or the
music genre of a specific audio file from testing database. Experimental
results indicate that by using GR the application can produce a satisfactory
result for music genre classification. After dimensionality reduction best
three features have been selected out of various features of audio file and in
this technique we will get more than 90% successful classification result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1492</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1492</id><created>2014-04-05</created><authors><author><keyname>Brofos</keyname><forenames>James</forenames></author></authors><title>Ensemble Committees for Stock Return Classification and Prediction</title><categories>stat.ML cs.LG</categories><comments>15 pages, 4 figures, Neukom Institute Computational Undergraduate
  Research prize - second place</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a portfolio trading strategy formulated by algorithms in
the field of machine learning. The profitability of the strategy is measured by
the algorithm's capability to consistently and accurately identify stock
indices with positive or negative returns, and to generate a preferred
portfolio allocation on the basis of a learned model. Stocks are characterized
by time series data sets consisting of technical variables that reflect market
conditions in a previous time interval, which are utilized produce binary
classification decisions in subsequent intervals. The learned model is
constructed as a committee of random forest classifiers, a non-linear support
vector machine classifier, a relevance vector machine classifier, and a
constituent ensemble of k-nearest neighbors classifiers. The Global Industry
Classification Standard (GICS) is used to explore the ensemble model's efficacy
within the context of various fields of investment including Energy, Materials,
Financials, and Information Technology. Data from 2006 to 2012, inclusive, are
considered, which are chosen for providing a range of market circumstances for
evaluating the model. The model is observed to achieve an accuracy of
approximately 70% when predicting stock price returns three months in advance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1498</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1498</id><created>2014-04-05</created><authors><author><keyname>Essahafi</keyname><forenames>Med.</forenames></author></authors><title>Model Predictive Control (MPC) Applied To Coupled Tank Liquid Level
  System</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Coupled Tank system used for liquid level control is a model of plant that
has usually been used in industries especially chemical process industries.
Level control is also very important for mixing reactant process. This survey
paper tries to presents in a systemic way an approach predictive control
strategy for a system that is similar to the process and is represented by two
liquid tanks. This system of coupled Tank is one of the most commonly available
systems representing a coupled Multiple Input Multiple Output (MIMO) system.
With 2 inputs and 2 outputs, it is the most primitive form of a coupled
multivariable system. Therefor the basic concept of how the coupled tanks
system works is by using a numerical system which it operates with a flow
control valve FCV as main control of the level of liquid in one tank or both
tanks. For this paper, MPC algorithm control is used which will be developed
below. And it is focuses on the design and modelling for coupled tanks system.
The steps followed for the design of the controller are: Developing a state
space system model for the coupled tank system then design an MPC controller
for the developed system model. And study the effect of the disturbance on
measured level output. Note that the implementation Model Predictive Controller
on flow controller valve in a Coupled Tank liquid level system is one of the
new methods of controlling liquid level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1503</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1503</id><created>2014-04-05</created><updated>2015-01-21</updated><authors><author><keyname>Ablayev</keyname><forenames>Farid</forenames></author><author><keyname>Ablayev</keyname><forenames>Marat</forenames></author></authors><title>Quantum Hashing via Classical $\epsilon$-universal Hashing Constructions</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, we define the concept of the quantum hash generator and offer
design, which allows to build a large amount of different quantum hash
functions. The construction is based on composition of classical
$\epsilon$-universal hash family and a given family of functions -- quantum
hash generator.
  The proposed construction combines the properties of robust presentation of
information by classical error-correcting codes together with the possibility
of highly compressed presentation of information by quantum systems.
  In particularly, we present quantum hash function based on Reed-Solomon code,
and we proved, that this construction is optimal in the sense of number of
qubits needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1504</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1504</id><created>2014-04-05</created><authors><author><keyname>Wiener</keyname><forenames>Yair</forenames></author><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author><author><keyname>El-Yaniv</keyname><forenames>Ran</forenames></author></authors><title>A Compression Technique for Analyzing Disagreement-Based Active Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new and improved characterization of the label complexity of
disagreement-based active learning, in which the leading quantity is the
version space compression set size. This quantity is defined as the size of the
smallest subset of the training data that induces the same version space. We
show various applications of the new characterization, including a tight
analysis of CAL and refined label complexity bounds for linear separators under
mixtures of Gaussians and axis-aligned rectangles under product densities. The
version space compression set size, as well as the new characterization of the
label complexity, can be naturally extended to agnostic learning problems, for
which we show new speedup results for two well known active learning
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1506</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1506</id><created>2014-04-05</created><updated>2014-09-03</updated><authors><author><keyname>Friedland</keyname><forenames>Shmuel</forenames></author><author><keyname>Li</keyname><forenames>Qun</forenames></author><author><keyname>Schonfeld</keyname><forenames>Dan</forenames></author><author><keyname>Bernal</keyname><forenames>Edgar A.</forenames></author></authors><title>Two algorithms for compressed sensing of sparse tensors</title><categories>cs.IT math.IT</categories><comments>23 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1305.5777</comments><msc-class>15A18, 15A69, 68P30, 94A08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) exploits the sparsity of a signal in order to
integrate acquisition and compression. CS theory enables exact reconstruction
of a sparse signal from relatively few linear measurements via a suitable
nonlinear minimization process. Conventional CS theory relies on vectorial data
representation, which results in good compression ratios at the expense of
increased computational complexity. In applications involving color images,
video sequences, and multi-sensor networks, the data is intrinsically of
high-order, and thus more suitably represented in tensorial form. Standard
applications of CS to higher-order data typically involve representation of the
data as long vectors that are in turn measured using large sampling matrices,
thus imposing a huge computational and memory burden. In this chapter, we
introduce Generalized Tensor Compressed Sensing (GTCS)--a unified framework for
compressed sensing of higher-order tensors which preserves the intrinsic
structure of tensorial data with reduced computational complexity at
reconstruction. We demonstrate that GTCS offers an efficient means for
representation of multidimensional data by providing simultaneous acquisition
and compression from all tensor modes. In addition, we propound two
reconstruction procedures, a serial method (GTCS-S) and a parallelizable method
(GTCS-P), both capable of recovering a tensor based on noiseless and noisy
observations. We then compare the performance of the proposed methods with
Kronecker compressed sensing (KCS) and multi-way compressed sensing (MWCS). We
demonstrate experimentally that GTCS outperforms KCS and MWCS in terms of both
reconstruction accuracy (within a range of compression ratios) and processing
speed. The major disadvantage of our methods (and of MWCS as well), is that the
achieved compression ratios may be worse than those offered by KCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1511</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1511</id><created>2014-04-05</created><authors><author><keyname>Plaat</keyname><forenames>Aske</forenames></author></authors><title>MTD(f), A Minimax Algorithm Faster Than NegaScout</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  MTD(f) is a new minimax search algorithm, simpler and more efficient than
previous algorithms. In tests with a number of tournament game playing programs
for chess, checkers and Othello it performed better, on average, than
NegaScout/PVS (the AlphaBeta variant used in practically all good chess,
checkers, and Othello programs). One of the strongest chess programs of the
moment, MIT's parallel chess program Cilkchess uses MTD(f) as its search
algorithm, replacing NegaScout, which was used in StarSocrates, the previous
version of the program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1514</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1514</id><created>2014-04-05</created><authors><author><keyname>Bhute</keyname><forenames>Avinash N</forenames></author><author><keyname>Meshram</keyname><forenames>B. B.</forenames></author></authors><title>Text Based Approach For Indexing And Retrieval Of Image And Video: A
  Review</title><categories>cs.IR cs.CV cs.DL cs.MM</categories><comments>12 pages</comments><journal-ref>Advances in Vision: An International Journal, Vol 1, no. 1, March
  2014</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Text data present in multimedia contain useful information for automatic
annotation, indexing. Extracted information used for recognition of the overlay
or scene text from a given video or image. The Extracted text can be used for
retrieving the videos and images. In this paper, firstly, we are discussed the
different techniques for text extraction from images and videos. Secondly, we
are reviewed the techniques for indexing and retrieval of image and videos by
using extracted text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1515</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1515</id><created>2014-04-05</created><authors><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Schaeffer</keyname><forenames>Jonathan</forenames></author><author><keyname>Pijls</keyname><forenames>Wim</forenames></author><author><keyname>de Bruin</keyname><forenames>Arie</forenames></author></authors><title>A New Paradigm for Minimax Search</title><categories>cs.AI</categories><comments>Novag Award 1994-1995 Best Computer Chess publication</comments><report-no>Univ Alberta TR 94-18</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper introduces a new paradigm for minimax game-tree search algo-
rithms. MT is a memory-enhanced version of Pearls Test procedure. By changing
the way MT is called, a number of best-first game-tree search algorithms can be
simply and elegantly constructed (including SSS*). Most of the assessments of
minimax search algorithms have been based on simulations. However, these
simulations generally do not address two of the key ingredients of high
performance game-playing programs: iterative deepening and memory usage. This
paper presents experimental data from three game-playing programs (checkers,
Othello and chess), covering the range from low to high branching factor. The
improved move ordering due to iterative deepening and memory usage results in
significantly different results from those portrayed in the literature. Whereas
some simulations show Alpha-Beta expanding almost 100% more leaf nodes than
other algorithms [12], our results showed variations of less than 20%. One new
instance of our framework (MTD-f) out-performs our best alpha- beta searcher
(aspiration NegaScout) on leaf nodes, total nodes and execution time. To our
knowledge, these are the first reported results that compare both depth-first
and best-first algorithms given the same amount of memory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1517</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1517</id><created>2014-04-05</created><authors><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Schaeffer</keyname><forenames>Jonathan</forenames></author><author><keyname>Pijls</keyname><forenames>Wim</forenames></author><author><keyname>de Bruin</keyname><forenames>Arie</forenames></author></authors><title>SSS* = Alpha-Beta + TT</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In 1979 Stockman introduced the SSS* minimax search algorithm that domi-
nates Alpha-Beta in the number of leaf nodes expanded. Further investigation of
the algorithm showed that it had three serious drawbacks, which prevented its
use by practitioners: it is difficult to understand, it has large memory
requirements, and it is slow. This paper presents an alternate formulation of
SSS*, in which it is implemented as a series of Alpha-Beta calls that use a
transposition table (AB- SSS*). The reformulation solves all three perceived
drawbacks of SSS*, making it a practical algorithm. Further, because the search
is now based on Alpha-Beta, the extensive research on minimax search
enhancements can be easily integrated into AB-SSS*. To test AB-SSS* in
practise, it has been implemented in three state-of-the- art programs: for
checkers, Othello and chess. AB-SSS* is comparable in performance to Alpha-Beta
on leaf node count in all three games, making it a viable alternative to
Alpha-Beta in practise. Whereas SSS* has usually been regarded as being
entirely different from Alpha-Beta, it turns out to be just an Alpha-Beta
enhancement, like null-window searching. This runs counter to published
simulation results. Our research leads to the surprising result that iterative
deepening versions of Alpha-Beta can expand fewer leaf nodes than iterative
deepening versions of SSS* due to dynamic move re-ordering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1518</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1518</id><created>2014-04-05</created><authors><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Schaeffer</keyname><forenames>Jonathan</forenames></author><author><keyname>Pijls</keyname><forenames>Wim</forenames></author><author><keyname>de Bruin</keyname><forenames>Arie</forenames></author></authors><title>Nearly Optimal Minimax Tree Search?</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Knuth and Moore presented a theoretical lower bound on the number of leaves
that any fixed-depth minimax tree-search algorithm traversing a uniform tree
must explore, the so-called minimal tree. Since real-life minimax trees are not
uniform, the exact size of this tree is not known for most applications.
Further, most games have transpositions, implying that there exists a minimal
graph which is smaller than the minimal tree. For three games (chess, Othello
and checkers) we compute the size of the minimal tree and the minimal graph.
Empirical evidence shows that in all three games, enhanced Alpha-Beta search is
capable of building a tree that is close in size to that of the minimal graph.
Hence, it appears game-playing programs build nearly optimal search trees.
However, the conventional definition of the minimal graph is wrong. There are
ways in which the size of the minimal graph can be reduced: by maximizing the
number of transpositions in the search, and generating cutoffs using branches
that lead to smaller search trees. The conventional definition of the minimal
graph is just a left-most approximation. Calculating the size of the real
minimal graph is too computationally intensive. However, upper bound
approximations show it to be significantly smaller than the left-most minimal
graph. Hence, it appears that game-playing programs are not searching as
efficiently as is widely believed. Understanding the left-most and real minimal
search graphs leads to some new ideas for enhancing Alpha-Beta search. One of
them, enhanced transposition cutoffs, is shown to significantly reduce search
tree size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1521</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1521</id><created>2014-04-05</created><updated>2014-04-15</updated><authors><author><keyname>Kulkarni</keyname><forenames>Vivek</forenames></author><author><keyname>Al-Rfou'</keyname><forenames>Rami</forenames></author><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>Exploring the power of GPU's for training Polyglot language models</title><categories>cs.LG cs.CL</categories><comments>version 2 (just corrected citation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major research trends currently is the evolution of heterogeneous
parallel computing. GP-GPU computing is being widely used and several
applications have been designed to exploit the massive parallelism that
GP-GPU's have to offer. While GPU's have always been widely used in areas of
computer vision for image processing, little has been done to investigate
whether the massive parallelism provided by GP-GPU's can be utilized
effectively for Natural Language Processing(NLP) tasks. In this work, we
investigate and explore the power of GP-GPU's in the task of learning language
models. More specifically, we investigate the performance of training Polyglot
language models using deep belief neural networks. We evaluate the performance
of training the model on the GPU and present optimizations that boost the
performance on the GPU.One of the key optimizations, we propose increases the
performance of a function involved in calculating and updating the gradient by
approximately 50 times on the GPU for sufficiently large batch sizes. We show
that with the above optimizations, the GP-GPU's performance on the task
increases by factor of approximately 3-4. The optimizations we made are generic
Theano optimizations and hence potentially boost the performance of other
models which rely on these operations.We also show that these optimizations
result in the GPU's performance at this task being now comparable to that on
the CPU. We conclude by presenting a thorough evaluation of the applicability
of GP-GPU's for this task and highlight the factors limiting the performance of
training a Polyglot model on the GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1530</identifier>
 <datestamp>2014-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1530</id><created>2014-04-05</created><updated>2014-06-02</updated><authors><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author></authors><title>Provable Deterministic Leverage Score Sampling</title><categories>cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH</categories><comments>20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explain theoretically a curious empirical phenomenon: &quot;Approximating a
matrix by deterministically selecting a subset of its columns with the
corresponding largest leverage scores results in a good low-rank matrix
surrogate&quot;. To obtain provable guarantees, previous work requires randomized
sampling of the columns with probabilities proportional to their leverage
scores.
  In this work, we provide a novel theoretical analysis of deterministic
leverage score sampling. We show that such deterministic sampling can be
provably as accurate as its randomized counterparts, if the leverage scores
follow a moderately steep power-law decay. We support this power-law assumption
by providing empirical evidence that such decay laws are abundant in real-world
data sets. We then demonstrate empirically the performance of deterministic
leverage score sampling, which many times matches or outperforms the
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1531</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1531</id><created>2014-04-05</created><updated>2014-04-13</updated><authors><author><keyname>Mogavero</keyname><forenames>Fabio</forenames></author><author><keyname>Perelli</keyname><forenames>Giuseppe</forenames></author></authors><title>On the Remarkable Features of Binding Forms</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hilbert's Entscheidungsproblem has given rise to a broad and productive line
of research in mathematical logic, where the classification process of
decidable classes of first-order sentences represent only one of the remarkable
results. According to the criteria used to identify the particular classes of
interest, this process was declined into several research programs, of which
some of the most deeply investigated are the ones classifying sentences in
prenex normal form in base of their prefix vocabulary. Unfortunately, almost
all of these approaches did not shed any light on the reasons why modal logic
is so robustly decidable. Trying to answer to this question, Andreka, van
Benthem, and Nemeti introduced the guarded fragment of first-order logic, which
generalizes the modal framework by essentially retaining all its fundamental
properties. They started, so, a completely new research program based on the
way quantifications can be relativized. Although this approach succeeded in its
original task, we cannot consider it satisfactory in spotting the reasons why
some complex extensions of modal logic are well-behaved. In particular, by just
using the related results, we are not able to derive the decidability of
multi-agent logics for strategic abilities. In this paper, aiming to lay the
foundation for a more thorough understanding of some of these decidability
questions, we introduce a new kind of classification based on the binding forms
that are admitted in a sentence, i.e., on the way the arguments of a relation
can be bound to a variable. We describe a hierarchy of first-order fragments
based on the Boolean combinations of these forms, showing that the less
expressive one is already incomparable with the guarded logic and related
extensions. We also prove, via a new model-theoretic technique, that it enjoys
the finite-model property and a PSpace satisfiability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1547</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1547</id><created>2014-04-06</created><updated>2014-09-28</updated><authors><author><keyname>Park</keyname><forenames>Jihong</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>Asymptotic Behavior of Ultra-Dense Cellular Networks and Its Economic
  Impact</title><categories>cs.IT cs.NI math.IT</categories><comments>This paper will appear in Proc. IEEE Global Commun. Conf. (GLOBECOM)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the relationship between base station (BS) density
and average spectral efficiency (SE) in the downlink of a cellular network.
This relationship has been well known for sparse deployment, i.e. when the
number of BSs is small compared to the number of users. In this case the SE is
independent of BS density. As BS density grows, on the other hand, it has
previously been shown that increasing the BS density increases the SE, but no
tractable form for the SE-BS density relationship has yet been derived. In this
paper we derive such a closed-form result that reveals the SE is asymptotically
a logarithmic function of BS density as the density grows. Further, we study
the impact of this result on the network operator's profit when user demand
varies, and derive the profit maximizing BS density and the optimal amount of
spectrum to be utilized in closed forms. In addition, we provide deployment
planning guidelines that will aid the operator in his decision if he should
invest in densifying his network or in acquiring more spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1559</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1559</id><created>2014-04-06</created><authors><author><keyname>Vidya</keyname><forenames>R.</forenames></author><author><keyname>Nasira</keyname><forenames>Dr. G. M.</forenames></author><author><keyname>Priyankka</keyname><forenames>R. P. Jaia</forenames></author></authors><title>Sparse Coding: A Deep Learning using Unlabeled Data for High - Level
  Representation</title><categories>cs.LG cs.NE</categories><comments>4 Pages, 3 Figures, 2014 World Congress on Computing and
  Communication Technologies (WCCCT)</comments><journal-ref>Vidya R, Dr. Naisra G.M, Priyankka R.P. Jaia, &quot;Sparse Coding: A
  Deep Learning using Unlabeled Data for High - Level Representation&quot; IEEE
  Xplore 2014</journal-ref><doi>10.1109/WCCCT.2014.69</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Sparse coding algorithm is an learning algorithm mainly for unsupervised
feature for finding succinct, a little above high - level Representation of
inputs, and it has successfully given a way for Deep learning. Our objective is
to use High - Level Representation data in form of unlabeled category to help
unsupervised learning task. when compared with labeled data, unlabeled data is
easier to acquire because, unlike labeled data it does not follow some
particular class labels. This really makes the Deep learning wider and
applicable to practical problems and learning. The main problem with sparse
coding is it uses Quadratic loss function and Gaussian noise mode. So, its
performs is very poor when binary or integer value or other Non- Gaussian type
data is applied. Thus first we propose an algorithm for solving the L1 -
regularized convex optimization algorithm for the problem to allow High - Level
Representation of unlabeled data. Through this we derive a optimal solution for
describing an approach to Deep learning algorithm by using sparse code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1560</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1560</id><created>2014-04-06</created><authors><author><keyname>Stadnik</keyname><forenames>Vadim</forenames></author></authors><title>Fast Sequential Summation Algorithms Using Augmented Data Structures</title><categories>cs.DS</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an introduction to the design of augmented data
structures that offer an efficient representation of a mathematical sequence
and fast sequential summation algorithms, which guarantee both logarithmic
running time and logarithmic computational cost in terms of the total number of
operations. In parallel summation algorithms, logarithmic running time is
achieved by high linear computational cost with a linear number of processors.
The important practical advantage of the fast sequential summation algorithms
is that they do not require supercomputers and can run on the cheapest single
processor systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1561</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1561</id><created>2014-04-06</created><updated>2014-05-27</updated><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Suter</keyname><forenames>David</forenames></author></authors><title>Fast Supervised Hashing with Decision Trees for High-Dimensional Data</title><categories>cs.CV cs.LG</categories><comments>Appearing in Proc. IEEE Conf. Computer Vision and Pattern
  Recognition, 2014, Ohio, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised hashing aims to map the original features to compact binary codes
that are able to preserve label based similarity in the Hamming space.
Non-linear hash functions have demonstrated the advantage over linear ones due
to their powerful generalization capability. In the literature, kernel
functions are typically used to achieve non-linearity in hashing, which achieve
encouraging retrieval performance at the price of slow evaluation and training
time. Here we propose to use boosted decision trees for achieving non-linearity
in hashing, which are fast to train and evaluate, hence more suitable for
hashing with high dimensional data. In our approach, we first propose
sub-modular formulations for the hashing binary code inference problem and an
efficient GraphCut based block search method for solving large-scale inference.
Then we learn hash functions by training boosted decision trees to fit the
binary codes. Experiments demonstrate that our proposed method significantly
outperforms most state-of-the-art methods in retrieval precision and training
time. Especially for high-dimensional data, our method is orders of magnitude
faster than many methods in terms of training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1568</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1568</id><created>2014-04-06</created><updated>2014-08-31</updated><authors><author><keyname>Eisenbrand</keyname><forenames>Friedrich</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Geometric Random Edge</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a variant of the random-edge pivoting rule results in a strongly
polynomial time simplex algorithm for linear programs $\max\{c^Tx \colon Ax\leq
b\}$, whose constraint matrix $A$ satisfies a geometric property introduced by
Brunsch and R\&quot;oglin: The sine of the angle of a row of $A$ to a hyperplane
spanned by $n-1$ other rows of $A$ is at least $\delta$. This property is a
geometric generalization of $A$ being integral and all sub-determinants of $A$
being bounded by $\Delta$ in absolute value (since $\delta \geq 1/(\Delta^2
n)$). In particular, linear programs defined by totally unimodular matrices are
captured in this famework ($\delta \geq 1/ n$) for which Dyer and Frieze
previously described a strongly polynomial-time randomized algorithm.
  The number of pivots of the simplex algorithm is polynomial in the dimension
and $1/\delta$ and independent of the number of constraints of the linear
program. Our main result can be viewed as an algorithmic realization of the
proof of small diameter for such polytopes by Bonifas et al., using the ideas
of Dyer and Frieze.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1577</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1577</id><created>2014-04-06</created><authors><author><keyname>Cho</keyname><forenames>YounSun</forenames></author></authors><title>Detecting a Corrupted Area in a 2-Dimensional Space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that 2-dimensional data have become popularly used in
many applications without being much considered its integrity checking. We
introduce the problem of detecting a corrupted area in a 2-dimensional space,
and investigate two possible efficient approaches and show their time and space
complexities. Also, we briefly introduce the idea of an approximation scheme
using a hash sieve and suggest a novel &quot;adaptive tree&quot; structure revealing
granularity of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1588</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1588</id><created>2014-04-06</created><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>Gaussian Networks Generated by Random Walks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>12 pages, 6 figures</comments><doi>10.1007/s10955-014-1175-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a random walks based model to generate complex networks. Many
authors studied and developed different methods and tools to analyze complex
networks by random walk processes. Just to cite a few, random walks have been
adopted to perform community detection, exploration tasks and to study temporal
networks. Moreover, they have been used also to generate scale-free networks.
In this work, we define a random walker that plays the role of
&quot;edges-generator&quot;. In particular, the random walker generates new connections
and uses these ones to visit each node of a network. As result, the proposed
model allows to achieve networks provided with a Gaussian degree distribution,
and moreover, some features as the clustering coefficient and the assortativity
show a critical behavior. Finally, we performed numerical simulations to study
the behavior and the properties of the cited model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1592</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1592</id><created>2014-04-06</created><updated>2014-07-29</updated><authors><author><keyname>Huang</keyname><forenames>Longbo</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Hao</keyname><forenames>Xiaohong</forenames></author></authors><title>The Power of Online Learning in Stochastic Network Optimization</title><categories>math.OC cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the power of online learning in stochastic
network optimization with unknown system statistics {\it a priori}. We are
interested in understanding how information and learning can be efficiently
incorporated into system control techniques, and what are the fundamental
benefits of doing so. We propose two \emph{Online Learning-Aided Control}
techniques, $\mathtt{OLAC}$ and $\mathtt{OLAC2}$, that explicitly utilize the
past system information in current system control via a learning procedure
called \emph{dual learning}. We prove strong performance guarantees of the
proposed algorithms: $\mathtt{OLAC}$ and $\mathtt{OLAC2}$ achieve the
near-optimal $[O(\epsilon), O([\log(1/\epsilon)]^2)]$ utility-delay tradeoff
and $\mathtt{OLAC2}$ possesses an $O(\epsilon^{-2/3})$ convergence time.
$\mathtt{OLAC}$ and $\mathtt{OLAC2}$ are probably the first algorithms that
simultaneously possess explicit near-optimal delay guarantee and sub-linear
convergence time. Simulation results also confirm the superior performance of
the proposed algorithms in practice. To the best of our knowledge, our attempt
is the first to explicitly incorporate online learning into stochastic network
optimization and to demonstrate its power in both theory and practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1601</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1601</id><created>2014-04-06</created><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Density Evolution for Min-Sum Decoding of LDPC Codes Under Unreliable
  Message Storage</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the performance of quantized min-sum decoding of low-density
parity-check codes under unreliable message storage. To this end, we introduce
a simple bit-level error model and show that decoder symmetry is preserved
under this model. Subsequently, we formulate the corresponding density
evolution equations to predict the average bit error probability in the limit
of infinite blocklength. We present numerical threshold results and we show
that using more quantization bits is not always beneficial in the context of
faulty decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1602</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1602</id><created>2014-04-06</created><updated>2014-04-23</updated><authors><author><keyname>Pallister</keyname><forenames>James</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author><author><keyname>Hollis</keyname><forenames>Simon J.</forenames></author><author><keyname>Bennett</keyname><forenames>Jeremy</forenames></author></authors><title>A high-level model of embedded flash energy consumption</title><categories>cs.AR</categories><doi>10.1145/2656106.2656108</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The alignment of code in the flash memory of deeply embedded SoCs can have a
large impact on the total energy consumption of a computation. We investigate
the effect of code alignment in six SoCs and find that a large proportion of
this energy (up to 15% of total SoC energy consumption) can be saved by changes
to the alignment.
  A flexible model is created to predict the read-access energy consumption of
flash memory on deeply embedded SoCs, where code is executed in place. This
model uses the instruction level memory accesses performed by the processor to
calculate the flash energy consumption of a sequence of instructions. We derive
the model parameters for five SoCs and validate them. The error is as low as
5%, with a 11% average normalized RMS deviation overall.
  The scope for using this model to optimize code alignment is explored across
a range of benchmarks and SoCs. Analysis shows that over 30% of loops can be
better aligned. This can significantly reduce energy while increasing code size
by less than 4%. We conclude that this effect has potential as an effective
optimization, saving significant energy in deeply embedded SoCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1610</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1610</id><created>2014-04-06</created><authors><author><keyname>Chung</keyname><forenames>Julianne</forenames></author><author><keyname>Chung</keyname><forenames>Matthias</forenames></author></authors><title>An Efficient Approach for Computing Optimal Low-Rank Regularized Inverse
  Matrices</title><categories>math.NA cs.NA</categories><comments>24 pages, 11 figures</comments><doi>10.1088/0266-5611/30/11/114009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard regularization methods that are used to compute solutions to
ill-posed inverse problems require knowledge of the forward model. In many
real-life applications, the forward model is not known, but training data is
readily available. In this paper, we develop a new framework that uses training
data, as a substitute for knowledge of the forward model, to compute an optimal
low-rank regularized inverse matrix directly, allowing for very fast
computation of a regularized solution. We consider a statistical framework
based on Bayes and empirical Bayes risk minimization to analyze theoretical
properties of the problem. We propose an efficient rank update approach for
computing an optimal low-rank regularized inverse matrix for various error
measures. Numerical experiments demonstrate the benefits and potential
applications of our approach to problems in signal and image processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1614</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1614</id><created>2014-04-06</created><authors><author><keyname>Churchill</keyname><forenames>Alexander W.</forenames></author><author><keyname>Sigtia</keyname><forenames>Siddharth</forenames></author><author><keyname>Fernando</keyname><forenames>Chrisantha</forenames></author></authors><title>A Denoising Autoencoder that Guides Stochastic Search</title><categories>cs.NE cs.LG</categories><comments>Submitted to Parallel Problem Solving from Nature 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm is described that adaptively learns a non-linear mutation
distribution. It works by training a denoising autoencoder (DA) online at each
generation of a genetic algorithm to reconstruct a slowly decaying memory of
the best genotypes so far. A compressed hidden layer forces the autoencoder to
learn hidden features in the training set that can be used to accelerate search
on novel problems with similar structure. Its output neurons define a
probability distribution that we sample from to produce offspring solutions.
The algorithm outperforms a canonical genetic algorithm on several
combinatorial optimisation problems, e.g. multidimensional 0/1 knapsack
problem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastrigin
and Rosenbrock functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1621</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1621</id><created>2014-04-06</created><updated>2014-07-11</updated><authors><author><keyname>Klimek</keyname><forenames>Radoslaw</forenames></author><author><keyname>Kotulski</keyname><forenames>Leszek</forenames></author></authors><title>Proposal of a multiagent-based smart environment for the IoT</title><categories>cs.SE</categories><comments>10th International Conference on Intelligent Environments (IE'14),
  9th Workshop on Artificial Intelligence Techniques for Ambient Intelligence
  (AITAmI'14), Shanghai, China, 30th June-1st of July 2014</comments><doi>10.3233/978-1-61499-411-4-37</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work relates to context-awareness of things that belong to IoT networks.
Preferences understood as a priority in selection are considered, and dynamic
preference models for such systems are built. Preference models are based on
formal logic, and they are built on-the-fly by software agents observing the
behavior of users/inhabitants, and gathering knowledge about preferences
expressed in terms of logical specifications. A 3-level structure of agents has
been introduced to support IoT inference. These agents cooperate with each
other basing on the graph representation of the system knowledge. An example of
such a system is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1622</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1622</id><created>2014-04-06</created><updated>2014-11-28</updated><authors><author><keyname>Liao</keyname><forenames>Ruizhi</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>MU-MIMO MAC Protocols for Wireless Local Area Networks: A Survey</title><categories>cs.NI</categories><comments>Accepted by IEEE Communications Surveys and Tutorials, 40 pages, 12
  figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As wireless devices boom, and bandwidth-hungry applications (e.g., video and
cloud uploading) get popular, today's Wireless Local Area Networks (WLANs)
become not only crowded but also stressed at throughput. Multi-user
Multiple-Input and Multiple-Output (MU-MIMO), an advanced form of MIMO, has
gained attention due to its huge potential in improving the performance of
WLANs.
  This paper surveys random access based MAC protocols for MU-MIMO enabled
WLANs. It first provides background information about the evolution and the
fundamental MAC schemes of IEEE 802.11 Standards and Amendments, and then
identifies the key requirements of designing MU-MIMO MAC protocols for WLANs.
After that, the most representative MU-MIMO MAC proposals in the literature are
overviewed by benchmarking their MAC procedures and examining the key
components, such as the channel state information acquisition, de/pre-coding
and scheduling schemes. Classifications and discussions on important findings
of the surveyed MAC protocols are provided, based on which, the research
challenges for designing effective MU-MIMO MAC protocols, as well as the
envisaged MAC's role in the future heterogeneous networks, are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1634</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1634</id><created>2014-04-06</created><authors><author><keyname>Kordy</keyname><forenames>Barbara</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Mauw</keyname><forenames>Sjouke</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Pieters</keyname><forenames>Wolter</forenames><affiliation>Delft University of Technology and University of Twente</affiliation></author></authors><title>Proceedings First International Workshop on Graphical Models for
  Security</title><categories>cs.CR</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 148, 2014</journal-ref><doi>10.4204/EPTCS.148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present volume contains the proceedings of the First International
Workshop on Graphical Models for Security (GraMSec'14). The workshop was held
in Grenoble, France, on April 12, 2014, as one of the satellite events of the
European Joint Conferences on Theory and Practice of Software 2014 (ETAPS'14).
  Graphical security models provide an intuitive but systematic methodology to
analyze security weaknesses of systems and to evaluate potential protection
measures. Such models have been subject of academic research and they have also
been widely accepted by the industrial sector, as a means to support and
facilitate threat analysis and risk management processes.
  The objective of GraMSec is to contribute to the development of well-founded
graphical security models, efficient algorithms for their analysis, as well as
methodologies for their practical usage. The workshop brings together academic
researchers and industry practitioners designing and employing visual models
for security in order to provide a platform for discussion, knowledge exchange
and collaborations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1637</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1637</id><created>2014-04-06</created><authors><author><keyname>Klimiankou</keyname><forenames>Yauhen</forenames></author></authors><title>An Enhanced Multi-Pager Environment Support for Second Generation
  Microkernels</title><categories>cs.OS</categories><comments>7 pages, 4 figures, 1 table</comments><journal-ref>International Journal of Computer Science and Information
  Security, Vol. 12 No. 1 JAN 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this paper is to present a mechanism of enhanced paging
support for the second generation microkernels in the form of explicit support
of multi-pager environment for the tasks running in the system. Proposed
mechanism is based on the intra-kernel high granularity pagers assignments per
virtual address space, which allow efficient and simple dispatching of page
faults to the appropriate pagers. The paging is one of the major features of
the virtual memory, which is extensively used by advanced operating systems to
provide an illusion of elastic memory. Original and present second generation
microkernels provide only limited, inflexible and unnatural support for paging.
Furthermore, facilities provided by current solutions for multi-pager support
on the runtime level introduce an overhead in terms of mode switches and thread
context switches which can be significantly reduced. Limited paging support
limits the attractiveness of the second generation microkernel based systems
use in real-life applications, in which processes usually have concurrent
servicing of multiple paging servers. The purpose of this paper is to present a
facilities for the efficient and flexible support of multi-pager environments
for the second generation microkernels. A comparison of the proposed solution
to the present architecture L4 + L4Re has been made and overhead of the page
fault handling critical path has been evaluated. Proposed solution is simple
enough and provides a natural and flexible support of multi-pager environments
for second generation microkernels in efficient way. It introduces a third less
overhead in terms of the mode switches and thread context switches in
comparison to the present L4 + L4Re solution implemented in the Fiasco.OC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1645</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1645</id><created>2014-04-06</created><authors><author><keyname>Hu</keyname><forenames>Li</forenames></author><author><keyname>Yuan`an</keyname><forenames>Liu</forenames></author><author><keyname>Dongming</keyname><forenames>Yuan</forenames></author><author><keyname>Hefei</keyname><forenames>Hu</forenames></author><author><keyname>Sirui</keyname><forenames>Duan</forenames></author></authors><title>Utility Optimal Scheduling in Degree-Limited Satellite Networks</title><categories>cs.NI</categories><comments>9 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1012.1945 by other authors</comments><journal-ref>Journal of Computational Information Systems, v 9, n 9, p
  3723-3731, May 1, 2013</journal-ref><doi>10.12733/jcis6031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of flow control together with power
allocation to antennas on satellite with arbitrary link states, so as to
maximize the utility function while stabilizing the network. Inspired by
Lyapunov optimization method, a Degree-Limited Scheduling Algorithm (DLSA) is
proposed with a control parameter V, which requires no stochastic knowledge of
link state. Discussion about implementation is carried out about the complexity
of DLSA and several approximation methods to reduce complexity. Analyze shows
DLSA stabilizes the network and the gap between utility function under DLSA and
the optimal value is arbitrarily close to zero on the order of O(1/V).
Simulation results verify DLSA on a simple network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1404.1646</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1404.1646</id><created>2014-04-06</created><authors><author><keyname>Ruiz</keyname><forenames>Guillermo</forenames></author><author><keyname>Ch&#xe1;vez</keyname><forenames>Edgar</forenames></author></authors><title>Proximal Navigation Graphs and t-spanners</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(X,\mathbf{d})$ be a metric space, $V\subseteq X$ a finite set, and $E
\subseteq V \times V$. We call the graph $G(E,V)$ a {\em metric} graph if each
edge $(u,v) \in E$ has weight $d(u,v)$. In particular edge $(u,u)$ is in the
graph and have distance $0$. We call $G$ a {\em proximal navigation graph} or
$PN$-graph if for each edge $(u,v) \in E$ either $u=v$ or there is a node $u_1$
such that $(u,u_1) \in E$ and $\mathbf{d}(u,v) &gt; \mathbf{d}(u_1,v)$. In such
graph it is possible to navigate greedily from an arbitrary source node to an
arbitrary target node by reducing the distance between the current node and the
target node in each step. The complete graph, the Delaunay triangulation and
the Half Space Proximal (HSP) graph (defined below in the paper) are examples
of $PN$-graphs.
  In this paper we study the relationship between $PN$-graphs and $t$-spanners
and prove that there are $PN$-graphs that are not $t$-spanners for any $t$. On
the positive side we give sufficient conditions for a $PN$-graph to be a
$t$-spanner and prove that any $PN$-graph over $\mathbb{R}^n$ under the
euclidean distance is a $t$-spanner.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="58000" completeListSize="102538">1122234|59001</resumptionToken>
</ListRecords>
</OAI-PMH>
