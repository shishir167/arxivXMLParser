<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:56:24Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|84001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05257</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05257</id><created>2015-09-17</created><authors><author><keyname>Lam</keyname><forenames>Hoang Thanh</forenames></author><author><keyname>Diaz-Aviles</keyname><forenames>Ernesto</forenames></author><author><keyname>Pascale</keyname><forenames>Alessandra</forenames></author><author><keyname>Gkoufas</keyname><forenames>Yiannis</forenames></author><author><keyname>Chen</keyname><forenames>Bei</forenames></author></authors><title>(Blue) Taxi Destination and Trip Time Prediction from Partial
  Trajectories</title><categories>stat.ML cs.AI cs.CY cs.LG</categories><comments>ECML/PKDD Discovery Challenge 2015</comments><acm-class>I.2.6; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time estimation of destination and travel time for taxis is of great
importance for existing electronic dispatch systems. We present an approach
based on trip matching and ensemble learning, in which we leverage the patterns
observed in a dataset of roughly 1.7 million taxi journeys to predict the
corresponding final destination and travel time for ongoing taxi trips, as a
solution for the ECML/PKDD Discovery Challenge 2015 competition. The results of
our empirical evaluation show that our approach is effective and very robust,
which led our team -- BlueTaxi -- to the 3rd and 7th position of the final
rankings for the trip time and destination prediction tasks, respectively.
Given the fact that the final rankings were computed using a very small test
set (with only 320 trips) we believe that our approach is one of the most
robust solutions for the challenge based on the consistency of our good results
across the test sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05265</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05265</id><created>2015-09-17</created><authors><author><keyname>Toosi</keyname><forenames>Farshad Ghassemi</forenames></author><author><keyname>Nikolov</keyname><forenames>Nikola S.</forenames></author></authors><title>Sync-and-Burst: Force-Directed Graph Drawing with Uniform Force
  Magnitudes</title><categories>cs.CG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a force-directed algorithm, called Sync-and-Burst, which falls
into the category of classical force-directed graph drawing algorithms. A
distinct feature in Sync-and-Burst is the use of simplified forces of
attraction and repulsion whose magnitude does not depend on the distance
between vertices. Instead, magnitudes are uniform throughout the graph at each
iteration and monotonically increase as the number of iterations grows. The
Sync-and-Burst layouts are always circular in shape with relatively even
distribution of vertices throughout the drawing area. We demonstrate that
aesthetically pleasing layouts are achieved in O(n) iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05267</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05267</id><created>2015-09-17</created><authors><author><keyname>Gibert</keyname><forenames>Xavier</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Deep Multi-task Learning for Railway Track Inspection</title><categories>cs.CV</categories><comments>Submitted to IEEE Trans. on Pattern Analysis and Machine Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Railroad tracks need to be periodically inspected and monitored to ensure
safe transportation. Automated track inspection using computer vision and
pattern recognition methods have recently shown the potential to improve safety
by allowing for more frequent inspections while reducing human errors.
Achieving full automation is still very challenging due to the number of
different possible failure modes as well as the broad range of image variations
that can potentially trigger false alarms. Also, the number of defective
components is very small, so not many training examples are available for the
machine to learn a robust anomaly detector. In this paper, we show that
detection performance can be improved by combining multiple detectors within a
multi-task learning framework. We show that this approach results in better
accuracy in detecting defects on railway ties and fasteners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05281</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05281</id><created>2015-09-17</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Network analysis of named entity interactions in written texts</title><categories>cs.CL physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of methods borrowed from statistics and physics has allowed for the
discovery of unprecedent patterns of human behavior and cognition by
establishing links between models features and language structure. While
current models have been useful to identify patterns via analysis of
syntactical and semantical networks, only a few works have probed the relevance
of investigating the structure arising from the relationship between relevant
entities such as characters, locations and organizations. In this study, we
introduce a model that links entities appearing in the same context in order to
capture the complexity of entities organization through a networked
representation. Computational simulations in books revealed that the proposed
model displays interesting topological features, such as short typical shortest
path length, high values of clustering coefficient and modular organization.
The effectiveness of the our model was verified in a practical pattern
recognition task in real networks. When compared with the traditional word
adjacency networks, our model displayed optimized results in identifying
unknown references in texts. Because the proposed model plays a complementary
role in characterizing unstructured documents via topological analysis of named
entities, we believe that it could be useful to improve the characterization
written texts when combined with other traditional approaches based on
statistical and deeper paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05290</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05290</id><created>2015-09-17</created><authors><author><keyname>Ortega</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Milano</keyname><forenames>Federico</forenames></author></authors><title>Generalized Model of VSC-based Energy Storage Systems for Transient
  Stability Analysis</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a generalized energy storage system model for voltage and
angle stability analysis. The proposed solution allows modeling most common
energy storage technologies through a given set of linear differential
algebraic equations (DAEs). In particular, the paper considers, but is not
limited to, compressed air, superconducting magnetic, electrochemical capacitor
and battery energy storage devices. While able to cope with a variety of
different technologies, the proposed generalized model proves to be accurate
for angle and voltage stability analysis, as it includes a balanced,
fundamental-frequency model of the voltage source converter (VSC) and the
dynamics of the dc link. Regulators with inclusion of hard limits are also
taken into account. The transient behavior of the generalized model is compared
with detailed fundamental-frequency balanced models as well as commonly-used
simplified models of energy storage devices. A comprehensive case study based
on the WSCC 9-bus test system is presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05293</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05293</id><created>2015-09-17</created><authors><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Decentralized Throughput Maximizing Policies for Deadline-Constrained
  Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-hop wireless networks serving multiple flows in which only
packets that meet hard end-to-end deadline constraints are useful, i.e., if a
packet is not delivered to its destination node by its deadline, it is dropped
from the network. We design decentralized scheduling policies for such
multi-hop networks that attain the maximum throughput of useful packets. The
resulting policy is decentralized in the sense that in order to make a
transmission decision, a node only needs to know the &quot;time-till-deadline&quot; of
the packets that are currently present at that node, and not the state of the
entire network. The key to obtaining an easy-to-implement and highly
decentralized policy is to replace the hard constraint on the number of
simultaneous packet transmissions that can take place on the outgoing links of
a node, by a time-average constraint on the number of transmissions. The policy
thus obtained is guaranteed to provide maximum throughput. Analysis can be
extended to the case of time-varying channel conditions in a straightforward
manner.
  Simulations showing significant improvement over existing policies for
deadline based scheduling, such as Earliest Deadline First, and supporting the
theory, are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05301</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05301</id><created>2015-09-17</created><authors><author><keyname>Schetinger</keyname><forenames>Victor</forenames></author><author><keyname>Oliveira</keyname><forenames>Manuel M.</forenames></author><author><keyname>da Silva</keyname><forenames>Roberto</forenames></author><author><keyname>Carvalho</keyname><forenames>Tiago J.</forenames></author></authors><title>Humans Are Easily Fooled by Digital Images</title><categories>cs.GR cs.CV cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital images are ubiquitous in our modern lives, with uses ranging from
social media to news, and even scientific papers. For this reason, it is
crucial evaluate how accurate people are when performing the task of identify
doctored images. In this paper, we performed an extensive user study evaluating
subjects capacity to detect fake images. After observing an image, users have
been asked if it had been altered or not. If the user answered the image has
been altered, he had to provide evidence in the form of a click on the image.
We collected 17,208 individual answers from 383 users, using 177 images
selected from public forensic databases. Different from other previously
studies, our method propose different ways to avoid lucky guess when evaluating
users answers. Our results indicate that people show inaccurate skills at
differentiating between altered and non-altered images, with an accuracy of
58%, and only identifying the modified images 46.5% of the time. We also track
user features such as age, answering time, confidence, providing deep analysis
of how such variables influence on the users' performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05305</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05305</id><created>2015-09-17</created><authors><author><keyname>Albert</keyname><forenames>Carlo</forenames></author><author><keyname>Ulzega</keyname><forenames>Simone</forenames></author></authors><title>Bayesian Parameter Inference for 1D Nonlinear Stochastic Differential
  Equation Models</title><categories>cs.DS stat.CO</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian statistics has become an indispensable tool in many applied
sciences, in particular, for the purpose of uncertainty analysis. Inferring
parametric uncertainty, for stochastic differential equation (SDE) models,
however, is a computationally hard problem due to the high dimensional
integrals that have to be calculated. Here, we consider the generic problem of
calibrating a one dimensional SDE model to time series and quantifying the
ensuing parametric uncertainty. We re-interpret the Bayesian posterior
distribution, for model parameters, as the partition function of a statistical
mechanical system and employ a Hamiltonian Monte Carlo algorithm to sample from
it. Depending on the number of discretization points and the number of
measurement points the dynamics of this system happens on very different time
scales. Thus, we employ a multiple time scale integration together with a
suitable re-parametrization to derive an efficient inference algorithm. While
the algorithm is presented by means of a simple SDE model from hydrology, it is
readily applicable to a wide range of inference problems. Furthermore the
algorithm is highly parallelizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05315</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05315</id><created>2015-09-17</created><authors><author><keyname>Albert</keyname><forenames>Carlo</forenames></author></authors><title>A Simulated Annealing Approach to Bayesian Inference</title><categories>stat.CO cs.AI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generic algorithm for the extraction of probabilistic (Bayesian)
information about model parameters from data is presented. The algorithm
propagates an ensemble of particles in the product space of model parameters
and outputs. Each particle update consists of a random jump in parameter space
followed by a simulation of a model output and a Metropolis
acceptance/rejection step based on a comparison of the simulated output to the
data. The distance of a particle to the data is interpreted as an energy and
the algorithm is reducing the associated temperature of the ensemble such that
entropy production is minimized. If this simulated annealing is not too fast
compared to the mixing speed in parameter space, the parameter marginal of the
ensemble approaches the Bayesian posterior distribution. Annealing is adaptive
and depends on certain extensive thermodynamic quantities that can easily be
measured throughout run-time. In the general case, we propose annealing with a
constant entropy production rate, which is optimal as long as annealing is not
too fast. For the practically relevant special case of no prior knowledge, we
derive an optimal fast annealing schedule with a non-constant entropy
production rate. The algorithm does not require the calculation of the density
of the model likelihood, which makes it interesting for Bayesian parameter
inference with stochastic models, whose likelihood functions are typically very
high dimensional integrals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05317</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05317</id><created>2015-09-17</created><authors><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Optimizing Quality of Experience of Dynamic Video Streaming over Fading
  Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of video streaming packets from an Access Point (AP)
to multiple clients over a shared wireless channel with fading. In such
systems, each client maintains a buffer of packets from which to play the
video, and an outage occurs in the streaming whenever the buffer is empty.
Clients can switch to a lower-quality of video packet, or request packet
transmission at a higher energy level in order to minimize the number of
outages plus the number of outage periods and the number of low quality video
packets streamed, while there is an average power constraint on the AP.
  We pose the problem of choosing the video quality and transmission power as a
Constrained Markov Decision Process (CMDP). We show that the problem involving
$N$ clients decomposes into $N$ MDPs, each involving only a single client, and
furthermore that the optimal policy has a threshold structure, in which the
decision to choose the video-quality and power-level of transmission depends
solely on the buffer-level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05318</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05318</id><created>2015-09-17</created><updated>2015-12-11</updated><authors><author><keyname>Dom&#xed;nguez</keyname><forenames>Jes&#xfa;s</forenames><affiliation>King's College London</affiliation></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames><affiliation>King's College London</affiliation></author></authors><title>From nominal to higher-order rewriting and back again</title><categories>cs.LO</categories><comments>41 pages, journal</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:9) 2015</journal-ref><doi>10.2168/LMCS-11(4:9)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a translation function from nominal rewriting systems (NRSs) to
combinatory reduction systems (CRSs), transforming closed nominal rules and
ground nominal terms to CRSs rules and terms, respectively, while preserving
the rewriting relation. We also provide a reduction-preserving translation in
the other direction, from CRSs to NRSs, improving over a previously defined
translation. These tools, together with existing translations between CRSs and
other higher-order rewriting formalisms, open up the path for a transfer of
results between higher-order and nominal rewriting. In particular, techniques
and properties of the rewriting relation, such as termination, can be exported
from one formalism to the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05322</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05322</id><created>2015-09-17</created><authors><author><keyname>Gairing</keyname><forenames>Martin</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>Computing stable outcomes in symmetric additively-separable hedonic
  games</title><categories>cs.GT</categories><comments>Combines a SAGT 2010 paper and a AAMAS 2011 paper by the same authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of finding stable outcomes in hedonic
games, which are a class of coalition formation games. We restrict our
attention to symmetric additively-separable hedonic games, which are a
nontrivial subclass of such games that are guaranteed to possess stable
outcomes. These games are specified by an undirected edge- weighted graph:
nodes are players, an outcome of the game is a partition of the nodes into
coalitions, and the utility of a node is the sum of incident edge weights in
the same coalition. We consider several stability requirements defined in the
literature. These are based on restricting feasible player deviations, for
example, by giving existing coalition members veto power. We extend these
restrictions by considering more general forms of preference aggregation for
coalition members. In particular, we consider voting schemes to decide if
coalition members will allow a player to enter or leave their coalition. For
all of the stability requirements we consider, the existence of a stable
outcome is guaranteed by a potential function argument, and local improvements
will converge to a stable outcome. We provide an almost complete
characterization of these games in terms of the tractability of computing such
stable outcomes. Our findings comprise positive results in the form of
polynomial-time algorithms, and negative (PLS-completeness) results. The
negative results extend to more general hedonic games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05329</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05329</id><created>2015-09-17</created><authors><author><keyname>S&#xf8;nderby</keyname><forenames>S&#xf8;ren Kaae</forenames></author><author><keyname>S&#xf8;nderby</keyname><forenames>Casper Kaae</forenames></author><author><keyname>Maal&#xf8;e</keyname><forenames>Lars</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Recurrent Spatial Transformer Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We integrate the recently proposed spatial transformer network (SPN)
[Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an
RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST
sequences. The proposed model achieves a single digit error of 1.5% compared to
2.9% for a convolutional networks and 2.0% for convolutional networks with SPN
layers. The SPN outputs a zoomed, rotated and skewed version of the input
image. We investigate different down-sampling factors (ratio of pixel in input
and output) for the SPN and show that the RNN-SPN model is able to down-sample
the input images without deteriorating performance. The down-sampling in
RNN-SPN can be thought of as adaptive down-sampling that minimizes the
information loss in the regions of interest. We attribute the superior
performance of the RNN-SPN to the fact that it can attend to a sequence of
regions of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05330</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05330</id><created>2015-09-15</created><authors><author><keyname>Fakra</keyname><forenames>Ali Hamada</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Miranville</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bigot</keyname><forenames>Dimitri</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author></authors><title>Elements of Validation of Artificial Lighting through the Software
  CODYRUN: Application to a Test Case of the International Commission on
  Illumination (CIE)</title><categories>cs.GR</categories><comments>IASTED Power and Energy Systems 2010, Sep 2010, Gaborone, Botswana.
  2010</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CODYRUN is a software for computational aeraulic and thermal simulation in
buildings developed by the Laboratory of Building Physics and Systems
(L.P.B.S). Numerical simulation codes of artificial lighting have been
introduced to extend the tool capacity. These calculation codes are able to
predict the amount of light received by any point of a given working plane and
from one or more sources installed on the ceiling of the room. The model used
for these calculations is original and semi-detailed (simplified). The test
case references of the task-3 TC-33 International Commission on Illumination
(CIE) were applied to the software to ensure reliability to properly handle
this photometric aspect. This allowed having a precise idea about the
reliability of the results of numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05338</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05338</id><created>2015-09-17</created><authors><author><keyname>Recario</keyname><forenames>Reginald Neil C.</forenames></author><author><keyname>de Robles</keyname><forenames>Marie Betel B.</forenames></author><author><keyname>Bautista</keyname><forenames>Kristine Elaine P.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Social Loafing Among Members of Undergraduate Software Engineering
  Groups: Persistence of Perception Seven Years After</title><categories>cs.CY</categories><comments>16 pages, 5 figures, contributed article submitted to the 13th
  National Conference on Information Technology Education(NCITE 2015), Angeles
  University Foundation, Angeles City, Pampanga, Philippines, 22-24 October
  2015. arXiv admin note: text overlap with arXiv:1507.08345</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We surveyed 169 undergraduate students who are enrolled in various courses.
They were members of software engineering groups formed to solve various
real-world computational problems by implementing software projects as part of
the requirements of the course. This time, our analysis show that task
visibility is negatively associated with social loafing while contributions,
dominance, aggression and sucker effect are positively correlated.
  We further found out that perception of social loafing exists and still
persists among members of computer programming groups. Compared to our 2008
analysis, we provide in this paper detailed analysis based on demographic
parameters such as gender, course taken, age group, type of residence (urban or
rural), and region of residence. The implication of this result is that aside
from the usual problems that an instructor faces in teaching software
engineering-related courses, the presence of social loafing also adds to the
impediment of teaching effectiveness. Thus, it is imperative that instructors
and course designers consider the implications associated with social loafing
when designing group projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05348</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05348</id><created>2015-09-17</created><authors><author><keyname>Strapasson</keyname><forenames>Jo&#xe3;o E.</forenames></author><author><keyname>Jorge</keyname><forenames>Grasiele C.</forenames></author><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author></authors><title>Quasi-perfect codes in the $\ell_p$ metric</title><categories>cs.IT math.IT</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider quasi-perfect codes in $\mathbb{Z}^n$ over the $\ell_p$ metric,
$2 \leq p &lt; \infty$. Through a computational approach, we determine all radii
for which there are linear quasi-perfect codes for $p = 2$ and $n = 2, 3$.
Moreover, we study codes with a certain \textit{degree of imperfection}, a
notion that generalizes the quasi-perfect codes. Numerical results concerning
the codes with the smallest degree of imperfection are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05351</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05351</id><created>2015-09-17</created><authors><author><keyname>Mostafanasab</keyname><forenames>Hojjat</forenames></author></authors><title>Triple cyclic codes over $\mathbb{Z}_2$</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><msc-class>94B05, 94B15, 11T71, 13M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $r,s,t$ be three positive integers and $\mathcal{C}$ be a binary linear
code of lenght $r+s+t$. We say that $\mathcal{C}$ is a triple cyclic code of
lenght $(r,s,t)$ over $\mathbb{Z}_2$ if the set of coordinates can be
partitioned into three parts that any cyclic shift of the coordinates of the
parts leaves invariant the code. These codes can be considered as
$\mathbb{Z}_2[x]$-submodules of $\frac{\mathbb{Z}_2[x]}{\langle
x^r-1\rangle}\times\frac{\mathbb{Z}_2[x]}{\langle
x^s-1\rangle}\times\frac{\mathbb{Z}_2[x]}{\langle x^t-1\rangle}$. We give the
minimal generating sets of this kind of codes. Also, we determine the
relationship between the generators of triple cyclic codes and their duals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05360</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05360</id><created>2015-09-17</created><updated>2015-10-18</updated><authors><author><keyname>Huang</keyname><forenames>Jiaji</forenames></author><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Geometry-aware Deep Transform</title><categories>cs.CV</categories><comments>to appear in ICCV2015, updated with minor revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many recent efforts have been devoted to designing sophisticated deep
learning structures, obtaining revolutionary results on benchmark datasets. The
success of these deep learning methods mostly relies on an enormous volume of
labeled training samples to learn a huge number of parameters in a network;
therefore, understanding the generalization ability of a learned deep network
cannot be overlooked, especially when restricted to a small training set, which
is the case for many applications. In this paper, we propose a novel deep
learning objective formulation that unifies both the classification and metric
learning criteria. We then introduce a geometry-aware deep transform to enable
a non-linear discriminative and robust feature transform, which shows
competitive performance on small training sets for both synthetic and
real-world data. We further support the proposed framework with a formal
$(K,\epsilon)$-robustness analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05364</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05364</id><created>2015-09-17</created><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>W&#xe4;chter</keyname><forenames>Jan Philipp</forenames></author></authors><title>The Word Problem for Omega-Terms over the Trotter-Weil Hierarchy</title><categories>cs.FL math.GR</categories><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-variable first-order logic FO2 received a lot of attention during the
last few years. Over finite words, there is a tight connection between the
quantifier alternation hierarchy inside FO2 and a hierarchy of finite monoids:
the Trotter-Weil Hierarchy. The various ways of climbing up this hierarchy
include Mal'cev products, deterministic and co-deterministic concatenation as
well as identities of omega-terms. We show that the word problem for
omega-terms over each level of the Trotter-Weil Hierarchy is decidable; this
means, for every variety V of the hierarchy and every identity u=v of
omega-terms, one can decide whether all monoids in V satisfy u=v. More
precisely, for every fixed variety V, the problem is solvable in
nondeterministic logarithmic space (NL). In addition, we give deterministic
polynomial time algorithms which are more efficient than straightforward
translations of the NL-algorithms. From a language perspective, the word
problem for omega-terms is the following: For every language variety V in the
Trotter-Weil Hierarchy and every language variety W given by an identity of
omega-terms, one can decide whether V is contained in W. This includes the case
where V is some level of the FO2 quantifier alternation hierarchy. As an
application of our results, we show that the separation problems for the
so-called corners of the Trotter-Weil Hierarchy are decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05366</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05366</id><created>2015-09-17</created><authors><author><keyname>Tanisik</keyname><forenames>Gokhan</forenames></author><author><keyname>Zalluhoglu</keyname><forenames>Cemil</forenames></author><author><keyname>Ikizler-Cinbis</keyname><forenames>Nazli</forenames></author></authors><title>Facial Descriptors for Human Interaction Recognition In Still Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach in a rarely studied area of computer
vision: Human interaction recognition in still images. We explore whether the
facial regions and their spatial configurations contribute to the recognition
of interactions. In this respect, our method involves extraction of several
visual features from the facial regions, as well as incorporation of scene
characteristics and deep features to the recognition. Extracted multiple
features are utilized within a discriminative learning framework for
recognizing interactions between people. Our designed facial descriptors are
based on the observation that relative positions, size and locations of the
faces are likely to be important for characterizing human interactions. Since
there is no available dataset in this relatively new domain, a comprehensive
new dataset which includes several images of human interactions is collected.
Our experimental results show that faces and scene characteristics contain
important information to recognize interactions between people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05370</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05370</id><created>2015-09-17</created><authors><author><keyname>Kenyon</keyname><forenames>Richard</forenames></author><author><keyname>Radin</keyname><forenames>Charles</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Sadun</keyname><forenames>Lorenzo</forenames></author></authors><title>Bipodal structure in oversaturated random graphs</title><categories>math.CO cs.IT math-ph math.IT math.MP math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the asymptotics of large simple graphs constrained by the limiting
density of edges and the limiting subgraph density of an arbitrary fixed graph
$H$. We prove that, for all but finitely many values of the edge density, if
the density of $H$ is constrained to be slightly higher than that for the
corresponding Erd\H{o}s-R\'enyi graph, the typical large graph is bipodal with
parameters varying analytically with the densities. Asymptotically, the
parameters depend only on the degree sequence of $H$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05371</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05371</id><created>2015-09-17</created><authors><author><keyname>Burkert</keyname><forenames>Peter</forenames></author><author><keyname>Trier</keyname><forenames>Felix</forenames></author><author><keyname>Afzal</keyname><forenames>Muhammad Zeshan</forenames></author><author><keyname>Dengel</keyname><forenames>Andreas</forenames></author><author><keyname>Liwicki</keyname><forenames>Marcus</forenames></author></authors><title>DeXpression: Deep Convolutional Neural Network for Expression
  Recognition</title><categories>cs.CV cs.LG</categories><comments>Under consideration for publication in Pattern Recognition Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a convolutional neural network (CNN) architecture for facial
expression recognition. The proposed architecture is independent of any
hand-crafted feature extraction and performs better than the earlier proposed
convolutional neural network based approaches. We visualize the automatically
extracted features which have been learned by the network in order to provide a
better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP)
and MMI Facial Expression Databse are used for the quantitative evaluation. On
the CKP set the current state of the art approach, using CNNs, achieves an
accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion
recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and
98.63% for MMI, therefore performing better than the state of the art using
CNNs. Automatic facial expression recognition has a broad spectrum of
applications such as human-computer interaction and safety systems. This is due
to the fact that non-verbal cues are important forms of communication and play
a pivotal role in interpersonal communication. The performance of the proposed
architecture endorses the efficacy and reliable usage of the proposed work for
real world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05376</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05376</id><created>2015-09-17</created><authors><author><keyname>Hamana</keyname><forenames>Makoto</forenames><affiliation>Department of Computer Science, Gunma University</affiliation></author></authors><title>Iteration Algebras for UnQL Graphs and Completeness for Bisimulation</title><categories>cs.LO cs.DB</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><acm-class>F.3.2; F.3.3; H.2.3</acm-class><journal-ref>EPTCS 191, 2015, pp. 75-89</journal-ref><doi>10.4204/EPTCS.191.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows an application of Bloom and Esik's iteration algebras to
model graph data in a graph database query language. About twenty years ago,
Buneman et al. developed a graph database query language UnQL on the top of a
functional meta-language UnCAL for describing and manipulating graphs.
Recently, the functional programming community has shown renewed interest in
UnCAL, because it provides an efficient graph transformation language which is
useful for various applications, such as bidirectional computation. However, no
mathematical semantics of UnQL/UnCAL graphs has been developed. In this paper,
we give an equational axiomatisation and algebraic semantics of UnCAL graphs.
The main result of this paper is to prove that completeness of our equational
axioms for UnCAL for the original bisimulation of UnCAL graphs via iteration
algebras. Another benefit of algebraic semantics is a clean characterisation of
structural recursion on graphs using free iteration algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05377</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05377</id><created>2015-09-17</created><authors><author><keyname>Wang</keyname><forenames>Haitao</forenames></author><author><keyname>Zhang</keyname><forenames>Jingru</forenames></author></authors><title>Computing the Rectilinear Center of Uncertain Points in the Plane</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the rectilinear one-center problem on uncertain
points in the plane. In this problem, we are given a set $P$ of $n$ (weighted)
uncertain points in the plane and each uncertain point has $m$ possible
locations each associated with a probability for the point appearing at that
location. The goal is to find a point $q^*$ in the plane which minimizes the
maximum expected rectilinear distance from $q^*$ to all uncertain points of
$P$, and $q^*$ is called a rectilinear center. We present an algorithm that
solves the problem in $O(mn)$ time. Since the input size of the problem is
$\Theta(mn)$, our algorithm is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05382</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05382</id><created>2015-09-17</created><updated>2015-11-04</updated><authors><author><keyname>Meichanetzoglou</keyname><forenames>Symeon</forenames></author><author><keyname>Ioannidis</keyname><forenames>Sotiris</forenames></author><author><keyname>Laoutaris</keyname><forenames>Nikolaos</forenames></author></authors><title>Testing for common sense (violation) in airline pricing or how
  complexity asymmetry defeated you and the web</title><categories>cs.CY</categories><comments>8 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have collected and analysed prices for more than 1.4 million flight
tickets involving 63 destinations and 125 airlines and have found that common
sense violation i.e., discrepancies between what consumers would expect and
what truly holds for those prices, are far more frequent than one would think.
For example, oftentimes the price of a single leg flight is higher than two-leg
flights that include it under similar terms of travel (class, luggage
allowance, etc.). This happened for up to 24.5% of available fares on a
specific route in our dataset invalidating the common expectation that &quot;further
is more expensive&quot;. Likewise, we found several two-leg fares where buying each
leg independently leads to lower overall cost than buying them together as a
single ticket. This happened for up to 37% of available fares on a specific
route invalidating the common expectation that &quot;bundling saves money&quot;. Last,
several single stop tickets in which the two legs were separated by 1-5 days
(called multicity fares), were oftentimes found to be costing more than
corresponding back-to-back fares with a small transit time. This was found to
be occurring in up to 7.5% fares on a specific route invalidating that &quot;a short
transit is better than a longer one&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05393</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05393</id><created>2015-09-17</created><updated>2015-09-18</updated><authors><author><keyname>Kleppmann</keyname><forenames>Martin</forenames></author></authors><title>A Critique of the CAP Theorem</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CAP Theorem is a frequently cited impossibility result in distributed
systems, especially among NoSQL distributed databases. In this paper we survey
some of the confusion about the meaning of CAP, including inconsistencies and
ambiguities in its definitions, and we highlight some problems in its
formalization. CAP is often interpreted as proof that eventually consistent
databases have better availability properties than strongly consistent
databases; although there is some truth in this, we show that more careful
reasoning is required. These problems cast doubt on the utility of CAP as a
tool for reasoning about trade-offs in practical systems. As alternative to
CAP, we propose a &quot;delay-sensitivity&quot; framework, which analyzes the sensitivity
of operation latency to network delay, and which may help practitioners reason
about the trade-offs between consistency guarantees and tolerance of network
faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05395</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05395</id><created>2015-09-17</created><authors><author><keyname>Gurakan</keyname><forenames>Berk</forenames></author><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Optimal Energy and Data Routing in Networks with Energy Cooperation</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the delay minimization problem in an energy harvesting
communication network with energy cooperation. In this network, nodes harvest
energy from nature for use in data transmission, and may transfer a portion of
their harvested energies to neighboring nodes through energy cooperation. For
fixed data and energy routing topologies, we determine the optimum data rates,
transmit powers and energy transfers, subject to flow and energy conservation
constraints, in order to minimize the network delay. We start with a simplified
problem with fixed data flows and optimize energy management at each node for
the case of a single energy harvest per node. This is tantamount to
distributing each node's available energy over its outgoing data links and
energy transfers to neighboring nodes. For this case, with no energy
cooperation, we show that each node should allocate more power to links with
more noise and/or more data flow. In addition, when there is energy
cooperation, our numerical results indicate that, energy is routed from nodes
with lower data loads to nodes with higher data loads. We extend this setting
to the case of multiple energy harvests per node over time. In this case, we
optimize each node's energy management over its outgoing data links and its
energy transfers to neighboring nodes, over multiple time slots. For this case,
with no energy cooperation, we show that, for any given node, the sum of powers
on the outgoing links is equal to the single-link optimal power over time.
Finally, we consider the problem of joint flow control and energy management
for the entire network. We determine the necessary conditions for joint
optimality of a power control, energy transfer and routing policy. We provide
an iterative algorithm that updates the data and energy flows, and power
distribution over outgoing data links. We show convergence to a Pareto-optimal
operating point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05396</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05396</id><created>2015-09-17</created><authors><author><keyname>Borchert</keyname><forenames>Adam</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>Words with many palindrome pair factors</title><categories>math.CO cs.FL</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a conjecture of Frid, Puzynina, and Zamboni, we investigate
infinite words with the property that for infinitely many n, every length-n
factor is a product of two palindromes. We show that every Sturmian word has
this property, but this does not characterize the class of Sturmian words. We
also show that the Thue-Morse word does not have this property. We investigate
finite words with the maximal number of distinct palindrome pair factors and
characterize the binary words that are not palindrome pairs but have the
property that every proper factor is a palindrome pair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05421</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05421</id><created>2015-09-17</created><authors><author><keyname>Koh</keyname><forenames>Jason</forenames></author><author><keyname>Balaji</keyname><forenames>Bharathan</forenames></author><author><keyname>Gupta</keyname><forenames>Rajesh</forenames></author><author><keyname>Agarwal</keyname><forenames>Yuvraj</forenames></author></authors><title>HVACMeter: Apportionment of HVAC Power to Thermal Zones and Air Handler
  Units</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Heating, Ventilation and Air Conditioning (HVAC) systems consume almost half
of the total energy use of commercial buildings. To optimize HVAC energy usage,
it is important to understand the energy consumption of individual HVAC
components at fine granularities. However, buildings typically only have
aggregate building level power and thermal meters. We present HVACMeter, a
system which leverages existing sensors in commercial HVAC systems to estimate
the energy consumed by individual components of the HVAC system, as well by
each thermal zone in buildings. HVACMeter can be generalized to any HVAC system
as it uses the basic understanding of HVAC operation, heat transfer equations,
and historical sensor data to estimate energy. We deploy HVACMeter to three
buildings on our campus, to identify the set of sensors that are important for
accurately disaggregating energy use at the level of each Air Handler Unit and
each thermal zone within these buildings. HVACMeter power estimations have on
an average 44.5 % less RMSE than that of mean power estimates. Furthermore, we
highlight the usefulness of HVACMeter energy estimation model for a building
fault detection application by quantifying the amount of energy that can be
saved by fixing particular faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05434</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05434</id><created>2015-09-17</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>A Study Investigating Typical Concepts and Guidelines for Ontology
  Building</title><categories>cs.AI</categories><comments>8 pages, 2 figures</comments><journal-ref>Journal of Emerging Trends in Computing and Information
  Sciences.Vol. 5, No. 12 December 2014, ISSN 2079-8407, pp.886-893</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In semantic technologies, the shared common understanding of the structure of
information among artifacts (people or software agents) can be realized by
building an ontology. To do this, it is imperative for an ontology builder to
answer several questions: a) what are the main components of an ontology? b)
How an ontology look likes and how it works? c) Verify if it is required to
consider reusing existing ontologies or not? c) What is the complexity of the
ontology to be developed? d) What are the principles of ontology design and
development? e) How to evaluate an ontology? This paper answers all the key
questions above. The aim of this paper is to present a set of guiding
principles to help ontology developers and also inexperienced users to answer
such questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05437</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05437</id><created>2015-09-17</created><authors><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>Class Association Rules Mining based Rough Set Method</title><categories>cs.DB cs.AI</categories><comments>10 pages, 2 figures</comments><journal-ref>International Journal of Engineering and Technology (IJET), Vol 6
  No 6 Dec 2014-Jan 2015, ISSN : 0975-4024, PP. 2786-2794</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper investigates the mining of class association rules with rough set
approach. In data mining, an association occurs between two set of elements
when one element set happen together with another. A class association rule set
(CARs) is a subset of association rules with classes specified as their
consequences. We present an efficient algorithm for mining the finest class
rule set inspired form Apriori algorithm, where the support and confidence are
computed based on the elementary set of lower approximation included in the
property of rough set theory. Our proposed approach has been shown very
effective, where the rough set approach for class association discovery is much
simpler than the classic association method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05445</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05445</id><created>2015-09-17</created><authors><author><keyname>Roncalla</keyname><forenames>Wilfredo Bardales</forenames></author><author><keyname>Laber</keyname><forenames>Eduardo</forenames></author><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author></authors><title>Searching for a superlinear lower bounds for the Maximum Consecutive
  Subsums Problem and the (min,+)-convolution</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a sequence of n numbers, the Maximum Consecutive Subsums Problem (MCSP)
asks for the maximum consecutive sum of lengths l for each l = 1,...,n. No
algorithm is known for this problem which is significantly better than the
naive quadratic solution. Nor a super linear lower bound is known. The best
known bound for the MCSP is based on the the computation of the
(min,+)-convolution, another problem for which neither an O(n^{2-{\epsilon}})
upper bound is known nor a super linear lower bound. We show that the two
problems are in fact computationally equivalent by providing linear reductions
between them. Then, we concentrate on the problem of finding super linear lower
bounds and provide empirical evidence for an {\Omega}(nlogn) lower bounds for
both problems in the decision tree model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05452</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05452</id><created>2015-09-17</created><authors><author><keyname>Baijal</keyname><forenames>Anant</forenames></author><author><keyname>Kim</keyname><forenames>Julia</forenames></author><author><keyname>Branje</keyname><forenames>Carmen</forenames></author><author><keyname>Russo</keyname><forenames>Frank</forenames></author><author><keyname>Fels</keyname><forenames>Deborah I.</forenames></author></authors><title>Composing vibrotactile music: A multisensory experience with the
  Emoti-chair</title><categories>cs.HC</categories><comments>IEEE HAPTICS Symposium 2012</comments><doi>10.1109/HAPTIC.2012.6183839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Emoti-Chair is a novel technology to enhance entertainment through
vibrotactile stimulation. We assessed the experience of this technology in two
workshops. In the first workshop, deaf film-makers experimented with creating
vibetracks for a movie clip using a professional movie editing software. In the
second workshop, trained opera singers sang and felt their voice through the
Emoti-Chair. Participants in both workshops generally found the overall
experience to be exciting and they were motivated to use the Chair for upcoming
projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05454</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05454</id><created>2015-09-17</created><authors><author><keyname>Strapasson</keyname><forenames>Jo&#xe3;o E.</forenames></author><author><keyname>Torezzan</keyname><forenames>Cristiano</forenames></author></authors><title>A heuristic approach for designing cyclic group codes</title><categories>cs.IT math.IT</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a heuristic technique for distributing points on the
surface of a unit n-dimensional Euclidean sphere, generated as the orbit of a
finite cyclic subgroup of orthogonal matrices, the so called cyclic group
codes. Massive numerical experiments were done and many new cyclic group codes
have been obtained in several dimensions at various rate. The obtained results
assure that the heuristic approach have performance comparable to a brute-force
search technique with the advantage of having low complexity, allowing for
designing codes with a large number of points in higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05463</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05463</id><created>2015-09-17</created><updated>2015-09-21</updated><authors><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Jiang</keyname><forenames>Shanshan</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author><author><keyname>Agam</keyname><forenames>Gady</forenames></author></authors><title>Learning from Synthetic Data Using a Stacked Multichannel Autoencoder</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning from synthetic data has many important and practical applications.
An example of application is photo-sketch recognition. Using synthetic data is
challenging due to the differences in feature distributions between synthetic
and real data, a phenomenon we term synthetic gap. In this paper, we
investigate and formalize a general framework-Stacked Multichannel Autoencoder
(SMCAE) that enables bridging the synthetic gap and learning from synthetic
data more efficiently. In particular, we show that our SMCAE can not only
transform and use synthetic data on the challenging face-sketch recognition
task, but that it can also help simulate real images, which can be used for
training classifiers for recognition. Preliminary experiments validate the
effectiveness of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05472</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05472</id><created>2015-09-17</created><authors><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Learning to Hash for Indexing Big Data - A Survey</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth in big data has attracted much attention in designing
efficient indexing and search methods recently. In many critical applications
such as large-scale search and pattern matching, finding the nearest neighbors
to a query is a fundamental research problem. However, the straightforward
solution using exhaustive comparison is infeasible due to the prohibitive
computational complexity and memory requirement. In response, Approximate
Nearest Neighbor (ANN) search based on hashing techniques has become popular
due to its promising performance in both efficiency and accuracy. Prior
randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore
data-independent hash functions with random projections or permutations.
Although having elegant theoretic guarantees on the search quality in certain
metric spaces, performance of randomized hashing has been shown insufficient in
many real-world applications. As a remedy, new approaches incorporating
data-driven learning methods in development of advanced hash functions have
emerged. Such learning to hash methods exploit information such as data
distributions or class labels when optimizing the hash codes or functions.
Importantly, the learned hash codes are able to preserve the proximity of
neighboring data in the original feature spaces in the hash code spaces. The
goal of this paper is to provide readers with systematic understanding of
insights, pros and cons of the emerging techniques. We provide a comprehensive
survey of the learning to hash framework and representative techniques of
various types, including unsupervised, semi-supervised, and supervised. In
addition, we also summarize recent hashing approaches utilizing the deep
learning models. Finally, we discuss the future direction and trends of
research in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05473</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05473</id><created>2015-09-17</created><authors><author><keyname>Milovanov</keyname><forenames>Alexey</forenames></author></authors><title>Algorithmic statistics, prediction and machine learning</title><categories>cs.LG cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic statistics considers the following problem: given a binary string
$x$ (e.g., some experimental data), find a &quot;good&quot; explanation of this data. It
uses algorithmic information theory to define formally what is a good
explanation. In this paper we extend this framework in two directions.
  First, the explanations are not only interesting in themselves but also used
for prediction: we want to know what kind of data we may reasonably expect in
similar situations (repeating the same experiment). We show that some kind of
hierarchy can be constructed both in terms of algorithmic statistics and using
the notion of a priori probability, and these two approaches turn out to be
equivalent.
  Second, a more realistic approach that goes back to machine learning theory,
assumes that we have not a single data string $x$ but some set of &quot;positive
examples&quot; $x_1,\ldots,x_l$ that all belong to some unknown set $A$, a property
that we want to learn. We want this set $A$ to contain all positive examples
and to be as small and simple as possible. We show how algorithmic statistic
can be extended to cover this situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05475</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05475</id><created>2015-09-17</created><authors><author><keyname>Marti</keyname><forenames>Gautier</forenames></author><author><keyname>Very</keyname><forenames>Philippe</forenames></author><author><keyname>Donnat</keyname><forenames>Philippe</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>A proposal of a methodological framework with experimental guidelines to
  investigate clustering stability on financial time series</title><categories>q-fin.ST cs.CE</categories><comments>Accepted at ICMLA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper an empirical framework motivated by the practitioner
point of view on stability. The goal is to both assess clustering validity and
yield market insights by providing through the data perturbations we propose a
multi-view of the assets' clustering behaviour. The perturbation framework is
illustrated on an extensive credit default swap time series database available
online at www.datagrapple.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05477</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05477</id><created>2015-09-17</created><authors><author><keyname>Vuffray</keyname><forenames>Marc</forenames></author><author><keyname>Misiakiewicz</keyname><forenames>Theodor</forenames></author></authors><title>Concentration to Zero Bit-Error Probability for Regular LDPC Codes on
  the Binary Symmetric Channel: Proof by Loop Calculus</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider regular low-density parity-check codes over a
binary-symmetric channel in the decoding regime. We prove that up to a certain
noise threshold the bit-error probability of the bit-sampling decoder converges
in mean to zero over the code ensemble and the channel realizations. To arrive
at this result we show that the bit-error probability of the sampling decoder
is equal to the derivative of a Bethe free entropy. The method that we
developed is new and is based on convexity of the free entropy and loop
calculus. Convexity is needed to exchange limit and derivative and the loop
series enables us to express the difference between the bit-error probability
and the Bethe free entropy. We control the loop series using combinatorial
techniques and a first moment method. We stress that our method is versatile
and we believe that it can be generalized for LDPC codes with general degree
distributions and for asymmetric channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05480</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05480</id><created>2015-09-17</created><authors><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Zhang</keyname><forenames>Hanrui</forenames></author></authors><title>Unit-sphere games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a class of games, called unit-sphere games, where
strategies are real vectors with unit 2-norms (or, on a unit-sphere). As a
result, they can no longer be interpreted as probability distributions over
actions, but rather be thought of as allocations of one unit of resource to
actions and the multiplicative payoff effect on each action is proportional to
square-root of the amount of resource allocated to that action. The new
definition generates a number of interesting consequences. We first
characterize sufficient and necessary conditions under which a two-player
unit-sphere game has a Nash equilibrium. The characterization effectively
reduces solving a unit-sphere game to finding all eigenvalues and eigenvectors
of the product of individual payoff matrices. For any unit-sphere game with
non-negative payoff matrices, there always exists a unique Nash equilibrium;
furthermore, the unique equilibrium is efficiently reachable via Cournot
adjustment. In addition, we show that any equilibrium in positive unit-sphere
games corresponds to approximate equilibria in the corresponding normal-form
games. Analogous but weaker results are extended to positive n-player
unit-sphere games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05486</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05486</id><created>2015-09-17</created><authors><author><keyname>Liu</keyname><forenames>Chenxi</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Artificial-Noise-Aided Transmission in Multi-Antenna Relay Wiretap
  Channels with Spatially Random Eavesdroppers</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a new secure transmission scheme in the relay wiretap channel where
a source communicates with a destination through a decode-and-forward relay in
the presence of spatially random-distributed eavesdroppers. For the sake of
practicality, we consider a general antenna configuration in which the source,
relay, destination, and eavesdroppers are equipped with multiple antennas. In
order to confuse the eavesdroppers, we assume that both the source and the
relay transmit artificial noise signals in addition to information signals. We
first derive a closed-form expression for the transmission outage probability
and an easy-to-compute expression for the secrecy outage probability. Notably,
these expressions are valid for an arbitrary number of antennas at the source,
relay, and destination. We then derive simple yet valuable expressions for the
asymptotic transmission outage probability and the asymptotic secrecy outage
probability, which reveal the secrecy performance when the number of antennas
at the source grows sufficiently large. Using our expressions, we quantify a
practical performance metric, namely the secrecy throughput, under a secrecy
outage probability constraint. We further determine the system and channel
parameters that maximize the secrecy throughput, leading to analytical security
solutions suitable for real-world deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05488</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05488</id><created>2015-09-17</created><updated>2015-12-27</updated><authors><author><keyname>Xiao</keyname><forenames>Han</forenames></author><author><keyname>Huang</keyname><forenames>Minlie</forenames></author><author><keyname>Hao</keyname><forenames>Yu</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author></authors><title>TransG : A Generative Mixture Model for Knowledge Graph Embedding</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, knowledge graph embedding, which projects symbolic entities and
relations into continuous vector space, has become a new, hot topic in
artificial intelligence. This paper addresses a new issue of \textbf{multiple
relation semantics} that a relation may have multiple meanings revealed by the
entity pairs associated with the corresponding triples, and proposes a novel
Gaussian mixture model for embedding, \textbf{TransG}. The new model can
discover latent semantics for a relation and leverage a mixture of relation
component vectors for embedding a fact triple. To the best of our knowledge,
this is the first generative model for knowledge graph embedding, which is able
to deal with multiple relation semantics. Extensive experiments show that the
proposed model achieves substantial improvements against the state-of-the-art
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05490</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05490</id><created>2015-09-17</created><updated>2015-09-27</updated><authors><author><keyname>Xiao</keyname><forenames>Han</forenames></author><author><keyname>Huang</keyname><forenames>Minlie</forenames></author><author><keyname>Hao</keyname><forenames>Yu</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author></authors><title>TransA: An Adaptive Approach for Knowledge Graph Embedding</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge representation is a major topic in AI, and many studies attempt to
represent entities and relations of knowledge base in a continuous vector
space. Among these attempts, translation-based methods build entity and
relation vectors by minimizing the translation loss from a head entity to a
tail one. In spite of the success of these methods, translation-based methods
also suffer from the oversimplified loss metric, and are not competitive enough
to model various and complex entities/relations in knowledge bases. To address
this issue, we propose \textbf{TransA}, an adaptive metric approach for
embedding, utilizing the metric learning ideas to provide a more flexible
embedding method. Experiments are conducted on the benchmark datasets and our
proposed method makes significant and consistent improvements over the
state-of-the-art baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05492</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05492</id><created>2015-09-17</created><updated>2015-09-29</updated><authors><author><keyname>Romanus</keyname><forenames>Melissa</forenames></author><author><keyname>Ross</keyname><forenames>Robert B.</forenames></author><author><keyname>Parashar</keyname><forenames>Manish</forenames></author></authors><title>Challenges and Considerations for Utilizing Burst Buffers in
  High-Performance Computing</title><categories>cs.DC</categories><comments>18 pages, 2 figures</comments><acm-class>B.4.3; D.4.2; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As high-performance computing (HPC) moves into the exascale era, computer
scientists and engineers must find innovative ways of transferring and
processing unprecedented amounts of data. As the scale and complexity of the
applications running on these machines increases, the cost of their
interactions and data exchanges (in terms of latency, energy, runtime, etc.)
can increase exponentially. In order to address I/O coordination and
communication issues, computing vendors are developing an intermediate layer
between compute nodes and the parallel file system composed of different types
of memory (NVRAM, DRAM, SSD). These large scale memory appliances are being
called 'burst buffers.' In this paper, we envision advanced memory at various
levels of HPC hardware and derive potential use cases for how to take advantage
of it. We then present the challenges and issues that arise when utilizing
burst buffers in next-generation supercomputers and map the challenges to the
use cases. Lastly, we discuss the emerging state-of-the-art burst buffer
solutions that are expected to become available by the end of the year in new
HPC systems and which use cases these implementations may satisfy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05494</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05494</id><created>2015-09-17</created><authors><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author><author><keyname>Yang</keyname><forenames>Kuan</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>FPTAS for Hardcore and Ising Models on Hypergraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardcore and Ising models are two most important families of two state spin
systems in statistic physics. Partition function of spin systems is the center
concept in statistic physics which connects microscopic particles and their
interactions with their macroscopic and statistical properties of materials
such as energy, entropy, ferromagnetism, etc. If each local interaction of the
system involves only two particles, the system can be described by a graph. In
this case, fully polynomial-time approximation scheme (FPTAS) for computing the
partition function of both hardcore and anti-ferromagnetic Ising model was
designed up to the uniqueness condition of the system. These result are the
best possible since approximately computing the partition function beyond this
threshold is NP-hard. In this paper, we generalize these results to general
physics systems, where each local interaction may involves multiple particles.
Such systems are described by hypergraphs. For hardcore model, we also provide
FPTAS up to the uniqueness condition, and for anti-ferromagnetic Ising model,
we obtain FPTAS where a slightly stronger condition holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05497</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05497</id><created>2015-09-17</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Cantoni</keyname><forenames>Michael</forenames></author></authors><title>Quadratic Gaussian Privacy Games</title><categories>math.OC cs.GT</categories><comments>Accepted for Presentation at the 54th IEEE Conference on Decision and
  Control (CDC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A game-theoretic model for analysing the effects of privacy on strategic
communication between agents is devised. In the model, a sender wishes to
provide an accurate measurement of the state to a receiver while also
protecting its private information (which is correlated with the state) private
from a malicious agent that may eavesdrop on its communications with the
receiver. A family of nontrivial equilibria, in which the communicated messages
carry information, is constructed and its properties are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05498</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05498</id><created>2015-09-17</created><updated>2016-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Renyuan</forenames></author><author><keyname>Cai</keyname><forenames>Kai</forenames></author><author><keyname>Wonham</keyname><forenames>W. M.</forenames></author></authors><title>Supervisor Localization of Discrete-Event Systems under Partial
  Observation</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently we developed supervisor localization, a top-down approach to
distributed control of discrete-event systems. Its essence is the allocation of
monolithic (global) control action among the local control strategies of
individual agents. In this paper, we extend supervisor localization by
considering partial observation; namely not all events are observable.
Specifically, we employ the recently proposed concept of relative observability
to compute a partial-observation monolithic supervisor, and then design a
suitable localization procedure to decompose the supervisor into a set of local
controllers. In the resulting local controllers, only observable events can
cause state change. Further, to deal with large-scale systems, we combine the
partial-observation supervisor localization with an efficient architectural
synthesis approach: first compute a heterarchical array of partial-observation
decentralized supervisors and coordinators, and then localize each of these
supervisors/coordinators into local controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05499</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05499</id><created>2015-09-17</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Cantoni</keyname><forenames>Michael</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author></authors><title>Scheduling Rigid Demands on Continuous-Time Linear Shift-Invariant
  Systems</title><categories>math.OC cs.SY math.NA</categories><comments>Accepted for Presentation at the 54th IEEE Conference on Decision and
  Control (CDC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider load scheduling on constrained continuous-time linear dynamical
systems, such as automated irrigation and other distribution networks. The
requested loads are rigid, i.e., the shapes cannot be changed. Hence, it is
only possible to shift the order back-and-forth in time to arrive at a feasible
schedule. We present a numerical algorithm based on using log-barrier functions
to include the state constraints in the social cost function (i.e., an
appropriate function of the scheduling delays). This algorithm requires a
feasible initialization. Further, in another algorithm, we treat the state
constraints as soft constraints and heavily penalize the constraint violations.
This algorithm can even be initialized at an infeasible point. The
applicability of both these numerical algorithms is demonstrated on an
automated irrigation network with two pools and six farms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05500</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05500</id><created>2015-09-17</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>On Reconstructability of Quadratic Utility Functions from the Iterations
  in Gradient Methods</title><categories>math.OC cs.CR math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a scenario where an eavesdropper can read the
content of messages transmitted over a network. The nodes in the network are
running a gradient algorithm to optimize a quadratic utility function where
such a utility optimization is a part of a decision making process by an
administrator. We are interested in understanding the conditions under which
the eavesdropper can reconstruct the utility function or a scaled version of it
and, as a result, gain insight into the decision-making process. We establish
that if the parameter of the gradient algorithm, i.e.,~the step size, is chosen
appropriately, the task of reconstruction becomes practically impossible for a
class of Bayesian filters with uniform priors. We establish what step-size
rules should be employed to ensure this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05502</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05502</id><created>2015-09-18</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Nair</keyname><forenames>Girish</forenames></author></authors><title>Mutual Information as Privacy-Loss Measure in Strategic Communication</title><categories>cs.GT cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A game is introduced to study the effect of privacy in strategic
communication between well-informed senders and a receiver. The receiver wants
to accurately estimate a random variable. The sender, however, wants to
communicate a message that balances a trade-off between providing an accurate
measurement and minimizing the amount of leaked private information, which is
assumed to be correlated with the to-be-estimated variable. The mutual
information between the transmitted message and the private information is used
as a measure of the amount of leaked information. An equilibrium is constructed
and its properties are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05505</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05505</id><created>2015-09-18</created><authors><author><keyname>Jauhri</keyname><forenames>Abhinav</forenames></author><author><keyname>Griss</keyname><forenames>Martin</forenames></author><author><keyname>Erdogmus</keyname><forenames>Hakan</forenames></author></authors><title>Small Polygon Compression For Integer Coordinates</title><categories>cs.IT math.IT</categories><comments>Publised at 3rd Conference on Weather Warnings and Communication 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe several polygon compression techniques to enable efficient
transmission of polygons representing geographical targets. The main
application is to embed compressed polygons to emergency alert messages that
have strict length restrictions, as in the case of Wireless Emergency Alert
messages. We are able to compress polygons to between 9.7% and 23.6% of
original length, depending on characteristics of the specific polygons,
reducing original polygon lengths from 43-331 characters to 8-55 characters.
The best techniques apply several heuristics to perform initial compression,
and then other algorithmic techniques, including higher base encoding. Further,
these methods are respectful of computation and storage constraints typical of
cell phones. Two of the best techniques include a \enquote{bignum} quadratic
combination of integer coordinates and a variable length encoding, which takes
advantage of a strongly skewed polygon coordinate distribution. Both techniques
applied to one of two \enquote{delta} representations of polygons are on
average able to reduce the size of polygons by some 80%. A repeated substring
dictionary can provide further compression, and a merger of these techniques
into a \enquote{polyalgorithm} can also provide additional improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05506</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05506</id><created>2015-09-18</created><authors><author><keyname>Yang</keyname><forenames>Howard H.</forenames></author><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Energy-Efficient Design of MIMO Heterogeneous Networks with Wireless
  Backhaul</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As future networks aim to meet the ever-increasing requirements of high data
rate applications, dense and heterogeneous networks (HetNets) will be deployed
to provide better coverage and throughput. Besides the important implications
for energy consumption, the trend towards densification calls for more and more
wireless links to forward a massive backhaul traffic into the core network. It
is critically important to take into account the presence of a wireless
backhaul for the energy-efficient design of HetNets. In this paper, we provide
a general framework to analyze the energy efficiency of a two-tier MIMO
heterogeneous network with wireless backhaul in the presence of both uplink and
downlink transmissions. We find that under spatial multiplexing the energy
efficiency of a HetNet is sensitive to the network load, and it should be taken
into account when controlling the number of users served by each base station.
We show that a two-tier HetNet with wireless backhaul can be significantly more
energy efficient than a one-tier cellular network. However, this requires the
bandwidth division between radio access links and wireless backhaul to be
optimally designed according to the load conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05512</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05512</id><created>2015-09-18</created><updated>2015-11-18</updated><authors><author><keyname>Ma</keyname><forenames>Jackie</forenames></author></authors><title>Stable reconstructions for the analysis formulation of
  $\ell^p$-minimization using redundant systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common method to compute sparse solutions in compressed sensing is based on
$\ell^1$-minimization. In this paper we consider $\ell^p$-minimization for
arbitrary $0&lt;p \leq1$. More precisely, we prove stability estimates for
solutions of the analysis formulation of the $\ell^p$-minimization problem for
$0&lt;p\leq 1$ from noisy measurements. Furthermore, our focus lies in arbitrary
frames that are not necessarily tight or Parseval. We will investigate the role
of the frame bounds and how they effect the sparsity as well as the number of
measurements that are needed. Furthermore, we will distinguish between the
minimization over frame coefficients and dual frame coefficients.
  Numerical experiments based on iterative reweighting are also presented that
suggest the possible benefit of using $\ell^p$-minimization over
$\ell^1$-minimization for sparse recovery for some applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05514</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05514</id><created>2015-09-18</created><authors><author><keyname>Daruki</keyname><forenames>Samira</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Streaming Verification in Data Analysis</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming interactive proofs (SIPs) are a framework to reason about
outsourced computation, where a data owner (the verifier) outsources a
computation to the cloud (the prover), but wishes to verify the correctness of
the solution provided by the cloud service. In this paper we present streaming
interactive proofs for problems in data analysis. We present protocols for
clustering and shape fitting problems, as well as an improved protocol for
rectangular matrix multiplication. The latter can in turn be used to verify $k$
eigenvectors of a (streamed) $n \times n$ matrix. In general our solutions use
polylogarithmic rounds of communication and polylogarithmic total communication
and verifier space. For special cases (when optimality certificates can be
verified easily), we present constant round protocols with similar costs. For
rectangular matrix multiplication and eigenvector verification, our protocols
work in the more restricted annotated data streaming model, and use sublinear
(but not polylogarithmic) communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05517</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05517</id><created>2015-09-18</created><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Forcada</keyname><forenames>Mikel L.</forenames></author></authors><title>A Light Sliding-Window Part-of-Speech Tagger for the Apertium
  Free/Open-Source Machine Translation Platform</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper describes a free/open-source implementation of the light
sliding-window (LSW) part-of-speech tagger for the Apertium free/open-source
machine translation platform. Firstly, the mechanism and training process of
the tagger are reviewed, and a new method for incorporating linguistic rules is
proposed. Secondly, experiments are conducted to compare the performances of
the tagger under different window settings, with or without Apertium-style
&quot;forbid&quot; rules, with or without Constraint Grammar, and also with respect to
the traditional HMM tagger in Apertium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05520</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05520</id><created>2015-09-18</created><authors><author><keyname>Chen</keyname><forenames>Zhe</forenames></author><author><keyname>Hong</keyname><forenames>Zhibin</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>An Experimental Survey on Correlation Filter-based Tracking</title><categories>cs.CV</categories><comments>13 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over these years, Correlation Filter-based Trackers (CFTs) have aroused
increasing interests in the field of visual object tracking, and have achieved
extremely compelling results in different competitions and benchmarks. In this
paper, our goal is to review the developments of CFTs with extensive
experimental results. 11 trackers are surveyed in our work, based on which a
general framework is summarized. Furthermore, we investigate different training
schemes for correlation filters, and also discuss various effective
improvements that have been made recently. Comprehensive experiments have been
conducted to evaluate the effectiveness and efficiency of the surveyed CFTs,
and comparisons have been made with other competing trackers. The experimental
results have shown that state-of-art performance, in terms of robustness, speed
and accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF.
We find that further improvements for correlation filter-based tracking can be
made on estimating scales, applying part-based tracking strategy and
cooperating with long-term tracking methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05526</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05526</id><created>2015-09-18</created><authors><author><keyname>Kaufmann</keyname><forenames>Matt</forenames></author><author><keyname>Rager</keyname><forenames>David L.</forenames></author></authors><title>Proceedings Thirteenth International Workshop on the ACL2 Theorem Prover
  and Its Applications</title><categories>cs.LO cs.AI</categories><comments>Celebrating the 25th anniversary of ACL2</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015</journal-ref><doi>10.4204/EPTCS.192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Thirteenth International Workshop
on the ACL2 Theorem Prover and Its Applications, ACL2 2015, a two-day workshop
held in Austin, Texas, USA, on October 1-2, 2015. ACL2 workshops occur at
approximately 18-month intervals and provide a major technical forum for
researchers to present and discuss improvements and extensions to the theorem
prover, comparisons of ACL2 with other systems, and applications of ACL2 in
formal verification.
  ACL2 is a state-of-the-art automated reasoning system that has been
successfully applied in academia, government, and industry for specification
and verification of computing systems and in teaching computer science courses.
In 2005, Boyer, Kaufmann, and Moore were awarded the 2005 ACM Software System
Award for their work on ACL2 and the other theorem provers in the Boyer-Moore
family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05534</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05534</id><created>2015-09-18</created><authors><author><keyname>Techakesari</keyname><forenames>O.</forenames></author><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author></authors><title>Tangential Interpolatory Projection for Model Reduction of Linear
  Quantum Stochastic Systems</title><categories>quant-ph cs.SY</categories><comments>28 pages, 8 figures. A preliminary version of Section 4 will appear
  in Proceedings of the 54th IEEE Conference on Decision and Control (CDC)
  (Osaka, Japan, December 15-18, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model reduction method for the class of linear quantum
stochastic systems often encountered in quantum optics and their related
fields. The approach is proposed on the basis of an interpolatory projection
ensuring that specific input-output responses of the original and the
reduced-order systems are matched at multiple selected points (or frequencies).
Importantly, the physical realizability property of the original quantum system
imposed by the law of quantum mechanics is preserved under our tangential
interpolatory projection. An error bound is established for the proposed model
reduction method and an avenue to select interpolation points is proposed. A
passivity preserving model reduction method is also presented. Examples of both
active and passive systems are provided to illustrate the merits of our
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05536</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05536</id><created>2015-09-18</created><authors><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Alavi</keyname><forenames>Azadeh</forenames></author><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Efficient Clustering on Riemannian Manifolds: A Kernelised Random
  Projection Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reformulating computer vision problems over Riemannian manifolds has
demonstrated superior performance in various computer vision applications. This
is because visual data often forms a special structure lying on a lower
dimensional space embedded in a higher dimensional space. However, since these
manifolds belong to non-Euclidean topological spaces, exploiting their
structures is computationally expensive, especially when one considers the
clustering analysis of massive amounts of data. To this end, we propose an
efficient framework to address the clustering problem on Riemannian manifolds.
This framework implements random projections for manifold points via kernel
space, which can preserve the geometric structure of the original space, but is
computationally efficient. Here, we introduce three methods that follow our
framework. We then validate our framework on several computer vision
applications by comparing against popular clustering methods on Riemannian
manifolds. Experimental results demonstrate that our framework maintains the
performance of the clustering whilst massively reducing computational
complexity by over two orders of magnitude in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05537</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05537</id><created>2015-09-18</created><authors><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author><author><keyname>Grivopoulos</keyname><forenames>S.</forenames></author><author><keyname>Petersen</keyname><forenames>I. R.</forenames></author></authors><title>The Transfer Function of Generic Linear Quantum Stochastic Systems Has a
  Pure Cascade Realization</title><categories>quant-ph cs.SY</categories><comments>23 pages, 1 figure. Provisionally accepted for publication in
  Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes that generic linear quantum stochastic systems have a
pure cascade realization of their transfer function, generalizing an earlier
result established only for the special class of completely passive linear
quantum stochastic systems. In particular, a cascade realization therefore
exists for generic active linear quantum stochastic systems that require an
external source of quanta to operate. The results facilitate a simplified
realization of generic linear quantum stochastic systems for applications such
as coherent feedback control and optical filtering. The key tools that are
developed are algorithms for symplectic QR and Schur decompositions. It is
shown that generic real square matrices of even dimension can be transformed
into a lower $2 \times 2$ block triangular form by a symplectic similarity
transformation. The linear algebraic results herein may be of independent
interest for applications beyond the problem of transfer function realization
for quantum systems. Numerical examples are included to illustrate the main
results. In particular, one example describes an equivalent realization of the
transfer function of a nondegenerate parametric amplifier as the cascade
interconnection of two degenerate parametric amplifiers with an additional
outcoupling mirror.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05559</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05559</id><created>2015-09-18</created><authors><author><keyname>Cai</keyname><forenames>Leizhen</forenames></author><author><keyname>Ye</keyname><forenames>Junjie</forenames></author></authors><title>Finding Two Edge-Disjoint Paths with Length Constraints</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding, for two pairs $(s_1,t_1)$ and $(s_2,t_2)$
of vertices in an undirected graphs, an $(s_1,t_1)$-path $P_1$ and an
$(s_2,t_2)$-path $P_2$ such that $P_1$ and $P_2$ share no edges and the length
of each $P_i$ satisfies $L_i$, where $L_i \in \{ \le k_i, \; = k_i, \; \ge k_i,
\; \le \infty\}$.
  We regard $k_1$ and $k_2$ as parameters and investigate the parameterized
complexity of the above problem when at least one of $P_1$ and $P_2$ has a
length constraint (note that $L_i = &quot;\le \infty&quot;$ indicates that $P_i$ has no
length constraint). For the nine different cases of $(L_1, L_2)$, we obtain FPT
algorithms for seven of them. Our algorithms uses random partition backed by
some structural results. On the other hand, we prove that the problem admits no
polynomial kernel for all nine cases unless $NP \subseteq coNP/poly$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05567</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05567</id><created>2015-09-18</created><authors><author><keyname>Pal</keyname><forenames>Dipasree</forenames></author><author><keyname>Mitra</keyname><forenames>Mandar</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Samar</forenames></author></authors><title>Exploring Query Categorisation for Query Expansion: A Study</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vocabulary mismatch problem is one of the important challenges facing
traditional keyword-based Information Retrieval Systems. The aim of query
expansion (QE) is to reduce this query-document mismatch by adding related or
synonymous words or phrases to the query.
  Several existing query expansion algorithms have proved their merit, but they
are not uniformly beneficial for all kinds of queries. Our long-term goal is to
formulate methods for applying QE techniques tailored to individual queries,
rather than applying the same general QE method to all queries. As an initial
step, we have proposed a taxonomy of query classes (from a QE perspective) in
this report. We have discussed the properties of each query class with
examples. We have also discussed some QE strategies that might be effective for
each query category.
  In future work, we intend to test the proposed techniques using standard
datasets, and to explore automatic query categorisation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05572</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05572</id><created>2015-09-18</created><authors><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author></authors><title>Randomised enumeration of small witnesses using a decision oracle</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many combinatorial problems involve determining whether a universe of $n$
elements contains a witness consisting of $k$ elements which have some
specified property. In some cases it is necessary to consider the entire
universe in order to determine whether a given subset is a witness, but in
others it suffices to consider only the elements of the witness themselves, and
the relationships between them. We are concerned with the problem of finding
\emph{all} witnesses in the latter case: we show that, if the corresponding
decision problem belongs to FPT, there is a randomised algorithm to enumerate
all witnesses in time $f(k)\cdot poly(n) \cdot N$, where $N$ is the total
number of objects to be enumerated and $f$ is a computable function. This also
gives rise to an efficient algorithm to count the total number of witnesses
when this number is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05582</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05582</id><created>2015-09-18</created><authors><author><keyname>Lee</keyname><forenames>Roy Ka-Wei</forenames></author><author><keyname>Li</keyname><forenames>Yingjiu</forenames></author></authors><title>SignEPC : A Digital Signature Scheme for Efficient and Scalable Access
  Control in EPCglobal Network</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EPCglobal network is a computer network which allows supply chain
companies to search for their unknown partners globally and share information
stored in product RFID tags with each other. Although there have been quite a
number of recent research works done to improve the security of EPCglobal
Network, the existing access control solutions are not efficient and scalable.
For instance, when a user queries Electronic Product Code Information Service
(EPCIS) for EPC event information, the EPCIS would have to query the Electronic
Product Code Discovery Service (EPCDS) to check the access rights of the user.
This implementation is not efficient and creates a bottleneck at EPCDS. In this
paper, we design and propose a digital signature scheme, SignEPC, as a more
efficient and scalable access control solution for EPCglobal network. Our paper
will also evaluate SignEPC by considering the various possible attacks that
could be done on our proposed model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05586</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05586</id><created>2015-09-18</created><authors><author><keyname>Benchetrit</keyname><forenames>Yohann</forenames></author><author><keyname>Seb&#x151;</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Ear-decompositions and the complexity of the matching polytope</title><categories>math.CO cs.DM math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of the matching polytope of graphs may be measured with the
maximum length $\beta$ of a starting sequence of odd ears in an
ear-decomposition. Indeed, a theorem of Edmonds and Pulleyblank shows that its
facets are defined by 2-connected factor-critical graphs, which have an odd
ear-decomposition (according to a theorem of Lov\'asz). In particular,
$\beta(G) \leq 1$ if and only if the matching polytope of the graph $G$ is
completely described by non-negativity, star and odd-circuit inequalities. This
is essentially equivalent to the h-perfection of the line-graph of $G$, as
observed by Cao and Nemhauser.
  The complexity of computing $\beta$ is apparently not known. We show that
deciding whether $\beta(G)\leq 1$ can be executed efficiently by looking at any
ear-decomposition starting with an odd circuit and performing basic modulo-2
computations. Such a greedy-approach is surprising in view of the complexity of
the problem in more special cases by Bruhn and Schaudt, and it is simpler than
using the Parity Minor Algorithm.
  Our results imply a simple polynomial-time algorithm testing h-perfection in
line-graphs (deciding h-perfection is open in general). We also generalize our
approach to binary matroids and show that computing $\beta$ is a
Fixed-Parameter-Tractable problem (FPT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05589</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05589</id><created>2015-09-18</created><authors><author><keyname>Psaras</keyname><forenames>Ioannis</forenames></author><author><keyname>Katsaros</keyname><forenames>Konstantinos V.</forenames></author><author><keyname>Saino</keyname><forenames>Lorenzo</forenames></author><author><keyname>Pavlou</keyname><forenames>George</forenames></author></authors><title>LIRA: A Location Independent Routing Layer based on Source-Provided
  Ephemeral Names</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify the obstacles hindering the deployment of Information Centric
Networking (ICN) and the shift from the current IP architecture. In particular,
we argue that scalability of name resolution and the lack of control of content
access from content providers are two important barriers that keep ICN away
from deployment. We design solutions to incentivise ICN deployment and present
a new network architecture that incorporates an extra layer in the protocol
stack (the Location Independent Routing Layer, LIRA) to integrate
location-independent content delivery. According to our design, content names
need not (and should not) be permanent, but rather should be ephemeral.
Resolution of non-permanent names requires the involvement of content
providers, enabling desirable features such as request logging and cache
purging, while avoiding the need for the deployment of a new name resolution
infrastructure. Our results show that with half of the network's nodes
operating under the LIRA framework, we can get the full gain of the ICN mode of
operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05590</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05590</id><created>2015-09-18</created><updated>2015-09-29</updated><authors><author><keyname>Hashimoto</keyname><forenames>Yasuhiro</forenames></author></authors><title>Growth fluctuation in preferential attachment dynamics</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>6 pages, 4 figures, 1 table revtex format. Ver.2: some references and
  related remarks were added in response to the comments we received</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Yule-Simon process, selection of words follows the preferential
attachment mechanism, resulting in the power-law growth in the cumulative
number of individual word occurrences. This is derived using mean-field
approximation, assuming a continuum limit of both the time and number of word
occurrences. However, time and word occurrences are inherently discrete in the
process, and it is natural to assume that the cumulative number of word
occurrences has a certain fluctuation around the average behavior predicted by
the mean-field approximation. We derive the exact and approximate forms of the
probability distribution of such fluctuation analytically and confirm that
those probability distributions are well supported by the numerical
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05592</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05592</id><created>2015-09-18</created><authors><author><keyname>Lee</keyname><forenames>Kwang Hee</forenames></author><author><keyname>Je</keyname><forenames>Changsoo</forenames></author><author><keyname>Lee</keyname><forenames>Sang Wook</forenames></author></authors><title>Color-Stripe Structured Light Robust to Surface Color and Discontinuity</title><categories>cs.CV cs.GR physics.optics</categories><comments>10 pages, 9 figures, 8th Asian Conference on Computer Vision (ACCV),
  Tokyo, Japan, November 2007, Proceedings, Part II</comments><acm-class>I.2.10; I.4.8</acm-class><journal-ref>Computer Vision - ACCV 2007, LNCS 4844, pp. 507-516, Springer
  Berlin Heidelberg, November 14, 2007</journal-ref><doi>10.1007/978-3-540-76390-1_50</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple color stripes have been employed for structured light-based rapid
range imaging to increase the number of uniquely identifiable stripes. The use
of multiple color stripes poses two problems: (1) object surface color may
disturb the stripe color and (2) the number of adjacent stripes required for
identifying a stripe may not be maintained near surface discontinuities such as
occluding boundaries. In this paper, we present methods to alleviate those
problems. Log-gradient filters are employed to reduce the influence of object
colors, and color stripes in two and three directions are used to increase the
chance of identifying correct stripes near surface discontinuities.
Experimental results demonstrate the effectiveness of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05600</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05600</id><created>2015-09-18</created><authors><author><keyname>Xia</keyname><forenames>Xiaochen</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Ma</keyname><forenames>Wenfeng</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Hardware Impairments Aware Transceiver for Full-Duplex Massive MIMO
  Relaying</title><categories>cs.IT math.IT</categories><comments>Extended version of 'Hardware Impairments Aware Transceiver for
  Full-Duplex Massive MIMO Relaying'(Doi: 10.1109/TSP.2015.2469635)</comments><doi>10.1109/TSP.2015.2469635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the massive MIMO full-duplex relaying (MM-FDR), where
multiple source-destination pairs communicate simultaneously with the help of a
common full-duplex relay equipped with very large antenna arrays. Different
from the traditional MM-FDR protocol, a general model where
sources/destinations are allowed to equip with multiple antennas is considered.
In contrast to the conventional MIMO system, massive MIMO must be built with
low-cost components which are prone to hardware impairments. In this paper, the
effect of hardware impairments is taken into consideration, and is modeled
using transmit/receive distortion noises. We propose a low complexity hardware
impairments aware transceiver scheme (named as HIA scheme) to mitigate the
distortion noises by exploiting the statistical knowledge of channels and
antenna arrays at sources and destinations. A joint degree of freedom and power
optimization algorithm is presented to further optimize the spectral efficiency
of HIA based MM-FDR. The results show that the HIA scheme can mitigate the
&quot;ceiling effect&quot; appears in traditional MM-FDR protocol, if the numbers of
antennas at sources and destinations can scale with that at the relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05612</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05612</id><created>2015-09-18</created><authors><author><keyname>Rai</keyname><forenames>Ashutosh</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>A Parameterized Algorithm for Mixed Cut</title><categories>cs.DS</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1207.4079 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical Menger's theorem states that in any undirected (or directed)
graph $G$, given a pair of vertices $s$ and $t$, the maximum number of vertex
(edge) disjoint paths is equal to the minimum number of vertices (edges) needed
to disconnect from $s$ and $t$. This min-max result can be turned into a
polynomial time algorithm to find the maximum number of vertex (edge) disjoint
paths as well as the minimum number of vertices (edges) needed to disconnect
$s$ from $t$. In this paper we study a mixed version of this problem, called
Mixed-Cut, where we are given an undirected graph $G$, vertices $s$ and $t$,
positive integers $k$ and $l$ and the objective is to test whether there exist
a $k$ sized vertex set $S \subseteq V(G)$ and an $l$ sized edge set $F
\subseteq E(G)$ such that deletion of $S$ and $F$ from $G$ disconnects from $s$
and $t$. We start with a small observation that this problem is NP-complete and
then study this problem, in fact a much stronger generalization of this, in the
realm of parameterized complexity. In particular we study the Mixed-Multiway
Cut-Uncut problem where along with a set of terminals $T$, we are also given an
equivalence relation $\mathcal{R}$ on $T$, and the question is whether we can
delete at most $k$ vertices and at most $l$ edges such that connectivity of the
terminals in the resulting graph respects $\mathcal{R}$. Our main results is a
fixed parameter algorithm for Mixed-Multiway Cut-Uncut using the method of
recursive understanding introduced by Chitnis et al. (FOCS 2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05618</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05618</id><created>2015-09-18</created><authors><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Relay Selection in Wireless Powered Cooperative Networks with Energy
  Storage</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Journal Selected Areas on Communications- Special Issue on Green
  Communications and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of relay selection in wireless powered
cooperative networks, where spatially random relays are equipped with energy
storage devices e.g., batteries. In contrast to conventional techniques and in
order to reduce complexity, the relay nodes can either harvest energy from the
source signal (in case of uncharged battery) or attempt to decode and forward
it (in case of charged battery). Several relay selection schemes that
correspond to different state information requirements and implementation
complexities are proposed. The charging/discharging behavior of the battery is
modeled as a two-state Markov chain and analytical expressions for the
steady-state distribution and the outage probability performance are derived
for each relay selection scheme. We prove that energy storage significantly
affects the performance of the system and results in a zeroth diversity gain at
high signal-to-noise ratios; the convergence floors depend on the steady-state
distribution of the battery and are derived in closed-form by using appropriate
approximations. The proposed relay selection schemes are generalized to a
large-scale network with multiple access points (APs), where relays assist the
closest AP and suffer from multi-user interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05623</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05623</id><created>2015-09-18</created><updated>2015-09-21</updated><authors><author><keyname>Mary</keyname><forenames>Arnaud</forenames></author><author><keyname>Strozecki</keyname><forenames>Yann</forenames></author></authors><title>Efficient enumeration of solutions produced by closure operations</title><categories>cs.CC</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper we address the problem of generating all elements obtained by
the saturation of an initial set by some operations. More precisely, we prove
that we can generate the closure by polymorphisms of a boolean relation with a
polynomial delay. This implies for instance that we can compute with polynomial
delay the closure of a family of sets by any set of &quot;set operations&quot; (e.g. by
union, intersection, difference, symmetric difference$\dots$). To do so, we
prove that for any set of operations $\mathcal{F}$, one can decide in
polynomial time whether an elements belongs to the closure by $\mathcal{F}$ of
a family of sets. When the relation is over a domain larger than two elements,
our generic enumeration method fails for some cases, since the associated
decision problem is $NP$-hard and we provide an alternative algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05625</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05625</id><created>2015-09-18</created><authors><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>Adaptive DRX Scheme to Improve Energy Efficiency in LTE Networks with
  Bounded Delay</title><categories>cs.NI</categories><journal-ref>IEEE Journal on Selected Areas in Communications - Series on Green
  Communications and Networking, vol. 33, no. 12, pp. 2963-2973, Dec 2015</journal-ref><doi>10.1109/JSAC.2015.2478996</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Discontinuous Reception (DRX) mechanism is commonly employed in current
LTE networks to improve energy efficiency of user equipment (UE). DRX allows
UEs to monitor the physical downlink control channel (PDCCH) discontinuously
when there is no downlink traffic for them, thus reducing their energy
consumption. However, DRX power savings are achieved at the expense of some
increase in packet delay since downlink traffic transmission must be deferred
until the UEs resume listening to the PDCCH. In this paper, we present a
promising mechanism that reduces energy consumption of UEs using DRX while
simultaneously maintaining average packet delay around a desired target.
Furthermore, our proposal is able to achieve significant power savings without
either increasing signaling overhead or requiring any changes to deployed
wireless protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05631</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05631</id><created>2015-09-18</created><updated>2015-11-16</updated><authors><author><keyname>Harder</keyname><forenames>Reed H.</forenames></author><author><keyname>Velasco</keyname><forenames>Alfredo J.</forenames></author><author><keyname>Evans</keyname><forenames>Michael S.</forenames></author><author><keyname>Rockmore</keyname><forenames>Daniel N.</forenames></author></authors><title>Measuring Verifiability in Online Information</title><categories>cs.SI cs.DL</categories><comments>12 pages, 4 figures, 2 tables; Second version adds clarifications,
  including one table and modified figures</comments><acm-class>H.5.4; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The verifiability of online information is important, but difficult to assess
systematically. We examine verifiability in the case of Wikipedia, one of the
world's largest and most consulted online information sources. We extend prior
work about quality of Wikipedia articles, knowledge production, and sources to
consider the quality of Wikipedia references. We propose a multidimensional
measure of verifiability that takes into account technical accuracy and
practical accessibility of sources. We calculate article verifiability scores
for a sample of 5,000 articles and 295,800 citations, and compare differently
weighted models to illustrate effects of emphasizing particular elements of
verifiability over others. We find that, while the quality of references in the
overall sample is reasonably high, verifiability varies significantly by
article, particularly when emphasizing the use of standard digital identifiers
and taking into account the practical availability of referenced sources. We
discuss the implications of these findings for measuring verifiability in
online information more generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05634</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05634</id><created>2015-09-18</created><authors><author><keyname>Golts</keyname><forenames>Alona</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Linearized Kernel Dictionary Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach of incorporating kernels into
dictionary learning. The kernel K-SVD algorithm (KKSVD), which has been
introduced recently, shows an improvement in classification performance, with
relation to its linear counterpart K-SVD. However, this algorithm requires the
storage and handling of a very large kernel matrix, which leads to high
computational cost, while also limiting its use to setups with small number of
training examples. We address these problems by combining two ideas: first we
approximate the kernel matrix using a cleverly sampled subset of its columns
using the Nystr\&quot;{o}m method; secondly, as we wish to avoid using this matrix
altogether, we decompose it by SVD to form new &quot;virtual samples,&quot; on which any
linear dictionary learning can be employed. Our method, termed &quot;Linearized
Kernel Dictionary Learning&quot; (LKDL) can be seamlessly applied as a
pre-processing stage on top of any efficient off-the-shelf dictionary learning
scheme, effectively &quot;kernelizing&quot; it. We demonstrate the effectiveness of our
method on several tasks of both supervised and unsupervised classification and
show the efficiency of the proposed scheme, its easy integration and
performance boosting properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05635</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05635</id><created>2015-09-18</created><authors><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Partitioning Graph Drawings and Triangulated Simple Polygons into
  Greedily Routable Regions</title><categories>cs.CG</categories><comments>full version of a paper appearing in ISAAC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A greedily routable region (GRR) is a closed subset of $\mathbb R^2$, in
which each destination point can be reached from each starting point by
choosing the direction with maximum reduction of the distance to the
destination in each point of the path.
  Recently, Tan and Kermarrec proposed a geographic routing protocol for dense
wireless sensor networks based on decomposing the network area into a small
number of interior-disjoint GRRs. They showed that minimum decomposition is
NP-hard for polygons with holes.
  We consider minimum GRR decomposition for plane straight-line drawings of
graphs. Here, GRRs coincide with self-approaching drawings of trees, a drawing
style which has become a popular research topic in graph drawing. We show that
minimum decomposition is still NP-hard for graphs with cycles, but can be
solved optimally for trees in polynomial time. Additionally, we give a
2-approximation for simple polygons, if a given triangulation has to be
respected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05636</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05636</id><created>2015-09-18</created><authors><author><keyname>Ramaiah</keyname><forenames>M. Seetha</forenames></author><author><keyname>Mukerjee</keyname><forenames>Amitabha</forenames></author><author><keyname>Chakraborty</keyname><forenames>Arindam</forenames></author><author><keyname>Sharma</keyname><forenames>Sadbodh</forenames></author></authors><title>Visual Generalized Coordinates</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open problem in robotics is that of using vision to identify a robot's own
body and the world around it. Many models attempt to recover the traditional
C-space parameters. Instead, we propose an alternative C-space by deriving
generalized coordinates from $n$ images of the robot. We show that the space of
such images is bijective to the motion space, so these images lie on a manifold
$\mathcal{V}$ homeomorphic to the canonical C-space. We now approximate this
manifold as a set of $n$ neighbourhood tangent spaces that result in a graph,
which we call the Visual Roadmap (VRM). Given a new robot image, we perform
inverse kinematics visually by interpolating between nearby images in the image
space. Obstacles are projected onto the VRM in $O(n)$ time by superimposition
of images, leading to the identification of collision poses. The edges joining
the free nodes can now be checked with a visual local planner, and free-space
motions computed in $O(nlogn)$ time. This enables us to plan paths in the image
space for a robot manipulator with unknown link geometries, DOF, kinematics,
obstacles, and camera pose. We sketch the proofs for the main theoretical
ideas, identify the assumptions, and demonstrate the approach for both
articulated and mobile robots. We also investigate the feasibility of the
process by investigating various metrics and image sampling densities, and
demonstrate it on simulated and real robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05637</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05637</id><created>2015-09-18</created><authors><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author></authors><title>The Shortest Path Problem with Edge Information Reuse is NP-Complete</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the following variation of the single-source shortest path
problem is NP-complete. Let a weighted, directed, acyclic graph $G=(V,E,w)$
with source and sink vertices $s$ and $t$ be given. Let in addition a mapping
$f$ on $E$ be given that associate information with the edges (e.g., a
pointer), such that $f(e)=f(e')$ means that edges $e$ and $e'$ carry the same
information; for such edges it is required that $w(e)=w(e')$. The length of a
simple $st$ path $U$ is the sum of the weights of the edges on $U$ but edges
with $f(e)=f(e')$ are counted only once. The problem is to determine a shortest
such $st$ path. We call this problem the \emph{edge information reuse shortest
path problem}. It is NP-complete by reduction from PARTITION.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05642</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05642</id><created>2015-09-18</created><updated>2016-02-11</updated><authors><author><keyname>Tremblay</keyname><forenames>Nicolas</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author></authors><title>Subgraph-based filterbanks for graph signals</title><categories>cs.IT cs.SI math.FA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a critically-sampled compact-support biorthogonal transform for
graph signals, via graph filterbanks. Instead of partitioning the nodes in two
sets so as to remove one every two nodes in the filterbank downsampling
operations, the design is based on a partition of the graph in connected
subgraphs. Coarsening is achieved by defining one &quot;supernode&quot; for each subgraph
and the edges for this coarsened graph derives from the connectivity between
the subgraphs. Unlike the &quot;one every two nodes&quot; downsampling on bipartite
graphs, this coarsening operation does not have an exact formulation in the
graph Fourier domain. Instead, we rely on the local Fourier bases of each
subgraph to define filtering operations. We apply successfully this method to
decompose graph signals, and show promising performance on compression and
denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05646</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05646</id><created>2015-09-18</created><authors><author><keyname>Kvam</keyname><forenames>Peter</forenames></author><author><keyname>Cesario</keyname><forenames>Joseph</forenames></author><author><keyname>Schossau</keyname><forenames>Jory</forenames></author><author><keyname>Eisthen</keyname><forenames>Heather</forenames></author><author><keyname>Hintze</keyname><forenames>Arend</forenames></author></authors><title>Computational evolution of decision-making strategies</title><categories>cs.NE q-bio.NC</categories><comments>Conference paper, 6 pages / 3 figures</comments><journal-ref>Proceedings of the 37th Annual Meeting of the Cognitive Science
  Society, 2015, pp. 1225-1230. Cognitive Science Society, Austin, TX</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most research on adaptive decision-making takes a strategy-first approach,
proposing a method of solving a problem and then examining whether it can be
implemented in the brain and in what environments it succeeds. We present a
method for studying strategy development based on computational evolution that
takes the opposite approach, allowing strategies to develop in response to the
decision-making environment via Darwinian evolution. We apply this approach to
a dynamic decision-making problem where artificial agents make decisions about
the source of incoming information. In doing so, we show that the complexity of
the brains and strategies of evolved agents are a function of the environment
in which they develop. More difficult environments lead to larger brains and
more information use, resulting in strategies resembling a sequential sampling
approach. Less difficult environments drive evolution toward smaller brains and
less information use, resulting in simpler heuristic-like strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05647</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05647</id><created>2015-09-18</created><updated>2015-11-25</updated><authors><author><keyname>Garber</keyname><forenames>Dan</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Fast and Simple PCA via Convex Optimization</title><categories>math.OC cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of principle component analysis (PCA) is traditionally solved by
spectral or algebraic methods. We show how computing the leading principal
component could be reduced to solving a \textit{small} number of
well-conditioned {\it convex} optimization problems. This gives rise to a new
efficient method for PCA based on recent advances in stochastic methods for
convex optimization.
  In particular we show that given a $d\times d$ matrix $\X =
\frac{1}{n}\sum_{i=1}^n\x_i\x_i^{\top}$ with top eigenvector $\u$ and top
eigenvalue $\lambda_1$ it is possible to: \begin{itemize} \item compute a unit
vector $\w$ such that $(\w^{\top}\u)^2 \geq 1-\epsilon$ in
$\tilde{O}\left({\frac{d}{\delta^2}+N}\right)$ time, where $\delta = \lambda_1
- \lambda_2$ and $N$ is the total number of non-zero entries in
$\x_1,...,\x_n$,
  \item compute a unit vector $\w$ such that $\w^{\top}\X\w \geq
\lambda_1-\epsilon$ in $\tilde{O}(d/\epsilon^2)$ time. \end{itemize} To the
best of our knowledge, these bounds are the fastest to date for a wide regime
of parameters. These results could be further accelerated when $\delta$ (in the
first case) and $\epsilon$ (in the second case) are smaller than $\sqrt{d/N}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05659</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05659</id><created>2015-09-18</created><updated>2015-12-30</updated><authors><author><keyname>Damiani</keyname><forenames>Ferruccio</forenames><affiliation>University of Torino</affiliation></author><author><keyname>Viroli</keyname><forenames>Mirko</forenames><affiliation>University of Bologna</affiliation></author></authors><title>Type-based Self-stabilisation for Computational Fields</title><categories>cs.LO cs.PL</categories><comments>Logical Methods in Computer Science accepted paper, 53 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:21) 2015</journal-ref><doi>10.2168/LMCS-11(4:21)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging network scenarios require the development of solid large-scale
situated systems. Unfortunately, the diffusion/aggregation computational
processes therein often introduce a source of complexity that hampers
predictability of the overall system behaviour. Computational fields have been
introduced to help engineering such systems: they are spatially distributed
data structures designed to adapt their shape to the topology of the underlying
(mobile) network and to the events occurring in it, with notable applications
to pervasive computing, sensor networks, and mobile robots. To assure
behavioural correctness, namely, correspondence of micro-level specification
(single device behaviour) with macro-level behaviour (resulting global spatial
pattern), we investigate the issue of self-stabilisation for computational
fields. We present a tiny, expressive, and type-sound calculus of computational
fields, and define sufficient conditions for self-stabilisation, defined as the
ability to react to changes in the environment finding a new stable state in
finite time. A type-based approach is used to provide a correct checking
procedure for self-stabilisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05662</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05662</id><created>2015-09-18</created><authors><author><keyname>Chang</keyname><forenames>Ching-Lueh</forenames></author></authors><title>Metric $1$-median selection: Query complexity vs. approximation ratio</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of finding a point in a metric space
$(\{1,2,\ldots,n\},d)$ with the minimum average distance to other points. We
show that this problem has no deterministic $o(n^{1+1/(h-1)})$-query
$(2h-\Omega(1))$-approximation algorithms for any constant
$h\in\mathbb{Z}^+\setminus\{1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05664</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05664</id><created>2015-09-18</created><authors><author><keyname>Faghih</keyname><forenames>Fathiyeh</forenames></author><author><keyname>Bonakdarpour</keyname><forenames>Borzoo</forenames></author><author><keyname>Tixeuil</keyname><forenames>Sebastien</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sandeep</forenames></author></authors><title>Specification-based Synthesis of Distributed Self-Stabilizing Protocols</title><categories>cs.SE cs.DC</categories><comments>17 pages, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an SMT based method that automatically
synthesizes a distributed self stabilizing protocol from a given high level
specification and the network topology. Unlike existing approaches, where
synthesis algorithms require the explicit description of the set of legitimate
states, our technique only needs the temporal behavior of the protocol. We also
extend our approach to synthesize ideal stabilizing protocols, where every
state is legitimate. Our proposed methods are fully implemented and we report
successful synthesis of Dijkstra`s token ring and a self stabilizing version of
Raymond`s mutual exclusion algorithm, as well as ideal stabilizing leader
election and local mutual exclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05668</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05668</id><created>2015-09-18</created><authors><author><keyname>Hammerich</keyname><forenames>Edwin</forenames></author></authors><title>Waterfilling theorems for linear time-varying channels and related
  nonstationary sources</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures; submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of the linear time-varying (LTV) channel, a continuous-time LTV
filter with additive white Gaussian noise (AWGN), is characterized by
waterfilling in the time-frequency plane. Similarly, the rate distortion
function for a related nonstationary source is characterized by reverse
waterfilling in the time-frequency plane. Constraints on the average energy or
on the squared-error distortion are used. The source is formed by the white
Gaussian noise response of the same LTV filter as before. The proofs of both
waterfilling theorems rely on a Szego theorem for a class of operators
associated with the filter. A self-contained proof of the Szego theorem is
given. The waterfilling theorems compare well with the classical results of
Gallager and Berger. In the case of a nonstationary source, it is observed that
the part of the classical power spectral density (PSD) is taken by the
Wigner-Ville spectrum (WVS). Our approach is based on the spread Weyl symbol of
the LTV filter, and is asymptotic in nature. For the spreading factor, a
necessary lower bound is derived by means of an uncertainty inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05669</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05669</id><created>2015-09-18</created><authors><author><keyname>Fuselier</keyname><forenames>Edward J.</forenames></author><author><keyname>Shankar</keyname><forenames>Varun</forenames></author><author><keyname>Wright</keyname><forenames>Grady B.</forenames></author></authors><title>A High-Order Radial Basis Function (RBF) Leray Projection Method for the
  Solution of the Incompressible Unsteady Stokes Equations</title><categories>math.NA cs.NA</categories><comments>34 pages, 8 figures</comments><msc-class>65M02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new projection method based on radial basis functions (RBFs) is presented
for discretizing the incompressible unsteady Stokes equations in irregular
geometries. The novelty of the method comes from the application of a new
technique for computing the Leray-Helmholtz projection of a vector field using
generalized interpolation with divergence-free and curl-free RBFs. Unlike
traditional projection methods, this new method enables matching both
tangential and normal components of divergence-free vector fields on the domain
boundary. This allows incompressibility of the velocity field to be enforced
without any time-splitting or pressure boundary conditions. Spatial derivatives
are approximated using collocation with global RBFs so that the method only
requires samples of the field at (possibly scattered) nodes over the domain.
Numerical results are presented demonstrating high-order convergence in both
space (between 5th and 6th order) and time (up to 4th order) for some model
problems in two dimensional irregular geometries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05671</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05671</id><created>2015-09-18</created><authors><author><keyname>Li</keyname><forenames>Yuncheng</forenames></author><author><keyname>Cong</keyname><forenames>Yang</forenames></author><author><keyname>Mei</keyname><forenames>Tao</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>User-Curated Image Collections: Modeling and Recommendation</title><categories>cs.MM cs.IR</categories><comments>in IEEE BigData 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art image retrieval and recommendation systems
predominantly focus on individual images. In contrast, socially curated image
collections, condensing distinctive yet coherent images into one set, are
largely overlooked by the research communities. In this paper, we aim to design
a novel recommendation system that can provide users with image collections
relevant to individual personal preferences and interests. To this end, two key
issues need to be addressed, i.e., image collection modeling and similarity
measurement. For image collection modeling, we consider each image collection
as a whole in a group sparse reconstruction framework and extract concise
collection descriptors given the pretrained dictionaries. We then consider
image collection recommendation as a dynamic similarity measurement problem in
response to user's clicked image set, and employ a metric learner to measure
the similarity between the image collection and the clicked image set. As there
is no previous work directly comparable to this study, we implement several
competitive baselines and related methods for comparison. The evaluations on a
large scale Pinterest data set have validated the effectiveness of our proposed
methods for modeling and recommending image collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05681</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05681</id><created>2015-09-18</created><authors><author><keyname>Bereg</keyname><forenames>Sergey</forenames></author><author><keyname>Fleszar</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Colored Non-Crossing Euclidean Steiner Forest</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of $k$-colored points in the plane, we consider the problem of
finding $k$ trees such that each tree connects all points of one color class,
no two trees cross, and the total edge length of the trees is minimized. For
$k=1$, this is the well-known Euclidean Steiner tree problem. For general $k$,
a $k\rho$-approximation algorithm is known, where $\rho \le 1.21$ is the
Steiner ratio.
  We present a PTAS for $k=2$, a $(5/3+\varepsilon)$-approximation algorithm
for $k=3$, and two approximation algorithms for general~$k$, with ratios
$O(\sqrt n \log k)$ and $k+\varepsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05696</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05696</id><created>2015-09-14</created><authors><author><keyname>Lahlou</keyname><forenames>Tarek A.</forenames></author><author><keyname>Makur</keyname><forenames>Anuran</forenames></author></authors><title>Transient Signal Spaces and Decompositions</title><categories>math.OC cs.IT cs.NA cs.SY math.CA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of transient signal analysis. A
signal-dependent algorithm is proposed which sequentially identifies the
countable sets of decay rates and expansion coefficients present in a given
signal. We qualitatively compare our method to existing techniques such as
orthogonal exponential transforms generated from orthogonal polynomial classes.
The presented algorithm has immediate utility to signal processing applications
wherein the decay rates and expansion coefficients associated with a transient
signal convey information. We also provide a functional interpretation of our
parameter extraction method via signal approximation using monomials over the
unit interval from the perspective of biorthogonal constraint satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05715</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05715</id><created>2015-09-18</created><authors><author><keyname>Hovhannisyan</keyname><forenames>Vahan</forenames></author><author><keyname>Parpas</keyname><forenames>Panos</forenames></author><author><keyname>Zafeiriou</keyname><forenames>Stefanos</forenames></author></authors><title>MAGMA: Multi-level accelerated gradient mirror descent algorithm for
  large-scale convex composite minimization</title><categories>math.OC cs.CV</categories><msc-class>65K05, 90-08, 90C06, 90C25, 90C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Composite convex optimization models arise in several applications, and are
especially prevalent in inverse problems with a sparsity inducing norm and in
general convex optimization with simple constraints. The most widely used
algorithms for convex composite models are accelerated first order methods,
however they can take a large number of iterations to compute an acceptable
solution for large-scale problems. In this paper we propose to speed up first
order methods by taking advantage of the structure present in many applications
and in image processing in particular. Our method is based on multi-level
optimization methods and exploits the fact that many applications that give
rise to large scale models can be modelled using varying degrees of fidelity.
We use Nesterov's acceleration techniques together with the multi-level
approach to achieve $\mathcal{O}(1/\sqrt{\epsilon})$ convergence rate, where
$\epsilon$ denotes the desired accuracy. The proposed method has a better
convergence rate than any other existing multi-level method for convex
problems, and in addition has the same rate as accelerated methods, which is
known to be optimal for first-order methods. However, as our numerical
experiments show, on large-scale face recognition problems our algorithm is
several times faster than the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05722</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05722</id><created>2015-09-18</created><authors><author><keyname>Zehnder</keyname><forenames>Michael</forenames></author><author><keyname>Wache</keyname><forenames>Holger</forenames></author><author><keyname>Witschel</keyname><forenames>Hans-Friedrich</forenames></author><author><keyname>Zanatta</keyname><forenames>Danilo</forenames></author><author><keyname>Rodriguez</keyname><forenames>Miguel</forenames></author></authors><title>Energy saving in smart homes based on consumer behaviour: A case study</title><categories>stat.ML cs.AI cs.MA cs.SY</categories><comments>To be presented on IEEE International Smart Cities Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a case study of a recommender system that can be used to
save energy in smart homes without lowering the comfort of the inhabitants. We
present an algorithm that uses consumer behavior data only and uses machine
learning to suggest actions for inhabitants to reduce the energy consumption of
their homes. The system mines for frequent and periodic patterns in the event
data provided by the Digitalstrom home automation system. These patterns are
converted into association rules, prioritized and compared with the current
behavior of the inhabitants. If the system detects an opportunities to save
energy without decreasing the comfort level it sends a recommendation to the
residents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05725</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05725</id><created>2015-09-18</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author><author><keyname>&#x17d;ivn&#xfd;</keyname><forenames>Stanislav</forenames></author></authors><title>Backdoors into Heterogeneous Classes of SAT and CSP</title><categories>cs.AI cs.CC</categories><comments>a preliminary version appeared in Proc. AAAI'14, pp. 2652-2658, AAAI
  Press 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the classical notion of strong and weak backdoor sets
by allowing that different instantiations of the backdoor variables result in
instances that belong to different base classes; the union of the base classes
forms a heterogeneous base class. Backdoor sets to heterogeneous base classes
can be much smaller than backdoor sets to homogeneous ones, hence they are much
more desirable but possibly harder to find. We draw a detailed complexity
landscape for the problem of detecting strong and weak backdoor sets into
heterogeneous base classes for SAT and CSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05736</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05736</id><created>2015-09-18</created><authors><author><keyname>Atoum</keyname><forenames>Issa</forenames></author><author><keyname>Bong</keyname><forenames>Chih How</forenames></author><author><keyname>Kulathuramaiyer</keyname><forenames>Narayanan</forenames></author></authors><title>Building a Pilot Software Quality-in-Use Benchmark Dataset</title><categories>cs.SE cs.CL</categories><comments>6 pages,3 figures, conference Proceedings of 9th International
  Conference on IT in Asia CITA (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prepared domain specific datasets plays an important role to supervised
learning approaches. In this article a new sentence dataset for software
quality-in-use is proposed. Three experts were chosen to annotate the data
using a proposed annotation scheme. Then the data were reconciled in a (no
match eliminate) process to reduce bias. The Kappa, k statistics revealed an
acceptable level of agreement; moderate to substantial agreement between the
experts. The built data can be used to evaluate software quality-in-use models
in sentiment analysis models. Moreover, the annotation scheme can be used to
extend the current dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05739</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05739</id><created>2015-09-18</created><authors><author><keyname>Thill</keyname><forenames>Matthew</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Low-Coherence Frames from Group Fourier Matrices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in areas such as compressive sensing and coding theory seek to
design a set of equal-norm vectors with large angular separation. This idea is
essentially equivalent to constructing a frame with low coherence. The elements
of such frames can in turn be used to build high-performance spherical codes,
quantum measurement operators, and compressive sensing measurement matrices, to
name a few applications.
  In this work, we allude to the group-frame construction first described by
Slepian and further explored in the works of Vale and Waldron. We present a
method for selecting representations of a finite group to construct a group
frame that achieves low coherence. Our technique produces a tight frame with a
small number of distinct inner product values between the frame elements, in a
sense approximating a Grassmanian frame. We identify special cases in which our
construction yields some previously-known frames with optimal coherence meeting
the Welch lower bound, and other cases in which the entries of our frame
vectors come from small alphabets. In particular, we apply our technique to the
problem choosing a subset of rows of a Hadamard matrix so that the resulting
columns form a low-coherence frame. Finally, we give an explicit calculation of
the average coherence of our frames, and find regimes in which they satisfy the
Strong Coherence Property described by Mixon, Bajwa, and Calderbank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05742</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05742</id><created>2015-09-18</created><authors><author><keyname>Wang</keyname><forenames>Haohan</forenames></author><author><keyname>Ganapathiraju</keyname><forenames>Madhavi K.</forenames></author></authors><title>Evaluation of Protein-protein Interaction Predictors with Noisy
  Partially Labeled Data Sets</title><categories>cs.AI stat.ML</categories><comments>preprint version, 9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein-protein interaction (PPI) prediction is an important problem in
machine learning and computational biology. However, there is no data set for
training or evaluation purposes, where all the instances are accurately
labeled. Instead, what is available are instances of positive class (with
possibly noisy labels) and no instances of negative class. The non-availability
of negative class data is typically handled with the observation that randomly
chosen protein-pairs have a nearly 100% chance of being negative class, as only
1 in 1,500 protein pairs expected is expected to be an interacting pair. In
this paper, we focused on the problem that non-availability of accurately
labeled testing data sets in the domain of protein-protein interaction (PPI)
prediction may lead to biased evaluation results. We first showed that not
acknowledging the inherent skew in the interactome (i.e. rare occurrence of
positive instances) leads to an over-estimated accuracy of the predictor. Then
we show that, with the belief that positive interactions are a rare category,
sampling random pairs of proteins excluding known interacting proteins set as
the negative testing data set could lead to an under-estimated evaluation
result. We formalized those two problems to validate the above claim, and based
on the formalization, we proposed a balancing method to cancel out the
over-estimation with under-estimation. Finally, our experiments validated the
theoretical aspects and showed that this balancing evaluation could evaluate
the exact performance without availability of golden standard data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05745</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05745</id><created>2015-09-17</created><authors><author><keyname>Kirthi</keyname><forenames>Krishnamurthy</forenames></author></authors><title>Narayana Sequences for Cryptographic Applications</title><categories>math.NT cs.CR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the randomness and cryptographic properties of the
Narayana series modulo p, where p is a prime number. It is shown that the
period of the Narayana series modulo p is either p*p+p+1 (or a divisor) or
p*p-1 (or a divisor). It is shown that the sequence has very good
autocorrelation and crosscorrelation properties which can be used in
cryptographic and key generation applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05751</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05751</id><created>2015-09-18</created><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Fox</keyname><forenames>Kyle</forenames></author><author><keyname>Nath</keyname><forenames>Abhinandan</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Computing the Gromov-Hausdorff Distance for Metric Trees</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gromov-Hausdorff distance is a natural way to measure distance between
two metric spaces. We give the first proof of hardness and first non-trivial
approximation algorithm for computing the Gromov-Hausdorff distance for
geodesic metrics in trees. Specifically, we prove it is NP-hard to approximate
the Gromov-Hausdorff distance better than a factor of 3. We complement this
result by providing a polynomial time $O(\min\{n, \sqrt{rn}\})$-approximation
algorithm where $r$ is the ratio of the longest edge length in both trees to
the shortest edge length. For metric trees with unit length edges, this yields
an $O(\sqrt{n})$-approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05760</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05760</id><created>2015-09-18</created><updated>2015-10-13</updated><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Yang</keyname><forenames>Scott</forenames></author></authors><title>Accelerating Optimization via Adaptive Prediction</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a powerful general framework for designing data-dependent
optimization algorithms, building upon and unifying recent techniques in
adaptive regularization, optimistic gradient predictions, and problem-dependent
randomization. We first present a series of new regret guarantees that hold at
any time and under very minimal assumptions, and then show how different
relaxations recover existing algorithms, both basic as well as more recent
sophisticated ones. Finally, we show how combining adaptivity, optimism, and
problem-dependent randomization can guide the design of algorithms that benefit
from more favorable guarantees than recent state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05765</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05765</id><created>2015-09-18</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>&quot;Oddball SGD&quot;: Novelty Driven Stochastic Gradient Descent for Training
  Deep Neural Networks</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is arguably the most popular of the machine
learning methods applied to training deep neural networks (DNN) today. It has
recently been demonstrated that SGD can be statistically biased so that certain
elements of the training set are learned more rapidly than others. In this
article, we place SGD into a feedback loop whereby the probability of selection
is proportional to error magnitude. This provides a novelty-driven oddball SGD
process that learns more rapidly than traditional SGD by prioritising those
elements of the training set with the largest novelty (error). In our DNN
example, oddball SGD trains some 50x faster than regular SGD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05789</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05789</id><created>2015-09-18</created><authors><author><keyname>Checco</keyname><forenames>Alessandro</forenames></author><author><keyname>Bianchi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Leith</keyname><forenames>Doug</forenames></author></authors><title>BLC: Private Matrix Factorization Recommenders via Automatic Group
  Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a privacy-enhanced matrix factorization recommender that exploits
the fact that users can often be grouped together by interest. This allows a
form of &quot;hiding in the crowd&quot; privacy. We introduce a novel matrix
factorization approach suited to making recommendations in a shared group (or
nym) setting and the BLC algorithm for carrying out this matrix factorization
in a privacy-enhanced manner. We demonstrate that the increased privacy does
not come at the cost of reduced recommendation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05798</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05798</id><created>2015-09-18</created><updated>2015-11-23</updated><authors><author><keyname>&#x17b;oga&#x142;a-Siudem</keyname><forenames>Barbara</forenames></author><author><keyname>Siudem</keyname><forenames>Grzegorz</forenames></author><author><keyname>Cena</keyname><forenames>Anna</forenames></author><author><keyname>Gagolewski</keyname><forenames>Marek</forenames></author></authors><title>Agent-based model for the h-index - Exact solution</title><categories>physics.soc-ph cs.DL</categories><doi>10.1140/epjb/e2015-60757-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hirsch's $h$-index is perhaps the most popular citation-based measure of
the scientific excellence. In 2013 G. Ionescu and B. Chopard proposed an
agent-based model for this index to describe a publications and citations
generation process in an abstract scientific community. With such an approach
one can simulate a single scientist's activity, and by extension investigate
the whole community of researchers. Even though this approach predicts quite
well the $h$-index from bibliometric data, only a solution based on simulations
was given. In this paper, we complete their results with exact, analytic
formulas. What is more, due to our exact solution we are able to simplify the
Ionescu-Chopard model which allows us to obtain a compact formula for
$h$-index. Moreover, a simulation study designed to compare both, approximated
and exact, solutions is included. The last part of this paper presents
evaluation of the obtained results on a real-word data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05806</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05806</id><created>2015-09-18</created><updated>2015-10-08</updated><authors><author><keyname>Bermejo-Vega</keyname><forenames>Juan</forenames></author><author><keyname>Zatloukal</keyname><forenames>Kevin C.</forenames></author></authors><title>Abelian Hypergroups and Quantum Computation</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>41 pages + 6 pages appendices. Added references and corrected typos
  in this version (sections 1-2)</comments><msc-class>43A62, 20N20</msc-class><acm-class>F.1.1; F.2.1; F.1.3; F.1.2; B.8.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a connection, described here for the first time, between the
hidden normal subgroup problem (HNSP) and abelian hypergroups (algebraic
objects that model collisions of physical particles), we develop a stabilizer
formalism using abelian hypergroups and an associated classical simulation
theorem (a la Gottesman-Knill). Using these tools, we develop the first
provably efficient quantum algorithm for finding hidden subhypergroups of
nilpotent abelian hypergroups and, via the aforementioned connection, a new,
hypergroup-based algorithm for the HNSP on nilpotent groups. We also give
efficient methods for manipulating non-unitary, non-monomial stabilizers and an
adaptive Fourier sampling technique of general interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05807</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05807</id><created>2015-09-18</created><authors><author><keyname>Avgustinovich</keyname><forenames>Sergey</forenames></author><author><keyname>Kitaev</keyname><forenames>Sergey</forenames></author><author><keyname>Potapov</keyname><forenames>Vladimir N.</forenames></author><author><keyname>Vajnovszki</keyname><forenames>Vincent</forenames></author></authors><title>Gray coding planar maps</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of (combinatorial) Gray codes is to list objects in question in such
a way that two successive objects differ in some pre-specified small way. In
this paper, we utilize beta-description trees to cyclicly Gray code three
classes of cubic planar maps, namely, bicubic planar maps, 3-connected cubic
planar maps, and cubic non-separable planar maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05808</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05808</id><created>2015-09-18</created><authors><author><keyname>Hashimoto</keyname><forenames>Tatsunori B.</forenames></author><author><keyname>Alvarez-Melis</keyname><forenames>David</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>Word, graph and manifold embedding from Markov processes</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous vector representations of words and objects appear to carry
surprisingly rich semantic content. In this paper, we advance both the
conceptual and theoretical understanding of word embeddings in three ways.
First, we ground embeddings in semantic spaces studied in
cognitive-psychometric literature and introduce new evaluation tasks. Second,
in contrast to prior work, we take metric recovery as the key object of study,
unify existing algorithms as consistent metric recovery methods based on
co-occurrence counts from simple Markov random walks, and propose a new
recovery algorithm. Third, we generalize metric recovery to graphs and
manifolds, relating co-occurence counts on random walks in graphs and random
processes on manifolds to the underlying metric to be recovered, thereby
reconciling manifold estimation and embedding algorithms. We compare embedding
algorithms across a range of tasks, from nonlinear dimensionality reduction to
three semantic language tasks, including analogies, sequence completion, and
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05809</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05809</id><created>2015-09-18</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Lower bounds for approximation schemes for Closest String</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Closest String problem one is given a family $\mathcal S$ of
equal-length strings over some fixed alphabet, and the task is to find a string
$y$ that minimizes the maximum Hamming distance between $y$ and a string from
$\mathcal S$. While polynomial-time approximation schemes (PTASes) for this
problem are known for a long time [Li et al., J. ACM'02], no efficient
polynomial-time approximation scheme (EPTAS) has been proposed so far. In this
paper, we prove that the existence of an EPTAS for Closest String is in fact
unlikely, as it would imply that $\mathrm{FPT}=\mathrm{W}[1]$, a highly
unexpected collapse in the hierarchy of parameterized complexity classes. Our
proof also shows that the existence of a PTAS for Closest String with running
time $f(\varepsilon)\cdot n^{o(1/\varepsilon)}$, for any computable function
$f$, would contradict the Exponential Time Hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05821</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05821</id><created>2015-09-18</created><authors><author><keyname>Solymosi</keyname><forenames>Jozsef</forenames></author><author><keyname>Zahl</keyname><forenames>Joshua</forenames></author></authors><title>New bounds on curve tangencies and orthogonalities</title><categories>math.CO cs.CG</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish new bounds on the number of tangencies and orthogonal
intersections determined by an arrangement of curves. First, given a set of $n$
complex algebraic plane curves of degree at most $D$, we show that there are
$O(n^{3/2})$ points where two or more curves are tangent. In particular, if no
three curves are mutually tangent at a common point, then there are
$O(n^{3/2})$ curve-curve tangencies. Second, given a family of degree $D$
complex algebraic plane curves and a set of $n$ curves from this family, we
show that either there are $O(n^{3/2})$ points where two or more curves are
orthogonal, or the family of curves has certain special properties.
  We obtain these bounds by transforming the arrangement of plane curves into
an arrangement of space curves so that tangency (or orthogonality) of the
original plane curves corresponds to intersection of space curves. We then
bound the number of intersections of the corresponding space curves. For the
case of curve-curve tangency, we use a polynomial method technique that is
reminiscent of Guth and Katz's proof of the joints theorem. For the case of
orthogonal curve intersections, we employ Guth and Zahl's bound for two-rich
points in space curve arrangements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05823</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05823</id><created>2015-09-18</created><authors><author><keyname>Jafarizadeh</keyname><forenames>Saber</forenames></author></authors><title>Optimizing the Convergence Rate of the Continuous Time Quantum Consensus</title><categories>cs.SY</categories><comments>48 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recent developments in the fields of quantum distributed
computing, quantum systems are analyzed as networks of quantum nodes to reduce
the complexity of the analysis. This gives rise to the distributed quantum
consensus algorithms. Focus of this paper is on optimizing the convergence rate
of the continuous time quantum consensus algorithm over a quantum network with
$N$ qudits. It is shown that the optimal convergence rate is independent of the
value of $d$ in qudits. First by classifying the induced graphs as the Schreier
graphs, they are categorized in terms of the partitions of integer $N$. Then
establishing the intertwining relation between one level dominant partitions in
the Hasse Diagram of integer $N$, it is proved that the spectrum of the induced
graph corresponding to the dominant partition is included in that of the less
dominant partition. Based on this result, the proof of the Aldous' conjecture
is extended to all possible induced graphs and the original optimization
problem is reduced to optimizing spectral gap of the smallest induced graph. By
providing the analytical solution to semidefinite programming formulation of
the obtained problem, closed-form expressions for the optimal results are
provided for a wide range of topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05828</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05828</id><created>2015-09-18</created><authors><author><keyname>Soulignac</keyname><forenames>Francisco J.</forenames></author></authors><title>A certifying and dynamic algorithm for the recognition of proper
  circular-arc graphs</title><categories>cs.DS cs.DM</categories><comments>44 pages, 8 figures, appendix with 11 pages and many figures</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a dynamic algorithm for the recognition of proper circular-arc
(PCA) graphs, that supports the insertion and removal of vertices (together
with its incident edges). The main feature of the algorithm is that it outputs
a minimally non-PCA induced subgraph when the insertion of a vertex fails. Each
operation cost $O(\log n + d)$ time, where $n$ is the number vertices and $d$
is the degree of the modified vertex. When removals are disallowed, each
insertion is processed in $O(d)$ time. The algorithm also provides two
constant-time operations to query if the dynamic graph is proper Helly (PHCA)
or proper interval (PIG). When the dynamic graph is not PHCA (resp. PIG), a
minimally non-PHCA (resp. non-PIG) induced subgraph is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05830</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05830</id><created>2015-09-18</created><authors><author><keyname>Ayvali</keyname><forenames>Elif</forenames></author><author><keyname>Srivatsan</keyname><forenames>Rangaprasad Arun</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author><author><keyname>Roy</keyname><forenames>Rajarshi</forenames></author><author><keyname>Simaan</keyname><forenames>Nabil</forenames></author><author><keyname>Choset</keyname><forenames>Howie</forenames></author></authors><title>Using Bayesian Optimization to Guide Probing of a Flexible Environment
  for Simultaneous Registration and Stiffness Mapping</title><categories>cs.RO</categories><comments>7 pages,10 figures, submitted to The International Conference on
  Robotics and Automation (ICRA 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the goals of computer-aided surgery is to match intraoperative data to
preoperative images of the anatomy and add complementary information that can
facilitate the task of surgical navigation. In this context, mechanical
palpation can reveal critical anatomical features such as arteries and
cancerous lumps which are stiffer that the surrounding tissue. This work uses
position and force measurements obtained during mechanical palpation for
registration and stiffness mapping. Prior approaches, including our own,
exhaustively palpated the entire organ to achieve this goal. To overcome the
costly palpation of the entire organ, a Bayesian optimization framework is
introduced to guide the end effector to palpate stiff regions while
simultaneously updating the registration of the end effector to an a priori
geometric model of the organ, hence enabling the fusion of ntraoperative data
into the a priori model obtained through imaging. This new framework uses
Gaussian processes to model the stiffness distribution and Bayesian
optimization to direct where to sample next for maximum information gain. The
proposed method was evaluated with experimental data obtained using a Cartesian
robot interacting with a silicone organ model and an ex vivo porcine liver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05831</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05831</id><created>2015-09-18</created><authors><author><keyname>Lozovskiy</keyname><forenames>Alexander</forenames></author></authors><title>A greedy algorithm for the minimization of a ratio of same-index element
  sums from two positive arrays</title><categories>math.CO cs.DS</categories><comments>9 pages</comments><msc-class>90C27 (Primary), 68P10, 68R05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider two ordered positive real number arrays of equal size. The problem
is to find such set of indices of given size that the ratio of the sums of the
array elements with those indices is minimized. In this work, in order to
mitigate the exponential complexity of the brute force search, we present a
greedy algorithm applied to the search of such an index set. The main result of
the paper is the theorem that states that the algorithm eliminates from
candidates all index sets that do not contain any elements from the greedily
selected set. We additionally prove exactness for a particular case of a ratio
of the sums of only two elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05842</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05842</id><created>2015-09-18</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author></authors><title>Structure Preserving Bisimilarity, Supporting an Operational Petri Net
  Semantics of CCSP</title><categories>cs.LO</categories><acm-class>F.1.2; F.3.2</acm-class><journal-ref>Proc. Correct System Design - Symposium in Honor of
  Ernst-R\&quot;udiger Olderog on the Occasion of His 60th Birthday (R. Meyer, A.
  Platzer &amp; H. Wehrheim, eds.), Oldenburg, Germany, September 8-9, 2015, LNCS
  9360, Springer, pp. 99-130</journal-ref><doi>10.1007/978-3-319-23506-6_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1987 Ernst-R\&quot;udiger Olderog provided an operational Petri net semantics
for a subset of CCSP, the union of Milner's CCS and Hoare's CSP. It assigns to
each process term in the subset a labelled, safe place/transition net. To
demonstrate the correctness of the approach, Olderog established agreement (1)
with the standard interleaving semantics of CCSP up to strong bisimulation
equivalence, and (2) with standard denotational interpretations of CCSP
operators in terms of Petri nets up to a suitable semantic equivalence that
fully respects the causal structure of nets. For the latter he employed a
linear-time semantic equivalence, namely having the same causal nets.
  This paper strengthens (2), employing a novel branching-time version of this
semantics---structure preserving bisimilarity---that moreover preserves
inevitability. I establish that it is a congruence for the operators of CCSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05844</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05844</id><created>2015-09-18</created><authors><author><keyname>Yang</keyname><forenames>Zhibo</forenames></author><author><keyname>Xu</keyname><forenames>Huanle</forenames></author><author><keyname>Fu</keyname><forenames>Keda</forenames></author><author><keyname>Xia</keyname><forenames>Yong</forenames></author></authors><title>Similar Handwritten Chinese Character Discrimination by Weakly
  Supervised Learning</title><categories>cs.CV</categories><comments>5 figures, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional approaches for handwritten Chinese character recognition suffer
in classifying similar characters. In this paper, we propose to discriminate
similar handwritten Chinese characters by using weakly supervised learning. Our
approach learns a discriminative SVM for each similar pair which simultaneously
localizes the discriminative region of similar character and makes the
classification. For the first time, similar handwritten Chinese character
recognition (SHCCR) is formulated as an optimization problem extended from SVM.
We also propose a novel feature descriptor, Gradient Context, and apply
bag-of-words model to represent regions with different scales. In our method,
we do not need to select a sized-fixed sub-window to differentiate similar
characters. The unconstrained property makes our method well adapted to high
variance in the size and position of discriminative regions in similar
handwritten Chinese characters. We evaluate our proposed approach over the
CASIA Chinese character data set and the results show that our method
outperforms the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05849</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05849</id><created>2015-09-19</created><authors><author><keyname>Ong</keyname><forenames>Frank</forenames></author><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Fast and Efficient Sparse 2D Discrete Fourier Transform using
  Sparse-Graph Codes</title><categories>cs.IT cs.MM cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm, named the 2D-FFAST, to compute a sparse
2D-Discrete Fourier Transform (2D-DFT) featuring both low sample complexity and
low computational complexity. The proposed algorithm is based on mixed concepts
from signal processing (sub-sampling and aliasing), coding theory (sparse-graph
codes) and number theory (Chinese-remainder-theorem) and generalizes the
1D-FFAST 2 algorithm recently proposed by Pawar and Ramchandran [1] to the 2D
setting. Concretely, our proposed 2D-FFAST algorithm computes a k-sparse
2D-DFT, with a uniformly random support, of size N = Nx x Ny using O(k)
noiseless spatial-domain measurements in O(k log k) computational time. Our
results are attractive when the sparsity is sub-linear with respect to the
signal dimension, that is, when k -&gt; infinity and k/N -&gt; 0. For the case when
the spatial-domain measurements are corrupted by additive noise, our 2D-FFAST
framework extends to a noise-robust version in sub-linear time of O(k log4 N )
using O(k log3 N ) measurements. Simulation results, on synthetic images as
well as real-world magnetic resonance images, are provided in Section VII and
demonstrate the empirical performance of the proposed 2D-FFAST algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05856</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05856</id><created>2015-09-19</created><updated>2016-01-24</updated><authors><author><keyname>Haddad</keyname><forenames>Serj</forenames></author><author><keyname>Leveque</keyname><forenames>Olivier</forenames></author></authors><title>On the Broadcast Capacity of Large Wireless Networks at Low SNR</title><categories>cs.IT math.IT</categories><comments>20 pages, 5 figures, presented at ISIT 2015, submitted to the IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper focuses on the problem of broadcasting information in the
most efficient manner in a large two-dimensional ad hoc wireless network at low
SNR and under line-of-sight propagation. A new communication scheme is
proposed, where source nodes first broadcast their data to the entire network,
despite the lack of sufficient available power. The signal's power is then
reinforced via successive back-and-forth beamforming transmissions between
different groups of nodes in the network, so that all nodes are able to decode
the transmitted information at the end. This scheme is shown to achieve
asymptotically the broadcast capacity of the network, which is expressed in
terms of the largest singular value of the matrix of fading coefficients
between the nodes in the network. A detailed mathematical analysis is then
presented to evaluate the asymptotic behavior of this largest singular value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05870</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05870</id><created>2015-09-19</created><authors><author><keyname>Fan</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Chengqian</forenames></author><author><keyname>Ma</keyname><forenames>Zongjie</forenames></author><author><keyname>Brankovic</keyname><forenames>LjiLjana</forenames></author><author><keyname>Estivill-Castro</keyname><forenames>Vladimir</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>Exploiting Reduction Rules and Data Structures: Local Search for Minimum
  Vertex Cover in Massive Graphs</title><categories>cs.DS cs.AI</categories><comments>7 pages, 3 figures, 2 tables, 6 algorithms, submitted to AAAI-16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Vertex Cover (MinVC) problem is a well-known NP-hard problem.
Recently there has been great interest in solving this problem on real-world
massive graphs. For such graphs, local search is a promising approach to
finding optimal or near-optimal solutions. In this paper we propose a local
search algorithm that exploits reduction rules and data structures to solve the
MinVC problem in such graphs. Experimental results on a wide range of real-word
massive graphs show that our algorithm finds better covers than
state-of-the-art local search algorithms for MinVC. Also we present interesting
results about the complexities of some well-known heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05874</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05874</id><created>2015-09-19</created><updated>2015-10-05</updated><authors><author><keyname>M.</keyname><forenames>Anjana A.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Index Coded PSK Modulation</title><categories>cs.IT math.IT</categories><comments>11 pages and 12 figures. Few tables have been included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider noisy index coding problem over AWGN channel. We
give an algorithm to map the index coded bits to appropriate sized PSK symbols
such that for the given index code, in general, the receiver with large amount
of side information will gain in probability of error performance compared to
the ones with lesser amount, depending upon the index code used. We call this
the \textbf{PSK side information coding gain}. Also, we show that receivers
with large amount of side information obtain this coding gain in addition to
the bandwidth gain whereas receivers with lesser amount of side information
trade off this coding gain with bandwidth gain. Moreover, in general, the
difference between the best and worst performance among the receivers is shown
to be proportional to the length of the index code employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05875</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05875</id><created>2015-09-19</created><authors><author><keyname>Yang</keyname><forenames>Wutong</forenames></author><author><keyname>Xu</keyname><forenames>Minxian</forenames></author><author><keyname>Li</keyname><forenames>Guozhong</forenames></author><author><keyname>Tian</keyname><forenames>Wenhong</forenames></author></authors><title>CloudSimNFV: Modeling and Simulation of Energy-Efficient NFV in Cloud
  Data Centers</title><categories>cs.DC</categories><comments>Cloud Computing, Network Function Virtualization, Simulation Toolkit,
  Energy Consumption Model, Scheduling Algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Function Virtualization (NFV) takes advantage of hardware
virtualization to undertake software processing for various functions, and
complements the drawbacks of traditional network technology. To speed up NFV
related research, we need a user friendly and easy to use research tool, which
could support data center simulation, scheduling algorithms implementation and
extension, and provide energy consumption simulation. As a cloud simulation
toolkit, CloudSim has strong extendibility that could be extended to simulate
NFV environment. This paper introduces a NFV cloud framework based on CloudSim
and an energy consumption model based on multi-dimensional extension,
implementing a toolkit named ClousimNFV to simulate the NFV scenario, proposing
several scheduling algorithm based on for NFV applications. The toolkit
validation and algorithm performance comparison are also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05877</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05877</id><created>2015-09-19</created><authors><author><keyname>Aminian</keyname><forenames>Gholamali</forenames></author><author><keyname>Ghazani</keyname><forenames>Maryam Farahnak</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>On the Capacity of Point-to-Point and Multiple-Access Molecular
  Communications with Ligand-Receptors</title><categories>cs.IT math.IT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the bacterial point-to-point and multiple-access
molecular communications with ligand-receptors. For the point-to-point
communication, we investigate common signaling methods, namely the Level
Scenario (LS), which uses one type of a molecule with different concentration
levels, and the Type Scenario (TS), which employs multiple types of molecules
with a single concentration level. We investigate the trade-offs between the
two scenarios from the capacity point of view. We derive an upper bound on the
capacity using a Binomial Channel (BIC) model and the symmetrized
Kullback-Leibler (KL) divergence. A lower bound is also derived when the
environment noise is negligible. For the TS, we also consider the effect of
blocking of a receptor by a different molecule type. Then, we consider
multiple-access communications, for which we investigate three scenarios based
on molecule and receptor types, i.e., same types of molecules with Different
Labeling and Same types of Receptors (DLSR), Different types of Molecules and
Receptors (DMDR), and Same types of Molecules and Receptors (SMSR). We
investigate the trade-offs among the three scenarios from the total capacity
point of view. We derive some inner bounds on the capacity region of these
scenarios when the environment noise is negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05882</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05882</id><created>2015-09-19</created><updated>2015-10-06</updated><authors><author><keyname>Patwardhan</keyname><forenames>Siddharth</forenames></author><author><keyname>Moulick</keyname><forenames>Subhayan Roy</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Efficient Controlled Quantum Secure Direct Communication Protocols</title><categories>quant-ph cs.CR</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study controlled quantum secure direct communication (CQSDC), a
cryptographic scheme where a sender can send a secret bit-string to an intended
recipient, without any secure classical channel, who can obtain the complete
bit-string only with the permission of a controller. We report an efficient
protocol to realize CQSDC using Cluster state and then go on to construct a
(2-3)-CQSDC using Brown state, where a coalition of any two of the three
controllers is required to retrieve the complete message. We argue both
protocols to be unconditionally secure and analyze the efficiency of the
protocols to show it to outperform the existing schemes while maintaining the
same security specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05883</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05883</id><created>2015-09-19</created><authors><author><keyname>Meinhardt</keyname><forenames>Holger Ingmar</forenames></author></authors><title>The Incorrect Usage of Propositional Logic in Game Theory: The Case of
  Disproving Oneself</title><categories>cs.GT</categories><comments>16 pages, 2 tables</comments><msc-class>03B05, 91A12, 91B24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we had to realize that more and more game theoretical articles have
been published in peer-reviewed journals with severe logical deficiencies. In
particular, we observed that the indirect proof was not applied correctly.
These authors confuse between statements of propositional logic. They apply an
indirect proof while assuming a prerequisite in order to get a contradiction.
For instance, to find out that &quot;if A then B&quot; is valid, they suppose that the
assumptions &quot;A and not B&quot; are valid to derive a contradiction in order to
deduce &quot;if A then B&quot;. Hence, they want to establish the equivalent proposition
&quot;A and not B implies A and not A&quot; to conclude that &quot;if A then B&quot; is valid. In
fact, they prove that a truth implies a falsehood, which is a wrong statement.
As a consequence, &quot;if A then B&quot; is invalid, disproving their own results. We
present and discuss some selected cases from the literature with severe logical
flaws, invalidating the articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05895</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05895</id><created>2015-09-19</created><updated>2016-01-04</updated><authors><author><keyname>Lahlou</keyname><forenames>Tarek A.</forenames></author><author><keyname>Oppenheim</keyname><forenames>Alan V.</forenames></author></authors><title>Trading Accuracy for Numerical Stability: Orthogonalization,
  Biorthogonalization and Regularization</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two novel regularization methods motivated in part by the
geometric significance of biorthogonal bases in signal processing applications.
These methods, in particular, draw upon the structural relevance of
orthogonality and biorthogonality principles and are presented from the
perspectives of signal processing, convex programming, continuation methods and
nonlinear projection operators. Each method is specifically endowed with either
a homotopy or tuning parameter to facilitate tradeoff analysis between accuracy
and numerical stability. An example involving a basis comprised of real
exponential signals illustrates the utility of the proposed methods on an
ill-conditioned inverse problem and the results are compared to standard
regularization techniques from the signal processing literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05896</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05896</id><created>2015-09-19</created><authors><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>On space efficiency of algorithms working on structural decompositions
  of graphs</title><categories>cs.CC cs.DS</categories><acm-class>F.1.3; F.2.3; G.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dynamic programming on path and tree decompositions of graphs is a technique
that is ubiquitous in the field of parameterized and exponential-time
algorithms. However, one of its drawbacks is that the space usage is
exponential in the decomposition's width. Following the work of Allender et al.
[Theory of Computing, '14], we investigate whether this space complexity
explosion is unavoidable. Using the idea of reparameterization of Cai and
Juedes [J. Comput. Syst. Sci., '03], we prove that the question is closely
related to a conjecture that the Longest Common Subsequence problem
parameterized by the number of input strings does not admit an algorithm that
simultaneously uses XP time and FPT space. Moreover, we complete the complexity
landscape sketched for pathwidth and treewidth by Allender et al. by
considering the parameter tree-depth. We prove that computations on tree-depth
decompositions correspond to a model of non-deterministic machines that work in
polynomial time and logarithmic space, with access to an auxiliary stack of
maximum height equal to the decomposition's depth. Together with the results of
Allender et al., this describes a hierarchy of complexity classes for
polynomial-time non-deterministic machines with different restrictions on the
access to working space, which mirrors the classic relations between treewidth,
pathwidth, and tree-depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05897</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05897</id><created>2015-09-19</created><authors><author><keyname>Yang</keyname><forenames>Xu</forenames></author></authors><title>Face Photo Sketch Synthesis via Larger Patch and Multiresolution Spline</title><categories>cs.CV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Face photo sketch synthesis has got some researchers' attention in recent
years because of its potential applications in digital entertainment and law
enforcement. Some patches based methods have been proposed to solve this
problem. These methods usually focus more on how to get a sketch patch for a
given photo patch than how to blend these generated patches. However, without
appropriately blending method, some jagged parts and mottled points will appear
in the entire face sketch. In order to get a smoother sketch, we propose a new
method to reduce such jagged parts and mottled points. In our system, we resort
to an existed method, which is Markov Random Fields (MRF), to train a crude
face sketch firstly. Then this crude sketch face sketch will be divided into
some larger patches again and retrained by Non-Negative Matrix Factorization
(NMF). At last, we use Multiresolution Spline and a blend trick named
full-coverage trick to blend these retrained patches. The experiment results
show that compared with some previous method, we can get a smoother face
sketch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05909</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05909</id><created>2015-09-19</created><updated>2016-02-18</updated><authors><author><keyname>Kendall</keyname><forenames>Alex</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>Modelling Uncertainty in Deep Learning for Camera Relocalization</title><categories>cs.CV cs.RO</categories><comments>ICRA 2016; Fixed numerical error with rotation results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robust and real-time monocular six degree of freedom visual
relocalization system. We use a Bayesian convolutional neural network to
regress the 6-DOF camera pose from a single RGB image. It is trained in an
end-to-end manner with no need of additional engineering or graph optimisation.
The algorithm can operate indoors and outdoors in real time, taking under 6ms
to compute. It obtains approximately 2m and 6 degrees accuracy for very large
scale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian
convolutional neural network implementation we obtain an estimate of the
model's relocalization uncertainty and improve state of the art localization
accuracy on a large scale outdoor dataset. We leverage the uncertainty measure
to estimate metric relocalization error and to detect the presence or absence
of the scene in the input image. We show that the model's uncertainty is caused
by images being dissimilar to the training dataset in either pose or
appearance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05935</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05935</id><created>2015-09-19</created><authors><author><keyname>Jain</keyname><forenames>Paras</forenames></author><author><keyname>Chen</keyname><forenames>Shang-Tse</forenames></author><author><keyname>Azimpourkivi</keyname><forenames>Mozhgan</forenames></author><author><keyname>Chau</keyname><forenames>Duen Horng</forenames></author><author><keyname>Carbunar</keyname><forenames>Bogdan</forenames></author></authors><title>Spotting Suspicious Reviews via (Quasi-)clique Extraction</title><categories>cs.SI</categories><comments>Appeared in IEEE Symposium on Security and Privacy 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to tell if a review is real or fake? What does the underworld of
fraudulent reviewing look like? Detecting suspicious reviews has become a major
issue for many online services. We propose the use of a clique-finding approach
to discover well-organized suspicious reviewers. From a Yelp dataset with over
one million reviews, we construct multiple Reviewer Similarity graphs to link
users that have unusually similar behavior: two reviewers are connected in the
graph if they have reviewed the same set of venues within a few days. From
these graphs, our algorithms extracted many large cliques and quasi-cliques,
the largest one containing a striking 11 users who coordinated their review
activities in identical ways. Among the detected cliques, a large portion
contain Yelp Scouts who are paid by Yelp to review venues in new areas. Our
work sheds light on their little-known operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05936</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05936</id><created>2015-09-19</created><authors><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Mesnard</keyname><forenames>Thomas</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author><author><keyname>Zhang</keyname><forenames>Saizheng</forenames></author><author><keyname>Wu</keyname><forenames>Yuhai</forenames></author></authors><title>An objective function for STDP</title><categories>cs.NE cs.LG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a predictive objective function for the rate aspect of
spike-timing dependent plasticity (STDP), i.e., ignoring the effects of
synchrony of spikes but looking at spiking {\em rate changes}. The proposed
weight update is proportional to the presynaptic spiking (or firing) rate times
the {\em temporal change} of the integrated postsynaptic activity. We present
an intuitive explanation for the relationship between spike-timing and weight
change that arises when the weight change follows this rule. Spike-based
simulations agree with the proposed relationship between spike timing and the
temporal change of postsynaptic activity. They show a strong correlation
between the biologically observed STDP behavior and the behavior obtained from
simulations where the weight change follows the gradient of the predictive
objective function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05937</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05937</id><created>2015-09-19</created><authors><author><keyname>Vikas</keyname><forenames>Vishesh</forenames></author><author><keyname>Grover</keyname><forenames>Piyush</forenames></author><author><keyname>Trimmer</keyname><forenames>Barry</forenames></author></authors><title>Model-free control framework for multi-limb soft robots</title><categories>cs.RO</categories><comments>IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2015</comments><doi>10.1109/IROS.2015.7353509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deformable and continuum nature of soft robots promises versatility and
adaptability. However, control of modular, multi-limbed soft robots for
terrestrial locomotion is challenging due to the complex robot structure,
actuator mechanics and robot-environment interaction. Traditionally, soft robot
control is performed by modeling kinematics using exact geometric equations and
finite element analysis. The research presents an alternative, model-free,
data-driven, reinforcement learning inspired approach, for controlling
multi-limbed soft material robots. This control approach can be summarized as a
four-step process of discretization, visualization, learning and optimization.
The first step involves identification and subsequent discretization of key
factors that dominate robot-environment, in turn, the robot control. Graph
theory is used to visualize relationships and transitions between the
discretized states. The graph representation facilitates mathematical
definition of periodic control patterns (simple cycles) and locomotion gaits.
Rewards corresponding to individual arcs of the graph are weighted displacement
and orientation change for robot state-to-state transitions. These rewards are
specific to surface of locomotion and are learned. Finally, the control
patterns result from optimization of reward dependent locomotion task (e.g.
translation) cost function. The optimization problem is an Integer Linear
Programming problem which can be quickly solved using standard solvers. The
framework is generic and independent of type of actuator, soft material
properties or the type of friction mechanism, as the control exists in the
robot's task space. Furthermore, the data-driven nature of the framework
imparts adaptability to the framework toward different locomotion surfaces by
re-learning rewards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05939</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05939</id><created>2015-09-19</created><authors><author><keyname>Azarafrooz</keyname><forenames>Mahdi</forenames></author><author><keyname>Chandramouli</keyname><forenames>R.</forenames></author></authors><title>Complexity Analysis of CSMA Scheduling via Dependencies Matrix</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of a CSMA algorithm has been translated to the norm properties
of a dependencies matrix.
  The maximum throughput optimization is reformulated by including the
dependencies matrix in the formulations. It has been shown that for the
interference graphs $\mathcal{G}$ that have minimum vertex cover size
$\mathcal{C}(\mathcal{G})=\log n$ where $n$ is the number of the links, the
optimal strategy of the links is to transmit with the probability 1, i.e a
service-rate agnostic approach.
  Several numerical analyses have been conducted in order to illustrate the
effect of the interference graph, transmission strategy and arrival rate on the
dependencies matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05957</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05957</id><created>2015-09-19</created><authors><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>Knapsack in graph groups, HNN-extensions and amalgamated products</title><categories>math.GR cs.FL</categories><comments>42 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is shown that the knapsack problem, which was introduced by Myasnikov et
al. for arbitrary finitely generated groups, can be solved in NP for graph
groups. This result even holds if the group elements are represented in a
compressed form by SLPs, which generalizes the classical NP-completeness result
of the integer knapsack problem. We also prove general transfer results:
NP-membership of the knapsack problem is passed on to finite extensions,
HNN-extensions over finite associated subgroups, and amalgamated products with
finite identified subgroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05958</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05958</id><created>2015-09-19</created><authors><author><keyname>Farooq</keyname><forenames>Umar</forenames></author><author><keyname>Bashir</keyname><forenames>Sajid</forenames></author><author><keyname>Tasneem</keyname><forenames>Tauseef</forenames></author><author><keyname>Saboor</keyname><forenames>A.</forenames></author><author><keyname>Rauf</keyname><forenames>A.</forenames></author></authors><title>Migration from Copper to Fiber Access Network using Passive Optical
  Network for Green and Dry Field Areas of Pakistan</title><categories>cs.NI</categories><comments>11 pages, International Journal of Soft Computing and Engineering
  (IJSCE), Volume-5, Issue-4, September 2015</comments><journal-ref>International Journal of Soft Computing and Engineering (IJSCE),
  Volume-5, Issue-4, September 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passive Optical Networks (PON) technology brings an evolution in the industry
of Telecommunication for the provisioning of High Speed Internet (HSI) and
Triple Play bundled Services that includes Voice, Data, and Video Streaming
throughout the world. In Pakistan most of the service providers are offering
broadband services on traditional copper OSP (Outside Plant) network since
2000. Demand for the high speed internet and broadband is increasing rapidly,
it is desired with great need to migrate from traditional copper based OSP
network to PON _ FTTx (Fiber To The x) infrastructure. Considering the
geographical requirements in Pakistan a scalable fiber network is required
which can be optimized as per the users requirements and demands with high
speed bandwidth efficiency, involving the minimum losses and with ideal capital
expenditure (CAPEX). In this work a platform for migration from copper to fiber
access network with a scalable and optimized PON _ FTTx infrastructure in green
field and dry field areas of Pakistan have been proposed using Geographic
Information system (GIS). In any developing country like Pakistan having the
same cultural and geographical topology, this platform can be used to migrate
from copper to fiber access network to provide the PON based telecom services.
The developed platform for migration from copper to PON based fiber has been
studied, planned, and then simulated on a selected geographical area of
Pakistan with physical execution that showed better and efficient results with
reduction in capital and operational expenditures. A factual plan without
ambiguities assists the operators of Pakistan to analyze and forecast bandwidth
requirements of an area, optimized network planning along with the in time and
efficient deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05959</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05959</id><created>2015-09-19</created><authors><author><keyname>Leng</keyname><forenames>Shiyang</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Objective Beamforming for Energy-Efficient SWIPT Systems</title><categories>cs.IT math.IT</categories><comments>accepted, 2016 International Conference on Computing, Networking and
  Communications, Wireless Communications Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the resource allocation algorithm design for
energy-efficient simultaneous wireless information and power transfer (SWIPT)
systems. The considered system comprises a transmitter, an information
receiver, and multiple energy harvesting receivers equipped with multiple
antennas. We propose a multi-objective optimization framework to study the
trade-off between the maximization of the energy efficiency of information
transmission and the maximization of wireless power transfer efficiency. The
proposed problem formulation takes into account the per antenna circuit power
consumption of the transmitter and the imperfect channel state information of
the energy harvesting receivers. The adopted non-convex multi-objective
optimization problem is transformed into an equivalent rank-constrained
semidefinite program (SDP) and optimally solved by SDP relaxation. Numerical
results unveil an interesting trade-off between the considered conflicting
system design objectives and reveal the benefits of multiple transmit antennas
for improving system energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05962</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05962</id><created>2015-09-19</created><authors><author><keyname>Achanta</keyname><forenames>Rakesh</forenames></author><author><keyname>Hastie</keyname><forenames>Trevor</forenames></author></authors><title>Telugu OCR Framework using Deep Learning</title><categories>stat.ML cs.AI cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the task of Optical Character Recognition(OCR) for
the Telugu script. We present an end-to-end framework that segments the text
image, classifies the characters and extracts lines using a language model. The
segmentation is based on mathematical morphology. The classification module,
which is the most challenging task of the three, is a deep convolutional neural
network. The language is modelled as a third degree markov chain at the glyph
level. Telugu script is a complex abugida and the language is agglutinative,
making the problem hard. In this paper we apply the latest advances in neural
networks to achieve acceptable error rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05967</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05967</id><created>2015-09-20</created><authors><author><keyname>Wu</keyname><forenames>Jingbo</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Distributed Filter Design for Cooperative H-Infinity-Type Estimation</title><categories>cs.SY</categories><comments>Short version of this paper is published at IEEE Multiconference on
  Systems and Control (MSC), Sydney, Australia, 2015</comments><journal-ref>IEEE Conference on Control Applications (CCA), Part of IEEE
  Multi-Conference on Systems and Control, p. 1373-1378, Sydney, Australia,
  2015</journal-ref><doi>10.1109/CCA.2015.7320803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the distributed robust filtering problem, where
estimator design is based on a set of coupled linear matrix inequalities
(LMIs). We separate the problem and show that the method of multipliers can be
applied to obtain a solution efficiently and in a decentralized fashion, i.e.
all local estimators can compute their filter gains locally and iteratively,
with communications restricted to their neighbours. The convergence properties
of the iterative algorithm are analyzed and interpreted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05969</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05969</id><created>2015-09-20</created><authors><author><keyname>Haas</keyname><forenames>Daniel</forenames></author><author><keyname>Wang</keyname><forenames>Jiannan</forenames></author><author><keyname>Wu</keyname><forenames>Eugene</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author></authors><title>CLAMShell: Speeding up Crowds for Low-latency Data Labeling</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data labeling is a necessary but often slow process that impedes the
development of interactive systems for modern data analysis. Despite rising
demand for manual data labeling, there is a surprising lack of work addressing
its high and unpredictable latency. In this paper, we introduce CLAMShell, a
system that speeds up crowds in order to achieve consistently low-latency data
labeling. We offer a taxonomy of the sources of labeling latency and study
several large crowd-sourced labeling deployments to understand their empirical
latency profiles. Driven by these insights, we comprehensively tackle each
source of latency, both by developing novel techniques such as straggler
mitigation and pool maintenance and by optimizing existing methods such as
crowd retainer pools and active learning. We evaluate CLAMShell in simulation
and on live workers on Amazon's Mechanical Turk, demonstrating that our
techniques can provide an order of magnitude speedup and variance reduction
over existing crowdsourced labeling strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05977</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05977</id><created>2015-09-20</created><authors><author><keyname>Kim</keyname><forenames>Dohan</forenames></author></authors><title>Group-theoretical vector space model</title><categories>cs.DM</categories><comments>This is an Accepted Manuscript of an article published by Taylor &amp;
  Francis Group in International Journal of Computer Mathematics on 16/09/2014,
  available online: http://dx.doi.org/10.1080/00207160.2014.958079</comments><journal-ref>Int. J. Comput. Math. 92(8): 1536-1550 (2015)</journal-ref><doi>10.1080/00207160.2014.958079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a group-theoretical vector space model (VSM) that extends
the VSM with a group action on a vector space of the VSM. We use group and its
representation theory to represent a dynamic transformation of information
objects, in which each information object is represented by a vector in a
vector space of the VSM. Several groups and their matrix representations are
employed for representing different kinds of dynamic transformations of
information objects used in the VSM. We provide concrete examples of how a
dynamic transformation of information objects is performed and discuss
algebraic properties involving certain dynamic transformations of information
objects used in the VSM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05982</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05982</id><created>2015-09-20</created><updated>2015-09-22</updated><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Denoising without access to clean data using a partitioned autoencoder</title><categories>cs.NE cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Training a denoising autoencoder neural network requires access to truly
clean data, a requirement which is often impractical. To remedy this, we
introduce a method to train an autoencoder using only noisy data, having
examples with and without the signal class of interest. The autoencoder learns
a partitioned representation of signal and noise, learning to reconstruct each
separately. We illustrate the method by denoising birdsong audio (available
abundantly in uncontrolled noisy datasets) using a convolutional autoencoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05988</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05988</id><created>2015-09-20</created><authors><author><keyname>Buls</keyname><forenames>J&#x101;nis</forenames></author><author><keyname>Gorbans</keyname><forenames>Imants</forenames></author><author><keyname>Kulesovs</keyname><forenames>Ivans</forenames></author><author><keyname>Straujums</keyname><forenames>Uldis</forenames></author></authors><title>The adaptation of Shamir's protocol for increasing the security of a
  mobile environment</title><categories>cs.CR</categories><comments>8 pages</comments><msc-class>94A60, 68P25</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the paper is to provide a solution which increases the security of
a mobile environment for both individuals and for workers in an enterprise. The
proposed solution adapts Shamir's approach for sharing a secret for encryption
key management. One part of the key is stored on a Bluetooth (or NFC) wristband
or on an enterprise server, while a mobile device is used to store all the
rest. The approach can be applied for both securing documents and voice data.
The solution is supported by a mathematical formality which is missing in the
currently known advice within cryptographic folklore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06003</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06003</id><created>2015-09-20</created><updated>2016-01-12</updated><authors><author><keyname>Liu</keyname><forenames>Fanghui</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Fu</keyname><forenames>Keren</forenames></author><author><keyname>Gu</keyname><forenames>Irene Y. H.</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author></authors><title>Robust Visual Tracking via Inverse Nonnegative Matrix Factorization</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to part-based
  representation. On one hand, not all data can be successfully identified as
  'parts' by NMF. On the other hand, inverse sparse representation could not
  fit this situation. I will give a clearer explanation from clustering instead
  of part-based representation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The establishment of robust target appearance model over time is an
overriding concern in visual tracking. In this paper, we propose an inverse
nonnegative matrix factorization (NMF) method for robust appearance modeling.
Rather than using a linear combination of nonnegative basis matrices for each
target image patch in the conventional NMF, the proposed method is a reverse
thought to conventional NMF tracker. It utilizes both the foreground and
background information, and imposes a local coordinate constraint, where the
basis matrix is sparse matrix from the linear combination of candidates with
corresponding nonnegative coefficient vectors. Inverse NMF is used as a feature
encoder, where the resulting coefficient vectors are fed into a SVM classifier
for separating the target from the background. The proposed method is tested on
several videos and compared with seven state-of-the-art methods. Our results
have provided further support to the effectiveness and robustness of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06004</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06004</id><created>2015-09-20</created><updated>2015-12-07</updated><authors><author><keyname>Olaru</keyname><forenames>Vlad</forenames></author><author><keyname>Florea</keyname><forenames>Mihai</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>A Parallel Framework for Parametric Maximum Flow Problems in Image
  Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework that supports the implementation of parallel
solutions for the widespread parametric maximum flow computational routines
used in image segmentation algorithms. The framework is based on supergraphs, a
special construction combining several image graphs into a larger one, and
works on various architectures (multi-core or GPU), either locally or remotely
in a cluster of computing nodes. The framework can also be used for performance
evaluation of parallel implementations of maximum flow algorithms. We present
the case study of a state-of-the-art image segmentation algorithm based on
graph cuts, Constrained Parametric Min-Cut (CPMC), that uses the parallel
framework to solve parametric maximum flow problems, based on a GPU
implementation of the well-known push-relabel algorithm. Our results indicate
that real-time implementations based on the proposed techniques are possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06011</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06011</id><created>2015-09-20</created><updated>2015-12-06</updated><authors><author><keyname>Zhou</keyname><forenames>Qing</forenames></author><author><keyname>Liu</keyname><forenames>Nan</forenames></author></authors><title>Energy-Efficient Data Transmission with A Non-FIFO Packet</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of energy-efficient packet transmission
with a non-FIFO Packet over a point-to-point additive white Gaussian noise
(AWGN) time-invariant channel under the feasibility constraints. More
specifically, we consider the scenario where there is a packet that has a
deadline that is earlier than that of the previously arrived packet. For this
problem, the First-In-First-Out (FIFO) transmission mode adopted in the
existing literatures is no longer optimal. We first propose a novel packet
split and reorder process which convert the inconsistency in the order of
deadlines and arrival instants of the packet sequence into a consistent one.
After the split and reorder process, the original problem considered in this
paper is transformed into the problem of finding the optimal split factor. We
propose an algorithm that finds the split factor which consists of checking
four possibilities by applying the existing optimal transmission strategy
\emph{&quot;String Tautening&quot;} for FIFO packets. In addition, we prove the
optimality of the proposed algorithm in the presence of a non-FIFO packet by
exploiting the optimality properties of the most energy efficient transmission
strategy. Based on the proposed optimal offline scheme, an efficient online
policy which assumes causal arrival information is also studied and shown to
achieve a comparable performance to the proposed optimal offline scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06016</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06016</id><created>2015-09-20</created><authors><author><keyname>Deng</keyname><forenames>Lei</forenames></author><author><keyname>Huang</keyname><forenames>Siyuan</forenames></author><author><keyname>Duan</keyname><forenames>Yueqi</forenames></author><author><keyname>Chen</keyname><forenames>Baohua</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author></authors><title>Image Set Querying Based Localization</title><categories>cs.CV</categories><comments>VCIP2015, 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional single image based localization methods usually fail to localize
a querying image when there exist large variations between the querying image
and the pre-built scene. To address this, we propose an image-set querying
based localization approach. When the localization by a single image fails to
work, the system will ask the user to capture more auxiliary images. First, a
local 3D model is established for the querying image set. Then, the pose of the
querying image set is estimated by solving a nonlinear optimization problem,
which aims to match the local 3D model against the pre-built scene. Experiments
have shown the effectiveness and feasibility of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06019</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06019</id><created>2015-09-20</created><authors><author><keyname>Kabore</keyname><forenames>Abraham</forenames></author><author><keyname>Meghdadi</keyname><forenames>Vahid</forenames></author><author><keyname>Cances</keyname><forenames>Jean-Pierre</forenames></author></authors><title>LT Codes Combined with Network Coding for Multihop Powerline Smart Grid
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel approach for combining Luby Transform (LT) codes
and Network Coding (NC) in the context of PowerLine Communications (PLC) smart
grid networks. Multihop transmissions of LT-encoded data on PLC networks are
considered and algorithms to combine data at relay nodes are proposed. Without
the need to decode and then re-encode the total received data stream, the relay
nodes can forward the received data stream while adding at the same time their
own data. Simulation results are provided confirming the good performance of
the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06024</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06024</id><created>2015-09-20</created><updated>2016-01-15</updated><authors><author><keyname>Yin</keyname><forenames>Haifan</forenames></author><author><keyname>Cottatellucci</keyname><forenames>Laura</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Ralf R.</forenames></author><author><keyname>He</keyname><forenames>Gaoning</forenames></author></authors><title>Robust Pilot Decontamination Based on Joint Angle and Power Domain
  Discrimination</title><categories>cs.IT math.IT</categories><comments>14 pages, 5 figures, accepted for publication in IEEE Transactions on
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of noise and interference corrupted channel estimation
in massive MIMO systems. Interference, which originates from pilot reuse (or
contamination), can in principle be discriminated on the basis of the
distributions of path angles and amplitudes. In this paper we propose novel
robust channel estimation algorithms exploiting path diversity in both angle
and power domains, relying on a suitable combination of the spatial filtering
and amplitude based projection. The proposed approaches are able to cope with a
wide range of system and topology scenarios, including those where, unlike in
previous works, interference channel may overlap with desired channels in terms
of multipath angles of arrival or exceed them in terms of received power. In
particular we establish analytically the conditions under which the proposed
channel estimator is fully decontaminated. Simulation results confirm the
overall system gains when using the new methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06026</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06026</id><created>2015-09-20</created><authors><author><keyname>Savage</keyname><forenames>Saiph</forenames></author><author><keyname>Monroy-Hernandez</keyname><forenames>Andres</forenames></author><author><keyname>Hollerer</keyname><forenames>Tobias</forenames></author></authors><title>Botivist: Calling Volunteers to Action Using Online Bots</title><categories>cs.SI cs.CY</categories><comments>9 pages, 3 figures, CSCW'16</comments><acm-class>H.5.2</acm-class><doi>10.1145/2818048.2819985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To help activists call new volunteers to action, we present Botivist: a
platform that uses Twitter bots to find potential volunteers and request
contributions. By leveraging different Twitter accounts, Botivist employs
different strategies to encourage participation. We explore how people respond
to bots calling them to action using a test case about corruption in Latin
America. Our results show that the majority of volunteers (&gt;80%) who responded
to Botivist's calls to action contributed relevant proposals to address the
assigned social problem. Different strategies produced differences in the
quantity and relevance of contributions. Some strategies that work well offline
and face-to-face appeared to hinder people's participation when used by an
online bot. We analyze user behavior in response to being approached by bots
with an activist purpose. We also provide strong evidence for the value of this
type of civic media, and derive design implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06029</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06029</id><created>2015-09-20</created><authors><author><keyname>Jain</keyname><forenames>Siddharth</forenames></author><author><keyname>Farnoud</keyname><forenames>Farzad</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Capacity and Expressiveness of Genomic Tandem Duplication</title><categories>cs.IT cs.DM cs.FL math.IT q-bio.GN</categories><comments>19 pages, 3 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of the human genome consists of repeated sequences. An important
type of repeated sequences common in the human genome are tandem repeats, where
identical copies appear next to each other. For example, in the sequence
$AGTC\underline{TGTG}C$, $TGTG$ is a tandem repeat, that may be generated from
$AGTCTGC$ by a tandem duplication of length $2$. In this work, we investigate
the possibility of generating a large number of sequences from a \textit{seed},
i.e.\ a small initial string, by tandem duplications of bounded length. We
study the capacity of such a system, a notion that quantifies the system's
generating power. Our results include \textit{exact capacity} values for
certain tandem duplication string systems. In addition, motivated by the role
of DNA sequences in expressing proteins via RNA and the genetic code, we define
the notion of the \textit{expressiveness} of a tandem duplication system as the
capability of expressing arbitrary substrings. We then \textit{completely}
characterize the expressiveness of tandem duplication systems for general
alphabet sizes and duplication lengths. In particular, based on a celebrated
result by Axel Thue from 1906, presenting a construction for ternary
square-free sequences, we show that for alphabets of size 4 or larger, bounded
tandem duplication systems, regardless of the seed and the bound on duplication
length, are not fully expressive, i.e. they cannot generate all strings even as
substrings of other strings. Note that the alphabet of size 4 is of particular
interest as it pertains to the genomic alphabet. Building on this result, we
also show that these systems do not have full capacity. In general, our results
illustrate that duplication lengths play a more significant role than the seed
in generating a large number of sequences for these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06032</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06032</id><created>2015-09-20</created><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author><author><keyname>Ye</keyname><forenames>Yuli</forenames></author></authors><title>Syntactic Complexity of Regular Ideals</title><categories>cs.FL</categories><comments>23 pages, 12 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:1403.2090</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state complexity of a regular language is the number of states in a
minimal deterministic finite automaton accepting the language. The syntactic
complexity of a regular language is the cardinality of its syntactic semigroup.
The syntactic complexity of a subclass of regular languages is the worst-case
syntactic complexity taken as a function of the state complexity $n$ of
languages in that class. We prove that $n^{n-1}$, $n^{n-1}+n-1$, and
$n^{n-2}+(n-2)2^{n-2}+1$ are tight upper bounds on the syntactic complexities
of right ideals and prefix-closed languages, left ideals and suffix-closed
languages, and two-sided ideals and factor-closed languages, respectively.
Moreover, we show that the transition semigroups meeting the upper bounds for
all three types of ideals are unique, and the numbers of generators (4, 5, and
6, respectively) cannot be reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06033</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06033</id><created>2015-09-20</created><authors><author><keyname>Mousavian</keyname><forenames>Arsalan</forenames></author><author><keyname>Kosecka</keyname><forenames>Jana</forenames></author></authors><title>Deep Convolutional Features for Image Based Retrieval and Scene
  Categorization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent approaches showed how the representations learned by
Convolutional Neural Networks can be repurposed for novel tasks. Most commonly
it has been shown that the activation features of the last fully connected
layers (fc7 or fc6) of the network, followed by a linear classifier outperform
the state-of-the-art on several recognition challenge datasets. Instead of
recognition, this paper focuses on the image retrieval problem and proposes a
examines alternative pooling strategies derived for CNN features. The presented
scheme uses the features maps from an earlier layer 5 of the CNN architecture,
which has been shown to preserve coarse spatial information and is semantically
meaningful. We examine several pooling strategies and demonstrate superior
performance on the image retrieval task (INRIA Holidays) at the fraction of the
computational cost, while using a relatively small memory requirements. In
addition to retrieval, we see similar efficiency gains on the SUN397 scene
categorization dataset, demonstrating wide applicability of this simple
strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from
different geographical locations in the world for image retrieval that stresses
more dramatic changes in appearance and viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06035</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06035</id><created>2015-09-20</created><authors><author><keyname>Ouahi</keyname><forenames>H.</forenames></author><author><keyname>Afdel</keyname><forenames>K.</forenames></author><author><keyname>Machkour</keyname><forenames>M.</forenames></author></authors><title>Image Retrieval Based on LBP Pyramidal Multiresolution using Reversible
  Watermarking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the medical field, images are increasingly used to facilitate diagnosis of
diseases. These images are stored in multimedia databases accompanied by doctor
s prescriptions and other information related to patients.Search for medical
images has become for clinical applications an essential tool to bring
effective aid in diagnosis. Content Based Image Retrieval (CBIR) is one of the
possible solutions to effectively manage these databases. Our contribution is
to define a relevant descriptor to retrieve images based on multiresolution
analysis of texture using Local Binary Pattern LBP. This descriptor once
calculated and information s relating to the patient; will be placed in the
image using the technique of reversible watermarking. Thereby, the image,
descriptor of its contents, the BFILE locator and patientrelated information
become a single entity, so even the administrator cannot have access to the
patient private data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06041</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06041</id><created>2015-09-20</created><authors><author><keyname>You</keyname><forenames>Quanzeng</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author></authors><title>Robust Image Sentiment Analysis Using Progressively Trained and Domain
  Transferred Deep Networks</title><categories>cs.CV cs.IR cs.LG</categories><comments>9 pages, 5 figures, AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis of online user generated content is important for many
social media analytics tasks. Researchers have largely relied on textual
sentiment analysis to develop systems to predict political elections, measure
economic indicators, and so on. Recently, social media users are increasingly
using images and videos to express their opinions and share their experiences.
Sentiment analysis of such large scale visual content can help better extract
user sentiments toward events or topics, such as those in image tweets, so that
prediction of sentiment from visual content is complementary to textual
sentiment analysis. Motivated by the needs in leveraging large scale yet noisy
training data to solve the extremely challenging problem of image sentiment
analysis, we employ Convolutional Neural Networks (CNN). We first design a
suitable CNN architecture for image sentiment analysis. We obtain half a
million training samples by using a baseline sentiment algorithm to label
Flickr images. To make use of such noisy machine labeled data, we employ a
progressive strategy to fine-tune the deep network. Furthermore, we improve the
performance on Twitter images by inducing domain transfer with a small number
of manually labeled Twitter images. We have conducted extensive experiments on
manually labeled Twitter images. The results show that the proposed CNN can
achieve better performance in image sentiment analysis than competing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06043</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06043</id><created>2015-09-20</created><authors><author><keyname>Chen</keyname><forenames>Hua</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author></authors><title>Fractional-order Generalized Principle of Self-Support (FOG PSS) in
  Control Systems Design</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews research that studies the principle of self-support (PSS)
in some control systems and proposes a fractional-order generalized PSS
framework for the first time. The existing PSS approach focuses on practical
tracking problem of integer-order systems including robotic dynamics, high
precision linear motor system, multi-axis high precision positioning system
with unmeasurable variables, imprecise sensor information, uncertain parameters
and external disturbances. More generally, by formulating the fractional PSS
concept as a new generalized framework, we will focus in the possible fields on
the fractional-order control problems such as practical tracking,
$\lambda$-tracking, etc. of robot systems, multiple mobile agents, discrete
dynamical systems, time delay systems and other uncertain nonlinear systems.
Finally, the practical tracking of a first-order uncertain model of automobile
is considered as a simple example to demonstrate the efficiency of the
fractional-order generalized principle of self-support (FOGPSS) control
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06048</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06048</id><created>2015-09-20</created><authors><author><keyname>Zehmakan</keyname><forenames>Abdolahad Noori</forenames></author><author><keyname>Eslahi</keyname><forenames>Mojtaba</forenames></author></authors><title>A linear approximation algorithm for the BPP with the best possible
  absolute approximation ratio</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bin Packing Problem is one of the most important Combinatorial
Optimization problems in optimization and has a lot of real-world applications.
Many approximation algorithms have been presented for this problem because of
its NP-hard nature. In this article also a new creative approximation algorithm
is presented for this important problem. It has been proven that the best
approximation ratio and the best time order for the Bin Packing Problem are 3/2
and O(n), respectively unless P=NP. The presented algorithm in this article has
the best possible factors, O(n) and 3/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06053</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06053</id><created>2015-09-20</created><authors><author><keyname>Escalante</keyname><forenames>Hugo Jair</forenames></author><author><keyname>Montes-y-G&#xf3;mez</keyname><forenames>Manuel</forenames></author><author><keyname>Villase&#xf1;or-Pineda</keyname><forenames>Luis</forenames></author><author><keyname>Errecalde</keyname><forenames>Marcelo Luis</forenames></author></authors><title>Early text classification: a Naive solution</title><categories>cs.CL</categories><comments>8 pages, preprint submitted to SDM'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text classification is a widely studied problem, and it can be considered
solved for some domains and under certain circumstances. There are scenarios,
however, that have received little or no attention at all, despite its
relevance and applicability. One of such scenarios is early text
classification, where one needs to know the category of a document by using
partial information only. A document is processed as a sequence of terms, and
the goal is to devise a method that can make predictions as fast as possible.
The importance of this variant of the text classification problem is evident in
domains like sexual predator detection, where one wants to identify an offender
as early as possible. This paper analyzes the suitability of the standard naive
Bayes classifier for approaching this problem. Specifically, we assess its
performance when classifying documents after seeing an increasingly number of
terms. A simple modification to the standard naive Bayes implementation allows
us to make predictions with partial information. To the best of our knowledge
naive Bayes has not been used for this purpose before. Throughout an extensive
experimental evaluation we show the effectiveness of the classifier for early
text classification. What is more, we show that this simple solution is very
competitive when compared with state of the art methodologies that are more
elaborated. We foresee our work will pave the way for the development of more
effective early text classification techniques based in the naive Bayes
formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06057</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06057</id><created>2015-09-20</created><authors><author><keyname>L&#xf3;pez-Caraballo</keyname><forenames>C. H.</forenames><affiliation>Departamento de F&#xed;sica y Astronom&#xed;a, Universidad de La Serena, Casilla 554, La Serena, Chile</affiliation></author><author><keyname>Lazz&#xfa;s</keyname><forenames>J. A.</forenames><affiliation>Departamento de F&#xed;sica y Astronom&#xed;a, Universidad de La Serena, Casilla 554, La Serena, Chile</affiliation></author><author><keyname>Salfate</keyname><forenames>I.</forenames><affiliation>Departamento de F&#xed;sica y Astronom&#xed;a, Universidad de La Serena, Casilla 554, La Serena, Chile</affiliation></author><author><keyname>Rojas</keyname><forenames>P.</forenames><affiliation>Departamento de F&#xed;sica y Astronom&#xed;a, Universidad de La Serena, Casilla 554, La Serena, Chile</affiliation></author><author><keyname>Rivera</keyname><forenames>M.</forenames><affiliation>Departamento de F&#xed;sica y Astronom&#xed;a, Universidad de La Serena, Casilla 554, La Serena, Chile</affiliation></author><author><keyname>Palma-Chilla</keyname><forenames>L.</forenames><affiliation>Departamento de F&#xed;sica y Astronom&#xed;a, Universidad de La Serena, Casilla 554, La Serena, Chile</affiliation></author></authors><title>Impact of noise on a dynamical system: prediction and uncertainties from
  a swarm-optimized neural network</title><categories>physics.comp-ph cs.NE</categories><comments>11 pages, 8 figures</comments><journal-ref>Computational Intelligence and Neuroscience. Volume 2015 (2015),
  Article ID 145874, 10 pages</journal-ref><doi>10.1155/2015/145874</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, an artificial neural network (ANN) based on particle swarm
optimization (PSO) was developed for the time series prediction. The hybrid
ANN+PSO algorithm was applied on Mackey--Glass chaotic time series in the
short-term $x(t+6)$. The performance prediction was evaluated and compared with
another studies available in the literature. Also, we presented properties of
the dynamical system via the study of chaotic behaviour obtained from the
predicted time series. Next, the hybrid ANN+PSO algorithm was complemented with
a Gaussian stochastic procedure (called {\it stochastic} hybrid ANN+PSO) in
order to obtain a new estimator of the predictions, which also allowed us to
compute uncertainties of predictions for noisy Mackey--Glass chaotic time
series. Thus, we studied the impact of noise for several cases with a white
noise level ($\sigma_{N}$) from 0.01 to 0.1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06065</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06065</id><created>2015-09-20</created><authors><author><keyname>Bhuiyan</keyname><forenames>Md Zakirul Alam</forenames></author><author><keyname>Wang</keyname><forenames>G.</forenames></author><author><keyname>Wu</keyname><forenames>J.</forenames></author><author><keyname>Cao</keyname><forenames>J.</forenames></author></authors><title>Dependable Structural Helath Monitoring Using Wireless Sensor Networks</title><categories>cs.DC cs.SY</categories><comments>46 pages, 22 figures</comments><acm-class>C.2; C.2.1; D.4.5; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an alternative to current wired-based networks, wireless sensor networks
(WSNs) are becoming an increasingly compelling platform for engineering
structural health monitoring (SHM) due to relatively low-cost, easy
installation, and so forth. However, there is still an unaddressed challenge:
the application-specific dependability in terms of sensor fault detection and
tolerance. The dependability is also affected by a reduction on the quality of
monitoring when mitigating WSN constrains (e.g., limited energy, narrow
bandwidth). We address these by designing a dependable distributed WSN
framework for SHM (called DependSHM) and then examining its ability to cope
with sensor faults and constraints. We find evidence that faulty sensors can
corrupt results of a health event (e.g., damage) in a structural system without
being detected. More specifically, we bring attention to an undiscovered yet
interesting fact, i.e., the real measured signals introduced by one or more
faulty sensors may cause an undamaged location to be identified as damaged
(false positive) or a damaged location as undamaged (false negative) diagnosis.
This can be caused by faults in sensor bonding, precision degradation,
amplification gain, bias, drift, noise, and so forth. In DependSHM, we present
a distributed automated algorithm to detect such types of faults, and we offer
an online signal reconstruction algorithm to recover from the wrong diagnosis.
Through comprehensive simulations and a WSN prototype system implementation, we
evaluate the effectiveness of DependSHM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06066</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06066</id><created>2015-09-20</created><authors><author><keyname>Najibi</keyname><forenames>Mahyar</forenames></author><author><keyname>Rastegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>On Large-Scale Retrieval: Binary or n-ary Coding?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing amount of data available in modern-day datasets makes the need to
efficiently search and retrieve information. To make large-scale search
feasible, Distance Estimation and Subset Indexing are the main approaches.
Although binary coding has been popular for implementing both techniques, n-ary
coding (known as Product Quantization) is also very effective for Distance
Estimation. However, their relative performance has not been studied for Subset
Indexing. We investigate whether binary or n-ary coding works better under
different retrieval strategies. This leads to the design of a new n-ary coding
method, &quot;Linear Subspace Quantization (LSQ)&quot; which, unlike other n-ary
encoders, can be used as a similarity-preserving embedding. Experiments on
image retrieval show that when Distance Estimation is used, n-ary LSQ
outperforms other methods. However, when Subset Indexing is applied,
interestingly, binary codings are more effective and binary LSQ achieves the
best accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06073</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06073</id><created>2015-09-20</created><authors><author><keyname>Adcock</keyname><forenames>Ben</forenames></author></authors><title>Infinite-dimensional compressed sensing and function interpolation</title><categories>math.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze a framework for function interpolation using
compressed sensing. This framework - which is based on weighted $\ell^1$
minimization - does not require a priori bounds on the expansion tail in either
its implementation or its theoretical guarantees. Moreover, in the absence of
noise it leads to genuinely interpolatory approximations. We also establish a
series of new recovery guarantees for compressed sensing with weighted $\ell^1$
minimization based on this framework. These guarantees convey three key
benefits. First, unlike some previous results, they are sharp (up to constants
and log factors) for large classes of functions regardless of the choice of
weights. Second, they allow one to determine a provably optimal weighting
strategy in the case of multivariate polynomial approximations in lower sets.
Third, they can be used to establish the benefits of weighting strategies where
the weights are chosen based on prior support information, thus providing a
theoretical basis for a number of numerical studies which have shown this to be
the case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06079</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06079</id><created>2015-09-20</created><authors><author><keyname>Swords</keyname><forenames>Sol</forenames><affiliation>Centaur Technology, Inc.</affiliation></author><author><keyname>Davis</keyname><forenames>Jared</forenames><affiliation>Centaur Technology, Inc.</affiliation></author></authors><title>Fix Your Types</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 3-16</journal-ref><doi>10.4204/EPTCS.192.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When using existing ACL2 datatype frameworks, many theorems require type
hypotheses. These hypotheses slow down the theorem prover, are tedious to
write, and are easy to forget. We describe a principled approach to types that
provides strong type safety and execution efficiency while avoiding type
hypotheses, and we present a library that automates this approach. Using this
approach, types help you catch programming errors and then get out of the way
of theorem proving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06080</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06080</id><created>2015-09-20</created><authors><author><keyname>Coglio</keyname><forenames>Alessandro</forenames></author></authors><title>Second-Order Functions and Theorems in ACL2</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 17-33</journal-ref><doi>10.4204/EPTCS.192.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SOFT ('Second-Order Functions and Theorems') is a tool to mimic second-order
functions and theorems in the first-order logic of ACL2. Second-order functions
are mimicked by first-order functions that reference explicitly designated
uninterpreted functions that mimic function variables. First-order theorems
over these second-order functions mimic second-order theorems universally
quantified over function variables. Instances of second-order functions and
theorems are systematically generated by replacing function variables with
functions. SOFT can be used to carry out program refinement inside ACL2, by
constructing a sequence of increasingly stronger second-order predicates over
one or more target functions: the sequence starts with a predicate that
specifies requirements for the target functions, and ends with a predicate that
provides executable definitions for the target functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06081</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06081</id><created>2015-09-20</created><authors><author><keyname>Cowles</keyname><forenames>John</forenames><affiliation>University of Wyoming</affiliation></author><author><keyname>Gamboa</keyname><forenames>Ruben</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>Perfect Numbers in ACL2</title><categories>cs.LO math.NT</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 53-59</journal-ref><doi>10.4204/EPTCS.192.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A perfect number is a positive integer n such that n equals the sum of all
positive integer divisors of n that are less than n. That is, although n is a
divisor of n, n is excluded from this sum. Thus 6 = 1 + 2 + 3 is perfect, but
12 &lt; 1 + 2 + 3 + 4 + 6 is not perfect. An ACL2 theory of perfect numbers is
developed and used to prove, in ACL2(r), this bit of mathematical folklore:
Even if there are infinitely many perfect numbers the series of the reciprocals
of all perfect numbers converges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06082</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06082</id><created>2015-09-20</created><authors><author><keyname>Peng</keyname><forenames>Yan</forenames><affiliation>University of British Columbia</affiliation></author><author><keyname>Greenstreet</keyname><forenames>Mark</forenames><affiliation>University of British Columbia</affiliation></author></authors><title>Extending ACL2 with SMT Solvers</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 61-77</journal-ref><doi>10.4204/EPTCS.192.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our extension of ACL2 with Satisfiability Modulo Theories (SMT)
solvers using ACL2's trusted clause processor mechanism. We are particularly
interested in the verification of physical systems including Analog and
Mixed-Signal (AMS) designs. ACL2 offers strong induction abilities for
reasoning about sequences and SMT complements deduction methods like ACL2 with
fast nonlinear arithmetic solving procedures. While SAT solvers have been
integrated into ACL2 in previous work, SMT methods raise new issues because of
their support for a broader range of domains including real numbers and
uninterpreted functions. This paper presents Smtlink, our clause processor for
integrating SMT solvers into ACL2. We describe key design and implementation
issues and describe our experience with its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06083</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06083</id><created>2015-09-20</created><authors><author><keyname>Hardin</keyname><forenames>David S.</forenames><affiliation>Rockwell Collins</affiliation></author></authors><title>Reasoning About LLVM Code Using Codewalker</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><acm-class>D.2.4;F.3.1</acm-class><journal-ref>EPTCS 192, 2015, pp. 79-92</journal-ref><doi>10.4204/EPTCS.192.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on initial experiments using J Moore's Codewalker to
reason about programs compiled to the Low-Level Virtual Machine (LLVM)
intermediate form. Previously, we reported on a translator from LLVM to the
applicative subset of Common Lisp accepted by the ACL2 theorem prover,
producing executable ACL2 formal models, and allowing us to both prove theorems
about the translated models as well as validate those models by testing. That
translator provided many of the benefits of a pure decompilation into logic
approach, but had the disadvantage of not being verified. The availability of
Codewalker as of ACL2 7.0 has provided an opportunity to revisit this idea, and
employ a more trustworthy decompilation into logic tool. Thus, we have employed
the Codewalker method to create an interpreter for a subset of the LLVM
instruction set, and have used Codewalker to analyze some simple array-based C
programs compiled to LLVM form. We discuss advantages and limitations of the
Codewalker-based method compared to the previous method, and provide some
challenge problems for future Codewalker development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06084</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06084</id><created>2015-09-20</created><authors><author><keyname>Moore</keyname><forenames>J. Strother</forenames><affiliation>Department of Computer Science, The University of Texas at Austin</affiliation></author></authors><title>Stateman: Using Metafunctions to Manage Large Terms Representing Machine
  States</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 93-109</journal-ref><doi>10.4204/EPTCS.192.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When ACL2 is used to model the operational semantics of computing machines,
machine states are typically represented by terms recording the contents of the
state components. When models are realistic and are stepped through thousands
of machine cycles, these terms can grow quite large and the cost of simplifying
them on each step grows. In this paper we describe an ACL2 book that uses HIDE
and metafunctions to facilitate the management of large terms representing such
states. Because the metafunctions for each state component updater are solely
responsible for creating state expressions (i.e., &quot;writing&quot;) and the
metafunctions for each state component accessor are solely responsible for
extracting values (i.e., &quot;reading&quot;) from such state expressions, they can
maintain their own normal form, use HIDE to prevent other parts of ACL2 from
inspecting them, and use honsing to uniquely represent state expressions. The
last feature makes it possible to memoize the metafunctions, which can improve
proof performance in some machine models. This paper describes a
general-purpose ACL2 book modeling a byte-addressed memory supporting &quot;mixed&quot;
reads and writes. By &quot;mixed&quot; we mean that reads need not correspond (in address
or number of bytes) with writes. Verified metafunctions simplify such
&quot;read-over-write&quot; expressions while hiding the potentially large state
expression. A key utility is a function that determines an upper bound on the
value of a symbolic arithmetic expression, which plays a role in resolving
writes to addresses given by symbolic expressions. We also report on a
preliminary experiment with the book, which involves the production of states
containing several million function calls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06085</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06085</id><created>2015-09-20</created><authors><author><keyname>Jain</keyname><forenames>Mitesh</forenames><affiliation>Northeastern University</affiliation></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames><affiliation>Northeastern University</affiliation></author></authors><title>Proving Skipping Refinement with ACL2s</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526. arXiv admin note: text
  overlap with arXiv:1502.02942</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 111-127</journal-ref><doi>10.4204/EPTCS.192.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe three case studies illustrating the use of ACL2s to prove the
correctness of optimized reactive systems using skipping refinement. Reasoning
about reactive systems using refinement involves defining an abstract,
high-level specification system and a concrete, low-level system. Next, one
shows that the behaviors of the implementation system are allowed by the
specification system. Skipping refinement allows us to reason about
implementation systems that can &quot;skip&quot; specification states due to
optimizations that allow the implementation system to take several
specification steps at once. Skipping refinement also allows implementation
systems to, i.e., to take several steps before completing a specification step.
We show how ACL2s can be used to prove skipping refinement theorems by modeling
and proving the correctness of three systems: a JVM-inspired stack machine, a
simple memory controller, and a scalar to vector compiler transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06086</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06086</id><created>2015-09-20</created><updated>2015-11-10</updated><authors><author><keyname>Wu</keyname><forenames>Zuxuan</forenames></author><author><keyname>Jiang</keyname><forenames>Yu-Gang</forenames></author><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Ye</keyname><forenames>Hao</forenames></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Fusing Multi-Stream Deep Networks for Video Classification</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies deep network architectures to address the problem of video
classification. A multi-stream framework is proposed to fully utilize the rich
multimodal information in videos. Specifically, we first train three
Convolutional Neural Networks to model spatial, short-term motion and audio
clues respectively. Long Short Term Memory networks are then adopted to explore
long-term temporal dynamics. With the outputs of the individual streams, we
propose a simple and effective fusion method to generate the final predictions,
where the optimal fusion weights are learned adaptively for each class, and the
learning process is regularized by automatically estimated class relationships.
Our contributions are two-fold. First, the proposed multi-stream framework is
able to exploit multimodal features that are more comprehensive than those
previously attempted. Second, we demonstrate that the adaptive fusion method
using the class relationship as a regularizer outperforms traditional
alternatives that estimate the weights in a &quot;free&quot; fashion. Our framework
produces significantly better results than the state of the arts on two popular
benchmarks, 92.2\% on UCF-101 (without using audio) and 84.9\% on Columbia
Consumer Videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06087</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06087</id><created>2015-09-20</created><authors><author><keyname>Chau</keyname><forenames>Cuong K.</forenames><affiliation>The University of Texas at Austin</affiliation></author><author><keyname>Kaufmann</keyname><forenames>Matt</forenames><affiliation>The University of Texas at Austin</affiliation></author><author><keyname>Hunt</keyname><forenames>Warren A.</forenames><suffix>Jr.</suffix><affiliation>The University of Texas at Austin</affiliation></author></authors><title>Fourier Series Formalization in ACL2(r)</title><categories>cs.LO</categories><comments>In Proceedings ACL2 2015, arXiv:1509.05526</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 192, 2015, pp. 35-51</journal-ref><doi>10.4204/EPTCS.192.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formalize some basic properties of Fourier series in the logic of ACL2(r),
which is a variant of ACL2 that supports reasoning about the real and complex
numbers by way of non-standard analysis. More specifically, we extend a
framework for formally evaluating definite integrals of real-valued, continuous
functions using the Second Fundamental Theorem of Calculus. Our extended
framework is also applied to functions containing free arguments. Using this
framework, we are able to prove the orthogonality relationships between
trigonometric functions, which are the essential properties in Fourier series
analysis. The sum rule for definite integrals of indexed sums is also
formalized by applying the extended framework along with the First Fundamental
Theorem of Calculus and the sum rule for differentiation. The Fourier
coefficient formulas of periodic functions are then formalized from the
orthogonality relations and the sum rule for integration. Consequently, the
uniqueness of Fourier sums is a straightforward corollary.
  We also present our formalization of the sum rule for definite integrals of
infinite series in ACL2(r). Part of this task is to prove the Dini Uniform
Convergence Theorem and the continuity of a limit function under certain
conditions. A key technique in our proofs of these theorems is to apply the
overspill principle from non-standard analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06088</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06088</id><created>2015-09-20</created><authors><author><keyname>Lu</keyname><forenames>Qiyi</forenames></author><author><keyname>Qiao</keyname><forenames>Xingye</forenames></author></authors><title>Significance Analysis of High-Dimensional, Low-Sample Size Partially
  Labeled Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification and clustering are both important topics in statistical
learning. A natural question herein is whether predefined classes are really
different from one another, or whether clusters are really there. Specifically,
we may be interested in knowing whether the two classes defined by some class
labels (when they are provided), or the two clusters tagged by a clustering
algorithm (where class labels are not provided), are from the same underlying
distribution. Although both are challenging questions for the high-dimensional,
low-sample size data, there has been some recent development for both. However,
when it is costly to manually place labels on observations, it is often that
only a small portion of the class labels is available. In this article, we
propose a significance analysis approach for such type of data, namely
partially labeled data. Our method makes use of the whole data and tries to
test the class difference as if all the labels were observed. Compared to a
testing method that ignores the label information, our method provides a
greater power, meanwhile, maintaining the size, illustrated by a comprehensive
simulation study. Theoretical properties of the proposed method are studied
with emphasis on the high-dimensional, low-sample size setting. Our simulated
examples help to understand when and how the information extracted from the
labeled data can be effective. A real data example further illustrates the
usefulness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06089</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06089</id><created>2015-09-20</created><updated>2016-01-13</updated><authors><author><keyname>Liu</keyname><forenames>Wanchun</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Blostein</keyname><forenames>Steven D.</forenames></author></authors><title>Energy Harvesting Wireless Sensor Networks: Delay Analysis Considering
  Energy Costs of Sensing and Transmission</title><categories>cs.IT math.IT</categories><comments>submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) provides a means of greatly enhancing the lifetime of
wireless sensor nodes. However, the randomness inherent in the EH process may
cause significant delay for performing sensing operation and transmitting the
sensed information to the sink. Unlike most existing studies on the delay
performance of EH sensor networks, where only the energy consumption of
transmission is considered, we consider the energy costs of both sensing and
transmission. Specifically, we consider an EH sensor that monitors some status
environmental property and adopts a harvest-then-use protocol to perform
sensing and transmission. To comprehensively study the delay performance, we
consider two complementary metrics and analytically derive their statistics:
(i) update age - measuring the time taken from when information is obtained by
the sensor to when the sensed information is successfully transmitted to the
sink, i.e., how timely the updated information at the sink is, and (ii) update
cycle - measuring the time duration between two consecutive successful
transmissions, i.e., how frequently the information at the sink is updated. Our
results show that the consideration of sensing energy cost leads to an
important tradeoff between the two metrics: more frequent updates result in
less timely information available at the sink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06092</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06092</id><created>2015-09-20</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>Universality for Barycentric subdivision</title><categories>math.SP cs.DM</categories><comments>17 pages, 2 figures</comments><msc-class>05C50, 57M15, 37Dxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectrum of the Laplacian of successive Barycentric subdivisions of a
graph converges exponentially fast to a limit which only depends on the clique
number of the initial graph and not on the graph itself. The proof uses an
explicit linear operator mapping the clique vector of a graph to the clique
vector of the Barycentric refinement. The eigenvectors of its transpose produce
integral geometric invariants for which Euler characteristic is one example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06094</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06094</id><created>2015-09-20</created><authors><author><keyname>Chakraborty</keyname><forenames>Nilesh</forenames></author><author><keyname>Mondal</keyname><forenames>Samrat</forenames></author></authors><title>A New Storage Optimized Honeyword Generation Approach for Enhancing
  Security and Usability</title><categories>cs.CR</categories><comments>8 pages, IEEE Conference format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverting the hash values by performing brute force computation is one of the
latest security threats on password based authentication technique. New
technologies are being developed for brute force computation and these increase
the success rate of inversion attack. Honeyword base authentication protocol
can successfully mitigate this threat by making password cracking detectable.
However, the existing schemes have several limitations like Multiple System
Vulnerability, Weak DoS Resistivity, Storage Overhead, etc. In this paper we
have proposed a new honeyword generation approach, identified as Paired
Distance Protocol (PDP) which overcomes almost all the drawbacks of previously
proposed honeyword generation approaches. The comprehensive analysis shows that
PDP not only attains a high detection rate of 97.23% but also reduces the
storage cost to a great extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06095</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06095</id><created>2015-09-20</created><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Multilayer bootstrap network for unsupervised speaker recognition</title><categories>cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply multilayer bootstrap network (MBN), a recent proposed unsupervised
learning method, to unsupervised speaker recognition. The proposed method first
extracts supervectors from an unsupervised universal background model, then
reduces the dimension of the high-dimensional supervectors by multilayer
bootstrap network, and finally conducts unsupervised speaker recognition by
clustering the low-dimensional data. The comparison results with 2 unsupervised
and 1 supervised speaker recognition techniques demonstrate the effectiveness
and robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06103</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06103</id><created>2015-09-20</created><authors><author><keyname>Wang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Wu</keyname><forenames>Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Pengyuan</forenames></author><author><keyname>Wang</keyname><forenames>Ziteng</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author><author><keyname>Li</keyname><forenames>Xu</forenames></author><author><keyname>Fu</keyname><forenames>Qiang</forenames></author><author><keyname>Yan</keyname><forenames>Yonghong</forenames></author></authors><title>Noise Robust IOA/CAS Speech Separation and Recognition System For The
  Third 'CHIME' Challenge</title><categories>cs.SD cs.CL</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the contribution to the third 'CHiME' speech separation
and recognition challenge including both front-end signal processing and
back-end speech recognition. In the front-end, Multi-channel Wiener filter
(MWF) is designed to achieve background noise reduction. Different from
traditional MWF, optimized parameter for the tradeoff between noise reduction
and target signal distortion is built according to the desired noise reduction
level. In the back-end, several techniques are taken advantage to improve the
noisy Automatic Speech Recognition (ASR) performance including Deep Neural
Network (DNN), Convolutional Neural Network (CNN) and Long short-term memory
(LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary
language model finite state transducer, and ROVER scheme. Experimental results
show the proposed system combining front-end and back-end is effective to
improve the ASR performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06109</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06109</id><created>2015-09-21</created><authors><author><keyname>Freeman</keyname><forenames>Dustin</forenames></author><author><keyname>Jota</keyname><forenames>Ricardo</forenames></author><author><keyname>Vogel</keyname><forenames>Daniel</forenames></author><author><keyname>Wigdor</keyname><forenames>Daniel</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Ravin</forenames></author></authors><title>A Dataset of Naturally Occurring, Whole-Body Background Activity to
  Reduce Gesture Conflicts</title><categories>cs.HC</categories><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real settings, natural body movements can be erroneously recognized by
whole-body input systems as explicit input actions. We call body activity not
intended as input actions &quot;background activity.&quot; We argue that understanding
background activity is crucial to the success of always-available whole-body
input in the real world. To operationalize this argument, we contribute a
reusable study methodology and software tools to generate standardized
background activity datasets composed of data from multiple Kinect cameras, a
Vicon tracker, and two high-definition video cameras. Using our methodology, we
create an example background activity dataset for a television-oriented living
room setting. We use this dataset to demonstrate how it can be used to redesign
a gestural interaction vocabulary to minimize conflicts with the real world.
The software tools and initial living room dataset are publicly available
(http://www.dgp.toronto.edu/~dustin/backgroundactivity/).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06110</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06110</id><created>2015-09-21</created><authors><author><keyname>Yi</keyname><forenames>Yang</forenames></author><author><keyname>Geng</keyname><forenames>Zhou</forenames></author><author><keyname>Jianqing</keyname><forenames>Zhang</forenames></author><author><keyname>Siyuan</keyname><forenames>Cheng</forenames></author><author><keyname>Mengyin</keyname><forenames>Fu</forenames></author></authors><title>Design, Modeling and Control of A Novel Amphibious Robot with
  Dual-swing-legs Propulsion Mechanism</title><categories>cs.RO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel amphibious robot, which adopts a dual-swing-legs
propulsion mechanism, proposing a new locomotion mode. The robot is called
FroBot, since its structure and locomotion are similar to frogs. Our
inspiration comes from the frog scooter and breaststroke. Based on its swing
leg mechanism, an unusual universal wheel structure is used to generate
propulsion on land, while a pair of flexible caudal fins functions like the
foot flippers of a frog to generate similar propulsion underwater. On the basis
of the prototype design and the dynamic model of the robot, some locomotion
control simulations and experiments were conducted for the purpose of adjusting
the parameters that affect the propulsion of the robot. Finally, a series of
underwater experiments were performed to verify the design feasibility of
FroBot and the rationality of the control algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06113</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06113</id><created>2015-09-21</created><updated>2016-03-01</updated><authors><author><keyname>Finn</keyname><forenames>Chelsea</forenames></author><author><keyname>Tan</keyname><forenames>Xin Yu</forenames></author><author><keyname>Duan</keyname><forenames>Yan</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Deep Spatial Autoencoders for Visuomotor Learning</title><categories>cs.LG cs.CV cs.RO</categories><comments>Published in the International Conference on Robotics and Automation
  (ICRA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning provides a powerful and flexible framework for
automated acquisition of robotic motion skills. However, applying reinforcement
learning requires a sufficiently detailed representation of the state,
including the configuration of task-relevant objects. We present an approach
that automates state-space construction by learning a state representation
directly from camera images. Our method uses a deep spatial autoencoder to
acquire a set of feature points that describe the environment for the current
task, such as the positions of objects, and then learns a motion skill with
these feature points using an efficient reinforcement learning method based on
local linear models. The resulting controller reacts continuously to the
learned feature points, allowing the robot to dynamically manipulate objects in
the world with closed-loop control. We demonstrate our method with a PR2 robot
on tasks that include pushing a free-standing toy block, picking up a bag of
rice using a spatula, and hanging a loop of rope on a hook at various
positions. In each task, our method automatically learns to track task-relevant
objects and manipulate their configuration with the robot's arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06114</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06114</id><created>2015-09-21</created><updated>2015-09-28</updated><authors><author><keyname>Shim</keyname><forenames>Inwook</forenames></author><author><keyname>Shin</keyname><forenames>Seunghak</forenames></author><author><keyname>Bok</keyname><forenames>Yunsu</forenames></author><author><keyname>Joo</keyname><forenames>Kyungdon</forenames></author><author><keyname>Choi</keyname><forenames>Dong-Geol</forenames></author><author><keyname>Lee</keyname><forenames>Joon-Young</forenames></author><author><keyname>Park</keyname><forenames>Jaesik</forenames></author><author><keyname>Oh</keyname><forenames>Jun-Ho</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>Vision System and Depth Processing for DRC-HUBO+</title><categories>cs.CV cs.RO</categories><comments>submitted in ICRA 2016</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a vision system and a depth processing algorithm for
DRC-HUBO+, the winner of the DRC finals 2015. Our system is designed to
reliably capture 3D information of a scene and objects robust to challenging
environment conditions. We also propose a depth-map upsampling method that
produces an outliers-free depth map by explicitly handling depth outliers. Our
system is suitable for an interactive robot with real-world that requires
accurate object detection and pose estimation. We evaluate our depth processing
algorithm over state-of-the-art algorithms on several synthetic and real-world
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06139</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06139</id><created>2015-09-21</created><authors><author><keyname>Gittenberger</keyname><forenames>Bernhard</forenames></author><author><keyname>Go&#x142;&#x119;biewski</keyname><forenames>Zbigniew</forenames></author></authors><title>On the number of lambda terms with prescribed size of their De Bruijn
  representation</title><categories>math.CO cs.DM cs.LO math.LO</categories><msc-class>03B40, 05A16, 68R05, 68N18</msc-class><acm-class>G.2.1; D.1.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  John Tromp introduced the so-called 'binary lambda calculus' as a way to
encode lambda terms in terms of binary words. Later, Grygiel and Lescanne
conjectured that the number of binary lambda terms with $m$ free indices and of
size $n$ (encoded as binary words of length $n$) is $o(n^{-3/2} \tau^{-n})$ for
$\tau \approx 1.963448\ldots$. We generalize the proposed notion of size and
show that for several classes of lambda terms, including binary lambda terms
with $m$ free indices, the number of terms of size $n$ is $\Theta(n^{-3/2}
\rho^{-n})$ with some class dependent constant $\rho$, which in particular
disproves the above mentioned conjecture. A way to obtain lower and upper
bounds for the constant near the leading term is presented and numerical
results for a few previously introduced classes of lambda terms are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06146</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06146</id><created>2015-09-21</created><authors><author><keyname>Roncoli</keyname><forenames>Claudio</forenames></author><author><keyname>Bekiaris-Liberis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Papageorgiou</keyname><forenames>Markos</forenames></author></authors><title>Highway traffic state estimation using speed measurements: case studies
  on NGSIM data and highway A20 in the Netherlands</title><categories>cs.SY</categories><comments>Submitted to the the 95th Annual Meeting of the Transportation
  Research Board (TRB) and for publication to Transportation Research Record
  (TRR) on July 28, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two case studies where a macroscopic model-based approach
for traffic state estimation, which we have recently developed, is employed and
tested. The estimation methodology is developed for a &quot;mixed&quot; traffic scenario,
where traffic is composed of both ordinary and connected vehicles. Only average
speed measurements, which may be obtained from connected vehicles reports, and
a minimum number (sufficient to guarantee observability) of spot sensor-based
total flow measurements are utilised. In the first case study, we use NGSIM
microscopic data in order to test the capability of estimating the traffic
state on the basis of aggregated information retrieved from moving vehicles and
considering various penetration rates of connected vehicles. In the second case
study, a longer highway stretch with internal congestion is utilised, in order
to test the capability of the proposed estimation scheme to produce appropriate
estimates for varying traffic conditions on long stretches. In both cases the
performances are satisfactory, and the obtained results demonstrate the
effectiveness of the methodology, both in qualitative and quantitative terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06160</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06160</id><created>2015-09-21</created><authors><author><keyname>ShenTu</keyname><forenames>QingChun</forenames></author><author><keyname>Yu</keyname><forenames>JianPing</forenames></author></authors><title>Transaction Remote Release (TRR): A New Anonymization Technology for
  Bitcoin</title><categories>cs.CR</categories><comments>18 pages, 5 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The anonymity of the Bitcoin system has some shortcomings. Analysis of
Transaction Chain (ATC) and Analysis of Bitcoin Protocol and Network (ABPN) are
two important methods of deanonymizing bitcoin transactions. Nowadays, there
are some anonymization methods to combat ATC but there has been little research
into ways to counter ABPN. This paper proposes a new anonymization technology
called Transaction Remote Release (TRR). Inspired by The Onion Router (TOR),
TRR is able to render several typical attacking methods of ABPN ineffective.
Furthermore, the performance of encryption and decryption of TRR is good and
the growth rate of the cipher is very limited. Hence, TRR is suited for
practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06161</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06161</id><created>2015-09-21</created><authors><author><keyname>Liu</keyname><forenames>Feng</forenames></author><author><keyname>Zeng</keyname><forenames>Dan</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Zhao</keyname><forenames>Qijun</forenames></author></authors><title>Cascaded Regressor based 3D Face Reconstruction from a Single Arbitrary
  View Image</title><categories>cs.CV</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art methods reconstruct three-dimensional (3D) face shapes from
a single image by fitting 3D face models to input images or by directly
learning mapping functions between two-dimensional (2D) images and 3D faces.
However, they are often difficult to use in real-world applications due to
expensive online optimization or to the requirement of frontal face images.
This paper approaches the 3D face reconstruction problem as a regression
problem rather than a model fitting problem. Given an input face image along
with some pre-defined facial landmarks on it, a series of shape adjustments to
the initial 3D face shape are computed through cascaded regressors based on the
deviations between the input landmarks and the landmarks obtained from the
reconstructed 3D faces. The cascaded regressors are offline learned from a set
of 3D faces and their corresponding 2D face images in various views. By
treating the landmarks that are invisible in large view angles as missing data,
the proposed method can handle arbitrary view face images in a unified way with
the same regressors. Experiments on the BFM and Bosphorus databases demonstrate
that the proposed method can reconstruct 3D faces from arbitrary view images
more efficiently and more accurately than existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06163</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06163</id><created>2015-09-21</created><authors><author><keyname>Trivedi</keyname><forenames>Shubhendu</forenames></author><author><keyname>Pardos</keyname><forenames>Zachary A.</forenames></author><author><keyname>Heffernan</keyname><forenames>Neil T.</forenames></author></authors><title>The Utility of Clustering in Prediction Tasks</title><categories>cs.LG</categories><comments>An experimental research report, dated 11 September 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the utility of clustering in reducing error in various prediction
tasks. Previous work has hinted at the improvement in prediction accuracy
attributed to clustering algorithms if used to pre-process the data. In this
work we more deeply investigate the direct utility of using clustering to
improve prediction accuracy and provide explanations for why this may be so. We
look at a number of datasets, run k-means at different scales and for each
scale we train predictors. This produces k sets of predictions. These
predictions are then combined by a na\&quot;ive ensemble. We observed that this use
of a predictor in conjunction with clustering improved the prediction accuracy
in most datasets. We believe this indicates the predictive utility of
exploiting structure in the data and the data compression handed over by
clustering. We also found that using this method improves upon the prediction
of even a Random Forests predictor which suggests this method is providing a
novel, and useful source of variance in the prediction process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06167</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06167</id><created>2015-09-21</created><updated>2015-09-29</updated><authors><author><keyname>Jekovec</keyname><forenames>Matev&#x17e;</forenames></author><author><keyname>Brodnik</keyname><forenames>Andrej</forenames></author></authors><title>Parallel Query in the Suffix Tree</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the query string of length $m$, we explore a parallel query in a static
suffix tree based data structure for $p \ll n$, where $p$ is the number of
processors and $n$ is the length of the text. We present three results on CREW
PRAM. The parallel query in the suffix trie requires $O(m + p)$ work, $O(m/p +
\lg p)$ time and $O(n^2)$ space in the worst case. We extend the same technique
to the suffix tree where we show it is, by design, inherently sequential in the
worst case. Finally we perform the parallel query using an interleaved approach
and achieve $O(m \lg p)$ work, $O(\frac{m}{p} \lg p)$ time and $O(n \lg p)$
space in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06180</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06180</id><created>2015-09-21</created><authors><author><keyname>Monemizadeh</keyname><forenames>Mostafa</forenames></author></authors><title>Comments on &quot;Achievable Rates in Cognitive Radio Channels&quot;</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a premier paper on the information-theoretic analysis of a two-user
cognitive interference channel(CIC), Devroye et al. presented an achievable
rate region for the two-user discrete memoryless CIC. The coding scheme
proposed by Devroye et al. is correct but unfortunately some rate-terms in the
derived achievable rate region are incorrect (in fact incomplete) because of
occurring some mistakes in decoding and analysis of error probability. We
correct and complete the wrong rate-terms and thereby show that the corrected
achievable rate region includes the rate region presented in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06184</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06184</id><created>2015-09-21</created><authors><author><keyname>Fairclough</keyname><forenames>Ruth</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>More Precise Methods for National Research Citation Impact Comparisons</title><categories>cs.DL</categories><comments>Accepted by the Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Governments sometimes need to analyse sets of research papers within a field
in order to monitor progress, assess the effect of recent policy changes, or
identify areas of excellence. They may compare the average citation impacts of
the papers by dividing them by the world average for the field and year. Since
citation data is highly skewed, however, simple averages may be too imprecise
to robustly identify differences within, rather than across, fields. In
response, this article introduces two new methods to identify national
differences in average citation impact, one based on linear modelling for
normalised data and the other using the geometric mean. Results from a sample
of 26 Scopus fields between 2009-2015 show that geometric means are the most
precise and so are recommended for smaller sample sizes, such as for individual
fields. The regression method has the advantage of distinguishing between
national contributions to internationally collaborative articles, but has
substantially wider confidence intervals than the geometric mean, undermining
its value for any except the largest sample sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06188</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06188</id><created>2015-09-21</created><authors><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>Strong converses for group testing in the finite blocklength regime</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove new strong converse results in a variety of group testing settings,
generalizing a result of Baldassini, Johnson and Aldridge. These results are
proved by two distinct approaches, corresponding to the non-adaptive and
adaptive cases. In the non-adaptive case, we mimic the hypothesis testing
argument introduced in the finite blocklength channel coding regime by
Polyanskiy, Poor and Verd\'{u}. In the adaptive case, we combine a formulation
based on directed information theory with ideas of Kemperman, Kesten and
Wolfowitz from the problem of channel coding with feedback. In both cases, we
prove results which are valid for finite sized problems, and imply capacity
results in the asymptotic regime. These results are illustrated graphically for
a range of models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06191</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06191</id><created>2015-09-21</created><authors><author><keyname>H&#x105;z&#x142;a</keyname><forenames>Jan</forenames></author><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author></authors><title>Lower Bounds on Same-Set Inner Product in Correlated Spaces</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{P}$ be a probability distribution over a finite alphabet
$\Omega^\ell$ with all $\ell$ marginals equal. Let $\underline{X}^{(1)},
\ldots, \underline{X}^{(\ell)}, \underline{X}^{(j)} = (X_1^{(j)}, \ldots,
X_n^{(j)})$ be random vectors such that for every coordinate$i \in [n]$ the
tuples $(X_i^{(1)}, \ldots, X_i^{(\ell)})$ are i.i.d. according to
$\mathcal{P}$. The question we address is: does there exist a function
$c_{\mathcal{P}}()$ independent of $n$ such that for every $f: \Omega^n \to [0,
1]$ with $\mathop{\mathrm{E}}[f(\underline{X}^{(1)})] = \mu &gt; 0$:
\begin{align*}
  \mathop{\mathrm{E}} \left[ \prod_{j=1}^\ell f(\underline{X}^{(j)}) \right]
  \ge c_{\mathcal{P}}(\mu) &gt; 0 \; ? \end{align*}
  We settle the question for $\ell = 2$ and when $\ell &gt; 2$ and $\mathcal{P}$
has bounded correlation $\rho(\mathcal{P}) &lt; 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06197</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06197</id><created>2015-09-21</created><updated>2016-01-15</updated><authors><author><keyname>Li</keyname><forenames>Ming-Xia</forenames></author><author><keyname>Xie</keyname><forenames>Wen-Jie</forenames></author><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames></author></authors><title>Communication cliques in mobile phone calling networks</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 10 figures</comments><journal-ref>J. Stat. Mech. Theor. Exp. 2015, P11007 (2015)</journal-ref><doi>10.1088/1742-5468/2015/11/P11007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People in modern societies form different social networks through numerous
means of communication. These communication networks reflect different aspects
of human's societal structure. The billing records of calls among mobile phone
users enable us to construct a directed calling network (DCN) and its
Bonferroni network (SVDCN) in which the preferential communications are
statistically validated. Here we perform a comparative investigation of the
cliques of the original DCN and its SVDCN constructed from the calling records
of more than nine million individuals in Shanghai over a period of 110 days. We
find that the statistical properties of the cliques of the two calling networks
are qualitatively similar and the clique members in the DCN and the SVDCN
exhibit idiosyncratic behaviors quantitatively. Members in large cliques are
found to be spatially close to each other. Based on the clique degree profile
of each mobile phone user, the most active users in the two calling networks
can be classified in to several groups. The users in different groups are found
to have different calling behaviors. Our study unveils interesting
communication behaviors among mobile phone users that are densely connected to
each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06207</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06207</id><created>2015-09-21</created><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Walter</keyname><forenames>Tobias</forenames></author></authors><title>Level Two of the Quantifier Alternation Hierarchy over Infinite Words</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of various decision problems for logic fragments has a long history
in computer science. This paper is on the membership problem for a fragment of
first-order logic over infinite words; the membership problem asks for a given
language whether it is definable in some fixed fragment. The alphabetic
topology was introduced as part of an effective characterization of the
fragment $\Sigma_2$ over infinite words. Here, $\Sigma_2$ consists of the
first-order formulas with two blocks of quantifiers, starting with an
existential quantifier. Its Boolean closure is $\mathbb{B}\Sigma_2$. Our first
main result is an effective characterization of the Boolean closure of the
alphabetic topology, that is, given an $\omega$-regular language $L$, it is
decidable whether $L$ is a Boolean combination of open sets in the alphabetic
topology. This is then used for transferring Place and Zeitoun's recent
decidability result for $\mathbb{B}\Sigma_2$ from finite to infinite words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06215</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06215</id><created>2015-09-21</created><updated>2015-11-09</updated><authors><author><keyname>Fleischer</keyname><forenames>Lukas</forenames></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author></authors><title>Efficient Algorithms for Morphisms over Omega-Regular Languages</title><categories>cs.FL cs.DS</categories><comments>Full version of a paper accepted to FSTTCS 2015</comments><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphisms to finite semigroups can be used for recognizing omega-regular
languages. The so-called strongly recognizing morphisms can be seen as a
deterministic computation model which provides minimal objects (known as the
syntactic morphism) and a trivial complementation procedure. We give a
quadratic-time algorithm for computing the syntactic morphism from any given
strongly recognizing morphism, thereby showing that minimization is easy as
well. In addition, we give algorithms for efficiently solving various decision
problems for weakly recognizing morphisms. Weakly recognizing morphism are
often smaller than their strongly recognizing counterparts. Finally, we
describe the language operations needed for converting formulas in monadic
second-order logic (MSO) into strongly recognizing morphisms, and we give some
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06220</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06220</id><created>2015-09-21</created><authors><author><keyname>Sergey</keyname><forenames>Ilya</forenames></author><author><keyname>Nanevski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Banerjee</keyname><forenames>Anindya</forenames></author><author><keyname>Delbianco</keyname><forenames>German Andres</forenames></author></authors><title>Hoare-style Specifications as Correctness Conditions for
  Non-linearizable Concurrent Objects</title><categories>cs.LO cs.PL</categories><comments>14 pages</comments><acm-class>D.3.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing scalable concurrent objects, which can be efficiently used on
multicore processors, often requires one to abandon standard specification
techniques, such as linearizability, in favor of more relaxed consistency
requirements. However, the variety of alternative correctness conditions makes
it difficult to choose which one to employ in a particular case, and to compose
them when using objects whose behaviors are specified via different criteria.
The lack of syntactic verification methods for most of these criteria poses
challenges in their systematic adoption and application.
  In this paper, we argue for using Hoare-style program logics as an
alternative and uniform approach for specification and compositional formal
verification of safety properties for concurrent objects and their client
programs. Through a series of case studies, we demonstrate how an existing
program logic for concurrency can be employed off-the-shelf to capture
important state and history invariants, allowing one to explicitly quantify
over interference of environment threads and provide intuitive and expressive
Hoare-style specifications for several non-linearizable concurrent objects that
were previously specified only via dedicated correctness criteria. We
illustrate the adequacy of our specifications by verifying a number of
concurrent client scenarios, that make use of the previously specified
concurrent objects, capturing the essence of such correctness conditions as
concurrency-aware linearizability, quiescent, and quantitative quiescent
consistency. All examples described in this paper are verified mechanically in
Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06231</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06231</id><created>2015-09-21</created><updated>2016-01-19</updated><authors><author><keyname>Becker</keyname><forenames>Ruben</forenames></author><author><keyname>Sagraloff</keyname><forenames>Michael</forenames></author><author><keyname>Sharma</keyname><forenames>Vikram</forenames></author><author><keyname>Yap</keyname><forenames>Chee</forenames></author></authors><title>A Simple Near-Optimal Subdivision Algorithm for Complex Root Isolation
  based on the Pellet Test and Newton Iteration</title><categories>cs.NA cs.SC math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a subdivision algorithm for isolating the complex roots of a
polynomial $F\in\mathbb{C}[x]$. Our model assumes that each coefficient of $F$
has an oracle to return an approximation to any absolute error bound. Given any
box $\mathcal{B}$ in the complex plane containing only simple roots of $F$, our
algorithm returns disjoint isolating disks for the roots in $\mathcal{B}$.
  Our complexity analysis bounds the absolute error to which the coefficients
of $F$ have to be provided, the total number of iterations, and the overall bit
complexity. This analysis shows that the complexity of our algorithm is
controlled by the geometry of the roots in a near neighborhood of the input box
$\mathcal{B}$, namely, the number of roots and their pairwise distances. The
number of subdivision steps is near-optimal. For the \emph{benchmark problem},
namely, to isolate all the roots of an integer polynomial of degree $n$ with
coefficients of bitsize less than $\tau$, our algorithm needs
$\tilde{O}(n^3+n^2\tau)$ bit operations, which is comparable to the record
bound of Pan (2002). It is the first time that such a bound has been achieved
using subdivision methods, and independent of divide-and-conquer techniques
such as Sch\&quot;onhage's splitting circle technique.
  Our algorithm uses the quadtree construction of Weyl (1924) with two key
ingredients: using Pellet's Theorem (1881) combined with Graeffe iteration, we
derive a soft test to count the number of roots in a disk. Using Newton
iteration combined with bisection, in a form inspired by the quadratic interval
method from Abbot (2006), we achieve quadratic convergence towards root
clusters. Relative to the divide-conquer algorithms, our algorithm is simple
with the potential of being practical. This paper is self-contained: we provide
pseudo-code for all subroutines used by our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06233</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06233</id><created>2015-09-21</created><authors><author><keyname>G&#xe9;cseg</keyname><forenames>Ferenc</forenames></author><author><keyname>Steinby</keyname><forenames>Magnus</forenames></author></authors><title>Tree Automata</title><categories>cs.FL</categories><msc-class>68Q70, 68Q45</msc-class><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a reissue of the book Tree Automata by F. G\'ecseg and M. Steinby
originally published in 1984 by Akad\'emiai Kiad\'o, Budapest. Some mistakes
have been corrected and a few obscure passages have been clarified. Moreover,
some more recent contributions and current lines of research are reviewed in an
appendix that also contains several new references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06242</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06242</id><created>2015-09-21</created><authors><author><keyname>Li</keyname><forenames>Fei</forenames></author><author><keyname>Wang</keyname><forenames>Qiuyan</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>A class of three-weight and five-weight linear codes</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><msc-class>94B05, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, linear codes with few weights have been widely studied, since they
have applications in data storage systems, communication systems and consumer
electronics. In this paper, we present a class of three-weight and five-weight
linear codes over Fp, where p is an odd prime and Fp denotes a finite field
with p elements. The weight distributions of the linear codes constructed in
this paper are also settled. Moreover, the linear codes illustrated in the
paper may have applications in secret sharing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06243</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06243</id><created>2015-09-21</created><authors><author><keyname>Gordo</keyname><forenames>Albert</forenames></author><author><keyname>Almazan</keyname><forenames>Jon</forenames></author><author><keyname>Murray</keyname><forenames>Naila</forenames></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames></author></authors><title>LEWIS: Latent Embeddings for Word Images and their Semantics</title><categories>cs.CV</categories><comments>Accepted for publication at the International Conference on Computer
  Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to bring semantics into the tasks of text
recognition and retrieval in natural images. Although text recognition and
retrieval have received a lot of attention in recent years, previous works have
focused on recognizing or retrieving exactly the same word used as a query,
without taking the semantics into consideration.
  In this paper, we ask the following question: \emph{can we predict semantic
concepts directly from a word image, without explicitly trying to transcribe
the word image or its characters at any point?} For this goal we propose a
convolutional neural network (CNN) with a weighted ranking loss objective that
ensures that the concepts relevant to the query image are ranked ahead of those
that are not relevant. This can also be interpreted as learning a Euclidean
space where word images and concepts are jointly embedded. This model is
learned in an end-to-end manner, from image pixels to semantic concepts, using
a dataset of synthetically generated word images and concepts mined from a
lexical database (WordNet). Our results show that, despite the complexity of
the task, word images and concepts can indeed be associated with a high degree
of accuracy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06252</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06252</id><created>2015-08-30</created><authors><author><keyname>Thron</keyname><forenames>Christopher</forenames></author><author><keyname>Aziz</keyname><forenames>Ahsan</forenames></author></authors><title>Algebraic Solution for Beamforming in Two-Way Relay Systems with Analog
  Network Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><msc-class>14Q99 (Primary), 93C40, 94A13 (Secondary)</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce the problem of optimal beamforming for two-way relay (TWR) systems
with perfect channel state infomation (CSI) that use analog network coding
(ANC) to a pair of algebraic equations in two variables that can be solved
inexpensively using numerical methods. The solution has greatly reduced
complexity compared to previous exact solutions via semidefinite programming
(SDP). Together with the linearized robust solution described in (Aziz and
Thron, 2014), it provides a high-performance, low-complexity robust beamforming
solution for 2-way relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06253</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06253</id><created>2015-09-04</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author></authors><title>Convergence of the Generalized Alternating Projection Algorithm for
  Compressive Sensing</title><categories>cs.IT math.IT stat.AP</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convergence of the generalized alternating projection (GAP) algorithm is
studied in this paper to solve the compressive sensing problem $\yv = \Amat \xv
+ \epsilonv$. By assuming that $\Amat\Amat\ts$ is invertible, we prove that GAP
converges linearly within a certain range of step-size when the sensing matrix
$\Amat$ satisfies restricted isometry property (RIP) condition of
$\delta_{2K}$, where $K$ is the sparsity of $\xv$. The theoretical analysis is
extended to the adaptively iterative thresholding (AIT) algorithms, for which
the convergence rate is also derived based on $\delta_{2K}$ of the sensing
matrix. We further prove that, under the same conditions, the convergence rate
of GAP is faster than that of AIT. Extensive simulation results confirm the
theoretical assertions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06254</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06254</id><created>2015-09-21</created><authors><author><keyname>Rodriguez-Mier</keyname><forenames>Pablo</forenames></author><author><keyname>Mucientes</keyname><forenames>Manuel</forenames></author><author><keyname>Lama</keyname><forenames>Manuel</forenames></author></authors><title>Hybrid Optimization Algorithm for Large-Scale QoS-Aware Service
  Composition</title><categories>cs.AI cs.NI</categories><comments>Preprint accepted to appear in IEEE Transactions on Services
  Computing 2015</comments><doi>10.1109/TSC.2015.2480396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a hybrid approach for automatic composition of Web
services that generates semantic input-output based compositions with optimal
end-to-end QoS, minimizing the number of services of the resulting composition.
The proposed approach has four main steps: 1) generation of the composition
graph for a request; 2) computation of the optimal composition that minimizes a
single objective QoS function; 3) multi-step optimizations to reduce the search
space by identifying equivalent and dominated services; and 4) hybrid
local-global search to extract the optimal QoS with the minimum number of
services. An extensive validation with the datasets of the Web Service
Challenge 2009-2010 and randomly generated datasets shows that: 1) the
combination of local and global optimization is a general and powerful
technique to extract optimal compositions in diverse scenarios; and 2) the
hybrid strategy performs better than the state-of-the-art, obtaining solutions
with less services and optimal QoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06257</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06257</id><created>2015-09-21</created><authors><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Communication Complexity (for Algorithm Designers)</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document collects the lecture notes from my course &quot;Communication
Complexity (for Algorithm Designers),'' taught at Stanford in the winter
quarter of 2015. The two primary goals of the course are: 1. Learn several
canonical problems that have proved the most useful for proving lower bounds
(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds
for fundamental algorithmic problems to communication complexity lower bounds.
Along the way, we'll also: 3. Get exposure to lots of cool computational models
and some famous results about them --- data streams and linear sketches,
compressive sensing, space-query time trade-offs in data structures,
sublinear-time algorithms, and the extension complexity of linear programs. 4.
Scratch the surface of techniques for proving communication complexity lower
bounds (fooling sets, corruption bounds, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06265</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06265</id><created>2015-09-21</created><authors><author><keyname>Keshishzadeh</keyname><forenames>Sarmen</forenames></author><author><keyname>Groote</keyname><forenames>Jan Friso</forenames></author></authors><title>Exact Real Arithmetic with Perturbation Analysis and Proof of
  Correctness</title><categories>cs.NA math.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this article, we consider a simple representation for real numbers and
propose top-down procedures to approximate various algebraic and transcendental
operations with arbitrary precision. Detailed algorithms and proofs are
provided to guarantee the correctness of the approximations. Moreover, we
develop and apply a perturbation analysis method to show that our approximation
procedures only recompute expressions when unavoidable.
  In the last decade, various theories have been developed and implemented to
realize real computations with arbitrary precision. Proof of correctness for
existing approaches typically consider basic algebraic operations, whereas
detailed arguments about transcendental operations are not available. Another
important observation is that in each approach some expressions might require
iterative computations to guarantee the desired precision. However, no formal
reasoning is provided to prove that such iterative calculations are essential
in the approximation procedures. In our approximations of real functions, we
explicitly relate the precision of the inputs to the guaranteed precision of
the output, provide full proofs and a precise analysis of the necessity of
iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06279</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06279</id><created>2015-09-18</created><authors><author><keyname>Baijal</keyname><forenames>Anant</forenames></author><author><keyname>Cho</keyname><forenames>Jaeyoun</forenames></author><author><keyname>Lee</keyname><forenames>Woojung</forenames></author><author><keyname>Ko</keyname><forenames>Byeong-Seob</forenames></author></authors><title>Sports highlights generation based on acoustic events detection: A rugby
  case study</title><categories>cs.SD cs.AI cs.LG</categories><comments>IEEE International Conference on Consumer Electronics (IEEE ICCE
  2015)</comments><doi>10.1109/ICCE.2015.7066303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We approach the challenging problem of generating highlights from sports
broadcasts utilizing audio information only. A language-independent,
multi-stage classification approach is employed for detection of key acoustic
events which then act as a platform for summarization of highlight scenes.
Objective results and human experience indicate that our system is highly
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06282</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06282</id><created>2015-09-21</created><updated>2015-09-24</updated><authors><author><keyname>Lahlou</keyname><forenames>Tarek A.</forenames></author><author><keyname>Baran</keyname><forenames>Thomas A.</forenames></author></authors><title>Web Services for Asynchronous, Distributed Optimization Using
  Conservative Signal Processing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a systematic approach for implementing a class of
nonlinear signal processing systems as a distributed web service, which in turn
is used to solve optimization problems in a distributed, asynchronous fashion.
As opposed to requiring a specialized server, the presented approach requires
only the use of a commodity database back-end as a central resource, as might
typically be used to serve data for websites having large numbers of concurrent
users. In this sense the presented approach leverages not only the scalability
and robustness of various database systems in sharing variables asynchronously
between workers, but also critically it leverages the tools of signal
processing in determining how the optimization algorithm might be organized and
distributed among various heterogeneous workers. A publicly-accessible
implementation is also presented, utilizing Firebase as a back-end server, and
illustrating the use of the presented approach in solving various optimization
problems commonly arising in the context of signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06290</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06290</id><created>2015-09-21</created><authors><author><keyname>Hawes</keyname><forenames>Matthew</forenames></author><author><keyname>Mihaylova</keyname><forenames>Lyudmila</forenames></author><author><keyname>Septier</keyname><forenames>Francois</forenames></author><author><keyname>Godsill</keyname><forenames>Simon</forenames></author></authors><title>A Bayesian Compressed Sensing Kalman Filter for Direction of Arrival
  Estimation</title><categories>stat.ML cs.IT math.IT</categories><comments>Fusion 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we look to address the problem of estimating the dynamic
direction of arrival (DOA) of a narrowband signal impinging on a sensor array
from the far field. The initial estimate is made using a Bayesian compressive
sensing (BCS) framework and then tracked using a Bayesian compressed sensing
Kalman filter (BCSKF). The BCS framework splits the angular region into N
potential DOAs and enforces a belief that only a few of the DOAs will have a
non-zero valued signal present. A BCSKF can then be used to track the change in
the DOA using the same framework. There can be an issue when the DOA approaches
the endfire of the array. In this angular region current methods can struggle
to accurately estimate and track changes in the DOAs. To tackle this problem,
we propose changing the traditional sparse belief associated with BCS to a
belief that the estimated signals will match the predicted signals given a
known DOA change. This is done by modelling the difference between the expected
sparse received signals and the estimated sparse received signals as a Gaussian
distribution. Example test scenarios are provided and comparisons made with the
traditional BCS based estimation method. They show that an improvement in
estimation accuracy is possible without a significant increase in computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06293</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06293</id><created>2015-09-21</created><authors><author><keyname>Foreman</keyname><forenames>Chris</forenames></author><author><keyname>Ragade</keyname><forenames>Rammohan K.</forenames></author><author><keyname>Graham</keyname><forenames>James H.</forenames></author></authors><title>An Immersive Visualization Tool for Teaching and Simulation of Smart
  Grid Technologies</title><categories>cs.HC cs.CY</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent power grid research, i.e. smart grid, involves many simultaneous
users spread over a relatively large geographical area. A tool for advancing
research and community education is presented utilizing large-scale
visualization centers, e.g. planetariums, in simulating smart grid
interactions. This approach immerses the user in virtual smart grid
visualization and allows the user, with several other users, to interact in
real time. This facilitates community education by demonstrating how the power
grid functions with smart technologies. The simulation is sophisticated enough
to also be used as a research tool for industry and higher education to test
software algorithms, deployment strategies, communications protocols, and even
new hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06305</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06305</id><created>2015-09-21</created><authors><author><keyname>Pandey</keyname><forenames>Pooja</forenames></author><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author></authors><title>On a linearization technique for solving quadratic set covering problem
  and variations</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we identify some inaccuracies in the paper by R.R. Saxena and
S.R. Arora, A Linearization technique for solving the Quadratic Set Covering
Problem, Optimization, 39 (1997) 33-42. In particular, we observe that their
algorithm need not guarantee optimality, contrary to what is claimed.
Experimental analysis with the algorithm has been carried out to evaluate its
merit as a heuristic and compared with CPLEX. The results disclose that for
some class of problems the algorithm is reasonably effective while for some
other class, it's performance is very poor. we also discussion similar
inaccuracies in another related paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06314</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06314</id><created>2015-08-24</created><authors><author><keyname>Taheri</keyname><forenames>Mina</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>Design and Analysis of Green Optical Line Terminal for TDM Passive
  Optical Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>9 Pages, 9 figures</comments><report-no>TR-ANL-2015-004</report-no><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper proposes a novel scheme which can efficiently reduce the energy
consumption of Optical Line Terminals (OLTs) in Time Division Multiplexing
(TDM) Passive Optical Networks (PONs) such as EPON and GPON. Currently, OLTs
consume a significant amount of energy in PON, which is one of the major FTTx
technologies. To be environmentally friendly, it is desirable to reduce energy
consumption of OLT as much as possible; such requirement becomes even more
urgent as OLT keeps increasing its provisioning data rate, and higher data rate
provisioning usually implies higher energy consumption. In this paper, we
propose a novel energy-efficient OLT structure which guarantees services of end
users with the smallest number of power-on OLT line cards. More specifically,
we adapt the number of power-on OLT line cards to the real-time incoming
traffic. Also, in order to avoid service disruption resulted by powering off
OLT line cards, proper optical switches are equipped in OLT to dynamically
configure the communications between OLT line cards and ONUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06321</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06321</id><created>2015-09-21</created><authors><author><keyname>Samek</keyname><forenames>Wojciech</forenames></author><author><keyname>Binder</keyname><forenames>Alexander</forenames></author><author><keyname>Montavon</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Bach</keyname><forenames>Sebastian</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author></authors><title>Evaluating the visualization of what a Deep Neural Network has learned</title><categories>cs.CV</categories><comments>13 pages, 8 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks (DNNs) have demonstrated impressive performance in
complex machine learning tasks such as image classification or speech
recognition. However, due to their multi-layer nonlinear structure, they are
not transparent, i.e., it is hard to grasp what makes them arrive at a
particular classification or recognition decision given a new unseen data
sample. Recently, several approaches have been proposed enabling one to
understand and interpret the reasoning embodied in a DNN for a single test
image. These methods quantify the ''importance'' of individual pixels wrt the
classification decision and allow a visualization in terms of a heatmap in
pixel/input space. While the usefulness of heatmaps can be judged subjectively
by a human, an objective quality measure is missing. In this paper we present a
general methodology based on region perturbation for evaluating ordered
collections of pixels such as heatmaps. We compare heatmaps computed by three
different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main
result is that the recently proposed Layer-wise Relevance Propagation (LRP)
algorithm qualitatively and quantitatively provides a better explanation of
what made a DNN arrive at a particular classification decision than the
sensitivity-based approach or the deconvolution method. We provide theoretical
arguments to explain this result and discuss its practical implications.
Finally, we investigate the use of heatmaps for unsupervised assessment of
neural network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06332</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06332</id><created>2015-09-21</created><updated>2015-09-23</updated><authors><author><keyname>Pandey</keyname><forenames>Pooja</forenames></author></authors><title>A note on linear fractional set packing problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we point out various errors in the paper by Rashmi Gupta and R.
R. Saxena, Set packing problem with linear fractional objective function,
International Journal of Mathematics and Computer Applications Research
(IJMCAR), 4 (2014) 9 - 18. We also provide some additional results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06333</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06333</id><created>2015-09-21</created><authors><author><keyname>Ma</keyname><forenames>Liang</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Leung</keyname><forenames>Kin K.</forenames></author></authors><title>Network Capability in Localizing Node Failures via End-to-end Path
  Measurements</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the capability of localizing node failures in communication
networks from binary states (normal/failed) of end-to-end paths. Given a set of
nodes of interest, uniquely localizing failures within this set requires that
different observable path states associate with different node failure events.
However, this condition is difficult to test on large networks due to the need
to enumerate all possible node failures. Our first contribution is a set of
sufficient/necessary conditions for identifying a bounded number of failures
within an arbitrary node set that can be tested in polynomial time. In addition
to network topology and locations of monitors, our conditions also incorporate
constraints imposed by the probing mechanism used. We consider three probing
mechanisms that differ according to whether measurement paths are (i)
arbitrarily controllable, (ii) controllable but cycle-free, or (iii)
uncontrollable (determined by the default routing protocol). Our second
contribution is to quantify the capability of failure localization through (1)
the maximum number of failures (anywhere in the network) such that failures
within a given node set can be uniquely localized, and (2) the largest node set
within which failures can be uniquely localized under a given bound on the
total number of failures. Both measures in (1-2) can be converted into
functions of a per-node property, which can be computed efficiently based on
the above sufficient/necessary conditions. We demonstrate how measures (1-2)
proposed for quantifying failure localization capability can be used to
evaluate the impact of various parameters, including topology, number of
monitors, and probing mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06338</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06338</id><created>2015-09-21</created><updated>2015-09-26</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Marshall</keyname><forenames>James A. R.</forenames></author><author><keyname>Tegn&#xe9;r</keyname><forenames>Jesper</forenames></author></authors><title>Approximations of Algorithmic and Structural Complexity Validate
  Cognitive-behavioural Experimental Results</title><categories>q-bio.QM cs.CC cs.IT math.IT</categories><comments>28 pages, 7 figures and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply methods for estimating the algorithmic complexity of sequences to
behavioural sequences of three landmark studies of animal behavior each of
increasing sophistication, including foraging communication by ants, flight
patterns of fruit flies, and tactical deception and competition strategies in
rodents. In each case, we demonstrate that approximations of Logical Depth and
Kolmogorv-Chaitin complexity capture and validate previously reported results,
in contrast to other measures such as Shannon Entropy, compression or ad hoc.
Our method is practically useful when dealing with short sequences, such as
those often encountered in cognitive-behavioural research. Our analysis
supports and reveals non-random behavior (LD and K complexity) in flies even in
the absence of external stimuli, and confirms the &quot;stochastic&quot; behaviour of
transgenic rats when faced that they cannot defeat by counter prediction. The
method constitutes a formal approach for testing hypotheses about the
mechanisms underlying animal behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06349</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06349</id><created>2015-09-21</created><authors><author><keyname>Peltom&#xe4;ki</keyname><forenames>Jarkko</forenames></author><author><keyname>Whiteland</keyname><forenames>Markus</forenames></author></authors><title>A square root map on Sturmian words</title><categories>cs.DM</categories><comments>Extended version. 40 pages, 5 figures, 2 tables</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a square root map on Sturmian words and study its properties.
Given a Sturmian word of slope $\alpha$, there exists exactly six minimal
squares in its language (a minimal square does not have a square as a proper
prefix). A Sturmian word $s$ of slope $\alpha$ can be written as a product of
these six minimal squares: $s = X_1^2 X_2^2 X_3^2 \cdots$. The square root of
$s$ is defined to be the word $\sqrt{s} = X_1 X_2 X_3 \cdots$. The main result
of this paper is that that $\sqrt{s}$ is also a Sturmian word of slope
$\alpha$. Further, we characterize the Sturmian fixed points of the square root
map, and we describe how to find the intercept of $\sqrt{s}$ and an occurrence
of any prefix of $\sqrt{s}$ in $s$. Related to the square root map, we
characterize the solutions of the word equation $X_1^2 X_2^2 \cdots X_n^2 =
(X_1 X_2 \cdots X_n)^2$ in the language of Sturmian words of slope $\alpha$
where the words $X_i^2$ are minimal squares of slope $\alpha$.
  We also study the square root map in a more general setting. We explicitly
construct an infinite set of non-Sturmian fixed points of the square root map.
We show that the subshifts $\Omega$ generated by these words have a curious
property: for all $w \in \Omega$ either $\sqrt{w} \in \Omega$ or $\sqrt{w}$ is
periodic. In particular, the square root map can map an aperiodic word to a
periodic word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06357</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06357</id><created>2015-09-21</created><authors><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author><author><keyname>Paulusma</keyname><forenames>Daniel</forenames></author></authors><title>Using Contracted Solution Graphs for Solving Reconfiguration Problems</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce in a general setting a dynamic programming method for solving
reconfiguration problems. Our method is based on contracted solution graphs,
which are obtained from solution graphs by performing an appropriate series of
edge contractions that decrease the graph size without losing any critical
information needed to solve the reconfiguration problem under consideration.
Our general framework captures the approach behind known reconfiguration
results of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example,
we apply the method to the following problem: given two $k$-colorings $\alpha$
and $\beta$ of a graph $G$, can $\alpha$ be modified into $\beta$ by recoloring
one vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This
problem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$.
By applying our method in combination with a thorough exploitation of the graph
structure we obtain a polynomial time algorithm for $(k-2)$-connected chordal
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06361</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06361</id><created>2015-09-21</created><updated>2015-09-21</updated><authors><author><keyname>Kulkarni</keyname><forenames>Raghav</forenames></author><author><keyname>Podder</keyname><forenames>Supartha</forenames></author></authors><title>Quantum Query Complexity of Subgraph Isomorphism and Homomorphism</title><categories>cs.CC quant-ph</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $H$ be a fixed graph on $n$ vertices. Let $f_H(G) = 1$ iff the input
graph $G$ on $n$ vertices contains $H$ as a (not necessarily induced) subgraph.
Let $\alpha_H$ denote the cardinality of a maximum independent set of $H$. In
this paper we show:
  \[Q(f_H) = \Omega\left(\sqrt{\alpha_H \cdot n}\right),\] where $Q(f_H)$
denotes the quantum query complexity of $f_H$.
  As a consequence we obtain a lower bounds for $Q(f_H)$ in terms of several
other parameters of $H$ such as the average degree, minimum vertex cover,
chromatic number, and the critical probability.
  We also use the above bound to show that $Q(f_H) = \Omega(n^{3/4})$ for any
$H$, improving on the previously best known bound of $\Omega(n^{2/3})$. Until
very recently, it was believed that the quantum query complexity is at least
square root of the randomized one. Our $\Omega(n^{3/4})$ bound for $Q(f_H)$
matches the square root of the current best known bound for the randomized
query complexity of $f_H$, which is $\Omega(n^{3/2})$ due to Gr\&quot;oger.
Interestingly, the randomized bound of $\Omega(\alpha_H \cdot n)$ for $f_H$
still remains open.
  We also study the Subgraph Homomorphism Problem, denoted by $f_{[H]}$, and
show that $Q(f_{[H]}) = \Omega(n)$.
  Finally we extend our results to the $3$-uniform hypergraphs. In particular,
we show an $\Omega(n^{4/5})$ bound for quantum query complexity of the Subgraph
Isomorphism, improving on the previously known $\Omega(n^{3/4})$ bound. For the
Subgraph Homomorphism, we obtain an $\Omega(n^{3/2})$ bound for the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06390</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06390</id><created>2015-09-21</created><authors><author><keyname>Cate</keyname><forenames>Balder ten</forenames><affiliation>University of California Santa Cruz</affiliation><affiliation>LogicBlox Inc</affiliation></author><author><keyname>Halpert</keyname><forenames>Richard L.</forenames><affiliation>University of California Santa Cruz</affiliation></author><author><keyname>Kolaitis</keyname><forenames>Phokion G.</forenames><affiliation>University of California Santa Cruz</affiliation><affiliation>IBM Research - Almaden</affiliation></author></authors><title>Exchange-Repairs: Managing Inconsistency in Data Exchange</title><categories>cs.DB</categories><comments>29 pages, 13 figures, submitted to the Journal on Data Semantics</comments><acm-class>H.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a data exchange setting with target constraints, it is often the case that
a given source instance has no solutions. In such cases, the semantics of
target queries trivialize. The aim of this paper is to introduce and explore a
new framework that gives meaningful semantics in such cases by using the notion
of exchange-repairs. Informally, an exchange-repair of a source instance is
another source instance that differs minimally from the first, but has a
solution. Exchange-repairs give rise to a natural notion of exchange-repair
certain answers (XR-certain answers) for target queries. We show that for
schema mappings specified by source-to-target GAV dependencies and target
equality-generating dependencies (egds), the XR-certain answers of a target
conjunctive query can be rewritten as the consistent answers (in the sense of
standard database repairs) of a union of conjunctive queries over the source
schema with respect to a set of egds over the source schema, making it possible
to use a consistent query-answering system to compute XR-certain answers in
data exchange. We then examine the general case of schema mappings specified by
source-to-target GLAV constraints, a weakly acyclic set of target tgds and a
set of target egds. The main result asserts that, for such settings, the
XR-certain answers of conjunctive queries can be rewritten as the certain
answers of a union of conjunctive queries with respect to the stable models of
a disjunctive logic program over a suitable expansion of the source schema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06397</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06397</id><created>2015-09-21</created><authors><author><keyname>Hallac</keyname><forenames>David</forenames></author><author><keyname>Wong</keyname><forenames>Christopher</forenames></author><author><keyname>Diamond</keyname><forenames>Steven</forenames></author><author><keyname>Sosic</keyname><forenames>Rok</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>SnapVX: A Network-Based Convex Optimization Solver</title><categories>cs.SI cs.MS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SnapVX is a high-performance Python solver for convex optimization problems
defined on networks. For these problems, it provides a fast and scalable
solution with guaranteed global convergence. SnapVX combines the capabilities
of two open source software packages: Snap.py and CVXPY. Snap.py is a large
scale graph processing library, and CVXPY provides a general modeling framework
for small-scale subproblems. SnapVX offers a customizable yet easy-to-use
interface with out-of-the-box functionality. Based on the Alternating Direction
Method of Multipliers (ADMM), it is able to efficiently store, analyze, and
solve large optimization problems from a variety of different applications.
Documentation, examples, and more can be found on the SnapVX website at
http://snap.stanford.edu/snapvx.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06418</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06418</id><created>2015-09-21</created><authors><author><keyname>Jog</keyname><forenames>Varun</forenames></author><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author></authors><title>Information-theoretic bounds for exact recovery in weighted stochastic
  block models using the Renyi divergence</title><categories>cs.IT cs.SI math.IT math.ST stat.TH</categories><comments>32 pages</comments><msc-class>62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive sharp thresholds for exact recovery of communities in a weighted
stochastic block model, where observations are collected in the form of a
weighted adjacency matrix, and the weight of each edge is generated
independently from a distribution determined by the community membership of its
endpoints. Our main result, characterizing the precise boundary between success
and failure of maximum likelihood estimation when edge weights are drawn from
discrete distributions, involves the Renyi divergence of order $\frac{1}{2}$
between the distributions of within-community and between-community edges. When
the Renyi divergence is above a certain threshold, meaning the edge
distributions are sufficiently separated, maximum likelihood succeeds with
probability tending to 1; when the Renyi divergence is below the threshold,
maximum likelihood fails with probability bounded away from 0. In the language
of graphical channels, the Renyi divergence pinpoints the information-theoretic
capacity of discrete graphical channels with binary inputs. Our results
generalize previously established thresholds derived specifically for
unweighted block models, and support an important natural intuition relating
the intrinsic hardness of community estimation to the problem of edge
classification. Along the way, we establish a general relationship between the
Renyi divergence and the probability of success of the maximum likelihood
estimator for arbitrary edge weight distributions. Finally, we discuss
consequences of our bounds for the related problems of censored block models
and submatrix localization, which may be seen as special cases of the framework
developed in our paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06420</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06420</id><created>2015-09-21</created><authors><author><keyname>Banerjee</keyname><forenames>Soumya</forenames></author><author><keyname>Hecker</keyname><forenames>Joshua</forenames></author></authors><title>A Multi-Agent System Approach to Load-Balancing and Resource Allocation
  for Distributed Computing</title><categories>cs.NE cs.DC</categories><comments>Complex Systems Digital Campus 2015 World eConference Conference on
  Complex Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research we use a decentralized computing approach to allocate and
schedule tasks on a massively distributed grid. Using emergent properties of
multi-agent systems, the algorithm dynamically creates and dissociates clusters
to serve the changing resource demands of a global task queue. The algorithm is
compared to a standard First-in First-out (FIFO) scheduling algorithm.
Experiments done on a simulator show that the distributed resource allocation
protocol (dRAP) algorithm outperforms the FIFO scheduling algorithm on time to
empty queue, average waiting time and CPU utilization. Such a decentralized
computing approach holds promise for massively distributed processing scenarios
like SETI@home and Google MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06429</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06429</id><created>2015-09-21</created><authors><author><keyname>Ramos</keyname><forenames>Arthur F.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Anjolina</forenames></author></authors><title>On Computational Paths and the Fundamental Groupoid of a Type</title><categories>cs.LO</categories><comments>15 pages, submitted to LFCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this work is to study mathematical properties of
computational paths. Originally proposed by de Queiroz \&amp; Gabbay (1994) as
`sequences of rewrites', computational paths can be seen as the grounds on
which the propositional equality between two computational objects stand. Using
computational paths and categorical semantics, we take any type $A$ of type
theory and construct a groupoid for this type. We call this groupoid the
fundamental groupoid of a type $A$, since it is similar to the one obtained
using the homotopical interpretation of the identity type. The main difference
is that instead of being just a semantical interpretation, computational paths
are entities of the syntax of type theory. We also expand our results, using
computational paths to construct fundamental groupoids of higher levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06430</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06430</id><created>2015-09-21</created><updated>2015-09-24</updated><authors><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Harris</keyname><forenames>David G.</forenames></author></authors><title>Improved bounds and parallel algorithms for the Lovasz Local Lemma</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lovasz Local Lemma (LLL) is a cornerstone principle in the probabilistic
method of combinatorics, and a seminal algorithm of Moser &amp; Tardos (2010)
provided an efficient randomized algorithm to implement it. This algorithm
could be parallelized to give an algorithm that uses polynomially many
processors and $O(\log^3 n)$ time, stemming from $O(\log n)$ adaptive
computations of a maximal independent set (MIS). Chung et. al. (2014) developed
faster local and parallel algorithms, potentially running in time $O(\log^2
n)$, but these algorithms work under significantly more stringent conditions
than the LLL.
  We give a new parallel algorithm, that works under essentially the same
conditions as the original algorithm of Moser &amp; Tardos, but uses only a single
MIS computation, thus running in $O(\log^2 n)$ time. This conceptually new
algorithm also gives a clean combinatorial description of a satisfying
assignment which might be of independent interest. Our techniques extend to the
deterministic LLL algorithm given by Chandrasekaran et al (2013) leading to an
NC-algorithm running in time $O(\log^2 n)$ as well.
  We also provide improved bounds on the run-times of the sequential and
parallel resampling-based algorithms originally developed by Moser &amp; Tardos.
These bounds extend to any problem instance in which the tighter Shearer LLL
criterion is satisfied. We also improve on the analysis of Kolipaka &amp; Szegedy
(2011) to give tighter concentration results. Interestingly, we are able to
give bounds which are independent of the (somewhat mysterious) weighting
function used in formulations of the asymmetric LLL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06434</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06434</id><created>2015-09-21</created><authors><author><keyname>Kfoury</keyname><forenames>Assaf</forenames></author><author><keyname>Mirzaei</keyname><forenames>Saber</forenames></author></authors><title>Efficient Reassembling of Graphs, Part 1: The Linear Case</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reassembling of a simple connected graph G = (V,E) is an abstraction of a
problem arising in earlier studies of network analysis. Its simplest
formulation is in two steps: (1) We cut every edge of G into two halves, thus
obtaining a collection of n=|V| one-vertex components. (2) We splice the two
halves of every edge together, not of all the edges at once, but in some
ordering \Theta of the edges that minimizes two measures that depend on the
edge-boundary degrees of assembled components.
  The edge-boundary degree of a component A (subset of V) is the number of
edges in G with one endpoint in A and one endpoint in V-A. We call the maximum
edge-boundary degree encountered during the reassembling process the
alpha-measure of the reassembling, and the sum of all edge-boundary degrees is
its beta-measure. The alpha-optimization (resp. beta-optimization) of the
reassembling of G is to determine an order \Theta for splicing the edges that
minimizes its alpha-measure (resp. beta-measure).
  There are different forms of reassembling. We consider only cases satisfying
the condition that if the an edge between disjoint components A and B is
spliced, then all the edges between A and B are spliced at the same time. In
this report, we examine the particular case of linear reassembling, which
requires that the next edge to be spliced must be adjacent to an already
spliced edge. We delay other forms of reassembling to follow-up reports.
  We prove that alpha-optimization of linear reassembling and minimum-cutwidth
linear arrangment (CutWidth) are polynomially reducible to each other, and that
beta-optimization of linear reassembling and minimum-cost linear arrangement
(MinArr) are polynomially reducible to each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06449</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06449</id><created>2015-09-21</created><authors><author><keyname>Yang</keyname><forenames>Yingxiang</forenames></author><author><keyname>Etesami</keyname><forenames>Jalal</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Efficient Neighborhood Selection for Gaussian Graphical Models</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of neighborhood selection for Gaussian
graphical models. We present two heuristic algorithms: a forward-backward
greedy algorithm for general Gaussian graphical models based on mutual
information test, and a threshold-based algorithm for walk summable Gaussian
graphical models. Both algorithms are shown to be structurally consistent, and
efficient. Numerical results show that both algorithms work very well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06451</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06451</id><created>2015-09-21</created><authors><author><keyname>Yang</keyname><forenames>Shuo</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>From Facial Parts Responses to Face Detection: A Deep Learning Approach</title><categories>cs.CV</categories><comments>To appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel deep convolutional network (DCN) that
achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically,
our method achieves a high recall rate of 90.99% on the challenging FDDB
benchmark, outperforming the state-of-the-art method by a large margin of
2.91%. Importantly, we consider finding faces from a new perspective through
scoring facial parts responses by their spatial structure and arrangement. The
scoring mechanism is carefully formulated considering challenging cases where
faces are only partially visible. This consideration allows our network to
detect faces under severe occlusion and unconstrained pose variation, which are
the main difficulty and bottleneck of most existing face detection approaches.
We show that despite the use of DCN, our network can achieve practical runtime
speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06457</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06457</id><created>2015-09-22</created><authors><author><keyname>Sarswat</keyname><forenames>Suneel</forenames></author><author><keyname>Abraham</keyname><forenames>Kandathil Mathew</forenames></author><author><keyname>Ghosh</keyname><forenames>Subir Kumar</forenames></author></authors><title>Identifying collusion groups using spectral clustering</title><categories>q-fin.TR cs.CE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an illiquid stock, traders can collude and place orders on a predetermined
price and quantity at a fixed schedule. This is usually done to manipulate the
price of the stock or to create artificial liquidity in the stock, which may
mislead genuine investors. Here, the problem is to identify such group of
colluding traders. We modeled the problem instance as a graph, where each
trader corresponds to a vertex of the graph and trade corresponds to edges of
the graph. Further, we assign weights on edges depending on total volume, total
number of trades, maximum change in the price and commonality between two
vertices. Spectral clustering algorithms are used on the constructed graph to
identify colluding group(s). We have compared our results with simulated data
to show the effectiveness of spectral clustering to detecting colluding groups.
Moreover, we also have used parameters of real data to test the effectiveness
of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06458</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06458</id><created>2015-09-22</created><authors><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Tian</keyname><forenames>Minghao</forenames></author></authors><title>Harmonic Extension</title><categories>cs.LG math.NA</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the harmonic extension problem, which is widely
used in many applications of machine learning. We find that the transitional
method of graph Laplacian fails to produce a good approximation of the
classical harmonic function. To tackle this problem, we propose a new method
called the point integral method (PIM). We consider the harmonic extension
problem from the point of view of solving PDEs on manifolds. The basic idea of
the PIM method is to approximate the harmonicity using an integral equation,
which is easy to be discretized from points. Based on the integral equation, we
explain the reason why the transitional graph Laplacian may fail to approximate
the harmonicity in the classical sense and propose a different approach which
we call the volume constraint method (VCM). Theoretically, both the PIM and the
VCM computes a harmonic function with convergence guarantees, and practically,
they are both simple, which amount to solve a linear system. One important
application of the harmonic extension in machine learning is semi-supervised
learning. We run a popular semi-supervised learning algorithm by Zhu et al.
over a couple of well-known datasets and compare the performance of the
aforementioned approaches. Our experiments show the PIM performs the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06461</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06461</id><created>2015-09-22</created><updated>2015-12-08</updated><authors><author><keyname>van Hasselt</keyname><forenames>Hado</forenames></author><author><keyname>Guez</keyname><forenames>Arthur</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Deep Reinforcement Learning with Double Q-learning</title><categories>cs.LG</categories><comments>AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popular Q-learning algorithm is known to overestimate action values under
certain conditions. It was not previously known whether, in practice, such
overestimations are common, whether they harm performance, and whether they can
generally be prevented. In this paper, we answer all these questions
affirmatively. In particular, we first show that the recent DQN algorithm,
which combines Q-learning with a deep neural network, suffers from substantial
overestimations in some games in the Atari 2600 domain. We then show that the
idea behind the Double Q-learning algorithm, which was introduced in a tabular
setting, can be generalized to work with large-scale function approximation. We
propose a specific adaptation to the DQN algorithm and show that the resulting
algorithm not only reduces the observed overestimations, as hypothesized, but
that this also leads to much better performance on several games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06464</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06464</id><created>2015-09-22</created><authors><author><keyname>Gibb</keyname><forenames>David</forenames></author><author><keyname>Kapron</keyname><forenames>Bruce</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Thorn</keyname><forenames>Nolan</forenames></author></authors><title>Dynamic graph connectivity with improved worst case update time and
  sublinear space</title><categories>cs.DS</categories><msc-class>68W20</msc-class><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers fully dynamic graph algorithms with both faster worst
case update time and sublinear space. The fully dynamic graph connectivity
problem is the following: given a graph on a fixed set of n nodes, process an
online sequence of edge insertions, edge deletions, and queries of the form &quot;Is
there a path between nodes a and b?&quot; In 2013, the first data structure was
presented with worst case time per operation which was polylogarithmic in n. In
this paper, we shave off a factor of log n from that time, to O(log^4 n) per
update. For sequences which are polynomial in length, our algorithm answers
queries in O(log n/\log\log n) time correctly with high probability and using
O(n \log^2 n) words (of size log n). This matches the amount of space used by
the most space-efficient graph connectivity streaming algorithm. We also show
that 2-edge connectivity can be maintained using O(n log^2 n) words with an
amortized update time of O(log^6 n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06470</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06470</id><created>2015-09-22</created><authors><author><keyname>Liao</keyname><forenames>Yiyi</forenames></author><author><keyname>Kodagoda</keyname><forenames>Sarath</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author></authors><title>Understand Scene Categories by Objects: A Semantic Regularized Scene
  Classifier Using Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene classification is a fundamental perception task for environmental
understanding in today's robotics. In this paper, we have attempted to exploit
the use of popular machine learning technique of deep learning to enhance scene
understanding, particularly in robotics applications. As scene images have
larger diversity than the iconic object images, it is more challenging for deep
learning methods to automatically learn features from scene images with less
samples. Inspired by human scene understanding based on object knowledge, we
address the problem of scene classification by encouraging deep neural networks
to incorporate object-level information. This is implemented with a
regularization of semantic segmentation. With only 5 thousand training images,
as opposed to 2.5 million images, we show the proposed deep architecture
achieves superior scene classification results to the state-of-the-art on a
publicly available SUN RGB-D dataset. In addition, performance of semantic
segmentation, the regularizer, also reaches a new record with refinement
derived from predicted scene labels. Finally, we apply our SUN RGB-D dataset
trained model to a mobile robot captured images to classify scenes in our
university demonstrating the generalization ability of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06484</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06484</id><created>2015-09-22</created><updated>2015-12-18</updated><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Detectability of the spectral method for sparse graph partitioning</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>6 pages, 2 figures</comments><journal-ref>Europhys. Lett. 112, 40007 (2015)</journal-ref><doi>10.1209/0295-5075/112/40007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that modularity maximization with the resolution parameter offers a
unifying framework of graph partitioning. In this framework, we demonstrate
that the spectral method exhibits universal detectability, irrespective of the
value of the resolution parameter, as long as the graph is partitioned.
Furthermore, we show that when the resolution parameter is sufficiently small,
a first-order phase transition occurs, resulting in the graph being
unpartitioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06501</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06501</id><created>2015-09-22</created><authors><author><keyname>Avellaneda</keyname><forenames>Florent</forenames><affiliation>LAAS-VERTICS, ACADIE</affiliation></author><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS-VERTICS</affiliation></author><author><keyname>Raclet</keyname><forenames>Jean-Baptiste</forenames><affiliation>ACADIE</affiliation></author></authors><title>On the Complexity of Flanked Finite State Automata</title><categories>cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new subclass of nondeterministic finite automata for
prefix-closed languages called Flanked Finite Automata (FFA). We show that this
class enjoys good complexity properties while preserving the succinctness of
nondeterministic automata. In particular, we show that the universality problem
for FFA is in linear time and that language inclusion can be checked in
polynomial time. A useful application of FFA is to provide an efficient way to
compute the quotient and inclusion of regular languages without the need to use
the powerset construction. These operations are the building blocks of several
verification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06503</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06503</id><created>2015-09-22</created><updated>2016-03-06</updated><authors><author><keyname>de Amorim</keyname><forenames>Arthur Azevedo</forenames></author><author><keyname>Collins</keyname><forenames>Nathan</forenames></author><author><keyname>DeHon</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Demange</keyname><forenames>Delphine</forenames></author><author><keyname>Hritcu</keyname><forenames>Catalin</forenames></author><author><keyname>Pichardie</keyname><forenames>David</forenames></author><author><keyname>Pierce</keyname><forenames>Benjamin C.</forenames></author><author><keyname>Pollack</keyname><forenames>Randy</forenames></author><author><keyname>Tolmach</keyname><forenames>Andrew</forenames></author></authors><title>A Verified Information-Flow Architecture</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SAFE is a clean-slate design for a highly secure computer system, with
pervasive mechanisms for tracking and limiting information flows. At the lowest
level, the SAFE hardware supports fine-grained programmable tags, with
efficient and flexible propagation and combination of tags as instructions are
executed. The operating system virtualizes these generic facilities to present
an information-flow abstract machine that allows user programs to label
sensitive data with rich confidentiality policies. We present a formal,
machine-checked model of the key hardware and software mechanisms used to
dynamically control information flow in SAFE and an end-to-end proof of
noninterference for this model.
  We use a refinement proof methodology to propagate the noninterference
property of the abstract machine down to the concrete machine level. We use an
intermediate layer in the refinement chain that factors out the details of the
information-flow control policy and devise a code generator for compiling such
information-flow policies into low-level monitor code. Finally, we verify the
correctness of this generator using a dedicated Hoare logic that abstracts from
low-level machine instructions into a reusable set of verified structured code
generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06506</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06506</id><created>2015-09-22</created><authors><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS-VERTICS</affiliation></author><author><keyname>Berthomieu</keyname><forenames>Bernard</forenames><affiliation>LAAS-VERTICS</affiliation></author><author><keyname>Botlan</keyname><forenames>Didier Le</forenames><affiliation>LAAS-VERTICS</affiliation></author></authors><title>Latency Analysis of an Aerial Video Tracking System Using Fiacre and
  Tina</title><categories>cs.PF</categories><comments>This work was presented at the FMTV verification challenge of WATERS
  2015 (https://waters2015.inria.fr/challenge/) , the 6th International
  Workshop on Analysis Tools and Methodologies for Embedded and Real-time
  Systems. Submissions were not published in the workshop proceedings</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our experience with modeling a video tracking system used to
detect and follow moving targets from an airplane. We provide a formal model
that takes into account the real-time properties of the system and use it to
compute the worst and best-case end to end latency. We also compute a lower
bound on the delay between the loss of two frames. Our approach is based on the
model-checking tool Tina, that provides state-space generation and
model-checking algorithms for an extension of Time Petri Nets with data and
priorities. We propose several models divided in two main categories: first
Time Petri Net models, which are used to study the behavior of the system in
the most basic way; then models based on the Fiacre specification language,
where we take benefit of richer data structures to directly model the buffering
of video information and the use of an unbounded number of frame identifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06507</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06507</id><created>2015-09-22</created><authors><author><keyname>Zilio</keyname><forenames>Silvano Dal</forenames><affiliation>LAAS-VERTICS</affiliation></author><author><keyname>Berthomieu</keyname><forenames>Bernard</forenames><affiliation>LAAS-VERTICS</affiliation></author></authors><title>Automating the Verification of Realtime Observers using Probes and the
  Modal mu-calculus</title><categories>cs.LO</categories><comments>This work was presented at TTCS 2015, the First IFIP International
  Conference on Topics in Theoretical Computer Science, August 26-28,2015.
  Institute for Research in Fundamental Sciences (IPM), Tehran, Iran</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical method for model-checking timed properties-such as those
expressed using timed extensions of temporal logic-is to rely on the use of
observers. In this context, a major problem is to prove the correctness of
observers. Essentially, this boils down to proving that: (1) every trace that
contradicts a property can be detected by the observer; but also that (2) the
observer is innocuous, meaning that it cannot interfere with the system under
observation. In this paper, we describe a method for automatically testing the
correctness of realtime observers. This method is obtained by automating an
approach often referred to as visual verification, in which the correctness of
a system is performed by inspecting a graphical representation of its state
space. Our approach has been implemented on the tool Tina, a model-checking
toolbox for Time Petri Net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06508</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06508</id><created>2015-09-22</created><authors><author><keyname>Liang</keyname><forenames>Liu</forenames></author><author><keyname>Rui</keyname><forenames>Zhang</forenames></author></authors><title>Downlink SINR Balancing in C-RAN under Limited Fronthaul Capacity</title><categories>cs.IT math.IT</categories><comments>a technical report of a paper submitted to ICASSP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud radio access network (C-RAN) with centralized baseband processing is
envisioned as a promising candidate for the next-generation wireless
communication network. However, the joint processing gain of C-RAN is
fundamentally constrained by the finite-capacity fronthaul links between the
central unit (CU) where joint processing is implemented and distributed access
points known as remote radio heads (RRHs). In this paper, we consider the
downlink communication in a C-RAN with multi-antenna RRHs and single-antenna
users, and investigate the joint RRH beamforming and user-RRH association
problem to maximize the minimum signal-to-interference-plus-noise ratio (SINR)
of all users subject to each RRH's individual fronthaul capacity constraint.
The formulated problem is in general NP-hard due to the fronthaul capacity
constraints and thus is difficult to be solved optimally. In this paper, we
propose a new iterative method for this problem which decouples the design of
beamforming and user association, where the number of users served by each RRH
is iteratively reduced until the obtained beamforming and user association
solution satisfies the fronthaul capacity constraints of all RRHs. A monotonic
convergence is proved for the proposed algorithm, and it is shown by simulation
that the algorithm achieves significant performance improvement over other
heuristic solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06517</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06517</id><created>2015-09-22</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Three Practical Aspects of Massive MIMO: Intermittent User Activity,
  Pilot Synchronism, and Asymmetric Deployment</title><categories>cs.IT math.IT</categories><comments>Published at the IEEE 2nd International Workshop on Massive MIMO:
  From theory to practice, IEEE GLOBECOM 2015, 6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers three aspects of Massive MIMO (multiple-input
multiple-output) communication networks that have received little attention in
previous works, but are important to understand when designing and implementing
this promising wireless technology. First, we analyze how bursty data traffic
behaviors affect the system. Using a probabilistic model for intermittent user
activity, we show that the spectral efficiency (SE) scales gracefully with
reduced user activity. Then, we make an analytic comparison between synchronous
and asynchronous pilot signaling, and prove that the choice between these has
no impact on the SE. Finally, we provide an analytical and numerical study of
the SE achieved with random network deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06524</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06524</id><created>2015-09-22</created><authors><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author><author><keyname>D'Acquisto</keyname><forenames>Giuseppe</forenames></author></authors><title>Option contracts for a privacy-aware market</title><categories>cs.CR q-fin.GN</categories><acm-class>H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppliers (including companies and individual prosumers) may wish to protect
their private information when selling items they have in stock. A market is
envisaged where private information can be protected through the use of
differential privacy and option contracts, while privacy-aware suppliers
deliver their stock at a reduced price. In such a marketplace a broker acts as
intermediary between privacy-aware suppliers and end customers, providing the
extra items possibly needed to fully meet the customers' demand, while end
customers book the items they need through an option contract. All stakeholders
may benefit from such a marketplace. A formula is provided for the option
price, and a budget equation is set for the mechanism to be profitable for the
broker/producer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06530</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06530</id><created>2015-09-22</created><authors><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Pentland</keyname><forenames>Alex Sandy</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>Physical Proximity and Spreading in Dynamic Social Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most infectious diseases spread on a dynamic network of human interactions.
Recent studies of social dynamics have provided evidence that spreading
patterns may depend strongly on detailed micro-dynamics of the social system.
We have recorded every single interaction within a large population, mapping
out---for the first time at scale---the complete proximity network for a
densely-connected system. Here we show the striking impact of
interaction-distance on the network structure and dynamics of spreading
processes. We create networks supporting close (intimate network, up to ~1m)
and longer distance (ambient network, up to ~10m) modes of transmission. The
intimate network is fragmented, with weak ties bridging densely-connected
neighborhoods, whereas the ambient network supports spread driven by random
contacts between strangers. While there is no trivial mapping from the
micro-dynamics of proximity networks to empirical epidemics, these networks
provide a telling approximation of droplet and airborne modes of pathogen
spreading. The dramatic difference in outbreak dynamics has implications for
public policy and methodology of data collection and modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06533</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06533</id><created>2015-09-22</created><authors><author><keyname>Elena</keyname><forenames>Gianfranco</forenames></author><author><keyname>Johnson</keyname><forenames>Christopher W.</forenames></author></authors><title>Factors influencing risk acceptance of Cloud Computing services in the
  UK Government</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing services are increasingly being made available by the UK
Government through the Government digital marketplace to reduce costs and
improve IT efficiency; however, little is known about factors influencing the
decision-making process to adopt cloud services within the UK Government. This
research aims to develop a theoretical framework to understand risk perception
and risk acceptance of cloud computing services. Study subjects (N=24) were
recruited from three UK Government organizations to attend a semi- structured
interview. Transcribed texts were analyzed using the approach termed
interpretive phenomenological analysis. Results showed that the most important
factors influencing risk acceptance of cloud services are: perceived benefits
and opportunities, organization risk culture and perceived risks. We focused on
perceived risks and perceived security concerns. Based on these results, we
suggest a number of implications for risk managers, policy makers and cloud
service providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06535</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06535</id><created>2015-09-22</created><authors><author><keyname>Probst</keyname><forenames>Malte</forenames></author><author><keyname>Rothlauf</keyname><forenames>Franz</forenames></author></authors><title>Deep Boltzmann Machines in Estimation of Distribution Algorithms for
  Combinatorial Optimization</title><categories>cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1503.01954</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Deep Boltzmann Machines
(DBMs) are generative neural networks with these desired properties. We
integrate a DBM into an EDA and evaluate the performance of this system in
solving combinatorial optimization problems with a single objective. We compare
the results to the Bayesian Optimization Algorithm. The performance of DBM-EDA
was superior to BOA for difficult additively decomposable functions, i.e.,
concatenated deceptive traps of higher order. For most other benchmark
problems, DBM-EDA cannot clearly outperform BOA, or other neural network-based
EDAs. In particular, it often yields optimal solutions for a subset of the runs
(with fewer evaluations than BOA), but is unable to provide reliable
convergence to the global optimum competitively. At the same time, the model
building process is computationally more expensive than that of other EDAs
using probabilistic models from the neural network family, such as DAE-EDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06536</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06536</id><created>2015-09-22</created><authors><author><keyname>Elena</keyname><forenames>Gianfranco</forenames></author><author><keyname>Johnson</keyname><forenames>Christopher W.</forenames></author></authors><title>Laypeople and Experts risk perception of Cloud Computing Services</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is revolutionising the way software services are procured and
used by Government organizations and SMEs. Quantitative risk assessment of
Cloud services is complex and undermined by specific security concerns
regarding data confidentiality, integrity and availability. This study explores
how the gap between the quantitative risk assessment and the perception of the
risk can produce a bias in the decision-making process about Cloud computing
adoption. The risk perception of experts in Cloud computing (N=37) and
laypeople (N=81) about ten Cloud computing services was investigated using the
psychometric paradigm. Results suggest that the risk perception of Cloud
services can be represented by two components, called dread risk and unknown
risk, which may explain up to 46% of the variance. Other factors influencing
the risk perception were perceived benefits, trust in regulatory authorities
and technology attitude. This study suggests some implications that could
support Government and non-Government organizations in their strategies for
Cloud computing adoption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06542</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06542</id><created>2015-09-22</created><authors><author><keyname>Roy</keyname><forenames>Spandan</forenames></author><author><keyname>Kar</keyname><forenames>Indra Narayan</forenames></author></authors><title>Adaptive-Robust Control of a Class of Nonlinear Systems with Unknown
  Input Delay</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper, the tracking control problem of a class of uncertain
Euler-Lagrange systems subjected to unknown input delay and bounded
disturbances is addressed. To this front, a novel delay dependent control law,
referred as Adaptive Robust Outer Loop Control (AROLC) is proposed. Compared to
the conventional predictor based approaches, the proposed controller is capable
of negotiating any input delay, within a stipulated range, without knowing the
delay or its variation. The maximum allowable input delay is computed through
Razumikhin-type stability analysis. AROLC also provides robustness against the
disturbances due to input delay, parametric variations and unmodelled dynamics
through switching control law. The novel adaptive law allows the switching gain
to modify itself online in accordance with the tracking error without any
prerequisite of the uncertainties. The uncertain system, employing AROLC, is
shown to be Uniformly Ultimately Bounded (UUB). As a proof of concept,
experimentation is carried out on a nonholonomic wheeled mobile robot with
various time varying as well as fixed input delay, and better tracking accuracy
of the proposed controller is noted compared to predictor based methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06544</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06544</id><created>2015-09-22</created><updated>2016-02-12</updated><authors><author><keyname>Leduc</keyname><forenames>Matt V.</forenames></author><author><keyname>Jackson</keyname><forenames>Matthew O.</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author></authors><title>Pricing and Referrals in Diffusion on Networks</title><categories>physics.soc-ph cs.GT cs.SI</categories><comments>34 pages, 3 tables, 5 figures</comments><msc-class>91D30, 05C82, 91A43</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a new product or technology is introduced, potential consumers can learn
its quality by trying the product, at a risk, or by letting others try it and
free-riding on the information that they generate. We propose a dynamic game to
study the adoption of technologies of uncertain value, when agents are
connected by a network and a monopolist seller chooses a policy to maximize
profits. Consumers with low degree (few friends) have incentives to adopt
early, while consumers with high degree have incentives to free ride. The
seller can induce high-degree consumers to adopt early by offering referral
incentives - rewards to early adopters whose friends buy in the second period.
Referral incentives thus lead to a `double-threshold strategy' by which low and
high-degree agents adopt the product early while middle-degree agents wait. We
show that referral incentives are optimal on certain networks while
inter-temporal price discrimination (i.e., a first-period price discount) is
optimal on others, and discuss welfare implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06553</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06553</id><created>2015-09-22</created><updated>2015-10-15</updated><authors><author><keyname>Rao</keyname><forenames>Vidyadhar</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Jawahar</keyname><forenames>C. V</forenames></author></authors><title>Diverse Yet Efficient Retrieval using Hash Functions</title><categories>cs.IR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical retrieval systems have three requirements: a) Accurate retrieval
i.e., the method should have high precision, b) Diverse retrieval, i.e., the
obtained set of points should be diverse, c) Retrieval time should be small.
However, most of the existing methods address only one or two of the above
mentioned requirements. In this work, we present a method based on randomized
locality sensitive hashing which tries to address all of the above requirements
simultaneously. While earlier hashing approaches considered approximate
retrieval to be acceptable only for the sake of efficiency, we argue that one
can further exploit approximate retrieval to provide impressive trade-offs
between accuracy and diversity. We extend our method to the problem of
multi-label prediction, where the goal is to output a diverse and accurate set
of labels for a given document in real-time. Moreover, we introduce a new
notion to simultaneously evaluate a method's performance for both the precision
and diversity measures. Finally, we present empirical results on several
different retrieval tasks and show that our method retrieves diverse and
accurate images/labels while ensuring $100x$-speed-up over the existing diverse
retrieval approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06557</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06557</id><created>2015-09-22</created><authors><author><keyname>Gao</keyname><forenames>Yongqiang</forenames></author><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Local Multi-Grouped Binary Descriptor with Ring-based Pooling
  Configuration and Optimization</title><categories>cs.CV</categories><comments>To appear in IEEE Trans. on Image Processing, 2015</comments><doi>10.1109/TIP.2015.2469093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local binary descriptors are attracting increasingly attention due to their
great advantages in computational speed, which are able to achieve real-time
performance in numerous image/vision applications. Various methods have been
proposed to learn data-dependent binary descriptors. However, most existing
binary descriptors aim overly at computational simplicity at the expense of
significant information loss which causes ambiguity in similarity measure using
Hamming distance. In this paper, by considering multiple features might share
complementary information, we present a novel local binary descriptor, referred
as Ring-based Multi-Grouped Descriptor (RMGD), to successfully bridge the
performance gap between current binary and floated-point descriptors. Our
contributions are two-fold. Firstly, we introduce a new pooling configuration
based on spatial ring-region sampling, allowing for involving binary tests on
the full set of pairwise regions with different shapes, scales and distances.
This leads to a more meaningful description than existing methods which
normally apply a limited set of pooling configurations. Then, an extended
Adaboost is proposed for efficient bit selection by emphasizing high variance
and low correlation, achieving a highly compact representation. Secondly, the
RMGD is computed from multiple image properties where binary strings are
extracted. We cast multi-grouped features integration as rankSVM or sparse SVM
learning problem, so that different features can compensate strongly for each
other, which is the key to discriminativeness and robustness. The performance
of RMGD was evaluated on a number of publicly available benchmarks, where the
RMGD outperforms the state-of-the-art binary descriptors significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06559</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06559</id><created>2015-09-22</created><authors><author><keyname>Iglesias</keyname><forenames>Jos&#xe9; A.</forenames></author><author><keyname>Rumpf</keyname><forenames>Martin</forenames></author><author><keyname>Scherzer</keyname><forenames>Otmar</forenames></author></authors><title>Shape Aware Matching of Implicit Surfaces based on Thin Shell Energies</title><categories>math.OC cs.CG</categories><comments>25 pages, 10 figures</comments><msc-class>65D18, 49J45, 74K25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A shape sensitive, variational approach for the matching of surfaces
considered as thin elastic shells is investigated. The elasticity functional to
be minimized takes into account two different types of nonlinear energies: a
membrane energy measuring the rate of tangential distortion when deforming the
reference shell into the template shell, and a bending energy measuring the
bending under the deformation in terms of the change of the shape operators
from the undeformed into the deformed configuration. The variational method
applies to surfaces described as level sets. It is mathematically well-posed
and an existence proof of an optimal matching deformation is given. The
variational model is implemented using a finite element discretization combined
with a narrow band approach on an efficient hierarchical grid structure. For
the optimization a regularized nonlinear conjugate gradient scheme and a
cascadic multilevel strategy are used. The features of the proposed approach
are studied for synthetic test cases and a collection of geometry processing
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06562</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06562</id><created>2015-09-22</created><authors><author><keyname>Melo</keyname><forenames>Rafael A.</forenames></author><author><keyname>Samer</keyname><forenames>Phillippe</forenames></author><author><keyname>Urrutia</keyname><forenames>Sebasti&#xe1;n</forenames></author></authors><title>An effective decomposition approach and heuristics to generate spanning
  trees with a small number of branch vertices</title><categories>cs.DM math.OC</categories><comments>16 pages, 2 figures, submitted for publication</comments><msc-class>90C27, 90C90, 05C05, 05C85</msc-class><acm-class>G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$, the minimum branch vertices problem (MBV) consists
in finding a spanning tree $T=(V,E')$ of $G$ minimizing the number of vertices
with degree greater than two. We consider a simple combinatorial lower bound
for the problem from which we propose a decomposition approach to break down
the problem into several smaller subproblems that are more tractable
computationally and then recombine the obtained solutions to generate a
solution to the original larger problem. We also propose effective constructive
heuristics to the problem which take into consideration the problem structure
in order to obtain good feasible solutions. Computational results show that our
decomposition approach is very fast and can drastically reduce the size of the
subproblems that have to be solved, allowing a branch-and-cut algorithm to
perform much better than when used over the full original problem. The results
also show that the proposed constructive heuristics are very rapid and generate
very good quality solutions, outperforming other heuristics available in the
literature in several situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06569</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06569</id><created>2015-09-22</created><updated>2015-12-20</updated><authors><author><keyname>Novikov</keyname><forenames>Alexander</forenames></author><author><keyname>Podoprikhin</keyname><forenames>Dmitry</forenames></author><author><keyname>Osokin</keyname><forenames>Anton</forenames></author><author><keyname>Vetrov</keyname><forenames>Dmitry</forenames></author></authors><title>Tensorizing Neural Networks</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks currently demonstrate state-of-the-art performance in
several domains. At the same time, models of this class are very demanding in
terms of computational resources. In particular, a large amount of memory is
required by commonly used fully-connected layers, making it hard to use the
models on low-end devices and stopping the further increase of the model size.
In this paper we convert the dense weight matrices of the fully-connected
layers to the Tensor Train format such that the number of parameters is reduced
by a huge factor and at the same time the expressive power of the layer is
preserved. In particular, for the Very Deep VGG networks we report the
compression factor of the dense weight matrix of a fully-connected layer up to
200000 times leading to the compression factor of the whole network up to 7
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06575</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06575</id><created>2015-09-22</created><authors><author><keyname>Sagarra</keyname><forenames>Oleguer</forenames></author><author><keyname>Guti&#xe9;rrez-Roig</keyname><forenames>Mario</forenames></author><author><keyname>Bonhoure</keyname><forenames>Isabelle</forenames></author><author><keyname>Perell&#xf3;</keyname><forenames>Josep</forenames></author></authors><title>Citizen Science practices for Computational Social Sciences research:
  The conceptualization of Pop-Up Experiments</title><categories>cs.CY physics.data-an physics.soc-ph</categories><comments>19 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under the name of Citizen Science, many innovative practices in which
volunteers partner with scientist to pose and answer real-world questions are
quickly growing worldwide. Citizen Science can furnish ready made solutions
with the active role of citizens. However, this framework is still far from
being well stablished to become a standard tool for Computational Social
Sciences research. We present our experience in bridging Computational Social
Sciences with Citizen Science philosophy, which in our case has taken the form
of what we call Pop-Up Experiments: Non-permanent, highly participatory
collective experiments which blend features developed by Big Data methodologies
and Behavioural Experiments protocols with ideals of Citizen Science. The main
issues to take into account whenever planning experiments of this type are
classified and discused grouped in three categories: public engagement, light
infrastructure and knowledge return to citizens. We explain the solutions
implemented providing practical examples grounded in our own experience in
urban contexts (Barcelona, Spain). We hope that this work serves as guideline
to groups willing to adopt and expand such \emph{in-vivo} practices and opens
the debate about the possibilities (but also the limitations) that the Citizen
Science framework can offer to study social phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06576</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06576</id><created>2015-09-22</created><authors><author><keyname>Boxer</keyname><forenames>Laurence</forenames></author><author><keyname>Staecker</keyname><forenames>P. Christopher</forenames></author></authors><title>Homotopy relations for digital images</title><categories>math.GN cs.CV</categories><msc-class>55P10, 55Q05</msc-class><acm-class>I.4.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce three generalizations of homotopy equivalence in digital images,
to allow us to express whether a finite and an infinite digital image are
similar with respect to homotopy.
  We show that these three generalizations are not equivalent to ordinary
homotopy equivalence, and give several examples. We show that, like homotopy
equivalence, our three generalizations imply isomorphism of fundamental groups,
and are
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06580</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06580</id><created>2015-09-22</created><updated>2016-01-22</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Hofer-Temmel</keyname><forenames>Christoph</forenames></author></authors><title>Graph-Based Lossless Markov Lumpings</title><categories>cs.IT math.IT math.PR</categories><comments>6 pages</comments><msc-class>60J10, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use results from zero-error information theory to determine the set of
non-injective functions through which a Markov chain can be projected without
losing information. These lumping functions can be found by clique partitioning
of a graph related to the Markov chain. Lossless lumping is made possible by
exploiting the (sufficiently sparse) temporal structure of the Markov chain.
Eliminating edges in the transition graph of the Markov chain trades the
required output alphabet size versus information loss, for which we present
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06584</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06584</id><created>2015-09-22</created><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Diao</keyname><forenames>Rui</forenames></author><author><keyname>Ye</keyname><forenames>Feng</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>An Efficient Inexact Newton-CG Algorithm for the Smallest Enclosing Ball
  Problem of Large Dimensions</title><categories>math.OC cs.CG</categories><comments>25 pages, 1 figure, Journal of the Operations Research Society of
  China, 2015</comments><doi>10.1007/s40305-015-0097-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of computing the smallest enclosing
ball (SEB) of a set of $m$ balls in $\mathbb{R}^n,$ where the product $mn$ is
large. We first approximate the non-differentiable SEB problem by its
log-exponential aggregation function and then propose a computationally
efficient inexact Newton-CG algorithm for the smoothing approximation problem
by exploiting its special (approximate) sparsity structure. The key difference
between the proposed inexact Newton-CG algorithm and the classical Newton-CG
algorithm is that the gradient and the Hessian-vector product are inexactly
computed in the proposed algorithm, which makes it capable of solving the
large-scale SEB problem. We give an adaptive criterion of inexactly computing
the gradient/Hessian and establish global convergence of the proposed
algorithm. We illustrate the efficiency of the proposed algorithm by using the
classical Newton-CG algorithm as well as the algorithm from [Zhou. {et al.} in
Comput. Opt. \&amp; Appl. 30, 147--160 (2005)] as benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06585</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06585</id><created>2015-09-22</created><authors><author><keyname>Cossu</keyname><forenames>Jean-Val&#xe8;re</forenames><affiliation>LIA</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIA</affiliation></author><author><keyname>Dugu&#xe9;</keyname><forenames>Nicolas</forenames></author></authors><title>A Review of Features for the Discrimination of Twitter Users:
  Application to the Prediction of Offline Influence</title><categories>cs.CL cs.SI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many works related to Twitter aim at characterizing its users in some way:
role on the service (spammers, bots, organizations, etc.), nature of the user
(socio-professional category, age, etc.), topics of interest, and others.
However, for a given user classification problem, it is very difficult to
select a set of appropriate features, because the many features described in
the literature are very heterogeneous, with name overlaps and collisions, and
numerous very close variants. In this article, we review a wide range of such
features. In order to present a clear state-of-the-art description, we unify
their names, definitions and relationships, and we propose a new, neutral,
typology. We then illustrate the interest of our review by applying a selection
of these features to the offline influence detection problem. This task
consists in identifying users which are influential in real-life, based on
their Twitter account and related data. We show that most features deemed
efficient to predict online influence, such as the numbers of retweets and
followers, are not relevant to this problem. However, We propose several
content-based approaches to label Twitter users as Influencers or not. We also
rank them according to a predicted influence level. Our proposals are evaluated
over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06589</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06589</id><created>2015-09-22</created><authors><author><keyname>Martino</keyname><forenames>Giovanni Da San</forenames></author><author><keyname>Navarin</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author></authors><title>Graph Kernels exploiting Weisfeiler-Lehman Graph Isomorphism Test
  Extensions</title><categories>cs.LG cs.AI</categories><journal-ref>Neural Information Processing, Volume 8835 of the series Lecture
  Notes in Computer Science pp 93-100, 2014 Springer International Publishing</journal-ref><doi>10.1007/978-3-319-12640-1_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel graph kernel framework inspired the by the
Weisfeiler-Lehman (WL) isomorphism tests. Any WL test comprises a relabelling
phase of the nodes based on test-specific information extracted from the graph,
for example the set of neighbours of a node. We defined a novel relabelling and
derived two kernels of the framework from it. The novel kernels are very fast
to compute and achieve state-of-the-art results on five real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06594</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06594</id><created>2015-09-22</created><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author><author><keyname>Lewis</keyname><forenames>Martha</forenames></author></authors><title>A Compositional Explanation of the Pet Fish Phenomenon</title><categories>cs.AI cs.CL math.CT</categories><comments>QI2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The `pet fish' phenomenon is often cited as a paradigm example of the
`non-compositionality' of human concept use. We show here how this phenomenon
is naturally accommodated within a compositional distributional model of
meaning. This model describes the meaning of a composite concept by accounting
for interaction between its constituents via their grammatical roles. We give
two illustrative examples to show how the qualitative phenomena are exhibited.
We go on to apply the model to experimental data, and finally discuss
extensions of the formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06602</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06602</id><created>2015-09-22</created><updated>2015-09-23</updated><authors><author><keyname>Yang</keyname><forenames>Gang</forenames></author><author><keyname>Moghadam</keyname><forenames>Mohammad R. Vedady</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Magnetic Beamforming for Wireless Power Transfer</title><categories>cs.IT math.IT</categories><comments>13 Pages, 3 figures, submitted to IEEE ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonant coupling (MRC) is an efficient method for realizing the
near-field wireless power transfer (WPT). The use of multiple transmitters
(TXs) each with one coil can be applied to enhance the WPT performance by
focusing the magnetic fields from all TX coils in a beam toward the receiver
(RX) coil, termed as &quot;magnetic beamforming&quot;. In this paper, we study the
optimal magnetic beamforming for an MRC-WPT system with multiple TXs and a
single RX. We formulate an optimization problem to jointly design the currents
flowing through different TXs so as to minimize the total power drawn from
their voltage sources, subject to the minimum power required by the RX load as
well as the TXs' constraints on the peak voltage and current. For the special
case of identical TX resistances and neglecting all TXs' constraints on the
peak voltage and current, we show that the optimal current magnitude of each TX
is proportional to the mutual inductance between its TX coil and the RX coil.
In general, the problem is a non-convex quadratically constrained quadratic
programming (QCQP) problem, which is reformulated as a semidefinite programming
(SDP) problem. We show that its semidefinite relaxation (SDR) is tight.
Numerical results show that magnetic beamforming significantly enhances the
deliverable power as well as the WPT efficiency over the uncoordinated
benchmark scheme of equal current allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06611</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06611</id><created>2015-09-22</created><updated>2016-02-23</updated><authors><author><keyname>Zhou</keyname><forenames>Bo</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Stochastic Content-Centric Multicast Scheduling for Cache-Enabled
  Heterogeneous Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Trans. on Wireless Communications. Conference
  version appears in ACM CoNEXT 2015 Workshop on Content Caching and Delivery
  in Wireless Networks (CCDWN)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching at small base stations (SBSs) has demonstrated significant benefits
in alleviating the backhaul requirement in heterogeneous cellular networks
(HetNets). While many existing works focus on what contents to cache at each
SBS, an equally important problem is what contents to deliver to satisfy
dynamic user demands given the cache status. In this paper, we study the
optimal content delivery strategy in cache-enabled HetNets by taking into
account the inherent multicast nature of wireless medium. We establish a
content-centric request queue model and then formulate a stochastic multicast
scheduling problem to jointly minimize the average network delay and power
costs. This problem is an infinite horizon average cost Markov decision process
(MDP). By using relative value iteration and the special properties of the
request queue dynamics, we characterize some properties of the value function
of the MDP. Based on these properties, we show that the optimal multicast
scheduling policy is of the threshold type. Then, we propose a structure-aware
optimal algorithm to obtain the optimal policy. To further reduce the
complexity, we propose a low complexity suboptimal policy, which possesses
similar structural properties to the optimal policy, and develop a low
complexity algorithm to obtain this policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06633</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06633</id><created>2015-09-22</created><authors><author><keyname>Singh</keyname><forenames>Abhinav</forenames></author><author><keyname>Humphries</keyname><forenames>Mark</forenames></author></authors><title>Finding communities in sparse networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 4 figures</comments><journal-ref>Scientific Reports 5, Article number: 8828 (2015)</journal-ref><doi>10.1038/srep08828</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spectral algorithms based on matrix representations of networks are often
used to detect communities but classic spectral methods based on the adjacency
matrix and its variants fail to detect communities in sparse networks. New
spectral methods based on non-backtracking random walks have recently been
introduced that successfully detect communities in many sparse networks.
However, the spectrum of non-backtracking random walks ignores hanging trees in
networks that can contain information about the community structure of
networks. We introduce the reluctant backtracking operators that explicitly
account for hanging trees as they admit a small probability of returning to the
immediately previous node unlike the non-backtracking operators that forbid an
immediate return. We show that the reluctant backtracking operators can detect
communities in certain sparse networks where the non-backtracking operators
cannot while performing comparably on benchmark stochastic block model networks
and real world networks. We also show that the spectrum of the reluctant
backtracking operator approximately optimises the standard modularity function
similar to the flow matrix. Interestingly, for this family of non- and
reluctant-backtracking operators the main determinant of performance on
real-world networks is whether or not they are normalised to conserve
probability at each node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06655</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06655</id><created>2015-09-22</created><updated>2015-10-22</updated><authors><author><keyname>Hu</keyname><forenames>Shih-Wei</forenames></author><author><keyname>Lin</keyname><forenames>Gang-Xuan</forenames></author><author><keyname>Hsieh</keyname><forenames>Sung-Hsien</forenames></author><author><keyname>Liang</keyname><forenames>Wei-Jie</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author></authors><title>Performance Analysis of Joint-Sparse Recovery from Multiple Measurements
  and Prior Information via Convex Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of compressed sensing with multiple measurement
vectors associated with prior information in order to better reconstruct an
original sparse matrix signal. $\ell_{2,1}-\ell_{2,1}$ minimization is used to
emphasize co-sparsity property and similarity between matrix signal and prior
information. We then derive the necessary and sufficient condition of
successfully reconstructing the original signal and establish the lower and
upper bounds of required measurements such that the condition holds from the
perspective of conic geometry. Our bounds further indicates what prior
information is helpful to improve the the performance of CS. Experimental
results validates the effectiveness of all our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06658</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06658</id><created>2015-09-22</created><updated>2015-10-08</updated><authors><author><keyname>Prabhu</keyname><forenames>Nikita</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Attribute-Graph: A Graph based approach to Image Ranking</title><categories>cs.CV</categories><comments>In IEEE International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel image representation, termed Attribute-Graph, to rank
images by their semantic similarity to a given query image. An Attribute-Graph
is an undirected fully connected graph, incorporating both local and global
image characteristics. The graph nodes characterise objects as well as the
overall scene context using mid-level semantic attributes, while the edges
capture the object topology. We demonstrate the effectiveness of
Attribute-Graphs by applying them to the problem of image ranking. We benchmark
the performance of our algorithm on the 'rPascal' and 'rImageNet' datasets,
which we have created in order to evaluate the ranking performance on complex
queries containing multiple objects. Our experimental evaluation shows that
modelling images as Attribute-Graphs results in improved ranking performance
over existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06659</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06659</id><created>2015-09-22</created><updated>2016-01-29</updated><authors><author><keyname>Nagpal</keyname><forenames>Chirag</forenames></author><author><keyname>Miller</keyname><forenames>Kyle</forenames></author><author><keyname>Boecking</keyname><forenames>Benedikt</forenames></author><author><keyname>Dubrawski</keyname><forenames>Artur</forenames></author></authors><title>An Entity Resolution approach to isolate instances of Human Trafficking
  online</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human trafficking is a challenging law enforcement problem, with a large
amount of such activity taking place online. Given the large, heterogeneous and
noisy structure of this data, the problem becomes even more challenging. In
this paper we propose and entity resolution pipeline using a notion of proxy
labels, in order to extract clusters from this data with prior history of human
trafficking activity. We apply this pipeline to 5M records from backpage.com
and report on the performance of this approach, challenges in terms of
scalability, and some significant domain specific characteristics of our
resolved entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06664</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06664</id><created>2015-09-22</created><updated>2016-03-01</updated><authors><author><keyname>Rockt&#xe4;schel</keyname><forenames>Tim</forenames></author><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Ko&#x10d;isk&#xfd;</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Reasoning about Entailment with Neural Attention</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><comments>ICLR 2016 camera-ready, 9 pages, 10 figures (incl. subfigures)</comments><msc-class>68T50</msc-class><acm-class>I.2.6; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While most approaches to automatically recognizing entailment relations have
used classifiers employing hand engineered features derived from complex
natural language processing pipelines, in practice their performance has been
only slightly better than bag-of-word pair classifiers using only lexical
similarity. The only attempt so far to build an end-to-end differentiable
neural network for entailment failed to outperform such a simple similarity
classifier. In this paper, we propose a neural model that reads two sentences
to determine entailment using long short-term memory units. We extend this
model with a word-by-word neural attention mechanism that encourages reasoning
over entailments of pairs of words and phrases. Furthermore, we present a
qualitative analysis of attention weights produced by this model, demonstrating
such reasoning capabilities. On a large entailment dataset this model
outperforms the previous best neural model and a classifier with engineered
features by a substantial margin. It is the first generic end-to-end
differentiable system that achieves state-of-the-art accuracy on a textual
entailment dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06689</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06689</id><created>2015-09-22</created><authors><author><keyname>Maaloul</keyname><forenames>Rihab</forenames><affiliation>ATNET</affiliation></author><author><keyname>Fourati</keyname><forenames>Lamia Chaari</forenames><affiliation>ATNET</affiliation></author><author><keyname>Cousin</keyname><forenames>Bernard</forenames><affiliation>ATNET</affiliation></author></authors><title>Energy-Aware Forwarding Strategy for Metro Ethernet Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy optimization has become a crucial issue in the realm of ICT. This
paper addresses the problem of energy consumption in a Metro Ethernet network.
Ethernet technology deployments have been increasing tremendously because of
their simplicity and low cost. However, much research remains to be conducted
to address energy efficiency in Ethernet networks. In this paper, we propose a
novel Energy Aware Forwarding Strategy for Metro Ethernet networks based on a
modification of the Internet Energy Aware Routing (EAR) algorithm. Our
contribution identifies the set of links to turn off and maintain links with
minimum energy impact on the active state. Our proposed algorithm could be a
superior choice for use in networks with low saturation, as it involves a
tradeoff between maintaining good network performance and minimizing the active
links in the network. Performance evaluation shows that, at medium load
traffic, energy savings of 60% can be achieved. At high loads, energy savings
of 40% can be achieved without affecting the network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06690</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06690</id><created>2015-09-22</created><authors><author><keyname>Kogan</keyname><forenames>Irina A.</forenames></author><author><keyname>Olver</keyname><forenames>Peter J.</forenames></author></authors><title>Invariants of objects and their images under surjective maps</title><categories>math.DG cs.CV</categories><comments>This paper includes corrections and additions to the published
  version</comments><msc-class>53A55, 14L24, 14H50, 68T45</msc-class><acm-class>I.2.10</acm-class><journal-ref>Lobachevskii J. Math. 36 (2015), 260--285</journal-ref><doi>10.1134/S1995080215030063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the relationships between the differential invariants of objects
and of their images under a surjective map. We analyze both the case when the
underlying transformation group is projectable and hence induces an action on
the image, and the case when only a proper subgroup of the entire group acts
projectably. In the former case, we establish a constructible isomorphism
between the algebra of differential invariants of the images and the algebra of
fiber-wise constant (gauge) differential invariants of the objects. In the
latter case, we describe residual effects of the full transformation group on
the image invariants. Our motivation comes from the problem of reconstruction
of an object from multiple-view images, with central and parallel projections
of curves from three-dimensional space to the two-dimensional plane serving as
our main examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06693</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06693</id><created>2015-09-22</created><authors><author><keyname>Vikas</keyname><forenames>Vishesh</forenames></author><author><keyname>Cohen</keyname><forenames>Eliad</forenames></author><author><keyname>Grassi</keyname><forenames>Rob</forenames></author><author><keyname>Sozer</keyname><forenames>Canberk</forenames></author><author><keyname>Trimmer</keyname><forenames>Barry</forenames></author></authors><title>Design and locomotion control of soft robot using friction manipulation
  and motor-tendon actuation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots built from soft materials can alter their shape and size in a
particular profile. This shape-changing ability could be extremely helpful for
rescue robots and those operating in unknown terrains and environments. In
changing shape, soft materials also store and release elastic energy, a feature
that can be exploited for effective robot movement. However, design and control
of these moving soft robots is non-trivial. The research presents design
methodology for a 3D-printed, motor-tendon actuated soft robot capable of
locomotion. The modular design of the robot facilitates rapid fabrication,
deployment and repair. In addition to shape change, the robot uses friction
manipulation mechanisms to effect locomotion. The motor-tendon actuators
comprise of nylon tendons embedded inside the soft body structure along a given
path with one end fixed on the body and the other attached to a motor. These
actuators directly control the deformation of the soft body which influences
the robot locomotion behavior. Static stress analysis is used as a tool for
designing the shape of the paths of these tendons embedded inside the body. The
research also presents a novel model-free data-driven control approach for soft
robots which interact with the environment at discrete contact points. This
approach involves discretization of factors dominating robot-environment
interactions as states, learning of the results as robot transitions between
these robot states and then optimization of a cost function to find desired
control sequence. The clever discretization allows the framework to exist in
robot's task space, hence, facilitating calculation of control sequences
without modeling the actuator, body material or details of the friction
mechanisms. The flexibility of the framework is experimentally explored by
applying it to robots with different friction mechanisms and different shapes
of tendon paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06695</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06695</id><created>2015-09-22</created><authors><author><keyname>Williams</keyname><forenames>Nadya</forenames></author><author><keyname>Stewart</keyname><forenames>Aimee</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Phil</forenames></author></authors><title>Virtualizing Lifemapper Software Infrastructure for Biodiversity
  Expedition</title><categories>cs.SE</categories><comments>5 pages, 5 figures, PRAGMA Workshop on International Clouds for Data
  Science (PRAGMA-ICDS 2015)</comments><acm-class>D.2.11</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the activities of the Pacific Rim Applications and Grid Middleware
Assembly (PRAGMA) is fostering Virtual Biodiversity Expeditions (VBEs) by
bringing domain scientists and cyber infrastructure specialists together as a
team. Over the past few years PRAGMA members have been collaborating on
virtualizing the Lifemapper software. Virtualization and cloud computing have
introduced great flexibility and efficiency into IT projects. Virtualization
provides application scalability, maximizes resources utilization, and creates
a more efficient, agile, and automated infrastructure. However, there are
downsides to the complexity inherent in these environments, including the need
for special techniques to deploy cluster hosts, dependence on virtual
environments, and challenging application installation, management, and
configuration. In this paper, we report on progress of the Lifemapper
virtualization framework focused on a reproducible and highly configurable
infrastructure capable of fast deployment. A key contribution of this work is
describing the practical experience in taking a complex, clustered,
domain-specific, data analysis and simulation system and making it available to
operate on a variety of system configurations. Uses of this portability range
from whole cluster replication to teaching and experimentation on a single
laptop. System virtualization is used to practically define and make portable
the full application stack, including all of its complex set of supporting
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06712</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06712</id><created>2015-09-22</created><authors><author><keyname>Cambazard</keyname><forenames>Hadrien</forenames></author><author><keyname>Mehta</keyname><forenames>Deepak</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author><author><keyname>Simonis</keyname><forenames>Helmut</forenames></author></authors><title>Bin Packing with Linear Usage Costs</title><categories>cs.DS</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bin packing is a well studied problem involved in many applications. The
classical bin packing problem is about minimising the number of bins and
ignores how the bins are utilised. We focus in this paper, on a variant of bin
packing that is at the heart of efficient management of data centres. In this
context, servers can be viewed as bins and virtual machines as items. The
efficient management of a data-centre involves minimising energy costs while
ensuring service quality. The assignment of virtual machines on servers and how
these servers are utilised has a huge impact on the energy consumption. We
focus on a bin packing problem where linear costs are associated to the use of
bins to model the energy consumption. We study lower bounds based on Linear
Programming and extend the bin packing global constraint with cost information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06720</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06720</id><created>2015-09-22</created><authors><author><keyname>Yasin</keyname><forenames>Hashim</forenames></author><author><keyname>Iqbal</keyname><forenames>Umar</forenames></author><author><keyname>Kr&#xfc;ger</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Weber</keyname><forenames>Andreas</forenames></author><author><keyname>Gall</keyname><forenames>Juergen</forenames></author></authors><title>3D Pose Estimation from a Single Monocular Image</title><categories>cs.CV</categories><report-no>CG-2015-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major challenge for 3D pose estimation from a single RGB image is the
acquisition of sufficient training data. In particular, collecting large
amounts of training data that contain unconstrained images and are annotated
with accurate 3D poses is infeasible. We therefore propose to use two
independent training sources. The first source consists of images with
annotated 2D poses and the second source consists of accurate 3D motion capture
data. To integrate both sources, we propose a dual-source approach that
combines 2D pose estimation with efficient and robust 3D pose retrieval. In our
experiments, we show that our approach achieves state-of-the-art results when
both sources are from the same dataset, but it also achieves competitive
results when the motion capture data is taken from a different dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06729</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06729</id><created>2015-09-22</created><authors><author><keyname>Tsakiris</keyname><forenames>Manolis C.</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Algebraic Clustering of Affine Subspaces</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering is an important problem in machine learning with many
applications in computer vision and pattern recognition. Prior work studied
this problem in the case of linear subspaces using algebraic, iterative,
statistical, low-rank and sparse representation techniques. While such methods
have been applied to the case of affine subspaces, the theory of affine
subspace clustering remains largely unstudied. This paper rigorously extends
the theory of algebraic subspace clustering to the case of affine subspaces,
which naturally arise in practical applications. We model a union of affine
subspaces as an affine variety whose irreducible components give the individual
subspaces. This leads to a decomposition method which, through a process called
projectivization, is shown to be equivalent to the classical algebraic method
for linear subspaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06731</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06731</id><created>2015-09-22</created><authors><author><keyname>Yakovenko</keyname><forenames>Nikolai</forenames></author><author><keyname>Cao</keyname><forenames>Liangliang</forenames></author><author><keyname>Raffel</keyname><forenames>Colin</forenames></author><author><keyname>Fan</keyname><forenames>James</forenames></author></authors><title>Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in
  Poker Games</title><categories>cs.AI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poker is a family of card games that includes many variations. We hypothesize
that most poker games can be solved as a pattern matching problem, and propose
creating a strong poker playing system based on a unified poker representation.
Our poker player learns through iterative self-play, and improves its
understanding of the game by training on the results of its previous actions
without sophisticated domain knowledge. We evaluate our system on three poker
games: single player video poker, two-player Limit Texas Hold'em, and finally
two-player 2-7 triple draw poker. We show that our model can quickly learn
patterns in these very different poker games while it improves from zero
knowledge to a competitive player against human experts.
  The contributions of this paper include: (1) a novel representation for poker
games, extendable to different poker variations, (2) a CNN based learning model
that can effectively learn the patterns in three different games, and (3) a
self-trained system that significantly beats the heuristic-based program on
which it is trained, and our system is competitive against human expert
players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06749</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06749</id><created>2015-09-22</created><authors><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>Leistedt</keyname><forenames>Boris</forenames></author><author><keyname>B&#xfc;ttner</keyname><forenames>Martin</forenames></author><author><keyname>Peiris</keyname><forenames>Hiranya V.</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Directional spin wavelets on the sphere</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>11 pages, 7 figures. Code available on www.s2let.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a directional spin wavelet framework on the sphere by
generalising the scalar scale-discretised wavelet transform to signals of
arbitrary spin. The resulting framework is the only wavelet framework defined
natively on the sphere that is able to probe the directional intensity of spin
signals. Furthermore, directional spin scale-discretised wavelets support the
exact synthesis of a signal on the sphere from its wavelet coefficients and
satisfy excellent localisation and uncorrelation properties. Consequently,
directional spin scale-discretised wavelets are likely to be of use in a wide
range of applications and in particular for the analysis of the polarisation of
the cosmic microwave background (CMB). We develop new algorithms to compute
(scalar and spin) forward and inverse wavelet transforms exactly and
efficiently for very large data-sets containing tens of millions of samples on
the sphere. By leveraging a novel sampling theorem on the rotation group
developed in a companion article, only half as many wavelet coefficients as
alternative approaches need be computed, while still capturing the full
information content of the signal under analysis. Our implementation of these
algorithms is made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06750</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06750</id><created>2015-09-22</created><updated>2015-12-21</updated><authors><author><keyname>Leistedt</keyname><forenames>Boris</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>Kitching</keyname><forenames>Thomas D.</forenames></author><author><keyname>Peiris</keyname><forenames>Hiranya V.</forenames></author></authors><title>3D weak lensing with spin wavelets on the ball</title><categories>astro-ph.CO astro-ph.IM cs.IT math.IT</categories><comments>24 pages, 4 figures, version accepted for publication in PRD</comments><doi>10.1103/PhysRevD.92.123010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct the spin flaglet transform, a wavelet transform to analyze spin
signals in three dimensions. Spin flaglets can probe signal content localized
simultaneously in space and frequency and, moreover, are separable so that
their angular and radial properties can be controlled independently. They are
particularly suited to analyzing of cosmological observations such as the weak
gravitational lensing of galaxies. Such observations have a unique 3D
geometrical setting since they are natively made on the sky, have spin angular
symmetries, and are extended in the radial direction by additional distance or
redshift information. Flaglets are constructed in the harmonic space defined by
the Fourier-Laguerre transform, previously defined for scalar functions and
extended here to signals with spin symmetries. Thanks to various sampling
theorems, both the Fourier-Laguerre and flaglet transforms are theoretically
exact when applied to bandlimited signals. In other words, in numerical
computations the only loss of information is due to the finite representation
of floating point numbers. We develop a 3D framework relating the weak lensing
power spectrum to covariances of flaglet coefficients. We suggest that the
resulting novel flaglet weak lensing estimator offers a powerful alternative to
common 2D and 3D approaches to accurately capture cosmological information.
While standard weak lensing analyses focus on either real or harmonic space
representations (i.e., correlation functions or Fourier-Bessel power spectra,
respectively), a wavelet approach inherits the advantages of both techniques,
where both complicated sky coverage and uncertainties associated with the
physical modeling of small scales can be handled effectively. Our codes to
compute the Fourier-Laguerre and flaglet transforms are made publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06755</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06755</id><created>2015-09-22</created><authors><author><keyname>Pilloni</keyname><forenames>A.</forenames></author><author><keyname>Pisano</keyname><forenames>A.</forenames></author><author><keyname>Orlov</keyname><forenames>Y.</forenames></author><author><keyname>Usai</keyname><forenames>E.</forenames></author></authors><title>Consensus-based control for a network of diffusion PDEs with boundary
  local interaction</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the problem of driving the state of a network of identical
agents, modeled by boundary-controlled heat equations, towards a common
steady-state profile is addressed. Decentralized consensus protocols are
proposed to address two distinct problems. The first problem is that of
steering the states of all agents towards the same constant steady-state
profile which corresponds to the spatial average of the agents initial
condition. A linear local interaction rule addressing this requirement is
given. The second problem deals with the case where the controlled boundaries
of the agents dynamics are corrupted by additive persistent disturbances. To
achieve synchronization between agents, while completely rejecting the effect
of the boundary disturbances, a nonlinear sliding-mode based consensus protocol
is proposed. Performance of the proposed local interaction rules are analyzed
by applying a Lyapunov-based approach. Simulation results are presented to
support the effectiveness of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06767</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06767</id><created>2015-09-22</created><authors><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author><author><keyname>Durastanti</keyname><forenames>Claudio</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Localisation of directional scale-discretised wavelets on the sphere</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scale-discretised wavelets yield a directional wavelet framework on the
sphere where a signal can be probed not only in scale and position but also in
orientation. Furthermore, a signal can be synthesised from its wavelet
coefficients exactly, in theory and practice (to machine precision).
Scale-discretised wavelets are closely related to spherical needlets (both were
developed independently at about the same time) but relax the axisymmetric
property of needlets so that directional signal content can be probed. Needlets
have been shown to satisfy important quasi-exponential localisation and
asymptotic uncorrelation properties. We show that these properties also hold
for directional scale-discretised wavelets on the sphere and derive similar
localisation and uncorrelation bounds in both the scalar and spin settings.
Scale-discretised wavelets can thus be considered as directional needlets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06774</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06774</id><created>2015-09-22</created><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author></authors><title>Preprint: Bringing immersive enjoyment to hyperbaric oxygen chamber
  users using virtual reality glasses</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on REHAB2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on REHAB2015. This paper proposed a
novel immersive entertainment system for the users of hyperbaric oxygen therapy
chamber. The system is a hybrid of hardware and software, the scheme is
described in this paper. The hardware is combined by a HMD (i.e. virtual
reality glasses shell), a smartphone and a waterproof bag. The software is able
to transfer the stereoscopic images of the 3D game to the screen of the
smartphone synchronously. The comparison and selection of the hardware are
discussed according to the practical running scene of the clinical hyperbaric
oxygen treatment. Finally, a preliminary guideline for designing this kind of
system is raised accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06776</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06776</id><created>2015-09-22</created><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Penades</keyname><forenames>Vicente</forenames></author><author><keyname>Blasco</keyname><forenames>Sonia</forenames></author><author><keyname>Chirivella</keyname><forenames>Javier</forenames></author><author><keyname>Gagliardo</keyname><forenames>Pablo</forenames></author></authors><title>Preprint: Intuitive Evaluation of Kinect2 based Balance Measurement
  Software</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on REHAB2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on REHAB2015. A balance measurement
software based on Kinect2 sensor is evaluated by comparing to golden standard
balance measure platform intuitively. The software analysis the tracked body
data from the user by Kinect2 sensor and get user's center of mass(CoM) as well
as its motion route on a plane. The software is evaluated by several comparison
tests, the evaluation results preliminarily prove the reliability of the
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06783</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06783</id><created>2015-09-22</created><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Penades</keyname><forenames>Vicente</forenames></author><author><keyname>Blasco</keyname><forenames>Sonia</forenames></author><author><keyname>Chirivella</keyname><forenames>Javier</forenames></author><author><keyname>Gagliardo</keyname><forenames>Pablo</forenames></author></authors><title>Preprint: Comparing Kinect2 based Balance Measurement Software to Wii
  Balance Board</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on REHAB2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on REHAB2015. A balance measurement
software based on Kinect2 sensor is evaluated by comparing to Wii balance board
in numerical analysis level, and further improved according to the
consideration of BFP (Body fat percentage) values of the user. Several person
with different body types are involved into the test. The algorithm is improved
by comparing the body type of the user to the 'golden- standard' body type. The
evaluation results of the optimized algorithm preliminarily prove the
reliability of the software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06791</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06791</id><created>2015-09-22</created><updated>2016-02-16</updated><authors><author><keyname>Zhang</keyname><forenames>Tianhao</forenames></author><author><keyname>Kahn</keyname><forenames>Gregory</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Learning Deep Control Policies for Autonomous Aerial Vehicles with
  MPC-Guided Policy Search</title><categories>cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model predictive control (MPC) is an effective method for controlling robotic
systems, particularly autonomous aerial vehicles such as quadcopters. However,
application of MPC can be computationally demanding, and typically requires
estimating the state of the system, which can be challenging in complex,
unstructured environments. Reinforcement learning can in principle forego the
need for explicit state estimation and acquire a policy that directly maps
sensor readings to actions, but is difficult to apply to unstable systems that
are liable to fail catastrophically during training before an effective policy
has been found. We propose to combine MPC with reinforcement learning in the
framework of guided policy search, where MPC is used to generate data at
training time, under full state observations provided by an instrumented
training environment. This data is used to train a deep neural network policy,
which is allowed to access only the raw observations from the vehicle's onboard
sensors. After training, the neural network policy can successfully control the
robot without knowledge of the full state, and at a fraction of the
computational cost of MPC. We evaluate our method by learning obstacle
avoidance policies for a simulated quadrotor, using simulated onboard sensors
and no explicit state estimation at test time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06792</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06792</id><created>2015-09-22</created><authors><author><keyname>Soltanian</keyname><forenames>Abbas</forenames></author><author><keyname>Salahuddin</keyname><forenames>Mohammad A.</forenames></author><author><keyname>Elbiaze</keyname><forenames>Halima</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author></authors><title>A Resource Allocation Mechanism for Video Mixing as a Cloud Computing
  Service in Multimedia Conferencing Applications</title><categories>cs.MM</categories><comments>6 pages, CNSM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimedia conferencing is the conversational exchange of multimedia content
between multiple parties. It has a wide range of applications (e.g. Massively
Multiplayer Online Games (MMOGs) and distance learning). Many multimedia
conferencing applications use video extensively, thus video mixing in
conferencing settings is of critical importance. Cloud computing is a
technology that can solve the scalability issue in multimedia conferencing,
while bringing other benefits, such as, elasticity, efficient use of resources,
rapid development, and introduction of new applications. However, proposed
cloud-based multimedia conferencing approaches so far have several deficiencies
when it comes to efficient resource usage while meeting Quality of Service
(QoS) requirements. We propose a solution to optimize resource allocation for
cloud-based video mixing service in multimedia conferencing applications, which
can support scalability in terms of number of users, while guaranteeing QoS. We
formulate the resource allocation problem mathematically as an Integer Linear
Programming (ILP) problem and design a heuristic for it. Simulation results
show that our resource allocation model can support more participants compared
to the state-of-the-art, while honoring QoS, with respect to end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06805</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06805</id><created>2015-09-22</created><authors><author><keyname>Liu</keyname><forenames>Jin-Hu</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Shao</keyname><forenames>Junming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Online Social Activity Reflects Economic Status</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>9 pages, 4 tables, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To characterize economic development and diagnose the economic health
condition, several popular indices such as gross domestic product (GDP),
industrial structure and income growth are widely applied. However, computing
these indices based on traditional economic census is usually costly and
resources consuming, and more importantly, following a long time delay. In this
paper, we analyzed nearly 200 million users' activities for four consecutive
years in the largest social network (Sina Microblog) in China, aiming at
exploring latent relationships between the online social activities and local
economic status. Results indicate that online social activity has a strong
correlation with local economic development and industrial structure, and more
interestingly, allows revealing the macro-economic structure instantaneously
with nearly no cost. Beyond, this work also provides a new venue to identify
risky signal in local economic structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06807</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06807</id><created>2015-09-22</created><authors><author><keyname>Li</keyname><forenames>Ke</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Bandit Label Inference for Weakly Supervised Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scarcity of data annotated at the desired level of granularity is a
recurring issue in many applications. Significant amounts of effort have been
devoted to developing weakly supervised methods tailored to each individual
setting, which are often carefully designed to take advantage of the particular
properties of weak supervision regimes, form of available data and prior
knowledge of the task at hand. Unfortunately, it is difficult to adapt these
methods to new tasks and/or forms of data, which often require different weak
supervision regimes or models. We present a general-purpose method that can
solve any weakly supervised learning problem irrespective of the weak
supervision regime or the model. The proposed method turns any off-the-shelf
strongly supervised classifier into a weakly supervised classifier and allows
the user to specify any arbitrary weakly supervision regime via a loss
function. We apply the method to several different weak supervision regimes and
demonstrate competitive results compared to methods specifically engineered for
those settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06808</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06808</id><created>2015-09-22</created><updated>2015-09-30</updated><authors><author><keyname>Gangavarapu</keyname><forenames>Karthik</forenames></author><author><keyname>Babji</keyname><forenames>Vyshakh</forenames></author><author><keyname>Mei&#xdf;ner</keyname><forenames>Tobias</forenames></author><author><keyname>Su</keyname><forenames>Andrew I.</forenames></author><author><keyname>Good</keyname><forenames>Benjamin M.</forenames></author></authors><title>Branch: An interactive, web-based tool for testing hypotheses and
  developing predictive models</title><categories>stat.AP cs.CY cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Branch is a web application that provides users with no programming with the
ability to interact directly with large biomedical datasets. The interaction is
mediated through a collaborative graphical user interface for building and
evaluating decision trees. These trees can be used to compose and test
sophisticated hypotheses and to develop predictive models. Decision trees are
evaluated based on a library of imported datasets and can be stored in a
collective area for sharing and re-use. Branch is hosted at
http://biobranch.org/ and the open source code is available at
http://bitbucket.org/sulab/biobranch/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06812</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06812</id><created>2015-09-22</created><authors><author><keyname>Ba</keyname><forenames>Jimmy</forenames></author><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Frey</keyname><forenames>Brendan</forenames></author></authors><title>Learning Wake-Sleep Recurrent Attention Models</title><categories>cs.LG</categories><comments>To appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their success, convolutional neural networks are computationally
expensive because they must examine all image locations. Stochastic
attention-based models have been shown to improve computational efficiency at
test time, but they remain difficult to train because of intractable posterior
inference and high variance in the stochastic gradient estimates. Borrowing
techniques from the literature on training deep generative models, we present
the Wake-Sleep Recurrent Attention Model, a method for training stochastic
attention networks which improves posterior inference and which reduces the
variability in the stochastic gradients. We show that our method can greatly
speed up the training time for stochastic attention networks in the domains of
image classification and caption generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06813</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06813</id><created>2015-09-22</created><authors><author><keyname>Nam</keyname><forenames>Junghyun</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author><author><keyname>Han</keyname><forenames>Sangchul</forenames></author><author><keyname>Kim</keyname><forenames>Moonseong</forenames></author><author><keyname>Paik</keyname><forenames>Juryon</forenames></author><author><keyname>Won</keyname><forenames>Dongho</forenames></author></authors><title>Efficient and Anonymous Two-Factor User Authentication in Wireless
  Sensor Networks: Achieving User Anonymity with Lightweight Sensor Computation</title><categories>cs.CR</categories><journal-ref>PLoS ONE 10(4): e0116709, 2015</journal-ref><doi>10.1371/journal.pone.0116709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A smart-card-based user authentication scheme for wireless sensor networks
(hereafter referred to as a SCA-WSN scheme) is designed to ensure that only
users who possess both a smart card and the corresponding password are allowed
to gain access to sensor data and their transmissions. Despite many research
efforts in recent years, it remains a challenging task to design an efficient
SCA-WSN scheme that achieves user anonymity. The majority of published SCA-WSN
schemes use only lightweight cryptographic techniques (rather than public-key
cryptographic techniques) for the sake of efficiency, and have been
demonstrated to suffer from the inability to provide user anonymity. Some
schemes employ elliptic curve cryptography for better security but require
sensors with strict resource constraints to perform computationally expensive
scalar-point multiplications; despite the increased computational requirements,
these schemes do not provide user anonymity. In this paper, we present a new
SCA-WSN scheme that not only achieves user anonymity but also is efficient in
terms of the computation loads for sensors. Our scheme employs elliptic curve
cryptography but restricts its use only to anonymous user-to-gateway
authentication, thereby allowing sensors to perform only lightweight
cryptographic operations. Our scheme also enjoys provable security in a formal
model extended from the widely accepted Bellare-Pointcheval-Rogaway (2000)
model to capture the user anonymity property and various SCA-WSN specific
attacks (e.g., stolen smart card attacks, node capture attacks, privileged
insider attacks, and stolen verifier attacks).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06815</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06815</id><created>2015-09-22</created><authors><author><keyname>Do</keyname><forenames>Quang</forenames></author><author><keyname>Martini</keyname><forenames>Ben</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author></authors><title>A Forensically Sound Adversary Model for Mobile Devices</title><categories>cs.CR</categories><journal-ref>PLoS ONE 10(9): e0138449, 2015</journal-ref><doi>10.1371/journal.pone.0138449</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an adversary model to facilitate forensic
investigations of mobile devices (e.g. Android, iOS and Windows smartphones)
that can be readily adapted to the latest mobile device technologies. This is
essential given the ongoing and rapidly changing nature of mobile device
technologies. An integral principle and significant constraint upon forensic
practitioners is that of forensic soundness. Our adversary model specifically
considers and integrates the constraints of forensic soundness on the
adversary, in our case, a forensic practitioner. One construction of the
adversary model is an evidence collection and analysis methodology for Android
devices. Using the methodology with six popular cloud apps, we were successful
in extracting various information of forensic interest in both the external and
internal storage of the mobile device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06824</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06824</id><created>2015-09-22</created><authors><author><keyname>Xie</keyname><forenames>Christopher</forenames></author><author><keyname>Patil</keyname><forenames>Sachin</forenames></author><author><keyname>Moldovan</keyname><forenames>Teodor</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Model-based Reinforcement Learning with Parametrized Physical Models and
  Optimism-Driven Exploration</title><categories>cs.LG cs.RO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a robotic model-based reinforcement learning method
that combines ideas from model identification and model predictive control. We
use a feature-based representation of the dynamics that allows the dynamics
model to be fitted with a simple least squares procedure, and the features are
identified from a high-level specification of the robot's morphology,
consisting of the number and connectivity structure of its links. Model
predictive control is then used to choose the actions under an optimistic model
of the dynamics, which produces an efficient and goal-directed exploration
strategy. We present real time experimental results on standard benchmark
problems involving the pendulum, cartpole, and double pendulum systems.
Experiments indicate that our method is able to learn a range of benchmark
tasks substantially faster than the previous best methods. To evaluate our
approach on a realistic robotic control task, we also demonstrate real time
control of a simulated 7 degree of freedom arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06825</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06825</id><created>2015-09-22</created><authors><author><keyname>Pinto</keyname><forenames>Lerrel</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700
  Robot Hours</title><categories>cs.LG cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current learning-based robot grasping approaches exploit human-labeled
datasets for training the models. However, there are two problems with such a
methodology: (a) since each object can be grasped in multiple ways, manually
labeling grasp locations is not a trivial task; (b) human labeling is biased by
semantics. While there have been attempts to train robots using trial-and-error
experiments, the amount of data used in such experiments remains substantially
low and hence makes the learner prone to over-fitting. In this paper, we take
the leap of increasing the available training data to 40 times more than prior
work, leading to a dataset size of 50K data points collected over 700 hours of
robot grasping attempts. This allows us to train a Convolutional Neural Network
(CNN) for the task of predicting grasp locations without severe overfitting. In
our formulation, we recast the regression problem to an 18-way binary
classification over image patches. We also present a multi-stage learning
approach where a CNN trained in one stage is used to collect hard negatives in
subsequent stages. Our experiments clearly show the benefit of using
large-scale datasets (and multi-stage training) for the task of grasping. We
also compare to several baselines and show state-of-the-art performance on
generalization to unseen objects for grasping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06836</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06836</id><created>2015-09-22</created><authors><author><keyname>Uddin</keyname><forenames>Shahadat</forenames></author><author><keyname>Khan</keyname><forenames>Arif</forenames></author><author><keyname>Baur</keyname><forenames>Louise A.</forenames></author></authors><title>A Framework to Explore the Knowledge Structure of Multidisciplinary
  Research Fields</title><categories>cs.DL</categories><journal-ref>PLoS ONE 10(4), 2015 PLoS ONE 10(4):e0123537</journal-ref><doi>10.1371/journal.pone.0123537</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Understanding emerging areas of a multidisciplinary research field is crucial
for researchers,policymakers and other stakeholders. For them a knowledge
structure based on longitudinal bibliographic data can be an effective
instrument. But with the vast amount of available online information it is
often hard to understand the knowledge structure for data. In this paper, we
present a novel approach for retrieving online bibliographic data and propose a
framework for exploring knowledge structure. We also present several
longitudinal analyses to interpret and visualize the last 20 years of published
obesity research data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06837</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06837</id><created>2015-09-22</created><authors><author><keyname>Newberry</keyname><forenames>X. Y.</forenames></author></authors><title>Generalization of the Truth-relevant Semantics to the Predicate Calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1952 P. F. Strawson proposed a logic of presuppositions. It is an
interpretation of Aristotelian logic, i.e. of the logic of the traditional
syllogism. In 1981 Richard Diaz published a monograph in which he presented
truth-relevant logic. This paper shows that truth-relevant logic is but a
propositional version of the logic of presuppositions. A semantics of the logic
of presuppositions is developed using truth-relevant logic. The semantics is
then further extended to polyadic logic and some consequences discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06839</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06839</id><created>2015-09-21</created><authors><author><keyname>Sreram</keyname><forenames>B.</forenames></author><author><keyname>Bounapane</keyname><forenames>F.</forenames></author><author><keyname>Subathra</keyname><forenames>B.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Seshadhri</forenames></author></authors><title>Estimating Random Delays in Modbus Network Using Experiments and General
  Linear Regression Neural Networks with Genetic Algorithm Smoothing</title><categories>cs.SY cs.NE</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-varying delays adversely affect the performance of networked control
sys-tems (NCS) and in the worst-case can destabilize the entire system.
Therefore, modelling network delays is important for designing NCS. However,
modelling time-varying delays is challenging because of their dependence on
multiple pa-rameters such as length, contention, connected devices, protocol
employed, and channel loading. Further, these multiple parameters are
inherently random and de-lays vary in a non-linear fashion with respect to
time. This makes estimating ran-dom delays challenging. This investigation
presents a methodology to model de-lays in NCS using experiments and general
regression neural network (GRNN) due to their ability to capture non-linear
relationship. To compute the optimal smoothing parameter that computes the best
estimates, genetic algorithm is used. The objective of the genetic algorithm is
to compute the optimal smoothing pa-rameter that minimizes the mean absolute
percentage error (MAPE). Our results illustrate that the resulting GRNN is able
to predict the delays with less than 3% error. The proposed delay model gives a
framework to design compensation schemes for NCS subjected to time-varying
delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06840</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06840</id><created>2015-09-22</created><authors><author><keyname>Farayev</keyname><forenames>Bakhtiyar</forenames></author><author><keyname>Sadi</keyname><forenames>Yalcin</forenames></author><author><keyname>Ergen</keyname><forenames>Sinem Coleri</forenames></author></authors><title>Optimal Power Control and Rate Adaptation for Ultra-Reliable M2M Control
  Applications</title><categories>cs.NI cs.IT cs.SY math.IT</categories><comments>6 pages, 3 figures, GLOBECOM Workshops 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main challenge of ultra-reliable machine-to-machine (M2M) control
applications is to meet the stringent timing and reliability requirements of
control systems, despite the adverse properties of wireless communication for
delay and packet errors, and limited battery resources of the sensor nodes.
Since the transmission delay and energy consumption of a sensor node are
determined by the transmission power and rate of that sensor node and the
concurrently transmitting nodes, the transmission schedule should be optimized
jointly with the transmission power and rate of the sensor nodes. Previously,
it has been shown that the optimization of power control and rate adaptation
for each node subset can be separately formulated, solved and then used in the
scheduling algorithm in the optimal solution of the joint optimization of power
control, rate adaptation and scheduling problem. However, the power control and
rate adaptation problem has been only formulated and solved for continuous rate
transmission model, in which Shannon's capacity formulation for an Additive
White Gaussian Noise (AWGN) wireless channel is used in the calculation of the
maximum achievable rate as a function of Signal-to-Interference-plus-Noise
Ratio (SINR). In this paper, we formulate the power control and rate adaptation
problem with the objective of minimizing the time required for the concurrent
transmission of a set of sensor nodes while satisfying their transmission
delay, reliability and energy consumption requirements based on the more
realistic discrete rate transmission model, in which only a finite set of
transmit rates are supported. We propose a polynomial time algorithm to solve
this problem and prove the optimality of the proposed algorithm. We then
combine it with the previously proposed scheduling algorithms and demonstrate
its close to optimal performance via extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06841</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06841</id><created>2015-09-23</created><authors><author><keyname>Fu</keyname><forenames>Justin</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation
  and Neural Network Priors</title><categories>cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges in applying reinforcement learning to complex
robotic control tasks is the need to gather large amounts of experience in
order to find an effective policy for the task at hand. Model-based
reinforcement learning can achieve good sample efficiency, but requires the
ability to learn a model of the dynamics that is good enough to learn an
effective policy. In this work, we develop a modelbased reinforcement learning
algorithm that combines prior knowledge from previous tasks with online
adaptation of the dynamics model. These two ingredients enable highly sample
efficient learning even in regimes where estimating the true dynamics is very
difficult, since the online model adaptation allows the method to locally
compensate for unmodeled variation in the dynamics. We encode the prior
experience into a neural network dynamics model, and adapt it online by
progressively refitting a local linear model of the dynamics. Our experimental
results show that this approach can be used to solve a variety of complex
robotic manipulation tasks in just a single attempt, using prior data from
other manipulation behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06842</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06842</id><created>2015-09-23</created><authors><author><keyname>Poursoltan</keyname><forenames>Shayan</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>A Feature-Based Comparison of Evolutionary Computing Techniques for
  Constrained Continuous Optimisation</title><categories>cs.AI cs.NE</categories><comments>16 Pagesm 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms have been frequently applied to constrained
continuous optimisation problems. We carry out feature based comparisons of
different types of evolutionary algorithms such as evolution strategies,
differential evolution and particle swarm optimisation for constrained
continuous optimisation. In our study, we examine how sets of constraints
influence the difficulty of obtaining close to optimal solutions. Using a
multi-objective approach, we evolve constrained continuous problems having a
set of linear and/or quadratic constraints where the different evolutionary
approaches show a significant difference in performance. Afterwards, we discuss
the features of the constraints that exhibit a difference in performance of the
different evolutionary approaches under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06847</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06847</id><created>2015-09-23</created><authors><author><keyname>Manvi</keyname></author><author><keyname>Bhatia</keyname><forenames>Komal Kumar</forenames></author><author><keyname>Dixit</keyname><forenames>Ashutosh</forenames></author></authors><title>Design and Implementation of Domain based Semantic Hidden Web Crawler</title><categories>cs.IR</categories><comments>12 pages,10 figures</comments><journal-ref>IJIACS 2015 Volume 4 Special Issue ICRDESM-15 Paper id: 9D2N6Y</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web is a wide term which mainly consists of surface web and hidden web. One
can easily access the surface web using traditional web crawlers, but they are
not able to crawl the hidden portion of the web. These traditional crawlers
retrieve contents from web pages, which are linked by hyperlinks ignoring the
information hidden behind form pages, which cannot be extracted using simple
hyperlink structure. Thus, they ignore large amount of data hidden behind
search forms. This paper emphasizes on the extraction of hidden data behind
html search forms. The proposed technique makes use of semantic mapping to fill
the html search form using domain specific database. Using semantics to fill
various fields of a form leads to more accurate and qualitative data
extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06849</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06849</id><created>2015-09-23</created><authors><author><keyname>Ahn</keyname><forenames>Sungsoo</forenames><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author><author><keyname>Park</keyname><forenames>Sejun</forenames><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames><affiliation>Los Alamos National Laboratory</affiliation></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author></authors><title>Minimum Weight Perfect Matching via Blossom Belief Propagation</title><categories>cs.DS cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-product Belief Propagation (BP) is a popular message-passing algorithm
for computing a Maximum-A-Posteriori (MAP) assignment over a distribution
represented by a Graphical Model (GM). It has been shown that BP can solve a
number of combinatorial optimization problems including minimum weight
matching, shortest path, network flow and vertex cover under the following
common assumption: the respective Linear Programming (LP) relaxation is tight,
i.e., no integrality gap is present. However, when LP shows an integrality gap,
no model has been known which can be solved systematically via sequential
applications of BP. In this paper, we develop the first such algorithm, coined
Blossom-BP, for solving the minimum weight matching problem over arbitrary
graphs. Each step of the sequential algorithm requires applying BP over a
modified graph constructed by contractions and expansions of blossoms, i.e.,
odd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs,
where n is the number of vertices in the original graph. In essence, the
Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom
algorithm by jumping at once over many sub-steps with a single BP. Moreover,
our result provides an interpretation of the Edmonds' algorithm as a sequence
of LPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06853</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06853</id><created>2015-09-23</created><authors><author><keyname>Gubbi</keyname><forenames>Abdullah</forenames></author><author><keyname>Azeem</keyname><forenames>Mohammed Fazle</forenames></author><author><keyname>Ansari</keyname><forenames>Zahid</forenames></author></authors><title>New Fuzzy LBP Features for Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many Local texture features each very in way they implement and
each of the Algorithm trying improve the performance. An attempt is made in
this paper to represent a theoretically very simple and computationally
effective approach for face recognition. In our implementation the face image
is divided into 3x3 sub-regions from which the features are extracted using the
Local Binary Pattern (LBP) over a window, fuzzy membership function and at the
central pixel. The LBP features possess the texture discriminative property and
their computational cost is very low. By utilising the information from LBP,
membership function, and central pixel, the limitations of traditional LBP is
eliminated. The bench mark database like ORL and Sheffield Databases are used
for the evaluation of proposed features with SVM classifier. For the proposed
approach K-fold and ROC curves are obtained and results are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06854</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06854</id><created>2015-09-23</created><authors><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Peng</forenames></author><author><keyname>Chan</keyname><forenames>W. K.</forenames></author><author><keyname>Zhang</keyname><forenames>Xinchao</forenames></author></authors><title>To What Extent Is Stress Testing of Android TV Applications Automated in
  Industrial Environments?</title><categories>cs.SE</categories><comments>17 pages</comments><doi>10.1109/TR.2015.2481601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Android-based smart Television (TV) must reliably run its applications in
an embedded program environment under diverse hardware resource conditions.
Owing to the diverse hardware components used to build numerous TV models, TV
simulators are usually not high enough in fidelity to simulate various TV
models, and thus are only regarded as unreliable alternatives when stress
testing such applications. Therefore, even though stress testing on real TV
sets is tedious, it is the de facto approach to ensure the reliability of these
applications in the industry. In this paper, we study to what extent stress
testing of smart TV applications can be fully automated in the industrial
environments. To the best of our knowledge, no previous work has addressed this
important question. We summarize the find-ings collected from 10 industrial
test engineers to have tested 20 such TV applications in a real production
environment. Our study shows that the industry required test automation
supports on high-level GUI object controls and status checking, setup of
resource conditions and the interplay between the two. With such supports, 87%
of the industrial test specifications of one TV model can be fully automated
and 71.4% of them were found to be fully reusable to test a subsequent TV model
with major up-grades of hardware, operating system and application. It
repre-sents a significant improvement with margins of 28% and 38%,
respectively, compared to stress testing without such supports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06856</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06856</id><created>2015-09-23</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author></authors><title>Unsupervised Online Bayesian Autonomic Happy Internet-of-Things
  Management</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Happy IoT, the revenue of service providers synchronizes to the
unobservable and dynamic usage-contexts (e.g. emotion, environmental
information, etc.) of Smart-device users. Hence, the usage-context-estimation
from the unreliable Smart-device sensed data is justified as an unsupervised
and non-linear optimization problem. Accordingly, Autonomic Happy IoT
Management is aimed at attracting initial user-groups based on the common
interests (i.e. recruitment ), then uncovering their latent usage-contexts from
unreliable sensed data (i.e. revenue-renewal ) and synchronizing to
usage-context dynamics (i.e. stochastic monetization). In this context, we have
proposed an unsupervised online Bayesian mechanism, namely Whiz (Greek word,
meaning Smart), in which, (a) once latent user-groups are initialized (i.e
measurement model ), (b) usage-context is iteratively estimated from the
unreliable sensed data (i.e. learning model ), (c) followed by online filtering
of Bayesian knowledge about usage-context (i.e. filtering model ). Finally, we
have proposed an Expectation Maximization (EM)-based iterative algorithm Whiz,
which facilitates Happy IoT by solving (a) recruitment, (b) revenue-renewal and
(c) stochastic- monetization problems with (a) measurement, (b) learning, and
(c) filtering models, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06858</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06858</id><created>2015-09-23</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Tronci</keyname><forenames>Enrico</forenames></author></authors><title>Proceedings Sixth International Symposium on Games, Automata, Logics and
  Formal Verification</title><categories>cs.LO cs.FL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015</journal-ref><doi>10.4204/EPTCS.193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Sixth International Symposium on
Games, Automata, Logic and Formal Verification (GandALF 2015). The symposium
took place in Genoa, Italy, on the 21st and 22nd of September 2015. The
proceedings of the symposium contain the abstracts of three invited talks and
13 papers that were accepted after a careful evaluation for presentation at the
conference. The topics of the accepted papers cover algorithmic game theory,
automata theory, formal verification, and modal and temporal logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06874</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06874</id><created>2015-09-23</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author></authors><title>Twitter as a Transport Layer Platform</title><categories>cs.SI cs.CY</categories><comments>submitted to Fruct conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet messengers and social networks have become an integral part of
modern digital life. We have in mind not only the interaction between
individual users but also a variety of applications that exist in these
applications. Typically, applications for social networks use the universal
login system and rely on data from social networks. Also, such applications are
likely to get more traction when they are inside of the big social network like
Facebook. At the same time, less attention is paid to communication
capabilities of social networks. In this paper, we target Twitter as a
messaging system at the first hand. We describe the way information systems can
use Twitter as a transport layer for own services. Our work introduces a
programmable service called 411 for Twitter, which supports user-defined and
application-specific commands through tweets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06882</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06882</id><created>2015-09-23</created><authors><author><keyname>Barfuss</keyname><forenames>Hendrik</forenames></author><author><keyname>Huemmer</keyname><forenames>Christian</forenames></author><author><keyname>Schwarz</keyname><forenames>Andreas</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Robust coherence-based spectral enhancement for distant speech
  recognition</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution to the 3rd CHiME Speech Separation and Recognition
Challenge (CHiME-3) we extend the acoustic front-end of the CHiME-3 baseline
speech recognition system by a coherence-based Wiener filter which is applied
to the output signal of the baseline beamformer. To compute the time- and
frequency-dependent postfilter gains the ratio between direct and diffuse
signal components at the output of the baseline beamformer is estimated and
used as approximation of the short-time signal-to-noise ratio. The proposed
spectral enhancement technique is evaluated with respect to word error rates of
the CHiME-3 challenge baseline speech recognition system using real speech
recorded in public environments. Results confirm the effectiveness of the
coherence-based postfilter when integrated into the front-end signal
enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06889</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06889</id><created>2015-09-23</created><authors><author><keyname>Liu</keyname><forenames>Jia</forenames></author><author><keyname>Sheng</keyname><forenames>Min</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Sun</keyname><forenames>Hongguang</forenames></author><author><keyname>Wang</keyname><forenames>Xijun</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author></authors><title>Throughput capacity of two-hop relay MANETs under finite buffers</title><categories>cs.PF cs.IT cs.NI math.IT</categories><doi>10.1109/PIMRC.2014.7136358</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the seminal work of Grossglauser and Tse [1], the two-hop relay
algorithm and its variants have been attractive for mobile ad hoc networks
(MANETs) due to their simplicity and efficiency. However, most literature
assumed an infinite buffer size for each node, which is obviously not
applicable to a realistic MANET. In this paper, we focus on the exact
throughput capacity study of two-hop relay MANETs under the practical finite
relay buffer scenario. The arrival process and departure process of the relay
queue are fully characterized, and an ergodic Markov chain-based framework is
also provided. With this framework, we obtain the limiting distribution of the
relay queue and derive the throughput capacity under any relay buffer size.
Extensive simulation results are provided to validate our theoretical framework
and explore the relationship among the throughput capacity, the relay buffer
size and the number of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06891</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06891</id><created>2015-09-23</created><authors><author><keyname>Mandal</keyname><forenames>Swagata</forenames></author><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Sau</keyname><forenames>Suman</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Subhasis</forenames></author></authors><title>A Novel Method for Soft Error Mitigation in FPGA using Adaptive Cross
  Parity Code</title><categories>cs.AR</categories><comments>Manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Field Programmable Gate Arrays (FPGAs) are more prone to be affected by
transient faults in presence of radiation and other environmental hazards
compared to Application Specific Integrated Circuits (ASICs). Hence, error
mitigation and recovery techniques are absolutely necessary to protect the FPGA
hardware from soft errors arising due to such transient faults. In this paper,
a new efficient multi-bit error correcting method for FPGAs is proposed using
adaptive cross parity check (ACPC) code. ACPC is easy to implement and the
needed decoding circuit is also simple. In the proposed scheme total
configuration memory is partitioned into two parts. One part will contain ACPC
hardware, which is static and assumed to be unaffected by any kind of errors.
Other portion will store the binary file for logic, which is to be protected
from transient error and is assumed to be dynamically reconfigurable (Partial
reconfigurable area). Binary file from the secondary memory passes through ACPC
hardware and the bits for forward error correction (FEC) field are calculated
before entering into the reconfigurable portion. In the runtime scenario, the
data from the dynamically reconfigurable portion of the configuration memory
will be read back and passed through the ACPC hardware. The ACPC hardware will
correct the errors before the data enters into the dynamic configuration
memory. We propose a first of its kind methodology for novel transient fault
correction using ACPC code for FPGAs. To validate the design we have tested the
proposed methodology with Kintex FPGA. We have also measured different
parameters like critical path, power consumption, overhead resource and error
correction efficiency to estimate the performance of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06893</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06893</id><created>2015-09-23</created><authors><author><keyname>Lokhov</keyname><forenames>Andrey Y.</forenames></author><author><keyname>Misiakiewicz</keyname><forenames>Theodor</forenames></author></authors><title>Efficient reconstruction of transmission probabilities in a spreading
  process from partial observations</title><categories>physics.soc-ph cond-mat.stat-mech cs.LG cs.SI stat.ML</categories><comments>5 pages, 9 pages of supplemental material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem of reconstruction of diffusion network and transmission
probabilities from the data has attracted a considerable attention in the past
several years. A number of recent papers introduced efficient algorithms for
the estimation of spreading parameters, based on the maximization of the
likelihood of observed cascades, assuming that the full information for all the
nodes in the network is available. In this work, we focus on a more realistic
and restricted scenario, in which only a partial information on the cascades is
available: either the set of activation times for a limited number of nodes, or
the states of nodes for a subset of observation times. To tackle this problem,
we first introduce a framework based on the maximization of the likelihood of
the incomplete diffusion trace. However, we argue that the computation of this
incomplete likelihood is a computationally hard problem, and show that a fast
and robust reconstruction of transmission probabilities in sparse networks can
be achieved with a new algorithm based on recently introduced dynamic
message-passing equations for the spreading processes. The suggested approach
can be easily generalized to a large class of discrete and continuous dynamic
models, as well as to the cases of dynamically-changing networks and noisy
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06898</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06898</id><created>2015-09-23</created><authors><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Size-Dependent Tile Self-Assembly: Constant-Height Rectangles and
  Stability</title><categories>cs.CG cs.ET</categories><comments>In proceedings of ISAAC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new model of algorithmic tile self-assembly called
size-dependent assembly. In previous models, supertiles are stable when the
total strength of the bonds between any two halves exceeds some constant
temperature. In this model, this constant temperature requirement is replaced
by an nondecreasing temperature function $\tau : \mathbb{N} \rightarrow
\mathbb{N}$ that depends on the size of the smaller of the two halves. This
generalization allows supertiles to become unstable and break apart, and
captures the increased forces that large structures may place on the bonds
holding them together.
  We demonstrate the power of this model in two ways. First, we give fixed tile
sets that assemble constant-height rectangles and squares of arbitrary input
size given an appropriate temperature function. Second, we prove that deciding
whether a supertile is stable is coNP-complete. Both results contrast with
known results for fixed temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06913</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06913</id><created>2015-09-23</created><authors><author><keyname>Kokkala</keyname><forenames>Janne I.</forenames></author><author><keyname>&#xd6;sterg&#xe5;rd</keyname><forenames>Patric R. J.</forenames></author></authors><title>A coloring of the square of the 8-cube with 13 colors</title><categories>math.CO cs.IT math.IT</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\chi_{\bar{k}}(n)$ be the number of colors required to color the
$n$-dimensional hypercube such that no two vertices with the same color are at
a distance at most $k$. In other words, $\chi_{\bar{k}}(n)$ is the minimum
number of binary codes with minimum distance at least $k+1$ required to
partition the $n$-dimensional Hamming space. By giving an explicit coloring, it
is shown that $\chi_{\bar{2}}(8)=13$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06916</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06916</id><created>2015-09-23</created><authors><author><keyname>Liu</keyname><forenames>Jia</forenames></author><author><keyname>Sheng</keyname><forenames>Min</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author></authors><title>On throughput capacity for a class of buffer-limited MANETs</title><categories>cs.IT cs.NI cs.PF math.IT</categories><doi>10.1016/j.adhoc.2015.08.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Available throughput performance studies for mobile ad hoc networks (MANETs)
suffer from two major limitations: they mainly focus on the scaling law study
of throughput, while the exact throughput of such networks remains largely
unknown; they usually consider the infinite buffer scenarios, which are not
applicable to the practical networks with limited buffer. As a step to address
these limitations, this paper develops a general framework for the exact
throughput capacity study of a class of buffer-limited MANETs with the two-hop
relay. We first provide analysis to reveal how the throughput capacity of such
a MANET is determined by its relay-buffer blocking probability (RBP). Based on
the Embedded Markov Chain Theory and Queuing Theory, a novel theoretical
framework is then developed to enable the RBP and closed-form expression for
exact throughput capacity to be derived. We further conduct case studies under
two typical transmission scheduling schemes to illustrate the applicability of
our framework and to explore the corresponding capacity optimization as well as
capacity scaling law. Finally, extensive simulation and numerical results are
provided to validate the efficiency of our framework and to show the impacts
brought by the buffer constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06921</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06921</id><created>2015-09-23</created><authors><author><keyname>Liu</keyname><forenames>Jia</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author></authors><title>End-to-end delay in two hop relay MANETs with limited buffer</title><categories>cs.PF cs.IT cs.NI math.IT</categories><doi>10.1109/CANDAR.2014.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite lots of literature has been dedicated to researching the delay
performance in two-hop relay (2HR) mobile ad hoc networks (MANETs), however,
they usually assume the buffer size of each node is infinite, so these studies
are not applicable to and thus may not reflect the real delay performance of a
practical MANET with limited buffer. To address this issue, in this paper we
explore the packet end-to-end delay in a 2HR MANET, where each node is equipped
with a bounded and shared relay-buffer for storing and forwarding packets of
all other flows. The transmission range of each node can be adjusted and a
group-based scheduling scheme is adopted to avoid interference between
simultaneous transmissions, meanwhile a handshake mechanism is added to the 2HR
routing algorithm to avoid packet loss. With the help of Markov Chain Theory
and Queuing Theory, we develop a new framework to fully characterize the packet
delivery processes, and obtain the relay-buffer blocking probability (RBP)
under any given exogenous packet input rate. Based on the RBP, we can compute
the packet queuing delay in its source node and delivery delay respectively,
and further derive the end-to-end delay in such a MANET with limited buffer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06925</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06925</id><created>2015-09-23</created><authors><author><keyname>Wang</keyname><forenames>Mengmeng</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author></authors><title>Robust Object Tracking with a Hierarchical Ensemble Framework</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous robots enjoy a wide popularity nowadays and have been applied in
many applications, such as home security, entertainment, delivery, navigation
and guidance. It is vital to robots to track objects accurately in these
applications, so it is necessary to focus on tracking algorithms to improve the
robustness and accuracy. In this paper, we propose a robust object tracking
algorithm based on a hierarchical ensemble framework which can incorporate
information including individual pixel features, local patches and holistic
target models. The framework combines multiple ensemble models simultaneously
instead of using a single ensemble model individually. A discriminative model
which accounts for the matching degree of local patches is adopted via a bottom
ensemble layer, and a generative model which exploits holistic templates is
used to search for the object through the middle ensemble layer as well as an
adaptive Kalman filter. We test the proposed tracker on challenging benchmark
image sequences. Both qualitative and quantitative evaluations demonstrate that
the proposed tracker performs superiorly against several state-of-the-art
algorithms, especially when the appearance changes dramatically and the
occlusions occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06928</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06928</id><created>2015-09-23</created><authors><author><keyname>Ali</keyname><forenames>Ahmed</forenames></author><author><keyname>Bell</keyname><forenames>Peter</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Automatic Dialect Detection in Arabic Broadcast Speech</title><categories>cs.CL</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We investigate different approaches for dialect identification in Arabic
broadcast speech, using phonetic, lexical features obtained from a speech
recognition system, and acoustic features using the i-vector framework. We
studied both generative and discriminate classifiers, and we combined these
features using a multi-class Support Vector Machine (SVM). We validated our
results on an Arabic/English language identification task, with an accuracy of
100%. We used these features in a binary classifier to discriminate between
Modern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We
further report results using the proposed method to discriminate between the
five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine,
North African, and MSA, with an accuracy of 52%. We discuss dialect
identification errors in the context of dialect code-switching between
Dialectal Arabic and MSA, and compare the error pattern between manually
labeled data, and the output from our classifier. We also release the train and
test data as standard corpus for dialect identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06932</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06932</id><created>2015-09-23</created><updated>2015-12-17</updated><authors><author><keyname>Zhang</keyname><forenames>Lin</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Wu</keyname><forenames>Gang</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Efficient Scheduling and Power Allocation for D2D-assisted Wireless
  Caching Networks</title><categories>cs.IT math.IT</categories><comments>This manuscript has been submitted to TCOM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an one-hop device-to-device (D2D) assisted wireless caching network,
where popular files are randomly and independently cached in the memory of
end-users. Each user may obtain the requested files from its own memory without
any transmission, or from a helper through an one-hop D2D transmission, or from
the base station (BS). We formulate a joint D2D link scheduling and power
allocation problem to maximize the system throughput. However, the problem is
non-convex and obtaining an optimal solution is computationally hard.
Alternatively, we decompose the problem into a D2D link scheduling problem and
an optimal power allocation problem. To solve the two subproblems, we first
develop a D2D link scheduling algorithm to select the largest number of D2D
links satisfying both the signal to interference plus noise ratio (SINR) and
the transmit power constraints. Then, we develop an optimal power allocation
algorithm to maximize the minimum transmission rate of the scheduled D2D links.
Numerical results indicate that both the number of the scheduled D2D links and
the system throughput can be improved simultaneously with the Zipf-distribution
caching scheme, the proposed D2D link scheduling algorithm, and the proposed
optimal power allocation algorithm compared with the state of arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06935</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06935</id><created>2015-09-23</created><authors><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author></authors><title>Implementing Parareal - OpenMP or MPI?</title><categories>cs.MS cs.DC math.NA</categories><msc-class>68W10, 65Y05, 68N19</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a comparison between MPI and OpenMP implementations of the
parallel-in-time integration method Parareal. A special-purpose, lightweight
FORTRAN code is described, which serves as a benchmark. To allow for a fair
comparison, an OpenMP implementation of Parareal with pipelining is introduced,
which relies on manual control of locks for synchronisation. Performance is
compared in terms of runtime, speedup, memory footprint and energy-to-solution.
The pipelined shared memory implementation is found to be the most efficient,
particularly with respect to memory footprint. Its higher implementation
complexity, however, might make it difficult to use in legacy codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06937</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06937</id><created>2015-09-23</created><authors><author><keyname>Winkler</keyname><forenames>Kurt</forenames></author><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author></authors><title>Fully automatic multi-language translation with a catalogue of phrases -
  successful employment for the Swiss avalanche bulletin</title><categories>cs.CL</categories><comments>Extended version of a previous workshop paper (arXiv:1405.6103),
  accepted for the journal Language Resources and Evaluation, Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Swiss avalanche bulletin is produced twice a day in four languages. Due
to the lack of time available for manual translation, a fully automated
translation system is employed, based on a catalogue of predefined phrases and
predetermined rules of how these phrases can be combined to produce sentences.
Because this catalogue of phrases is limited to a small sublanguage, the system
is able to automatically translate such sentences from German into the target
languages French, Italian and English without subsequent proofreading or
correction. Having been operational for two winter seasons, we assess here the
quality of the produced texts based on two different surveys where participants
rated texts from real avalanche bulletins from both origins, the catalogue of
phrases versus manually written and translated texts. With a mean recognition
rate of 55%, users can hardly distinguish between thetwo types of texts, and
give very similar ratings with respect to their language quality. Overall, the
output from the catalogue system can be considered virtually equivalent to a
text written by avalanche forecasters and then manually translated by
professional translators. Furthermore, forecasters declared that all relevant
situations were captured by the system with sufficient accuracy. Forecaster's
working load did not change with the introduction of the catalogue: the extra
time to find matching sentences is compensated by the fact that they no longer
need to double-check manually translated texts. The reduction of daily
translation costs is expected to offset the initial development costs within a
few years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06939</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06939</id><created>2015-09-23</created><authors><author><keyname>Pasquale</keyname><forenames>Giulia</forenames></author><author><keyname>Mar</keyname><forenames>Tanis</forenames></author><author><keyname>Ciliberto</keyname><forenames>Carlo</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Enabling Depth-driven Visual Attention on the iCub Humanoid Robot:
  Instructions for Use and New Perspectives</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of depth perception in the interactions that humans have
within their nearby space is a well established fact. Consequently, it is also
well known that the possibility of exploiting good stereo information would
ease and, in many cases, enable, a large variety of attentional and interactive
behaviors on humanoid robotic platforms. However, the difficulty of computing
real-time and robust binocular disparity maps from moving stereo cameras often
prevents from relying on this kind of cue to visually guide robots' attention
and actions in real-world scenarios. The contribution of this paper is
two-fold: first, we show that the Efficient Large-scale Stereo Matching
algorithm (ELAS) by A. Geiger et al. 2010 for computation of the disparity map
is well suited to be used on a humanoid robotic platform as the iCub robot;
second, we show how, provided with a fast and reliable stereo system,
implementing relatively challenging visual behaviors in natural settings can
require much less effort. As a case of study we consider the common situation
where the robot is asked to focus the attention on one object close in the
scene, showing how a simple but effective disparity-based segmentation solves
the problem in this case. Indeed this example paves the way to a variety of
other similar applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06947</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06947</id><created>2015-09-23</created><authors><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Davies</keyname><forenames>Mike</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Recipes for stable linear embeddings from Hilbert spaces to
  $\mathbb{R}^m$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing a linear map from a Hilbert space
$\mathcal{H}$ (possibly infinite dimensional) to $\mathbb{R}^m$ that satisfies
a restricted isometry property (RIP) on an arbitrary signal model $\mathcal{S}
\subset \mathcal{H}$. We present a generic framework that handles a large class
of low-dimensional subsets but also unstructured and structured linear maps. We
provide a simple recipe to prove that a random linear map satisfies a general
RIP on $\mathcal{S}$ with high probability. We also describe a generic
technique to construct linear maps that satisfy the RIP. Finally, we detail how
to use our results in several examples, which allow us to recover and extend
many known compressive sampling results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06948</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06948</id><created>2015-09-23</created><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>Konrad</forenames></author></authors><title>Dynamic concurrent van Emde Boas array</title><categories>cs.DS</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing popularity of shared-memory multiprocessor machines has caused
significant changes in the design of concurrent software. In this approach, the
concurrently running threads communicate and synchronize with each other
through data structures in shared memory. Hence, the efficiency of these
structures is essential for the performance of concurrent applications. The
need to find new concurrent data structures prompted the author some time ago
to propose the cvEB array modeled on the van Emde Boas Tree structure as a
dynamic set alternative. This paper describes an improved version of that
structure - the dcvEB array (Dynamic Concurrent van Emde Boas Array). One of
the improvements involves memory usage optimization. This enhancement required
the design of a tree which grows and shrinks at both: the top (root) and the
bottom (leaves) level. Another enhancement concerns the successor (and
predecessor) search strategy. The tests performed seem to confirm the high
performance of the dcvEB array. They are especially visible when the range of
keys is significantly larger than the number of elements in the collection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06957</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06957</id><created>2015-09-23</created><authors><author><keyname>Hyv&#xf6;nen</keyname><forenames>Ville</forenames></author><author><keyname>Pitk&#xe4;nen</keyname><forenames>Teemu</forenames></author><author><keyname>Tasoulis</keyname><forenames>Sotiris</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Roos</keyname><forenames>Teemu</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author></authors><title>Fast k-NN search</title><categories>stat.ML cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random projection trees have proven to be effective for approximate nearest
neighbor searches in high dimensional spaces where conventional methods are not
applicable due to excessive usage of memory and computational time. We show
that building multiple trees on the same data can improve the performance even
further, without significantly increasing the total computational cost of
queries when executed in a modern parallel computing environment. Our
experiments identify suitable parameter values to achieve accurate searches
with extremely fast query times, while also retaining a feasible complexity for
index construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06969</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06969</id><created>2015-09-23</created><authors><author><keyname>Liu</keyname><forenames>Jia</forenames></author><author><keyname>Sheng</keyname><forenames>Min</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author></authors><title>End-to-end delay modeling in buffer-limited MANETs: a general
  theoretical framework</title><categories>cs.IT cs.NI cs.PF math.IT</categories><doi>10.1109/TWC.2015.2475258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on a class of important two-hop relay mobile ad hoc
networks (MANETs) with limited-buffer constraint and any mobility model that
leads to the uniform distribution of the locations of nodes in steady state,
and develops a general theoretical framework for the end-to-end (E2E) delay
modeling there. We first combine the theories of Fixed-Point,
Quasi-Birth-and-Death process and embedded Markov chain to model the limiting
distribution of the occupancy states of a relay buffer, and then apply the
absorbing Markov chain theory to characterize the packet delivery process, such
that a complete theoretical framework is developed for the E2E delay analysis.
With the help of this framework, we derive a general and exact expression for
the E2E delay based on the modeling of both packet queuing delay and delivery
delay. To demonstrate the application of our framework, case studies are
further provided under two network scenarios with different MAC protocols to
show how the E2E delay can be analytically determined for a given network
scenario. Finally, we present extensive simulation and numerical results to
illustrate the efficiency of our delay analysis as well as the impacts of
network parameters on delay performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06983</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06983</id><created>2015-09-23</created><updated>2015-09-24</updated><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Fritz</keyname><forenames>Adrian</forenames></author><author><keyname>Wieseke</keyname><forenames>Nicolas</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Techniques for the Cograph Editing Problem: Module Merge is equivalent
  to Editing P4s</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cographs are graphs in which no four vertices induce a simple connected path
$P_4$. Cograph editing is to find for a given graph $G = (V,E)$ a set of at
most $k$ edge additions and deletions that transform $G$ into a cograph. This
combinatorial optimization problem is NP-hard. It has, recently found
applications in the context of phylogenetics, hence good heuristics are of
practical importance.
  It is well-known that the cograph editing problem can be solved independently
on the so-called strong prime modules of the modular decomposition of $G$. We
show here that editing the induced $P_4$'s of a given graph is equivalent to
resolving strong prime modules by means of a newly defined merge operation on
the submodules. This observation leads to a new exact algorithm for the cograph
editing problem that can be used as a starting point for the construction of
novel heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06984</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06984</id><created>2015-09-23</created><authors><author><keyname>Bannach</keyname><forenames>Max</forenames></author><author><keyname>Stockhusen</keyname><forenames>Christoph</forenames></author><author><keyname>Tantau</keyname><forenames>Till</forenames></author></authors><title>Fast Parallel Fixed-Parameter Algorithms via Color Coding</title><categories>cs.CC cs.DS</categories><acm-class>F.1.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fixed-parameter algorithms have been successfully applied to solve numerous
difficult problems within acceptable time bounds on large inputs. However, most
fixed-parameter algorithms are inherently \emph{sequential} and, thus, make no
use of the parallel hardware present in modern computers. We show that parallel
fixed-parameter algorithms do not only exist for numerous parameterized
problems from the literature -- including vertex cover, packing problems,
cluster editing, cutting vertices, finding embeddings, or finding matchings --
but that there are parallel algorithms working in \emph{constant} time or at
least in time \emph{depending only on the parameter} (and not on the size of
the input) for these problems. Phrased in terms of complexity classes, we place
numerous natural parameterized problems in parameterized versions of AC$^0$. On
a more technical level, we show how the \emph{color coding} method can be
implemented in constant time and apply it to embedding problems for graphs of
bounded tree-width or tree-depth and to model checking first-order formulas in
graphs of bounded degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06991</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06991</id><created>2015-09-23</created><authors><author><keyname>Chawathaworncharoen</keyname><forenames>Varat</forenames></author><author><keyname>Visoottiviseth</keyname><forenames>Vasaka</forenames></author><author><keyname>Takano</keyname><forenames>Ryousei</forenames></author></authors><title>Feasibility Evaluation of 6LoWPAN over Bluetooth Low Energy</title><categories>cs.NI</categories><comments>4 pages, PRAGMA Workshop on International Clouds for Data Science
  (PRAGMA-ICDS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IPv6 over Low power Wireless Personal Area Network (6LoWPAN) is an emerging
technology to enable ubiquitous IoT services. However, there are very few
studies of the performance evaluation on real hardware environments. This paper
demonstrates the feasibility of 6LoWPAN through conducting a preliminary
performance evaluation of a commodity hardware environment, including Bluetooth
Low Energy (BLE) network, Raspberry Pi, and a laptop PC. Our experimental
results show that the power consumption of 6LoWPAN over BLE is one-tenth lower
than that of IP over WiFi; the performance significantly depends on the
distance between devices and the message size; and the communication completely
stops when bursty traffic transfers. This observation provides our optimistic
conclusions on the feasibility of 6LoWPAN although the maturity of
implementations is a remaining issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.06992</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.06992</id><created>2015-09-23</created><authors><author><keyname>Bisoffi</keyname><forenames>Andrea</forenames></author><author><keyname>Forni</keyname><forenames>Fulvio</forenames></author><author><keyname>Da Lio</keyname><forenames>Mauro</forenames></author><author><keyname>Zaccarian</keyname><forenames>Luca</forenames></author></authors><title>Global results on reset-induced periodic trajectories of planar systems</title><categories>cs.SY math.DS</categories><comments>7 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the existence of asymptotically stable periodic trajectories induced
by reset feedback. The analysis is developed for a planar system. Casting the
problem into the hybrid setting, we show that a periodic orbit arises from the
balance between the energy dissipated during flows and the energy restored by
resets, at jumps. The stability of the periodic orbit is studied with hybrid
Lyapunov tools. The satisfaction of the so-called hybrid basic conditions
ensures the robustness of the asymptotic stability. Extensions of the approach
to more general mechanical systems are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07007</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07007</id><created>2015-09-23</created><authors><author><keyname>Annamalai</keyname><forenames>Chidambaram</forenames></author></authors><title>Finding Perfect Matchings in Bipartite Hypergraphs</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Haxell's condition is a natural hypergraph analog of Hall's condition, which
is a well-known necessary and sufficient condition for a bipartite graph to
admit a perfect matching. That is, when Haxell's condition holds it forces the
existence of a perfect matching in the bipartite hypergraph. Unlike in graphs,
however, there is no known polynomial time algorithm to find the hypergraph
perfect matching that is guaranteed to exist when Haxell's condition is
satisfied.
  We prove the existence of an efficient algorithm to find perfect matchings in
bipartite hypergraphs whenever a stronger version of Haxell's condition holds.
Our algorithm can be seen as a generalization of the classical Hungarian
algorithm for finding perfect matchings in bipartite graphs. The techniques we
use to achieve this result could be of use more generally in other
combinatorial problems on hypergraphs where disjointness structure is crucial,
e.g. Set Packing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07009</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07009</id><created>2015-09-23</created><updated>2016-01-28</updated><authors><author><keyname>Dai</keyname><forenames>Dengxin</forenames></author><author><keyname>Wang</keyname><forenames>Yujian</forenames></author><author><keyname>Chen</keyname><forenames>Yuhua</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Is Image Super-resolution Helpful for Other Vision Tasks?</title><categories>cs.CV</categories><comments>1. Super-Resolution Forest added 2. Scene Recognition task added 3.
  Title changed 4. More work cited, WACV 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the great advances made in the field of image super-resolution (ISR)
during the last years, the performance has merely been evaluated perceptually.
Thus, it is still unclear whether ISR is helpful for other vision tasks. In
this paper, we present the first comprehensive study and analysis of the
usefulness of ISR for other vision applications. In particular, six ISR methods
are evaluated on four popular vision tasks, namely edge detection, semantic
image segmentation, digit recognition, and scene recognition. We show that
applying ISR to input images of other vision systems does improve their
performance when the input images are of low-resolution. We also study the
correlation between four standard perceptual evaluation criteria (namely PSNR,
SSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments
show that they correlate well with each other in general, but perceptual
criteria are still not accurate enough to be used as full proxies for the
usefulness. We hope this work will inspire the community to evaluate ISR
methods also in real vision applications, and to adopt ISR as a pre-processing
step of other vision tasks if the resolution of their input images is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07029</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07029</id><created>2015-09-23</created><authors><author><keyname>Lo</keyname><forenames>Yuan-Hsun</forenames></author><author><keyname>Zhang</keyname><forenames>Yijin</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author><author><keyname>Fu</keyname><forenames>Hung-Lin</forenames></author></authors><title>The Global Packing Number for an Optical Network</title><categories>cs.DM</categories><comments>23 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global packing number problem arises from the investigation of optimal
wavelength allocation in an optical network that employs Wavelength Division
Multiplexing (WDM). Consider an optical network that is represented by a
connected, simple graph $G$. We assume all communication channels are
bidirectional, so that all links and paths are undirected. It follows that
there are ${|G|\choose 2}$ distinct node pairs associated with $G$, where $|G|$
is the number of nodes in $G$. A path system $\mathcal{P}$ of $G$ consists of
${|G|\choose 2}$ paths, one path to connect each of the node pairs. The global
packing number of a path system $\mathcal{P}$, denoted by
$\Phi(G,\mathcal{P})$, is the minimum integer $k$ to guarantee the existence of
a mapping $\omega:\mathcal{P}\to\{1,2,\ldots,k\}$, such that
$\omega(P)\neq\omega(P')$ if $P$ and $P'$ have common edge(s). The global
packing number of $G$, denoted by $\Phi(G)$, is defined to be the minimum
$\Phi(G,\mathcal{P})$ among all possible path systems $\mathcal{P}$. If there
is no wavelength conversion along any optical transmission path for any node
pair in the network, the global packing number signifies the minimum number of
wavelengths required to support simultaneous communication for all pairs in the
network.
  In this paper, the focus is on ring networks, so that $G$ is a cycle.
Explicit formulas for the global packing number of a cycle is derived. The
investigation is further extended to chain networks. A path system,
$\mathcal{P}$, that enjoys $\Phi(G,\mathcal{P})=\Phi(G)$ is called ideal. A
characterization of ideal path systems is also presented. We also describe an
efficient heuristic algorithm to assign wavelengths that can be applied to a
general network with more complicated traffic load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07032</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07032</id><created>2015-09-23</created><authors><author><keyname>Deijfen</keyname><forenames>Maria</forenames></author><author><keyname>Lindholm</keyname><forenames>Mathias</forenames></author></authors><title>Growing networks with preferential addition and deletion of edges</title><categories>physics.soc-ph cs.SI math.PR</categories><journal-ref>Physica A 388, 4297-4303 (2009)</journal-ref><doi>10.1016/j.physa.2009.06.032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A preferential attachment model for a growing network incorporating deletion
of edges is studied and the expected asymptotic degree distribution is
analyzed. At each time step $t=1,2,\ldots$, with probability $\pi_1&gt;0$ a new
vertex with one edge attached to it is added to the network and the edge is
connected to an existing vertex chosen proportionally to its degree, with
probability $\pi_2$ a vertex is chosen proportionally to its degree and an edge
is added between this vertex and a randomly chosen other vertex, and with
probability $\pi_3=1-\pi_1-\pi_2&lt;1/2$ a vertex is chosen proportionally to its
degree and a random edge of this vertex is deleted. The model is intended to
capture a situation where high-degree vertices are more dynamic than low-degree
vertices in the sense that their connections tend to be changing. A recursion
formula is derived for the expected asymptotic fraction $p_k$ of vertices with
degree $k$, and solving this recursion reveals that, for $\pi_3&lt;1/3$, we have
$p_k\sim k^{-(3-7\pi_3)/(1-3\pi_3)}$, while, for $\pi_3&gt;1/3$, the fraction
$p_k$ decays exponentially at rate $(\pi_1+\pi_2)/2\pi_3$. There is hence a
non-trivial upper bound for how much deletion the network can incorporate
without loosing the power-law behavior of the degree distribution. The
analytical results are supported by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07035</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07035</id><created>2015-09-23</created><updated>2015-09-24</updated><authors><author><keyname>Jimenez-Romero</keyname><forenames>Cristian</forenames></author><author><keyname>Sousa-Rodrigues</keyname><forenames>David</forenames></author><author><keyname>Johnson</keyname><forenames>Jeffrey H.</forenames></author></authors><title>Designing Behaviour in Bio-inspired Robots Using Associative Topologies
  of Spiking-Neural-Networks</title><categories>cs.RO cs.AI cs.NE</categories><comments>Paper submitted to the BICT 2015 Conference in New York City, United
  States</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study explores the design and control of the behaviour of agents and
robots using simple circuits of spiking neurons and Spike Timing Dependent
Plasticity (STDP) as a mechanism of associative and unsupervised learning.
Based on a &quot;reward and punishment&quot; classical conditioning, it is demonstrated
that these robots learnt to identify and avoid obstacles as well as to identify
and look for rewarding stimuli. Using the simulation and programming
environment NetLogo, a software engine for the Integrate and Fire model was
developed, which allowed us to monitor in discrete time steps the dynamics of
each single neuron, synapse and spike in the proposed neural networks. These
spiking neural networks (SNN) served as simple brains for the experimental
robots. The Lego Mindstorms robot kit was used for the embodiment of the
simulated agents. In this paper the topological building blocks are presented
as well as the neural parameters required to reproduce the experiments. This
paper summarizes the resulting behaviour as well as the observed dynamics of
the neural circuits. The Internet-link to the NetLogo code is included in the
annex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07036</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07036</id><created>2015-09-23</created><authors><author><keyname>Rogers</keyname><forenames>David M.</forenames></author></authors><title>Towards a Direct, By-Need Evaluator for Dependently Typed Languages</title><categories>cs.PL</categories><comments>Submitted Version, 8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a C-language implementation of the lambda-pi calculus by extending
the (call-by-need) stack machine of Ariola, Chang and Felleisen to hold types,
using a typeless- tagless- final interpreter strategy. It has the advantage of
expressing all operations as folds over terms, including by-need evaluation,
recovery of the initial syntax-tree encoding for any term, and eliminating most
garbage-collection tasks. These are made possible by a disciplined approach to
handling the spine of each term, along with a robust stack-based API. Type
inference is not covered in this work, but also derives several advantages from
the present stack transformation. Timing and maximum stack space usage results
for executing benchmark problems are presented. We discuss how the design
choices for this interpreter allow the language to be used as a high-level
scripting language for automatic distributed parallel execution of common
scientific computing workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07038</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07038</id><created>2015-09-23</created><authors><author><keyname>Wang</keyname><forenames>Le-Zhi</forenames></author><author><keyname>Su</keyname><forenames>Ri-Qi</forenames></author><author><keyname>Huang</keyname><forenames>Zi-Gang</forenames></author><author><keyname>Wang</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Wenxu</forenames></author><author><keyname>Grebogi</keyname><forenames>Celso</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>Control and controllability of nonlinear dynamical networks: a
  geometrical approach</title><categories>q-bio.MN cs.SY nlin.CD physics.bio-ph</categories><comments>22 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of the recent interest and advances in linear controllability of
complex networks, controlling nonlinear network dynamics remains to be an
outstanding problem. We develop an experimentally feasible control framework
for nonlinear dynamical networks that exhibit multistability (multiple
coexisting final states or attractors), which are representative of, e.g., gene
regulatory networks (GRNs). The control objective is to apply parameter
perturbation to drive the system from one attractor to another, assuming that
the former is undesired and the latter is desired. To make our framework
practically useful, we consider RESTRICTED parameter perturbation by imposing
the following two constraints: (a) it must be experimentally realizable and (b)
it is applied only temporarily. We introduce the concept of ATTRACTOR NETWORK,
in which the nodes are the distinct attractors of the system, and there is a
directional link from one attractor to another if the system can be driven from
the former to the latter using restricted control perturbation. Introduction of
the attractor network allows us to formulate a controllability framework for
nonlinear dynamical networks: a network is more controllable if the underlying
attractor network is more strongly connected, which can be quantified. We
demonstrate our control framework using examples from various models of
experimental GRNs. A finding is that, due to nonlinearity, noise can
counter-intuitively facilitate control of the network dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07040</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07040</id><created>2015-09-23</created><updated>2015-10-07</updated><authors><author><keyname>Bu</keyname><forenames>Yuheng</forenames></author><author><keyname>Zou</keyname><forenames>Shaofeng</forenames></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Universal Outlying sequence detection For Continuous Observations</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The following detection problem is studied, in which there are $M$ sequences
of samples out of which one outlier sequence needs to be detected. Each typical
sequence contains $n$ independent and identically distributed (i.i.d.)
continuous observations from a known distribution $\pi$, and the outlier
sequence contains $n$ i.i.d. observations from an outlier distribution $\mu$,
which is distinct from $\pi$, but otherwise unknown. A universal test based on
KL divergence is built to approximate the maximum likelihood test, with known
$\pi$ and unknown $\mu$. A data-dependent partitions based KL divergence
estimator is employed. Such a KL divergence estimator is further shown to
converge to its true value exponentially fast when the density ratio satisfies
$0&lt;K_1\leq \frac{d\mu}{d\pi}\leq K_2$, where $K_1$ and $K_2$ are positive
constants, and this further implies that the test is exponentially consistent.
The performance of the test is compared with that of a recently introduced test
for this problem based on the machine learning approach of maximum mean
discrepancy (MMD). We identify regimes in which the KL divergence based test is
better than the MMD based test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07053</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07053</id><created>2015-09-23</created><authors><author><keyname>Gruber</keyname><forenames>Jakob</forenames></author></authors><title>Practical Concurrent Priority Queues</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priority queues are abstract data structures which store a set of key/value
pairs and allow efficient access to the item with the minimal (maximal) key.
Such queues are an important element in various areas of computer science such
as algorithmics (i.e. Dijkstra's shortest path algorithm) and operating system
(i.e. priority schedulers).
  The recent trend towards multiprocessor computing requires new
implementations of basic data structures which are able to be used concurrently
and scale well to a large number of threads. In particular, lock-free
structures promise superior scalability by avoiding the use of blocking
synchronization primitives.
  Concurrent priority queues have been extensively researched over the past
decades. In this paper, we discuss three major ideas within the field:
fine-grained locking employs multiple locks to avoid a single bottleneck within
the queue; SkipLists are search structures which use randomization and
therefore do not require elaborate reorganization schemes; and relaxed data
structures trade semantic guarantees for improved scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07062</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07062</id><created>2015-09-23</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Harrenstein</keyname><forenames>Paul</forenames></author><author><keyname>Lang</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Wooldridge</keyname><forenames>Michael</forenames></author></authors><title>Boolean Hedonic Games</title><categories>cs.GT cs.AI</categories><comments>This paper was orally presented at the Eleventh Conference on Logic
  and the Foundations of Game and Decision Theory (LOFT 2014) in Bergen,
  Norway, July 27-30, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study hedonic games with dichotomous preferences. Hedonic games are
cooperative games in which players desire to form coalitions, but only care
about the makeup of the coalitions of which they are members; they are
indifferent about the makeup of other coalitions. The assumption of dichotomous
preferences means that, additionally, each player's preference relation
partitions the set of coalitions of which that player is a member into just two
equivalence classes: satisfactory and unsatisfactory. A player is indifferent
between satisfactory coalitions, and is indifferent between unsatisfactory
coalitions, but strictly prefers any satisfactory coalition over any
unsatisfactory coalition. We develop a succinct representation for such games,
in which each player's preference relation is represented by a propositional
formula. We show how solution concepts for hedonic games with dichotomous
preferences are characterised by propositional formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07065</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07065</id><created>2015-09-23</created><authors><author><keyname>Chaki</keyname><forenames>Soumi</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Mohanty</keyname><forenames>William K.</forenames></author></authors><title>A Novel Pre-processing Scheme to Improve the Prediction of Sand Fraction
  from Seismic Attributes using Neural Networks</title><categories>cs.CE cs.LG</categories><comments>13 pages, volume 8, no 4, pp. 1808-1820, April 2015 in IEE Journal of
  Selected Topics in Applied Earth Observations and Remote Sensing, 2015</comments><doi>10.1109/JSTARS.2015.2404808</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel pre-processing scheme to improve the prediction
of sand fraction from multiple seismic attributes such as seismic impedance,
amplitude and frequency using machine learning and information filtering. The
available well logs along with the 3-D seismic data have been used to benchmark
the proposed pre-processing stage using a methodology which primarily consists
of three steps: pre-processing, training and post-processing. An Artificial
Neural Network (ANN) with conjugate-gradient learning algorithm has been used
to model the sand fraction. The available sand fraction data from the high
resolution well logs has far more information content than the low resolution
seismic attributes. Therefore, regularization schemes based on Fourier
Transform (FT), Wavelet Decomposition (WD) and Empirical Mode Decomposition
(EMD) have been proposed to shape the high resolution sand fraction data for
effective machine learning. The input data sets have been segregated into
training, testing and validation sets. The test results are primarily used to
check different network structures and activation function performances. Once
the network passes the testing phase with an acceptable performance in terms of
the selected evaluators, the validation phase follows. In the validation stage,
the prediction model is tested against unseen data. The network yielding
satisfactory performance in the validation stage is used to predict
lithological properties from seismic attributes throughout a given volume.
Finally, a post-processing scheme using 3-D spatial filtering is implemented
for smoothing the sand fraction in the volume. Prediction of lithological
properties using this framework is helpful for Reservoir Characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07074</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07074</id><created>2015-09-23</created><authors><author><keyname>Verma</keyname><forenames>Akhilesh K</forenames></author><author><keyname>Chaki</keyname><forenames>Soumi</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Mohanty</keyname><forenames>William K</forenames></author><author><keyname>Jenamani</keyname><forenames>Mamata</forenames></author></authors><title>Quantification of sand fraction from seismic attributes using
  Neuro-Fuzzy approach</title><categories>cs.CE cs.AI</categories><comments>Journal of Applied Geophysics, volume 111, page 141-155</comments><doi>10.1016/j.jappgeo.2014.10.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we illustrate the modeling of a reservoir property (sand
fraction) from seismic attributes namely seismic impedance, seismic amplitude,
and instantaneous frequency using Neuro-Fuzzy (NF) approach. Input dataset
includes 3D post-stacked seismic attributes and six well logs acquired from a
hydrocarbon field located in the western coast of India. Presence of thin sand
and shale layers in the basin area makes the modeling of reservoir
characteristic a challenging task. Though seismic data is helpful in
extrapolation of reservoir properties away from boreholes; yet, it could be
challenging to delineate thin sand and shale reservoirs using seismic data due
to its limited resolvability. Therefore, it is important to develop
state-of-art intelligent methods for calibrating a nonlinear mapping between
seismic data and target reservoir variables. Neural networks have shown its
potential to model such nonlinear mappings; however, uncertainties associated
with the model and datasets are still a concern. Hence, introduction of Fuzzy
Logic (FL) is beneficial for handling these uncertainties. More specifically,
hybrid variants of Artificial Neural Network (ANN) and fuzzy logic, i.e., NF
methods, are capable for the modeling reservoir characteristics by integrating
the explicit knowledge representation power of FL with the learning ability of
neural networks. The documented results in this study demonstrate acceptable
resemblance between target and predicted variables, and hence, encourage the
application of integrated machine learning approaches such as Neuro-Fuzzy in
reservoir characterization domain. Furthermore, visualization of the variation
of sand probability in the study area would assist in identifying placement of
potential wells for future drilling operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07075</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07075</id><created>2015-09-23</created><authors><author><keyname>Ahuja</keyname><forenames>Siddhant</forenames></author><author><keyname>Iles</keyname><forenames>Peter</forenames></author><author><keyname>Waslander</keyname><forenames>Steven L.</forenames></author></authors><title>3D Scan Registration using Curvelet Features in Planetary Environments</title><categories>cs.CV cs.RO</categories><comments>27 pages in Journal of Field Robotics, 2015</comments><doi>10.1002/rob.21616</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topographic mapping in planetary environments relies on accurate 3D scan
registration methods. However, most global registration algorithms relying on
features such as FPFH and Harris-3D show poor alignment accuracy in these
settings due to the poor structure of the Mars-like terrain and variable
resolution, occluded, sparse range data that is hard to register without some
a-priori knowledge of the environment. In this paper, we propose an alternative
approach to 3D scan registration using the curvelet transform that performs
multi-resolution geometric analysis to obtain a set of coefficients indexed by
scale (coarsest to finest), angle and spatial position. Features are detected
in the curvelet domain to take advantage of the directional selectivity of the
transform. A descriptor is computed for each feature by calculating the 3D
spatial histogram of the image gradients, and nearest neighbor based matching
is used to calculate the feature correspondences. Correspondence rejection
using Random Sample Consensus identifies inliers, and a locally optimal
Singular Value Decomposition-based estimation of the rigid-body transformation
aligns the laser scans given the re-projected correspondences in the metric
space. Experimental results on a publicly available data-set of planetary
analogue indoor facility, as well as simulated and real-world scans from Neptec
Design Group's IVIGMS 3D laser rangefinder at the outdoor CSA Mars yard
demonstrates improved performance over existing methods in the challenging
sparse Mars-like terrain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07076</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07076</id><created>2015-09-23</created><authors><author><keyname>Amanatidis</keyname><forenames>Georgios</forenames></author><author><keyname>Green</keyname><forenames>Bradley</forenames></author><author><keyname>Mihail</keyname><forenames>Milena</forenames></author></authors><title>Graphic Realizations of Joint-Degree Matrices</title><categories>cs.DM</categories><comments>Unpublished manuscript of 2009</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce extensions and modifications of the classical
degree sequence graphic realization problem studied by Erd\H{o}s-Gallai and
Havel-Hakimi, as well as of the corresponding connected graphic realization
version. We define the joint-degree matrix graphic (resp. connected graphic)
realization problem, where in addition to the degree sequence, the exact number
of desired edges between vertices of different degree classes is also
specified. We give necessary and sufficient conditions, and polynomial time
decision and construction algorithms for the graphic and connected graphic
realization problems. These problems arise naturally in the current topic of
graph modeling for complex networks. From the technical point of view, the
joint-degree matrix realization algorithm is straightforward. However, the
connected joint-degree matrix realization algorithm involves a novel recursive
search of suitable local graph modifications. Also, we outline directions for
further work of both theoretical and practical interest. In particular, we give
a Markov chain which converges to the uniform distribution over all
realizations. We show that the underline state space is connected, and we leave
the question of the mixing rate open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07079</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07079</id><created>2015-09-23</created><authors><author><keyname>Chaki</keyname><forenames>Soumi</forenames></author><author><keyname>Verma</keyname><forenames>Akhilesh K</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Mohanty</keyname><forenames>William K</forenames></author><author><keyname>Jenamani</keyname><forenames>Mamata</forenames></author></authors><title>Well Tops Guided Prediction of Reservoir Properties using Modular Neural
  Network Concept A Case Study from Western Onshore, India</title><categories>cs.NE cs.CE</categories><comments>in Journal of Petroleum Science and Engineering, 2014</comments><doi>10.1016/j.petrol.2014.06.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a complete framework consisting pre-processing, modeling,
and post-processing stages to carry out well tops guided prediction of a
reservoir property (sand fraction) from three seismic attributes (seismic
impedance, instantaneous amplitude, and instantaneous frequency) using the
concept of modular artificial neural network (MANN). The data set used in this
study comprising three seismic attributes and well log data from eight wells,
is acquired from a western onshore hydrocarbon field of India. Firstly, the
acquired data set is integrated and normalized. Then, well log analysis and
segmentation of the total depth range into three different units (zones)
separated by well tops are carried out. Secondly, three different networks are
trained corresponding to three different zones using combined data set of seven
wells and then trained networks are validated using the remaining test well.
The target property of the test well is predicted using three different tuned
networks corresponding to three zones; and then the estimated values obtained
from three different networks are concatenated to represent the predicted log
along the complete depth range of the testing well. The application of multiple
simpler networks instead of a single one improves the prediction accuracy in
terms of performance metrics such as correlation coefficient, root mean square
error, absolute error mean and program execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07087</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07087</id><created>2015-09-23</created><authors><author><keyname>Gan</keyname><forenames>Zhe</forenames></author><author><keyname>Li</keyname><forenames>Chunyuan</forenames></author><author><keyname>Henao</keyname><forenames>Ricardo</forenames></author><author><keyname>Carlson</keyname><forenames>David</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Deep Temporal Sigmoid Belief Networks for Sequence Modeling</title><categories>stat.ML cs.LG</categories><comments>to appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep dynamic generative models are developed to learn sequential dependencies
in time-series data. The multi-layered model is designed by constructing a
hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential
stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden
state, inherited from the previous SBNs in the sequence, and is used to
regulate its hidden bias. Scalable learning and inference algorithms are
derived by introducing a recognition model that yields fast sampling from the
variational posterior. This recognition model is trained jointly with the
generative model, by maximizing its variational lower bound on the
log-likelihood. Experimental results on bouncing balls, polyphonic music,
motion capture, and text streams show that the proposed approach achieves
state-of-the-art predictive performance, and has the capacity to synthesize
various sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07092</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07092</id><created>2015-09-23</created><authors><author><keyname>Harrison</keyname><forenames>Willie K.</forenames></author><author><keyname>Sarmento</keyname><forenames>Dinis</forenames></author><author><keyname>Vilela</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Gomes</keyname><forenames>Marco</forenames></author></authors><title>Analysis of Short Blocklength Codes for Secrecy</title><categories>cs.CR cs.IT math.IT</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide secrecy metrics applicable to physical-layer coding
techniques with finite blocklengths over Gaussian and fading wiretap channel
models. Our metrics go beyond some of the known practical secrecy measures,
such as bit error rate and security gap, so as to make lower bound
probabilistic guarantees on error rates over short blocklengths both preceding
and following a secrecy decoder. Our techniques are especially useful in cases
where application of traditional information-theoretic security measures is
either impractical or simply not yet understood. The metrics can aid both
practical system analysis, and practical system design for physical-layer
security codes. Furthermore, these new measures fill a void in the current
landscape of practical security measures for physical-layer security coding,
and may assist in the wide-scale adoption of physical-layer techniques for
security in real-world systems. We also show how the new metrics provide
techniques for reducing realistic channel models to simpler discrete memoryless
wiretap channel equivalents over which existing secrecy code designs may
achieve information-theoretic security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07093</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07093</id><created>2015-09-23</created><authors><author><keyname>Nova</keyname><forenames>David</forenames></author><author><keyname>Estevez</keyname><forenames>Pablo A.</forenames></author></authors><title>A review of learning vector quantization classifiers</title><categories>cs.LG astro-ph.IM cs.NE stat.ML</categories><comments>14 pages</comments><journal-ref>Neural Computing &amp; Applications, vol. 25, pp. 511-524, 2014</journal-ref><doi>10.1007/s00521-013-1535-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a review of the state of the art of Learning Vector
Quantization (LVQ) classifiers. A taxonomy is proposed which integrates the
most relevant LVQ approaches to date. The main concepts associated with modern
LVQ approaches are defined. A comparison is made among eleven LVQ classifiers
using one real-world and two artificial datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07106</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07106</id><created>2015-09-23</created><authors><author><keyname>Sanguinetti</keyname><forenames>Bruno</forenames></author><author><keyname>Martin</keyname><forenames>Anthony</forenames></author><author><keyname>Traverso</keyname><forenames>Giulia</forenames></author><author><keyname>Lavoie</keyname><forenames>Jonathan</forenames></author><author><keyname>Zbinden</keyname><forenames>Hugo</forenames></author></authors><title>Perfectly secure steganography: hiding information in the quantum noise
  of a photograph</title><categories>quant-ph cs.CR</categories><comments>5 pages, 3 figures + appendix : 5 pages, 6 figures</comments><journal-ref>Phys. Rev. A 93, 012336 (2016)</journal-ref><doi>10.1103/PhysRevA.93.012336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the quantum nature of light can be used to hide a secret message
within a photograph. Using this physical principle we achieve
information-theoretic secure steganography, which had remained elusive until
now. The protocol is such that the digital picture in which the secret message
is embedded is perfectly undistinguishable from an ordinary photograph. This
implies that, on a fundamental level, it is impossible to discriminate a
private communication from an exchange of photographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07107</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07107</id><created>2015-09-23</created><updated>2015-12-06</updated><authors><author><keyname>Vaughn</keyname><forenames>David</forenames></author><author><keyname>Justice</keyname><forenames>Derek</forenames></author></authors><title>On The Direct Maximization of Quadratic Weighted Kappa</title><categories>cs.LG</categories><comments>realized some inaccuracies, and some sloppy reasoning. Need some time
  to fix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, quadratic weighted kappa has been growing in popularity in
the machine learning community as an evaluation metric in domains where the
target labels to be predicted are drawn from integer ratings, usually obtained
from human experts. For example, it was the metric of choice in several recent,
high profile machine learning contests hosted on Kaggle :
https://www.kaggle.com/c/asap-aes , https://www.kaggle.com/c/asap-sas ,
https://www.kaggle.com/c/diabetic-retinopathy-detection . Yet, little is
understood about the nature of this metric, its underlying mathematical
properties, where it fits among other common evaluation metrics such as mean
squared error (MSE) and correlation, or if it can be optimized analytically,
and if so, how. Much of this is due to the cumbersome way that this metric is
commonly defined. In this paper we first derive an equivalent but much simpler,
and more useful, definition for quadratic weighted kappa, and then employ this
alternate form to address the above issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07109</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07109</id><created>2015-09-23</created><authors><author><keyname>Ghiasi-Shirazi</keyname><forenames>Sayed Kamaledin</forenames></author><author><keyname>Mohseni</keyname><forenames>Mahdi</forenames></author><author><keyname>Darvishan</keyname><forenames>Majid</forenames></author><author><keyname>Yousefzadeh</keyname><forenames>Reza</forenames></author></authors><title>RSCM Technology for Developing Runtime-Reconfigurable Telecommunication
  Applications</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runtime reconfiguration is a fundamental requirement of many
telecommunication applications which also has been addressed by management
standards like 3GPP TS 32.602, and NETCONF. Two basic commands considered by
these standards are CREATE and DELETE which operate on managed objects inside
an application. The available configuration management technologies, like JMX,
OSGi, and Fractal, do not support the CREATE and DELETE reconfiguration
commands of the telecommunication standards. In this paper, we introduce a
novel technology, called RSCM, for development of runtime reconfigurable
applications complying with the telecommunication standards. The RSCM subagent
takes the responsibility of loading the application from the configuration
file, executing the runtime reconfiguration commands (including CREATE and
DELETE), enforcing validity of the configuration state, and updating the
configuration file according to the latest reconfiguration commands. We exploit
the modular and object oriented features of the XML technology to extend the
object oriented and modular design principles to the configuration file. In
addition, a novel programming approach based on indirect referencing is
proposed which allows safe and almost immediate deletion of managed objects at
runtime. The RSCM technology has been successfully used in several commercial
telecommunication applications; including a SMS service center, a SMS gateway,
and a SMS hub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07127</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07127</id><created>2015-09-23</created><updated>2016-01-07</updated><authors><author><keyname>Junge</keyname><forenames>Marius</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Universal recovery from a decrease of quantum relative entropy</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v2: 23 pages, 1 figure, revised argument for the infinite-dimensional
  case and added application to approximate quantum error correction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data processing inequality states that the quantum relative entropy
between two states $\rho$ and $\sigma$ can never increase by applying the same
quantum channel $\mathcal{N}$ to both states. This inequality can be
strengthened with a remainder term in the form of a distance between $\rho$ and
the closest recovered state $(\mathcal{R} \circ \mathcal{N})(\rho)$, where
$\mathcal{R}$ is a recovery map with the property that $\sigma = (\mathcal{R}
\circ \mathcal{N})(\sigma)$. We show the existence of an explicit recovery map
that is universal in the sense that it depends only on $\sigma$ and the quantum
channel $\mathcal{N}$ to be reversed. This result gives an alternate,
information-theoretic characterization of the conditions for approximate
quantum error correction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07145</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07145</id><created>2015-09-23</created><authors><author><keyname>Kabatiansky</keyname><forenames>Grigory</forenames></author><author><keyname>Tavernier</keyname><forenames>Cedric</forenames></author><author><keyname>Vladuts</keyname><forenames>Serge</forenames></author></authors><title>On the Doubly Sparse Compressed Sensing Problem</title><categories>cs.IT math.IT</categories><comments>6 pages, IMACC2015 (accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new variant of the Compressed Sensing problem is investigated when the
number of measurements corrupted by errors is upper bounded by some value l but
there are no more restrictions on errors. We prove that in this case it is
enough to make 2(t+l) measurements, where t is the sparsity of original data.
Moreover for this case a rather simple recovery algorithm is proposed. An
analog of the Singleton bound from coding theory is derived what proves
optimality of the corresponding measurement matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07151</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07151</id><created>2015-09-19</created><authors><author><keyname>Pu</keyname><forenames>Cunlai</forenames></author><author><keyname>Li</keyname><forenames>Siyuan</forenames></author><author><keyname>Yang</keyname><forenames>Xianxia</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>Information transport in multiplex networks</title><categories>physics.soc-ph cs.NI</categories><comments>7figures</comments><doi>10.1016/j.physa.2015.12.057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study information transport in multiplex networks comprised
of two coupled subnetworks. The upper subnetwork, called the logical layer,
employs the shortest paths protocol to determine the logical paths for packets
transmission, while the lower subnetwork acts as the physical layer, in which
packets are delivered by the biased random walk mechanism characterized with a
parameter $\alpha$. Through simulation, we obtain the optimal $\alpha$
corresponding to the maximum network lifetime and the maximum number of the
arrival packets. Assortative coupling is better than the random coupling and
the disassortative coupling, since it achieves much better transmission
performances. Generally, the more homogeneous the lower subnetwork, the better
the transmission performances are, which is opposite for the upper subnetwork.
Finally, we propose an attack centrality for nodes based on the topological
information of both subnetworks, and further investigate the transmission
performances under targeted attacks. Our work helps to understand the spreading
and robustness issues of multiplex networks and provides some clues about the
designing of more efficient and robust routing architectures in communication
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07157</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07157</id><created>2015-09-23</created><authors><author><keyname>Gonz&#xe1;lez-Valiente</keyname><forenames>C. L.</forenames></author></authors><title>Analysis of the impact of studies published by Internext - Revista
  Eletr\^onica de Neg\'ocios Internacionais</title><categories>cs.DL</categories><journal-ref>Internext-Review of International Business; 2015, 10(2), 6-17</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a citation analysis of Internext-Review of International
Business to detect the impact caused by papers published for the period
2006-2013. The Publish or Perish (PoP) software is used, which retrieves
articles and citations from Google Scholar database. As part of the applied
indicators are: the distribution of authors by articles, citations per year,
citation vs. self-citation, journal's citable vs. non citable documents,
journal's cited vs. uncited documents, co-word analysis, and H Index. A total
of 131 articles were obtained for 153 citations made until June, 2014. Most
articles present multiple authorship. It is also detected an ascending line in
the citation. The Journal has very low levels of self-citation, showing that
most citing sources are Brazilian journals. The most cited articles have been
published in the early years (2006-2008); whose main topics are related with
the internationalization theory and strategy, the transaction analysis, and the
corporate governance. The Internext' H Index is 6 and the G Index is 9.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07164</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07164</id><created>2015-09-23</created><authors><author><keyname>Carpenter</keyname><forenames>Bob</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author><author><keyname>Brubaker</keyname><forenames>Marcus</forenames></author><author><keyname>Lee</keyname><forenames>Daniel</forenames></author><author><keyname>Li</keyname><forenames>Peter</forenames></author><author><keyname>Betancourt</keyname><forenames>Michael</forenames></author></authors><title>The Stan Math Library: Reverse-Mode Automatic Differentiation in C++</title><categories>cs.MS</categories><comments>96 pages, 9 figures</comments><acm-class>G.1.0; G.1.3; G.1.4; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As computational challenges in optimization and statistical inference grow
ever harder, algorithms that utilize derivatives are becoming increasingly more
important. The implementation of the derivatives that make these algorithms so
powerful, however, is a substantial user burden and the practicality of these
algorithms depends critically on tools like automatic differentiation that
remove the implementation burden entirely. The Stan Math Library is a C++,
reverse-mode automatic differentiation library designed to be usable, extensive
and extensible, efficient, scalable, stable, portable, and redistributable in
order to facilitate the construction and utilization of such algorithms.
  Usability is achieved through a simple direct interface and a cleanly
abstracted functional interface. The extensive built-in library includes
functions for matrix operations, linear algebra, differential equation solving,
and most common probability functions. Extensibility derives from a
straightforward object-oriented framework for expressions, allowing users to
easily create custom functions. Efficiency is achieved through a combination of
custom memory management, subexpression caching, traits-based metaprogramming,
and expression templates. Partial derivatives for compound functions are
evaluated lazily for improved scalability. Stability is achieved by taking care
with arithmetic precision in algebraic expressions and providing stable,
compound functions where possible. For portability, the library is
standards-compliant C++ (03) and has been tested for all major compilers for
Windows, Mac OS X, and Linux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07170</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07170</id><created>2015-09-23</created><authors><author><keyname>Di Cairano</keyname><forenames>Stefano</forenames></author></authors><title>Indirect-adaptive Model Predictive Control for Linear Systems with
  Polytopic Uncertainty</title><categories>cs.SY math.OC</categories><msc-class>93B51, 93D21, 93C40, 49N10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an indirect-adaptive model predictive control algorithm for
uncertain linear systems subject to constraints. The system is modeled as a
polytopic linear parameter varying system where the convex combination vector
is constant but unknown. Robust constraint satisfaction is obtained by
constraints enforcing a robust control invariant. The terminal cost and set are
constructed from a parameter-dependent Lyapunov function and the associated
control law. The proposed design ensures robust constraint satisfaction and
recursive feasibility, is input-to-state stable with respect to the parameter
estimation error and it only requires the online solution of quadratic
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07175</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07175</id><created>2015-09-23</created><updated>2016-02-13</updated><authors><author><keyname>Murdock</keyname><forenames>Jaimie</forenames></author><author><keyname>Allen</keyname><forenames>Colin</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Exploration and Exploitation of Victorian Science in Darwin's Reading
  Notebooks</title><categories>cs.CL cs.AI cs.CY cs.DL physics.soc-ph</categories><comments>revision to separate biographical and Bayesian epoch descriptions; 9
  pages, plus 10 pages supplemental and 3 pages references; 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Search in an environment with an uncertain distribution of resources involves
a trade-off between exploitation of past discoveries and further exploration.
This extends to information foraging, where a knowledge-seeker shifts between
reading in depth and studying new domains. We study this process in Charles
Darwin by modeling the full-text of books listed in his
chronologically-organized reading journals. We use the information-theoretic
Kullback-Liebler Divergence, or relative surprise, between books for both his
local (book-to-book) and global (book-to-past) reading decisions. Rather than a
pattern of surprise-minimization, corresponding to a pure exploitation
strategy, Darwin's behavior shifts from early exploitation to later
exploration, seeking unusually high levels of cognitive surprise relative to
previous eras. These shifts, detected by an unsupervised Bayesian model,
correlate with major intellectual epochs of his career as identified both by
traditional, qualitative scholarship and Darwin's own self-commentary. In
addition to quantifying Darwin's individual-level foraging, our methods allow
us to compare his consumption of texts with their publication order. We find
Darwin's consumption more exploratory than the culture's production, suggesting
that underneath gradual societal changes are the explorations of individual
synthesis and discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07178</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07178</id><created>2015-09-23</created><authors><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Yuan</keyname><forenames>Jianbo</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>America Tweets China: A Fine-Grained Analysis of the State and
  Individual Characteristics Regarding Attitudes towards China</title><categories>cs.SI</categories><comments>8 pages, 5 figures and 4 tables, IEEE BigData 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The U.S.-China relationship is arguably the most important bilateral
relationship in the 21st century. Typically it is measured through opinion
polls, for example, by Gallup and Pew Institute. In this paper, we propose a
new method to measure U.S.-China relations using data from Twitter, one of the
most popular social networks. Compared with traditional opinion polls, our
method has two distinctive advantages. First, our sample size is significantly
larger. National opinion polls have at most a few thousand samples. Our data
set has 724,146 samples. The large size of our data set enables us to perform
state level analysis, which so far even large opinion polls have left
unexplored. Second, our method can control for fixed state and date effects. We
first demonstrate the existence of inter-state and inter-day variances and then
control for these variances in our regression analysis. Empirically, our study
is able to replicate the stylized results from opinion polls as well as
generate new insights. At the state level, we find New York, Michigan, Indiana
and Arizona are the top four most China-friendly states. Wyoming, South Dakota,
Kansas and Nevada are most homogeneous. At the individual level, we find
attitudes towards China improve as an individual's Twitter experience grows
longer and more intense. We also find individuals of Chinese ethnicity are
statistically more China-friendly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07179</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07179</id><created>2015-09-23</created><authors><author><keyname>Chang</keyname><forenames>Kai-Wei</forenames></author><author><keyname>Upadhyay</keyname><forenames>Shyam</forenames></author><author><keyname>Chang</keyname><forenames>Ming-Wei</forenames></author><author><keyname>Srikumar</keyname><forenames>Vivek</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>IllinoisSL: A JAVA Library for Structured Prediction</title><categories>cs.LG cs.CL stat.ML</categories><comments>http://cogcomp.cs.illinois.edu/software/illinois-sl</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IllinoisSL is a Java library for learning structured prediction models. It
supports structured Support Vector Machines and structured Perceptron. The
library consists of a core learning module and several applications, which can
be executed from command-lines. Documentation is provided to guide users. In
Comparison to other structured learning libraries, IllinoisSL is efficient,
general, and easy to use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07181</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07181</id><created>2015-09-23</created><updated>2015-10-09</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Ghosh</keyname><forenames>Anirban</forenames></author></authors><title>Lower bounds on the dilation of plane spanners</title><categories>cs.CG math.CO</categories><comments>Section 5 revised; 26 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (I) We exhibit a set of 23 points in the plane that has dilation at least
$1.4308$, improving the previously best lower bound of $1.4161$ for the
worst-case dilation of plane spanners. (II) For every integer $n\geq13$, there
exists an $n$-element point set $S$ such that the degree 3 dilation of $S$
denoted by $\delta_0(S,3) \text{ equals } 1+\sqrt{3}=2.7321\ldots$ in the
domain of plane geometric spanners. In the same domain, we show that for every
integer $n\geq6$, there exists a an $n$-element point set $S$ such that the
degree 4 dilation of $S$ denoted by $\delta_0(S,4) \text{ equals } 1 +
\sqrt{(5-\sqrt{5})/2}=2.1755\ldots$ The previous best lower bound of $1.4161$
holds for any degree. (III) For every integer $n\geq6 $, there exists an
$n$-element point set $S$ such that the stretch factor of the greedy
triangulation of $S$ is at least $2.0268$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07199</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07199</id><created>2015-09-23</created><authors><author><keyname>Hoffmann</keyname><forenames>Philipp</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>Negotiation Games</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858. arXiv admin note:
  substantial text overlap with arXiv:1405.6820</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 31-42</journal-ref><doi>10.4204/EPTCS.193.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Negotiations, a model of concurrency with multi party negotiation as
primitive, have been recently introduced by J. Desel and J. Esparza. We
initiate the study of games for this model. We study coalition problems: can a
given coalition of agents force that a negotiation terminates (resp. block the
negotiation so that it goes on forever)?; can the coalition force a given
outcome of the negotiation? We show that for arbitrary negotiations the
problems are EXPTIME-complete. Then we show that for sound and deterministic or
even weakly deterministic negotiations the problems can be solved in PTIME.
Notice that the input of the problems is a negotiation, which can be
exponentially more compact than its state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07200</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07200</id><created>2015-09-23</created><authors><author><keyname>Wurm</keyname><forenames>Christian</forenames><affiliation>Universit&#xe4;t D&#xfc;sseldorf</affiliation></author></authors><title>Synchronous Subsequentiality and Approximations to Undecidable Problems</title><categories>cs.FL cs.CC</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 58-72</journal-ref><doi>10.4204/EPTCS.193.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the class of synchronous subsequential relations, a subclass of
the synchronous relations which embodies some properties of subsequential
relations. If we take relations of this class as forming the possible
transitions of an infinite automaton, then most decision problems (apart from
membership) still remain undecidable (as they are for synchronous and
subsequential rational relations), but on the positive side, they can be
approximated in a meaningful way we make precise in this paper. This might make
the class useful for some applications, and might serve to establish an
intermediate position in the trade-off between issues of expressivity and
(un)decidability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07201</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07201</id><created>2015-09-23</created><authors><author><keyname>Mancini</keyname><forenames>Toni</forenames><affiliation>Computer Science Department - Sapienza University of Rome</affiliation></author><author><keyname>Mari</keyname><forenames>Federico</forenames><affiliation>Computer Science Department - Sapienza University of Rome</affiliation></author><author><keyname>Massini</keyname><forenames>Annalisa</forenames><affiliation>Computer Science Department - Sapienza University of Rome</affiliation></author><author><keyname>Melatti</keyname><forenames>Igor</forenames><affiliation>Computer Science Department - Sapienza University of Rome</affiliation></author><author><keyname>Tronci</keyname><forenames>Enrico</forenames><affiliation>Computer Science Department - Sapienza University of Rome</affiliation></author></authors><title>Simulator Semantics for System Level Formal Verification</title><categories>cs.SE cs.SY</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 86-99</journal-ref><doi>10.4204/EPTCS.193.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many simulation based Bounded Model Checking approaches to System Level
Formal Verification (SLFV) have been devised. Typically such approaches exploit
the capability of simulators to save computation time by saving and restoring
the state of the system under simulation. However, even though such approaches
aim to (bounded) formal verification, as a matter of fact, the simulator
behaviour is not formally modelled and the proof of correctness of the proposed
approaches basically relies on the intuitive notion of simulator behaviour.
This gap makes it hard to check if the optimisations introduced to speed up the
simulation do not actually omit checking relevant behaviours of the system
under verification.
  The aim of this paper is to fill the above gap by presenting a formal
semantics for simulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07202</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07202</id><created>2015-09-23</created><authors><author><keyname>Dimitrova</keyname><forenames>Rayna</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Reachability Analysis of Reversal-bounded Automata on Series-Parallel
  Graphs</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 100-114</journal-ref><doi>10.4204/EPTCS.193.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensions to finite-state automata on strings, such as multi-head automata
or multi-counter automata, have been successfully used to encode many
infinite-state non-regular verification problems. In this paper, we consider a
generalization of automata-theoretic infinite-state verification from strings
to labeled series-parallel graphs. We define a model of non-deterministic,
2-way, concurrent automata working on series-parallel graphs and communicating
through shared registers on the nodes of the graph. We consider the following
verification problem: given a family of series-parallel graphs described by a
context-free graph transformation system (GTS), and a concurrent automaton over
series-parallel graphs, is some graph generated by the GTS accepted by the
automaton? The general problem is undecidable already for (one-way) multi-head
automata over strings. We show that a bounded version, where the automata make
a fixed number of reversals along the graph and use a fixed number of shared
registers is decidable, even though there is no bound on the sizes of
series-parallel graphs generated by the GTS. Our decidability result is based
on establishing that the number of context switches is bounded and on an
encoding of the computation of bounded concurrent automata to reduce the
emptiness problem to the emptiness problem for pushdown automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07203</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07203</id><created>2015-09-23</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames><affiliation>University of Genova</affiliation></author><author><keyname>Montali</keyname><forenames>Marco</forenames><affiliation>Free University Bolzano</affiliation></author></authors><title>Well Structured Transition Systems with History</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><acm-class>F.3.1, I.2.2</acm-class><journal-ref>EPTCS 193, 2015, pp. 115-128</journal-ref><doi>10.4204/EPTCS.193.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formal model of concurrent systems in which the history of a
computation is explicitly represented as a collection of events that provide a
view of a sequence of configurations. In our model events generated by
transitions become part of the system configurations leading to operational
semantics with historical data. This model allows us to formalize what is
usually done in symbolic verification algorithms. Indeed, search algorithms
often use meta-information, e.g., names of fired transitions, selected
processes, etc., to reconstruct (error) traces from symbolic state exploration.
The other interesting point of the proposed model is related to a possible new
application of the theory of well-structured transition systems (wsts). In our
setting wsts theory can be applied to formally extend the class of properties
that can be verified using coverability to take into consideration (ordered and
unordered) historical data. This can be done by using different types of
representation of collections of events and by combining them with wsts by
using closure properties of well-quasi orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07204</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07204</id><created>2015-09-23</created><authors><author><keyname>Hella</keyname><forenames>Lauri</forenames><affiliation>University of Tampere</affiliation></author><author><keyname>Stumpf</keyname><forenames>Johanna</forenames><affiliation>TU Darmstadt</affiliation></author></authors><title>The expressive power of modal logic with inclusion atoms</title><categories>cs.LO math.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 129-143</journal-ref><doi>10.4204/EPTCS.193.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modal inclusion logic is the extension of basic modal logic with inclusion
atoms, and its semantics is defined on Kripke models with teams. A team of a
Kripke model is just a subset of its domain. In this paper we give a complete
characterisation for the expressive power of modal inclusion logic: a class of
Kripke models with teams is definable in modal inclusion logic if and only if
it is closed under k-bisimulation for some integer k, it is closed under
unions, and it has the empty team property. We also prove that the same
expressive power can be obtained by adding a single unary nonemptiness operator
to modal logic. Furthermore, we establish an exponential lower bound for the
size of the translation from modal inclusion logic to modal logic with the
nonemptiness operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07205</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07205</id><created>2015-09-23</created><authors><author><keyname>Bouyer</keyname><forenames>Patricia</forenames><affiliation>LSV - CNRS and ENS Cachan - France</affiliation></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames><affiliation>LSV - CNRS and ENS Cachan - France</affiliation></author><author><keyname>Randour</keyname><forenames>Mickael</forenames><affiliation>LSV - CNRS and ENS Cachan - France</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University - Denmark</affiliation></author><author><keyname>Laursen</keyname><forenames>Simon</forenames><affiliation>Aalborg University - Denmark</affiliation></author></authors><title>Average-energy games</title><categories>cs.LO cs.FL cs.GT</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 1-15</journal-ref><doi>10.4204/EPTCS.193.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-player quantitative zero-sum games provide a natural framework to
synthesize controllers with performance guarantees for reactive systems within
an uncontrollable environment. Classical settings include mean-payoff games,
where the objective is to optimize the long-run average gain per action, and
energy games, where the system has to avoid running out of energy.
  We study average-energy games, where the goal is to optimize the long-run
average of the accumulated energy. We show that this objective arises naturally
in several applications, and that it yields interesting connections with
previous concepts in the literature. We prove that deciding the winner in such
games is in NP inter coNP and at least as hard as solving mean-payoff games,
and we establish that memoryless strategies suffice to win. We also consider
the case where the system has to minimize the average-energy while maintaining
the accumulated energy within predefined bounds at all times: this corresponds
to operating with a finite-capacity storage for energy. We give results for
one-player and two-player games, and establish complexity bounds and memory
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07206</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07206</id><created>2015-09-23</created><authors><author><keyname>Zimmermann</keyname><forenames>Martin</forenames><affiliation>Saarland University</affiliation></author></authors><title>Parameterized Linear Temporal Logics Meet Costs: Still not Costlier than
  LTL</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 144-157</journal-ref><doi>10.4204/EPTCS.193.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the investigation of parameterized extensions of Linear Temporal
Logic (LTL) that retain the attractive algorithmic properties of LTL: a
polynomial space model checking algorithm and a doubly-exponential time
algorithm for solving games. Alur et al. and Kupferman et al. showed that this
is the case for Parametric LTL (PLTL) and PROMPT-LTL respectively, which have
temporal operators equipped with variables that bound their scope in time.
Later, this was also shown to be true for Parametric LDL (PLDL), which extends
PLTL to be able to express all omega-regular properties.
  Here, we generalize PLTL to systems with costs, i.e., we do not bound the
scope of operators in time, but bound the scope in terms of the cost
accumulated during time. Again, we show that model checking and solving games
for specifications in PLTL with costs is not harder than the corresponding
problems for LTL. Finally, we discuss PLDL with costs and extensions to
multiple cost functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07207</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07207</id><created>2015-09-23</created><authors><author><keyname>Gazda</keyname><forenames>Maciej</forenames></author><author><keyname>Willemse</keyname><forenames>Tim A. C.</forenames></author></authors><title>Improvement in Small Progress Measures</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 158-171</journal-ref><doi>10.4204/EPTCS.193.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small Progress Measures is one of the classical parity game solving
algorithms. For games with n vertices, m edges and d different priorities, the
original algorithm computes the winning regions and a winning strategy for one
of the players in O(dm.(n/floor(d/2))^floor(d/2)) time. Computing a winning
strategy for the other player requires a re-run of the algorithm on that
player's winning region, thus increasing the runtime complexity to
O(dm.(n/ceil(d/2))^ceil(d/2)) for computing the winning regions and winning
strategies for both players. We modify the algorithm so that it derives the
winning strategy for both players in one pass. This reduces the upper bound on
strategy derivation for SPM to O(dm.(n/floor(d/2))^floor(d/2)). At the basis of
our modification is a novel operational interpretation of the least progress
measure that we provide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07208</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07208</id><created>2015-09-23</created><authors><author><keyname>Laroussinie</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LIAFA, Univ. Paris Diderot and CNRS, France</affiliation></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames><affiliation>LSV, ENS Cachan and CNRS, France</affiliation></author><author><keyname>Sangnier</keyname><forenames>Arnaud</forenames><affiliation>LIAFA, Univ. Paris Diderot and CNRS, France</affiliation></author></authors><title>ATLsc with partial observation</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><acm-class>D2.4; F3.1; F4.1</acm-class><journal-ref>EPTCS 193, 2015, pp. 43-57</journal-ref><doi>10.4204/EPTCS.193.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternating-time temporal logic with strategy contexts (ATLsc) is a powerful
formalism for expressing properties of multi-agent systems: it extends CTL with
strategy quantifiers, offering a convenient way of expressing both
collaboration and antagonism between several agents. Incomplete observation of
the state space is a desirable feature in such a framework, but it quickly
leads to undecidable verification problems. In this paper, we prove that
uniform incomplete observation (where all players have the same observation)
preserves decidability of the model-checking problem, even for very expressive
logics such as ATLsc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07209</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07209</id><created>2015-09-23</created><authors><author><keyname>Sin'ya</keyname><forenames>Ryoma</forenames><affiliation>Tokyo Institute of Technology</affiliation></author></authors><title>An Automata Theoretic Approach to the Zero-One Law for Regular
  Languages: Algorithmic and Logical Aspects</title><categories>cs.FL cs.LO</categories><comments>In Proceedings GandALF 2015, arXiv:1509.06858</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 193, 2015, pp. 172-185</journal-ref><doi>10.4204/EPTCS.193.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A zero-one language L is a regular language whose asymptotic probability
converges to either zero or one. In this case, we say that L obeys the zero-one
law. We prove that a regular language obeys the zero-one law if and only if its
syntactic monoid has a zero element, by means of Eilenberg's variety theoretic
approach. Our proof gives an effective automata characterisation of the
zero-one law for regular languages, and it leads to a linear time algorithm for
testing whether a given regular language is zero-one. In addition, we discuss
the logical aspects of the zero-one law for regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07211</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07211</id><created>2015-09-23</created><authors><author><keyname>Pang</keyname><forenames>Zaihu</forenames></author><author><keyname>Zhu</keyname><forenames>Fengyun</forenames></author></authors><title>Noise-Robust ASR for the third 'CHiME' Challenge Exploiting
  Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent
  Neural Network</title><categories>cs.SD cs.CL</categories><comments>The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages,
  1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Lingban entry to the third 'CHiME' speech separation and
recognition challenge is presented. A time-frequency masking based speech
enhancement front-end is proposed to suppress the environmental noise utilizing
multi-channel coherence and spatial cues. The state-of-the-art speech
recognition techniques, namely recurrent neural network based acoustic and
language modeling, state space minimum Bayes risk based discriminative acoustic
modeling, and i-vector based acoustic condition modeling, are carefully
integrated into the speech recognition back-end. To further improve the system
performance by fully exploiting the advantages of different technologies, the
final recognition results are obtained by lattice combination and rescoring.
Evaluations carried out on the official dataset prove the effectiveness of the
proposed systems. Comparing with the best baseline result, the proposed system
obtains consistent improvements with over 57% relative word error rate
reduction on the real-data test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07214</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07214</id><created>2015-09-23</created><authors><author><keyname>Bae</keyname><forenames>Sang Won</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author></authors><title>Computing the Geodesic Centers of a Polygonal Domain</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that computes the geodesic center of a given
polygonal domain. The running time of our algorithm is $O(n^{12+\epsilon})$ for
any $\epsilon&gt;0$, where $n$ is the number of corners of the input polygonal
domain. Prior to our work, only the very special case where a simple polygon is
given as input has been intensively studied in the 1980s, and an $O(n \log
n)$-time algorithm is known by Pollack et al. Our algorithm is the first one
that can handle general polygonal domains having one or more polygonal holes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07223</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07223</id><created>2015-09-23</created><authors><author><keyname>Liu</keyname><forenames>Chenxi</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author></authors><title>Secure Transmission for Relay Wiretap Channels in the Presence of
  Spatially Random Eavesdroppers</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures, accepted by IEEE Globecom 2015 Workshop on
  Trusted Communications with Physical Layer Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a secure transmission scheme for a relay wiretap channel, where a
source communicates with a destination via a decode-and-forward relay in the
presence of spatially random-distributed eavesdroppers. We assume that the
source is equipped with multiple antennas, whereas the relay, the destination,
and the eavesdroppers are equipped with a single antenna each. In the proposed
scheme, in addition to information signals, the source transmits artificial
noise signals in order to confuse the eavesdroppers. With the target of
maximizing the secrecy throughput of the relay wiretap channel, we derive a
closed-form expression for the transmission outage probability and an
easy-to-compute expression for the secrecy outage probability. Using these
expressions, we determine the optimal power allocation factor and wiretap code
rates that guarantee the maximum secrecy throughput, while satisfying a secrecy
outage probability constraint. Furthermore, we examine the impact of source
antenna number on the secrecy throughput, showing that adding extra transmit
antennas at the source brings about a significant increase in the secrecy
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07225</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07225</id><created>2015-09-23</created><authors><author><keyname>Sun</keyname><forenames>Chen</forenames></author><author><keyname>Gan</keyname><forenames>Chuang</forenames></author><author><keyname>Nevatia</keyname><forenames>Ram</forenames></author></authors><title>Automatic Concept Discovery from Parallel Text and Visual Corpora</title><categories>cs.CV</categories><comments>To appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans connect language and vision to perceive the world. How to build a
similar connection for computers? One possible way is via visual concepts,
which are text terms that relate to visually discriminative entities. We
propose an automatic visual concept discovery algorithm using parallel text and
visual corpora; it filters text terms based on the visual discriminative power
of the associated images, and groups them into concepts using visual and
semantic similarities. We illustrate the applications of the discovered
concepts using bidirectional image and sentence retrieval task and image
tagging task, and show that the discovered concepts not only outperform several
large sets of manually selected concepts significantly, but also achieves the
state-of-the-art performance in the retrieval task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07234</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07234</id><created>2015-09-24</created><authors><author><keyname>Ding</keyname><forenames>Yin</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Sparsity-based Correction of Exponential Artifacts</title><categories>cs.LG</categories><doi>10.1016/j.sigpro.2015.09.017</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper describes an exponential transient excision algorithm (ETEA). In
biomedical time series analysis, e.g., in vivo neural recording and
electrocorticography (ECoG), some measurement artifacts take the form of
piecewise exponential transients. The proposed method is formulated as an
unconstrained convex optimization problem, regularized by smoothed l1-norm
penalty function, which can be solved by majorization-minimization (MM) method.
With a slight modification of the regularizer, ETEA can also suppress more
irregular piecewise smooth artifacts, especially, ocular artifacts (OA) in
electroencephalog- raphy (EEG) data. Examples of synthetic signal, EEG data,
and ECoG data are presented to illustrate the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07236</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07236</id><created>2015-09-24</created><authors><author><keyname>Vinck</keyname><forenames>A. J. Han</forenames></author><author><keyname>Rouissi</keyname><forenames>F.</forenames></author><author><keyname>Shongwe</keyname><forenames>T.</forenames></author><author><keyname>Colen</keyname><forenames>G. R.</forenames></author><author><keyname>Oliveira</keyname><forenames>L. G.</forenames></author></authors><title>Impulse Noise and Narrowband PLC</title><categories>cs.IT math.IT</categories><comments>To be presented at WSPLC in Klagenfurt, Austria, September 21-22,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the influence of random- and periodic impulse noise on narrowband
(&lt; 500 kHz frequency band) Power Line Communications. We start with random
impulse noise and compare the properties of the measured impulse noise with the
common theoretical models like Middleton Class-A and Mixed Gaussian. The main
difference is the fact that the measured impulse noise is noise with memory for
the narrowband communication, whereas the theoretical models are memoryless.
Since the FFT can be seen as a randomizing, operation, the impulse noise is
assumed to appear as Gaussian noise after the FFT operation with a variance
that is determined by the energy of the impulses. We investigate the problem of
capacity loss due to this FFT operation. Another topic is that of periodical
noise. Since periodic in the time domain means periodic in the frequency
domain, this type of noise directly influences the output of the FFT for an
OFDM based transmission. Randomization is necessary to avoid bursty- or
dependent errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07238</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07238</id><created>2015-09-24</created><authors><author><keyname>Pritchard</keyname><forenames>David</forenames></author></authors><title>Frequency Distribution of Error Messages</title><categories>cs.SE cs.CY cs.PL stat.AP</categories><comments>To appear at PLATEAU 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which programming error messages are the most common? We investigate this
question, motivated by writing error explanations for novices. We consider
large data sets in Python and Java that include both syntax and run-time
errors. In both data sets, after grouping essentially identical messages, the
error message frequencies empirically resemble Zipf-Mandelbrot distributions.
We use a maximum-likelihood approach to fit the distribution parameters. This
gives one possible way to contrast languages or compilers quantitatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07244</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07244</id><created>2015-09-24</created><updated>2015-10-13</updated><authors><author><keyname>Andrews</keyname><forenames>Shawn</forenames></author><author><keyname>Hamarneh</keyname><forenames>Ghassan</forenames></author></authors><title>Multi-Region Probabilistic Dice Similarity Coefficient using the
  Aitchison Distance and Bipartite Graph Matching</title><categories>cs.CV</categories><comments>8 pages. 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Validation of image segmentation methods is of critical importance.
Probabilistic image segmentation is increasingly popular as it captures
uncertainty in the results. Image segmentation methods that support
multi-region (as opposed to binary) delineation are more favourable as they
capture interactions between the different objects in the image. The Dice
similarity coefficient (DSC) has been a popular metric for evaluating the
accuracy of automated or semi-automated segmentation methods by comparing their
results to the ground truth. In this work, we develop an extension of the DSC
to multi-region probabilistic segmentations (with unordered labels). We use
bipartite graph matching to establish label correspondences and propose two
functions that extend the DSC, one based on absolute probability differences
and one based on the Aitchison distance. These provide a robust and accurate
measure of multi-region probabilistic segmentation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07250</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07250</id><created>2015-09-24</created><authors><author><keyname>Wang</keyname><forenames>Taotao</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>Shi</keyname><forenames>Long</forenames></author></authors><title>A Lattice Approach for Optimal Rate-Diverse Wireless Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an encoding/decoding framework for achieving the optimal
channel capacity of the two-user broadcast channel where each user has message
targeted for the other user as side information. Since the link qualities of
the channels from the base station to users are different, the channel
capacities are different. This fact implies different data rates for different
users. For this scenario, the network coding technique can be employed to
improve the transmission efficiency. However, how to simultaneously achieve the
channel capacities for the two users is not straightforward. This problem is
referred to as the rate-diverse wireless network coding problem. In this paper,
we present a capacity-achieving framework based on linear structured nested
lattice codes for rate-diverse wireless network coding. The significance of the
proposed framework, besides its theoretical optimality, is its suggested design
principle for linear rate-diverse wireless network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07260</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07260</id><created>2015-09-24</created><updated>2015-09-25</updated><authors><author><keyname>Basu</keyname><forenames>Soumya</forenames></author><author><keyname>Lianeas</keyname><forenames>Thanasis</forenames></author><author><keyname>Nikolova</keyname><forenames>Evdokia</forenames></author></authors><title>New Complexity Results and Algorithms for the Minimum Tollbooth Problem</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inefficiency of the Wardrop equilibrium of nonatomic routing games can be
eliminated by placing tolls on the edges of a network so that the socially
optimal flow is induced as an equilibrium flow. A solution where the minimum
number of edges are tolled may be preferable over others due to its ease of
implementation in real networks. In this paper we consider the minimum
tollbooth (MINTB) problem, which seeks social optimum inducing tolls with
minimum support. We prove for single commodity networks with linear latencies
that the problem is NP-hard to approximate within a factor of $1.1377$ through
a reduction from the minimum vertex cover problem. Insights from network design
motivate us to formulate a new variation of the problem where, in addition to
placing tolls, it is allowed to remove unused edges by the social optimum. We
prove that this new problem remains NP-hard even for single commodity networks
with linear latencies, using a reduction from the partition problem. On the
positive side, we give the first exact polynomial solution to the MINTB problem
in an important class of graphs---series-parallel graphs. Our algorithm solves
MINTB by first tabulating the candidate solutions for subgraphs of the
series-parallel network and then combining them optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07266</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07266</id><created>2015-09-24</created><authors><author><keyname>Roy</keyname><forenames>Smita</forenames></author><author><keyname>Mondal</keyname><forenames>Samrat</forenames></author><author><keyname>Ekbal</keyname><forenames>Asif</forenames></author></authors><title>CRDT: Correlation Ratio Based Decision Tree Model for Healthcare Data
  Mining</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The phenomenal growth in the healthcare data has inspired us in investigating
robust and scalable models for data mining. For classification problems
Information Gain(IG) based Decision Tree is one of the popular choices.
However, depending upon the nature of the dataset, IG based Decision Tree may
not always perform well as it prefers the attribute with more number of
distinct values as the splitting attribute. Healthcare datasets generally have
many attributes and each attribute generally has many distinct values. In this
paper, we have tried to focus on this characteristics of the datasets while
analysing the performance of our proposed approach which is a variant of
Decision Tree model and uses the concept of Correlation Ratio(CR). Unlike IG
based approach, this CR based approach has no biasness towards the attribute
with more number of distinct values. We have applied our model on some
benchmark healthcare datasets to show the effectiveness of the proposed
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07276</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07276</id><created>2015-09-24</created><authors><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author><author><keyname>Kobayashi</keyname><forenames>Hirotada</forenames></author><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author><author><keyname>Tamate</keyname><forenames>Shuhei</forenames></author><author><keyname>Tani</keyname><forenames>Seiichiro</forenames></author></authors><title>Power of Quantum Computation with Few Clean Qubits</title><categories>quant-ph cs.CC</categories><comments>44 pages + cover page; the results in Section 8 are overlapping with
  the main results in arXiv:1409.6777</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the power of polynomial-time quantum computation in
which only a very limited number of qubits are initially clean in the |0&gt;
state, and all the remaining qubits are initially in the totally mixed state.
No initializations of qubits are allowed during the computation, nor
intermediate measurements. The main results of this paper are unexpectedly
strong error-reducible properties of such quantum computations. It is proved
that any problem solvable by a polynomial-time quantum computation with
one-sided bounded error that uses logarithmically many clean qubits can also be
solvable with exponentially small one-sided error using just two clean qubits,
and with polynomially small one-sided error using just one clean qubit. It is
further proved in the case of two-sided bounded error that any problem solvable
by such a computation with a constant gap between completeness and soundness
using logarithmically many clean qubits can also be solvable with exponentially
small two-sided error using just two clean qubits. If only one clean qubit is
available, the problem is again still solvable with exponentially small error
in one of the completeness and soundness and polynomially small error in the
other. As an immediate consequence of the above result for the two-sided-error
case, it follows that the TRACE ESTIMATION problem defined with fixed constant
threshold parameters is complete for the classes of problems solvable by
polynomial-time quantum computations with completeness 2/3 and soundness 1/3
using logarithmically many clean qubits and just one clean qubit. The
techniques used for proving the error-reduction results may be of independent
interest in themselves, and one of the technical tools can also be used to show
the hardness of weak classical simulations of one-clean-qubit computations
(i.e., DQC1 computations).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07278</identifier>
 <datestamp>2015-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07278</id><created>2015-09-24</created><authors><author><keyname>Gurski</keyname><forenames>Frank</forenames></author><author><keyname>Rethmann</keyname><forenames>Jochen</forenames></author><author><keyname>Wanke</keyname><forenames>Egon</forenames></author></authors><title>Integer Programming Models and Parameterized Algorithms for Controlling
  Palletizers</title><categories>cs.DS</categories><comments>27 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1307.1915</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the combinatorial FIFO Stack-Up problem, where bins have to be
stacked-up from conveyor belts onto pallets. This is done by palletizers or
palletizing robots. Given k sequences of labeled bins and a positive integer p,
the goal is to stack-up the bins by iteratively removing the first bin of one
of the k sequences and put it onto a pallet located at one of p stack-up
places. Each of these pallets has to contain bins of only one label, bins of
different labels have to be placed on different pallets. After all bins of one
label have been removed from the given sequences, the corresponding stack-up
place becomes available for a pallet of bins of another label. The FIFO
Stack-Up problem asks whether there is some processing of the sequences of bins
such that at most p stack-up places are used.
  In this paper, we introduce three digraph models, namely the processing
graph, the decision graph, and the sequence graph, and two linear programming
models for the problem. There is a processing of some list of sequences with at
most p stack-up places if and only if the sequence graph of this list has
directed pathwidth at most p-1. This connection implies that the FIFO Stack-Up
problem is NP-complete in general. Based on these characterizations we give
parameterized algorithms for various parameters which imply efficient solutions
for the FIFO Stack-Up problem restricted to small parameter values. We also
consider an approximation result for the problem. Our experimental study of
running times shows that a breadth first search solution on the decision graph
can be used to solve practical instances on several thousand bins of the FIFO
Stack-Up problem. Further we analyze two integer programming approaches
implemented in CPLEX and GLPK. As expected CPLEX can solve the instances much
faster than GLPK and our pallet solution approach is much better than the bin
solution approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07285</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07285</id><created>2015-09-24</created><updated>2016-01-13</updated><authors><author><keyname>Alstott</keyname><forenames>Jeff</forenames></author><author><keyname>Triulzi</keyname><forenames>Giorgio</forenames></author><author><keyname>Yan</keyname><forenames>Bowen</forenames></author><author><keyname>Luo</keyname><forenames>Jianxi</forenames></author></authors><title>Mapping Technology Space by Normalizing Technology Relatedness Networks</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>12 pages + 22 pages SI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology is a complex system, with technologies relating to each other in a
space that can be mapped as a network. The technology network's structure can
reveal properties of technologies and of human behavior, if it can be mapped
accurately. Technology networks have been made from patent data, using several
measures of proximity. These measures, however, are influenced by factors of
the patenting system that do not reflect technologies or their proximity. We
introduce a method to precisely normalize out multiple impinging factors in
patent data and extract the true signal of technological proximity, by
comparing the empirical proximity measures with what they would be in random
situations that remove the impinging factors. With this method, we created
technology networks, using data from 3.9 million patents. After normalization,
different measures of proximity became more correlated with each other,
approaching a single dimension of technological proximity. The normalized
technology networks were sparse, with few pairs of technology domains being
significantly related. The normalized network corresponded with human behavior:
we analyzed the patenting histories of 2.8 million inventors and found they
were more likely to invent in two different technology domains if the pair was
closely related in the technology network. We also analyzed 250 thousand firms'
patents and found that, in contrast, firms' inventive activities were only
modestly associated with the technology network; firms' portfolios combined
pairs of technology domains about twice as often as inventors. These results
suggest that controlling for impinging factors provides meaningful measures of
technological proximity for patent-based mapping of the technology space, and
that this map can be used to aid in technology innovation planning and
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07298</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07298</id><created>2015-09-24</created><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Universal Background Sparse Coding and Multilayer Bootstrap Network for
  Speaker Recognition</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In speaker recognition, Gaussian mixture model based universal background
model is a standard for extracting high-dimensional supervectors, and factor
analysis is a recent state-of-the-art method for reducing the high-dimensional
supervectors to low-dimensional representations. In this abstract paper, we
propose an alternative to the aforementioned techniques by multilayer bootstrap
networks (MBN). We first learn a high-dimensional sparse code for each frame by
a universal background MBN, then accumulate the sparse codes of the frames in a
session (a.k.a. utterance) into a single high-dimensional sparse supervector,
and finally reduce the session-level sparse supervectors to a low-dimensional
subspace by MBN where clustering is conducted. Our initial result on a
small-scale problem demonstrates the effectiveness of the proposed method. Note
that this abstract paper is used to \textit{protect the idea}. A full version
with large-scale experiments will be announced later.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07302</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07302</id><created>2015-09-24</created><updated>2015-10-09</updated><authors><author><keyname>Pedroni</keyname><forenames>Bruno U.</forenames></author><author><keyname>Das</keyname><forenames>Srinjoy</forenames></author><author><keyname>Arthur</keyname><forenames>John V.</forenames></author><author><keyname>Merolla</keyname><forenames>Paul A.</forenames></author><author><keyname>Jackson</keyname><forenames>Bryan L.</forenames></author><author><keyname>Modha</keyname><forenames>Dharmendra S.</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Kenneth</forenames></author><author><keyname>Cauwenberghs</keyname><forenames>Gert</forenames></author></authors><title>Mapping Generative Models onto a Network of Digital Spiking Neurons</title><categories>cs.NE q-bio.NC</categories><comments>A similar version of this manuscript has been submitted to IEEE
  TBioCAS for revision in October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic neural networks such as Restricted Boltzmann Machines (RBMs) have
been successfully used in applications ranging from speech recognition to image
classification. Inference and learning in these algorithms use a Markov Chain
Monte Carlo procedure called Gibbs sampling, where a logistic function forms
the kernel of this sampler. On the other side of the spectrum, neuromorphic
systems have shown great promise for low-power and parallelized cognitive
computing, but lack well-suited applications and automation procedures. In this
work, we propose a systematic method for bridging the RBM algorithm and digital
neuromorphic systems, with a generative pattern completion task as proof of
concept. For this, we first propose a method of producing the Gibbs sampler
using bio-inspired digital noisy integrate-and-fire neurons. Next, we describe
the process of mapping generative RBMs trained offline onto the IBM TrueNorth
neurosynaptic processor -- a low-power digital neuromorphic VLSI substrate.
Mapping these algorithms onto neuromorphic hardware presents unique challenges
in network connectivity and weight and bias quantization, which, in turn,
require architectural and design strategies for the physical realization.
Generative performance metrics are analyzed to validate the neuromorphic
requirements and to best select the neuron parameters for the model. Lastly, we
describe a design automation procedure which achieves optimal resource usage,
accounting for the novel hardware adaptations. This work represents the first
implementation of generative RBM inference on a neuromorphic VLSI substrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07308</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07308</id><created>2015-09-24</created><updated>2016-02-28</updated><authors><author><keyname>Vuli&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>Moens</keyname><forenames>Marie-Francine</forenames></author></authors><title>Bilingual Distributed Word Representations from Document-Aligned
  Comparable Data</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new model for learning bilingual word representations from
non-parallel document-aligned data. Following the recent advances in word
representation learning, our model learns dense real-valued word vectors, that
is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which
heavily relied on parallel sentence-aligned corpora and/or readily available
translation resources such as dictionaries, the article reveals that BWEs may
be learned solely on the basis of document-aligned comparable data without any
additional lexical resources nor syntactic information. We present a comparison
of our approach with previous state-of-the-art models for learning bilingual
word representations from comparable data that rely on the framework of
multilingual probabilistic topic modeling (MuPTM), as well as with
distributional local context-counting models. We demonstrate the utility of the
induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2)
suggesting word translations in context for polysemous words. Our simple yet
effective BWE-based models significantly outperform the MuPTM-based and
context-counting representation models from comparable data as well as prior
BWE-based models, and acquire the best reported results on both tasks for all
three tested language pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07313</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07313</id><created>2015-09-24</created><authors><author><keyname>Banerjee</keyname><forenames>Soumya</forenames></author></authors><title>Analysis of a Planetary Scale Scientific Collaboration Dataset Reveals
  Novel Patterns</title><categories>cs.SI cs.DL</categories><comments>Complex Systems Digital Campus 2015 World eConference Conference on
  Complex Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific collaboration networks are an important component of scientific
output and contribute significantly to expanding our knowledge and to the
economy and gross domestic product of nations. Here we examine a dataset from
the Mendeley scientific collaboration network. We analyze this data using a
combination of machine learning techniques and dynamical models. We find
interesting clusters of countries with different characteristics of
collaboration. Some of these clusters are dominated by developed countries that
have higher number of self connections compared with connections to other
countries. Another cluster is dominated by impoverished nations that have
mostly connections and collaborations with other countries but fewer self
connections. We also propose a complex systems dynamical model that explains
these characteristics. Our model explains how the scientific collaboration
networks of impoverished and developing nations change over time. We also find
interesting patterns in the behaviour of countries that may reflect past
foreign policies and contemporary geopolitics. Our model and analysis gives
insights and guidelines into how scientific development of developing countries
can be guided. This is intimately related to fostering economic development of
impoverished nations and creating a richer and more prosperous society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07314</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07314</id><created>2015-09-24</created><updated>2016-02-11</updated><authors><author><keyname>Roy</keyname><forenames>Spandan</forenames></author><author><keyname>Kar</keyname><forenames>Indra Narayan</forenames></author></authors><title>Adaptive-Robust Control of a Class of Uncertain Nonlinear Systems
  Utilizing Time-Delayed Input and Position Feedback</title><categories>cs.SY</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, the tracking control problem of a class of Euler-Lagrange
systems subjected to unknown uncertainties is addressed and an adaptive-robust
control strategy, christened as Time-Delayed Adaptive Robust Control (TARC) is
presented. The proposed control strategy approximates the unknown dynamics
through time-delayed logic, and the switching logic provides robustness against
the approximation error. The novel adaptation law for the switching gain, in
contrast to the conventional adaptive-robust control methodologies, does not
require either nominal modelling or predefined bounds of the uncertainties.
Also, the proposed adaptive law circumvents the overestimation-underestimation
problem of switching gain. The state derivatives in the proposed control law is
estimated from past data of the state to alleviate the measurement error when
state derivatives are not available directly. Moreover, a new stability notion
for time-delayed control is proposed which in turn provides a selection
criterion for controller gain and sampling interval. Experimental result of the
proposed methodology using a nonholonomic wheeled mobile robot (WMR) is
presented and improved tracking accuracy of the proposed control law is noted
compared to time-delayed control and adaptive sliding mode control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07315</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07315</id><created>2015-09-24</created><authors><author><keyname>Faulwasser</keyname><forenames>Timm</forenames></author><author><keyname>Korda</keyname><forenames>Milan</forenames></author><author><keyname>Jones</keyname><forenames>Colin N.</forenames></author><author><keyname>Bonvin</keyname><forenames>Dominique</forenames></author></authors><title>On Turnpike and Dissipativity Properties of Continuous-Time Optimal
  Control Problems</title><categories>cs.SY math.OC</categories><comments>Submitted</comments><msc-class>49N90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the relations between three different properties,
which are of importance in continuous-time optimal control problems:
dissipativity of the underlying dynamics with respect to a specific supply
rate, optimal operation at steady state, and the turnpike property. We show
that dissipativity with respect to a steady state implies optimal operation at
this steady state and the existence of a turnpike at the same steady state.
  We establish novel converse turnpike results, i.e., we show that the
existence of a turnpike at a steady state implies optimal operation at this
steady state and dissipativity with respect to this steady state. Finally, we
invoke an assumption on the uniqueness of the optimal steady state to establish
the equivalence of the three properties. We draw upon a numerical example to
illustrate our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07326</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07326</id><created>2015-09-24</created><authors><author><keyname>Potvin</keyname><forenames>Pascal</forenames></author><author><keyname>Bonja</keyname><forenames>Mario</forenames></author><author><keyname>Bailey</keyname><forenames>Gordon</forenames></author><author><keyname>Busnel</keyname><forenames>Pierre</forenames></author></authors><title>An IMS DSL Developed at Ericsson</title><categories>cs.SE cs.PL</categories><comments>19 pages, 2 figures, 1 table, oral presentation at SDL 2013:
  Model-Driven Dependability Engineering conference</comments><journal-ref>SDL 2013: Model-Driven Dependability Engineering Volume 7916 of
  the series Lecture Notes in Computer Science pp 144-162</journal-ref><doi>10.1007/978-3-642-38911-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present how we created a Domain Specific Language (DSL)
dedicated to IP Multimedia Subsystem (IMS) at Ericsson. First, we introduce IMS
and how developers are burdened by its complexity when integrating it in their
application. Then we describe the principles we followed to create our new IMS
DSL from its core in the Scala language to its syntax. We then present how we
integrated it in two existing projects and show how it can save time for
developers and how readable the syntax of the IMS DSL is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07329</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07329</id><created>2015-09-24</created><authors><author><keyname>Niu</keyname><forenames>Yong</forenames><affiliation>Oliver</affiliation></author><author><keyname>Gao</keyname><forenames>Chuhan</forenames><affiliation>Oliver</affiliation></author><author><keyname>Li</keyname><forenames>Yong</forenames><affiliation>Oliver</affiliation></author><author><keyname>Jin</keyname><forenames>Depeng</forenames><affiliation>Oliver</affiliation></author><author><keyname>Su</keyname><forenames>Li</forenames><affiliation>Oliver</affiliation></author><author><keyname>Dapeng</keyname><affiliation>Oliver</affiliation></author><author><keyname>Wu</keyname></author></authors><title>Boosting Spatial Reuse via Multiple Paths Multi-Hop Scheduling for
  Directional mmWave WPANs</title><categories>cs.NI</categories><comments>14 pages, 17 figures, to appear in IEEE Transactions on Vehicular
  Technology</comments><doi>10.1109/TVT.2015.2479633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With huge unlicensed bandwidth available in most parts of the world,
millimeter wave (mmWave) communications in the 60 GHz band has been considered
as one of the most promising candidates to support multi-gigabit wireless
services. Due to high propagation loss of mmWave channels, beamforming is
likely to become adopted as an essential technique. Consequently, transmission
in 60 GHz band is inherently directional. Directivity enables concurrent
transmissions (spatial reuse), which can be fully exploited to improve network
capacity. In this paper, we propose a multiple paths multi-hop scheduling
scheme, termed MPMH, for mmWave wireless personal area networks, where the
traffic across links of low channel quality is transmitted through multiple
paths of multiple hops to unleash the potential of spatial reuse. We formulate
the problem of multiple paths multi-hop scheduling as a mixed integer linear
program (MILP), which is generally NP-hard. To enable the implementation of the
multiple paths multi-hop transmission in practice, we propose a heuristic
scheme including path selection, traffic distribution, and multiple paths
multi-hop scheduling to efficiently solve the formulated problem. Finally,
through extensive simulations, we demonstrate MPMH achieves near-optimal
network performance in terms of transmission delay and throughput, and enhances
the network performance significantly compared with existing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07330</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07330</id><created>2015-09-24</created><updated>2015-09-29</updated><authors><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author><author><keyname>Rayaprolu</keyname><forenames>Gautam</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>The Storable Good Monopoly Problem with Indivisible Demand</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamic pricing problem faced by a monopolist who sells a
storable good - a good that can be stored for later consumption. In this
framework, the two major pricing mechanisms studied in the theoretic literature
are the price-commitment and the threat (no-commitment) mechanisms. We analyse
and compare these mechanisms in the setting where the good can be purchased in
indivisible atomic quantities and where demand is time-dependent. First, we
show that, given linear storage costs, the monopolist can compute an optimal
price-commitment strategy in polynomial time. Moreover, under such a strategy,
the consumers do not need to store units in order to anticipate price rises.
Second we show that, under a threat mechanism rather than a price-commitment
mechanism, (i) prices can be lower, (ii) profits can be higher, and (iii)
consumer surplus can be higher. This result is surprising, in that these three
facts are in complete contrast to the case of a monopolist for divisible
storable goods (Dudine et al., 2006). Third, we quantify exactly how much more
profitable a threat mechanism can be with respect to a price-commitment
mechanism. Specifically, for a market with $N$ consumers, a threat mechanism
can produce a multiplicative factor of $\Omega(log N)$ more profits than a
price-commitment mechanism, and this bound is tight. Again, this result is
slightly surprising. A special case of this model, is the durable good
monopolist model of Bagnoli et al. (1989). For a durable good monopolist, it
was recently shown (Berbeglia et al., 2014) that the profits of the
price-commitment and the threat mechanisms are always within an additive
constant. Finally, we consider extensions to the case where inventory storage
costs are concave.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07337</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07337</id><created>2015-09-24</created><authors><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author><author><keyname>Yuan</keyname><forenames>Chen</forenames></author></authors><title>A new class of rank-metric codes and their list decoding beyond the
  unique decoding radius</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared with classical block codes, efficient list decoding of rank-metric
codes seems more difficult. Although the list decodability of random
rank-metric codes and limits to list decodability have been completely
determined, little work on efficient list decoding rank-metric codes has been
done. The only known efficient list decoding of rank-metric codes $\mC$ gives
decoding radius up to the Singleton bound $1-R-\Ge$ with positive rate $R$ when
$\rho(\mC)$ is extremely small, i.e., $\Theta(\Ge^2)$ , where $\rho(\mC)$
denotes the ratio of the number of rows over the number of columns of $\mC$
\cite[STOC2013]{Guru2013}. It is commonly believed that list decoding of
rank-metric codes $\mC$ with not small constant ratio $\rho(\mC)$ is hard.
  The main purpose of the present paper is to explicitly construct a class of
rank-metric codes $\mC$ with not small constant ratio $\rho(\mC)$ and
efficiently list decode these codes with decoding radius beyond $(1-R)/2$. Our
key idea is to employ two-variable polynomials $f(x,y)$, where $f$ is
linearized in variable $x$ and the variable $y$ is used to &quot;fold&quot; the code. In
other words, rows are used to correct rank errors and columns are used to
&quot;fold&quot; the code to enlarge decoding radius. Apart from the above algebraic
technique, we have to prune down the list. The algebraic idea enables us to pin
down the messages into a structured subspace of dimension linear in the number
$n$ of columns. This &quot;periodic&quot; structure allows us to pre-encoding the
messages to prune down the list. More precisely, we use subspace design
introduced in \cite[STOC2013]{Guru2013} to get a deterministic algorithm with a
larger constant list size and employ hierarchical subspace-evasive sets
introduced in \cite[STOC2012]{Guru2012} to obtain a randomized algorithm with a
smaller constant list size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07340</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07340</id><created>2015-09-24</created><authors><author><keyname>Niu</keyname><forenames>Yong</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Gao</keyname><forenames>Chuhan</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Exploiting Device-to-Device Communications to Enhance Spatial Reuse for
  Popular Content Downloading in Directional mmWave Small Cells</title><categories>cs.NI</categories><comments>12 pages, to appear in IEEE Transactions on Vehicular Technology</comments><doi>10.1109/TVT.2015.2466656</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of mobile demand, small cells in millimeter wave
(mmWave) bands underlying the macrocell networks have attracted intense
interest from both academia and industry. MmWave communications in the 60 GHz
band are able to utilize the huge unlicensed bandwidth to provide multiple Gbps
transmission rates. In this case, device-to-device (D2D) communications in
mmWave bands should be fully exploited due to no interference with the
macrocell networks and higher achievable transmission rates. In addition, due
to less interference by directional transmission, multiple links including D2D
links can be scheduled for concurrent transmissions (spatial reuse). With the
popularity of content-based mobile applications, popular content downloading in
the small cells needs to be optimized to improve network performance and
enhance user experience. In this paper, we develop an efficient scheduling
scheme for popular content downloading in mmWave small cells, termed PCDS
(popular content downloading scheduling), where both D2D communications in
close proximity and concurrent transmissions are exploited to improve
transmission efficiency. In PCDS, a transmission path selection algorithm is
designed to establish multi-hop transmission paths for users, aiming at better
utilization of D2D communications and concurrent transmissions. After
transmission path selection, a concurrent transmission scheduling algorithm is
designed to maximize the spatial reuse gain. Through extensive simulations
under various traffic patterns, we demonstrate PCDS achieves near-optimal
performance in terms of delay and throughput, and also superior performance
compared with other existing protocols, especially under heavy load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07344</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07344</id><created>2015-09-24</created><authors><author><keyname>Hasnat</keyname><forenames>Md. Abul</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Bonnevay</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Jacques</keyname><forenames>Julien</forenames></author></authors><title>Opinion mining from twitter data using evolutionary multinomial mixture
  models</title><categories>cs.IR stat.ML</categories><comments>Submitted to the Annals of Applied Statistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image of an entity can be defined as a structured and dynamic representation
which can be extracted from the opinions of a group of users or population.
Automatic extraction of such an image has certain importance in political
science and sociology related studies, e.g., when an extended inquiry from
large-scale data is required. We study the images of two politically
significant entities of France. These images are constructed by analyzing the
opinions collected from a well known social media called Twitter. Our goal is
to build a system which can be used to automatically extract the image of
entities over time.
  In this paper, we propose a novel evolutionary clustering method based on the
parametric link among Multinomial mixture models. First we propose the
formulation of a generalized model that establishes parametric links among the
Multinomial distributions. Afterward, we follow a model-based clustering
approach to explore different parametric sub-models and select the best model.
For the experiments, first we use synthetic temporal data. Next, we apply the
method to analyze the annotated social media data. Results show that the
proposed method is better than the state-of-the-art based on the common
evaluation metrics. Additionally, our method can provide interpretation about
the temporal evolution of the clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07345</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07345</id><created>2015-09-24</created><authors><author><keyname>Beaude</keyname><forenames>Olivier</forenames></author><author><keyname>Wan</keyname><forenames>Cheng</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Composite charging games in networks of electric vehicles</title><categories>cs.GT math.OC</categories><comments>8 pages, 6 figures, keywords: EV charging - Electricity Distribution
  Networks - Composite game - Composite Equilibrium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important scenario for smart grids which encompass distributed electrical
networks is given by the simultaneous presence of aggregators and individual
consumers. In this work, an aggregator is seen as an entity (a coalition) which
is able to manage jointly the energy demand of a large group of consumers or
users. More precisely, the demand consists in charging an electrical vehicle
(EV) battery. The way the EVs user charge their batteries matters since it
strongly impacts the network, especially the distribution network costs (e.g.,
in terms of Joule losses or transformer ageing). Since the charging policy is
chosen by the users or the aggregators, the charging problem is naturally
distributed. It turns out that one of the tools suited to tackle this
heterogenous scenario has been introduced only recently namely, through the
notion of composite games. This paper exploits for the first time in the
literature of smart grids the notion of composite game and equilibrium. By
assuming a rectangular charging profile for an EV, a composite equilibrium
analysis is conducted, followed by a detailed analysis of a case study which
assumes three possible charging periods or time-slots. Both the provided
analytical and numerical results allow one to better understand the
relationship between the size (which is a measure) of the coalition and the
network sum-cost. In particular, a social dilemma, a situation where everybody
prefers unilaterally defecting to cooperating, while the consequence is the
worst for all, is exhibited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07349</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07349</id><created>2015-09-24</created><authors><author><keyname>Beaude</keyname><forenames>Olivier</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Hennebel</keyname><forenames>Martin</forenames></author></authors><title>Charging Games in Networks of Electrical Vehicles</title><categories>cs.GT</categories><comments>8 pages, 4 figures, keywords: Charging games - electrical vehicle -
  distribution networks - potential games - Nash equilibrium - price of anarchy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a static non-cooperative game formulation of the problem of
distributed charging in electrical vehicle (EV) networks is proposed. This
formulation allows one to model the interaction between several EV which are
connected to a common residential distribution transformer. Each EV aims at
choosing the time at which it starts charging its battery in order to minimize
an individual cost which is mainly related to the total power delivered by the
transformer, the location of the time interval over which the charging
operation is performed, and the charging duration needed for the considered EV
to have its battery fully recharged. As individual cost functions are assumed
to be memoryless, it is possible to show that the game of interest is always an
ordinal potential game. More precisely, both an atomic and nonatomic versions
of the charging game are considered. In both cases, equilibrium analysis is
conducted. In particular, important issues such as equilibrium uniqueness and
efficiency are tackled. Interestingly, both analytical and numerical results
show that the efficiency loss due to decentralization (e.g., when cost
functions such as distribution network Joule losses or life of residential
distribution transformers when no thermal inertia is assumed) induced by
charging is small and the corresponding &quot;efficiency&quot;, a notion close to the
Price of Anarchy, tends to one when the number of EV increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07360</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07360</id><created>2015-09-24</created><authors><author><keyname>Sultan</keyname><forenames>Mujahid</forenames></author><author><keyname>Miranskyy</keyname><forenames>Andriy</forenames></author></authors><title>Ordering stakeholder viewpoint concerns for holistic and incremental
  Enterprise Architecture: the W6H framework</title><categories>cs.SE</categories><comments>Due to ArXiv constraint 'The abstract field cannot be longer than
  1,920 characters', the abstract appearing here is shorter than the one in the
  manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Enterprise Architecture (EA) is a discipline which has evolved to
structure the business and its alignment with the IT systems. One of the
popular enterprise architecture frameworks is Zachman framework (ZF). This
framework focuses on describing the enterprise from six viewpoint perspectives
of the stakeholders. These six perspectives are based on English language
interrogatives 'what', 'where', 'who', 'when', 'why', and 'how' (thus the term
W5H Journalists and police investigators use the W5H to describe an event.
However, EA is not an event, creation and evolution of EA challenging.
Moreover, the ordering of viewpoints is not defined in the existing EA
frameworks, making data capturing process difficult. Our goals are to 1) assess
if W5H is sufficient to describe modern EA and 2) explore the ordering and
precedence among the viewpoint concerns. Method: we achieve our goals by
bringing tools from the Linguistics, focusing on a full set of English Language
interrogatives to describe viewpoint concerns and the inter-relationships and
dependencies among these. Application of these tools is validated using
pedagogical EA examples. Results: 1) We show that addition of the seventh
interrogative 'which' to the W5H set (we denote this extended set as W6H)
yields extra and necessary information enabling creation of holistic EA. 2) We
discover that particular ordering of the interrogatives, established by
linguists (based on semantic and lexical analysis of English language
interrogatives), define starting points and the order in which viewpoints
should be arranged for creating complete EA. 3) We prove that adopting W6H
enables creation of EA for iterative and agile SDLCs, e.g. Scrum. Conclusions:
We believe that our findings complete creation of EA using ZF by practitioners,
and provide theoreticians with tools needed to improve other EA frameworks,
e.g., TOGAF and DoDAF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07385</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07385</id><created>2015-09-24</created><updated>2015-10-01</updated><authors><author><keyname>Shaham</keyname><forenames>Uri</forenames></author><author><keyname>Cloninger</keyname><forenames>Alexander</forenames></author><author><keyname>Coifman</keyname><forenames>Ronald R.</forenames></author></authors><title>Provable approximation properties for deep neural networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss approximation of functions using deep neural nets. Given a
function $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, we
construct a sparsely-connected depth-4 neural network and bound its error in
approximating $f$. The size of the network depends on dimension and curvature
of the manifold $\Gamma$, the complexity of $f$, in terms of its wavelet
description, and only weakly on the ambient dimension $m$. Essentially, our
network computes wavelet functions, which are computed from Rectified Linear
Units (ReLU)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07395</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07395</id><created>2015-09-24</created><updated>2016-01-08</updated><authors><author><keyname>Zahavi</keyname><forenames>Eitan</forenames></author><author><keyname>Shpiner</keyname><forenames>Alex</forenames></author><author><keyname>Rottenstreich</keyname><forenames>Ori</forenames></author><author><keyname>Kolodny</keyname><forenames>Avinoam</forenames></author><author><keyname>Keslassy</keyname><forenames>Isaac</forenames></author></authors><title>Links as a Service (LaaS): Feeling Alone in the Shared Cloud</title><categories>cs.DC cs.NI</categories><comments>CCIT Report 888 September 2015, EE Pub No. 1845, Technion, Israel</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most demanding tenants of shared clouds require complete isolation from
their neighbors, in order to guarantee that their application performance is
not affected by other tenants. Unfortunately, while shared clouds can offer an
option whereby tenants obtain dedicated servers, they do not offer any network
provisioning service, which would shield these tenants from network
interference. In this paper, we introduce Links as a Service, a new abstraction
for cloud service that provides physical isolation of network links. Each
tenant gets an exclusive set of links forming a virtual fat tree, and is
guaranteed to receive the exact same bandwidth and delay as if it were alone in
the shared cloud. Under simple assumptions, we derive theoretical conditions
for enabling LaaS without capacity over-provisioning in fat-trees. New tenants
are only admitted in the network when they can be allocated hosts and links
that maintain these conditions. Using experiments on real clusters as well as
simulations with real-life tenant sizes, we show that LaaS completely avoids
the performance degradation caused by traffic from concurrent tenants on shared
links. Compared to mere host isolation, LaaS can improve the application
performance by up to 200%, at the cost of a 10% reduction in the cloud
utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07404</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07404</id><created>2015-09-24</created><authors><author><keyname>Kim</keyname><forenames>Eunjung</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Parameterized Algorithms for Min-Max Multiway Cut and List Digraph
  Homomorphism</title><categories>cs.DS math.CO</categories><comments>An extended abstract of this work will appear in the Proceedings of
  the 10th International Symposium on Parameterized and Exact Computation
  (IPEC), Patras, Greece, September 2015</comments><msc-class>05C85, 68R10</msc-class><acm-class>G.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we design {\sf FPT}-algorithms for two parameterized problems.
The first is \textsc{List Digraph Homomorphism}: given two digraphs $G$ and $H$
and a list of allowed vertices of $H$ for every vertex of $G$, the question is
whether there exists a homomorphism from $G$ to $H$ respecting the list
constraints. The second problem is a variant of \textsc{Multiway Cut}, namely
\textsc{Min-Max Multiway Cut}: given a graph $G$, a non-negative integer
$\ell$, and a set $T$ of $r$ terminals, the question is whether we can
partition the vertices of $G$ into $r$ parts such that (a) each part contains
one terminal and (b) there are at most $\ell$ edges with only one endpoint in
this part. We parameterize \textsc{List Digraph Homomorphism} by the number $w$
of edges of $G$ that are mapped to non-loop edges of $H$ and we give a time
$2^{O(\ell\cdot\log h+\ell^2\cdot \log \ell)}\cdot n^{4}\cdot \log n$
algorithm, where $h$ is the order of the host graph $H$. We also prove that
\textsc{Min-Max Multiway Cut} can be solved in time $2^{O((\ell r)^2\log \ell
r)}\cdot n^{4}\cdot \log n$. Our approach introduces a general problem, called
{\sc List Allocation}, whose expressive power permits the design of
parameterized reductions of both aforementioned problems to it. Then our
results are based on an {\sf FPT}-algorithm for the {\sc List Allocation}
problem that is designed using a suitable adaptation of the {\em randomized
contractions} technique (introduced by [Chitnis, Cygan, Hajiaghayi, Pilipczuk,
and Pilipczuk, FOCS 2012]).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07411</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07411</id><created>2015-09-24</created><authors><author><keyname>Stanton</keyname><forenames>Richard</forenames></author><author><keyname>Brookes</keyname><forenames>Mike</forenames></author></authors><title>Speech Dereverberation in the STFT Domain</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reverberation is damaging to both the quality and the intelligibility of a
speech signal. We propose a novel single-channel method of dereverberation
based on a linear filter in the Short Time Fourier Transform domain. Each
enhanced frame is constructed from a linear sum of nearby frames based on the
channel impulse response. The results show that the method can resolve any
reverberant signal with knowledge of the impulse response to a non-reverberant
signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07417</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07417</id><created>2015-09-24</created><authors><author><keyname>Fischer</keyname><forenames>Johannes</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>K&#xf6;ppl</keyname><forenames>Dominik</forenames></author></authors><title>Deterministic Sparse Suffix Sorting on Rewritable Texts</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a rewriteable text $T$ of length $n$ on an alphabet of size $\sigma$,
we propose an online algorithm that computes the sparse suffix array and the
sparse longest common prefix array of $T$ in $\Oh{\abs{\Comlcp} \sqrt{\lg n} +
m \lg m \lg n \lg^* n}$ time by using the text space and $\Oh{m}$ additional
working space, where $m$ is the number of some positions $\Pos$ on $[1..n]$,
provided online and arbitrarily, and $\Comlcp = \bigcup_{p,p' \in \Pos, p \neq
p'} [p..p+\lcp(T[p..], T[p'..])]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07422</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07422</id><created>2015-09-24</created><authors><author><keyname>Wilson</keyname><forenames>Craig</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Adaptive Sequential Optimization with Applications to Machine Learning</title><categories>cs.LG cs.DS</categories><comments>submitted to ICASSP 2016, extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is introduced for solving a sequence of slowly changing
optimization problems, including those arising in regression and classification
applications, using optimization algorithms such as stochastic gradient descent
(SGD). The optimization problems change slowly in the sense that the minimizers
change at either a fixed or bounded rate. A method based on estimates of the
change in the minimizers and properties of the optimization algorithm is
introduced for adaptively selecting the number of samples needed from the
distributions underlying each problem in order to ensure that the excess risk,
i.e., the expected gap between the loss achieved by the approximate minimizer
produced by the optimization algorithm and the exact minimizer, does not exceed
a target level. Experiments with synthetic and real data are used to confirm
that this approach performs well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07435</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07435</id><created>2015-09-24</created><authors><author><keyname>Gamble</keyname><forenames>Jennifer</forenames></author><author><keyname>Chintakunta</keyname><forenames>Harish</forenames></author><author><keyname>Wilkerson</keyname><forenames>Adam</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author></authors><title>Node Dominance: Revealing Community and Core-Periphery Structure in
  Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study relates the local property of node dominance to local and global
properties of a network. Iterative removal of dominated nodes yields a
distributed algorithm for computing a core-periphery decomposition of a social
network, where nodes in the network core are seen to be essential in terms of
network flow and global structure. Additionally, the connected components in
the periphery give information about the community structure of the network,
aiding in community detection. A number of explicit results are derived,
relating the core and periphery to network flow, community structure and global
network structure, which are corroborated by observational results. The method
is illustrated using a real world network (DBLP co-authorship network), with
ground-truth communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07437</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07437</id><created>2015-09-24</created><authors><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author><author><keyname>Pieterse</keyname><forenames>Astrid</forenames></author></authors><title>Sparsification Upper and Lower Bounds for Graph Problems and
  Not-All-Equal SAT</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several sparsification lower and upper bounds for classic problems
in graph theory and logic. For the problems 4-Coloring, (Directed) Hamiltonian
Cycle, and (Connected) Dominating Set, we prove that there is no
polynomial-time algorithm that reduces any n-vertex input to an equivalent
instance, of an arbitrary problem, with bitsize O(n^{2-e}) for e &gt; 0, unless NP
is in coNP/poly and the polynomial-time hierarchy collapses. These results
imply that existing linear-vertex kernels for k-Nonblocker and k-Max Leaf
Spanning Tree (the parametric duals of (Connected) Dominating Set) cannot be
improved to have O(k^{2-e}) edges, unless NP is in coNP/poly. We also present a
positive result and exhibit a non-trivial sparsification algorithm for
d-Not-All-Equal SAT. We give an algorithm that reduces an n-variable input with
clauses of size at most d to an equivalent input with O(n^{d-1}) clauses, for
any fixed d. Our algorithm is based on a linear-algebraic proof of Lovasz that
bounds the number of hyperedges in critically 3-chromatic d-uniform n-vertex
hypergraphs by n choose d-1. We show that our kernel is tight under the
assumption that NP is not a subset of coNP/poly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07445</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07445</id><created>2015-09-24</created><updated>2015-09-27</updated><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author></authors><title>Multi-Objective Weighted Sampling</title><categories>cs.DB cs.DS</categories><comments>9 pages; full(er) version of a HotWeb 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key value data sets of the form $\{(x,w_x)\}$ where $w_x &gt;0$ are prevalent.
Common queries over such data are {\em segment $f$-statistics} $Q(f,H) =
\sum_{x\in H}f(w_x)$, specified for a segment $H$ of the keys and a function
$f$. Different choices of $f$ correspond to count, sum, moments, capping, and
threshold statistics.
  When the data set is large, we can compute a smaller sample from which we can
quickly estimate statistics. A weighted sample of keys taken with respect to
$f(w_x)$ provides estimates with statistically guaranteed quality for
$f$-statistics. Such a sample $S^{(f)}$ can be used to estimate $g$-statistics
for $g\not=f$, but quality degrades with the disparity between $g$ and $f$. In
this paper we address applications that require quality estimates for a set $F$
of different functions. A naive solution is to compute and work with a
different sample $S^{(f)}$ for each $f\in F$. Instead, this can be achieved
more effectively and seamlessly using a single {\em multi-objective} sample
$S^{(F)}$ of a much smaller size.
  We review multi-objective sampling schemes and place them in our context of
estimating $f$-statistics. We show that a multi-objective sample for $F$
provides quality estimates for any $f$ that is a positive linear combination of
functions from $F$. We then establish a surprising and powerful result when the
target set $M$ is {\em all} monotone non-decreasing functions, noting that $M$
includes most natural statistics. We provide efficient multi-objective sampling
algorithms for $M$ and show that a sample size of $k \ln n$ (where $n$ is the
number of active keys) provides the same estimation quality, for any $f\in M$,
as a dedicated weighted sample of size $k$ for $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07449</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07449</id><created>2015-09-24</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Structural Vulnerability of Power Grids to Disasters: Bounds,
  Adversarial Attacks and Reinforcement</title><categories>cs.SY math.OC</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural Disasters like hurricanes, floods or earthquakes can damage power
grid devices and create cascading blackouts and islands. The nature of failure
propagation and extent of damage is dependent on the structural features of the
grid, which is different from that of random networks. This paper analyzes the
structural vulnerability of real power grids to impending disasters and
presents intuitive graphical metrics to quantify the extent of damage. Two
improved graph eigen-value based bounds on the grid vulnerability are developed
and demonstrated through simulations of failure propagation on IEEE test cases
and real networks. Finally this paper studies adversarial attacks aimed at
weakening the grid's structural resilience and presents two approximate schemes
to determine the critical transmission lines that may be attacked to minimize
grid resilience. The framework can be also be used to design protection schemes
to secure the grid against such adversarial attacks. Simulations on power
networks are used to compare the performance of the attack schemes in reducing
grid resilience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07450</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07450</id><created>2015-09-22</created><updated>2015-09-27</updated><authors><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Yao</keyname><forenames>Enyi</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>A 128 channel Extreme Learning Machine based Neural Decoder for Brain
  Machine Interfaces</title><categories>cs.LG cs.HC</categories><comments>13 pages, 17 figures, accepted by IEEE Transactions on Biomedical
  Circuits and Systems, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, state-of-the-art motor intention decoding algorithms in
brain-machine interfaces are mostly implemented on a PC and consume significant
amount of power. A machine learning co-processor in 0.35um CMOS for motor
intention decoding in brain-machine interfaces is presented in this paper.
Using Extreme Learning Machine algorithm and low-power analog processing, it
achieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz.
The learning in second stage and corresponding digitally stored coefficients
are used to increase robustness of the core analog processor. The chip is
verified with neural data recorded in monkey finger movements experiment,
achieving a decoding accuracy of 99.3% for movement type. The same co-processor
is also used to decode time of movement from asynchronous neural spikes. With
time-delayed feature dimension enhancement, the classification accuracy can be
increased by 5% with limited number of input channels. Further, a sparsity
promoting training scheme enables reduction of number of programmable weights
by ~2X.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07454</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07454</id><created>2015-09-24</created><authors><author><keyname>Krishnan</keyname><forenames>Sanjay</forenames></author><author><keyname>Wang</keyname><forenames>Jiannan</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Goldberg</keyname><forenames>Ken</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author></authors><title>Stale View Cleaning: Getting Fresh Answers from Stale Materialized Views</title><categories>cs.DB</categories><journal-ref>Proceedings of the VLDB Endowment - Proceedings of the 41st
  International Conference on Very Large Data Bases, Kohala Coast, Hawaii
  Volume 8 Issue 12, August 2015 Pages 1370-1381</journal-ref><doi>10.14778/2824032.2824037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Materialized views (MVs), stored pre-computed results, are widely used to
facilitate fast queries on large datasets. When new records arrive at a high
rate, it is infeasible to continuously update (maintain) MVs and a common
solution is to defer maintenance by batching updates together. Between batches
the MVs become increasingly stale with incorrect, missing, and superfluous rows
leading to increasingly inaccurate query results. We propose Stale View
Cleaning (SVC) which addresses this problem from a data cleaning perspective.
In SVC, we efficiently clean a sample of rows from a stale MV, and use the
clean sample to estimate aggregate query results. While approximate, the
estimated query results reflect the most recent data. As sampling can be
sensitive to long-tailed distributions, we further explore an outlier indexing
technique to give increased accuracy when the data distributions are skewed.
SVC complements existing deferred maintenance approaches by giving accurate and
bounded query answers between maintenance. We evaluate our method on a
generated dataset from the TPC-D benchmark and a real video distribution
application. Experiments confirm our theoretical results: (1) cleaning an MV
sample is more efficient than full view maintenance, (2) the estimated results
are more accurate than using the stale MV, and (3) SVC is applicable for a wide
variety of MVs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07455</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07455</id><created>2015-09-24</created><authors><author><keyname>Giannakopoulos</keyname><forenames>Yiannis</forenames></author><author><keyname>Kyropoulou</keyname><forenames>Maria</forenames></author></authors><title>The VCG Mechanism for Bayesian Scheduling</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of scheduling $m$ tasks to $n$ selfish, unrelated
machines in order to minimize the makespan, where the execution times are
independent random variables, identical across machines. We show that the VCG
mechanism, which myopically allocates each task to its best machine, achieves
an approximation ratio of $O\left(\frac{\ln n}{\ln \ln n}\right)$. This
improves significantly on the previously best known bound of
$O\left(\frac{m}{n}\right)$ for prior-independent mechanisms, given by Chawla
et al. [STOC'13] under the additional assumption of Monotone Hazard Rate (MHR)
distributions. Although we demonstrate that this is in general tight, if we do
maintain the MHR assumption, then we get improved, (small) constant bounds for
$m\geq n\ln n$ i.i.d. tasks, while we also identify a sufficient condition on
the distribution that yields a constant approximation ratio regardless of the
number of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07466</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07466</id><created>2015-09-24</created><authors><author><keyname>Bavarian</keyname><forenames>Mohammad</forenames></author><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author><author><keyname>Yuen</keyname><forenames>Henry</forenames></author></authors><title>Anchoring games for parallel repetition</title><categories>quant-ph cs.CC</categories><comments>37 pages. Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two major open problems regarding the parallel repetition of games are
whether an analogue of Raz's parallel-repetition theorem holds for (a) games
with more than two players, and (b) games with quantum players using
entanglement. We make progress on both problems: we introduce a class of games
we call anchored, and prove exponential-decay parallel repetition theorems for
anchored games in the multiplayer and entangled-player settings. We introduce a
simple transformation on games called anchoring and show that this
transformation turns any game into an anchored game. Together, our parallel
repetition theorem and our anchoring transformation provide a simple and
efficient hardness-amplification technique in both the classical multiplayer
and quantum settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07469</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07469</id><created>2015-09-24</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Channel Vector Subspace Estimation from Low-Dimensional Projections</title><categories>cs.IT math.IT stat.ML</categories><comments>2 Figures, 35 pages. A shorter version of this paper is submitted to
  the International Zurich Seminar on Communications (IZS 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose efficient algorithms for estimating the signal
subspace of mobile users in a wireless communication environment with a
multi-antenna base-station with $M$ antennas. We assume that, for reducing the
RF front-end complexity and overall A/D conversion rate, the JSDM
transmitter/receiver is split into the product of a baseband linear projection
(digital) and an RF reconfigurable beamforming network (analog) with only $m
\ll M$ RF chains. This implies that only $m$ analog observations can be
obtained for subspace estimation, and the standard sample covariance estimator
is not available.
  We develop efficient algorithms that estimate the dominant signal subspace
from sampling only $m=O(2 \sqrt{M})$ specific array elements according to a
coprime scheme, and for a given $p \leq M$, return a $p$-dimensional beamformer
that has a performance comparable with the best $p$-dim beamformer designed by
knowing the exact covariance matrix of the received signal. We asses the
performance of our proposed estimators both analytically and empirically via
numerical simulations, and compare it with that of the other state-of-the-art
methods proposed in the literature, which are also reviewed and put in the
context of estimating the subspace of the signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07470</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07470</id><created>2015-09-24</created><authors><author><keyname>Zhang</keyname><forenames>Yan-Yu</forenames></author><author><keyname>Yu</keyname><forenames>Hong-Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Jian-Kang</forenames></author><author><keyname>Zhu</keyname><forenames>Yi-Jun</forenames></author><author><keyname>Wang</keyname><forenames>Jin-Long</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Space Codes for MIMO Optical Wireless Communications: Error Performance
  Criterion and Code Construction</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multiple-input-multiple-output optical wireless
communication (MIMO-OWC) system in the presence of log-normal fading. In this
scenario, a general criterion for the design of full-diversity space code
(FDSC) with the maximum likelihood (ML) detector is developed. This criterion
reveals that in a high signal-to-noise ratio (SNR) regime, MIMO-OWC offers both
large-scale diversity gain, governing the exponential decaying of the error
curve, and small-scale diversity gain, producing traditional power-law
decaying. Particularly for a two by two MIMO-OWC system with unipolar pulse
amplitude modulation (PAM), a closed-form solution to the design problem of a
linear FDSC optimizing both diversity gains is attained by taking advantage of
the available properties on the successive terms of Farey sequences in number
theory as well as by developing new properties on the disjoint intervals formed
by the Farey sequence terms to attack the continuous and discrete variables
mixed max-min design problem. In fact, this specific design not only proves
that a repetition code (RC) is the optimal linear FDSC optimizing both the
diversity gains, but also uncovers a significant difference between MIMO radio
frequency (RF) communications and MIMO-OWC that space dimension alone is
sufficient for a full large-scale diversity achievement. Computer simulations
demonstrate that FDSC substantially outperforms uncoded spatial multiplexing
with the same total optical power and spectral efficiency, and the latter
provides only the small-scale diversity gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07473</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07473</id><created>2015-09-24</created><authors><author><keyname>Veit</keyname><forenames>Andreas</forenames></author><author><keyname>Kovacs</keyname><forenames>Balazs</forenames></author><author><keyname>Bell</keyname><forenames>Sean</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Bala</keyname><forenames>Kavita</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author></authors><title>Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences</title><categories>cs.CV</categories><comments>ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid proliferation of smart mobile devices, users now take millions
of photos every day. These include large numbers of clothing and accessory
images. We would like to answer questions like `What outfit goes well with this
pair of shoes?' To answer these types of questions, one has to go beyond
learning visual similarity and learn a visual notion of compatibility across
categories. In this paper, we propose a novel learning framework to help answer
these types of questions. The main idea of this framework is to learn a feature
transformation from images of items into a latent space that expresses
compatibility. For the feature transformation, we use a Siamese Convolutional
Neural Network (CNN) architecture, where training examples are pairs of items
that are either compatible or incompatible. We model compatibility based on
co-occurrence in large-scale user behavior data; in particular co-purchase data
from Amazon.com. To learn cross-category fit, we introduce a strategic method
to sample training data, where pairs of items are heterogeneous dyads, i.e.,
the two elements of a pair belong to different high-level categories. While
this approach is applicable to a wide variety of settings, we focus on the
representative problem of learning compatible clothing style. Our results
indicate that the proposed framework is capable of learning semantic
information about visual style and is able to generate outfits of clothes, with
items from different categories, that go well together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07476</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07476</id><created>2015-09-24</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Oliveira</keyname><forenames>Igor C.</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>Near-optimal small-depth lower bounds for small distance connectivity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any depth-$d$ circuit for determining whether an $n$-node graph
has an $s$-to-$t$ path of length at most $k$ must have size
$n^{\Omega(k^{1/d}/d)}$. The previous best circuit size lower bounds for this
problem were $n^{k^{\exp(-O(d))}}$ (due to Beame, Impagliazzo, and Pitassi
[BIP98]) and $n^{\Omega((\log k)/d)}$ (following from a recent formula size
lower bound of Rossman [Ros14]). Our lower bound is quite close to optimal,
since a simple construction gives depth-$d$ circuits of size $n^{O(k^{2/d})}$
for this problem (and strengthening our bound even to $n^{k^{\Omega(1/d)}}$
would require proving that undirected connectivity is not in $\mathsf{NC^1}.$)
  Our proof is by reduction to a new lower bound on the size of small-depth
circuits computing a skewed variant of the &quot;Sipser functions&quot; that have played
an important role in classical circuit lower bounds [Sip83, Yao85, H{\aa}s86].
A key ingredient in our proof of the required lower bound for these Sipser-like
functions is the use of \emph{random projections}, an extension of random
restrictions which were recently employed in [RST15]. Random projections allow
us to obtain sharper quantitative bounds while employing simpler arguments,
both conceptually and technically, than in the previous works [Ajt89, BPU92,
BIP98, Ros14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07479</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07479</id><created>2015-09-24</created><updated>2015-09-28</updated><authors><author><keyname>Wilber</keyname><forenames>Michael J.</forenames></author><author><keyname>Kwak</keyname><forenames>Iljung S.</forenames></author><author><keyname>Kriegman</keyname><forenames>David</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author></authors><title>Learning Concept Embeddings with Combined Human-Machine Expertise</title><categories>cs.CV</categories><comments>To appear at ICCV 2015. (This version has updated author affiliations
  and updated footnotes.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our work on &quot;SNaCK,&quot; a low-dimensional concept embedding
algorithm that combines human expertise with automatic machine similarity
kernels. Both parts are complimentary: human insight can capture relationships
that are not apparent from the object's visual similarity and the machine can
help relieve the human from having to exhaustively specify many constraints. We
show that our SNaCK embeddings are useful in several tasks: distinguishing
prime and nonprime numbers on MNIST, discovering labeling mistakes in the
Caltech UCSD Birds (CUB) dataset with the help of deep-learned features,
creating training datasets for bird classifiers, capturing subjective human
taste on a new dataset of 10,000 foods, and qualitatively exploring an
unstructured set of pictographic characters. Comparisons with the
state-of-the-art in these tasks show that SNaCK produces better concept
embeddings that require less human supervision than the leading methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07481</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07481</id><created>2015-09-24</created><authors><author><keyname>Wang</keyname><forenames>Zhiguang</forenames></author><author><keyname>Oates</keyname><forenames>Tim</forenames></author></authors><title>Spatially Encoding Temporal Correlations to Classify Temporal Data Using
  Convolutional Neural Networks</title><categories>cs.LG</categories><comments>Submit to JCSS. Preliminary versions are appeared in AAAI 2015
  workshop and IJCAI 2016 [arXiv:1506.00327]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an off-line approach to explicitly encode temporal patterns
spatially as different types of images, namely, Gramian Angular Fields and
Markov Transition Fields. This enables the use of techniques from computer
vision for feature learning and classification. We used Tiled Convolutional
Neural Networks to learn high-level features from individual GAF, MTF, and
GAF-MTF images on 12 benchmark time series datasets and two real
spatial-temporal trajectory datasets. The classification results of our
approach are competitive with state-of-the-art approaches on both types of
data. An analysis of the features and weights learned by the CNNs explains why
the approach works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07495</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07495</id><created>2015-09-24</created><updated>2016-01-26</updated><authors><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Unbounded Lookahead in WMSO+U Games</title><categories>cs.GT cs.FL</categories><comments>Removed Section 5 about the (non-effective) reduction to delay-free
  games due to a bug in Lemma 5. arXiv admin note: text overlap with
  arXiv:1412.3978</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay games are two-player games of infinite duration in which one player may
delay her moves to obtain a lookahead on her opponent's moves. We consider
delay games with winning conditions expressed in weak monadic second order
logic with the unbounding quantifier (WMSO+U), which is able to express
(un)boundedness properties. It is decidable whether the delaying player is able
to win such a game with bounded lookahead, i.e., if she only skips a finite
number of moves.
  However, bounded lookahead is not always sufficient: we present a game that
can be won with unbounded lookahead, but not with bounded lookahead. Then, we
consider WMSO+U delay games with unbounded lookahead and show that the exact
evolution of the lookahead is irrelevant: the winner is always the same, as
long as the initial lookahead is large enough and the lookahead tends to
infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07513</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07513</id><created>2015-09-24</created><authors><author><keyname>Valenzuela-Esc&#xe1;rcega</keyname><forenames>Marco A.</forenames></author><author><keyname>Hahn-Powell</keyname><forenames>Gus</forenames></author><author><keyname>Surdeanu</keyname><forenames>Mihai</forenames></author></authors><title>Description of the Odin Event Extraction Framework and Rule Language</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes the Odin framework, which is a domain-independent
platform for developing rule-based event extraction models. Odin aims to be
powerful (the rule language allows the modeling of complex syntactic
structures) and robust (to recover from syntactic parsing errors, syntactic
patterns can be freely mixed with surface, token-based patterns), while
remaining simple (some domain grammars can be up and running in minutes), and
fast (Odin processes over 100 sentences/second in a real-world domain with over
200 rules). Here we include a thorough definition of the Odin rule language,
together with a description of the Odin API in the Scala language, which allows
one to apply these rules to arbitrary texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07538</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07538</id><created>2015-09-24</created><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Design Aspects of Short Range Millimeter Wave Networks: A MAC Layer
  Perspective</title><categories>cs.IT cs.NI math.IT</categories><comments>submitted to IEEE Network, 10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increased density of wireless devices, ever growing demands for extremely
high data rate, and spectrum scarcity at microwave bands make the millimeter
wave (mmWave) frequencies an important player in future wireless networks.
However, mmWave communication systems exhibit severe attenuation, blockage,
deafness, and may need microwave networks for coordination and fall-back
support. To compensate for high attenuation, mmWave systems exploit highly
directional operation, which in turn substantially reduces the interference
footprint. The significant differences between mmWave networks and legacy
communication technologies challenge the classical design approaches,
especially at the medium access control (MAC) layer, which has received
comparatively less attention than PHY and propagation issues in the literature
so far. In this paper, the MAC layer design aspects of short range mmWave
networks are discussed. In particular, we explain why current mmWave standards
fail to fully exploit the potential advantages of short range mmWave
technology, and argue for the necessity of new collision-aware hybrid resource
allocation frameworks with on-demand control messages, the advantages of a
collision notification message, and the potential of multihop communication to
provide reliable mmWave connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07543</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07543</id><created>2015-09-24</created><authors><author><keyname>Veit</keyname><forenames>Andreas</forenames></author><author><keyname>Wilber</keyname><forenames>Michael</forenames></author><author><keyname>Vaish</keyname><forenames>Rajan</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Davis</keyname><forenames>James</forenames></author><author><keyname>Anand</keyname><forenames>Vishal</forenames></author><author><keyname>Aviral</keyname><forenames>Anshu</forenames></author><author><keyname>Chakrabarty</keyname><forenames>Prithvijit</forenames></author><author><keyname>Chandak</keyname><forenames>Yash</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Sidharth</forenames></author><author><keyname>Devaraj</keyname><forenames>Chinmaya</forenames></author><author><keyname>Dhall</keyname><forenames>Ankit</forenames></author><author><keyname>Dwivedi</keyname><forenames>Utkarsh</forenames></author><author><keyname>Gupte</keyname><forenames>Sanket</forenames></author><author><keyname>Sridhar</keyname><forenames>Sharath N.</forenames></author><author><keyname>Paga</keyname><forenames>Karthik</forenames></author><author><keyname>Pahuja</keyname><forenames>Anuj</forenames></author><author><keyname>Raisinghani</keyname><forenames>Aditya</forenames></author><author><keyname>Sharma</keyname><forenames>Ayush</forenames></author><author><keyname>Sharma</keyname><forenames>Shweta</forenames></author><author><keyname>Sinha</keyname><forenames>Darpana</forenames></author><author><keyname>Thakkar</keyname><forenames>Nisarg</forenames></author><author><keyname>Vignesh</keyname><forenames>K. Bala</forenames></author><author><keyname>Verma</keyname><forenames>Utkarsh</forenames></author><author><keyname>Abhishek</keyname><forenames>Kanniganti</forenames></author><author><keyname>Agrawal</keyname><forenames>Amod</forenames></author><author><keyname>Aishwarya</keyname><forenames>Arya</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Aurgho</forenames></author><author><keyname>Dhanasekar</keyname><forenames>Sarveshwaran</forenames></author><author><keyname>Gullapalli</keyname><forenames>Venkata Karthik</forenames></author><author><keyname>Gupta</keyname><forenames>Shuchita</forenames></author><author><keyname>G</keyname><forenames>Chandana</forenames></author><author><keyname>Jain</keyname><forenames>Kinjal</forenames></author><author><keyname>Kapur</keyname><forenames>Simran</forenames></author><author><keyname>Kasula</keyname><forenames>Meghana</forenames></author><author><keyname>Kumar</keyname><forenames>Shashi</forenames></author><author><keyname>Kundaliya</keyname><forenames>Parth</forenames></author><author><keyname>Mathur</keyname><forenames>Utkarsh</forenames></author><author><keyname>Mishra</keyname><forenames>Alankrit</forenames></author><author><keyname>Mudgal</keyname><forenames>Aayush</forenames></author><author><keyname>Nadimpalli</keyname><forenames>Aditya</forenames></author><author><keyname>Nihit</keyname><forenames>Munakala Sree</forenames></author><author><keyname>Periwal</keyname><forenames>Akanksha</forenames></author><author><keyname>Sagar</keyname><forenames>Ayush</forenames></author><author><keyname>Shah</keyname><forenames>Ayush</forenames></author><author><keyname>Sharma</keyname><forenames>Vikas</forenames></author><author><keyname>Sharma</keyname><forenames>Yashovardhan</forenames></author><author><keyname>Siddiqui</keyname><forenames>Faizal</forenames></author><author><keyname>Singh</keyname><forenames>Virender</forenames></author><author><keyname>S.</keyname><forenames>Abhinav</forenames></author><author><keyname>Yadav</keyname><forenames>Anurag. D.</forenames></author></authors><title>On Optimizing Human-Machine Task Assignments</title><categories>cs.HC cs.CV</categories><comments>HCOMP 2015 Work in Progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When crowdsourcing systems are used in combination with machine inference
systems in the real world, they benefit the most when the machine system is
deeply integrated with the crowd workers. However, if researchers wish to
integrate the crowd with &quot;off-the-shelf&quot; machine classifiers, this deep
integration is not always possible. This work explores two strategies to
increase accuracy and decrease cost under this setting. First, we show that
reordering tasks presented to the human can create a significant accuracy
improvement. Further, we show that greedily choosing parameters to maximize
machine accuracy is sub-optimal, and joint optimization of the combined system
improves performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07552</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07552</id><created>2015-09-24</created><authors><author><keyname>Guttman</keyname><forenames>Joshua D.</forenames></author><author><keyname>Liskov</keyname><forenames>Moses D.</forenames></author><author><keyname>Ramsdell</keyname><forenames>John D.</forenames></author><author><keyname>Rowe</keyname><forenames>Paul D.</forenames></author></authors><title>Formal Support for Standardizing Protocols with State</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many cryptographic protocols are designed to achieve their goals using only
messages passed over an open network. Numerous tools, based on well-understood
foundations, exist for the design and analysis of protocols that rely purely on
message passing. However, these tools encounter difficulties when faced with
protocols that rely on non-local, mutable state to coordinate several local
sessions.
  We adapt one of these tools, {\cpsa}, to provide automated support for
reasoning about state. We use Ryan's Envelope Protocol as an example to
demonstrate how the message-passing reasoning can be integrated with state
reasoning to yield interesting and powerful results.
  Keywords: protocol analysis tools, stateful protocols, TPM, PKCS#11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07553</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07553</id><created>2015-09-24</created><authors><author><keyname>Sutherland</keyname><forenames>Dougal J.</forenames></author><author><keyname>Oliva</keyname><forenames>Junier B.</forenames></author><author><keyname>P&#xf3;czos</keyname><forenames>Barnab&#xe1;s</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author></authors><title>Linear-time Learning on Distributions with Approximate Kernel Embeddings</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many interesting machine learning problems are best posed by considering
instances that are distributions, or sample sets drawn from distributions.
Previous work devoted to machine learning tasks with distributional inputs has
done so through pairwise kernel evaluations between pdfs (or sample sets).
While such an approach is fine for smaller datasets, the computation of an $N
\times N$ Gram matrix is prohibitive in large datasets. Recent scalable
estimators that work over pdfs have done so only with kernels that use
Euclidean metrics, like the $L_2$ distance. However, there are a myriad of
other useful metrics available, such as total variation, Hellinger distance,
and the Jensen-Shannon divergence. This work develops the first random features
for pdfs whose dot product approximates kernels using these non-Euclidean
metrics, allowing estimators using such kernels to scale to large datasets by
working in a primal space, without computing large Gram matrices. We provide an
analysis of the approximation error in using our proposed random features and
show empirically the quality of our approximation both in estimating a Gram
matrix and in solving learning tasks in real-world and synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07566</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07566</id><created>2015-09-24</created><authors><author><keyname>Ligo</keyname><forenames>Jonathan G.</forenames></author><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Rate Analysis for Detection of Sparse Mixtures</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the rate of decay of the probability of error for
distinguishing between a sparse signal with noise, modeled as a sparse mixture,
from pure noise. This problem has many applications in signal processing,
evolutionary biology, bioinformatics, astrophysics and feature selection for
machine learning. We let the mixture probability tend to zero as the number of
observations tends to infinity and derive oracle rates at which the error
probability can be driven to zero for a general class of signal and noise
distributions. In contrast to the problem of detection of non-sparse signals,
we see the log-probability of error decays sublinearly rather than linearly and
is characterized through the $\chi^2$-divergence rather than the
Kullback-Leibler divergence. Our contributions are: (i) the first
characterization of the rate of decay of the error probability for this
problem; and (ii) the construction of a test based on the $L^1$-Wasserstein
metric that achieves the oracle rate in a Gaussian setting without prior
knowledge of the sparsity level or the signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07569</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07569</id><created>2015-09-24</created><authors><author><keyname>Vikas</keyname><forenames>Vishesh</forenames></author><author><keyname>Templeton</keyname><forenames>Paul</forenames></author><author><keyname>Trimmer</keyname><forenames>Barry</forenames></author></authors><title>Design and control of a soft, shape-changing, crawling robot</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft materials have many important roles in animal locomotion and object
manipulation. In robotic applications soft materials can store and release
energy, absorb impacts, increase compliance and increase the range of possible
shape profiles using minimal actuators. The shape changing ability is also a
potential tool to manipulate friction forces caused by contact with the
environment. These advantages are accompanied by challenges of soft material
actuation and the need to exploit frictional interactions to generate
locomotion. Accordingly, the design of soft robots involves exploitation of
continuum properties of soft materials for manipulating frictional interactions
that result in robot locomotion. The research presents design and control of a
soft body robot that uses its shape change capability for locomotion. The
bioinspired (caterpillar) modular robot design is a soft monolithic body which
interacts with the environment at discrete contact points (caterpillar
prolegs). The deformable body is actuated by muscle-like shape memory alloy
coils and the discrete contact points manipulate friction in a binary manner.
This novel virtual grip mechanism combines two materials with different
coefficients of frictions (sticky-slippery) to control the robot-environment
friction interactions. The research also introduces a novel control concept
that discretizes the robot-environment-friction interaction into binary states.
This facilitates formulation of a control framework that is independent of the
specific actuator or soft material properties and can be applied to
multi-limbed soft robots. The transitions between individual robot states are
assigned a reward that allow optimized state transition control sequences to be
calculated. This conceptual framework is extremely versatile and we show how it
can be applied to situations in which the robot loses limb function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07577</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07577</id><created>2015-09-24</created><authors><author><keyname>Vergara</keyname><forenames>Jorge R.</forenames></author><author><keyname>Est&#xe9;vez</keyname><forenames>Pablo A.</forenames></author></authors><title>A Review of Feature Selection Methods Based on Mutual Information</title><categories>cs.LG stat.ML</categories><journal-ref>Neural Computing &amp; Applications, vol. 24 (1), pp. 175-186, 2014</journal-ref><doi>10.1007/s00521-013-1368-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a review of the state of the art of information
theoretic feature selection methods. The concepts of feature relevance,
redundance and complementarity (synergy) are clearly defined, as well as Markov
blanket. The problem of optimal feature selection is defined. A unifying
theoretical framework is described, which can retrofit successful heuristic
criteria, indicating the approximations made by each method. A number of open
problems in the field are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07578</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07578</id><created>2015-09-23</created><authors><author><keyname>Srinivasan</keyname><forenames>Uma</forenames></author><author><keyname>Uddin</keyname><forenames>Shahadat</forenames></author></authors><title>A Social Network Framework to Explore Healthcare Collaboration</title><categories>cs.SI cs.CY</categories><comments>A chapter of the book entitled &quot;Healthcare Informatics and
  Analytics:Emerging Issues and Trends&quot; by Madjid Tavana, Amir Hossein
  Ghapanchi and Amir Talaei-Khoei</comments><doi>10.4018/978-1-4666-6316-9</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  A patient-centric approach to healthcare leads to an informal social network
among medical professionals. This chapter presents a research framework to:
identify the collaboration structure among physicians that is effective and
efficient for patients, discover effective structural attributes of a
collaboration network that evolves during the course of providing care, and
explore the impact of socio-demographic characteristics of healthcare
professionals, patients, and hospitals on collaboration structures, from the
point of view of measurable outcomes such as cost and quality of care. The
framework uses illustrative examples drawn from a data set of patients
undergoing hip replacement surgery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07582</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07582</id><created>2015-09-25</created><authors><author><keyname>Konidaris</keyname><forenames>George</forenames></author></authors><title>Constructing Abstraction Hierarchies Using a Skill-Symbol Loop</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a framework for building abstraction hierarchies whereby an agent
alternates skill- and representation-acquisition phases to construct a sequence
of increasingly abstract Markov decision processes. Our formulation builds on
recent results showing that the appropriate abstract representation of a
problem is specified by the agent's skills. We describe how such a hierarchy
can be used for fast planning, and illustrate the construction of an
appropriate hierarchy for the Taxi domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07588</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07588</id><created>2015-09-25</created><authors><author><keyname>Chistikov</keyname><forenames>Dmitry</forenames></author><author><keyname>Iv&#xe1;n</keyname><forenames>Szabolcs</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Fractional coverings, greedy coverings, and rectifier networks</title><categories>cs.CC cs.FL math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rectifier network is a directed acyclic graph with distinguished sources
and sinks; it is said to compute a Boolean matrix $M$ that has a $1$ in the
entry $(i,j)$ iff there is a path from the $j$th source to the $i$th sink. The
smallest number of edges in a rectifier network that computes $M$ is a classic
complexity measure on matrices, which has been studied for more than half a
century.
  We explore two well-known techniques that have hitherto found little to no
applications in this theory. Both of them build upon a basic fact that
depth-$2$ rectifier networks are essentially weighted coverings of Boolean
matrices with rectangles. We obtain new results by using fractional and greedy
coverings (defined in the standard way).
  First, we show that all fractional coverings of the so-called full triangular
matrix have large cost. This provides (a fortiori) a new proof of the $n \log
n$ lower bound on its depth-$2$ complexity (the exact value has been known
since 1965, but previous proofs are based on different arguments). Second, we
show that the greedy heuristic is instrumental in tightening the upper bound on
the depth-$2$ complexity of the Kneser-Sierpi\'nski (disjointness) matrix. The
previous upper bound is $O(n^{1.28})$, and we improve it to $O(n^{1.17})$,
while the best known lower bound is $\Omega(n^{1.16})$. Third, using fractional
coverings, we obtain a form of direct product theorem that gives a lower bound
on unbounded-depth complexity of Kronecker (tensor) products of matrices. In
this case, the greedy heuristic shows (by an argument due to Lov\'asz) that our
result is only a logarithmic factor away from the &quot;full&quot; direct product
theorem. Our second and third results constitute progress on open problems 7.3
and 7.5 from a recent book by Jukna and Sergeev (in Foundations and Trends in
Theoretical Computer Science (2013)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07594</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07594</id><created>2015-09-25</created><updated>2016-02-06</updated><authors><author><keyname>Ye</keyname><forenames>Qiaoyang</forenames></author><author><keyname>Bursalioglu</keyname><forenames>Ozgun Y.</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Haralabos C.</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>User Association and Interference Management in Massive MIMO HetNets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two key traits of 5G cellular networks are much higher base station (BS)
densities - especially in the case of low-power BSs - and the use of massive
MIMO at these BSs. This paper explores how massive MIMO can be used to jointly
maximize the offloading gains and minimize the interference challenges arising
from adding small cells. We consider two interference management approaches:
joint transmission (JT) with local precoding, where users are served
simultaneously by multiple BSs without requiring channel state information
exchanges among cooperating BSs, and resource blanking, where some macro BS
resources are left blank to reduce the interference in the small cell downlink.
A key advantage offered by massive MIMO is channel hardening, which enables to
predict instantaneous rates a priori. This allows us to develop a unified
framework, where resource allocation is cast as a network utility maximization
(NUM) problem, and to demonstrate large gains in cell-edge rates based on the
NUM solution. We propose an efficient dual subgradient based algorithm, which
converges towards the NUM solution. A scheduling scheme is also proposed to
approach the NUM solution. Simulations illustrate more than 2x rate gain for
10th percentile users vs. an optimal association without interference
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07596</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07596</id><created>2015-09-25</created><authors><author><keyname>Zeilberger</keyname><forenames>Noam</forenames></author></authors><title>Counting isomorphism classes of $\beta$-normal linear lambda terms</title><categories>cs.LO math.CO math.LO</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unanticipated connections between different fragments of lambda calculus and
different families of embedded graphs (a.k.a. &quot;maps&quot;) motivate the problem of
enumerating $\beta$-normal linear lambda terms. In this brief note, it is shown
(by appeal to a theorem of Arqu\`es and Beraud) that the sequence counting
isomorphism classes of $\beta$-normal linear lambda terms up to free exchange
of adjacent lambda abstractions coincides with the sequence counting
isomorphism classes of rooted maps on oriented surfaces (A000698).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07599</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07599</id><created>2015-09-25</created><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Polukarov</keyname><forenames>Maria</forenames></author><author><keyname>Venanzi</keyname><forenames>Matteo</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Cooperative Equilibrium beyond Social Dilemmas: Pareto Solvable Games</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently introduced concept of &quot;cooperative equilibrium&quot;, based on the
assumption that players have a natural attitude to cooperation, has been proven
a powerful tool in predicting human behaviour in social dilemmas. In this
paper, we extend this idea to more general game models, termed &quot;Pareto
solvable&quot; games, which in particular include the Nash Bargaining Problem and
the Ultimatum Game. We show that games in this class possess a unique pure
cooperative equilibrium. Furthermore, for the Ultimatum Game, this notion
appears to be strongly correlated with a suitably defined variant of the
Dictator Game. We support this observation with the results of a behavioural
experiment conducted using Amazon Mechanical Turk, which demonstrates that our
approach allows for making statistically precise predictions of average
behaviour in such settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07600</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07600</id><created>2015-09-25</created><authors><author><keyname>Higashikawa</keyname><forenames>Yuya</forenames></author><author><keyname>Cheng</keyname><forenames>Siu-Wing</forenames></author><author><keyname>Kameda</keyname><forenames>Tsunehiko</forenames></author><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Saburi</keyname><forenames>Shun</forenames></author></authors><title>Minimax Regret 1-Median Problem in Dynamic Path Networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the minimax regret 1-median problem in dynamic path
networks. In our model, we are given a dynamic path network consisting of an
undirected path with positive edge lengths, uniform positive edge capacity, and
nonnegative vertex supplies. Here, each vertex supply is unknown but only an
interval of supply is known. A particular assignment of supply to each vertex
is called a scenario. Given a scenario s and a sink location x in a dynamic
path network, let us consider the evacuation time to x of a unit supply given
on a vertex by s. The cost of x under s is defined as the sum of evacuation
times to x for all supplies given by s, and the median under s is defined as a
sink location which minimizes this cost. The regret for x under s is defined as
the cost of x under s minus the cost of the median under s. Then, the problem
is to find a sink location such that the maximum regret for all possible
scenarios is minimized. We propose an O(n^3) time algorithm for the minimax
regret 1-median problem in dynamic path networks with uniform capacity, where n
is the number of vertices in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07607</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07607</id><created>2015-09-25</created><authors><author><keyname>Paix&#xe3;o</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Spreer</keyname><forenames>Jonathan</forenames></author></authors><title>Random collapsibility and 3-sphere recognition</title><categories>math.GT cs.CG</categories><comments>18 pages, 6 figures</comments><msc-class>57Q15, 57N12, 57M15, 90C59</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A triangulation of a $3$-manifold can be shown to be homeomorphic to the
$3$-sphere by describing a discrete Morse function on it with only two critical
faces, that is, a sequence of elementary collapses from the triangulation with
one tetrahedron removed down to a single vertex. Unfortunately, deciding
whether such a sequence exist is believed to be very difficult in general.
  In this article we present a method, based on uniform spanning trees, to
estimate how difficult it is to collapse a given $3$-sphere triangulation after
removing a tetrahedron. In addition we show that out of all $3$-sphere
triangulations with eight vertices or less, exactly $22$ admit a non-collapsing
sequence onto a contractible non-collapsible $2$-complex. As a side product we
classify all minimal triangulations of the dunce hat, and all contractible
non-collapsible $2$-complexes with at most $18$ triangles. This is complemented
by large scale experiments on the collapsing difficulty of $9$- and $10$-vertex
spheres.
  Finally, we propose an easy-to-compute characterisation of $3$-sphere
triangulations which experimentally exhibit a low proportion of collapsing
sequences, leading to a heuristic to produce $3$-sphere triangulations with
difficult combinatorial properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07611</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07611</id><created>2015-09-25</created><authors><author><keyname>Tanaka</keyname><forenames>Kanji</forenames></author></authors><title>Incremental Loop Closure Verification by Guided Sampling</title><categories>cs.CV</categories><comments>Technical report, 7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loop closure detection, the task of identifying locations revisited by a
robot in a sequence of odometry and perceptual observations, is typically
formulated as a combination of two subtasks: (1) bag-of-words image retrieval
and (2) post-verification using RANSAC geometric verification. The main
contribution of this study is the proposal of a novel post-verification
framework that achieves good precision recall trade-off in loop closure
detection. This study is motivated by the fact that not all loop closure
hypotheses are equally plausible (e.g., owing to mutual consistency between
loop closure constraints) and that if we have evidence that one hypothesis is
more plausible than the others, then it should be verified more frequently. We
demonstrate that the problem of loop closure detection can be viewed as an
instance of a multi-model hypothesize-and-verify framework and build guided
sampling strategies on the framework where loop closures proposed using image
retrieval are verified in a planned order (rather than in a conventional
uniform order) to operate in a constant time. Experimental results using a
stereo SLAM system confirm that the proposed strategy, the use of loop closure
constraints and robot trajectory hypotheses as a guide, achieves promising
results despite the fact that there exists a significant number of false
positive constraints and hypotheses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07612</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07612</id><created>2015-09-25</created><authors><author><keyname>Haldenwang</keyname><forenames>Nils</forenames></author><author><keyname>Vornberger</keyname><forenames>Oliver</forenames></author></authors><title>Sentiment Uncertainty and Spam in Twitter Streams and Its Implications
  for General Purpose Realtime Sentiment Analysis</title><categories>cs.CL</categories><comments>3 pages, 1 figure, accepted at GSCL '15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State of the art benchmarks for Twitter Sentiment Analysis do not consider
the fact that for more than half of the tweets from the public stream a
distinct sentiment cannot be chosen. This paper provides a new perspective on
Twitter Sentiment Analysis by highlighting the necessity of explicitly
incorporating uncertainty. Moreover, a dataset of high quality to evaluate
solutions for this new problem is introduced and made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07615</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07615</id><created>2015-09-25</created><authors><author><keyname>Liu</keyname><forenames>Enfu</forenames></author><author><keyname>Tanaka</keyname><forenames>Kanji</forenames></author></authors><title>Discriminative Map Retrieval Using View-Dependent Map Descriptor</title><categories>cs.CV</categories><comments>Technical Report, 8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Map retrieval, the problem of similarity search over a large collection of 2D
pointset maps previously built by mobile robots, is crucial for autonomous
navigation in indoor and outdoor environments. Bag-of-words (BoW) methods
constitute a popular approach to map retrieval; however, these methods have
extremely limited descriptive ability because they ignore the spatial layout
information of the local features. The main contribution of this paper is an
extension of the bag-of-words map retrieval method to enable the use of spatial
information from local features. Our strategy is to explicitly model a unique
viewpoint of an input local map; the pose of the local feature is defined with
respect to this unique viewpoint, and can be viewed as an additional invariant
feature for discriminative map retrieval. Specifically, we wish to determine a
unique viewpoint that is invariant to moving objects, clutter, occlusions, and
actual viewpoints. Hence, we perform scene parsing to analyze the scene
structure, and consider the &quot;center&quot; of the scene structure to be the unique
viewpoint. Our scene parsing is based on a Manhattan world grammar that imposes
a quasi-Manhattan world constraint to enable the robust detection of a scene
structure that is invariant to clutter and moving objects. Experimental results
using the publicly available radish dataset validate the efficacy of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07616</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07616</id><created>2015-09-25</created><authors><author><keyname>Lee</keyname><forenames>Jounghyun</forenames></author><author><keyname>Lee</keyname><forenames>Keun Young</forenames></author><author><keyname>Jeong</keyname><forenames>Karpjoo</forenames></author><author><keyname>Jiang</keyname><forenames>Meilan</forenames></author><author><keyname>Kim</keyname><forenames>Bomchul</forenames></author><author><keyname>Hwang</keyname><forenames>Suntae</forenames></author></authors><title>A Cyberinfrastructure-based Approach to Real Time Water Temperature
  Prediction</title><categories>cs.DC</categories><comments>10 pages, 14 figures, PRAGMA-ICDS-15</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The prediction of water temperature is crucial for aquatic ecosystem studies
and management. In this paper, we raise challenging issues in supporting real
time water temperature prediction and present a system called WT-Agabus to
address those issues. The WT-Agabus system is designed to be a
cyberinfrastructure and to support various prediction models in a uniform way.
In addition, we present a neural network-based water temperature prediction
model to use only data available online from Korea Meteorological
Administration (KMA). In this paper, we also show the current prototype
implementation of the WT-Agabus system to support the prediction model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07617</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07617</id><created>2015-09-25</created><authors><author><keyname>Trip</keyname><forenames>Sebastian</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Optimal frequency regulation in nonlinear structure preserving power
  networks including turbine dynamics: an incremental passivity approach</title><categories>math.OC cs.SY</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of optimal frequency regulation in power
networks, represented by a nonlinear structure preserving model, including
turbine-governor dynamics. Exploiting an incremental passivity property of the
power network, distributed controllers are proposed that regulate the frequency
and minimize generation costs, requiring only frequency measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07618</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07618</id><created>2015-09-25</created><authors><author><keyname>Tsukamoto</keyname><forenames>Taisho</forenames></author><author><keyname>Tanaka</keyname><forenames>Kanji</forenames></author></authors><title>Self-localization Using Visual Experience Across Domains</title><categories>cs.CV</categories><comments>Technical Report, 8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we aim to solve the single-view robot self-localization
problem by using visual experience across domains. Although the bag-of-words
method constitutes a popular approach to single-view localization, it fails
badly when it's visual vocabulary is learned and tested in different domains.
Further, we are interested in using a cross-domain setting, in which the visual
vocabulary is learned in different seasons and routes from the input
query/database scenes. Our strategy is to mine a cross-domain visual
experience, a library of raw visual images collected in different domains, to
discover the relevant visual patterns that effectively explain the input scene,
and use them for scene retrieval. In particular, we show that the appearance
and the pose of the mined visual patterns of a query scene can be efficiently
and discriminatively matched against those of the database scenes by employing
image-to-class distance and spatial pyramid matching. Experimental results
obtained using a novel cross-domain dataset show that our system achieves
promising results despite our visual vocabulary being learned and tested in
different domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07626</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07626</id><created>2015-09-25</created><authors><author><keyname>Wong</keyname><forenames>Lok</forenames></author><author><keyname>Shimojo</keyname><forenames>Shinji</forenames></author><author><keyname>Teranishi</keyname><forenames>Yuuichi</forenames></author><author><keyname>Yoshihisa</keyname><forenames>Tomoki</forenames></author><author><keyname>Haga</keyname><forenames>Jason H.</forenames></author></authors><title>Interactive Museum Exhibits with Microcontrollers: A Use-Case Scenario</title><categories>cs.CY</categories><comments>6 pages, 6 figures, PRAGMA Workshop on International Clouds for Data
  Science (PRAGMA-ICDS 2015)</comments><acm-class>C.3; J.5</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The feasibility of using microcontrollers in real life applications is
becoming more widespread. These applications have grown from do-it-yourself
(DIY) projects of computer enthusiasts or robotics projects to larger scale
efforts and deployments. This project developed and deployed a prototype
application that allows the public to interact with features of a model and
view videos from a first-person perspective on the train. Through testing the
microcontrollers and their usage in a public setting, it was demonstrated that
interactive features could be implemented in model train exhibits, which are
featured in traditional museum environments that lack technical infrastructure.
Specifically, the Arduino and Raspberry Pi provide the necessary linkages
between the Internet and hardware, allowing for a greater interactive
experience for museum visitors. These results provide an important use-case
scenario that cultural heritage institutions can utilize when implementing
microcontrollers on a larger scale, for the purpose of increasing visitors
experience through greater interaction and engagement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07627</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07627</id><created>2015-09-25</created><authors><author><keyname>Kataoka</keyname><forenames>Hirokatsu</forenames></author><author><keyname>Iwata</keyname><forenames>Kenji</forenames></author><author><keyname>Satoh</keyname><forenames>Yutaka</forenames></author></authors><title>Feature Evaluation of Deep Convolutional Neural Networks for Object
  Recognition and Detection</title><categories>cs.CV cs.AI cs.MM</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we evaluate convolutional neural network (CNN) features using
the AlexNet architecture and very deep convolutional network (VGGNet)
architecture. To date, most CNN researchers have employed the last layers
before output, which were extracted from the fully connected feature layers.
However, since it is unlikely that feature representation effectiveness is
dependent on the problem, this study evaluates additional convolutional layers
that are adjacent to fully connected layers, in addition to executing simple
tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and
transformation, using tools such as principal component analysis. In our
experiments, we carried out detection and classification tasks using the
Caltech 101 and Daimler Pedestrian Benchmark Datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07642</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07642</id><created>2015-09-25</created><authors><author><keyname>Li</keyname><forenames>Zhen</forenames></author><author><keyname>Xu</keyname><forenames>Jianjun</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Prediction of Brain States of Concentration and Relaxation in Real Time
  with Portable Electroencephalographs</title><categories>cs.HC</categories><comments>18 pages,7 figures, 4 tables</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our research, we attempt to help people recognize their brain state of
concentration or relaxation more conveniently and in real time. Considering the
inconvenience of wearing traditional multiple electrode electroencephalographs,
we choose Muse to collect data which is a portable headband launched lately
with a number of useful functions and channels and it is much easier for the
public to use. Besides, traditional online analysis did not focus on the
synchronism between users and computers and the time delay problem did exist.
To solve the problem, by building the Analytic Hierarchy Model, we choose the
two gamma wave channels of F7 and F8 as the data source instead of using both
beta and alpha channels traditionally.Using the Common Space Pattern algorithm
and the Support Vector Machine model, the channels we choose have a higher
recognition accuracy rate and smaller amount of data to be dealt with by the
computer than the traditional ones. Furthermore, we make use of the Feedforward
Neural Network Model to predict subjects'brain states in half a second.
Finally, we design a plane program in Python where a plane can be controlled to
go up or down when users concentrate or relax. The SVM model and the
Feedforward Neural Network model have both been tested by 12 subjects and they
give an evaluation ranging from 1 to 10 points. The former gets 7.58 points
while the latter gets 8.83, which proves that the time delay problem is
improved once more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07659</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07659</id><created>2015-09-25</created><authors><author><keyname>Rebollo-Neira</keyname><forenames>Laura</forenames></author><author><keyname>Aggarwal</keyname><forenames>Gagan</forenames></author></authors><title>A dedicated greedy pursuit algorithm for sparse spectral modelling of
  music sound</title><categories>cs.SD cs.MS</categories><comments>Routines for implementing the approach are available on
  http://www.nonlinear-approx.info/examples/node02.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dedicated algorithm for sparse spectral modeling of music sound is
presented. The goal is to enable the representation of a piece of music signal,
as a linear superposition of as few spectral components as possible. A
representation of this nature is said to be sparse. In the present context
sparsity is accomplished by greedy selection of the spectral components, from
an overcomplete set called a dictionary. The proposed algorithm is tailored to
be applied with trigonometric dictionaries. Its distinctive feature being that
it avoids the need for the actual construction of the whole dictionary, by
implementing the required operations via the Fast Fourier Transform. The
achieved sparsity is theoretically equivalent to that rendered by the
Orthogonal Matching Pursuit method. The contribution of the proposed dedicated
implementation is to extend the applicability of the standard Orthogonal
Matching Pursuit algorithm, by reducing its storage and computational demands.
The suitability of the approach for producing sparse spectral models is
illustrated by comparison with the traditional method, in the line of the Short
Fast Fourier Transform, involving only the corresponding orthonormal
trigonometric basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07669</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07669</id><created>2015-09-25</created><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Pratt</keyname><forenames>Simon</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Roeloffzen</keyname><forenames>Marcel</forenames></author></authors><title>Time-Space Trade-offs for Triangulating a Simple Polygon</title><categories>cs.CG</categories><comments>15 pages, 5 figures, 1 algorithm, submitted to STACS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An s-workspace algorithm is an algorithm that has read-only access to the
values of the input and only uses O(s) additional words of space. We give a
randomized s-workspace algorithm for triangulating a simple polygon P of n
vertices, for any $s \in {\Omega}(log n) \cap O(n)$. The algorithm runs in
$O(n^2/s)$ expected time. We also extend the approach to compute other similar
structures such as the shortest-path map (or tree) of any point $p \in P$ , or
how to partition P using only diagonals of the polygon so that the resulting
sub-polygons have {\Theta}(s) vertices each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07675</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07675</id><created>2015-09-25</created><authors><author><keyname>Mijumbi</keyname><forenames>Rashid</forenames></author><author><keyname>Serrat</keyname><forenames>Joan</forenames></author><author><keyname>Gorricho</keyname><forenames>Juan Luis</forenames></author><author><keyname>Bouten</keyname><forenames>Niels</forenames></author><author><keyname>De Turck</keyname><forenames>Filip</forenames></author><author><keyname>Boutaba</keyname><forenames>Raouf</forenames></author></authors><title>Network Function Virtualization: State-of-the-art and Research
  Challenges</title><categories>cs.NI</categories><comments>28 Pages in IEEE Communications Surveys and Tutorials. September 2015</comments><doi>10.1109/COMST.2015.2477041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Function Virtualization (NFV) has drawn significant attention from
both industry and academia as an important shift in telecommunication service
provisioning. By decoupling Network Functions (NFs) from the physical devices
on which they run, NFV has the potential to lead to significant reductions in
Operating Expenses (OPEX) and Capital Expenses (CAPEX) and facilitate the
deployment of new services with increased agility and faster time-to-value. The
NFV paradigm is still in its infancy and there is a large spectrum of
opportunities for the research community to develop new architectures, systems
and applications, and to evaluate alternatives and trade-offs in developing
technologies for its successful deployment. In this paper, after discussing NFV
and its relationship with complementary fields of Software Defined Networking
(SDN) and cloud computing, we survey the state-of-the-art in NFV, and identify
promising research directions in this area. We also overview key NFV projects,
standardization efforts, early implementations, use cases and commercial
products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07680</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07680</id><created>2015-09-25</created><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Li</keyname><forenames>Zhentao</forenames></author><author><keyname>Reed</keyname><forenames>Bruce</forenames></author></authors><title>Connectivity Preserving Iterative Compaction and Finding 2 Disjoint
  Rooted Paths in Linear Time</title><categories>cs.DS cs.DM math.CO</categories><comments>83 pages, 1 figure</comments><msc-class>05C83, 05C85</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to combine two algorithmic techniques to obtain
linear time algorithms for various optimization problems on graphs, and present
a subroutine which will be useful in doing so.
  The first technique is iterative shrinking. In the first phase of an
iterative shrinking algorithm, we construct a sequence of graphs of decreasing
size $G_1,\ldots,G_\ell$ where $G_1$ is the initial input, $G_\ell$ is a graph
on which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some
shrinking algorithm. In the second phase we work through the sequence in
reverse, repeatedly constructing a solution for a graph from the solution for
its successor. In an iterative compaction algorithm, we insist that the graphs
decrease by a constant fraction of the entire graph.
  Another approach to solving optimization problems is to exploit the
structural properties implied by the connectivity of the input graph. This
approach can be used on graphs which are not highly connected by decomposing an
input graph into its highly connected pieces, solving subproblems on these
specially structured pieces and then combining their solutions.
  We combine these two techniques by developing compaction algorithms which
when applied to the highly connected pieces preserve their connectivity
properties. The structural properties this connectivity implies can be helpful
both in finding further compactions in later iterations and when we are
manipulating solutions in the second phase of an iterative compaction
algorithm.
  To illustrate how this compaction algorithm can be used as a subroutine, we
present a linear time algorithm that given four vertices $\{s_1,s_2,t_1,t_2\}$
of a graph $G$, either finds a pair of disjoint paths $P_1$ and $P_2$ of $G$
such that $P_i$ has endpoints $s_i$ and $t_i$, or returns a planar embedding of
an auxiliary graph which shows that no such pair exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07684</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07684</id><created>2015-09-25</created><authors><author><keyname>Mijumbi</keyname><forenames>Rashid</forenames></author><author><keyname>Serrat</keyname><forenames>Joan</forenames></author><author><keyname>Gorricho</keyname><forenames>Juan-Luis</forenames></author><author><keyname>Boutaba</keyname><forenames>Raouf</forenames></author></authors><title>A Path Generation Approach to Embedding of Virtual Networks</title><categories>cs.NI</categories><comments>14 Pages in IEEE Transactions on Network and Service Management,
  September 2015</comments><doi>10.1109/TNSM.2015.2459073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the virtualization of networks continues to attract attention from both
industry and academia, the Virtual Network Embedding (VNE) problem remains a
focus of researchers. This paper proposes a one-shot, unsplittable flow VNE
solution based on column generation. We start by formulating the problem as a
path-based mathematical program called the primal, for which we derive the
corresponding dual problem. We then propose an initial solution which is used,
first, by the dual problem and then by the primal problem to obtain a final
solution. Unlike most approaches, our focus is not only on embedding accuracy
but also on the scalability of the solution. In particular, the one-shot nature
of our formulation ensures embedding accuracy, while the use of column
generation is aimed at enhancing the computation time to make the approach more
scalable. In order to assess the performance of the proposed solution, we
compare it against four state of the art approaches as well as the optimal
link-based formulation of the one-shot embedding problem. Experiments on a
large mix of Virtual Network (VN) requests show that our solution is near
optimal (achieving about 95% of the acceptance ratio of the optimal solution),
with a clear improvement over existing approaches in terms of VN acceptance
ratio and average Substrate Network (SN) resource utilization, and a
considerable improvement (92% for a SN of 50 nodes) in time complexity compared
to the optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07686</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07686</id><created>2015-09-25</created><authors><author><keyname>Cardinali</keyname><forenames>Ilaria</forenames></author><author><keyname>Giuzzi</keyname><forenames>Luca</forenames></author></authors><title>Polar Grassmannians and their Codes</title><categories>math.CO cs.IT math.IT</categories><comments>This is a copy of the Extended Abstract accepted for presentation at
  MEGA2015 in Trento</comments><msc-class>510A50, 51E22, 51A45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a concise description of Orthogonal Polar Grassmann Codes and
motivate their relevance. We also describe efficient encoding and decoding
algorithms for the case of Line Grassmannians and introduce some open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07687</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07687</id><created>2015-09-25</created><authors><author><keyname>Brinke</keyname><forenames>Chiel B. Ten</forenames></author><author><keyname>van Houten</keyname><forenames>Frank J. P.</forenames></author><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author></authors><title>Practical Algorithms for Linear Boolean-width</title><categories>cs.CC cs.DM</categories><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a number of new exact algorithms and heuristics to
compute linear boolean decompositions, and experimentally evaluate these
algorithms. The experimental evaluation shows that significant improvements can
be made with respect to running time without increasing the width of the
generated decompositions. We also evaluated dynamic programming algorithms on
linear boolean decompositions for several vertex subset problems. This
evaluation shows that such algorithms are often much faster (up to several
orders of magnitude) compared to theoretical worst case bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07694</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07694</id><created>2015-09-25</created><authors><author><keyname>Yodaiken</keyname><forenames>Victor</forenames></author></authors><title>Folding a Tree into a Map</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of the retrieval architecture of the highly influential UNIX file
system (\cite{Ritchie}\cite{multicsfs}) provides insight into design methods,
constraints, and possible alternatives. The basic architecture can be
understood in terms of function composition and recursion by anyone with some
mathematical maturity. Expertise in operating system coding or in any
specialized &quot;formal method&quot; is not required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07698</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07698</id><created>2015-09-25</created><authors><author><keyname>Tserpes</keyname><forenames>Konstantinos</forenames></author></authors><title>CONSENSUS Project: Identifying publicly acceptable policy
  implementations</title><categories>cs.CY</categories><comments>14th IFIP Electronic Government (EGOV) and 7th Electronic
  Participation (ePart) Conference 2015, Thessaloniki, Greece; 08/2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though it is unrealistic to expect citizens to pinpoint the policy
implementation that they prefer from the set of alternatives, it is still
possible to infer such information through an exercise of ranking the
importance of policy objectives according to their opinion. Assuming that the
mapping between policy options and objective evaluations is a priori known
(through models and simulations), this can be achieved either implicitly
through appropriate analysis of social media content related to the policy
objective in question or explicitly through the direct feedback provided in the
frame of a game. This document focuses on the presentation of a policy model,
which reduces the policy to a multi-objective optimization problem and
mitigates the shortcoming of the lack of social objective functions (public
opinion models) with a black-box, games-for-crowds approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07702</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07702</id><created>2015-09-25</created><updated>2015-10-05</updated><authors><author><keyname>Richard</keyname><forenames>Adrien</forenames></author></authors><title>Fixed points and connections between positive and negative cycles in
  Boolean networks</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the relationships between the number of fixed points in
a Boolean network $f:\{0,1\}^ n\to\{0,1\}^n$ and its interaction graph $G$,
which is the signed digraph on $\{1,\dots,n\}$ that describes the positive and
negative influences between the components of the network. A fundamental
theorem of Aracena, suggested by the biologist Thomas, says that if $G$ has no
positive (resp. negative) cycles, then $f$ has at most (resp. at least) one
fixed point; the sign of a cycle being the product of the signs of its arcs.
Here we generalize this result by taking into account the influence of
connections between positive and negative cycles. In particular, we prove that
if every positive (resp. negative) cycle of $G$ has an arc $a$ such that
$G\setminus a$ has a non-trivial initial strongly connected component
containing the final vertex of $a$ and only negative (resp. positive) cycles,
then $f$ has at most (resp. at least) one fixed point. Besides, Aracena proved
that if $G$ is strongly connected and has no negative cycles, then $f$ has two
fixed points with Hamming distance $n$, and we prove that the same conclusion
can be obtained under the following condition: $G$ is strongly connected, has a
unique negative cycle $C$, has at least one positive cycle, and $f$ canalizes
no arc of $C$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07714</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07714</id><created>2015-09-25</created><authors><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author><author><keyname>Kumari</keyname><forenames>Priti</forenames></author></authors><title>Cyclic codes from the first class two-prime Whiteman's generalized
  cyclotomic sequence with order 6</title><categories>cs.IT math.IT</categories><comments>21 pages. arXiv admin note: text overlap with arXiv:1507.05506</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary Whiteman's cyclotomic sequences of orders 2 and 4 have a number of
good randomness properties. In this paper, we compute the autocorrelation
values and linear complexity of the first class two-prime Whiteman's
generalized cyclotomic sequence (WGCS-I) of order $d=6$. Our results show that
the autocorrelation values of this sequence is four-valued or five-valued if
$(n_1-1)(n_2-1)/36$ is even or odd respectively, where $n_1$ and $n_2$ are two
distinct odd primes and their linear complexity is quite good. We employ the
two-prime WGCS-I of order 6 to construct several classes of cyclic codes over
$\mathrm{GF}(q)$ with length $n_1n_2$. We also obtain the lower bounds on the
minimum distance of these cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07715</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07715</id><created>2015-09-25</created><authors><author><keyname>Li</keyname><forenames>Yixuan</forenames></author><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Bindel</keyname><forenames>David</forenames></author><author><keyname>Hopcroft</keyname><forenames>John</forenames></author></authors><title>Uncovering the Small Community Structure in Large Networks: A Local
  Spectral Approach</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>10pages, published in WWW2015 proceedings</comments><acm-class>G.2.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large graphs arise in a number of contexts and understanding their structure
and extracting information from them is an important research area. Early
algorithms on mining communities have focused on the global structure, and
often run in time functional to the size of the entire graph. Nowadays, as we
often explore networks with billions of vertices and find communities of size
hundreds, it is crucial to shift our attention from macroscopic structure to
microscopic structure when dealing with large networks. A growing body of work
has been adopting local expansion methods in order to identify the community
from a few exemplary seed members.
  In this paper, we propose a novel approach for finding overlapping
communities called LEMON (Local Expansion via Minimum One Norm). Different from
PageRank-like diffusion methods, LEMON finds the community by seeking a sparse
vector in the span of the local spectra such that the seeds are in its support.
We show that LEMON can achieve the highest detection accuracy among
state-of-the-art proposals. The running time depends on the size of the
community rather than that of the entire graph. The algorithm is easy to
implement, and is highly parallelizable.
  Moreover, given that networks are not all similar in nature, a comprehensive
analysis on how the local expansion approach is suited for uncovering
communities in different networks is still lacking. We thoroughly evaluate our
approach using both synthetic and real-world datasets across different domains,
and analyze the empirical variations when applying our method to inherently
different networks in practice. In addition, the heuristics on how the quality
and quantity of the seed set would affect the performance are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07720</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07720</id><created>2015-09-23</created><authors><author><keyname>Sousbie</keyname><forenames>Thierry</forenames></author><author><keyname>Colombi</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>ColDICE: a parallel Vlasov-Poisson solver using moving adaptive
  simplicial tessellation</title><categories>physics.comp-ph astro-ph.CO cs.CG physics.flu-dyn</categories><comments>Code and illustration movies available at:
  http://www.vlasix.org/index.php?n=Main.ColDICE - Article submitted to Journal
  of Computational Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resolving numerically Vlasov-Poisson equations for initially cold systems can
be reduced to following the evolution of a three-dimensional sheet evolving in
six-dimensional phase-space. We describe a public parallel numerical algorithm
consisting in representing the phase-space sheet with a conforming,
self-adaptive simplicial tessellation of which the vertices follow the
Lagrangian equations of motion. The algorithm is implemented both in six- and
four-dimensional phase-space. Refinement of the tessellation mesh is performed
using the bisection method and a local representation of the phase-space sheet
at second order relying on additional tracers created when needed at runtime.
In order to preserve in the best way the Hamiltonian nature of the system,
refinement is anisotropic and constrained by measurements of local Poincar\'e
invariants. Resolution of Poisson equation is performed using the fast Fourier
method on a regular rectangular grid, similarly to particle in cells codes. To
compute the density projected onto this grid, the intersection of the
tessellation and the grid is calculated using the method of Franklin and
Kankanhalli (1993) generalised to linear order. As preliminary tests of the
code, we study in four dimensional phase-space the evolution of an initially
small patch in a chaotic potential and the cosmological collapse of a
fluctuation composed of two sinusoidal waves. We also perform a &quot;warm&quot; dark
matter simulation in six-dimensional phase-space that we use to check the
parallel scaling of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07728</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07728</id><created>2015-09-25</created><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Online Stochastic Linear Optimization under One-bit Feedback</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a special bandit setting of online stochastic linear
optimization, where only one-bit of information is revealed to the learner at
each round. This problem has found many applications including online
advertisement and online recommendation. We assume the binary feedback is a
random variable generated from the logit model, and aim to minimize the regret
defined by the unknown linear function. Although the existing method for
generalized linear bandit can be applied to our problem, the high computational
cost makes it impractical for real-world problems. To address this challenge,
we develop an efficient online learning algorithm by exploiting particular
structures of the observation model. Specifically, we adopt online Newton step
to estimate the unknown parameter and derive a tight confidence region based on
the exponential concavity of the logistic loss. Our analysis shows that the
proposed algorithm achieves a regret bound of $O(d\sqrt{T})$, which matches the
optimal result of stochastic linear bandits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07741</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07741</id><created>2015-09-25</created><authors><author><keyname>Ochando</keyname><forenames>Manuel Bl&#xe1;zquez</forenames></author></authors><title>A vulnerability in Google AdSense: Automatic extraction of links to ads</title><categories>cs.CY cs.CR</categories><comments>8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the basis of the XSS (Cross Site Scripting) and Web Crawler techniques it
is possible to go through the barriers of the Google Adsense advertising system
by obtaining the validated links of the ads published on a website. Such method
involves obtaining the source code built for the Google java applet for
publishing and handling ads and for the final link retrieval. Once the links of
the ads have been obtained, you can use the user sessions visiting other
websites to load such links, in the background, by a simple re-direction,
through a hidden iframe, so that the IP addresses clicking are different in
each case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07755</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07755</id><created>2015-09-25</created><authors><author><keyname>Chang</keyname><forenames>Cheng-Shang</forenames></author><author><keyname>Liao</keyname><forenames>Wanjiun</forenames></author><author><keyname>Chen</keyname><forenames>Yu-Sheng</forenames></author><author><keyname>Liou</keyname><forenames>Li-Heng</forenames></author></authors><title>A Mathematical Theory for Clustering in Metric Spaces</title><categories>cs.LG</categories><doi>10.1109/TNSE.2016.2516339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is one of the most fundamental problems in data analysis and it
has been studied extensively in the literature. Though many clustering
algorithms have been proposed, clustering theories that justify the use of
these clustering algorithms are still unsatisfactory. In particular, one of the
fundamental challenges is to address the following question:
  What is a cluster in a set of data points?
  In this paper, we make an attempt to address such a question by considering a
set of data points associated with a distance measure (metric). We first
propose a new cohesion measure in terms of the distance measure. Using the
cohesion measure, we define a cluster as a set of points that are cohesive to
themselves. For such a definition, we show there are various equivalent
statements that have intuitive explanations. We then consider the second
question:
  How do we find clusters and good partitions of clusters under such a
definition?
  For such a question, we propose a hierarchical agglomerative algorithm and a
partitional algorithm. Unlike standard hierarchical agglomerative algorithms,
our hierarchical agglomerative algorithm has a specific stopping criterion and
it stops with a partition of clusters. Our partitional algorithm, called the
K-sets algorithm in the paper, appears to be a new iterative algorithm. Unlike
the Lloyd iteration that needs two-step minimization, our K-sets algorithm only
takes one-step minimization.
  One of the most interesting findings of our paper is the duality result
between a distance measure and a cohesion measure. Such a duality result leads
to a dual K-sets algorithm for clustering a set of data points with a cohesion
measure. The dual K-sets algorithm converges in the same way as a sequential
version of the classical kernel K-means algorithm. The key difference is that a
cohesion measure does not need to be positive semi-definite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07757</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07757</id><created>2015-09-25</created><authors><author><keyname>Das</keyname><forenames>Soham</forenames></author><author><keyname>Halder</keyname><forenames>Kishaloy</forenames></author><author><keyname>Pratihar</keyname><forenames>Sanjoy</forenames></author><author><keyname>Bhowmick</keyname><forenames>Partha</forenames></author></authors><title>Properties of Farey Sequence and their Applications to Digital Image
  Processing</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Farey sequence has been a topic of interest to the mathematicians since the
very beginning of last century. With the emergence of various algorithms
involving the digital plane in recent times, several interesting works related
with the Farey sequence have come up. Our work is related with the problem of
searching an arbitrary fraction in a Farey sequence and its relevance to image
processing. Given an arbitrary fraction p/q (0 &lt; p &lt; q) and a Farey sequence Fn
of order n, we propose a novel algorithm using the Regula Falsi method and the
concept of Farey table to efficiently find the fraction of Fn closest to p/q.
All computations are in the integer domain only, which is its added benefit.
Some contemporary applications of image processing have also been shown where
such concepts can be incorporated. Experimental results have been furnished to
demonstrate its efficiency and elegance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07759</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07759</id><created>2015-09-25</created><updated>2015-10-10</updated><authors><author><keyname>Wei</keyname><forenames>Xiaohan</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Delay Optimal Power Aware Opportunistic Scheduling with Mutual
  Information Accumulation</title><categories>cs.IT cs.PF math.IT</categories><comments>Double column, 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers optimization of power and delay in a time-varying
wireless link using rateless codes. The link serves a sequence of
variable-length packets. Each packet is coded and transmitted over multiple
slots. Channel conditions can change from slot to slot and are unknown to the
transmitter. The amount of mutual information accumulated on each slot depends
on the random channel realization and the power used. The goal is to minimize
average service delay subject to an average power constraint. We formulate this
problem as a frame-based stochastic optimization problem and solve it via an
online algorithm. We show that the subproblem within each frame is a simple
integer program which can be effectively solved using a dynamic program. The
optimality of this online algorithm is proved using the frame-based Lyapunov
drift analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07761</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07761</id><created>2015-09-25</created><updated>2015-12-08</updated><authors><author><keyname>Novak</keyname><forenames>Petra Kralj</forenames></author><author><keyname>Smailovi&#x107;</keyname><forenames>Jasmina</forenames></author><author><keyname>Sluban</keyname><forenames>Borut</forenames></author><author><keyname>Mozeti&#x10d;</keyname><forenames>Igor</forenames></author></authors><title>Sentiment of Emojis</title><categories>cs.CL</categories><journal-ref>PLoS ONE 10(12): e0144296, 2015</journal-ref><doi>10.1371/journal.pone.0144296</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a new generation of emoticons, called emojis, that is increasingly
being used in mobile communications and social media. In the past two years,
over ten billion emojis were used on Twitter. Emojis are Unicode graphic
symbols, used as a shorthand to express concepts and ideas. In contrast to the
small number of well-known emoticons that carry clear emotional contents, there
are hundreds of emojis. But what are their emotional contents? We provide the
first emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw a
sentiment map of the 751 most frequently used emojis. The sentiment of the
emojis is computed from the sentiment of the tweets in which they occur. We
engaged 83 human annotators to label over 1.6 million tweets in 13 European
languages by the sentiment polarity (negative, neutral, or positive). About 4%
of the annotated tweets contain emojis. The sentiment analysis of the emojis
allows us to draw several interesting conclusions. It turns out that most of
the emojis are positive, especially the most popular ones. The sentiment
distribution of the tweets with and without emojis is significantly different.
The inter-annotator agreement on the tweets with emojis is higher. Emojis tend
to occur at the end of the tweets, and their sentiment polarity increases with
the distance. We observe no significant differences in the emoji rankings
between the 13 languages and the Emoji Sentiment Ranking. Consequently, we
propose our Emoji Sentiment Ranking as a European language-independent resource
for automated sentiment analysis. Finally, the paper provides a formalization
of sentiment and a novel visualization in the form of a sentiment bar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07766</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07766</id><created>2015-09-25</created><authors><author><keyname>Sattath</keyname><forenames>Or</forenames></author><author><keyname>Morampudi</keyname><forenames>Siddhardh C.</forenames></author><author><keyname>Laumann</keyname><forenames>Christopher R.</forenames></author><author><keyname>Moessner</keyname><forenames>Roderich</forenames></author></authors><title>When must a local Hamiltonian be frustration free?</title><categories>quant-ph cond-mat.stat-mech cond-mat.str-el cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A broad range of quantum optimisation problems can be phrased as the question
whether a specific system has a ground state at zero energy, i.e. whether its
Hamiltonian is frustration free. Frustration-free Hamiltonians, in turn, play a
central role for constructing and understanding new phases of matter in quantum
many-body physics. Unfortunately, determining whether this is the case is known
to be a complexity-theoretically intractable problem. This makes it highly
desirable to search for efficient heuristics and algorithms in order to, at
least, partially answer this question. Here we prove a general criterion -- a
sufficient condition -- under which a local Hamiltonian is guaranteed to be
frustration free by lifting Shearer's theorem from classical probability theory
to the quantum world. Remarkably, evaluating this condition proceeds via a
fully classical analysis of a hard-core lattice gas at negative fugacity on the
Hamiltonian's interaction graph which as a statistical mechanics problem is of
interest in their own right. We concretely apply this criterion to local
Hamiltonians on various regular lattices, while bringing to bear the tools of
spin glass theory which permit us to obtain new bounds on the SAT/UNSAT
transition in random quantum satisfiability. These also lead us to natural
conjectures for when such bounds will be exact (tight), as well as to a novel
notion of universality for these computer science problems. Besides providing
concrete algorithms leading to detailed and quantitative insights, this
underscores the power of marrying classical statistical mechanics with quantum
computation and complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07776</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07776</id><created>2015-09-25</created><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author><author><keyname>Ryabko</keyname><forenames>Boris</forenames></author></authors><title>Predicting the outcomes of every process for which an asymptotically
  accurate stationary predictor exists is impossible</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>appears in the proceedings of ISIT 2015, pp. 1204-1206, Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of prediction consists in forecasting the conditional
distribution of the next outcome given the past. Assume that the source
generating the data is such that there is a stationary ergodic predictor whose
error converges to zero (in a certain sense). The question is whether there is
a universal predictor for all such sources, that is, a predictor whose error
goes to zero if any of the sources that have this property is chosen to
generate the data. This question is answered in the negative, contrasting a
number of previously established positive results concerning related but
smaller sets of processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07789</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07789</id><created>2015-09-25</created><authors><author><keyname>de Beaudrap</keyname><forenames>Niel</forenames></author></authors><title>On exact counting and quasi-quantum complexity</title><categories>cs.CC quant-ph</categories><comments>22 pages, 6 figures. Revised draft of a submission to TQC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present characterisations of &quot;exact&quot; gap-definable classes, in terms of
indeterministic models of computation which slightly modify the standard model
of quantum computation. This follows on work of Aaronson
[arXiv:quant-ph/0412187], who shows that the counting class PP can be
characterised in terms of bounded-error &quot;quantum&quot; algorithms which use
invertible (and possibly non-unitary) transformations, or postselections on
events of non-zero probability. Our work considers similar modifications of the
quantum computational model, but in the setting of exact algorithms, and
algorithms with zero error and constant success probability. We show that the
gap-definable counting classes [J. Comput. Syst. Sci. 48 (1994), p.116] which
bound exact and zero-error quantum algorithms can be characterised in terms of
&quot;quantum-like&quot; algorithms involving nonunitary gates, and that postselection
and nonunitarity have equivalent power for exact quantum computation only if
these classes collapse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07791</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07791</id><created>2015-09-25</created><authors><author><keyname>Chen</keyname><forenames>Yu Christine</forenames></author><author><keyname>Dhople</keyname><forenames>Sairaj</forenames></author></authors><title>Power Divider</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives analytical closed-form expressions that uncover the
contributions of nodal active- and reactive-power injections to the active- and
reactive-power flows on transmission lines in an AC electrical network. Paying
due homage to current- and voltage-divider laws that are similar in spirit, we
baptize these as the power divider laws. Derived from a circuit-theoretic
examination of AC power-flow expressions, the constitution of the power divider
laws reflects the topology and voltage profile of the network. We demonstrate
the utility of the power divider laws to the analysis of power networks by
highlighting applications to transmission-network allocation, transmission-loss
allocation, and identifying feasible injections while respecting line
active-power flow set points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07808</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07808</id><created>2015-09-25</created><authors><author><keyname>Levey</keyname><forenames>Elaine</forenames></author><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>A Lasserre-based $(1+\varepsilon)$-approximation for $Pm \mid
  p_j=1,\textrm{prec} \mid C_{\max}$</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a classical problem in scheduling, one has $n$ unit size jobs with a
precedence order and the goal is to find a schedule of those jobs on $m$
identical machines as to minimize the makespan. It is one of the remaining four
open problems from the book of Garey &amp; Johnson whether or not this problem is
$\mathbf{NP}$-hard for $m=3$.
  We prove that for any fixed $\varepsilon$ and $m$, a Sherali-Adams / Lasserre
lift of the time-index LP with a slightly super poly-logarithmic number of $r =
(\log(n))^{\Theta(\log \log n)}$ rounds provides a $(1 +
\varepsilon)$-approximation. This implies an algorithm that yields a
$(1+\varepsilon)$-approximation in time $n^{O(r)}$. The previously best
approximation algorithms guarantee a $2 - \frac{7}{3m+1}$-approximation in
polynomial time for $m \geq 4$ and $\frac{4}{3}$ for $m=3$. Our algorithm is
based on a recursive scheduling approach where in each step we reduce the
correlation in form of long chains. Our method adds to the rather short list of
examples where hierarchies are actually useful to obtain better approximation
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07813</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07813</id><created>2015-09-25</created><authors><author><keyname>Salau</keyname><forenames>Kehinde R.</forenames></author><author><keyname>Baggio</keyname><forenames>Jacopo A.</forenames></author><author><keyname>Janssen</keyname><forenames>Marco A.</forenames></author><author><keyname>Abbott</keyname><forenames>Joshua K.</forenames></author><author><keyname>Fenichel</keyname><forenames>Eli P.</forenames></author></authors><title>Taking a moment to measure Networks - A hierarchical approach</title><categories>cs.SI physics.soc-ph</categories><comments>Main Paper: 32 Pages, Suppl0Material: 9 pages</comments><acm-class>I.2.11; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-theoretic tools contribute to understanding real-world system
dynamics, e.g., in wildlife conservation, epidemics, and power outages. Network
visualization helps illustrate structural heterogeneity; however, details about
heterogeneity are lost when summarizing networks with a single mean-style
measure. Researchers have indicated that a hierarchical system composed of
multiple metrics may be a more useful determinant of structure, but a formal
method for grouping metrics is still lacking. We develop a hierarchy using the
statistical concept of moments and systematically test the hypothesis that this
system of metrics is sufficient to explain the variation in processes that take
place on networks, using an ecological systems example. Results indicate that
the moments approach outperforms single summary metrics and accounts for a
majority of the variation in process outcomes. The hierarchical measurement
scheme is helpful for indicating when additional structural information is
needed to describe system process outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07815</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07815</id><created>2015-09-25</created><authors><author><keyname>Escriva</keyname><forenames>Robert</forenames></author><author><keyname>Wong</keyname><forenames>Bernard</forenames></author><author><keyname>Sirer</keyname><forenames>Emin G&#xfc;n</forenames></author></authors><title>Warp: Lightweight Multi-Key Transactions for Key-Value Stores</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional NoSQL systems scale by sharding data across multiple servers and
by performing each operation on a small number of servers. Because transactions
on multiple keys necessarily require coordination across multiple servers,
NoSQL systems often explicitly avoid making transactional guarantees in order
to avoid such coordination. Past work on transactional systems control this
coordination by either increasing the granularity at which transactions are
ordered, sacrificing serializability, or by making clock synchronicity
assumptions.
  This paper presents a novel protocol for providing serializable transactions
on top of a sharded data store. Called acyclic transactions, this protocol
allows multiple transactions to prepare and commit simultaneously, improving
concurrency in the system, while ensuring that no cycles form between
concurrently-committing transactions. We have fully implemented acyclic
transactions in a document store called Warp. Experiments show that Warp
achieves 4 times higher throughput than Sinfonia's mini-transactions on the
standard TPC-C benchmark with no aborts. Further, the system achieves 75% of
the throughput of the non-transactional key-value store it builds upon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07821</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07821</id><created>2015-09-25</created><authors><author><keyname>Escriva</keyname><forenames>Robert</forenames></author><author><keyname>Sirer</keyname><forenames>Emin G&#xfc;n</forenames></author></authors><title>The Design and Implementation of the Wave Transactional Filesystem</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Wave Transactional Filesystem (WTF), a novel,
transactional, POSIX-compatible filesystem based on a new file slicing API that
enables efficient file transformations. WTF provides transactional access to a
distributed filesystem, eliminating the possibility of inconsistencies across
multiple files. Further, the file slicing API enables applications to construct
files from the contents of other files without having to rewrite or relocate
data. Combined, these enable a new class of high-performance applications.
Experiments show that WTF can qualitatively outperform the industry-standard
HDFS distributed filesystem, up to a factor of four in a sorting benchmark, by
reducing I/O costs. Microbenchmarks indicate that the new features of WTF
impose only a modest overhead on top of the POSIX-compatible API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07823</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07823</id><created>2015-09-25</created><authors><author><keyname>Huijse</keyname><forenames>Pablo</forenames></author><author><keyname>Estevez</keyname><forenames>Pablo A.</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author><author><keyname>Zegers</keyname><forenames>Pablo</forenames></author></authors><title>Computational Intelligence Challenges and Applications on Large-Scale
  Astronomical Time Series Databases</title><categories>astro-ph.IM cs.LG</categories><journal-ref>IEEE Computational Intelligence Magazine, vol. 9, n. 3, pp. 27-39,
  2014</journal-ref><doi>10.1109/MCI.2014.2326100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-domain astronomy (TDA) is facing a paradigm shift caused by the
exponential growth of the sample size, data complexity and data generation
rates of new astronomical sky surveys. For example, the Large Synoptic Survey
Telescope (LSST), which will begin operations in northern Chile in 2022, will
generate a nearly 150 Petabyte imaging dataset of the southern hemisphere sky.
The LSST will stream data at rates of 2 Terabytes per hour, effectively
capturing an unprecedented movie of the sky. The LSST is expected not only to
improve our understanding of time-varying astrophysical objects, but also to
reveal a plethora of yet unknown faint and fast-varying phenomena. To cope with
a change of paradigm to data-driven astronomy, the fields of astroinformatics
and astrostatistics have been created recently. The new data-oriented paradigms
for astronomy combine statistics, data mining, knowledge discovery, machine
learning and computational intelligence, in order to provide the automated and
robust methods needed for the rapid detection and classification of known
astrophysical objects as well as the unsupervised characterization of novel
phenomena. In this article we present an overview of machine learning and
computational intelligence applications to TDA. Future big data challenges and
new lines of research in TDA, focusing on the LSST, are identified and
discussed from the viewpoint of computational intelligence/machine learning.
Interdisciplinary collaboration will be required to cope with the challenges
posed by the deluge of astronomical data coming from the LSST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07831</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07831</id><created>2015-09-25</created><authors><author><keyname>Sung</keyname><forenames>Jaeyong</forenames></author><author><keyname>Lenz</keyname><forenames>Ian</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds,
  Language and Trajectories</title><categories>cs.RO cs.AI cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robot operating in a real-world environment needs to perform reasoning with
a variety of sensing modalities. However, manually designing features that
allow a learning algorithm to relate these different modalities can be
extremely challenging. In this work, we consider the task of manipulating novel
objects and appliances. To this end, we learn to embed point-cloud, natural
language, and manipulation trajectory data into a shared embedding space using
a deep neural network. In order to learn semantically meaningful spaces
throughout our network, we introduce a method for pre-training its lower layers
for multimodal feature embedding and a method for fine-tuning this embedding
space using a loss-based margin. We test our model on the Robobarista dataset
[22], where we achieve significant improvements in both accuracy and inference
time over the previous state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07838</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07838</id><created>2015-09-25</created><updated>2015-12-05</updated><authors><author><keyname>Ionescu</keyname><forenames>Catalin</forenames></author><author><keyname>Vantzos</keyname><forenames>Orestis</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Training Deep Networks with Structured Layers by Matrix Backpropagation</title><categories>cs.CV cs.AI</categories><comments>This is an extended version of our ICCV 2015 article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural network architectures have recently produced excellent results in
a variety of areas in artificial intelligence and visual recognition, well
surpassing traditional shallow architectures trained using hand-designed
features. The power of deep networks stems both from their ability to perform
local computations followed by pointwise non-linearities over increasingly
larger receptive fields, and from the simplicity and scalability of the
gradient-descent training procedure based on backpropagation. An open problem
is the inclusion of layers that perform global, structured matrix computations
like segmentation (e.g. normalized cuts) or higher-order pooling (e.g.
log-tangent space metrics defined over the manifold of symmetric positive
definite matrices) while preserving the validity and efficiency of an
end-to-end deep training framework. In this paper we propose a sound
mathematical apparatus to formally integrate global structured computation into
deep computation architectures. At the heart of our methodology is the
development of the theory and practice of backpropagation that generalizes to
the calculus of adjoint matrix variations. The proposed matrix backpropagation
methodology applies broadly to a variety of problems in machine learning or
computational perception. Here we illustrate it by performing visual
segmentation experiments using the BSDS and MSCOCO benchmarks, where we show
that deep networks relying on second-order pooling and normalized cuts layers,
trained end-to-end using matrix backpropagation, outperform counterparts that
do not take advantage of such global layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07845</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07845</id><created>2015-09-25</created><authors><author><keyname>Singh</keyname><forenames>Bharat</forenames></author><author><keyname>Han</keyname><forenames>Xintong</forenames></author><author><keyname>Wu</keyname><forenames>Zhe</forenames></author><author><keyname>Morariu</keyname><forenames>Vlad I.</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Selecting Relevant Web Trained Concepts for Automated Event Retrieval</title><categories>cs.CV cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex event retrieval is a challenging research problem, especially when no
training videos are available. An alternative to collecting training videos is
to train a large semantic concept bank a priori. Given a text description of an
event, event retrieval is performed by selecting concepts linguistically
related to the event description and fusing the concept responses on unseen
videos. However, defining an exhaustive concept lexicon and pre-training it
requires vast computational resources. Therefore, recent approaches automate
concept discovery and training by leveraging large amounts of weakly annotated
web data. Compact visually salient concepts are automatically obtained by the
use of concept pairs or, more generally, n-grams. However, not all visually
salient n-grams are necessarily useful for an event query--some combinations of
concepts may be visually compact but irrelevant--and this drastically affects
performance. We propose an event retrieval algorithm that constructs pairs of
automatically discovered concepts and then prunes those concepts that are
unlikely to be helpful for retrieval. Pruning depends both on the query and on
the specific video instance being evaluated. Our approach also addresses
calibration and domain adaptation issues that arise when applying concept
detectors to unseen videos. We demonstrate large improvements over other vision
based systems on the TRECVID MED 13 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07857</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07857</id><created>2015-09-25</created><authors><author><keyname>Pritychenko</keyname><forenames>B.</forenames></author></authors><title>Fractional Authorship in Nuclear Physics</title><categories>cs.DL nucl-th physics.soc-ph</categories><comments>8 pages, 4 Figures</comments><report-no>Brookhaven National Laboratory Report BNL-108445-2015-JA</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large, multi-institutional groups or collaborations of scientists are engaged
in nuclear physics research projects, and the number of research facilities is
dwindling. These collaborations have their own authorship rules, and they
produce a large number of highly-cited papers. Multiple authorship of nuclear
physics publications creates a problem with the assessment of an individual
author's productivity relative to his/her colleagues and renders ineffective a
performance metrics solely based on annual publication and citation counts.
Many institutions are increasingly relying on the total number of first-author
papers; however, this approach becomes counterproductive for large research
collaborations with an alphabetical order of authors. A concept of fractional
authorship (the claiming of credit for authorship by more than one individual)
helps to clarify this issue by providing a more complete picture of research
activities. In the present work, nuclear physics fractional and total
authorships have been investigated using nuclear data mining techniques.
Historic total and fractional authorship averages have been extracted from the
Nuclear Science References (NSR) database, and the current range of fractional
contributions has been deduced. The results of this study and their
implications are discussed and conclusions presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07859</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07859</id><created>2015-09-25</created><updated>2016-01-24</updated><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Information Limits for Recovering a Hidden Community</title><categories>stat.ML cs.IT math.IT</categories><comments>v2 establishes information limits of both weak and exact recovery
  with sharp constants for general P and Q</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recovering a hidden community of cardinality $K$ from
an $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$,
$A_{ij} \sim P$ if $i, j$ both belong to the community and $A_{ij} \sim Q$
otherwise, for two known probability distributions $P$ and $Q$ depending on
$n$. If $P={\rm Bern}(p)$ and $Q={\rm Bern}(q)$ with $p&gt;q$, it reduces to the
problem of finding a densely-connected $K$-subgraph planted in a large
Erd\&quot;os-R\'enyi graph; if $P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with
$\mu&gt;0$, it corresponds to the problem of locating a $K \times K$ principal
submatrix of elevated means in a large Gaussian random matrix. We focus on two
types of asymptotic recovery guarantees as $n \to \infty$: (1) weak recovery:
expected number of classification errors is $o(K)$; (2) exact recovery:
probability of classifying all indices correctly converges to one. Under mild
assumptions on $P$ and $Q$, and allowing the community size to scale
sublinearly with $n$, we derive a set of sufficient conditions and a set of
necessary conditions for recovery, which are asymptotically tight with sharp
constants. The results hold in particular for the Gaussian case, and for the
case of bounded log likelihood ratio, including the Bernoulli case whenever
$\frac{p}{q}$ and $\frac{1-p}{1-q}$ are bounded away from zero and infinity. An
important algorithmic implication is that, whenever exact recovery is
information theoretically possible, any algorithm that provides weak recovery
when the community size is concentrated near $K$ can be upgraded to achieve
exact recovery in linear additional time by a simple voting procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07860</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07860</id><created>2015-09-25</created><authors><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author><author><keyname>Farahmand</keyname><forenames>Amir-massoud</forenames></author><author><keyname>Xia</keyname><forenames>Meng</forenames></author></authors><title>Learning-Based Modular Indirect Adaptive Control for a Class of
  Nonlinear Systems</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1507.05120</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper the problem of adaptive trajectory tracking control
for a class of nonlinear systems with parametric uncertainties. We propose to
use a modular approach, where we first design a robust nonlinear state feedback
which renders the closed loop input-to-state stable (ISS), where the input is
considered to be the estimation error of the uncertain parameters, and the
state is considered to be the closed-loop output tracking error. Next, we
augment this robust ISS controller with a model-free learning algorithm to
estimate the model uncertainties. We implement this method with two different
learning approaches. The first one is a model-free multi-parametric extremum
seeking (MES) method and the second is a Bayesian optimization-based method
called Gaussian Process Upper Confidence Bound (GP-UCB). The combination of the
ISS feedback and the learning algorithms gives a learning-based modular
indirect adaptive controller. We show the efficiency of this approach on a
two-link robot manipulator example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07892</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07892</id><created>2015-09-25</created><authors><author><keyname>Kantchelian</keyname><forenames>Alex</forenames></author><author><keyname>Tygar</keyname><forenames>J. D.</forenames></author><author><keyname>Joseph</keyname><forenames>Anthony D.</forenames></author></authors><title>Evasion and Hardening of Tree Ensemble Classifiers</title><categories>cs.LG cs.CR stat.ML</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has successfully constructed adversarial &quot;evading&quot; instances for
differentiable prediction models. However generating adversarial instances for
tree ensembles, a piecewise constant class of models, has remained an open
problem. In this paper, we construct both exact and approximate evasion
algorithms for tree ensembles: for a given instance x we find the &quot;nearest&quot;
instance x' such that the classifier predictions of x and x' are different.
First, we show that finding such instances is practically possible despite tree
ensemble models being non-differentiable and the optimal evasion problem being
NP-hard.
  In addition, we quantify the susceptibility of such models applied to the
task of recognizing handwritten digits by measuring the distance between the
original instance and the modified instance under the L0, L1, L2 and L-infinity
norms. We also analyze a wide variety of classifiers including linear and
RBF-kernel models, max-ensemble of linear models, and neural networks for
comparison purposes. Our analysis shows that tree ensembles produced by a
state-of-the-art gradient boosting method are consistently the least robust
models notwithstanding their competitive accuracy. Finally, we show that a
sufficient number of retraining rounds with L0-adversarial instances makes the
hardened model three times harder to evade. This retraining set also marginally
improves classification accuracy, but simultaneously makes the model more
susceptible to L1, L2 and L-infinity evasions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07897</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07897</id><created>2015-09-16</created><authors><author><keyname>Vol</keyname><forenames>E. D.</forenames></author></authors><title>Quantum Look at two Common Logics: the Logic of Primitive Thinking and
  the Logic of Everyday Human Reasoning</title><categories>q-bio.NC cs.AI math.LO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on ideas of quantum theory of open systems and psychological dual
system theory we propose two novel versions of Non-Boolean logic. The first
version can be interpreted in our opinion as simplified description of
primitive (mythological) thinking and the second one as the toy model of
everyday human reasoning in which aside from logical deduction, heuristic
elements and beliefs also play the considerable role. Several arguments in
favor of the interpretations proposed are adduced and discussed in the paper as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07899</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07899</id><created>2015-09-25</created><authors><author><keyname>Ferenbaugh</keyname><forenames>Charles R.</forenames></author></authors><title>Cultural Barriers to Software Productivity Practices at Los Alamos</title><categories>cs.SE</categories><comments>2 pages. Submission to Computational Science &amp; Engineering Software
  Sustainability and Productivity Challenges (CSESSP) Workshop, Rockville, MD,
  October 15th-16th, 2015</comments><report-no>LA-UR-15-24226</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, code projects in the nuclear weapons program at Los Alamos
National Laboratory (LANL) have given increased attention to modern software
productivity practices. We found that some of the biggest barriers to adoption
of new practices were not technical but cultural. This paper describes several
of the cultural differences between the physics and computer science
communities at LANL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07919</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07919</id><created>2015-09-25</created><authors><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Serban</keyname><forenames>Radu</forenames></author><author><keyname>Negrut</keyname><forenames>Dan</forenames></author></authors><title>Analysis of A Splitting Approach for the Parallel Solution of Linear
  Systems on GPU Cards</title><categories>cs.DC cs.MS cs.NA</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss an approach for solving sparse or dense banded linear systems
${\bf A} {\bf x} = {\bf b}$ on a Graphics Processing Unit (GPU) card. The
matrix ${\bf A} \in {\mathbb{R}}^{N \times N}$ is possibly nonsymmetric and
moderately large; i.e., $10000 \leq N \leq 500000$. The ${\it split\ and\
parallelize}$ (${\tt SaP}$) approach seeks to partition the matrix ${\bf A}$
into diagonal sub-blocks ${\bf A}_i$, $i=1,\ldots,P$, which are independently
factored in parallel. The solution may choose to consider or to ignore the
matrices that couple the diagonal sub-blocks ${\bf A}_i$. This approach, along
with the Krylov subspace-based iterative method that it preconditions, are
implemented in a solver called ${\tt SaP::GPU}$, which is compared in terms of
efficiency with three commonly used sparse direct solvers: ${\tt PARDISO}$,
${\tt SuperLU}$, and ${\tt MUMPS}$. ${\tt SaP::GPU}$, which runs entirely on
the GPU except several stages involved in preliminary row-column permutations,
is robust and compares well in terms of efficiency with the aforementioned
direct solvers. In a comparison against Intel's ${\tt MKL}$, ${\tt SaP::GPU}$
also fares well when used to solve dense banded systems that are close to being
diagonally dominant. ${\tt SaP::GPU}$ is publicly available and distributed as
open source under a permissive BSD3 license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07927</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07927</id><created>2015-09-25</created><authors><author><keyname>Hanawal</keyname><forenames>Manjesh K.</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Algorithms for Linear Bandits on Polyhedral Sets</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study stochastic linear optimization problem with bandit feedback. The set
of arms take values in an $N$-dimensional space and belong to a bounded
polyhedron described by finitely many linear inequalities. We provide a lower
bound for the expected regret that scales as $\Omega(N\log T)$. We then provide
a nearly optimal algorithm and show that its expected regret scales as
$O(N\log^{1+\epsilon}(T))$ for an arbitrary small $\epsilon &gt;0$. The algorithm
alternates between exploration and exploitation intervals sequentially where
deterministic set of arms are played in the exploration intervals and greedily
selected arm is played in the exploitation intervals. We also develop an
algorithm that achieves the optimal regret when sub-Gaussianity parameter of
the noise term is known. Our key insight is that for a polyhedron the optimal
arm is robust to small perturbations in the reward function. Consequently, a
greedily selected arm is guaranteed to be optimal when the estimation error
falls below some suitable threshold. Our solution resolves a question posed by
Rusmevichientong and Tsitsiklis (2011) that left open the possibility of
efficient algorithms with asymptotic logarithmic regret bounds. We also show
that the regret upper bounds hold with probability $1$. Our numerical
investigations show that while theoretical results are asymptotic the
performance of our algorithms compares favorably to state-of-the-art algorithms
in finite time as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07928</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07928</id><created>2015-09-25</created><authors><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author></authors><title>Quantized Massive MU-MIMO-OFDM Uplink</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coarse quantization at the base station (BS) of a massive multi-user (MU)
multiple-input multiple-output (MIMO) wireless system promises significant
power and cost savings. Coarse quantization also enables significant reductions
of the raw analog-to-digital converter (ADC) data that must be transferred from
a spatially-separated antenna array to the baseband processing unit. The
theoretical limits as well as practical transceiver algorithms for such
quantized MU-MIMO systems operating over frequency-flat, narrowband channels
have been studied extensively. However, the practically relevant scenario where
such communication systems operate over frequency-selective, wideband channels
is less well understood. This paper investigates the uplink performance of a
quantized massive MU-MIMO system that deploys orthogonal frequency-division
multiplexing (OFDM) for wideband communication. We propose new algorithms for
quantized maximum a-posteriori (MAP) channel estimation and data detection, and
we study the associated performance/quantization trade-offs. Our results
demonstrate that coarse quantization (e.g., four to six bits, depending on the
ratio between the number of BS antennas and the number of users) in massive
MU-MIMO-OFDM systems entails virtually no performance loss compared to the
infinite-precision case at no additional cost in terms of baseband processing
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07935</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07935</id><created>2015-09-25</created><updated>2015-12-15</updated><authors><author><keyname>Li</keyname><forenames>Weidong</forenames></author><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaolu</forenames></author><author><keyname>Zhang</keyname><forenames>Xuejie</forenames></author></authors><title>A note on the dynamic dominant resource fairness mechanism</title><categories>cs.GT cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-resource fair allocation has beena hot topic of resource allocation.
Most recently, a dynamic dominant resource fairness (DRF) mechanism is proposed
for dynamic multi-resource fair allocation. In this paper, we prove that the
competitive ratio of the dynamic DRF mechanism is the reciprocal of the number
of resource types, for two different objectives. Moreover, we develop a
linear-time algorithm to find a dynamic DRF solution at each step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07943</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07943</id><created>2015-09-25</created><authors><author><keyname>Huang</keyname><forenames>Qingqing</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author></authors><title>Super-Resolution Off the Grid</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Super-resolution is the problem of recovering a superposition of point
sources using bandlimited measurements, which may be corrupted with noise. This
signal processing problem arises in numerous imaging problems, ranging from
astronomy to biology to spectroscopy, where it is common to take (coarse)
Fourier measurements of an object. Of particular interest is in obtaining
estimation procedures which are robust to noise, with the following desirable
statistical and computational properties: we seek to use coarse Fourier
measurements (bounded by some cutoff frequency); we hope to take a
(quantifiably) small number of measurements; we desire our algorithm to run
quickly.
  Suppose we have k point sources in d dimensions, where the points are
separated by at least \Delta from each other (in Euclidean distance). This work
provides an algorithm with the following favorable guarantees: - The algorithm
uses Fourier measurements, whose frequencies are bounded by O(1/\Delta) (up to
log factors). Previous algorithms require a cutoff frequency which may be as
large as {\Omega}( d/\Delta). - The number of measurements taken by and the
computational complexity of our algorithm are bounded by a polynomial in both
the number of points k and the dimension d, with no dependence on the
separation \Delta. In contrast, previous algorithms depended inverse
polynomially on the minimal separation and exponentially on the dimension for
both of these quantities.
  Our estimation procedure itself is simple: we take random bandlimited
measurements (as opposed to taking an exponential number of measurements on the
hyper-grid). Furthermore, our analysis and algorithm are elementary (based on
concentration bounds for sampling and the singular value decomposition).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07946</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07946</id><created>2015-09-26</created><authors><author><keyname>Song</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>A Revisit of Infinite Population Models for Evolutionary Algorithms on
  Continuous Optimization Problems</title><categories>cs.NE math.OC</categories><comments>Submitted to IEEE Transactions on Evolutionary Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infinite population models are important tools for studying population
dynamics of evolutionary algorithms. They describe how the distributions of
populations change between consecutive generations. In general, infinite
population models are derived from Markov chains by exploiting symmetries
between individuals in the population and analyzing the limit as the population
size goes to infinity. In this paper, we study the theoretical foundations of
infinite population models of evolutionary algorithms on continuous
optimization problems. First, we show that the convergence proofs in a widely
cited study were in fact problematic and incomplete. We further show that the
modeling assumption of exchangeability of individuals cannot yield the
transition equation. Then, in order to analyze infinite population models, we
build an analytical framework based on convergence in distribution of random
elements which take values in the metric space of infinite sequences. The
framework is concise and mathematically rigorous. It also provides an
infrastructure for studying the convergence of the stacking of operators and of
iterating the algorithm which previous studies failed to address. Finally, we
use the framework to prove the convergence of infinite population models for
the mutation operator and the $k$-ary recombination operator. We show that
these operators can provide accurate predictions for real population dynamics
as the population size goes to infinity, provided that the initial population
is identically and independently distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07947</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07947</id><created>2015-09-26</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author><author><keyname>Huang</keyname><forenames>Kuan-Wen</forenames></author><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author></authors><title>A new result of the scaling law of weighted L1 minimization</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper study recovery conditions of weighted L1 minimization for signal
reconstruction from compressed sensing measurements. A sufficient condition for
exact recovery by using the general weighted L1 minimization is derived, which
builds a direct relationship between the weights and the recoverability.
Simulation results indicates that this sufficient condition provides a precise
prediction of the scaling law for the weighted L1 minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07950</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07950</id><created>2015-09-26</created><updated>2015-10-01</updated><authors><author><keyname>Zhang</keyname><forenames>Ti-Cao</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author></authors><title>Mixed-ADC Massive MIMO Detectors: Performance Analysis and Design
  Optimization</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures, 3 tables, submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a very low-resolution analog-to-digital convertor (ADC) unit at each
antenna can remarkably reduce the hardware cost and power consumption of a
massive multiple-input multiple-output (MIMO) system. However, such a pure
low-resolution ADC architecture also complicates parameter estimation problems
such as time/frequency synchronization and channel estimation. A mixed-ADC
architecture, where most of the antennas are equipped with low-precision ADCs
while a few antennas have full-precision ADCs, can solve these issues and
actualize the potential of the pure low-resolution ADC architecture. In this
paper, we present a unified framework to develop a family of detectors over the
massive MIMO uplink system with the mixed-ADC receiver architecture by
exploiting probabilistic Bayesian inference. As a basic setup, an optimal
detector is developed to provide a minimum mean-squared-error (MMSE) estimate
on data symbols. Considering the highly nonlinear steps involved in the
quantization process, we also investigate the potential for complexity
reduction on the optimal detector by postulating the common
\emph{pseudo-quantization noise} (PQN) model. In particular, we provide
asymptotic performance expressions including the MSE and bit error rate for the
optimal and suboptimal MIMO detectors. The asymptotic performance expressions
can be evaluated quickly and efficiently; thus, they are useful in system
design optimization. We show that in the low signal-to-noise ratio (SNR)
regime, the distortion caused by the PQN model can be ignored, whereas in the
high-SNR regime, such distortion may cause 1-bit detection performance loss.
The performance gap resulting from the PQN model can be narrowed by a small
fraction of high-precision ADCs in the mixed-ADC architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07951</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07951</id><created>2015-09-26</created><authors><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Chen</keyname><forenames>Fei</forenames></author><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Error Gradient-based Variable-Lp Norm Constraint LMS Algorithm for
  Sparse System Identification</title><categories>cs.SY</categories><comments>Submitted to 41st IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP 2016), 5 pages, 2 tables, 2 figures, 15
  equations, 15 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse adaptive filtering has gained much attention due to its wide
applicability in the field of signal processing. Among the main algorithm
families, sparse norm constraint adaptive filters develop rapidly in recent
years. However, when applied for system identification, most priori work in
sparse norm constraint adaptive filtering suffers from the difficulty of
adaptability to the sparsity of the systems to be identified. To address this
problem, we propose a novel variable p-norm constraint least mean square (LMS)
algorithm, which serves as a variant of the conventional Lp-LMS algorithm
established for sparse system identification. The parameter p is iteratively
adjusted by the gradient descent method applied to the instantaneous square
error. Numerical simulations show that this new approach achieves better
performance than the traditional Lp-LMS and LMS algorithms in terms of
steady-state error and convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07952</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07952</id><created>2015-09-26</created><authors><author><keyname>Chimani</keyname><forenames>Markus</forenames></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author></authors><title>Inserting Multiple Edges into a Planar Graph</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a connected planar (but not yet embedded) graph and $F$ a set of
additional edges not yet in $G$. The {multiple edge insertion} problem (MEI)
asks for a drawing of $G+F$ with the minimum number of pairwise edge crossings,
such that the subdrawing of $G$ is plane. An optimal solution to this problem
approximates the crossing number of the graph $G+F$.
  Finding an exact solution to MEI is NP-hard for general $F$, but linear time
solvable for the special case of $|F|=1$ (SODA01, Algorithmica) or when all of
$F$ are incident to a new vertex (SODA09).
  The complexity for general $F$ but with constant $k=|F|$ was open, but
algorithms both with relative and absolute approximation guarantees have been
presented (SODA11, ICALP11). We show that the problem is fixed parameter
tractable (FPT) in $k$ for biconnected $G$, or if the cut vertices of $G$ have
degrees bounded by a constant. We give the first exact algorithm for this
problem; it requires only $O(|V(G)|)$ time for any constant $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07963</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07963</id><created>2015-09-26</created><authors><author><keyname>Wang</keyname><forenames>Yunpeng</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Load Shifting in the Smart Grid: To Participate or Not?</title><categories>cs.GT</categories><comments>9 pages, 7 figures, journal, accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand-side management (DSM) has emerged as an important smart grid feature
that allows utility companies to maintain desirable grid loads. However, the
success of DSM is contingent on active customer participation. Indeed, most
existing DSM studies are based on game-theoretic models that assume customers
will act rationally and will voluntarily participate in DSM. In contrast, in
this paper, the impact of customers' subjective behavior on each other's DSM
decisions is explicitly accounted for. In particular, a noncooperative game is
formulated between grid customers in which each customer can decide on whether
to participate in DSM or not. In this game, customers seek to minimize a cost
function that reflects their total payment for electricity. Unlike classical
game-theoretic DSM studies which assume that customers are rational in their
decision-making, a novel approach is proposed, based on the framework of
prospect theory (PT), to explicitly incorporate the impact of customer behavior
on DSM decisions. To solve the proposed game under both conventional game
theory and PT, a new algorithm based on fictitious player is proposed using
which the game will reach an epsilon-mixed Nash equilibrium. Simulation results
assess the impact of customer behavior on demand-side management. In
particular, the overall participation level and grid load can depend
significantly on the rationality level of the players and their risk aversion
tendency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07966</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07966</id><created>2015-09-26</created><authors><author><keyname>Mehta</keyname><forenames>Anoop</forenames></author><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author></authors><title>An Efficient Local Strategy to Control Information Spreading in Network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social networks, control of rumor spread is an active area of research.
SIR model is generally used to study the rumor dynamics in network while
considering the rumor as an epidemic. In disease spreading model, epidemic is
controlled by removing central nodes in the network. Full network information
is needed for such removal. To have the information of complete network is
difficult proposition. As a consequence, the search of an algorithm that may
control epidemic without needing global information is a matter of great
interest. In this paper, an immunization strategy is proposed that uses only
local information available at a node, viz. degree of the node and average
degree of its neighbour nodes. Proposed algorithm has been evaluated for
scale-free network using SIR model. Numerical results show that proposed method
has less complexity and gives significantly better results in comparison with
other strategies while using only local information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07968</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07968</id><created>2015-09-26</created><authors><author><keyname>Ikeda</keyname><forenames>Takuya</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Ono</keyname><forenames>Shunsuke</forenames></author></authors><title>Discrete-Valued Control by Sum-of-Absolute-Values Optimization</title><categories>cs.SY</categories><comments>submitted to IEEE Transactions on Automatic Control; 11 pages with 2
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new design method of discrete-valued control for
continuous-time linear time-invariant systems based on sum-of-absolute-values
(SOAV) optimization. We first formulate the discrete-valued control design as a
finite-horizon SOAV optimal control, which is an extended version of L1 optimal
control. We then give simple conditions that guarantee the existence,
discreteness, and uniqueness of the SOAV optimal control. Also, we give the
continuity property of the value function, by which we prove the stability of
infinite-horizon model predictive SOAV control systems. We provide a fast
algorithm for the SOAV optimization based on the alternating direction method
of multipliers (ADMM), which has an important advantage in real-time control
computation. A simulation result shows the effectiveness of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07975</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07975</id><created>2015-09-26</created><authors><author><keyname>Girdhar</keyname><forenames>Yogesh</forenames></author><author><keyname>Dudek</keyname><forenames>Gregory</forenames></author></authors><title>Modeling Curiosity in a Mobile Robot for Long-Term Autonomous
  Exploration and Monitoring</title><categories>cs.RO cs.CV cs.LG</categories><comments>20 pages, in-press, Autonomous Robots, 2015. arXiv admin note:
  substantial text overlap with arXiv:1310.6767</comments><doi>10.1007/s10514-015-9500-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to modeling curiosity in a mobile robot,
which is useful for monitoring and adaptive data collection tasks, especially
in the context of long term autonomous missions where pre-programmed missions
are likely to have limited utility. We use a realtime topic modeling technique
to build a semantic perception model of the environment, using which, we plan a
path through the locations in the world with high semantic information content.
The life-long learning behavior of the proposed perception model makes it
suitable for long-term exploration missions. We validate the approach using
simulated exploration experiments using aerial and underwater data, and
demonstrate an implementation on the Aqua underwater robot in a variety of
scenarios. We find that the proposed exploration paths that are biased towards
locations with high topic perplexity, produce better terrain models with high
discriminative power. Moreover, we show that the proposed algorithm implemented
on Aqua robot is able to do tasks such as coral reef inspection, diver
following, and sea floor exploration, without any prior training or
preparation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07979</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07979</id><created>2015-09-26</created><updated>2016-02-15</updated><authors><author><keyname>Girdhar</keyname><forenames>Yogesh</forenames></author><author><keyname>Cho</keyname><forenames>Walter</forenames></author><author><keyname>Campbell</keyname><forenames>Matthew</forenames></author><author><keyname>Pineda</keyname><forenames>Jesus</forenames></author><author><keyname>Clarke</keyname><forenames>Elizabeth</forenames></author><author><keyname>Singh</keyname><forenames>Hanumant</forenames></author></authors><title>Anomaly Detection in Unstructured Environments using Bayesian
  Nonparametric Scene Modeling</title><categories>cs.CV cs.RO</categories><comments>6 pages, ICRA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the use of a Bayesian non-parametric topic modeling
technique for the purpose of anomaly detection in video data. We present
results from two experiments. The first experiment shows that the proposed
technique is automatically able characterize the underlying terrain, and detect
anomalous flora in image data collected by an underwater robot. The second
experiment shows that the same technique can be used on images from a static
camera in a dynamic unstructured environment. In the second dataset, consisting
of video data from a static seafloor camera capturing images of a busy coral
reef, the proposed technique was able to detect all three instances of an
underwater vehicle passing in front of the camera, amongst many other
observations of fishes, debris, lighting changes due to surface waves, and
benthic flora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07983</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07983</id><created>2015-09-26</created><authors><author><keyname>Iguchi</keyname><forenames>Takayuki</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Peterson</keyname><forenames>Jesse</forenames></author><author><keyname>Villar</keyname><forenames>Soledad</forenames></author></authors><title>Probably certifiably correct k-means clustering</title><categories>cs.IT cs.DS cs.LG math.IT math.ST stat.TH</categories><comments>28 pages, 2 figures. This paper is an extension of and improvement to
  the authors' preprint [arXiv:1505.04778]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Bandeira [arXiv:1509.00824] introduced a new type of algorithm (the
so-called probably certifiably correct algorithm) that combines fast solvers
with the optimality certificates provided by convex relaxations. In this paper,
we devise such an algorithm for the problem of k-means clustering. First, we
prove that a certain semidefinite relaxation of k-means is tight with high
probability under a distribution of planted clusters called the stochastic ball
model. Our proof follows from a new dual certificate for integral solutions of
this semidefinite program. Next, we show how to test the optimality of a
proposed k-means solution using this dual certificate in quasilinear time.
Finally, we propose and analyze a version of spectral clustering that is
designed to solve k-means in the case of two clusters. In particular, we show
that this method has quasilinear complexity and typically recovers planted
clusters under the stochastic ball model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07986</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07986</id><created>2015-09-26</created><authors><author><keyname>Rossi</keyname><forenames>Giovanni</forenames></author></authors><title>Continuous set packing and near-Boolean functions</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a family of feasible subsets of a ground set, the packing problem is to
find a largest subfamily of pairwise disjoint family members.
Non-approximability renders heuristics attractive viable options, while
efficient methods with worst-case guarantee are a key concern in computational
complexity. This work proposes a novel near-Boolean optimization method relying
on a polynomial multilinear form with variables ranging each in a
high-dimensional unit simplex, rather than in the unit interval as usual. The
variables are the elements of the ground set, and distribute each a unit
membership over those feasible subsets where they are included. The given
problem is thus translated into a continuous version where the objective is to
maximize a function taking values on collections of points in a unit hypercube.
Maximizers are shown to always include collections of hypercube disjoint
vertices, i.e. partitions of the ground set, which constitute feasible
solutions for the original discrete version of the problem. A gradient-based
local search in the expanded continuous domain is designed. Approximations with
polynomial multilinear form of bounded degree and near-Boolean games are also
finally discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07989</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07989</id><created>2015-09-26</created><updated>2015-12-24</updated><authors><author><keyname>Singha</keyname><forenames>Nitin</forenames></author><author><keyname>Gupta</keyname><forenames>Ruchir</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Resource allocation in Peer-to-Peer Networks: A Control-Theoretical
  Perspective</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P2P system rely on voluntary allocation of resources by its members due to
absence of any central controlling authority. This resource allocation can be
viewed as classical control problem where feedback is the amount of resource
received, which controls the output i.e. the amount of resources shared back to
the network by the node. The motivation behind the use of control system in
resource allocation is to exploit already existing tools in control theory to
improve the overall allocation process and thereby solving the problem of
freeriding and whitewashing in the network. At the outset, we have derived the
transfer function to model the P2P system. Subsequently, through the simulation
results we have shown that transfer function was able to provide optimal value
of resource sharing for the peers during the normal as well as high degree of
overloading in the network. Thereafter we verified the accuracy of the transfer
function derived by comparing its output with the simulated P2P network. To
demonstrate how control system reduces free riding it has been shown through
simulations how the control systems penalizes the nodes indulging in different
levels of freeriding. Our proposed control system shows considerable gain over
existing state of art algorithm. This improvement is achieved through PI action
of controller. Since low reputation peers usually subvert reputation system by
whitewashing. We propose and substantiate a technique modifying transfer
function such that systems' sluggishness becomes adaptive in such a way that it
encourage genuine new comers to enter network and discourages member peers to
whitewash.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.07996</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.07996</id><created>2015-09-26</created><authors><author><keyname>Li</keyname><forenames>Yixuan</forenames></author><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Bindel</keyname><forenames>David</forenames></author><author><keyname>Hopcroft</keyname><forenames>John</forenames></author></authors><title>Overlapping Community Detection via Local Spectral Clustering</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>Extended version to the conference proceeding in WWW'15</comments><acm-class>I.5.3, G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large graphs arise in a number of contexts and understanding their structure
and extracting information from them is an important research area. Early
algorithms on mining communities have focused on the global structure, and
often run in time functional to the size of the entire graph. Nowadays, as we
often explore networks with billions of vertices and find communities of size
hundreds, it is crucial to shift our attention from macroscopic structure to
microscopic structure in large networks. A growing body of work has been
adopting local expansion methods in order to identify the community members
from a few exemplary seed members.
  In this paper, we propose a novel approach for finding overlapping
communities called LEMON (Local Expansion via Minimum One Norm). The algorithm
finds the community by seeking a sparse vector in the span of the local spectra
such that the seeds are in its support. We show that LEMON can achieve the
highest detection accuracy among state-of-the-art proposals. The running time
depends on the size of the community rather than that of the entire graph. The
algorithm is easy to implement, and is highly parallelizable. We further
provide theoretical analysis on the local spectral properties, bounding the
measure of tightness of extracted community in terms of the eigenvalues of
graph Laplacian.
  Moreover, given that networks are not all similar in nature, a comprehensive
analysis on how the local expansion approach is suited for uncovering
communities in different networks is still lacking. We thoroughly evaluate our
approach using both synthetic and real-world datasets across different domains,
and analyze the empirical variations when applying our method to inherently
different networks in practice. In addition, the heuristics on how the seed set
quality and quantity would affect the performance are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08001</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08001</id><created>2015-09-26</created><authors><author><keyname>Wang</keyname><forenames>Fanzhao</forenames></author><author><keyname>Guo</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Song</keyname><forenames>Qingyang</forenames></author><author><keyname>Jamalipour</keyname><forenames>Abbas</forenames></author></authors><title>Approaching Single-Hop Performance in Multi-Hop Networks: End-To-End
  Known-Interference Cancellation (E2E-KIC)</title><categories>cs.NI cs.DC cs.IT math.IT</categories><doi>10.1109/TVT.2015.2482124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the efficiency of wireless data communications, new physical-layer
transmission methods based on known-interference cancellation (KIC) have been
developed. These methods share the common idea that the interference can be
cancelled when the content of it is known. Existing work on KIC mainly focuses
on single-hop or two-hop networks, with physical-layer network coding (PNC) and
full-duplex (FD) communications as typical examples. This paper extends the
idea of KIC to general multi-hop networks, and proposes an end-to-end KIC
(E2E-KIC) transmission method together with its MAC design. With E2E-KIC,
multiple nodes in a flow passing through a few nodes in an arbitrary topology
can simultaneously transmit and receive on the same channel. We first present a
theoretical analysis on the effectiveness of E2E-KIC in an idealized case.
Then, to support E2E-KIC in multi-hop networks with arbitrary topology, we
propose an E2E-KIC-supported MAC protocol (E2E-KIC MAC), which is based on an
extension of the Request-to-Send/Clear-to-Send (RTS/CTS) mechanism in the IEEE
802.11 MAC. We also analytically analyze the performance of the proposed
E2E-KIC MAC in the presence of hidden terminals. Simulation results illustrate
that the proposed E2E-KIC MAC protocol can improve the network throughput and
reduce the end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08003</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08003</id><created>2015-09-26</created><updated>2015-09-29</updated><authors><author><keyname>Armstrong</keyname><forenames>Timothy J.</forenames></author></authors><title>Avoiding Contradictions in the Paradoxes, the Halting Problem, and
  Diagonalization</title><categories>cs.LO</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The fundamental proposal in this article is that logical formulas of the form
(f &lt;-&gt; ~f) are not contradictions, and that formulas of the form (t &lt;-&gt; t) are
not tautologies. Such formulas, wherever they appear in mathematics, are
instead reason to conclude that f and t have a third truth value, different
from true and false. These formulas are circular definitions of f and t. We can
interpret the implication formula (f &lt;-&gt; ~f) as a rule, a procedure, to find
the truth value of f on the left side: we just need to find the truth value of
f on the right side. When we use the rules to ask if f and t are true or false,
we need to keep asking if they are true or false over and over, forever.
  Russell's paradox and the liar paradox have the form (f &lt;-&gt; ~f). The truth
value provides a straightforward means of avoiding contradictions in these
problems. One broad consequence is that the technique of proof by contradiction
involving formulas of the form (f &lt;-&gt; ~f) becomes invalid. One such proof by
contradiction is one form of proof that the halting problem is uncomputable.
The truth value also appears in Cantor's diagonal argument, Berry's paradox,
and the Grelling-Nelson paradox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08018</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08018</id><created>2015-09-26</created><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Cognitive Access-Transmission Policies under a Primary ARQ process via
  Chain Decoding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel technique that enables access by a cognitive
secondary user (SU) to a spectrum occupied by an incumbent primary user (PU)
that employs Type-I Hybrid ARQ. The technique allows the SU to perform
selective retransmissions of SU data packets that have not been successfully
decoded in the previous attempts. The temporal redundancy introduced by the PU
ARQ protocol and by the selective retransmission process of the SU can be
exploited by the SU receiver to perform interference cancellation (IC) over
multiple transmission slots, thus creating a &quot;clean&quot; channel for the decoding
of the concurrent SU or PU packets. The chain decoding technique is initiated
by a successful decoding operation of a SU or PU packet and proceeds by an
iterative application of IC in order to decode the buffered signals that
represent packets that could not be decoded before. Based on this scheme, an
optimal policy is designed that maximizes the SU throughput under a constraint
on the average long-term PU performance. The optimality of the chain decoding
protocol is proved, which determines which packet the SU should send at a given
time. Moreover, a decoupling principle is proved, which establishes the
optimality of decoupling the secondary access strategy from the chain decoding
protocol. Specifically, first, the SU access policy, optimized via dynamic
programming, specifies whether the SU should access the channel or not, based
on a compact state representation of the protocol; and second, the chain
decoding protocol embeds four basic rules that are used to determine which
packet should be transmitted by the SU. Chain decoding provably yields the
maximum improvement that can be achieved by any scheme under our assumptions,
and thus it is the ultimate scheme, which completely closes the gap between
previous schemes and optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08035</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08035</id><created>2015-09-26</created><updated>2015-10-06</updated><authors><author><keyname>Sharma</keyname><forenames>Sugam</forenames></author></authors><title>An Extended Classification and Comparison of NoSQL Big Data Models</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In last few years, the volume of the data has grown manyfold. The data
storages have been inundated by various disparate potential data outlets,
leading by social media such as Facebook, Twitter, etc. The existing data
models are largely unable to illuminate the full potential of Big Data; the
information that may serve as the key solution to several complex problems is
left unexplored. The existing computation capacity falls short for the
increasingly expanded storage capacity. The fast-paced volume expansion of the
unorganized data entails a complete paradigm shift in new age data computation
and witnesses the evolution of new capable data engineering techniques such as
capture, curation, visualization, analyses, etc. In this paper, we provide the
first level classification for modern Big Data models. Some of the leading
representatives of each classification that claim to best process the Big Data
in reliable and efficient way are also discussed. Also, the classification is
further strengthened by the intra-class and inter-class comparisons and
discussions of the undertaken Big Data models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08037</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08037</id><created>2015-09-26</created><authors><author><keyname>Kawabe</keyname><forenames>Takahiro</forenames></author><author><keyname>Fukiage</keyname><forenames>Taiki</forenames></author><author><keyname>Sawayama</keyname><forenames>Masataka</forenames></author><author><keyname>Nishida</keyname><forenames>Shin'ya</forenames></author></authors><title>Deformation Lamps: A Projection Technique to Make a Static Object
  Dynamic</title><categories>cs.GR cs.HC</categories><comments>21 pages, 8 figures</comments><acm-class>H.5.21; H.1.2; I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Light projection is a powerful technique to edit appearances of objects in
the real world. Based on pixel-wise modification of light transport, previous
techniques have successfully modified static surface properties such as surface
color, dynamic range, gloss and shading. Here, we propose an alternative light
projection technique that adds a variety of illusory, yet realistic distortions
to a wide range of static 2D and 3D projection targets. The key idea of our
technique, named Deformation Lamps, is to project only dynamic luminance
information, which effectively activates the motion (and shape) processing in
the visual system, while preserving the color and texture of the original
object. Although the projected dynamic luminance information is spatially
inconsistent with the color and texture of the target object, the observer's
brain automatically com- bines these sensory signals in such a way as to
correct the inconsistency across visual attributes. We conducted a
psychophysical experiment to investigate the characteristics of the
inconsistency correction, and found that the correction was dependent
critically on the retinal magnitude of inconsistency. Another experiment showed
that perceived magnitude of image deformation by our techniques was
underestimated. The results ruled out the possibility that the effect by our
technique stemmed simply from the physical change of object appearance by light
projection. Finally, we discuss how our techniques can make the observers
perceive a vivid and natural movement, deformation, or oscillation of a variety
of static objects, including drawn pictures, printed photographs, sculptures
with 3D shading, objects with natural textures including human bodies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08038</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08038</id><created>2015-09-26</created><authors><author><keyname>Zhu</keyname><forenames>Wentao</forenames></author><author><keyname>Miao</keyname><forenames>Jun</forenames></author><author><keyname>Qing</keyname><forenames>Laiyun</forenames></author><author><keyname>Chen</keyname><forenames>Xilin</forenames></author></authors><title>Deep Trans-layer Unsupervised Networks for Representation Learning</title><categories>cs.NE cs.CV cs.LG</categories><comments>21 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning features from massive unlabelled data is a vast prevalent topic for
high-level tasks in many machine learning applications. The recent great
improvements on benchmark data sets achieved by increasingly complex
unsupervised learning methods and deep learning models with lots of parameters
usually requires many tedious tricks and much expertise to tune. However,
filters learned by these complex architectures are quite similar to standard
hand-crafted features visually. In this paper, unsupervised learning methods,
such as PCA or auto-encoder, are employed as the building block to learn filter
banks at each layer. The lower layer responses are transferred to the last
layer (trans-layer) to form a more complete representation retaining more
information. In addition, some beneficial methods such as local contrast
normalization and whitening are added to the proposed deep trans-layer networks
to further boost performance. The trans-layer representations are followed by
block histograms with binary encoder schema to learn translation and rotation
invariant representations, which are utilized to do high-level tasks such as
recognition and classification. Compared to traditional deep learning methods,
the implemented feature learning method has much less parameters and is
validated in several typical experiments, such as digit recognition on MNIST
and MNIST variations, object recognition on Caltech 101 dataset and face
verification on LFW dataset. The deep trans-layer unsupervised learning
achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per
class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset,
87.10% on LFW dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08043</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08043</id><created>2015-09-26</created><authors><author><keyname>Niu</keyname><forenames>Yong</forenames></author><author><keyname>Gao</keyname><forenames>Chuhan</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author></authors><title>Exploiting Multi-Hop Relaying to Overcome Blockage in Directional mmWave
  Small Cells</title><categories>cs.NI</categories><comments>11 pages, 12 figures, to appear in Journal of communications and
  networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With vast amounts of spectrum available in the millimeter wave (mmWave) band,
small cells at mmWave frequencies densely deployed underlying the conventional
homogeneous macrocell network have gained considerable interest from academia,
industry, and standards bodies. Due to high propagation loss at higher
frequencies, mmWave communications are inherently directional, and concurrent
transmissions (spatial reuse) under low inter-link interference can be enabled
to significantly improve network capacity. On the other hand, mmWave links are
easily blocked by obstacles such as human body and furniture. In this paper, we
develop a Multi-Hop Relaying Transmission scheme, termed as MHRT, to steer
blocked flows around obstacles by establishing multi-hop relay paths. InMHRT, a
relay path selection algorithmis proposed to establish relay paths for blocked
flows for better use of concurrent transmissions. After relay path selection,
we use a multi-hop transmission scheduling algorithm to compute near-optimal
schedules by fully exploiting the spatial reuse. Through extensive simulations
under various traffic patterns and channel conditions, we demonstrate MHRT
achieves superior performance in terms of network throughput and connection
robustness compared with other existing protocols, especially under serious
blockage conditions. The performance of MHRT with different hop limitations is
also simulated and analyzed for a better choice of themaximum hop number in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08048</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08048</id><created>2015-09-26</created><authors><author><keyname>Niu</keyname><forenames>Yong</forenames></author><author><keyname>Gao</keyname><forenames>Chuhan</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author></authors><title>Energy Efficient Scheduling for mmWave Backhauling of Small Cells in
  Heterogeneous Cellular Networks</title><categories>cs.NI</categories><comments>14 pages, 17 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous cellular networks with small cells densely deployed underlying
the conventional homogeneous macrocells are emerging as a promising candidate
for the fifth generation (5G) mobile network. When a large number of base
stations are deployed, the cost-effective, flexible, and green backhaul
solution becomes one of the most urgent and critical challenges. With vast
amounts of spectrum available, wireless backhaul in the millimeter wave
(mmWave) band is able to provide several-Gbps transmission rates. To overcome
high propagation loss at higher frequencies, mmWave backhaul utilize
beamforming to achieve directional transmission, and concurrent transmissions
(spatial reuse) under low inter-link interference can be enabled to
significantly improve network capacity. To achieve an energy efficient solution
for the mmWave backhauling of small cells, we first formulate the problem of
minimizing the energy consumption via concurrent transmission scheduling and
power control into a mixed integer nonlinear programming problem. Then we
develop an energy efficient and practical mmWave backhauling scheme, where the
maximum independent set based scheduling algorithm and the power control
algorithm are proposed to exploit the spatial reuse for low energy consumption
and high energy efficiency. We also theoretically analyze the conditions that
our scheme reduces energy consumption, and the choice of the interference
threshold for energy reduction. Through extensive simulations under various
traffic patterns and system parameters, we demonstrate the superior performance
of our scheme in terms of energy consumption and energy efficiency, and also
analyze the choice of the interference threshold under different traffic loads,
BS distributions, and the maximum transmission power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08056</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08056</id><created>2015-09-27</created><authors><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Huang</keyname><forenames>Biwei</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Besserve</keyname><forenames>Michel</forenames></author><author><keyname>Watanabe</keyname><forenames>Masataka</forenames></author><author><keyname>Zhu</keyname><forenames>Dajiang</forenames></author></authors><title>Towards Robust and Specific Causal Discovery from fMRI</title><categories>cs.AI q-bio.NC stat.ME</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are several issues with causal discovery from fMRI. First, the sampling
frequency is so low that the time-delayed dependence between different regions
is very small, making time-delayed causal relations weak and unreliable.
Moreover, the complex correspondence between neural activity and the BOLD
signal makes it difficult to formulate a causal model to represent the effect
as a function of the cause. Second, the fMRI experiment may last a relatively
long time period, during which the causal influences are likely to change along
with certain unmeasured states (e.g., the attention) of the subject which can
be written as a function of time, and ignoring the time-dependence will lead to
spurious connections. Likewise, the causal influences may also vary as a
function of the experimental condition (e.g., health, disease, and behavior).
  In this paper we aim to develop a principled framework for robust and time-
or condition-specific causal discovery, by addressing the above issues.
Motivated by a simplified fMRI generating process, we show that the
time-delayed conditional independence relationships at the proper causal
frequency of neural activities are consistent with the instantaneous
conditional independence relationships between brain regions in fMRI
recordings. Then we propose an enhanced constraint-based method for robust
discovery of the underlying causal skeletons, where we include time or
condition as an additional variable in the system; it helps avoid spurious
causal connections between brain regions and discover time- or
condition-specific regions. It also has additional benefit in causal direction
determination. Experiments on both simulated fMRI data and real data give
encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08062</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08062</id><created>2015-09-27</created><authors><author><keyname>Heigold</keyname><forenames>Georg</forenames></author><author><keyname>Moreno</keyname><forenames>Ignacio</forenames></author><author><keyname>Bengio</keyname><forenames>Samy</forenames></author><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author></authors><title>End-to-End Text-Dependent Speaker Verification</title><categories>cs.LG cs.SD</categories><comments>submitted to ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a data-driven, integrated approach to speaker
verification, which maps a test utterance and a few reference utterances
directly to a single score for verification and jointly optimizes the system's
components using the same evaluation protocol and metric as at test time. Such
an approach will result in simple and efficient systems, requiring little
domain-specific knowledge and making few model assumptions. We implement the
idea by formulating the problem as a single neural network architecture,
including the estimation of a speaker model on only a few utterances, and
evaluate it on our internal &quot;Ok Google&quot; benchmark for text-dependent speaker
verification. The proposed approach appears to be very effective for big data
applications like ours that require highly accurate, easy-to-maintain systems
with a small footprint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08065</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08065</id><created>2015-09-27</created><authors><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Sun</keyname><forenames>Yiwei</forenames></author><author><keyname>Bindel</keyname><forenames>David</forenames></author><author><keyname>Hopcroft</keyname><forenames>John E</forenames></author><author><keyname>Li</keyname><forenames>Yixuan</forenames></author></authors><title>Detecting Overlapping Communities from Local Spectral Subspaces</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 8 figures</comments><acm-class>H.3.3; G.2.2</acm-class><journal-ref>ICDM 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the definition of local spectral subspace, we propose a novel
approach called LOSP for local overlapping community detection. Instead of
using the invariant subspace spanned by the dominant eigenvectors of the entire
network, we run the power method for a few steps to approximate the leading
eigenvectors that depict the embedding of the local neighborhood structure
around seeds of interest. We then seek a sparse approximate indicator vector in
the local spectral subspace spanned by these vectors such that the seeds are in
its support.
  We evaluate LOSP on five large real world networks across various domains
with labeled ground-truth communities and compare the results with the
state-of-the-art community detection approaches. LOSP identifies the members of
a target community with high accuracy from very few seed members, and
outperforms the local Heat Kernel or PageRank diffusions as well as the global
baselines.
  Two candidate definitions of the local spectral subspace are analyzed, and
different community scoring functions for determining the community boundary,
including two new metrics, are thoroughly evaluated. The structural properties
of different seed sets and the impact of the seed set size are discussed. We
observe low degree seeds behave better, and LOSP is robust even when started
from a single random seed.
  Using LOSP as a subroutine and starting from each ego connected component, we
try the harder yet significant task of identifying all communities a single
vertex is in. Experiments show that the proposed method achieves high F1
measures on the detected multiple local overlapping communities containing the
seed vertex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08067</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08067</id><created>2015-09-27</created><authors><author><keyname>Wu</keyname><forenames>Tianfu</forenames></author><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Online Object Tracking, Learning and Parsing with And-Or Graphs</title><categories>cs.CV cs.LG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method, called \textit{AOGTracker}, for simultaneously
tracking, learning and parsing (TLP) objects in video sequences with a
hierarchical and compositional And-Or graph (AOG) representation. In our TLP
framework, the AOG explores latent part configurations to represent a target
object. The TLP is formulated in the Bayesian framework and a spatial-temporal
dynamic programming (DP) algorithm is derived to infer object bounding boxes on
the fly. During online learning, the AOG is discriminatively trained in the
latent structural SVM framework to account for the appearance (e.g., lighting
and partial occlusion) and structural (e.g., different poses and viewpoints)
variations of the object, as well as the distractors (e.g., similar objects) in
the background scene. Three key issues in online learning are addressed: (i)
maintaining the purity of positive and negative datasets collected online with
the help from the spatial-temporal DP algorithm, (ii) controling the model
complexity in latent structure learning, and (iii) identifying the critical
moments to re-learn the structure of AOG based on its intrackability. The
intrackability measures the uncertainty of the AOG based on its score maps. In
experiments, our AOGTracker is tested in two main tracking benchmarks with the
same parameter setting: the TB-100/50/CVPR2013 benchmarks, and the VOT
benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In
the former, our AOGTracker outperforms state-of-the-art tracking algorithms
including two trackers based on deep convolutional network. In the latter, our
AOGTracker outperforms all other trackers in VOT2013 and is comparable to the
state-of-the-art methods in VOT2014 (comparison results of VOT2015 and
VOT-TIR2015 will be released by the benchmark authors at the VOT2015 workshop
in conjunction with ICCV2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08068</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08068</id><created>2015-09-27</created><authors><author><keyname>Jangda</keyname><forenames>Abhinav</forenames></author></authors><title>Block-Level Parallelism in Parsing Block Structured Languages</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Softwares source code is becoming large and complex. Compilation of large
base code is a time consuming process. Parallel compilation of code will help
in reducing the time complexity. Parsing is one of the phases in compiler in
which significant amount of time of compilation is spent. Techniques have
already been developed to extract the parallelism available in parser. Current
LR(k) parallel parsing techniques either face difficulty in creating Abstract
Syntax Tree or requires modification in the grammar or are specific to less
expressive grammars. Most of the programming languages like C, ALGOL are
block-structured, and in most languages grammars the grammar of different
blocks is independent, allowing different blocks to be parsed in parallel. We
are proposing a block level parallel parser derived from Incremental Jump Shift
Reduce Parser by [13]. Block Parallelized Parser (BPP) can even work as a block
parallel incremental parser. We define a set of Incremental Categories and
create the partitions of a grammar based on a rule. When parser reaches the
start of the block symbol it will check whether the current block is related to
any incremental category. If block parallel parser find the incremental
category for it, parser will parse the block in parallel. Block parallel parser
is developed for LR(1) grammar. Without making major changes in Shift Reduce
(SR) LR(1) parsing algorithm, block parallel parser can create an Abstract
Syntax tree easily. We believe this parser can be easily extended to LR (k)
grammars and also be converted to an LALR (1) parser. We implemented BPP and SR
LR(1) parsing algorithm for C Programming Language. We evaluated performance of
both techniques by parsing 10 random files from Linux Kernel source. BPP showed
28% and 52% improvement in the case of including header files and excluding
header files respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08071</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08071</id><created>2015-09-27</created><authors><author><keyname>Lin</keyname><forenames>Hao-Min</forenames></author><author><keyname>Tsai</keyname><forenames>Hsin-Mu</forenames></author><author><keyname>Boban</keyname><forenames>Mate</forenames></author></authors><title>Scooter-to-X Communications: Antenna Placement, Human Body Shadowing,
  and Channel Modeling</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In countries such as Taiwan, with a high percentage of scooters,
scooter-related accidents are responsible for most injuries and deaths of all
traffic accidents. One viable approach to reduce the number of accidents is to
utilize short-range wireless communications between the scooter and other
vehicles. This would help neighboring vehicles to detect the scooter and
vice-versa, thus reducing the probability of a collision. In this paper, we
perform extensive measurements to characterize communication links between a
scooter and other vehicles. Our results suggest that, when the line-of-sight
propagation path is blocked by the body of the scooter driver (and possibly
also a passenger), shadowing of the human body results in significant signal
attenuation, ranging from 9 to 18 dB on average, presenting challenging channel
characteristics unique to scooters. In addition, we perform simulations, which
show that it is imperative to incorporate the body shadowing effect to obtain
realistic simulation results. We also develop a model to determine whether
human body shadowing is in effect, given the relative positions of the
transmitter and the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08075</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08075</id><created>2015-09-27</created><authors><author><keyname>Izadinia</keyname><forenames>Hamid</forenames></author><author><keyname>Sadeghi</keyname><forenames>Fereshteh</forenames></author><author><keyname>Divvala</keyname><forenames>Santosh Kumar</forenames></author><author><keyname>Choi</keyname><forenames>Yejin</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author></authors><title>Segment-Phrase Table for Semantic Segmentation, Visual Entailment and
  Paraphrasing</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Segment-Phrase Table (SPT), a large collection of bijective
associations between textual phrases and their corresponding segmentations.
Leveraging recent progress in object recognition and natural language
semantics, we show how we can successfully build a high-quality segment-phrase
table using minimal human supervision. More importantly, we demonstrate the
unique value unleashed by this rich bimodal resource, for both vision as well
as natural language understanding. First, we show that fine-grained textual
labels facilitate contextual reasoning that helps in satisfying semantic
constraints across image segments. This feature enables us to achieve
state-of-the-art segmentation results on benchmark datasets. Next, we show that
the association of high-quality segmentations to textual phrases aids in richer
semantic understanding and reasoning of these textual phrases. Leveraging this
feature, we motivate the problem of visual entailment and visual paraphrasing,
and demonstrate its utility on a large dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08082</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08082</id><created>2015-09-27</created><authors><author><keyname>Welk</keyname><forenames>Martin</forenames></author></authors><title>Multivariate Median Filters and Partial Differential Equations</title><categories>cs.CV</categories><comments>48 pages, 10 figures, 8 tables</comments><acm-class>I.4.3; G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate median filters have been proposed as generalisations of the
well-established median filter for grey-value images to multi-channel images.
As multivariate median, most of the recent approaches use the $L^1$ median,
i.e.\ the minimiser of an objective function that is the sum of distances to
all input points. Many properties of univariate median filters generalise to
such a filter. However, the famous result by Guichard and Morel about
approximation of the mean curvature motion PDE by median filtering does not
have a comparably simple counterpart for $L^1$ multivariate median filtering.
We discuss the affine equivariant Oja median and the affine equivariant
transformation--retransformation $L^1$ median as alternatives to $L^1$ median
filtering. We analyse multivariate median filters in a space-continuous
setting, including the formulation of a space-continuous version of the
transformation--retransformation $L^1$ median, and derive PDEs approximated by
these filters in the cases of bivariate planar images, three-channel volume
images and three-channel planar images. The PDEs for the affine equivariant
filters can be interpreted geometrically as combinations of a diffusion and a
principal-component-wise curvature motion contribution with a cross-effect term
based on torsions of principal components. Numerical experiments are presented
that demonstrate the validity of the approximation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08083</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08083</id><created>2015-09-27</created><authors><author><keyname>Kolleck</keyname><forenames>Anton</forenames></author><author><keyname>Vyb&#xed;ral</keyname><forenames>Jan</forenames></author></authors><title>Non-asymptotic Analysis of $\ell_1$-norm Support Vector Machines</title><categories>cs.IT cs.LG math.FA math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machines (SVM) with $\ell_1$ penalty became a standard tool in
analysis of highdimensional classification problems with sparsity constraints
in many applications including bioinformatics and signal processing. Although
SVM have been studied intensively in the literature, this paper has to our
knowledge first non-asymptotic results on the performance of $\ell_1$-SVM in
identification of sparse classifiers. We show that a $d$-dimensional $s$-sparse
classification vector can be (with high probability) well approximated from
only $O(s\log(d))$ Gaussian trials. The methods used in the proof include
concentration of measure and probability in Banach spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08086</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08086</id><created>2015-09-27</created><authors><author><keyname>Kumar</keyname><forenames>Arvind</forenames></author><author><keyname>Anand</keyname><forenames>Adarsh</forenames></author><author><keyname>Garg</keyname><forenames>Pankaj Kumar</forenames></author><author><keyname>Agarwal</keyname><forenames>Mohini</forenames></author></authors><title>Optimal Release Time Decision from Fuzzy Mathematical Programming
  Perspective</title><categories>cs.AI math.OC</categories><comments>10 Pages. arXiv admin note: substantial overlap with text by other
  authors
  http://archive.org/stream/Software_Reliability_Assessment_with_OR_Applications/Software_Reliability_Assessment_with_OR_Applications_djvu.txt</comments><msc-class>90C29 ? 90C70 ? 90C90</msc-class><journal-ref>International Journal of Pure and Applied Mathematics, Volume 103
  No. 2 2015, 359-376</journal-ref><doi>10.12732/ijpam.v103i2.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand for high software reliability requires rigorous testing followed by
requirement of robust modeling techniques for software quality prediction. On
one side, firms have to steadily manage the reliability by testing it
vigorously, the optimal release time determination is their biggest concern. In
past many models have been developed and much research has been devoted towards
assessment of release time of software. However, majority of the work deals in
crisp study. This paper addresses the problem of release time prediction using
fuzzy Logic. Here we have formulated a Fuzzy release time problem considering
the cost of testing under the impact of warranty period. Results show that
fuzzy model has good adaptability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08088</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08088</id><created>2015-09-27</created><authors><author><keyname>Hazon</keyname><forenames>Noam</forenames></author><author><keyname>Gonen</keyname><forenames>Mira</forenames></author><author><keyname>Kleb</keyname><forenames>Max</forenames></author></authors><title>Approximation and Heuristic Algorithms for Probabilistic Physical Search
  on General Graphs</title><categories>cs.MA cs.AI</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an agent seeking to obtain an item, potentially available at
different locations in a physical environment. The traveling costs between
locations are known in advance, but there is only probabilistic knowledge
regarding the possible prices of the item at any given location. Given such a
setting, the problem is to find a plan that maximizes the probability of
acquiring the good while minimizing both travel and purchase costs. Sample
applications include agents in search-and-rescue or exploration missions, e.g.,
a rover on Mars seeking to mine a specific mineral. These probabilistic
physical search problems have been previously studied, but we present the first
approximation and heuristic algorithms for solving such problems on general
graphs. We establish an interesting connection between these problems and
classical graph-search problems, which led us to provide the approximation
algorithms and hardness of approximation results for our settings. We further
suggest several heuristics for practical use, and demonstrate their
effectiveness with simulation on real graph structure and synthetic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08089</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08089</id><created>2015-09-27</created><updated>2015-12-17</updated><authors><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Tao</keyname><forenames>Jing</forenames></author><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Moss: A Scalable Tool for Efficiently Sampling and Counting 4- and
  5-Node Graphlets</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting the frequencies of 3-, 4-, and 5-node undirected motifs (also know
as graphlets) is widely used for understanding complex networks such as social
and biology networks. However, it is a great challenge to compute these metrics
for a large graph due to the intensive computation. Despite recent efforts to
count triangles (i.e., 3-node undirected motif counting), little attention has
been given to developing scalable tools that can be used to characterize 4- and
5-node motifs. In this paper, we develop computational efficient methods to
sample and count 4- and 5- node undirected motifs. Our methods provide unbiased
estimators of motif frequencies, and we derive simple and exact formulas for
the variances of the estimators. Moreover, our methods are designed to fit
vertex centric programming models, so they can be easily applied to current
graph computing systems such as Pregel and GraphLab. We conduct experiments on
a variety of real-word datasets, and experimental results show that our methods
are several orders of magnitude faster than the state-of-the-art methods under
the same estimation errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08091</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08091</id><created>2015-09-27</created><authors><author><keyname>Farrow</keyname><forenames>Paul</forenames></author><author><keyname>Reed</keyname><forenames>Martin</forenames></author><author><keyname>Glowiak</keyname><forenames>Maciej</forenames></author><author><keyname>Mambretti</keyname><forenames>Joe</forenames></author></authors><title>Transcoder Migration For Real Time Video Streaming Systems</title><categories>cs.NI</categories><comments>13 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increase in real time ultra-high definition video services presents a
challenging issue to current network infrastructures, because of its high
bandwidth usage, which saturate network links. The required bandwidth is
related to strict QoS requirements for digital media. There are systems in
place currently to help reduce these problems, such as transcoders and
application layer multicasting. However, these approaches are limited because
they are usually implemented as static resources. In contrast, by using the
OpenFlow based system presented in this paper, it is possible to provide a more
effective approach using dynamic resources - by both optimally placing
transcoders in the network, as well as by migrating them to different locations
while the streaming is taking place. This migration mechanism provides a near
seamless switchover with minimal interruption to the clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08095</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08095</id><created>2015-09-27</created><authors><author><keyname>Alessandretti</keyname><forenames>Laura</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Gauvin</keyname><forenames>Laetitia</forenames></author></authors><title>User-based representation of time-resolved multimodal public
  transportation networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>24 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodal transportation systems can be represented as time-resolved
multilayer networks where different transportation modes connecting the same
set of nodes are associated to distinct network layers. Their quantitative
description became possible recently due to openly accessible datasets
describing the geolocalised transportation dynamics of large urban areas.
Advancements call for novel analytics, which combines earlier established
methods and exploits the inherent complexity of the data. Here, our aim is to
provide a novel user-based methodological framework to represent public
transportation systems considering the total travel time, its variability
across the schedule, and taking into account the number of transfers necessary.
Using this framework we analyse public transportation systems in several French
municipal areas. We incorporate travel routes and times over multiple
transportation modes to identify efficient transportation connections and
non-trivial connectivity patterns. The proposed method enables us to quantify
the network's overall efficiency as compared to the specific demand and to the
car alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08101</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08101</id><created>2015-09-27</created><updated>2015-09-29</updated><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Representation Benefits of Deep Feedforward Networks</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note provides a family of classification problems, indexed by a positive
integer $k$, where all shallow networks with fewer than exponentially (in $k$)
many nodes exhibit error at least $1/6$, whereas a deep network with 2 nodes in
each of $2k$ layers achieves zero error, as does a recurrent network with 3
distinct nodes iterated $k$ times. The proof is elementary, and the networks
are standard feedforward networks with ReLU (Rectified Linear Unit)
nonlinearities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08102</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08102</id><created>2015-09-27</created><updated>2016-01-17</updated><authors><author><keyname>Ando</keyname><forenames>Shin</forenames></author></authors><title>Discriminative Learning of the Prototype Set for Nearest Neighbor
  Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nearest neighbor rule is one of the most widely used models for
classification and selecting a compact set of prototype instances is an
important problem for its applications. Many existing approaches on the
prototype selection problem have relied on instance-based analyses of the class
distribution, which can be computationally expensive for large datasets. In
this paper, we revisit this problem to explore a parametric approach, which
approximates the violation of the nearest neighbor rule over the training set
and learns the prioritization of prototypes that minimizes the violation. We
show that our approach reduces the problem to large-margin learning and
demonstrate its advantage by empirical comparisons using public benchmark data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08105</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08105</id><created>2015-09-27</created><authors><author><keyname>Yapar</keyname><forenames>&#xc7;a&#x11f;kan</forenames></author><author><keyname>Pohl</keyname><forenames>Volker</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>Compressive phase retrieval of sparse bandlimited signals</title><categories>cs.IT math.IT</categories><comments>Submitted to ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution proposes a two stage strategy to allow for phase retrieval
in state of the art sub-Nyquist sampling schemes for sparse multiband signals.
The proposed strategy is based on data acquisition via modulated wideband
converters known from sub-Nyquist sampling. This paper describes how the
modulators have to be modified such that signal recovery from sub-Nyquist
amplitude samples becomes possible and a corresponding recovery algorithm is
given which is computational efficient. In addition, the proposed strategy is
fairly general, allowing for several constructions and recovery algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08111</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08111</id><created>2015-09-27</created><updated>2015-11-22</updated><authors><author><keyname>Zabolotny</keyname><forenames>Wojciech M.</forenames></author></authors><title>Automatic latency balancing in VHDL-implemented complex pipelined
  systems</title><categories>cs.AR</categories><comments>Updated bibliography. Small language corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Balancing (equalization) of latency in parallel paths in the pipelined data
processing system is an important problem. Without that the data from different
paths arrive at the processing blocks in different clock cycles, and incorrect
results are produced. Manual correction of latencies is a tedious and
error-prone work. This paper presents an automatic method of latency
equalization in systems described in VHDL. The method is based on simulation
and is portable between different simulation and synthesis tools. The method
does not increase the complexity of the synthesized design comparing to the
solution based on manual latency adjustment. The example implementation of the
proposed methodology together with a simple design demonstrating its use is
available as an open source project under BSD license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08112</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08112</id><created>2015-09-27</created><authors><author><keyname>Preet</keyname><forenames>Phool</forenames></author><author><keyname>Batra</keyname><forenames>Sanjit Singh</forenames></author><author><keyname>Jayadeva</keyname></author></authors><title>Feature Selection for classification of hyperspectral data by minimizing
  a tight bound on the VC dimension</title><categories>cs.LG</categories><comments>basic papers are on http://www.jayadeva.net</comments><msc-class>68T05, 68T10, 68Q32</msc-class><acm-class>I.5.1, I.5.2, I.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral data consists of large number of features which require
sophisticated analysis to be extracted. A popular approach to reduce
computational cost, facilitate information representation and accelerate
knowledge discovery is to eliminate bands that do not improve the
classification and analysis methods being applied. In particular, algorithms
that perform band elimination should be designed to take advantage of the
specifics of the classification method being used. This paper employs a
recently proposed filter-feature-selection algorithm based on minimizing a
tight bound on the VC dimension. We have successfully applied this algorithm to
determine a reasonable subset of bands without any user-defined stopping
criteria on widely used hyperspectral images and demonstrate that this method
outperforms state-of-the-art methods in terms of both sparsity of feature set
as well as accuracy of classification.\end{abstract}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08123</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08123</id><created>2015-09-27</created><authors><author><keyname>Grossman</keyname><forenames>Ofer</forenames></author><author><keyname>Moshkovitz</keyname><forenames>Dana</forenames></author></authors><title>Amplification and Derandomization Without Slowdown</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present techniques for decreasing the error probability of randomized
algorithms and for converting randomized algorithms to deterministic
(non-uniform) algorithms. Unlike most existing techniques that involve
repetition of the randomized algorithm and hence a slowdown, our techniques
produce algorithms with a similar run-time to the original randomized
algorithms. The amplification technique is related to a certain stochastic
multi-armed bandit problem. The derandomization technique - which is the main
contribution of this work - points to an intriguing connection between
derandomization and sketching/sparsification.
  We demonstrate the techniques by showing applications to Max-Cut on dense
graphs, approximate clique on graphs that contain a large clique, constraint
satisfaction problems on dense bipartite graphs and the list decoding to unique
decoding problem for the Reed-Muller code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08144</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08144</id><created>2015-09-27</created><updated>2016-01-11</updated><authors><author><keyname>Marti</keyname><forenames>Gautier</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Donnat</keyname><forenames>Philippe</forenames></author></authors><title>Optimal Copula Transport for Clustering Multivariate Time Series</title><categories>cs.LG stat.ML</categories><comments>Accepted at ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new methodology for clustering multivariate time series
leveraging optimal transport between copulas. Copulas are used to encode both
(i) intra-dependence of a multivariate time series, and (ii) inter-dependence
between two time series. Then, optimal copula transport allows us to define two
distances between multivariate time series: (i) one for measuring
intra-dependence dissimilarity, (ii) another one for measuring inter-dependence
dissimilarity based on a new multivariate dependence coefficient which is
robust to noise, deterministic, and which can target specified dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08145</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08145</id><created>2015-09-27</created><authors><author><keyname>Mirzaei</keyname><forenames>Saber</forenames></author><author><keyname>Kfoury</keyname><forenames>Assaf</forenames></author></authors><title>Linear Arrangement of Halin Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Optimal Linear Arrangement (OLA) problem of Halin graphs, one of
the simplest classes of non-outerplanar graphs. We present several properties
of OLA of general Halin graphs. We prove a lower bound on the cost of OLA of
any Halin graph, and define classes of Halin graphs for which the cost of OLA
matches this lower bound. We show for these classes of Halin graphs, OLA can be
computed in O(n log n), where n is the number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08146</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08146</id><created>2015-09-27</created><updated>2016-01-31</updated><authors><author><keyname>Tzoumas</keyname><forenames>Vasileios</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Sensor Placement for Optimal Kalman Filtering: Fundamental Limits,
  Submodularity, and Algorithms</title><categories>math.OC cs.SY</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on sensor placement in linear dynamic estimation,
where the objective is to place a small number of sensors in a system of
interdependent states so to design an estimator with a desired estimation
performance. In particular, we consider a linear time-variant system that is
corrupted with process and measurement noise, and study how the selection of
its sensors affects the estimation error of the corresponding Kalman filter
over a finite observation interval. Our contributions are threefold: First, we
prove that the minimum mean square error of the Kalman filter decreases only
linearly as the number of sensors increases. That is, adding extra sensors so
to reduce this estimation error is ineffective, a fundamental design limit.
Similarly, we prove that the number of sensors grows linearly with the system's
size for fixed minimum mean square error and number of output measurements over
an observation interval; this is another fundamental limit, especially for
systems where the system's size is large. Second, we prove that the logdet of
the error covariance of the Kalman filter, which captures the volume of the
corresponding confidence ellipsoid, with respect to the system's initial
condition and process noise is a supermodular and non-increasing set function
in the choice of the sensor set. Therefore, it exhibits the diminishing returns
property. Third, we provide efficient approximation algorithms that select a
small number sensors so to optimize the Kalman filter with respect to this
estimation error ---the worst-case performance guarantees of these algorithms
are provided as well. Finally, we illustrate the efficiency of our algorithms
using the problem of surface-based monitoring of CO2 sequestration sites
studied in Weimer et al. (2008).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08147</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08147</id><created>2015-09-27</created><updated>2015-10-01</updated><authors><author><keyname>Kar</keyname><forenames>Abhishek</forenames></author><author><keyname>Tulsiani</keyname><forenames>Shubham</forenames></author><author><keyname>Carreira</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Amodal Completion and Size Constancy in Natural Scenes</title><categories>cs.CV</categories><comments>Accepted to ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of enriching current object detection systems with
veridical object sizes and relative depth estimates from a single image. There
are several technical challenges to this, such as occlusions, lack of
calibration data and the scale ambiguity between object size and distance.
These have not been addressed in full generality in previous work. Here we
propose to tackle these issues by building upon advances in object recognition
and using recently created large-scale datasets. We first introduce the task of
amodal bounding box completion, which aims to infer the the full extent of the
object instances in the image. We then propose a probabilistic framework for
learning category-specific object size distributions from available annotations
and leverage these in conjunction with amodal completion to infer veridical
sizes in novel images. Finally, we introduce a focal length prediction approach
that exploits scene recognition to overcome inherent scaling ambiguities and we
demonstrate qualitative results on challenging real-world scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08150</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08150</id><created>2015-09-27</created><updated>2015-10-01</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo Bela</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author></authors><title>Random-resistor-random-temperature Kirchhoff-law-Johnson-noise
  (RRRT-KLJN) key exchange</title><categories>cs.ET cs.CR</categories><comments>submitted for journal publication; 8 pages, 4 figures</comments><journal-ref>Metrology and Measurement Systems, Volume 23, Issue 1, Pages 3-11,
  2016</journal-ref><doi>10.1515/mms-2016-0007</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce two new Kirchhoff-law-Johnson-noise (KLJN) secure key
distribution schemes which are generalizations of the original KLJN scheme. The
first of these, the Random-Resistor (RR-) KLJN scheme, uses random resistors
with values chosen from a quasi-continuum set. It is well-known since the
creation of the KLJN concept that such a system could work in cryptography,
because Alice and Bob can calculate the unknown resistance value from
measurements, but the RR-KLJN system has not been addressed in prior
publications since it was considered impractical. The reason for discussing it
now is the second scheme, the Random-Resistor-Random-Temperature (RRRT-) KLJN
key exchange, inspired by a recent paper of Vadai, Mingesz and Gingl, wherein
security was shown to be maintained at non-zero power flow. In the RRRT-KLJN
secure key exchange scheme, both the resistances and their temperatures are
continuum random variables. We prove that the security of the RRRT-KLJN scheme
can prevail at non-zero power flow, and thus the physical law guaranteeing
security is not the Second Law of Thermodynamics but the
Fluctuation-Dissipation Theorem. Alice and Bob know their own resistances and
temperatures and can calculate the resistance and temperature values at the
other end of the communication channel from measured voltage, current and
power-flow data in the wire. However, Eve cannot determine these values
because, for her, there are four unknown quantities while she can set up only
three equations. The RRRT-KLJN scheme has several advantages and makes all
former attacks on the KLJN scheme invalid or incomplete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08155</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08155</id><created>2015-09-27</created><authors><author><keyname>Mu</keyname><forenames>Beipeng</forenames></author><author><keyname>Paull</keyname><forenames>Liam</forenames></author><author><keyname>Agha-mohammadi</keyname><forenames>Ali-akbar</forenames></author><author><keyname>Leonard</keyname><forenames>John</forenames></author><author><keyname>How</keyname><forenames>Jonathan</forenames></author></authors><title>Information-based Active SLAM via Topological Feature Graphs</title><categories>cs.RO</categories><comments>submitted to ICRA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active SLAM is the task of actively planning robot paths while simultaneously
building a map and localizing within. It is challenging in that the feasibility
of paths will depend on future observations along the path, and the complexity
of the problem grows quickly with the size of the map and the length of robot
trajectory. This work proposes a Topological Feature Graph (TFG) representation
of the map that scales well and develops an active SLAM algorithm with it. The
TFG enables a unified quantification of exploration and exploitation gains with
a single entropy metric and hence facilitates a natural and principled balance
between map exploration and refinement. A probabilistic roadmap path-planner is
used to generate robot paths in real time. Results from hardware experiment
show that the proposed approach achieves better accuracy than the traditional
grid-map based approaches while requiring orders of magnitude less computation
and memory resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08169</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08169</id><created>2015-09-27</created><authors><author><keyname>Bertrand</keyname><forenames>Nathalie</forenames><affiliation>Inria Rennes, France</affiliation></author><author><keyname>Tribastone</keyname><forenames>Mirco</forenames><affiliation>IMT - Institute for Advanced Studies Lucca, Italy</affiliation></author></authors><title>Proceedings Thirteenth Workshop on Quantitative Aspects of Programming
  Languages and Systems</title><categories>cs.LO cs.PF cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 194, 2015</journal-ref><doi>10.4204/EPTCS.194</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Thirteenth Workshop on
Quantitative Aspects of Programming Languages and Systems (QAPL 2015), held in
London, UK, on 11 and 12 April, 2015. QAPL 2015 was a satellite event of the
European Joint Conferences on Theory and Practice of Software (ETAPS) focussing
on quantitative aspects of computation. The Program Committee of QAPL 2015
selected 8 regular papers and 2 presentation-only papers. The workshop
programme included two QAPL keynote presentations by Catuscia Palamidessi
(Inria/LIX, France) on &quot;Quantitative Aspects of Privacy and Information Flow,&quot;
and Holger Hermanns (Saarland University, Germany) on &quot;Optimal Continuous Time
Markov Decisions.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08182</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08182</id><created>2015-09-27</created><authors><author><keyname>Dai</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Bin</forenames></author></authors><title>Robust video object tracking using particle filter with likelihood based
  feature fusion and adaptive template updating</title><categories>cs.CV</categories><comments>5 pages, 5 pages, conference</comments><msc-class>68T45</msc-class><acm-class>I.4.8; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust algorithm solution is proposed for tracking an object in complex
video scenes. In this solution, the bootstrap particle filter (PF) is
initialized by an object detector, which models the time-evolving background of
the video signal by an adaptive Gaussian mixture. The motion of the object is
expressed by a Markov model, which defines the state transition prior. The
color and texture features are used to represent the object, and a marginal
likelihood based feature fusion approach is proposed. A corresponding object
template model updating procedure is developed to account for possible scale
changes of the object in the tracking process. Experimental results show that
our algorithm beats several existing alternatives in tackling challenging
scenarios in video tracking tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08183</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08183</id><created>2015-09-27</created><authors><author><keyname>Liu</keyname><forenames>Quan-Hui</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Hai-Feng</forenames></author></authors><title>Impacts of complex behavioral responses on asymmetric interacting
  spreading dynamics in multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:1405.1905</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information diffusion and disease spreading in communication-contact layered
network are typically asymmetrically coupled with each other, in which how an
individual being aware of disease responds to the disease can significantly
affect the disease spreading. Many recent studies have demonstrated that human
behavioral adoption is a complex and non-Markovian process, where the
probability of adopting one behavior is dependent on the cumulative times of
the received information and the social reinforcement effect of these
cumulative information. We study the impact of such a non-Markovian vaccination
adoption behavior on the epidemic dynamics and the control effects. We find
that this complex adoption behavior caused from the communication layer can
significantly increase the epidemic threshold and reduce the final infection
rate. By defining the social cost as the sum of the cost of vaccination and the
cost of treatment, we show that there exists an optimal social reinforcement
effect or optimal information transmission rate allowing the minimal social
cost. We also develop a mean field based theory to verify the correctness of
the simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08184</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08184</id><created>2015-09-27</created><authors><author><keyname>Crane</keyname><forenames>Harry</forenames></author><author><keyname>Dempsey</keyname><forenames>Walter</forenames></author></authors><title>Atypical scaling behavior persists in real world interaction networks</title><categories>cs.SI physics.soc-ph stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scale-free power law structure describes complex networks derived from a wide
range of real world processes. The extensive literature focuses almost
exclusively on networks with power law exponent strictly larger than 2, which
can be explained by constant vertex growth and preferential attachment. The
complementary scale-free behavior in the range between 1 and 2 has been mostly
neglected as atypical because there is no known generating mechanism to explain
how networks with this property form. However, empirical observations reveal
that scaling in this range is an inherent feature of real world networks
obtained from repeated interactions within a population, as in social,
communication, and collaboration networks. A generative model explains the
observed phenomenon through the realistic dynamics of constant edge growth and
a positive feedback mechanism. Our investigation, therefore, yields a novel
empirical observation grounded in a strong theoretical basis for its
occurrence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08193</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08193</id><created>2015-09-28</created><updated>2016-02-14</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Cantoni</keyname><forenames>Michael</forenames></author></authors><title>Budget-Constrained Contract Design for Effort-Averse Sensors in
  Averaging Based Estimation</title><categories>math.OC cs.GT cs.SY</categories><comments>Improved literature review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a group of effort-averse, or lazy, sensors that seek to minimize the
effort invested to collect measurements of a variable. Increasing the effort
invested by the sensors improves the quality of the measurements provided to
the central planner but this incurs increased costs to the sensors. The central
planner, which processes the sensor measurements, employs an averaging
estimator. It also determines contracts for rewarding sensors based on the
measurements obtained. The problem of designing a contract that yields an
estimation-error based quality-of-service level in return for the reward
extended to sensors is investigated in this paper. To this end, a game is
formulated between the central planner and the sensors. Conditions for the
existence and uniqueness of an equilibrium are identified. The equilibrium is
constructed explicitly and its properties in response to a reward based
contract are studied. It turns out that the central planner, while not being
able to directly measure the effort invested by the sensors, can enhance the
estimation quality by rewarding each sensor based on the distance of its
measurements from the output of the averaging estimator. Ultimately, optimal
contracts are designed from the perspective of the budget required for
achieving a specified level of estimation error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08194</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08194</id><created>2015-09-28</created><authors><author><keyname>Sinha</keyname><forenames>Abhishek</forenames></author><author><keyname>Mani</keyname><forenames>Pradeepkumar</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Flavel</keyname><forenames>Ashley</forenames></author><author><keyname>Maltz</keyname><forenames>David A.</forenames></author></authors><title>Distributed Load Management in Anycast-based CDNs</title><categories>cs.NI</categories><comments>To appear in Allerton, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anycast is an internet addressing protocol where multiple hosts share the
same IP-address. A popular architecture for modern Content Distribution
Networks (CDNs) for geo-replicated HTTP-services consists of multiple layers of
proxy nodes for service and co-located DNS-servers for load-balancing on
different proxies. Both the proxies and the DNS-servers use anycast addressing,
which offers simplicity of design and high availability of service at the cost
of partial loss of routing control. Due to the very nature of anycast,
load-management decisions by a co-located DNS-server also affects loads at
nearby proxies in the network. This makes the problem of distributed load
management highly challenging. In this paper, we propose an analytical
framework to formulate and solve the load-management problem in this context.
We consider two distinct algorithms. In the first half of the paper, we pose
the load-management problem as a convex optimization problem. Following a dual
decomposition technique, we propose a fully-distributed load management
algorithm by introducing FastControl packets. This algorithm utilizes the
underlying anycast mechanism itself to enable effective coordination among the
nodes, thus obviating the need for any external control channel. In the second
half of the paper, we consider an alternative greedy load-management heuristic,
currently in production in a major commercial CDN. We study its dynamical
characteristics and analytically identify its operational and stability
properties. Finally, we critically evaluate both the algorithms and explore
their optimality-vs-complexity trade-off using trace-driven simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08197</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08197</id><created>2015-09-28</created><authors><author><keyname>Luo</keyname><forenames>Xuan</forenames></author><author><keyname>Bai</keyname><forenames>Xuejiao</forenames></author><author><keyname>Li</keyname><forenames>Shuo</forenames></author><author><keyname>Lu</keyname><forenames>Hongtao</forenames></author><author><keyname>Kamata</keyname><forenames>Sei-ichiro</forenames></author></authors><title>Fast Non-local Stereo Matching based on Hierarchical Disparity
  Prediction</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stereo matching is the key step in estimating depth from two or more images.
Recently, some tree-based non-local stereo matching methods have been proposed,
which achieved state-of-the-art performance. The algorithms employed some tree
structures to aggregate cost and thus improved the performance and reduced the
coputation load of the stereo matching. However, the computational complexity
of these tree-based algorithms is still high because they search over the
entire disparity range. In addition, the extreme greediness of the minimum
spanning tree (MST) causes the poor performance in large areas with similar
colors but varying disparities. In this paper, we propose an efficient stereo
matching method using a hierarchical disparity prediction (HDP) framework to
dramatically reduce the disparity search range so as to speed up the tree-based
non-local stereo methods. Our disparity prediction scheme works on a graph
pyramid derived from an image whose disparity to be estimated. We utilize the
disparity of a upper graph to predict a small disparity range for the lower
graph. Some independent disparity trees (DT) are generated to form a disparity
prediction forest (HDPF) over which the cost aggregation is made. When combined
with the state-of-the-art tree-based methods, our scheme not only dramatically
speeds up the original methods but also improves their performance by
alleviating the second drawback of the tree-based methods. This is partially
because our DTs overcome the extreme greediness of the MST. Extensive
experimental results on some benchmark datasets demonstrate the effectiveness
and efficiency of our framework. For example, the segment-tree based stereo
matching becomes about 25.57 times faster and 2.2% more accurate over the
Middlebury 2006 full-size dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08205</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08205</id><created>2015-09-28</created><authors><author><keyname>Sachdeva</keyname><forenames>Niharika</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Characterising Behavior and Emotions on Social Media for Safety:
  Exploring Online Communication between Police and Citizens</title><categories>cs.CY</categories><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increased use of social media by police to connect with citizens has
encouraged researchers to study different aspects of information exchange (e.g.
type of information, credibility and propagation) during emergency and crisis
situation. Research studies lack understanding of human behavior such as
engagement, emotions and social interaction between citizen and police
department on social media. Several social media studies explore and show
technological implications of human behavioral aspects in various contexts such
as workplace interaction and depression in young mothers. In this paper, we
study online interactions between citizens and Indian police in context of
day-to-day policing, including safety concerns, advisories, etc. Indian police
departments use Facebook to issue advisories, send alerts and receive citizen
complaints and suggestions regarding safety issues and day-to-day policing. We
explore how citizens express their emotions and social support on Facebook. Our
work discusses technological implications of behavioral aspects on social well
being of citizens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08215</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08215</id><created>2015-09-28</created><authors><author><keyname>Abbas</keyname><forenames>Hosny</forenames></author><author><keyname>Shaheen</keyname><forenames>Samir</forenames></author><author><keyname>Amin</keyname><forenames>Mohammed</forenames></author></authors><title>Adaptive Agent-Based SCADA System</title><categories>cs.SY cs.MA</categories><comments>10</comments><journal-ref>Int'l Journal of Computing, Communications &amp; Instrumentation Engg.
  (IJCCIE) Vol. 2, Issue 1 (2015) ISSN 2349-1469 EISSN 2349-1477</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern supervisory control and data acquisition (SCADA) systems comprise
variety of industrial equipment such as physical control processes, logical
control systems, communication networks, computers, and communication
protocols. They are concerned with control and supervision of production
control processes. Modern SCADA networks contain highly distributed
information, control, and location. Moreover, they contain large number of
heterogeneous components situated in highly changing and uncertain
environments. As a result, engineering modern SCADA is a challenging issue and
conventional engineering approaches are no longer suitable for them because of
their increasing complexity and highly distribution. In this research,
Multi-Agent Systems (MAS) are used to enable building adaptive agent-based
SCADA system by modeling system components as agents in the micro level and as
organizations or societies of agents in the macro level. A prototype has been
implemented and evaluated within a simulation environment for demonstrating the
adaptive behavior of the system-to-be, which results in continuous improvement
of system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08216</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08216</id><created>2015-09-28</created><updated>2015-12-31</updated><authors><author><keyname>Kuszmaul</keyname><forenames>William</forenames></author></authors><title>Fast Algorithms for Finding Pattern Avoiders and Counting Pattern
  Occurrences in Permutations</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $\Pi \subseteq S_k$ and $w \in S_n$, define $P_\Pi(w)$ to be the number
of $k$-letter subsequences of $w$ that are order-isomorphic to some $\pi \in
\Pi$. We introduce an $O(n!k)$-time algorithm computing $P_\Pi(w)$ for each $w
\in S_n$, and an $O(|S_{\le {n-1}}(\Pi)|\cdot k + |S_n(\Pi)|)$-time algorithm
computing $S_{\le n}(\Pi) = \{w : w \in S_{\le n}, P_{\Pi}(w) = 0\}$.
Surprisingly, when $|\Pi| = 1$, we can improve the run-time to $\Theta(n!)$ for
computing $P_\Pi(w)$ for each $w \in S_n$. In contrast, the best previous
algorithms, based on generate-and-check, take exponential time per permutation
analyzed.
  If we want to only count permutations of according to $\Pi$-patterns, rather
than to store information about every permutation, then all of our algorithms
can be implemented in $O(n^{k+1}k)$ space.
  Using our algorithms, we generated $|S_5(\Pi)|, \ldots, |S_{16}(\Pi)|$ for
each $\Pi \subseteq S_4$ with $|\Pi| &gt; 4$, and analyzed OEIS matches. We
obtained thousands of novel pattern-avoidance conjectures, thirteen of which we
present.
  Our algorithms extend to considering permutations in any set closed under
normalization of subsequences. Our algorithms also partially adapt to
considering vincular patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08222</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08222</id><created>2015-09-28</created><authors><author><keyname>Zieli&#x144;ski</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Stream-based aggregation of unreliable heterogeneous network links</title><categories>cs.NI</categories><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Last mile link is often a bottleneck for end user. However, users typically
have multiple ways of accessing the Internet (cellular, ADSL, public Wifi).
This observation led to creation of protocols like mTCP or R-MTP. Current
bandwidth aggregation protocols are packet based. However, this is not always
practical - for example, non-TCP protocols are often blocked on firewalls.
Moreover, a lot of effort was devoted over the years into making single-path
TCP work well over various types of links. In this paper we introduce protocol
which uses multiple TCP streams to establish single reliable connection
attempting to maximize bandwidth and minimize latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08231</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08231</id><created>2015-09-28</created><authors><author><keyname>Yu</keyname><forenames>Hsi-En</forenames></author><author><keyname>Huang</keyname><forenames>Weicheng</forenames></author></authors><title>Building a Virtual HPC Cluster with Auto Scaling by the Docker</title><categories>cs.DC</categories><comments>PRAGMA-ICDS 15</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Solving the software dependency issue under the HPC environment has always
been a difficult task for both computing system administrators and application
scientists. This work would like to tackle the issue by introducing the modern
container technology, the Docker, to be specific. By integrating the
auto-scaling feature of service discovery with the light-weight virtualization
tool, the Docker, the construction of a virtual cluster on top of physical
cluster hardware is attempted. Thus, through the isolation of computing
environment, a remedy of software dependency of HPC environment is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08239</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08239</id><created>2015-09-28</created><authors><author><keyname>Albayati</keyname><forenames>Mohanad</forenames></author><author><keyname>Issac</keyname><forenames>Biju</forenames></author></authors><title>Analysis of Intelligent Classifiers and Enhancing the Detection Accuracy
  for Intrusion Detection System</title><categories>cs.CR cs.LG</categories><journal-ref>International Journal of Computational Intelligence Systems, 8:5,
  841-853 (2015)</journal-ref><doi>10.1080/18756891.2015.1084705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss and analyze some of the intelligent classifiers
which allows for automatic detection and classification of networks attacks for
any intrusion detection system. We will proceed initially with their analysis
using the WEKA software to work with the classifiers on a well-known IDS
(Intrusion Detection Systems) dataset like NSL-KDD dataset. The NSL-KDD dataset
of network attacks was created in a military network by MIT Lincoln Labs. Then
we will discuss and experiment some of the hybrid AI (Artificial Intelligence)
classifiers that can be used for IDS, and finally we developed a Java software
with three most efficient classifiers and compared it with other options. The
outputs would show the detection accuracy and efficiency of the single and
combined classifiers used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08240</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08240</id><created>2015-09-28</created><authors><author><keyname>Brodal</keyname><forenames>Gerth St&#xf8;lting</forenames></author></authors><title>External Memory Three-Sided Range Reporting and Top-$k$ Queries with
  Sublogarithmic Updates</title><categories>cs.CG cs.DS</categories><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An external memory data structure is presented for maintaining a dynamic set
of $N$ two-dimensional points under the insertion and deletion of points, and
supporting 3-sided range reporting queries and top-$k$ queries, where top-$k$
queries report the $k$~points with highest $y$-value within a given $x$-range.
For any constant $0&lt;\varepsilon\leq \frac{1}{2}$, a data structure is
constructed that supports updates in amortized $O(\frac{1}{\varepsilon
B^{1-\varepsilon}}\log_B N)$ IOs and queries in amortized
$O(\frac{1}{\varepsilon}\log_B N+K/B)$ IOs, where $B$ is the external memory
block size, and $K$ is the size of the output to the query (for top-$k$ queries
$K$ is the minimum of $k$ and the number of points in the query interval). The
data structure uses linear space. The update bound is a significant factor
$B^{1-\varepsilon}$ improvement over the previous best update bounds for the
two query problems, while staying within the same query and space bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08251</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08251</id><created>2015-09-28</created><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames></author><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Tight Lower and Upper Bounds for the Complexity of Canonical Colour
  Refinement</title><categories>cs.DS cs.CC</categories><comments>An extended abstract of this paper appeared in the proceedings of
  ESA'13, LNCS 8125, pp. 145-156</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An assignment of colours to the vertices of a graph is stable if any two
vertices of the same colour have identically coloured neighbourhoods. The goal
of colour refinement is to find a stable colouring that uses a minimum number
of colours. This is a widely used subroutine for graph isomorphism testing
algorithms, since any automorphism needs to be colour preserving. We give an
$O((m+n)\log n)$ algorithm for finding a canonical version of such a stable
colouring, on graphs with $n$ vertices and $m$ edges. We show that no faster
algorithm is possible, under some modest assumptions about the type of
algorithm, which captures all known colour refinement algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08254</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08254</id><created>2015-09-28</created><authors><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author><author><keyname>Gorodnik</keyname><forenames>Alexander</forenames></author></authors><title>Towards a complete DMT classification of division algebra codes</title><categories>cs.IT math.IT math.NT</categories><comments>7 pages, 1 figure, conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims at providing new bounds for the diversity multiplexing gain
trade-off of a general class of division algebra based lattice codes. In the
low multiplexing gain regime, some bounds were previously obtained from the
high signal-to-noise ratio estimate of the union bound for the pairwise error
probabilities. Here these results are extended to cover a larger range of
multiplexing gains. The improvement is achieved by using ergodic theory in Lie
groups to estimate the behavior of the sum arising from the union bound. In
particular, the new bounds for lattice codes derived from Q-central division
algebras suggest that these codes can be divided into two subclasses based on
their Hasse invariants at the infinite places. Algebras with ramification at
the infinite place seem to provide better diversity-multiplexing gain tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08255</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08255</id><created>2015-09-28</created><updated>2015-10-08</updated><authors><author><keyname>Byrne</keyname><forenames>Fergal</forenames></author></authors><title>Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in
  Hierarchical Temporal Memory</title><categories>cs.NE cs.AI</categories><comments>Updated reference to unofficial revision of Hawkins and Ahmad, 2011</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM)
as a model of neocortical computation, the theory and the algorithms have
evolved dramatically. This paper presents a detailed description of HTM's
Cortical Learning Algorithm (CLA), including for the first time a rigorous
mathematical formulation of all aspects of the computations. Prediction
Assisted CLA (paCLA), a refinement of the CLA is presented, which is both
closer to the neuroscience and adds significantly to the computational power.
Finally, we summarise the key functions of neocortex which are expressed in
paCLA implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08257</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08257</id><created>2015-09-28</created><authors><author><keyname>Li</keyname><forenames>Zhen</forenames></author><author><keyname>Xu</keyname><forenames>Jianjun</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Recognition of Brain Waves of Left and Right Hand Movement Imagery with
  Portable Electroencephalographs</title><categories>cs.HC</categories><comments>13 pages,4 figures,4 tables. arXiv admin note: substantial text
  overlap with arXiv:1509.07642</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of the modern society, mind control applied to both the
recovery of disabled individuals and auxiliary control of normal people has
obtained great attention in numerous researches. In our research, we attempt to
recognize the brain waves of left and right hand movement imagery with portable
electroencephalographs. Considering the inconvenience of wearing traditional
multiple-electrode electroencephalographs, we choose Muse to collect data which
is a portable headband launched lately with a number of useful functions and
channels and it is much easier for the public to use. Additionally, previous
researches generally focused on discrimination of EEG of left and right hand
movement imagery by using data from C3 and C4 electrodes which locate on the
top of the head. However, we choose the gamma wave channels of F7 and F8 and
obtain data when subjects imagine their left or right hand to move with their
eyeballs rotated in the corresponding direction. With the help of the Common
Space Pattern algorithm to extract features of brain waves between left and
right hand movement imagery, we make use of the Support Vector Machine to
classify different brain waves. Traditionally, the accuracy rate of
classification was approximately 90% using the EEG data from C3 and C4
electrode poles; however, the accuracy rate reaches 95.1% by using the gamma
wave data from F7 and F8 in our experiment. Finally, we design a plane program
in Python where a plane can be controlled to go left or right when users
imagine their left or right hand to move. 8 subjects are tested and all of them
can control the plane flexibly which reveals that our model can be applied to
control hardware which is useful for disabled individuals and normal people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08262</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08262</id><created>2015-09-28</created><updated>2016-01-11</updated><authors><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>Secure Communication Via a Wireless Energy Harvesting Untrusted Relay</title><categories>cs.IT cs.NI math.IT</categories><comments>The paper has been submitted for possible journal publication.
  Revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broadcast nature of the wireless medium allows unintended users to
eavesdrop the confidential information transmission. In this regard, we
investigate the problem of secure communication between a source and a
destination via a wireless energy harvesting untrusted node which acts as a
helper to relay the information; however, the source and destination nodes wish
to keep the information confidential from the relay node. To realize the
positive secrecy rate, we use destination-assisted jamming. Being an
energy-starved node, the untrusted relay harvests energy from the received
radio frequency signals, which include the source's information signal and the
destination's jamming signal. Thus, we utilize the jamming signal efficiently
by leveraging it as a useful energy source. At the relay, to enable energy
harvesting and information processing, we adopt power splitting (PS) and time
switching (TS) policies. To evaluate the secrecy performance of this proposed
scenario, we derive analytical expressions for two important metrics, viz., the
secrecy outage probability and the ergodic secrecy rate. The numerical analysis
reveals the design insights into the effects of different system parameters
like power splitting ratio, energy harvesting time, target secrecy rate,
transmit signal-to-noise ratio (SNR), relay location, and energy conversion
efficiency factor, on the secrecy performance. Specifically, the PS policy
achieves better optimal secrecy outage probability and optimal ergodic secrecy
rate than that of the TS policy at higher target secrecy rate and transmit SNR,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08267</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08267</id><created>2015-09-28</created><updated>2015-10-04</updated><authors><author><keyname>Singhal</keyname><forenames>Nandini</forenames></author><author><keyname>Peri</keyname><forenames>Sathya</forenames></author><author><keyname>Kalyanasundaram</keyname><forenames>Subrahmanyam</forenames></author></authors><title>Multi-threaded Graph Coloring Algorithm for Shared Memory Architecture</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present multi-threaded algorithms for graph coloring
suitable to the shared memory programming model. We modify an existing
algorithm widely used in the literature and prove the correctness of the
modified algorithm. We also propose a new approach to solve the problem of
coloring using locks. Using datasets from real world graphs, we evaluate the
performance of the algorithms on the Intel platform. We compare the performance
of the sequential approach v/s our proposed approach and analyze the speedup
obtained against the existing algorithm from the literature. The results show
that the speedup obtained is consequential. We also provide a direction for
future work towards improving the performance further in terms of different
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08285</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08285</id><created>2015-09-28</created><updated>2016-02-17</updated><authors><author><keyname>Friedrichs</keyname><forenames>Stephan</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>King</keyname><forenames>James</forenames></author><author><keyname>Schmidt</keyname><forenames>Christiane</forenames></author></authors><title>The Continuous 1.5D Terrain Guarding Problem: Discretization, Optimal
  Solutions, and PTAS</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the NP-hard continuous 1.5D Terrain Guarding Problem (TGP) we are given an
$x$-monotone chain of line segments in $\mathbb{R}^2$ (the terrain $T$), and
ask for the minimum number of guards (located anywhere on $T$) required to
guard all of $T$. We construct guard candidate and witness sets $G, W \subset
T$ of polynomial size such that any feasible (optimal) guard cover $G^*
\subseteq G$ for $W$ is also feasible (optimal) for the continuous TGP. This
discretization allows us to: (1) settle NP-completeness for the continuous TGP;
(2) provide a Polynomial Time Approximation Scheme (PTAS) for the continuous
TGP using the PTAS for the discrete TGP by Gibson et al.; (3) formulate the
continuous TGP as an Integer Linear Program (IP). Furthermore, we propose
several filtering techniques reducing the size of our discretization, allowing
us to devise an efficient IP-based algorithm that reliably provides optimal
guard placements for terrains with up to $10^6$ vertices within minutes on a
standard desktop computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08295</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08295</id><created>2015-09-28</created><updated>2015-09-29</updated><authors><author><keyname>Jensen</keyname><forenames>Pablo</forenames></author><author><keyname>Morini</keyname><forenames>Matteo</forenames></author><author><keyname>Karsai</keyname><forenames>Marton</forenames></author><author><keyname>Venturini</keyname><forenames>Tommaso</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author><author><keyname>Jacomy</keyname><forenames>Mathieu</forenames></author><author><keyname>Cointet</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Merckle</keyname><forenames>Pierre</forenames></author><author><keyname>Fleury</keyname><forenames>Eric</forenames></author></authors><title>Detecting global bridges in networks</title><categories>cs.SI physics.soc-ph</categories><comments>Journal of Complex Networks Preprint; 14 pages; 6 figures</comments><doi>10.1093/comnet/cnv022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of nodes occupying important positions in a network
structure is crucial for the understanding of the associated real-world system.
Usually, betweenness centrality is used to evaluate a node capacity to connect
different graph regions. However, we argue here that this measure is not
adapted for that task, as it gives equal weight to &quot;local&quot; centers (i.e. nodes
of high degree central to a single region) and to &quot;global&quot; bridges, which
connect different communities. This distinction is important as the roles of
such nodes are different in terms of the local and global organisation of the
network structure. In this paper we propose a decomposition of betweenness
centrality into two terms, one highlighting the local contributions and the
other the global ones. We call the latter bridgeness centrality and show that
it is capable to specifically spot out global bridges. In addition, we
introduce an effective algorithmic implementation of this measure and
demonstrate its capability to identify global bridges in air transportation and
scientific collaboration networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08299</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08299</id><created>2015-09-28</created><authors><author><keyname>Budkuley</keyname><forenames>Amitalok J.</forenames></author><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Communication in the Presence of a State-Aware Adversary</title><categories>cs.IT math.IT</categories><comments>39 pages, 6 figures, final version submitted to IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study communication systems over state-dependent channels in the presence
of a malicious state-aware adversary. Taking an Arbitrarily Varying Channel
(AVC) approach, we consider two setups, namely, the discrete memoryless
Gelfand-Pinsker (GP) AVC and the additive white Gaussian Dirty Paper (DP) AVC.
We determine the randomized coding capacity of both the AVCs under a maximum
probability of error criterion. Even with non-causal knowledge of the state, we
prove that the state-aware adversary can do no worse than choosing a memoryless
strategy. Thus, the AVC capacity characterization is given in terms of the
capacity of the worst memoryless channels with state, induced by the adversary
employing a memoryless jamming scheme. For the DP-AVC, it is further shown that
among memoryless jamming strategies, none impact the communication more than an
independent and identically distributed (i.i.d.) Gaussian jamming strategy
which completely disregards the knowledge of the state. Thus, the capacity of
the DP-AVC equals that of a standard AWGN channel with two independent sources
of additive white Gaussian noise, i.e., the channel noise and the jamming
noise.
  Motivated by the AVC communication setups, we then study two mutual
information zero sum games, viz. the Gelfand-Pinsker (GP) game and the Dirty
Paper (DP) game. For each game, we show that there exists a simple pure
strategy Nash equilibrium. Here, the equilibrium user and jammer strategies can
be seen as analogues to the achievability coding scheme and the optimal jamming
scheme respectively, under the corresponding AVC setups. We show that the
unique Nash equilibrium utility equals the randomized coding capacity of the
corresponding AVC communication setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08302</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08302</id><created>2015-09-25</created><authors><author><keyname>parvizi</keyname><forenames>Mahdi</forenames></author><author><keyname>Shadkam</keyname><forenames>Elham</forenames></author><author><keyname>Jahani</keyname><forenames>Niloofar</forenames></author></authors><title>A hybrid COA$\epsilon$-constraint method for solving multi-objective
  problems</title><categories>cs.NE</categories><doi>10.5121/ijfcst.2015.5503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a hybrid method for solving multi-objective problem has been
provided. The proposed method is combining the {\epsilon}-Constraint and the
Cuckoo algorithm. First the multi objective problem transfers into a
single-objective problem using $\epsilon$-Constraint, then the Cuckoo
optimization algorithm will optimize the problem in each task. At last the
optimized Pareto frontier will be drawn. The advantage of this method is the
high accuracy and the dispersion of its Pareto frontier. In order to testing
the efficiency of the suggested method, a lot of test problems have been solved
using this method. Comparing the results of this method with the results of
other similar methods shows that the Cuckoo algorithm is more suitable for
solving the multi-objective problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08303</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08303</id><created>2015-09-28</created><authors><author><keyname>Zwolf</keyname><forenames>Carlo Maria</forenames></author><author><keyname>Harrison</keyname><forenames>Paul</forenames></author><author><keyname>Garrido</keyname><forenames>Julian</forenames></author><author><keyname>Ruiz</keyname><forenames>Jose Enrique</forenames></author><author><keyname>Petit</keyname><forenames>Franck Le</forenames></author></authors><title>IVOA recommendation: Parameter Description Language Version 1.0</title><categories>astro-ph.IM cs.SE</categories><proxy>IVOA Document Coordinator</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document discusses the definition of the Parameter Description Language
(PDL). In this language parameters are described in a rigorous data model. With
no loss of generality, we will represent this data model using XML. It intends
to be a expressive language for self-descriptive web services exposing the
semantic nature of input and output parameters, as well as all necessary
complex constraints. PDL is a step forward towards true web services
interoperability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08304</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08304</id><created>2015-09-25</created><updated>2015-10-15</updated><authors><author><keyname>Ceran</keyname><forenames>Elif Tugce</forenames></author><author><keyname>Erkilic</keyname><forenames>Tugce</forenames></author><author><keyname>Uysal-Biyikoglu</keyname><forenames>Elif</forenames></author><author><keyname>Girici</keyname><forenames>Tolga</forenames></author><author><keyname>Leblebicioglu</keyname><forenames>Kemal</forenames></author></authors><title>Optimal Energy Allocation Policies for a High Altitude Flying Wireless
  Access Point</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper is an extended version of the paper &quot;Optimizing the
  service policy of a wireless access point on the move with renewable energy&quot;
  in Proceedings of 52nd Annual Allerton Conference on Communication, Control,
  and Computing (Allerton), Monticello Illinois, pp.967-974, Sept. 30 2014-Oct.
  3 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by recent industrial efforts toward high altitude flying wireless
access points powered by renewable energy, an online resource allocation
problem for a mobile access point (AP) travelling at high altitude is
formulated. The AP allocates its resources (available energy) to maximize the
total utility (reward) provided to a sequentially observed set of users
demanding service. The problem is formulated as a 0/1 dynamic knapsack problem
with incremental capacity over a finite time horizon, the solution of which is
quite open in the literature. We address the problem through deterministic and
stochastic formulations. For the deterministic problem, several online
approximations are proposed based on an instantaneous threshold that can adapt
to short-time-scale dynamics. For the stochastic model, after showing the
optimality of a threshold based solution on a dynamic programming (DP)
formulation, an approximate threshold based policy is obtained. The
performances of proposed policies are compared with that of the optimal
solution obtained through DP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08305</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08305</id><created>2015-09-28</created><authors><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Bjornson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Random Access for Massive MIMO Systems with Intra-Cell Pilot
  Contamination</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO systems, where the base stations are equipped with hundreds of
antenna elements, are an attractive way to attain unprecedented spectral
efficiency in future wireless networks. In the &quot;classical&quot; massive MIMO
setting, the terminals are assumed fully loaded and a main impairment to the
performance comes from the inter-cell pilot contamination, i.e., interference
from terminals in neighboring cells using the same pilots as in the home cell.
However, when the terminals are active intermittently, it is viable to avoid
inter-cell contamination by pre-allocation of pilots, while same-cell terminals
use random access to select the allocated pilot sequences. This leads to the
problem of intra-cell pilot contamination. We propose a framework for random
access in massive MIMO networks and derive new uplink sum rate expressions that
take intra-cell pilot collisions, intermittent terminal activity, and
interference into account. We use these expressions to optimize the terminal
activation probability and pilot length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08308</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08308</id><created>2015-09-28</created><authors><author><keyname>Yuan</keyname><forenames>Fang</forenames></author></authors><title>Tucker Decomposition For Rotated Codebook in 3D MIMO System Under
  Spatially Correlated Channel</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This correspondence proposes a new rotated codebook for three-dimensional
(3D) multi-input-multi-output (MIMO) system under spatially correlated channel.
To avoid the problem of high dimensionality led by large antenna array, the
rotation matrix in the rotated codebook is proposed to be decomposed by Tucker
decomposition into three lowdimensional units, i.e., statistical channel
direction information in horizontal and vertical directions respectively, and
statistical channel power in the joint horizontal and vertical direction. A
closed-form suboptimal solution is provided to reduce the computational
complexity in Tucker decomposition. The proposed codebook has a significant
dimension reduction from conventional rotated codebooks, and is applicable for
3D MIMO system with arbitrary form of antenna array. Simulation results
demonstrate that the proposed codebook works very well for various 3D MIMO
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08309</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08309</id><created>2015-09-28</created><authors><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Matthiesen</keyname><forenames>Bho</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Energy Efficiency in MIMO Underlay and Overlay Device-to-Device
  Communications and Cognitive Radio Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of resource allocation for systems in which
a primary and a secondary link share the available spectrum by an underlay or
overlay approach. After observing that such a scenario models both cognitive
radio and D2D communications, we formulate the problem as the maximization of
the secondary energy efficiency subject to a minimum rate requirement for the
primary user. This leads to challenging non-convex, fractional problems. In the
underlay scenario, we obtain the global solution by means of a suitable
reformulation. In the overlay scenario, two algorithms are proposed. The first
one yields a resource allocation fulfilling the first-order optimality
conditions of the resource allocation problem, by solving a sequence of easier
fractional problems. The second one enjoys a weaker optimality claim, but an
even lower computational complexity. Numerical results demonstrate the merits
of the proposed algorithms both in terms of energy-efficient performance and
complexity, also showing that the two proposed algorithms for the overlay
scenario perform very similarly, despite the different complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08315</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08315</id><created>2015-09-28</created><authors><author><keyname>Jaffke</keyname><forenames>Lars</forenames></author><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author></authors><title>Definability Equals Recognizability for $k$-Outerplanar Graphs</title><categories>cs.LO math.CO</categories><comments>40 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most famous algorithmic meta-theorems states that every graph
property that can be defined by a sentence in counting monadic second order
logic (CMSOL) can be checked in linear time for graphs of bounded treewidth,
which is known as Courcelle's Theorem. These algorithms are constructed as
finite state tree automata, and hence every CMSOL-definable graph property is
recognizable. Courcelle also conjectured that the converse holds, i.e. every
recognizable graph property is definable in CMSOL for graphs of bounded
treewidth. We prove this conjecture for $k$-outerplanar graphs, which are known
to have treewidth at most $3k-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08316</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08316</id><created>2015-09-28</created><authors><author><keyname>Kumbhar</keyname><forenames>Abhaykumar</forenames></author><author><keyname>Koohifar</keyname><forenames>Farshad</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Mueller</keyname><forenames>Bruce</forenames></author></authors><title>A Survey on Legacy and Emerging Technologies for Public Safety
  Communications</title><categories>cs.NI</categories><comments>Submitted at IEEE Communications Surveys and Tutorials</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective emergency and natural disaster management depend on the efficient
mission-critical voice and data communication between first responders and
victims. Land Mobile Radio System (LMRS) is a legacy narrowband technology used
for critical voice communications with limited use for data applications.
Recently Long Term Evolution (LTE) emerged as a broadband communication
technology that has a potential to transform the capabilities of public safety
technologies by providing broadband, ubiquitous, and mission-critical voice and
data support. For example, in the United States, FirstNet is building a
nationwide coast-to-coast public safety network based of LTE broadband
technology. This paper presents a comparative survey of legacy and the
LTE-based public safety networks, and discusses the LMRS-LTE convergence as
well as mission-critical push-to-talk over LTE. A simulation study of LMRS and
LTE band class 14 technologies is provided using the NS-3 open source tool. An
experimental study of APCO-25 and LTE band class 14 is also conducted using
software-defined radio, to enhance the understanding of the public safety
systems. Finally, emerging technologies that may have strong potential for use
in public safety networks are reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08323</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08323</id><created>2015-09-28</created><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author><author><keyname>Ryder</keyname><forenames>Nicholas</forenames></author></authors><title>On the geometry of border rank algorithms for n x 2 by 2 x 2 matrix
  multiplication</title><categories>cs.NA math.AG</categories><comments>19 pages, two figures</comments><msc-class>68Q17, 68Q25, 15A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We make an in-depth study of the known border rank (i.e. approximate)
algorithms for the matrix multiplication tensor encoding the multiplication of
an n x 2 matrix by a 2 x 2 matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08324</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08324</id><created>2015-09-28</created><authors><author><keyname>Ablinger</keyname><forenames>J.</forenames></author><author><keyname>Behring</keyname><forenames>A.</forenames></author><author><keyname>Bl&#xfc;mlein</keyname><forenames>J.</forenames></author><author><keyname>De Freitas</keyname><forenames>A.</forenames></author><author><keyname>von Manteuffel</keyname><forenames>A.</forenames></author><author><keyname>Schneider</keyname><forenames>C.</forenames></author></authors><title>Calculating Three Loop Ladder and V-Topologies for Massive Operator
  Matrix Elements by Computer Algebra</title><categories>hep-ph cs.SC hep-th math-ph math.MP</categories><comments>110 pages Latex, 4 Figures</comments><report-no>DESY 15--049, DO--TH 15/06, MITP/15-080</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three loop ladder and $V$-topology diagrams contributing to the massive
operator matrix element $A_{Qg}$ are calculated. The corresponding objects can
all be expressed in terms of nested sums and recurrences depending on the
Mellin variable $N$ and the dimensional parameter $\varepsilon$. Given these
representations, the desired Laurent series expansions in $\varepsilon$ can be
obtained with the help of our computer algebra toolbox. Here we rely on
generalized hypergeometric functions and Mellin-Barnes representations, on
difference ring algorithms for symbolic summation, on an optimized version of
the multivariate Almkvist-Zeilberger algorithm for symbolic integration, and on
new methods to calculate Laurent series solutions of coupled systems of
differential equations. The solutions can be computed for general coefficient
matrices directly for any basis also performing the expansion in the
dimensional parameter in case it is expressible in terms of indefinite nested
product-sum expressions. This structural result is based on new results of our
difference ring theory. In the cases discussed we deal with iterative sum- and
integral-solutions over general alphabets. The final results are expressed in
terms of special sums, forming quasi-shuffle algebras, such as nested harmonic
sums, generalized harmonic sums, and nested binomially weighted (cyclotomic)
sums. Analytic continuations to complex values of $N$ are possible through the
recursion relations obeyed by these quantities and their analytic asymptotic
expansions. The latter lead to a host of new constants beyond the multiple zeta
values, the infinite generalized harmonic and cyclotomic sums in the case of
$V$-topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08329</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08329</id><created>2015-09-28</created><authors><author><keyname>Escalante-B.</keyname><forenames>Alberto N.</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA
  for the Design of Training Graphs</title><categories>cs.AI cs.CV stat.ML</categories><comments>29 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slow feature analysis (SFA) is an unsupervised learning algorithm that
extracts slowly varying features from a time series. Graph-based SFA (GSFA) is
a supervised extension that can solve regression problems if followed by a
post-processing regression algorithm. A training graph specifies arbitrary
connections between the training samples. The connections in current graphs,
however, only depend on the rank of the involved labels. Exploiting the exact
label values makes further improvements in estimation accuracy possible.
  In this article, we propose the exact label learning (ELL) method to create a
graph that codes the desired label explicitly, so that GSFA is able to extract
a normalized version of it directly. The ELL method is used for three tasks:
(1) We estimate gender from artificial images of human faces (regression) and
show the advantage of coding additional labels, particularly skin color. (2) We
analyze two existing graphs for regression. (3) We extract compact
discriminative features to classify traffic sign images. When the number of
output features is limited, a higher classification rate is obtained compared
to a graph equivalent to nonlinear Fisher discriminant analysis. The method is
versatile, directly supports multiple labels, and provides higher accuracy
compared to current graphs for the problems considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08331</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08331</id><created>2015-09-28</created><authors><author><keyname>Gao</keyname><forenames>Xiaobin</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Optimal Estimation with Limited Measurements and Noisy Communication</title><categories>cs.SY cs.IT math.IT</categories><comments>X. Gao, E. Akyol, and T. Basar. Optimal estimation with limited
  measurements and noisy communication. In 54th IEEE Conference on Decision and
  Control (CDC15), 2015, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a sequential estimation and sensor scheduling problem
with one sensor and one estimator. The sensor makes sequential observations
about the state of an underlying memoryless stochastic process, and makes a
decision as to whether or not to send this measurement to the estimator. The
sensor and the estimator have the common objective of minimizing expected
distortion in the estimation of the state of the process, over a finite time
horizon, with the constraint that the sensor can transmit its observation only
a limited number of times. As opposed to the prior work where communication
between the sensor and the estimator was assumed to be perfect (noiseless), in
this work an additive noise channel with fixed power constraint is considered;
hence, the sensor has to encode its message before transmission. For some
specific source and channel noise densities, we obtain the optimal encoding and
estimation policies in conjunction with the optimal transmission schedule. The
impact of the presence of a noisy channel is analyzed numerically based on
dynamic programming. This analysis yields some rather surprising results such
as a phase-transition phenomenon in the number of used transmission
opportunities, which was not encountered in the noiseless communication
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08333</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08333</id><created>2015-09-28</created><updated>2016-02-16</updated><authors><author><keyname>Yu</keyname><forenames>Hsiang-Fu</forenames></author><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>High-dimensional Time Series Prediction with Missing Values</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-dimensional time series prediction is needed in applications as diverse
as demand forecasting and climatology. Often, such applications require methods
that are both highly scalable, and deal with noisy data in terms of corruptions
or missing values. Classical time series methods usually fall short of handling
both these issues. In this paper, we propose to adapt matrix matrix completion
approaches that have previously been successfully applied to large scale noisy
data, but which fail to adequately model high-dimensional time series due to
temporal dependencies. We present a novel temporal regularized matrix
factorization (TRMF) framework which supports data-driven temporal dependency
learning and enables forecasting ability to our new matrix factorization
approach. TRMF is highly general, and subsumes many existing matrix
factorization approaches for time series data. We make interesting connections
to graph regularized matrix factorization methods in the context of learning
the dependencies. Experiments on both real and synthetic data show that TRMF
outperforms several existing approaches for common time series tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08343</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08343</id><created>2015-09-28</created><authors><author><keyname>Pereira</keyname><forenames>Pedro O.</forenames></author><author><keyname>Boskos</keyname><forenames>Dimitris</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>A Common Framework for Attitude Synchronization of Unit Vectors in
  Networks with Switching Topology</title><categories>cs.SY</categories><comments>Companion manuscript submitted to ACC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study attitude synchronization for elements in the unit
sphere of R3 and for elements in the 3D rotation group, for a network with
switching topology. The agents angular velocities are assumed to be the control
inputs, and a switching control law for each agent is devised that guarantees
synchronization, provided that all elements are initially contained in a given
region, unknown to the network. The control law is decentralized and it does
not require a common orientation frame among all agents. We refer to
synchronization of unit vectors in R3 as incomplete synchronization, and of 3D
rotation matrices as complete synchronization. Our main contribution lies on
showing that these two problems can be analyzed under a common framework, where
all elements' dynamics are transformed into unit vectors dynamics on a sphere
of appropriate dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08346</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08346</id><created>2015-09-28</created><updated>2015-10-06</updated><authors><author><keyname>Najafabadi</keyname><forenames>SayedJalil Modares</forenames></author><author><keyname>Mastronarde</keyname><forenames>Nicholas</forenames></author><author><keyname>Medley</keyname><forenames>Michael J.</forenames></author><author><keyname>Matyjas</keyname><forenames>John D.</forenames></author></authors><title>UB-ANC: An Open Platform Testbed for Software-Defined Airborne
  Networking and Communications</title><categories>cs.NI cs.RO</categories><comments>16 pages (including references), 6 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the University at Buffalo's Software-Defined Airborne Networking
and Communications Testbed (UB-ANC). UB-ANC is an open software/hardware
platform that aims to facilitate rapid testing and repeatable comparative
evaluation of various airborne networking and communications protocols at
different layers of the network protocol stack. In particular, it combines
quadcopters capable of autonomous flight with USRP E310 embedded
software-defined radios (SDRs), which enable flexible deployment of novel
communications and networking protocols, and real-time reconfigurable cognitive
radio frequency (RF) links. This is in contrast to existing airborne network
testbeds, which typically rely on inflexible off-the-shelf radios based on
fixed standards and protocols, e.g., IEEE 802.11 (Wi-Fi) or IEEE 802.15.4
(Zigbee). UB-ANC is designed with emphasis on modularity in terms of both
hardware and software, which makes it highly customizable. It is built on two
open-source frameworks, namely, GNU Radio for the physical and medium access
control layers and C++ Qt for the higher layers of the network protocol stack
including the network, transport, and application layers. In this paper, we
present the hardware and software architectures underlying UB-ANC, describe the
implementation and operation of its most important components, and discuss how
these components can be modified to achieve desired behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08353</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08353</id><created>2015-09-28</created><updated>2016-01-31</updated><authors><author><keyname>Frahm</keyname><forenames>Gabriel</forenames></author></authors><title>A Note on Bayesian Rationality and Correlated Equilibrium</title><categories>cs.GT math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian rationality in strategic games presumes that it is possible to
translate strategic uncertainty into imperfect information. Correlated
equilibrium is guided by the idea that players are Bayes rational, have a
common prior, and choose their strategies independently. I show that an
essential condition for Bayesian rationality is violated in every game with
imperfect information. Moreover, without strategic uncertainty, players cannot
choose their strategies independently. This means strategic independence
requires strategic uncertainty. If we distinguish between strategic certainty
and uncertainty, we are able to explain both the existence of the cooperative
and the noncooperative solution of the prisoner's dilemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08357</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08357</id><created>2015-09-28</created><authors><author><keyname>Patel</keyname><forenames>Utkarsh R.</forenames></author><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>Skin Effect Modeling in Conductors of Arbitrary Shape Through a Surface
  Admittance Operator and the Contour Integral Method</title><categories>cs.CE</categories><comments>This paper has been submitted for publication to the IEEE
  Transactions on Microwave Theory and Techniques on September 27, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate modeling of skin effect inside conductors is of capital
importance to solve transmission line and scattering problems. This paper
presents a surface-based formulation to model skin effect in conductors of
arbitrary cross section, and compute the per-unit-length impedance of a
multiconductor transmission line. The proposed formulation is based on the
Dirichlet-Neumann operator that relates the longitudinal electric field to the
tangential magnetic field on the boundary of a conductor. We demonstrate how
the surface operator can be obtained through the contour integral method for
conductors of arbitrary shape. The proposed algorithm is simple to implement,
efficient, and can handle arbitrary cross-sections, which is a main advantage
over the existing approach based on eigenfunctions, which is available only for
canonical conductor's shapes. The versatility of the method is illustrated
through a diverse set of examples, which includes transmission lines with
trapezoidal, curved, and V-shaped conductors. Numerical results demonstrate the
accuracy, versatility, and efficiency of the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08360</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08360</id><created>2015-09-28</created><authors><author><keyname>Ramasamy</keyname><forenames>Dinesh</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Compressive spectral embedding: sidestepping the SVD</title><categories>stat.ML cs.LG</categories><comments>NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral embedding based on the Singular Value Decomposition (SVD) is a
widely used &quot;preprocessing&quot; step in many learning tasks, typically leading to
dimensionality reduction by projecting onto a number of dominant singular
vectors and rescaling the coordinate axes (by a predefined function of the
singular value). However, the number of such vectors required to capture
problem structure grows with problem size, and even partial SVD computation
becomes a bottleneck. In this paper, we propose a low-complexity it compressive
spectral embedding algorithm, which employs random projections and finite order
polynomial expansions to compute approximations to SVD-based embedding. For an
m times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)),
and the embedding dimension is O(log(m+n)), both of which are independent of
the number of singular vectors whose effect we wish to capture. To the best of
our knowledge, this is the first work to circumvent this dependence on the
number of singular vectors for general SVD-based embeddings. The key to
sidestepping the SVD is the observation that, for downstream inference tasks
such as clustering and classification, we are only interested in using the
resulting embedding to evaluate pairwise similarity metrics derived from the
euclidean norm, rather than capturing the effect of the underlying matrix on
arbitrary vectors as a partial SVD tries to do. Our numerical results on
network datasets demonstrate the efficacy of the proposed method, and motivate
further exploration of its application to large-scale inference tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08368</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08368</id><created>2015-09-28</created><updated>2015-10-27</updated><authors><author><keyname>Coviello</keyname><forenames>Lorenzo</forenames></author><author><keyname>Franceschetti</keyname><forenames>Massimo</forenames></author><author><keyname>Garcia-Herranz</keyname><forenames>Manuel</forenames></author><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author></authors><title>Limits of Friendship Networks in Predicting Epidemic Risk</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>74 pages, 28 figures, 12 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of an infection on a real-world social network is determined by
the interplay of two processes: the dynamics of the network, whose structure
changes over time according to the encounters between individuals, and the
dynamics on the network, whose nodes can infect each other after an encounter.
Physical encounter is the most common vehicle for the spread of infectious
diseases, but detailed information about encounters is often unavailable
because expensive, unpractical to collect or privacy sensitive. We asks whether
the friendship ties between the individuals in a social network successfully
predict who is at risk. Using a dataset from a popular online review service,
we build a time-varying network that is a proxy of physical encounter between
users and a static network based on reported friendship. Through computer
simulations, we compare infection processes on the resulting networks and show
that, whereas distance on the friendship network is correlated to epidemic
risk, friendship provides a poor identification of the individuals at risk if
the infection is driven by physical encounter. Such limit is not due to the
randomness of the infection, but to the structural differences of the two
networks. In contrast to the macroscopic similarity between processes spreading
on different networks, the differences in local connectivity determined by the
two definitions of edges result in striking differences between the dynamics at
a microscopic level. Despite the limits highlighted, we show that periodical
and relatively infrequent monitoring of the real infection on the encounter
network allows to correct the predicted infection on the friendship network and
to achieve satisfactory prediction accuracy. In addition, the friendship
network contains valuable information to effectively contain epidemic outbreaks
when a limited budget is available for immunization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08373</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08373</id><created>2015-09-28</created><authors><author><keyname>Notarnicola</keyname><forenames>Ivano</forenames></author><author><keyname>Notarstefano</keyname><forenames>Giuseppe</forenames></author></authors><title>Asynchronous Distributed Optimization via Randomized Dual Proximal
  Gradient</title><categories>cs.SY math.OC</categories><comments>submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider distributed optimization problems in which the cost
function is separable, i.e., a sum of possibly non-smooth functions all sharing
a common variable, and can be split into a strongly convex term and a convex
one. The second term is typically used to encode constraints or to regularize
the solution. We propose a class of distributed optimization algorithms based
on proximal gradient methods applied to the dual problem. We show that, by
means of a proper choice of primal variable copies, the dual problem is itself
separable when written in terms of conjugate functions, and the dual variables
can be stacked into non-overlapping blocks associated to the computing nodes.
We first show that a weighted proximal gradient on the dual function leads to a
synchronous distributed algorithm with proper local dual proximal gradient
updates at each node. Then, as main paper contribution, we develop asynchronous
versions of the algorithm in which the node updates are triggered by local
timers without any global iteration counter. The algorithms are shown to be
proper randomized block-coordinate proximal gradient updates on the dual
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08376</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08376</id><created>2015-09-28</created><authors><author><keyname>Duursma</keyname><forenames>Iwan M.</forenames></author></authors><title>Matrix Theory for Minimal Trellises</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trellises provide a graphical representation for the row space of a matrix.
The product construction of Kschischang and Sorokine builds minimal
conventional trellises from matrices in minimal span form. Koetter and Vardy
showed that minimal tail-biting trellises can be obtained by applying the
product construction to submatrices of a characteristic matrix.
  We introduce the unique reduced minimal span form of a matrix and we obtain
an expression for the unique reduced characteristic matrix. Among new
properties of characteristic matrices we prove that characteristic matrices are
in duality if and only if they have orthogonal column spaces, and that the
transpose of a characteristic matrix is again a characteristic matrix if and
only if the characteristic matrix is reduced. These properties have clear
interpretations for the unwrapped unit memory convolutional code of a
tail-biting trellis, they explain the duality for the class of Koetter and
Vardy trellises, and they give a natural relation between the characteristic
matrix based Koetter-Vardy construction and the displacement matrix based
Nori-Shankar construction.
  For a pair of reduced characteristic matrices in duality, one is
lexicographically first in a forward direction and the other is
lexicographically first in the reverse direction. This confirms a conjecture by
Koetter and Vardy after taking into account the different directions for the
lexicographical ordering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08379</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08379</id><created>2015-09-28</created><updated>2015-12-07</updated><authors><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author><author><keyname>Wu</keyname><forenames>Ying Nian</forenames></author></authors><title>Learning FRAME Models Using CNN Filters</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convolutional neural network (ConvNet or CNN) has proven to be very
successful in many tasks such as those in computer vision. In this conceptual
paper, we study the generative perspective of the discriminative CNN. In
particular, we propose to learn the generative FRAME (Filters, Random field,
And Maximum Entropy) model using the highly expressive filters pre-learned by
the CNN at the convolutional layers. We show that the learning algorithm can
generate realistic and rich object and texture patterns in natural scenes. We
explain that each learned model corresponds to a new CNN unit at a layer above
the layer of filters employed by the model. We further show that it is possible
to learn a new layer of CNN units using a generative CNN model, which is a
product of experts model, and the learning algorithm admits an EM
interpretation with binary latent variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08383</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08383</id><created>2015-09-28</created><authors><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Tang</keyname><forenames>Feng</forenames></author><author><keyname>Guo</keyname><forenames>Yanwen</forenames></author><author><keyname>Tao</keyname><forenames>Hai</forenames></author></authors><title>Efficient Discriminative Nonorthogonal Binary Subspace with its
  Application to Visual Tracking</title><categories>cs.CV</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the crucial problems in visual tracking is how the object is
represented. Conventional appearance-based trackers are using increasingly more
complex features in order to be robust. However, complex representations
typically not only require more computation for feature extraction, but also
make the state inference complicated. We show that with a careful feature
selection scheme, extremely simple yet discriminative features can be used for
robust object tracking. The central component of the proposed method is a
succinct and discriminative representation of the object using discriminative
non-orthogonal binary subspace (DNBS) which is spanned by Haar-like features.
The DNBS representation inherits the merits of the original NBS in that it
efficiently describes the object. It also incorporates the discriminative
information to distinguish foreground from background. However, the problem of
finding the DNBS bases from an over-complete dictionary is NP-hard. We propose
a greedy algorithm called discriminative optimized orthogonal matching pursuit
(D-OOMP) to solve this problem. An iterative formulation named iterative D-OOMP
is further developed to drastically reduce the redundant computation between
iterations and a hierarchical selection strategy is integrated for reducing the
search space of features. The proposed DNBS representation is applied to object
tracking through SSD-based template matching. We validate the effectiveness of
our method through extensive experiments on challenging videos with comparisons
against several state-of-the-art trackers and demonstrate its capability to
track objects in clutter and moving background.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08387</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08387</id><created>2015-09-28</created><authors><author><keyname>Lipor</keyname><forenames>John</forenames></author><author><keyname>Balzano</keyname><forenames>Laura</forenames></author><author><keyname>Kerkez</keyname><forenames>Branko</forenames></author><author><keyname>Scavia</keyname><forenames>Don</forenames></author></authors><title>Quantile Search: A Distance-Penalized Active Learning Algorithm for
  Spatial Sampling</title><categories>stat.ML cs.LG</categories><comments>19 pages, 12 figures. This is an expanded version of the same paper
  to be published in Proc. 53rd Annual Allerton Conf. on Communication,
  Control, and Computing</comments><msc-class>62L05</msc-class><acm-class>G.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive sampling theory has shown that, with proper assumptions on the
signal class, algorithms exist to reconstruct a signal in $\mathbb{R}^{d}$ with
an optimal number of samples. We generalize this problem to when the cost of
sampling is not only the number of samples but also the distance traveled
between samples. This is motivated by our work studying regions of low oxygen
concentration in the Great Lakes. We show that for one-dimensional threshold
classifiers, a tradeoff between number of samples and distance traveled can be
achieved using a generalization of binary search, which we refer to as quantile
search. We derive the expected total sampling time for noiseless measurements
and the expected number of samples for an extension to the noisy case. We
illustrate our results in simulations relevant to our sampling application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08388</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08388</id><created>2015-09-28</created><authors><author><keyname>Nakasan</keyname><forenames>Chawanat</forenames></author><author><keyname>Ichikawa</keyname><forenames>Kohei</forenames></author><author><keyname>Iida</keyname><forenames>Hajimu</forenames></author><author><keyname>Uthayopas</keyname><forenames>Putchong</forenames></author></authors><title>A Simple Multipath OpenFlow Controller using topology-based algorithm
  for Multipath TCP</title><categories>cs.NI</categories><comments>8 pages, submitted for PRAGMA-ICDS 2015</comments><acm-class>C.2.1; C.2.2</acm-class><doi>10.6084/m9.figshare.1558361</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multipath TCP, or MPTCP, is a widely-researched mechanism that allows a
single application-level connection to be split to more than one TCP stream,
and consequently more than one network interface, as opposed to the traditional
TCP/IP model. Being a transport layer protocol, MPTCP can easily interact
between the application using it and the network supporting it. However, MPTCP
does not have control of its own route. Default IP routing behavior generally
takes all traffic through the shortest or best-metric path. However, this
behavior may actually cause paths to collide with each other, creating
contention for bandwidth in a number of edges. This can result in a bottleneck
which limits the throughput of the network. Therefore, a multipath routing
mechanism is necessary to ensure smooth operation of MPTCP. We created smoc, a
Simple Multipath OpenFlow Controller, that uses only topology information of
the network to avoid collision where possible. Evaluation of smoc in a virtual
local-area and a physical wide-area SDNs showed favorable results as smoc
provided better performance than simple or spanning-tree routing mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08392</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08392</id><created>2015-09-28</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Mauroy</keyname><forenames>Alexandre</forenames></author></authors><title>Properties of Eventually Positive Linear Input-Output Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the so-called eventually positive systems, that is
systems with trajectories originating in a positive orthant becoming positive
after some finite transient. We extend this notion to the input-output system
case. Our extension is performed in such a manner, that some valuable
properties of internally positive input-output systems are preserved. For
example, the stable systems have sum-separable Lyapunov functions, induced
norms can be computed using linear programming and the energy functions have
nonnegative derivatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08396</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08396</id><created>2015-09-28</created><authors><author><keyname>Manral</keyname><forenames>Jai</forenames></author><author><keyname>Hossain</keyname><forenames>Mohammed Alamgir</forenames></author></authors><title>An Innovative Approach for online Meta Search Engine Optimization</title><categories>cs.IR cs.AI</categories><comments>The 6th Conference on Software, Knowledge, Information Management and
  Applications, Chengdu, China, September 9-11 2012, #57</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to identify efficient techniques used in Web
Search Engine Optimization (SEO). Understanding SEO factors which can influence
page ranking in search engine is significant for webmasters who wish to attract
large number of users to their website. Different from previous relevant
research, in this study we developed an intelligent Meta search engine which
aggregates results from various search engines and ranks them based on several
important SEO parameters. The research tries to establish that using more SEO
parameters in ranking algorithms helps in retrieving better search results thus
increasing user satisfaction. Initial results generated from Meta search engine
outperformed existing search engines in terms of better retrieved search
results with high precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08401</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08401</id><created>2015-09-28</created><authors><author><keyname>Manral</keyname><forenames>Jai</forenames></author></authors><title>Automated Test Case Generation using Petri Nets</title><categories>cs.SE</categories><comments>Software Testing, Petri Net, UML, High Level Petri Nets, Test Cases</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing is the process of determining the precision, quality,
completeness and security of the software systems. An important step in testing
software is the generation of test cases, whose quality plays a vital role in
determining the time for testing and subsequently its cost. In this research,
it is shown that both structural and behavioural diagrams can be used to
represent specifications in a single model using High Level Petri Nets (HLPN).
This research focuses on automated generation of test models from Petri nets.
Moreover, generating consistent formal models (HLPN) from informal models (UML)
is the highlight of this research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08409</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08409</id><created>2015-09-28</created><authors><author><keyname>Gates</keyname><forenames>Alexander J.</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Control of complex networks requires both structure and dynamics</title><categories>q-bio.MN cs.SY math.OC</categories><comments>39 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of network structure has uncovered organizational principles in
complex systems. However, there is also a need to understand how to control
them; for example, to revert a diseased cell to a healthy state, or a mature
cell to a pluripotent state. Two recent methodologies suggest that the
controllability of complex multivariate systems can be predicted solely from
the graph of interactions between variables, without considering variable
dynamics: structural controllability and minimum dominating sets. Both
methodologies utilize idealized assumptions about multivariate dynamics, yet
most accurate models of real-world systems do not abide by these assumptions.
Here, we study the relationship between network structure and the control of
multivariate dynamics using three distinct measures of controllability in
Boolean Networks. We demonstrate that structure-only methods fail to properly
characterize controllability in these nonlinear systems; even in very simple
networks, a large variation of possible dynamics can occur for the same
structure, each with different control profiles. Our methodology is also used
to characterize critical control variables in three models of biochemical
regulation: the Drosophila melanogaster single-cell segment polarity network,
the eukaryotic cell cycle of budding yeast Saccharomyces cerevisiae, and the
floral organ arrangement in Arabidopsis thaliana. Structure-only methods both
undershoot and overshoot the number and which sets of variables actually
control these models, highlighting the importance of the system dynamics in
determining control. Our analysis further shows that the logic of automata
transition functions, namely how canalizing they are, plays a role in the
extent to which structure predicts dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08418</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08418</id><created>2015-09-28</created><authors><author><keyname>Li</keyname><forenames>Zheng</forenames></author><author><keyname>O'Brien</keyname><forenames>Liam</forenames></author><author><keyname>Yang</keyname><forenames>Ye</forenames></author></authors><title>The more Product Complexity, the more Actual Effort? An Empirical
  Investigation into Software Developments</title><categories>cs.SE</categories><journal-ref>ASWEC 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  [Background:] Software effort prediction methods and models typically assume
positive correlation between software product complexity and development
effort. However, conflicting observations, i.e. negative correlation between
product complexity and actual effort, have been witnessed from our experience
with the COCOMO81 dataset. [Aim:] Given our doubt about whether the observed
phenomenon is a coincidence, this study tries to investigate if an increase in
product complexity can result in the abovementioned counter-intuitive trend in
software development projects. [Method:] A modified association rule mining
approach is applied to the transformed COCOMO81 dataset. To reduce noise of
analysis, this approach uses a constant antecedent (Complexity increases while
Effort decreases) to mine potential consequents with pruning. [Results:] The
experiment has respectively mined four, five, and seven association rules from
the general, embedded, and organic projects data. The consequents of the mined
rules suggested two main aspects, namely human capability and product scale, to
be particularly concerned in this study. [Conclusions:] The negative
correlation between complexity and effort is not a coincidence under particular
conditions. In a software project, interactions between product complexity and
other factors, such as Programmer Capability and Analyst Capability, can
inevitably play a &quot;friction&quot; role in weakening the practical influences of
product complexity on actual development effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08420</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08420</id><created>2015-09-28</created><authors><author><keyname>Ichikawa</keyname><forenames>Kohei</forenames></author><author><keyname>Tsugawa</keyname><forenames>Mauricio</forenames></author><author><keyname>Haga</keyname><forenames>Jason</forenames></author><author><keyname>Yamanaka</keyname><forenames>Hiroaki</forenames></author><author><keyname>Liu</keyname><forenames>Te-Lung</forenames></author><author><keyname>Kido</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>U-Chupala</keyname><forenames>Pongsakorn</forenames></author><author><keyname>Huang</keyname><forenames>Che</forenames></author><author><keyname>Nakasan</keyname><forenames>Chawanat</forenames></author><author><keyname>Chang</keyname><forenames>Jo-Yu</forenames></author><author><keyname>Ku</keyname><forenames>Li-Chi</forenames></author><author><keyname>Tsai</keyname><forenames>Whey-Fone</forenames></author><author><keyname>Date</keyname><forenames>Susumu</forenames></author><author><keyname>Shimojo</keyname><forenames>Shinji</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Philip</forenames></author><author><keyname>Fortes</keyname><forenames>Jose</forenames></author></authors><title>PRAGMA-ENT: Exposing SDN Concepts to Domain Scientists in the Pacific
  Rim</title><categories>cs.NI</categories><comments>8 pages, 12 figures, PRAGMA-ICDS 2015</comments><acm-class>C.2.1; C.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Pacific Rim Application and Grid Middleware Assembly (PRAGMA) is an
international community of researchers that actively collaborate to address
problems and challenges of common interest in eScience. The PRAGMA Experimental
Network Testbed (PRAGMA-ENT) was established with the goal of constructing an
international software-defined network (SDN) testbed to offer the necessary
networking support to the PRAGMA cyberinfrastructure. PRAGMA-ENT is isolated,
and PRAGMA researchers have complete freedom to access network resources to
develop, experiment, and evaluate new ideas without the concerns of interfering
with production networks.
  In the first phase, PRAGMA-ENT focused on establishing an international L2
backbone. With support from the Florida Lambda Rail (FLR), Internet2,
PacificWave, JGN-X, and TWAREN, PRAGMA-ENT backbone connects Open\-Flow-enabled
switches at University of Florida (UF), University of California San Diego
(UCSD), Nara Institute of Science and Technology (NAIST, Japan), Osaka
University (Japan), National Institute of Advanced Industrial Science and
Technology (AIST, Japan), and National Center for High-Performance Computing
(Taiwan).
  The second phase of PRAGMA-ENT consisted of evaluation of technologies for
the control plane that enables multiple experiments (i.e., OpenFlow
controllers) to co-exist. Preliminary experiments with FlowVisor revealed some
limitations leading to the development of a new approach, called AutoVFlow.
This paper will share our experience in the establishment of PRAGMA-ENT
backbone (with international L2 links), its current status, and control plane
plans. Discussion on preliminary application ideas, including optimization of
routing control; multipath routing control; and remote visualization will also
be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08434</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08434</id><created>2015-09-28</created><authors><author><keyname>Mirsoleimani</keyname><forenames>S. Ali</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Herik</keyname><forenames>Jaap van den</forenames></author></authors><title>Ensemble UCT Needs High Exploitation</title><categories>cs.AI</categories><comments>7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results have shown that the MCTS algorithm (a new, adaptive,
randomized optimization algorithm) is effective in a remarkably diverse set of
applications in Artificial Intelligence, Operations Research, and High Energy
Physics. MCTS can find good solutions without domain dependent heuristics,
using the UCT formula to balance exploitation and exploration. It has been
suggested that the optimum in the exploitation- exploration balance differs for
different search tree sizes: small search trees needs more exploitation; large
search trees need more exploration. Small search trees occur in variations of
MCTS, such as parallel and ensemble approaches. This paper investigates the
possibility of improving the performance of Ensemble UCT by increasing the
level of exploitation. As the search trees becomes smaller we achieve an
improved performance. The results are important for improving the performance
of large scale parallelism of MCTS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08439</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08439</id><created>2015-09-28</created><authors><author><keyname>Narayan</keyname><forenames>Sanath</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Kalpathi R.</forenames></author></authors><title>Hyper-Fisher Vectors for Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel encoding scheme combining Fisher vector and
bag-of-words encodings has been proposed for recognizing action in videos. The
proposed Hyper-Fisher vector encoding is sum of local Fisher vectors which are
computed based on the traditional Bag-of-Words (BoW) encoding. Thus, the
proposed encoding is simple and yet an effective representation over the
traditional Fisher Vector encoding. By extensive evaluation on challenging
action recognition datasets, viz., Youtube, Olympic Sports, UCF50 and HMDB51,
we show that the proposed Hyper-Fisher Vector encoding improves the recognition
performance by around 2-3% compared to the improved Fisher Vector encoding. We
also perform experiments to show that the performance of the Hyper-Fisher
Vector is robust to the dictionary size of the BoW encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08443</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08443</id><created>2015-09-28</created><authors><author><keyname>Dubey</keyname><forenames>Ayush</forenames></author><author><keyname>Hill</keyname><forenames>Greg D.</forenames></author><author><keyname>Escriva</keyname><forenames>Robert</forenames></author><author><keyname>Sirer</keyname><forenames>Emin G&#xfc;n</forenames></author></authors><title>Weaver: A High-Performance, Transactional Graph Store Based on Refinable
  Timestamps</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed systems for storing and processing large graphs have become an
increasingly common infrastructure component. Yet existing systems either
operate on offline snapshots, provide poor consistency guarantees for
dynamically changing graphs, or employ expensive concurrency control techniques
that limit performance. In this paper, we introduce a new distributed graph
store, called Weaver, which enables efficient, transactional graph analyses as
well as strictly serializable read-write transactions on dynamic graphs. The
key insight that enables Weaver to combine strict serializability with
horizontal scalability and high performance is a novel request ordering
mechanism called refinable timestamps. This technique couples coarse-grained
vector timestamps with a fine-grained timeline oracle to pay the overhead of
strong consistency only when needed. Experiments show that Weaver enables a
Bitcoin blockchain explorer that is 8x faster than Blockchain.info, and
achieves 12x higher throughput than the Titan graph database on social network
workloads and 4x lower latency than GraphLab on offline graph traversal
workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08451</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08451</id><created>2015-09-24</created><authors><author><keyname>Qian</keyname><forenames>Cheng</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Huang</keyname><forenames>Kejun</forenames></author><author><keyname>Huang</keyname><forenames>Lei</forenames></author><author><keyname>So</keyname><forenames>H. C.</forenames></author></authors><title>Phase Retrieval Using Feasible Point Pursuit: Algorithms and
  Cram\'er-Rao Bound</title><categories>cs.IT math.IT math.NA math.OC math.ST stat.TH</categories><comments>13 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstructing a signal from squared linear (rank-one quadratic) measurements
is a challenging problem with important applications in optics and imaging,
where it is known as phase retrieval. This paper proposes two new phase
retrieval algorithms based on non-convex quadratically constrained quadratic
programming (QCQP) formulations, and a recently proposed approximation
technique dubbed feasible point pursuit (FPP). The first is designed for
uniformly distributed bounded measurement errors, such as those arising from
high-rate quantization (B-FPP). The second is designed for Gaussian measurement
errors, using a least squares criterion (LS-FPP). Their performance is measured
against state-of-the-art algorithms and the Cram\'er-Rao bound (CRB), which is
also derived here. Simulations show that LS-FPP outperforms the state-of-art
and operates close to the CRB. Compact CRB expressions, properties, and
insights are obtained by explicitly computing the CRB in various special cases
-- including when the signal of interest admits a sparse parametrization, using
harmonic retrieval as an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08455</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08455</id><created>2015-09-28</created><authors><author><keyname>Karl</keyname><forenames>Maximilian</forenames></author><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>Efficient Empowerment</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empowerment quantifies the influence an agent has on its environment. This is
formally achieved by the maximum of the expected KL-divergence between the
distribution of the successor state conditioned on a specific action and a
distribution where the actions are marginalised out. This is a natural
candidate for an intrinsic reward signal in the context of reinforcement
learning: the agent will place itself in a situation where its action have
maximum stability and maximum influence on the future. The limiting factor so
far has been the computational complexity of the method: the only way of
calculation has so far been a brute force algorithm, reducing the applicability
of the method to environments with a small set discrete states. In this work,
we propose to use an efficient approximation for marginalising out the actions
in the case of continuous environments. This allows fast evaluation of
empowerment, paving the way towards challenging environments such as real world
robotics. The method is presented on a pendulum swing up problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08456</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08456</id><created>2015-09-26</created><authors><author><keyname>Rossi</keyname><forenames>Giovanni</forenames></author></authors><title>Multilinear objective function-based clustering</title><categories>cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:1509.07986</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The input of most clustering algorithms is a symmetric matrix quantifying
similarity within data pairs. Such a matrix is here turned into a quadratic set
function measuring cluster score or similarity within data subsets larger than
pairs. In general, any set function reasonably assigning a cluster score to
data subsets gives rise to an objective function-based clustering problem. When
considered in pseudo-Boolean form, cluster score enables to evaluate fuzzy
clusters through multilinear extension MLE, while the global score of fuzzy
clusterings simply is the sum over constituents fuzzy clusters of their MLE
score. This is shown to be no greater than the global score of hard clusterings
or partitions of the data set, thereby expanding a known result on extremizers
of pseudo-Boolean functions. Yet, a multilinear objective function allows to
search for optimality in the interior of the hypercube. The proposed method
only requires a fuzzy clustering as initial candidate solution, for the
appropriate number of clusters is implicitly extracted from the given data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08465</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08465</id><created>2015-09-28</created><updated>2015-10-26</updated><authors><author><keyname>de Melo</keyname><forenames>Pedro O. S. Vaz</forenames></author></authors><title>How Many Political Parties Should Brazil Have? A Data-driven Method to
  Assess and Reduce Fragmentation in Multi-Party Political Systems</title><categories>cs.SI cs.CY cs.MA</categories><msc-class>91Cxx</msc-class><acm-class>H.2.8</acm-class><doi>10.1371/journal.pone.0147656</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In June 2013, Brazil faced the largest and most significant mass protests in
a generation. These were exacerbated by the population's disenchantment towards
its highly fragmented party system, which is composed by a very large number of
political parties. Under these circumstances, presidents are constrained by
informal coalition governments, bringing very harmful consequences to the
country. In this work I propose ARRANGE, a dAta dRiven method foR Assessing and
reduciNG party fragmEntation in a country. ARRANGE uses as input the roll call
data for congress votes on bills and amendments as a proxy for political
preferences and ideology. With that, ARRANGE finds the minimum number of
parties required to house all congressmen without decreasing party discipline.
When applied to Brazil's historical roll call data, ARRANGE was able to
generate 23 distinct configurations that, compared with the status quo, have
(i) a significant smaller number of parties, (ii) a higher discipline of
partisans towards their parties and (iii) a more even distribution of partisans
into parties. ARRANGE is fast and parsimonious, relying on a single, intuitive
parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08490</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08490</id><created>2015-09-28</created><authors><author><keyname>Wei</keyname><forenames>Xiaohan</forenames></author><author><keyname>Ling</keyname><forenames>Qing</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Recoverability of Group Sparse Signals from Corrupted Measurements via
  Robust Group Lasso</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of recovering a group sparse signal matrix
$\mathbf{Y} = [\mathbf{y}_1, \cdots, \mathbf{y}_L]$ from sparsely corrupted
measurements $\mathbf{M} = [\mathbf{A}_{(1)}\mathbf{y}_{1}, \cdots,
\mathbf{A}_{(L)}\mathbf{y}_{L}] + \mathbf{S}$, where $\mathbf{A}_{(i)}$'s are
known sensing matrices and $\mathbf{S}$ is an unknown sparse error matrix. A
robust group lasso (RGL) model is proposed to recover $\mathbf{Y}$ and
$\mathbf{S}$ through simultaneously minimizing the $\ell_{2,1}$-norm of
$\mathbf{Y}$ and the $\ell_1$-norm of $\mathbf{S}$ under the measurement
constraints. We prove that $\mathbf{Y}$ and $\mathbf{S}$ can be exactly
recovered from the RGL model with a high probability for a very general class
of $\mathbf{A}_{(i)}$'s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08496</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08496</id><created>2015-09-28</created><authors><author><keyname>Cao</keyname><forenames>Nianxia</forenames></author><author><keyname>Brahma</keyname><forenames>Swastik</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Optimal Auction Design with Quantized Bids</title><categories>cs.GT</categories><comments>6 pages, 3 figures, TSP letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter considers the design of an auction mechanism to sell the object
of a seller when the buyers quantize their private value estimates regarding
the object prior to communicating them to the seller. The designed auction
mechanism maximizes the utility of the seller (i.e., the auction is optimal),
prevents buyers from communicating falsified quantized bids (i.e., the auction
is incentive-compatible), and ensures that buyers will participate in the
auction (i.e., the auction is individually-rational). The letter also
investigates the design of the optimal quantization thresholds using which
buyers quantize their private value estimates. Numerical results provide
insights regarding the influence of the quantization thresholds on the auction
mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08497</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08497</id><created>2015-09-25</created><authors><author><keyname>Beaude</keyname><forenames>Olivier</forenames></author><author><keyname>He</keyname><forenames>Yujun</forenames></author><author><keyname>Hennebel</keyname><forenames>Martin</forenames></author></authors><title>Introducing Decentralized EV Charging Coordination for the Voltage
  Regulation</title><categories>cs.SY</categories><comments>5 pages, 7 figures, keywords: Voltage control - Decentralized
  algorithms - EV charging - Game theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a decentralized optimization methodology to
coordinate Electric Vehicles (EV) charging in order to contribute to the
voltage control on a residential electrical distribution feeder. This aims to
maintain the voltage level in function of the EV's power injection using the
sensitivity matrix approach. The decentralized optimization is tested with two
different methods, respectively global and local, when EV take into account
their impact on all the nodes of the network or only on a local neighborhood of
their connection point. EV can also update their decisions asynchronously or
synchronously. While only the global approach with asynchronous update is
theoretically proven to converge, using results from game theory, simulations
show the potential of other algorithms for which fewer iterations or fewer
informations are necessary. Finally, using Monte Carlo simulations over a wide
range of EV localization configurations, the first analysis have also shown a
promising performance in comparison with uncoordinated charging or with a
&quot;voltage droop charging control&quot; recently proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08519</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08519</id><created>2015-09-28</created><authors><author><keyname>Simic</keyname><forenames>Slavko</forenames></author></authors><title>Refinement of some moment inequalities</title><categories>cs.IT math.IT math.PR</categories><msc-class>60E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give refinements of some convex and log-convex moment
inequalities of the first and second order using a special kind of positive
semi-definite form. An open problem concerning eight parameter refinement of
second order is also stated with some applications in Information Theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08520</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08520</id><created>2015-09-28</created><authors><author><keyname>Shitrit</keyname><forenames>Annabel Sharon</forenames></author><author><keyname>Murin</keyname><forenames>Yonathan</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author><author><keyname>Keren</keyname><forenames>Osnat</forenames></author></authors><title>A New Approach to UEP-HARQ via Convolutional Codes</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel type-II hybrid automatic repeat request (HARQ)
transmission scheme which is based on pruned convolutional codes (CCs) and
supports unequal error protection (UEP). The data to be transmitted is assumed
to consist of important bits (IB) and standard bits (SB). In the proposed
scheme, all the bits are first encoded using a single mother CC and transmitted
over the channel. If a decoding error is detected in the IB, then
retransmission of {\em only} the IB takes place using a CC, typically with a
better error correction capability. Next, taking advantage of the properties of
pruned CCs, the decoded IB are used to increase the decoding reliability for
the SB. Numerical simulations indicate that the proposed scheme offers strong
protection for the IB along with improved reliability for the remaining bits
compared to other UEP schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08535</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08535</id><created>2015-09-28</created><updated>2016-02-04</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Boolean Matrix Factorization and Noisy Completion via Message Passing</title><categories>math.ST cs.AI cs.DM stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean matrix factorization and Boolean matrix completion from noisy
observations are desirable unsupervised data-analysis methods due to their
interpretability, but hard to perform due to their NP-hardness. We treat these
problems as maximum a posteriori inference problems in a graphical model and
present a message passing approach that scales linearly with the number of
observations and factors. Our empirical study demonstrates that message passing
is able to recover low-rank Boolean matrices, in the boundaries of
theoretically possible recovery and compares favorably with state-of-the-art in
real-world applications, such collaborative filtering with large-scale Boolean
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08559</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08559</id><created>2015-09-28</created><authors><author><keyname>Aldini</keyname><forenames>Alessandro</forenames><affiliation>University of Urbino</affiliation></author><author><keyname>Bernardo</keyname><forenames>Marco</forenames><affiliation>University of Urbino</affiliation></author></authors><title>Expected-Delay-Summing Weak Bisimilarity for Markov Automata</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 194, 2015, pp. 1-15</journal-ref><doi>10.4204/EPTCS.194.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new weak bisimulation semantics is defined for Markov automata that, in
addition to abstracting from internal actions, sums up the expected values of
consecutive exponentially distributed delays possibly intertwined with internal
actions. The resulting equivalence is shown to be a congruence with respect to
parallel composition for Markov automata. Moreover, it turns out to be
comparable with weak bisimilarity for timed labeled transition systems, thus
constituting a step towards reconciling the semantics for stochastic time and
deterministic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08560</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08560</id><created>2015-09-28</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames><affiliation>Saarland University, University of Trieste, ISTI-CNR</affiliation></author><author><keyname>De Nicola</keyname><forenames>Rocco</forenames><affiliation>IMT Lucca</affiliation></author><author><keyname>Galpin</keyname><forenames>Vashti</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Gilmore</keyname><forenames>Stephen</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Hillston</keyname><forenames>Jane</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Latella</keyname><forenames>Diego</forenames><affiliation>ISTI-CNR</affiliation></author><author><keyname>Loreti</keyname><forenames>Michele</forenames><affiliation>Universit&#xe0; di Firenze, IMT Lucca</affiliation></author><author><keyname>Massink</keyname><forenames>Mieke</forenames><affiliation>ISTI-CNR</affiliation></author></authors><title>CARMA: Collective Adaptive Resource-sharing Markovian Agents</title><categories>cs.PL cs.DC cs.PF</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><acm-class>C.4; B.8.2</acm-class><journal-ref>EPTCS 194, 2015, pp. 16-31</journal-ref><doi>10.4204/EPTCS.194.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present CARMA, a language recently defined to support
specification and analysis of collective adaptive systems. CARMA is a
stochastic process algebra equipped with linguistic constructs specifically
developed for modelling and programming systems that can operate in open-ended
and unpredictable environments. This class of systems is typically composed of
a huge number of interacting agents that dynamically adjust and combine their
behaviour to achieve specific goals. A CARMA model, termed a collective,
consists of a set of components, each of which exhibits a set of attributes. To
model dynamic aggregations, which are sometimes referred to as ensembles, CARMA
provides communication primitives that are based on predicates over the
exhibited attributes. These predicates are used to select the participants in a
communication. Two communication mechanisms are provided in the CARMA language:
multicast-based and unicast-based. In this paper, we first introduce the basic
principles of CARMA and then we show how our language can be used to support
specification with a simple but illustrative example of a socio-technical
collective adaptive system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08561</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08561</id><created>2015-09-28</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames><affiliation>University of Trieste</affiliation></author><author><keyname>Hillston</keyname><forenames>Jane</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Efficient Checking of Individual Rewards Properties in Markov Population
  Models</title><categories>cs.LO cs.PF cs.SY</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 194, 2015, pp. 32-47</journal-ref><doi>10.4204/EPTCS.194.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years fluid approaches to the analysis of Markov populations models
have been demonstrated to have great pragmatic value. Initially developed to
estimate the behaviour of the system in terms of the expected values of
population counts, the fluid approach has subsequently been extended to more
sophisticated interrogations of models through its embedding within model
checking procedures. In this paper we extend recent work on checking CSL
properties of individual agents within a Markovian population model, to
consider the checking of properties which incorporate rewards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08562</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08562</id><created>2015-09-28</created><authors><author><keyname>Kawamoto</keyname><forenames>Yusuke</forenames></author><author><keyname>Given-Wilson</keyname><forenames>Thomas</forenames></author></authors><title>Quantitative Information Flow for Scheduler-Dependent Systems</title><categories>cs.CR cs.IT cs.PL math.IT</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><acm-class>D.4.6; H.1.1</acm-class><journal-ref>EPTCS 194, 2015, pp. 48-62</journal-ref><doi>10.4204/EPTCS.194.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative information flow analyses measure how much information on
secrets is leaked by publicly observable outputs. One area of interest is to
quantify and estimate the information leakage of composed systems. Prior work
has focused on running disjoint component systems in parallel and reasoning
about the leakage compositionally, but has not explored how the component
systems are run in parallel or how the leakage of composed systems can be
minimised. In this paper we consider the manner in which parallel systems can
be combined or scheduled. This considers the effects of scheduling channels
where resources may be shared, or whether the outputs may be incrementally
observed. We also generalise the attacker's capability, of observing outputs of
the system, to consider attackers who may be imperfect in their observations,
e.g. when outputs may be confused with one another, or when assessing the time
taken for an output to appear. Our main contribution is to present how
scheduling and observation effect information leakage properties. In
particular, that scheduling can hide some leaked information from perfect
observers, while some scheduling may reveal secret information that is hidden
to imperfect observers. In addition we present an algorithm to construct a
scheduler that minimises the min-entropy leakage and min-capacity in the
presence of any observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08563</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08563</id><created>2015-09-28</created><authors><author><keyname>Latella</keyname><forenames>Diego</forenames><affiliation>CNR/ISTI, Pisa</affiliation></author><author><keyname>Massink</keyname><forenames>Mieke</forenames><affiliation>CNR/ISTI, Pisa</affiliation></author><author><keyname>de Vink</keyname><forenames>Erik</forenames><affiliation>TU/e, Eindhoven</affiliation></author></authors><title>A Definition Scheme for Quantitative Bisimulation</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><acm-class>D.2.4, F.3.2</acm-class><journal-ref>EPTCS 194, 2015, pp. 63-78</journal-ref><doi>10.4204/EPTCS.194.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FuTS, state-to-function transition systems are generalizations of labeled
transition systems and of familiar notions of quantitative semantical models as
continuous-time Markov chains, interactive Markov chains, and Markov automata.
A general scheme for the definition of a notion of strong bisimulation
associated with a FuTS is proposed. It is shown that this notion of
bisimulation for a FuTS coincides with the coalgebraic notion of behavioral
equivalence associated to the functor on Set given by the type of the FuTS. For
a series of concrete quantitative semantical models the notion of bisimulation
as reported in the literature is proven to coincide with the notion of
quantitative bisimulation obtained from the scheme. The comparison includes
models with orthogonal behaviour, like interactive Markov chains, and with
multiple levels of behavior, like Markov automata. As a consequence of the
general result relating FuTS bisimulation and behavioral equivalence we obtain,
in a systematic way, a coalgebraic underpinning of all quantitative
bisimulations discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08564</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08564</id><created>2015-09-28</created><authors><author><keyname>Lee</keyname><forenames>Matias D.</forenames><affiliation>FaMAF, UNC-CONICET, Cordoba</affiliation></author><author><keyname>de Vink</keyname><forenames>Erik P.</forenames><affiliation>TU/e, Eindhoven</affiliation></author></authors><title>Rooted branching bisimulation as a congruence for probabilistic
  transition systems</title><categories>cs.LO</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169. arXiv admin note: text
  overlap with arXiv:1508.06710</comments><proxy>EPTCS</proxy><acm-class>F.1.2,F.3.2</acm-class><journal-ref>EPTCS 194, 2015, pp. 79-94</journal-ref><doi>10.4204/EPTCS.194.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic transition system specification format, referred
to as probabilistic RBB safe, for which rooted branching bisimulation is a
congruence. The congruence theorem is based on the approach of Fokkink for the
qualitative case. For this to work, the theory of transition system
specifications in the setting of labeled transition systems needs to be
extended to deal with probability distributions, both syntactically and
semantically. We provide a scheduler-free characterization of probabilistic
branching bisimulation as adapted from work of Andova et al. for the
alternating model. Counter examples are given to justify the various conditions
required by the format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08565</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08565</id><created>2015-09-28</created><authors><author><keyname>Martinelli</keyname><forenames>Fabio</forenames><affiliation>IIT-CNR</affiliation></author><author><keyname>Matteucci</keyname><forenames>Ilaria</forenames><affiliation>IIT-CNR</affiliation></author><author><keyname>Santini</keyname><forenames>Francesco</forenames><affiliation>IIT-CNR</affiliation></author></authors><title>Semiring-based Specification Approaches for Quantitative Security</title><categories>cs.LO cs.CR cs.FL cs.PL</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><acm-class>D.2.4 Software/Program Verification F.4.1 Mahematical Logic F.4.3
  Formal Languages K.6.5 Security and Protection</acm-class><journal-ref>EPTCS 194, 2015, pp. 95-109</journal-ref><doi>10.4204/EPTCS.194.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to provide different semiring-based formal tools for the
specification of security requirements: we quantitatively enhance the
open-system approach, according to which a system is partially specified.
Therefore, we suppose the existence of an unknown and possibly malicious agent
that interacts in parallel with the system. Two specification frameworks are
designed along two different (but still related) lines. First, by comparing the
behaviour of a system with the expected one, or by checking if such system
satisfies some security requirements: we investigate a novel approximate
behavioural-equivalence for comparing processes behaviour, thus extending the
Generalised Non Deducibility on Composition (GNDC) approach with scores. As a
second result, we equip a modal logic with semiring values with the purpose to
have a weight related to the satisfaction of a formula that specifies some
requested property. Finally, we generalise the classical partial model-checking
function, and we name it as quantitative partial model-checking in such a way
to point out the necessary and sufficient conditions that a system has to
satisfy in order to be considered as secure, with respect to a fixed
security/functionality threshold-value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08566</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08566</id><created>2015-09-28</created><authors><author><keyname>Rosendahl</keyname><forenames>Mads</forenames><affiliation>Roskilde University, Denmark</affiliation></author><author><keyname>Kirkeby</keyname><forenames>Maja H.</forenames><affiliation>Roskilde University, Denmark</affiliation></author></authors><title>Probabilistic Output Analysis by Program Manipulation</title><categories>cs.PL</categories><comments>In Proceedings QAPL 2015, arXiv:1509.08169</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 194, 2015, pp. 110-124</journal-ref><doi>10.4204/EPTCS.194.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of a probabilistic output analysis is to derive a probability
distribution of possible output values for a program from a probability
distribution of its input. We present a method for performing static output
analysis, based on program transformation techniques. It generates a
probability function as a possibly uncomputable expression in an intermediate
language. This program is then analyzed, transformed, and approximated. The
result is a closed form expression that computes an over approximation of the
output probability distribution for the program. We focus on programs where the
possible input follows a known probability distribution. Tests in programs are
not assumed to satisfy the Markov property of having fixed branching
probabilities independently of previous history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08567</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08567</id><created>2015-09-28</created><authors><author><keyname>Kalabi&#x107;</keyname><forenames>Uro&#x161;</forenames></author><author><keyname>Gupta</keyname><forenames>Rohit</forenames></author><author><keyname>Di Cairano</keyname><forenames>Stefano</forenames></author><author><keyname>Bloch</keyname><forenames>Anthony</forenames></author><author><keyname>Kolmanovsky</keyname><forenames>Ilya</forenames></author></authors><title>MPC on manifolds with applications to the control of systems on matrix
  Lie groups</title><categories>math.OC cs.SY</categories><comments>27 pages, 7 figures, submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model predictive control scheme for systems with
discrete-time dynamics evolving on configuration spaces that are smooth
manifolds. The scheme exhibits similar properties to that of ordinary model
predictive control applied to dynamics evolving on R^n. It also exhibits global
asymptotic stability properties even in the case when there does not exist a
globally stabilizing, continuous control law, implying that the model
predictive control law is discontinuous.
  We also demonstrate that there do not exist globally stabilizing, continuous
control laws for manifolds with Euler characteristic equal to 1. In particular,
the case of matrix Lie groups is considered, which are manifolds whose Euler
characteristic is equal to 0. An application to spacecraft attitude control is
also considered in the paper, in which spacecraft attitude evolves on the
matrix Lie group SO(3). Two simulation results are reported. The first
demonstrates that the scheme is able to enforce constraints. The second
simulation considers the unconstrained case in order to test properties
relating to global stability and it is shown that the stabilizing model
predictive control law is discontinuous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08571</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08571</id><created>2015-09-28</created><authors><author><keyname>Delgosha</keyname><forenames>Payam</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Akbarpour</keyname><forenames>Mohammad</forenames></author></authors><title>High Probability Guarantees in Repeated Games: Theory and Applications
  in Information Theory</title><categories>cs.GT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a &quot;high probability&quot; framework for repeated games with
incomplete information. In our non-equilibrium setting, players aim to
guarantee a certain payoff with high probability, rather than in expected
value. We provide a high probability counterpart of the classical result of
Mertens and Zamir for the zero-sum repeated games. Any payoff that can be
guaranteed with high probability can be guaranteed in expectation, but the
reverse is not true. Hence, unlike the average payoff case where the payoff
guaranteed by each player is the negative of the payoff by the other player,
the two guaranteed payoffs would differ in the high probability framework. One
motivation for this framework comes from information transmission systems,
where it is customary to formulate problems in terms of asymptotically
vanishing probability of error. An application of our results to a class of
compound arbitrarily varying channels is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08572</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08572</id><created>2015-09-28</created><updated>2016-02-20</updated><authors><author><keyname>Como</keyname><forenames>Giacomo</forenames></author><author><keyname>Fagnani</keyname><forenames>Fabio</forenames></author></authors><title>From local averaging to emergent global behaviors: the fundamental role
  of network interconnections</title><categories>cs.SY cs.MA math.OC</categories><comments>10 pages</comments><doi>10.1016/j.sysconle.2016.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed averaging is one of the simplest and most studied network
dynamics. Its applications range from cooperative inference in sensor networks,
to robot formation, to opinion dynamics. A number of fundamental results and
examples scattered through the literature are gathered here and originally
presented, emphasizing the deep interplay between the network interconnection
structure and the emergent global behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08577</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08577</id><created>2015-09-28</created><authors><author><keyname>Sun</keyname><forenames>Pengfei</forenames></author><author><keyname>Yuan</keyname><forenames>Fang</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author><author><keyname>Zhu</keyname><forenames>Dalin</forenames></author></authors><title>A Novel Scattered Pilot Design for FBMC/OQAM Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filter bank multi-carrier with offset quadrature amplitude modulation
(FBMC/OQAM) has been heavily studied as an alternative waveform for 5G systems.
Its advantages of higher spectrum efficiency, localized frequency response and
insensitivity to synchronization errors may enable promising performance when
orthogonal frequency division multiplexing (OFDM) fails. However, performing
channel estimation under the intrinsic interference has been a fundamental
obstacle towards adopting FBMC/OQMA in a practical system. Several schemes are
available but the performance is far from satisfaction. In this paper, we will
show the existing methods are trapped by the paradigm that a clean pilot is
mandatory so as to explicitly carry a reference symbol to the receiver for the
purpose of channel estimation. By breaking this paradigm, a novel dual
dependent pilot scheme is proposed, which gives up the independent pilot and
derives dual pilots from the imposed interference. By doing this, the
interference between pilots can be fully utilized. Consequentially, the new
scheme significantly outperforms existing solutions and the simulation results
show FBMC/OQAM can achieve close-to-OFDM performance in a practical system even
with the presence of strong intrinsic interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08581</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08581</id><created>2015-09-28</created><updated>2015-11-29</updated><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author></authors><title>Optimization over Sparse Symmetric Sets via a Nonmonotone Projected
  Gradient Method</title><categories>math.OC cs.LG cs.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing a Lipschitz differentiable function
over a class of sparse symmetric sets that has wide applications in engineering
and science. For this problem, it is known that any accumulation point of the
classical projected gradient (PG) method with a constant stepsize $1/L$
satisfies the $L$-stationarity optimality condition that was introduced in [3].
In this paper we introduce a new optimality condition that is stronger than the
$L$-stationarity optimality condition. We also propose a nonmonotone projected
gradient (NPG) method for this problem by incorporating some support-changing
and coordintate-swapping strategies into a projected gradient method with
variable stepsizes. It is shown that any accumulation point of NPG satisfies
the new optimality condition and moreover it is a coordinatewise stationary
point. Under some suitable assumptions, we further show that it is a global or
a local minimizer of the problem. Numerical experiments are conducted to
compare the performance of PG and NPG. The computational results demonstrate
that NPG has substantially better solution quality than PG, and moreover, it is
at least comparable to, but sometimes can be much faster than PG in terms of
speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08605</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08605</id><created>2015-09-29</created><updated>2016-01-08</updated><authors><author><keyname>Engelmann</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Olderog</keyname><forenames>Ernst-R&#xfc;diger</forenames></author></authors><title>A Sound and Complete Hoare Logic for Dynamically-Typed, Object-Oriented
  Programs -- Extended Version --</title><categories>cs.PL cs.LO</categories><comments>Extended Version -- contains all proofs, proof rules and additional
  information; new version -- elaborated explanations in section 7, added
  reference, minor visual improvements; new version -- incorporated reviews &amp;
  improved formalizations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple dynamically-typed, (purely) object-oriented language is defined. A
structural operational semantics as well as a Hoare-style program logic for
reasoning about programs in the language in multiple notions of correctness are
given. The Hoare logic is proved to be both sound and (relative) complete and
is -- to the best of our knowledge -- the first such logic presented for a
dynamically-typed language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08608</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08608</id><created>2015-09-29</created><authors><author><keyname>Thankachan</keyname><forenames>Sharma V.</forenames></author><author><keyname>Patil</keyname><forenames>Manish</forenames></author><author><keyname>Shah</keyname><forenames>Rahul</forenames></author><author><keyname>Biswas</keyname><forenames>Sudip</forenames></author></authors><title>Probabilistic Threshold Indexing for Uncertain Strings</title><categories>cs.DB cs.DS</categories><comments>14 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strings form a fundamental data type in computer systems. String searching
has been extensively studied since the inception of computer science.
Increasingly many applications have to deal with imprecise strings or strings
with fuzzy information in them. String matching becomes a probabilistic event
when a string contains uncertainty, i.e. each position of the string can have
different probable characters with associated probability of occurrence for
each character. Such uncertain strings are prevalent in various applications
such as biological sequence data, event monitoring and automatic ECG
annotations. We explore the problem of indexing uncertain strings to support
efficient string searching. In this paper we consider two basic problems of
string searching, namely substring searching and string listing. In substring
searching, the task is to find the occurrences of a deterministic string in an
uncertain string. We formulate the string listing problem for uncertain
strings, where the objective is to output all the strings from a collection of
strings, that contain probable occurrence of a deterministic query string.
Indexing solution for both these problems are significantly more challenging
for uncertain strings than for deterministic strings. Given a construction time
probability value $\tau$, our indexes can be constructed in linear space and
supports queries in near optimal time for arbitrary values of probability
threshold parameter greater than $\tau$. To the best of our knowledge, this is
the first indexing solution for searching in uncertain strings that achieves
strong theoretical bound and supports arbitrary values of probability threshold
parameter. We also propose an approximate substring search index that can
answer substring search queries with an additive error in optimal time. We
conduct experiments to evaluate the performance of our indexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08623</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08623</id><created>2015-09-29</created><authors><author><keyname>Drmota</keyname><forenames>Michael</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Spiegelhofer</keyname><forenames>Lukas</forenames></author></authors><title>On a Conjecture of Cusick Concerning the Sum of Digits of n and n + t</title><categories>math.CO cs.SC math.NT</categories><msc-class>11A63, 05A20, 05A16, 11B50, 11B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a nonnegative integer $t$, let $c_t$ be the asymptotic density of natural
numbers $n$ for which $s(n + t) \geq s(n)$, where $s(n)$ denotes the sum of
digits of $n$ in base~$2$. We prove that $c_t &gt; 1/2$ for $t$ in a set of
asymptotic density~$1$, thus giving a partial solution to a conjecture of T. W.
Cusick stating that $c_t &gt; 1/2$ for all t. Interestingly this problem has
several equivalent formulations, for example that the polynomial $X(X +
1)\cdots(X + t - 1)$ has less than $2^t$ zeros modulo $2^{t+1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08627</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08627</id><created>2015-09-29</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Semantics, Representations and Grammars for Deep Learning</title><categories>cs.LG cs.NE stat.ML</categories><comments>20 pages, many diagrams</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning is currently the subject of intensive study. However,
fundamental concepts such as representations are not formally defined --
researchers &quot;know them when they see them&quot; -- and there is no common language
for describing and analyzing algorithms. This essay proposes an abstract
framework that identifies the essential features of current practice and may
provide a foundation for future developments.
  The backbone of almost all deep learning algorithms is backpropagation, which
is simply a gradient computation distributed over a neural network. The main
ingredients of the framework are thus, unsurprisingly: (i) game theory, to
formalize distributed optimization; and (ii) communication protocols, to track
the flow of zeroth and first-order information. The framework allows natural
definitions of semantics (as the meaning encoded in functions), representations
(as functions whose semantics is chosen to optimized a criterion) and grammars
(as communication protocols equipped with first-order convergence guarantees).
  Much of the essay is spent discussing examples taken from the literature. The
ultimate aim is to develop a graphical language for describing the structure of
deep learning algorithms that backgrounds the details of the optimization
procedure and foregrounds how the components interact. Inspiration is taken
from probabilistic graphical models and factor graphs, which capture the
essential structural features of multivariate distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08628</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08628</id><created>2015-09-29</created><authors><author><keyname>Dorn</keyname><forenames>Britta</forenames></author><author><keyname>Kr&#xfc;ger</keyname><forenames>Dominikus</forenames></author><author><keyname>Scharpfenecker</keyname><forenames>Patrick</forenames></author></authors><title>Often harder than in the Constructive Case: Destructive Bribery in
  CP-nets</title><categories>cs.CC</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the destructive bribery problem---an external
agent tries to prevent a disliked candidate from winning by bribery
actions---in voting over combinatorial domains, where the set of candidates is
the Cartesian product of several issues. This problem is related to the concept
of the margin of victory of an election which constitutes a measure of
robustness of the election outcome and plays an important role in the context
of electronic voting. In our setting, voters have conditional preferences over
assignments to these issues, modelled by CP-nets. We settle the complexity of
all combinations of this problem based on distinctions of four voting rules,
five cost schemes, three bribery actions, weighted and unweighted voters, as
well as the negative and the non-negative scenario. We show that almost all of
these cases are NP-complete or NP-hard for weighted votes while approximately
half of the cases can be solved in polynomial time for unweighted votes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08634</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08634</id><created>2015-09-29</created><authors><author><keyname>Osogami</keyname><forenames>Takayuki</forenames></author><author><keyname>Otsuka</keyname><forenames>Makoto</forenames></author></authors><title>Learning dynamic Boltzmann machines with spike-timing dependent
  plasticity</title><categories>cs.NE cs.AI cs.LG stat.ML</categories><comments>Preliminary and substantially different version of the paper appeared
  in http://www.nature.com/articles/srep14149</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a particularly structured Boltzmann machine, which we refer to as
a dynamic Boltzmann machine (DyBM), as a stochastic model of a
multi-dimensional time-series. The DyBM can have infinitely many layers of
units but allows exact and efficient inference and learning when its parameters
have a proposed structure. This proposed structure is motivated by postulates
and observations, from biological neural networks, that the synaptic weight is
strengthened or weakened, depending on the timing of spikes (i.e., spike-timing
dependent plasticity or STDP). We show that the learning rule of updating the
parameters of the DyBM in the direction of maximizing the likelihood of given
time-series can be interpreted as STDP with long term potentiation and long
term depression. The learning rule has a guarantee of convergence and can be
performed in a distributed matter (i.e., local in space) with limited memory
(i.e., local in time).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08639</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08639</id><created>2015-09-29</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Tuned and GPU-accelerated parallel data mining from comparable corpora</title><categories>cs.CL cs.AI cs.DS</categories><comments>Machine translation, comparable corpora, Machine learning, NLP,
  Knowledge-free learning, Unsupervised bi-lingual data mining</comments><journal-ref>Lecture Notes in Artificial Intelligence, p. 32-40, ISBN:
  978-3-319-24032-9, Springer, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multilingual nature of the world makes translation a crucial requirement
today. Parallel dictionaries constructed by humans are a widely-available
resource, but they are limited and do not provide enough coverage for good
quality translation purposes, due to out-of-vocabulary words and neologisms.
This motivates the use of statistical translation systems, which are
unfortunately dependent on the quantity and quality of training data. Such has
a very limited availability especially for some languages and very narrow text
domains. Is this research we present our improvements to Yalign mining
methodology by reimplementing the comparison algorithm, introducing a tuning
scripts and by improving performance using GPU computing acceleration. The
experiments are conducted on various text domains and bi-data is extracted from
the Wikipedia dumps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08643</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08643</id><created>2015-09-29</created><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Active Eavesdropping via Spoofing Relay Attack</title><categories>cs.IT math.IT</categories><comments>submitted for possible conference publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a new active eavesdropping technique via the so-called
spoofing relay attack, which could be launched by the eavesdropper to
significantly enhance the information leakage rate from the source over
conventional passive eaves-dropping. With this attack, the eavesdropper acts as
a relay to spoof the source to vary transmission rate in favor of its
eavesdropping performance by either enhancing or degrading the effective
channel of the legitimate link. The maxi-mum information leakage rate
achievable by the eavesdropper and the corresponding optimal operation at the
spoofing relay are obtained. It is shown that such a spoofing relay attack
could impose new challenges from a physical-layer security perspective since it
leads to significantly higher information leakage rate than conventional
passive eavesdropping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08644</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08644</id><created>2015-09-29</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Neural-based machine translation for medical text domain. Based on
  European Medicines Agency leaflet texts</title><categories>cs.CL cs.CY cs.NE stat.ML</categories><comments>machine translation, statistical machine translation, neural machine
  trasnlation, nlp, text processing, medical communication</comments><journal-ref>Procedia Computer Science, 2015, 64: 2-9</journal-ref><doi>10.1016/j.procs.2015.08.456</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of machine translation is rapidly evolving. Today one can find
several machine translation systems on the web that provide reasonable
translations, although the systems are not perfect. In some specific domains,
the quality may decrease. A recently proposed approach to this domain is neural
machine translation. It aims at building a jointly-tuned single neural network
that maximizes translation performance, a very different approach from
traditional statistical machine translation. Recently proposed neural machine
translation models often belong to the encoder-decoder family in which a source
sentence is encoded into a fixed length vector that is, in turn, decoded to
generate a translation. The present research examines the effects of different
training methods on a Polish-English Machine Translation system used for
medical data. The European Medicines Agency parallel text corpus was used as
the basis for training of neural and statistical network-based translation
systems. The main machine translation evaluation metrics have also been used in
analysis of the systems. A comparison and implementation of a real-time medical
translator is the main focus of our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08647</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08647</id><created>2015-09-29</created><authors><author><keyname>Pereira</keyname><forenames>Eduardo M.</forenames></author><author><keyname>Cardoso</keyname><forenames>Jaime S.</forenames></author><author><keyname>Morla</keyname><forenames>Ricardo</forenames></author></authors><title>Long-Range Trajectories from Global and Local Motion Representations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion is a fundamental cue for scene analysis and human activity understan-
ding in videos. It can be encoded in trajectories for tracking objects and for
action recognition, or in form of flow to address behaviour analysis in crowded
scenes. Each approach can only be applied on limited scenarios. We propose a
motion-based system that represents the spatial and temporal features of the
flow in terms of long-range trajectories. The novelty resides on the system
formulation, its generic approach to handle scene variability and motion
variations, motion integration from local and global representations, and the
resulting long-range trajectories that overcome trajectory-based approach
problems. We report the results and conclusions that state its pertinence on
different scenarios, comparing and correlating the extracted trajectories of
individual pedestrians, manually annotated. We also propose an evaluation
framework and stress the diverse system characteristics that can be used for
human activity tasks, namely on motion segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08654</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08654</id><created>2015-09-29</created><authors><author><keyname>Stolikj</keyname><forenames>Milosh</forenames></author><author><keyname>Meyfroyt</keyname><forenames>Thomas M. M.</forenames></author><author><keyname>Cuijpers</keyname><forenames>Pieter J. L.</forenames></author><author><keyname>Lukkien</keyname><forenames>Johan J.</forenames></author></authors><title>Improving the Performance of Trickle-Based Data Dissemination in
  Low-Power Networks</title><categories>cs.NI</categories><journal-ref>Wireless Sensor Networks, Lecture Notes in Computer Science, vol.
  8965. Springer, 2015, 186-201</journal-ref><doi>10.1007/978-3-319-15582-1_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trickle is a polite gossip algorithm for managing communication traffic. It
is of particular interest in low-power wireless networks for reducing the
amount of control traffic, as in routing protocols (RPL), or reducing network
congestion, as in multicast protocols (MPL). Trickle is used at the network or
application level, and relies on up-to-date information on the activity of
neighbors. This makes it vulnerable to interference from the media access
control layer, which we explore in this paper. We present several scenarios how
the MAC layer in low-power radios violates Trickle timing. As a case study, we
analyze the impact of CSMA/CA with ContikiMAC on Trickle's performance.
Additionally, we propose a solution called Cleansing that resolves these
issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08658</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08658</id><created>2015-09-29</created><updated>2016-01-25</updated><authors><author><keyname>Meng</keyname><forenames>Xiangming</forenames></author><author><keyname>Wu</keyname><forenames>Sheng</forenames></author><author><keyname>Kuang</keyname><forenames>Linling</forenames></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>Concise Derivation of Complex Bayesian Approximate Message Passing via
  Expectation Propagation</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of recovering complex-valued signals
from a set of complex-valued linear measurements. Approximate message passing
(AMP) is one state-of-the-art algorithm to recover real-valued sparse signals.
However, the extension of AMP to complex-valued case is nontrivial and no
detailed and rigorous derivation has been explicitly presented. To fill this
gap, we extend AMP to complex Bayesian approximate message passing (CB-AMP)
using expectation propagation (EP). This novel perspective leads to a concise
derivation of CB-AMP without sophisticated transformations between the complex
domain and the real domain. In addition, we have derived state evolution
equations to predict the reconstruction performance of CB-AMP. Simulation
results are presented to demonstrate the efficiency of CB-AMP and state
evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08660</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08660</id><created>2015-09-29</created><authors><author><keyname>Fernandez-Bes</keyname><forenames>Jesus</forenames></author><author><keyname>Arroyo-Valles</keyname><forenames>Roc&#xed;o</forenames></author><author><keyname>Arenas-Garc&#xed;a</keyname><forenames>Jer&#xf3;nimo</forenames></author><author><keyname>Cid-Sueiro</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>Censoring Diffusion for Harvesting WSNs</title><categories>cs.SY cs.MA math.OC stat.ML</categories><comments>Accepted in 2015 IEEE International Workshop on Computational
  Advances in Multi-Sensor Adaptive Processing (CAMSAP 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze energy-harvesting adaptive diffusion networks for a
distributed estimation problem. In order to wisely manage the available energy
resources, we propose a scheme where a censoring algorithm is jointly applied
over the diffusion strategy. An energy-aware variation of a diffusion algorithm
is used, and a new way of measuring the relevance of the estimates in diffusion
networks is proposed in order to apply a subsequent censoring mechanism.
Simulation results show the potential benefit of integrating censoring schemes
in energy-constrained diffusion networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08664</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08664</id><created>2015-09-29</created><authors><author><keyname>Meyfroyt</keyname><forenames>Thomas M. M.</forenames></author><author><keyname>Stolikj</keyname><forenames>Milosh</forenames></author><author><keyname>Lukkien</keyname><forenames>Johan J.</forenames></author></authors><title>Adaptive Broadcast Suppression for Trickle-Based Protocols</title><categories>cs.NI</categories><journal-ref>Proceedings of the 16th IEEE International Symposium on a World of
  Wireless, Mobile and Multimedia Networks (WoWMoM), 2015, pp.1-9</journal-ref><doi>10.1109/WoWMoM.2015.7158134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-power wireless networks play an important role in the Internet of Things.
Typically, these networks consist of a very large number of lossy and
low-capacity devices, challenging the current state of the art in protocol
design. In this context the Trickle algorithm plays an important role, serving
as the basic mechanism for message dissemination in notable protocols such as
RPL and MPL. While Trickle's broadcast suppression mechanism has been proven to
be efficient, recent work has shown that it is intrinsically unfair in terms of
load distribution and that its performance relies strongly on network topology.
This can lead to increased end-to-end delays (MPL), or creation of sub-optimal
routes (RPL). Furthermore, as highlighted in this work, there is no clear
consensus within the research community about what the proper parameter
settings of the suppression mechanism should be. We propose an extension to the
Trickle algorithm, called adaptive-k, which allows nodes to individually adapt
their suppression mechanism to local node density. Supported by analysis and a
case study with RPL, we show that this extension allows for an easier
configuration of Trickle, making it more robust to network topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08665</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08665</id><created>2015-09-29</created><authors><author><keyname>Meyfroyt</keyname><forenames>Thomas M. M.</forenames></author><author><keyname>Borst</keyname><forenames>Sem C.</forenames></author><author><keyname>Boxma</keyname><forenames>Onno J.</forenames></author><author><keyname>Denteneer</keyname><forenames>Dee</forenames></author></authors><title>On the Scalability and Message Count of Trickle-based Broadcasting
  Schemes</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.6034</comments><msc-class>60J05 60J20 90B18</msc-class><journal-ref>Queueing Systems: Volume 81, Issue 2 (2015), Page 203-230</journal-ref><doi>10.1007/s11134-015-9438-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the use of wireless sensor networks increases, the need for efficient and
reliable broadcasting algorithms grows. Ideally, a broadcasting algorithm
should have the ability to quickly disseminate data, while keeping the number
of transmissions low. In this paper, we analyze the popular Trickle algorithm,
which has been proposed as a suitable communication protocol for code
maintenance and propagation in wireless sensor networks. We show that the
broadcasting process of a network using Trickle can be modeled by a Markov
chain and that this chain falls under a class of Markov chains, closely related
to residual lifetime distributions. It is then shown that this class of Markov
chains admits a stationary distribution of a special form. These results are
used to analyze the Trickle algorithm and its message count. Our results prove
conjectures made in the literature concerning the effect of a listen-only
period. Besides providing a mathematical analysis of the algorithm, we propose
a generalized version of Trickle, with an additional parameter defining the
length of a listen-only period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08667</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08667</id><created>2015-09-29</created><updated>2015-10-29</updated><authors><author><keyname>Singh</keyname><forenames>Pushpendra</forenames></author></authors><title>LINOEP vectors, spiral of Theodorus, and nonlinear time-invariant system
  models of mode decomposition</title><categories>cs.IT cs.SY math.IT math.NA</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general method to obtain a set of Linearly
Independent Non-Orthogonal yet Energy (square of the norm) Preserving (LINOEP)
vectors using iterative filtering operation and we refer it as Filter Mode
Decomposition (FDM). We show that the general energy preserving theorem (EPT),
which is valid for both linearly independent (orthogonal and nonorthogonal) and
linearly dependent set of vectors, proposed by Singh P. et al. is a
generalization of the discrete spiral of Theodorus (or square root spiral or
Einstein spiral or Pythagorean spiral). From the EPT, we obtain the (2D)
discrete spiral of Theodorus and show that the multidimensional discrete
spirals (e.g. a 3D spiral) can be easily generated using a set of
multidimensional energy preserving unit vectors. We also establish that the
recently proposed methods (e.g. Empirical Mode Decomposition (EMD),
Synchrosqueezed Wavelet Transforms (SSWT), Variational Mode Decomposition
(VMD), Eigenvalue Decomposition (EVD), Fourier Decomposition Method (FDM),
etc.), for nonlinear and nonstationary time series analysis, are nonlinear
time-invariant (NTI) system models of filtering. Simulation and numerical
results demonstrate the efficacy of LINOEP vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08671</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08671</id><created>2015-09-29</created><authors><author><keyname>Kazemian</keyname><forenames>Iman</forenames></author><author><keyname>Aref</keyname><forenames>Samin</forenames></author></authors><title>A green perspective on capacitated time-dependent vehicle routing
  problem with time windows</title><categories>math.OC cs.DS</categories><msc-class>90B35, 90C27, 65K05, 90B06, 90-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a novel approach to the vehicle routing problem by
focusing on greenhouse gas emissions and fuel consumption aiming to mitigate
adverse environmental effects of transportation. A time-dependent model with
time windows is developed to incorporate speed and schedule in transportation.
The model considers speed limits for different times of the day in a realistic
delivery context. Due to the complexity of solving the model, a simulated
annealing algorithm is proposed to find solutions with high quality in a timely
manner. Our method can be used in practice to lower fuel consumption and
greenhouse gas emissions while total route cost is also controlled to some
extent. The capability of method is depicted by numerical examples productively
solved within 3.5% to the exact optimal for small and mid-sized problems.
Moreover, comparatively appropriate solutions are obtained for large problems
in averagely one tenth of the exact method restricted computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08690</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08690</id><created>2015-09-29</created><updated>2015-11-27</updated><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>Kempe's Universality Theorem for Rational Space Curves</title><categories>cs.CG cs.RO cs.SC math.AG math.RA</categories><msc-class>70B05, 13F20, 65D17, 68U07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every bounded rational space curve of degree d and circularity
c can be drawn by a linkage with 9/2 d - 6c + 1 revolute joints. Our proof is
based on two ingredients. The first one is the factorization theory of motion
polynomials. The second one is the construction of a motion polynomial of
minimum degree with given orbit. Our proof also gives the explicity
construction of the linkage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08692</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08692</id><created>2015-09-29</created><authors><author><keyname>Yu</keyname><forenames>Chengpu</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author><author><keyname>Kovalsky</keyname><forenames>Shahar</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>Identification of Structured LTI MIMO State-Space Models</title><categories>math.OC cs.SY</categories><comments>Accepted to IEEE Conference on Decision and Control (CDC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of structured state-space model has been intensively
studied for a long time but still has not been adequately addressed. The main
challenge is that the involved estimation problem is a non-convex (or bilinear)
optimization problem. This paper is devoted to developing an identification
method which aims to find the global optimal solution under mild computational
burden. Key to the developed identification algorithm is to transform a
bilinear estimation to a rank constrained optimization problem and further a
difference of convex programming (DCP) problem. The initial condition for the
DCP problem is obtained by solving its convex part of the optimization problem
which happens to be a nuclear norm regularized optimization problem. Since the
nuclear norm regularized optimization is the closest convex form of the
low-rank constrained estimation problem, the obtained initial condition is
always of high quality which provides the DCP problem a good starting point.
The DCP problem is then solved by the sequential convex programming method.
Finally, numerical examples are included to show the effectiveness of the
developed identification algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08700</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08700</id><created>2015-09-29</created><authors><author><keyname>Oulamara</keyname><forenames>Mendes</forenames><affiliation>ENS Paris</affiliation></author><author><keyname>Venet</keyname><forenames>Arnaud</forenames><affiliation>NASA - ARC</affiliation></author></authors><title>Abstract Interpretation with Higher-Dimensional Ellipsoids and Conic
  Extrapolation</title><categories>cs.SY</categories><comments>Proceedings, Part I, Computer Aided Verification 27th International
  Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-21690-4_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inference and the verification of numerical relationships among variables
of a program is one of the main goals of static analysis. In this paper, we
propose an Abstract Interpretation framework based on higher-dimensional
ellipsoids to automatically discover symbolic quadratic invariants within
loops, using loop counters as implicit parameters. In order to obtain
non-trivial invariants, the diameter of the set of values taken by the
numerical variables of the program has to evolve (sub-)linearly during loop
iterations. These invariants are called ellipsoidal cones and can be seen as an
extension of constructs used in the static analysis of digital filters.
Semidefinite programming is used to both compute the numerical results of the
domain operations and provide proofs (witnesses) of their correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08709</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08709</id><created>2015-09-29</created><authors><author><keyname>K&#xf6;hler</keyname><forenames>Ekkehard</forenames></author><author><keyname>Strehler</keyname><forenames>Martin</forenames></author></authors><title>Traffic signal optimization: combining static and dynamic models</title><categories>cs.DM</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we present a cyclically time-expanded network model for
simultaneous optimization of traffic assignment and traffic signal parameters,
in particular offsets, split times, and phase orders. Since travel times are of
great importance for developing realistic solutions for traffic assignment and
traffic signal coordination in urban road networks, we perform an extensive
analysis of the model. We show that a linear time-expanded model can reproduce
realistic travel times especially for use with traffic signals and we verify
this by simulation. Furthermore, we show how exact mathematical programming
techniques can be used for optimizing the control of traffic signals. We
provide computational results for real world instances and demonstrate the
capabilities of the cyclically time-expanded by simulation results obtained
with state-of-the-art traffic simulation tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08715</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08715</id><created>2015-09-29</created><authors><author><keyname>Marazzato</keyname><forenames>Roberto</forenames></author><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>Retinex filtering of foggy images: generation of a bulk set with
  selection and ranking</title><categories>cs.CV</categories><comments>Keywords: GIMP Retinex, GIMP, Image processing, Bulk generation of
  images, Bulk manipulation of images</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are proposing the use of GIMP Retinex, a filter of the GNU
Image Manipulation Program, for enhancing foggy images. This filter involves
adjusting four different parameters to find the output image which has to be
preferred according to some specific purposes. Aiming to obtain a processing,
which is able of choosing automatically the best image from a given set, we are
proposing a method for the generation a bulk set of GIMP Retinex filtered
images and a preliminary approach for selecting and ranking them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08717</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08717</id><created>2015-09-29</created><authors><author><keyname>Alaya</keyname><forenames>Nourh&#xe8;ne</forenames></author><author><keyname>Yahia</keyname><forenames>Sadok Ben</forenames></author><author><keyname>Lamolle</keyname><forenames>Myriam</forenames></author></authors><title>Towards Unveiling the Ontology Key Features Altering Reasoner
  Performances</title><categories>cs.AI cs.IR cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning with ontologies is one of the core fields of research in
Description Logics. A variety of efficient reasoner with highly optimized
algorithms have been developed to allow inference tasks on expressive ontology
languages such as OWL(DL). However, reasoner reported computing times have
exceeded and sometimes fall behind the expected theoretical values. From an
empirical perspective, it is not yet well understood, which particular aspects
in the ontology are reasoner performance degrading factors. In this paper, we
conducted an investigation about state of art works that attempted to portray
potential correlation between reasoner empirical behaviour and particular
ontological features. These works were analysed and then broken down into
categories. Further, we proposed a set of ontology features covering a broad
range of structural and syntactic ontology characteristics. We claim that these
features are good indicators of the ontology hardness level against reasoning
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08731</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08731</id><created>2015-09-29</created><authors><author><keyname>Mohamed</keyname><forenames>Shakir</forenames></author><author><keyname>Rezende</keyname><forenames>Danilo Jimenez</forenames></author></authors><title>Variational Information Maximisation for Intrinsically Motivated
  Reinforcement Learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>Proceedings of the 29th Conference on Neural Information Processing
  Systems (NIPS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mutual information is a core statistical quantity that has applications
in all areas of machine learning, whether this is in training of density models
over multiple data modalities, in maximising the efficiency of noisy
transmission channels, or when learning behaviour policies for exploration by
artificial agents. Most learning algorithms that involve optimisation of the
mutual information rely on the Blahut-Arimoto algorithm --- an enumerative
algorithm with exponential complexity that is not suitable for modern machine
learning applications. This paper provides a new approach for scalable
optimisation of the mutual information by merging techniques from variational
inference and deep learning. We develop our approach by focusing on the problem
of intrinsically-motivated learning, where the mutual information forms the
definition of a well-known internal drive known as empowerment. Using a
variational lower bound on the mutual information, combined with convolutional
networks for handling visual input streams, we develop a stochastic
optimisation algorithm that allows for scalable information maximisation and
empowerment-based reasoning directly from pixels to actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08742</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08742</id><created>2015-09-29</created><updated>2015-10-23</updated><authors><author><keyname>Eswaran</keyname><forenames>K.</forenames></author></authors><title>A non iterative method of separation of points by planes in n dimensions
  and its application</title><categories>cs.CG</categories><comments>36 pages, 12 figures</comments><msc-class>68T99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of N points, we have discovered an algorithm that can separate
these points from one another by n-dimensional planes. Each point is chosen at
random and put into a set S and planes which separate them are determined and
put into S. The algorithm gives a method of choosing points and planes which
separate them, till all the points are separated. A proof is provided with a
worked example.
  The algorithm is non iterative and always halts successfully and the
algorithm strictly follows Shannon's principle of making optimal use of
information as it advances stage by stage. It also has a restart facility and
can take care of new points from where it left off.At some later stage if the
dimension of the data is increased from n to n+r, the algorithm can still
continue from where it left off, after some simple adjustments, and tackle the
new data points which are of a higher dimension. and separate them. The
computational complexity is O(n.N log(N)) + O(n3 log(N)), where N is the given
number of points and n3 is the cube of n - the dimension of space. The
algorithm is made possible because a new concept called Orientation Vector is
used. This vector is a Hamming vector and is associated with each point and has
been so devised that it has all the information necessary to ascertain if two
points are separate or not when among a collection of planes.Its application to
data retrieval problems in very large medical data bases is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08743</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08743</id><created>2015-09-29</created><authors><author><keyname>Sensarma</keyname><forenames>Debajit</forenames></author><author><keyname>Sarma</keyname><forenames>Samar Sen</forenames></author></authors><title>Data Hiding using Graphical Code based Steganography Technique</title><categories>cs.IT cs.MM math.IT</categories><comments>5 pages, 3 figures, 2 tables, International Journal of Engineering
  Trends and Technology (IJETT),Volume 27 Number 3, September 2015</comments><doi>10.14445/22315381/IJETT-V27P225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data hiding has received much attention due to rapid development of internet
and multimedia technologies where security of information is a very important
concern. This is achieved by Steganography, which is the art or science of
hiding data into another data, so that human eyes cannot catch the hidden
information easily. There are many ways to hide information-like inside an
image, text, audio/ video etc. Among them image steganography is a very
attractive research area. The goal is to transmit a data within a modified
image (called stego-image)by minimizing the number of bit flips. In this paper,
a new steganography technique has been proposed using Graphical codes and also
comparison with steganography technique using BCH codes has been studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08745</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08745</id><created>2015-09-29</created><updated>2016-01-07</updated><authors><author><keyname>Souli&#xe9;</keyname><forenames>Guillaume</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Robert</keyname><forenames>Ma&#xeb;lys</forenames></author></authors><title>Compression of Deep Neural Networks on the Fly</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to their state-of-the-art performance, deep neural networks are
increasingly used for object recognition. To achieve these results, they use
millions of parameters to be trained. However, when targeting embedded
applications the size of these models becomes problematic. As a consequence,
their usage on smartphones or other resource limited devices is prohibited. In
this paper we introduce a novel compression method for deep neural networks
that is performed during the learning phase. It consists in adding an extra
regularization term to the cost function of fully-connected layers. We combine
this method with Product Quantization (PQ) of the trained weights for higher
savings in storage consumption. We evaluate our method on two data sets (MNIST
and CIFAR10), on which we achieve significantly larger compression rates than
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08761</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08761</id><created>2015-09-29</created><authors><author><keyname>Borgwardt</keyname><forenames>Stefan</forenames></author><author><keyname>Pe&#xf1;aloza</keyname><forenames>Rafael</forenames></author></authors><title>Reasoning in Infinitely Valued G-IALCQ</title><categories>cs.AI cs.LO</categories><comments>Workshop on Weighted Logics for Artificial Intelligence, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fuzzy Description Logics (FDLs) are logic-based formalisms used to represent
and reason with vague or imprecise knowledge. It has been recently shown that
reasoning in most FDLs using truth values from the interval [0,1] becomes
undecidable in the presence of a negation constructor and general concept
inclusion axioms. One exception to this negative result are FDLs whose
semantics is based on the infinitely valued G\&quot;odel t-norm (G). In this paper,
we extend previous decidability results for G-IALC to deal also with qualified
number restrictions. Our novel approach is based on a combination of the known
crispification technique for finitely valued FDLs and the automata-based
procedure originally developed for reasoning in G-IALC. The proposed approach
combines the advantages of these two methods, while removing their respective
drawbacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08763</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08763</id><created>2015-09-21</created><authors><author><keyname>Homri</keyname><forenames>Lazhar</forenames><affiliation>I2M</affiliation></author><author><keyname>Teissandier</keyname><forenames>Denis</forenames><affiliation>I2M</affiliation></author><author><keyname>Ballu</keyname><forenames>Alex</forenames><affiliation>I2M</affiliation></author></authors><title>Tolerance Analysis by Polytopes</title><categories>cs.CG math.MG</categories><proxy>ccsd</proxy><journal-ref>Computer-Aided Design, Elsevier, 2015, 62 (C), pp.112--130.
  \&amp;lt;10.1016/j.cad.2014.11.005\&amp;gt;</journal-ref><doi>10.1016/j.cad.2014.11.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To determine the relative position of any two surfaces in a system, one
approach is to useoperations (Minkowski sum and intersection) on sets of
constraints. These constraints aremade compliant with half-spaces of R^n where
each set of half-spaces defines an operandpolyhedron. These operands are
generally unbounded due to the inclusion of degrees ofinvariance for surfaces
and degrees of freedom for joints defining theoretically
unlimiteddisplacements. To solve operations on operands, Minkowski sums in
particular, &quot;cap&quot; halfspacesare added to each polyhedron to make it compliant
with a polytope which is bydefinition a bounded polyhedron. The difficulty of
this method lies in controlling the influenceof these additional half-spaces on
the topology of polytopes calculated by sum or intersection.This is necessary
to validate the geometric tolerances that ensure the compliance of amechanical
system in terms of functional requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08764</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08764</id><created>2015-09-29</created><authors><author><keyname>Ha</keyname><forenames>Quang Minh</forenames></author><author><keyname>Deville</keyname><forenames>Yves</forenames></author><author><keyname>Pham</keyname><forenames>Quang Dung</forenames></author><author><keyname>H&#xe0;</keyname><forenames>Minh Ho&#xe0;ng</forenames></author></authors><title>Heuristic methods for the Traveling Salesman Problem with Drone</title><categories>cs.AI</categories><comments>13 pages, 9 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Once known to be used exclusively in military domain, unmanned aerial
vehicles (UAV) have stepped up to become a part of new logistic method in
commercial sector called &quot;last-mile delivery&quot;. In this novel approach, small
UAVs, also known as drones, are deployed in tandem with the trucks to deliver
goods to customers. Under research context, it gives rise to a new variant of
the traveling salesman problem (TSP), of which we call TSP with drone (TSP-D).
In this paper, we propose two heuristics: route first - cluster second, and
cluster first - route second, to solve the problem efficiently. A new mixed
integer programming formulation is also introduced to handle the cluster step
in both heuristics. We conduct an experiment, adapting different profit
functions of the MIP model, in both heuristics, to many instances with
different sizes and characteristics. The numerical analysis shows not only a
significant savings compare to truck-only delivery but also a superior
performance against the previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08773</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08773</id><created>2015-09-28</created><authors><author><keyname>Sharma</keyname><forenames>Rohan</forenames></author><author><keyname>Adhikari</keyname><forenames>Bibhas</forenames></author></authors><title>Self-Coordinated Corona Graphs: a model for complex networks</title><categories>cs.DM cs.SI math.CO</categories><comments>21 pages, 31 figures</comments><msc-class>62-09</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, real world networks having constant/shrinking diameter along with
power-law degree distribution are observed and investigated in literature.
Taking an inspiration from these findings, we propose a deterministic complex
network model, which we call Self-Coordinated Corona Graphs (SCCG), based on
the corona product of graphs. As it has also been established that self
coordination/organization of nodes gives rise to emergence of power law in
degree distributions of several real networks, the networks in the proposed
model are generated by the virtue of self coordination of nodes in corona
graphs. Alike real networks, the SCCG inherit motifs which act as the seed
graphs for the generation of SCCG. We also analytically prove that the power
law exponent of SCCG is approximately $2$ and the diameter of SCCG produced by
a class of motifs is constant. Finally, we compare different properties of the
proposed model with that of the BA and Pseudofractal scale-free models for
complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08778</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08778</id><created>2015-09-29</created><authors><author><keyname>Dias</keyname><forenames>Gabriel Martins</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author></authors><title>Reducing the energy consumption in WSNs: A data scientific mechanism</title><categories>cs.NI</categories><comments>27 pages, 7 figures, submitted to the &quot;Pervasive and Mobile
  Computing&quot; journal</comments><msc-class>62P30</msc-class><acm-class>C.2.4; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio transmissions are the operations with the highest energy consumption in
wireless sensor nodes. Therefore, the most promising way to extend Wireless
Sensor Network (WSN) lifetime is to reduce the number of packet transmissions,
if this does not mean to lose the quality of the information that they can
offer. In this work, we present a WSN model and design a mechanism for data
reduction. Then, based on a data study, we show how effective can be its use to
reduce the number of transmissions in WSNs and increase their lifetime.
Simulation results show that the energy consumption can be reduced by almost
85% in the sensor nodes with the highest work load, and we detail the impact of
predicting and aggregating the transmissions in the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08789</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08789</id><created>2015-09-28</created><authors><author><keyname>Nagler</keyname><forenames>Robert</forenames></author><author><keyname>Bruhwiler</keyname><forenames>David</forenames></author><author><keyname>Moeller</keyname><forenames>Paul</forenames></author><author><keyname>Webb</keyname><forenames>Stephen</forenames></author></authors><title>Sustainability and Reproducibility via Containerized Computing</title><categories>cs.SE</categories><comments>2 pages</comments><acm-class>D.2.m; D.2.12; K.6.3; K.6.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent developments in the commercial open source community have catalysed
the use of Linux containers for scalable deployment of web-based applications
to the cloud. Scientific software can be containerized with dependencies,
configuration files, post-processing tools and even simulation results,
referred to as containerized computing. This new approach promises to
significantly improve sustainability, productivity and reproducibility. We
present our experiences, technology, and future plans for open source
containerization of software used to model particle and radiation beams.
Vagrant is central to our approach, using Docker for cloud deployment and
VirtualBox virtual machines for deployment to Mac OS and Windows computers. Our
technology enables seamless switching between the desktop and the cloud to
simplify simulation development and execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08790</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08790</id><created>2015-09-25</created><authors><author><keyname>Mallenahalli</keyname><forenames>Naresh Kumar</forenames></author></authors><title>Hybrid architecture for satellite data processing workflow management</title><categories>cs.SE</categories><comments>5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The ever growing demand for remote sensing data products by user community
has resulted in many Indian and foreign remote sensing satellites being
launched. The diversity in the remote sensing sensors has resulted in
heterogeneous software and hardware environments for generating geospatial data
products. The workflow automation software knows as information management
system is in place at National Remote Sensing Centre (NRSC) catering to the
needs of the data processing and data dissemination. The software components of
workflow are interfaced in different heterogeneous environments that get
executed at data processing software in automated and semi automated modes. For
every new satellite being launched, the software is modified or upgraded if new
business processes are introduced. In this study, we propose a software
architecture that gives more flexible automation with very less manageable
code. The study also addresses utilization and extraction of useful information
from historic production and customer details. A comparison of the current
workflow software architecture with existing practices in industry like Service
Oriented Architecture (SOA), Extensible Markup Languages (XML), and Event based
architectures has been made. A new hybrid approach based on the industry
practices is proposed to improve the existing workflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08792</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08792</id><created>2015-09-27</created><authors><author><keyname>Consoli</keyname><forenames>Sergio</forenames></author><author><keyname>P&#xe8;rez</keyname><forenames>Jos&#xe8; Andr&#xe8;s Moreno</forenames></author></authors><title>An intelligent extension of Variable Neighbourhood Search for labelling
  graph problems</title><categories>cs.AI</categories><comments>MIC 2015: The XI Metaheuristics International Conference, 3 pages,
  Agadir, June 7-10, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe an extension of the Variable Neighbourhood Search
(VNS) which integrates the basic VNS with other complementary approaches from
machine learning, statistics and experimental algorithmic, in order to produce
high-quality performance and to completely automate the resulting optimization
strategy. The resulting intelligent VNS has been successfully applied to a
couple of optimization problems where the solution space consists of the
subsets of a finite reference set. These problems are the labelled spanning
tree and forest problems that are formulated on an undirected labelled graph; a
graph where each edge has a label in a finite set of labels L. The problems
consist on selecting the subset of labels such that the subgraph generated by
these labels has an optimal spanning tree or forest, respectively. These
problems have several applications in the real-world, where one aims to ensure
connectivity by means of homogeneous connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08795</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08795</id><created>2015-09-28</created><authors><author><keyname>Olsen</keyname><forenames>Anders Hval</forenames></author></authors><title>The Evolution of eSports: An Analysis of its origin and a look at its
  prospective future growth as enhanced by Information Technology Management
  tools</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the last years have shown a massive growth within the field of electronic
sports (eSports), several questions emerge, such as how much is it growing, and
will it continue to grow? This research thesis sees this as its statement of
problem, and further aims to define and measure the main factors that caused
the growth of eSports. To further enhance the growth, the benefits and
disbenefits of implementing Information Technology Management tools is
appraised, which additionally gives an understanding of the future of eSports.
  Through observation of the data collection, the themes games played, games
genre, and single-player or team-player was deemed most important. A
statistical analysis was additionally done on the numerical data, which showed
significant correlation between the themes prize pool, participants and
participating countries towards live audience. This indicates that the
mentioned themes affect how many people attends live eSport event. The analysis
also consists of a trend analysis, which presents that nearly every theme
analysed will continue to grow in a five-year period. The trend analysis
further expects a 305.51% growth in the live audience from 2014-2019, while the
virtual streaming audience is expected to experience a growth of 140.61%. To
enhance this growth, the research thesis recommends using the presented trends
and patterns found in the short-term, to conduct successful eSport events.
However, in the long-term an implementation of a Data-driven Decision Support
System can be done, which will periodically gather data to know the best
possible way to deliver eSport events to the eSport community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08798</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08798</id><created>2015-09-26</created><updated>2015-09-30</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Wagner</keyname><forenames>Caroline</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Replicability and the public/private divide</title><categories>cs.DL</categories><comments>Letter to the Editor; accepted for publication in the Journal of the
  Association for Information Science and Technology (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent letter, Carlos Vilchez-Roman criticizes Bornmann et al. (2015)
for using data which cannot be reproduced without access to an in-house version
of the Web-of-Science (WoS) at the Max Planck Digital Libraries (MPDL, Munich).
We agree with the norm of replicability and therefore returned to our data. Is
the problem only a practical one of automation or does the in-house processing
add analytical value to the data? Is the newly emerging situation in any sense
different from a further professionalization of the field? In our opinion, a
political economy of science indicators has in the meantime emerged with a
competitive dynamic that affects the intellectual organization of the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08806</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08806</id><created>2015-09-28</created><authors><author><keyname>Pajouhi</keyname><forenames>Zoha</forenames></author><author><keyname>Fong</keyname><forenames>Xuanyao</forenames></author><author><keyname>Raghunathan</keyname><forenames>Anand</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Yield, Area and Energy Optimization in Stt-MRAMs using failure aware ECC</title><categories>cs.OH</categories><comments>This paper was submitted to ACM JETC journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spin Transfer Torque MRAMs are attractive due to their non-volatility, high
density and zero leakage. However, STT-MRAMs suffer from poor reliability due
to shared read and write paths. Additionally, conflicting requirements for data
retention and write-ability (both related to the energy barrier height of the
magnet) makes design more challenging. Furthermore, the energy barrier height
depends on the physical dimensions of the free layer. Any variations in the
dimensions of the free layer lead to variations in the energy barrier height.
In order to address poor reliability of STT-MRAMs, usage of Error Correcting
Codes (ECC) have been proposed. Unlike traditional CMOS memory technologies,
ECC is expected to correct both soft and hard errors in STT_MRAMs. To achieve
acceptable yield with low write power, stronger ECC is required, resulting in
increased number of encoded bits and degraded memory efficiency. In this paper,
we propose Failure aware ECC (FaECC), which masks permanent faults while
maintaining the same correction capability for soft errors without increased
encoded bits. Furthermore, we investigate the impact of process variations on
run-time reliability of STT-MRAMs. We provide an analysis on the impact of
process variations on the life-time of the free layer and retention failures.
In order to analyze the effectiveness of our methodology, we developed a
cross-layer simulation framework that consists of device, circuit and array
level analysis of STT-MRAM memory arrays. Our results show that using FaECC
relaxes the requirements on the energy barrier height, which reduces the write
energy and results in smaller access transistor size and memory array area.
Keywords: STT-MRAM, reliability, Error Correcting Codes, ECC, magnetic memory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08807</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08807</id><created>2015-09-29</created><authors><author><keyname>Aravind</keyname><forenames>N. R.</forenames></author><author><keyname>Sandeep</keyname><forenames>R. B.</forenames></author><author><keyname>Sivadasan</keyname><forenames>Naveen</forenames></author></authors><title>Parameterized Lower Bounds and Dichotomy Results for the NP-completeness
  of $H$-free Edge Modification Problems</title><categories>cs.DS</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1507.06341</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $H$, the $H$-free Edge Deletion problem asks whether there exist
at most $k$ edges whose deletion from the input graph $G$ results in a graph
without any induced copy of $H$. $H$-free Edge Completion and $H$-free Edge
Editing are defined similarly where only completion (addition) of edges are
allowed in the former and both completion and deletion are allowed in the
latter. We completely settle the classical complexities of these problems by
proving that $H$-free Edge Deletion is NP-complete if and only if $H$ is a
graph with at least two edges, $H$-free Edge Completion is NP-complete if and
only if $H$ is a graph with at least two non-edges and $H$-free Edge Editing is
NP-complete if and only if $H$ is a graph with at least three vertices.
Additionally, we prove that, these NP-complete problems cannot be solved in
parameterized subexponential time, i.e., in time $2^{o(k)}\cdot |G|^{O(1)}$,
unless Exponential Time Hypothesis fails. Furthermore, we obtain implications
on the incompressibility of these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08825</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08825</id><created>2015-09-29</created><authors><author><keyname>Huang</keyname><forenames>Xiang</forenames></author><author><keyname>Stull</keyname><forenames>D. M.</forenames></author></authors><title>Polynomial Space Randomness in Analysis with Application to the Lebesgue
  Differentiation Theorem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the interaction between polynomial space randomness and a
fundamental result of analysis, the Lebesgue differentiation theorem. We
generalize Ko's framework for polynomial space computability in $\mathbb{R}^n$
to define \textit{weakly pspace-random} points, a new variant of polynomial
space randomness. We show that the Lebesgue differentiation theorem holds for
every weakly pspace-random point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08830</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08830</id><created>2015-09-29</created><authors><author><keyname>Schlesinger</keyname><forenames>Michail</forenames></author><author><keyname>Vodolazskiy</keyname><forenames>Evgeniy</forenames></author></authors><title>How to Formulate and Solve Statistical Recognition and Learning Problems</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate problems of statistical recognition and learning in a common
framework of complex hypothesis testing. Based on arguments from multi-criteria
optimization, we identify strategies that are improper for solving these
problems and derive a common form of the remaining strategies. We show that
some widely used approaches to recognition and learning are improper in this
sense. We then propose a generalized formulation of the recognition and
learning problem which embraces the whole range of sizes of the learning
sample, including the zero size. Learning becomes a special case of recognition
without learning. We define the concept of closest to optimal strategy, being a
solution to the formulated problem, and describe a technique for finding such a
strategy. On several illustrative cases, the strategy is shown to be superior
to the widely used learning methods based on maximal likelihood estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08834</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08834</id><created>2015-09-29</created><authors><author><keyname>Phan</keyname><forenames>Ly</forenames></author><author><keyname>Rugonyi</keyname><forenames>Sandra</forenames></author><author><keyname>Grimm</keyname><forenames>Cindy</forenames></author></authors><title>Visualization techniques for the developing chicken heart</title><categories>cs.GR q-bio.TO</categories><comments>Longer version of conference paper published in 11th International
  Symposium on Visual Computing (December 2015)</comments><acm-class>I.3.5, J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a geometric surface parameterization algorithm and several
visualization techniques adapted to the problem of understanding the 4D
peristaltic-like motion of the outflow tract (OFT) in an embryonic chick heart.
We illustrated the techniques using data from hearts under normal conditions
(four embryos), and hearts in which blood flow conditions are altered through
OFT banding (four embryos). The overall goal is to create quantitative measures
of the temporal heart-shape change both within a single subject and between
multiple subjects. These measures will help elucidate how altering hemodynamic
conditions changes the shape and motion of the OFT walls, which in turn
influence the stresses and strains on the developing heart, causing it to
develop differently. We take advantage of the tubular shape and periodic motion
of the OFT to produce successively lower dimensional visualizations of the
cardiac motion (e.g. curvature, volume, and cross-section) over time, and
quantifications of such visualizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08836</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08836</id><created>2015-09-29</created><authors><author><keyname>Buchali</keyname><forenames>Fred</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Idler</keyname><forenames>Wilfried</forenames></author><author><keyname>Schmalen</keyname><forenames>Laurent</forenames></author><author><keyname>Schulte</keyname><forenames>Patrick</forenames></author><author><keyname>Steiner</keyname><forenames>Fabian</forenames></author></authors><title>Experimental Demonstration of Capacity Increase and Rate-Adaptation by
  Probabilistically Shaped 64-QAM</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation as postdeadline paper at ECOC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We implemented a flexible transmission system operating at adjustable data
rate and fixed bandwidth, baudrate, constellation and overhead using
probabilistic shaping. We demonstrated in a transmission experiment up to 15%
capacity and 43% reach increase versus 200 Gbit/s 16-QAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08842</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08842</id><created>2015-09-29</created><authors><author><keyname>Shaw</keyname><forenames>Ryan</forenames></author></authors><title>Automatically Segmenting Oral History Transcripts</title><categories>cs.CL</categories><comments>13 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dividing oral histories into topically coherent segments can make them more
accessible online. People regularly make judgments about where coherent
segments can be extracted from oral histories. But making these judgments can
be taxing, so automated assistance is potentially attractive to speed the task
of extracting segments from open-ended interviews. When different people are
asked to extract coherent segments from the same oral histories, they often do
not agree about precisely where such segments begin and end. This low agreement
makes the evaluation of algorithmic segmenters challenging, but there is reason
to believe that for segmenting oral history transcripts, some approaches are
more promising than others. The BayesSeg algorithm performs slightly better
than TextTiling, while TextTiling does not perform significantly better than a
uniform segmentation. BayesSeg might be used to suggest boundaries to someone
segmenting oral histories, but this segmentation task needs to be better
defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08844</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08844</id><created>2015-09-29</created><updated>2015-11-13</updated><authors><author><keyname>Pirzadeh</keyname><forenames>Hessam</forenames></author><author><keyname>Razavizadeh</keyname><forenames>S. Mohammad</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author></authors><title>Subverting Massive MIMO by Smart Jamming</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications Letters, 4 pages, 3 figures</comments><doi>10.1109/LWC.2015.2487960 10.1109/LWC.2015.2487960</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider uplink transmission of a massive multi-user multiple-input
multiple-output (MU-MIMO) system in the presence of a smart jammer. The jammer
aims to degrade the sum spectral efficiency of the legitimate system by
attacking both the training and data transmission phases. First, we derive a
closed-form expression for the sum spectral efficiency by taking into account
the presence of a smart jammer. Then, we determine how a jammer with a given
energy budget should attack the training and data transmission phases to induce
the maximum loss to the sum spectral efficiency. Numerical results illustrate
the impact of optimal jamming specifically in the large limit of the number of
base station (BS) antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08847</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08847</id><created>2015-09-29</created><authors><author><keyname>Monshizadeh</keyname><forenames>Pooya</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Monshizadeh</keyname><forenames>Nima</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan J.</forenames></author></authors><title>A Communication-Free Master-Slave Microgrid with Power Sharing</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a design of a master-slave microgrid consisting of
grid-supporting current source inverters and a synchronous generator is
proposed. The inverters are following the frequency of the grid imposed by the
synchronous generator. Hence, the proposed structure of the microgrid is
steadily synchronized. We show that the method achieves power sharing without
the need of communication. Furthermore, no change in operation mode is needed
during transitions of the microgrid between islanded and grid-connected modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08855</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08855</id><created>2015-09-29</created><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Sharma</keyname><forenames>Shantanu</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan R.</forenames></author></authors><title>Computing Marginals Using MapReduce</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the data-cube marginals of a fixed order
$k$ (i.e., all marginals that aggregate over $k$ dimensions), using a single
round of MapReduce. The focus is on the relationship between the reducer size
(number of inputs allowed at a single reducer) and the replication rate (number
of reducers to which an input is sent). We show that the replication rate is
minimized when the reducers receive all the inputs necessary to compute one
marginal of higher order. That observation lets us view the problem as one of
covering sets of $k$ dimensions with sets of a larger size $m$, a problem that
has been studied under the name &quot;covering numbers.&quot; We offer a number of
constructions that, for different values of $k$ and $m$ meet or come close to
yielding the minimum possible replication rate for a given reducer size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08863</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08863</id><created>2015-09-29</created><authors><author><keyname>Tremblay</keyname><forenames>Nicolas</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author><author><keyname>Gribonval</keyname><forenames>Remi</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Accelerated Spectral Clustering Using Graph Filtering Of Random Signals</title><categories>cs.SI cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build upon recent advances in graph signal processing to propose a faster
spectral clustering algorithm. Indeed, classical spectral clustering is based
on the computation of the first k eigenvectors of the similarity matrix'
Laplacian, whose computation cost, even for sparse matrices, becomes
prohibitive for large datasets. We show that we can estimate the spectral
clustering distance matrix without computing these eigenvectors: by graph
filtering random signals. Also, we take advantage of the stochasticity of these
random vectors to estimate the number of clusters k. We compare our method to
classical spectral clustering on synthetic data, and show that it reaches equal
performance while being faster by a factor at least two for large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08874</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08874</id><created>2015-09-29</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Polish - English Speech Statistical Machine Translation Systems for the
  IWSLT 2014</title><categories>cs.CL</categories><comments>Machine Translation, West slavic, Proceedings of the 11th
  International Workshop on Spoken Language Translation, Tahoe Lake, USA, 2014.
  arXiv admin note: text overlap with arXiv:1409.0473 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research explores effects of various training settings between Polish
and English Statistical Machine Translation systems for spoken language.
Various elements of the TED parallel text corpora for the IWSLT 2014 evaluation
campaign were used as the basis for training of language models, and for
development, tuning and testing of the translation system as well as Wikipedia
based comparable corpora prepared by us. The BLEU, NIST, METEOR and TER metrics
were used to evaluate the effects of data preparations on translation results.
Our experiments included systems, which use lemma and morphological information
on Polish words. We also conducted a deep analysis of provided Polish data as
preparatory work for the automatic data correction and cleaning phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08880</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08880</id><created>2015-09-29</created><updated>2015-11-25</updated><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author><author><keyname>Storcheus</keyname><forenames>Dmitry</forenames></author></authors><title>Foundations of Coupled Nonlinear Dimensionality Reduction</title><categories>stat.ML cs.LG</categories><comments>12 pages, 3 figures, authors in alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and analyze the learning scenario of \emph{coupled
nonlinear dimensionality reduction}, which combines two major steps of machine
learning pipeline: projection onto a manifold and subsequent supervised
learning. First, we present new generalization bounds for this scenario and,
second, we introduce an algorithm that follows from these bounds. The
generalization error bound is based on a careful analysis of the empirical
Rademacher complexity of the relevant hypothesis set. In particular, we show an
upper bound on the Rademacher complexity that is in $\widetilde
O(\sqrt{\Lambda_{(r)}/m})$, where $m$ is the sample size and $\Lambda_{(r)}$
the upper bound on the Ky-Fan $r$-norm of the associated kernel matrix. We give
both upper and lower bound guarantees in terms of that Ky-Fan $r$-norm, which
strongly justifies the definition of our hypothesis set. To the best of our
knowledge, these are the first learning guarantees for the problem of coupled
dimensionality reduction. Our analysis and learning guarantees further apply to
several special cases, such as that of using a fixed kernel with supervised
dimensionality reduction or that of unsupervised learning of a kernel for
dimensionality reduction followed by a supervised learning algorithm. Based on
theoretical analysis, we suggest a structural risk minimization algorithm
consisting of the coupled fitting of a low dimensional manifold and a
separation function on that manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08881</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08881</id><created>2015-09-29</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Building Subject-aligned Comparable Corpora and Mining it for Truly
  Parallel Sentence Pairs</title><categories>cs.CL cs.IR stat.ML</categories><journal-ref>Procedia Technology, 18, Elsevier, p.126-132, 2014</journal-ref><doi>10.1016/j.protcy.2014.11.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel sentences are a relatively scarce but extremely useful resource for
many applications including cross-lingual retrieval and statistical machine
translation. This research explores our methodology for mining such data from
previously obtained comparable corpora. The task is highly practical since
non-parallel multilingual data exist in far greater quantities than parallel
corpora, but parallel sentences are a much more useful resource. Here we
propose a web crawling method for building subject-aligned comparable corpora
from Wikipedia articles. We also introduce a method for extracting truly
parallel sentences that are filtered out from noisy or just comparable sentence
pairs. We describe our implementation of a specialized tool for this task as
well as training and adaption of a machine translation system that supplies our
filter with additional information about the similarity of comparable sentence
pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08885</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08885</id><created>2015-09-29</created><authors><author><keyname>Radtke</keyname><forenames>Paul K.</forenames></author><author><keyname>Schimansky-Geier</keyname><forenames>Lutz</forenames></author></authors><title>A Nonlinear HP-Type Complementary Resistive Switch</title><categories>cond-mat.mes-hall cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resistive Switching (RS) is the change in resistance of a dielectric under
the influence of an external current or electric field. This change is
non-volatile, and the basis of both the memristor and resistive random access
memory. In the latter, high integration densities favor the anti-serial
combination of two RS-elements to a single cell, termed the complementary
resistive switch (CRS). Motivated by the irregular shape of the filament
protruding into the device, we suggest a nonlinearity in the
resistance-interpolation function, and thereby expand the original
HP-memristor. We numerically simulate and analytically solve this model.
Further, the nonlinearity allows for its application to the CRS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08888</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08888</id><created>2015-09-29</created><authors><author><keyname>Hassanzadeh</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Phan</keyname><forenames>John H.</forenames></author><author><keyname>Wang</keyname><forenames>May D.</forenames></author></authors><title>A Semi-Supervised Method for Predicting Cancer Survival Using Incomplete
  Clinical Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction of survival for cancer patients is an open area of research.
However, many of these studies focus on datasets with a large number of
patients. We present a novel method that is specifically designed to address
the challenge of data scarcity, which is often the case for cancer datasets.
Our method is able to use unlabeled data to improve classification by adopting
a semi-supervised training approach to learn an ensemble classifier. The
results of applying our method to three cancer datasets show the promise of
semi-supervised learning for prediction of cancer survival.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08891</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08891</id><created>2015-09-23</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author></authors><title>The Computational Principles of Learning Ability</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It has been quite a long time since AI researchers in the field of computer
science stop talking about simulating human intelligence or trying to explain
how brain works. Recently, represented by deep learning techniques, the field
of machine learning is experiencing unprecedented prosperity and some
applications with near human-level performance bring researchers confidence to
imply that their approaches are the promising candidate for understanding the
mechanism of human brain. However apart from several ancient philological
criteria and some imaginary black box tests (Turing test, Chinese room) there
is no computational level explanation, definition or criteria about
intelligence or any of its components. Base on the common sense that learning
ability is one critical component of intelligence and inspect from the
viewpoint of mapping relations, this paper presents two laws which explains
what is the &quot;learning ability&quot; as we familiar with and under what conditions a
mapping relation can be acknowledged as &quot;Learning Model&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08892</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08892</id><created>2015-09-29</created><authors><author><keyname>Jiang</keyname><forenames>Xin</forenames></author><author><keyname>Reynaud-Bouret</keyname><forenames>Patricia</forenames></author><author><keyname>Rivoirard</keyname><forenames>Vincent</forenames></author><author><keyname>Sansonnet</keyname><forenames>Laure</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>A data-dependent weighted LASSO under Poisson noise</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>23 pages (49 pages with appendix), 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse linear inverse problems appear in a variety of settings, but often the
noise contaminating observations cannot accurately be described as bounded by
or arising from a Gaussian distribution. Poisson observations in particular are
a characteristic feature of several real-world applications. Previous work on
sparse Poisson inverse problems encountered several limiting technical hurdles.
This paper describes a novel alternative analysis approach for sparse Poisson
inverse problems that (a) sidesteps the technical challenges present in
previous work, (b) admits estimators that can readily be computed using
off-the-shelf LASSO algorithms, and (c) hints at a general weighted LASSO
framework for broad classes of problems. At the heart of this new approach lies
a weighted LASSO estimator for which data-dependent weights are based on
Poisson concentration inequalities. Unlike previous analyses of the weighted
LASSO, the proposed analysis depends on conditions which can be checked or
shown to hold in general settings with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08896</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08896</id><created>2015-09-29</created><updated>2015-11-12</updated><authors><author><keyname>Lee</keyname><forenames>Holden</forenames></author></authors><title>Quadratic polynomials of small modulus cannot represent OR</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An open problem in complexity theory is to find the minimal degree of a
polynomial representing the $n$-bit OR function modulo composite $m$. This
problem is related to understanding the power of circuits with $\text{MOD}_m$
gates where $m$ is composite. The OR function is of particular interest because
it is the simplest function not amenable to bounds from communication
complexity. Tardos and Barrington established a lower bound of $\Omega((\log
n)^{O_m(1)})$, and Barrington, Beigel, and Rudich established an upper bound of
$n^{O_m(1)}$. No progress has been made on closing this gap for twenty years,
and progress will likely require new techniques.
  We make progress on this question viewed from a different perspective: rather
than fixing the modulus $m$ and bounding the minimum degree $d$ in terms of the
number of variables $n$, we fix the degree $d$ and bound $n$ in terms of the
modulus $m$. For degree $d=2$, we prove a quasipolynomial bound of $n\le
m^{O(d)}\le m^{O(\log m)}$, improving the previous best bound of $2^{O(m)}$
implied by Tardos and Barrington's general bound.
  To understand the computational power of quadratic polynomials modulo $m$, we
introduce a certain dichotomy which may be of independent interest. Namely, we
define a notion of boolean rank of a quadratic polynomial $f$ and relate it to
the notion of diagonal rigidity. Using additive combinatorics, we show that
when the rank is low, $f(\mathbf x)=0$ must have many solutions. Using
techniques from exponential sums, we show that when the rank of $f$ is high,
$f$ is close to equidistributed. In either case, $f$ cannot represent the OR
function in many variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08902</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08902</id><created>2015-09-29</created><authors><author><keyname>Sharma</keyname><forenames>Gaurav</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Scalable Nonlinear Embeddings for Semantic Category-based Image
  Retrieval</title><categories>cs.CV</categories><comments>ICCV 2015 preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel algorithm for the task of supervised discriminative
distance learning by nonlinearly embedding vectors into a low dimensional
Euclidean space. We work in the challenging setting where supervision is with
constraints on similar and dissimilar pairs while training. The proposed method
is derived by an approximate kernelization of a linear Mahalanobis-like
distance metric learning algorithm and can also be seen as a kernel neural
network. The number of model parameters and test time evaluation complexity of
the proposed method are O(dD) where D is the dimensionality of the input
features and d is the dimension of the projection space - this is in contrast
to the usual kernelization methods as, unlike them, the complexity does not
scale linearly with the number of training examples. We propose a stochastic
gradient based learning algorithm which makes the method scalable (w.r.t. the
number of training examples), while being nonlinear. We train the method with
up to half a million training pairs of 4096 dimensional CNN features. We give
empirical comparisons with relevant baselines on seven challenging datasets for
the task of low dimensional semantic category based image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08909</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08909</id><created>2015-09-29</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Polish -English Statistical Machine Translation of Medical Texts</title><categories>cs.CL cs.IR stat.ML</categories><comments>New Research in Multimedia and Internet Systems, Springer. 09/2014,
  ISSN: 1867-5662. arXiv admin note: text overlap with arXiv:1509.08874</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This new research explores the effects of various training methods on a
Polish to English Statistical Machine Translation system for medical texts.
Various elements of the EMEA parallel text corpora from the OPUS project were
used as the basis for training of phrase tables and language models and for
development, tuning and testing of the translation system. The BLEU, NIST,
METEOR, RIBES and TER metrics have been used to evaluate the effects of various
system and data preparations on translation results. Our experiments included
systems that used POS tagging, factored phrase models, hierarchical models,
syntactic taggers, and many different alignment methods. We also conducted a
deep analysis of Polish data as preparatory work for automatic data correction
such as true casing and punctuation normalization phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08932</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08932</id><created>2015-09-29</created><updated>2015-10-19</updated><authors><author><keyname>Chow</keyname><forenames>Yinlam</forenames></author><author><keyname>Yu</keyname><forenames>Jia Yuan</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Two Phase $Q-$learning for Bidding-based Vehicle Sharing</title><categories>cs.AI math.OC</categories><comments>Submitted to AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider one-way vehicle sharing systems where customers can rent a car at
one station and drop it off at another. The problem we address is to optimize
the distribution of cars, and quality of service, by pricing rentals
appropriately. We propose a bidding approach that is inspired from auctions and
takes into account the significant uncertainty inherent in the problem data
(e.g., pick-up and drop-off locations, time of requests, and duration of
trips). Specifically, in contrast to current vehicle sharing systems, the
operator does not set prices. Instead, customers submit bids and the operator
decides whether to rent or not. The operator can even accept negative bids to
motivate drivers to rebalance available cars to unpopular destinations within a
city. We model the operator's sequential decision-making problem as a
\emph{constrained Markov decision problem} (CMDP) and propose and rigorously
analyze a novel two phase $Q$-learning algorithm for its solution. Numerical
experiments are presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08937</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08937</id><created>2015-09-29</created><authors><author><keyname>Bikakis</keyname><forenames>Nikos</forenames></author><author><keyname>Benouaret</keyname><forenames>Karim</forenames></author><author><keyname>Sacharidis</keyname><forenames>Dimitris</forenames></author></authors><title>Finding Desirable Objects under Group Categorical Preferences</title><categories>cs.DB cs.DS</categories><comments>To appear in Knowledge and Information Systems Journal (KAIS),
  Springer 2015</comments><msc-class>97R50, 68P05, 68P15</msc-class><acm-class>E.1; H.2.8; H.3.1; I.3.5; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering a group of users, each specifying individual preferences over
categorical attributes, the problem of determining a set of objects that are
objectively preferable by all users is challenging on two levels. First, we
need to determine the preferable objects based on the categorical preferences
for each user, and second we need to reconcile possible conflicts among users'
preferences. A naive solution would first assign degrees of match between each
user and each object, by taking into account all categorical attributes, and
then for each object combine these matching degrees across users to compute the
total score of an object. Such an approach, however, performs two series of
aggregation, among categorical attributes and then across users, which
completely obscure and blur individual preferences. Our solution, instead of
combining individual matching degrees, is to directly operate on categorical
attributes, and define an objective Pareto-based aggregation for group
preferences. Building on our interpretation, we tackle two distinct but
relevant problems: finding the Pareto-optimal objects, and objectively ranking
objects with respect to the group preferences. To increase the efficiency when
dealing with categorical attributes, we introduce an elegant transformation of
categorical attribute values into numerical values, which exhibits certain nice
properties and allows us to use well-known index structures to accelerate the
solutions to the two problems. In fact, experiments on real and synthetic data
show that our index-based techniques are an order of magnitude faster than
baseline approaches, scaling up to millions of objects and thousands of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08955</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08955</id><created>2015-09-29</created><authors><author><keyname>Subratie</keyname><forenames>Kensworth</forenames></author><author><keyname>Aditya</keyname><forenames>Saumitra</forenames></author><author><keyname>Figueiredo</keyname><forenames>Renato</forenames></author><author><keyname>Carey</keyname><forenames>Cayelan C.</forenames></author><author><keyname>Hanson</keyname><forenames>Paul</forenames></author></authors><title>GRAPLEr: A Distributed Collaborative Environment for Lake Ecosystem
  Modeling that Integrates Overlay Networks, High-throughput Computing, and Web
  Services</title><categories>cs.DC</categories><comments>8 pages, 7 figures. PRAGMA29</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GLEON Research And PRAGMA Lake Expedition -- GRAPLE -- is a collaborative
effort between computer science and lake ecology researchers. It aims to
improve our understanding and predictive capacity of the threats to the water
quality of our freshwater resources, including climate change. This paper
presents GRAPLEr, a distributed computing system used to address the modeling
needs of GRAPLE researchers. GRAPLEr integrates and applies overlay virtual
network, high-throughput computing, and Web service technologies in a novel
way. First, its user-level IP-over-P2P (IPOP) overlay network allows compute
and storage resources distributed across independently-administered
institutions (including private and public clouds) to be aggregated into a
common virtual network, despite the presence of firewalls and network address
translators. Second, resources aggregated by the IPOP virtual network run
unmodified high-throughput computing middleware (HTCondor) to enable large
numbers of model simulations to be executed concurrently across the distributed
computing resources. Third, a Web service interface allows end users to submit
job requests to the system using client libraries that integrate with the R
statistical computing environment. The paper presents the GRAPLEr architecture,
describes its implementation and reports on its performance for batches of
General Lake Model (GLM) simulations across three cloud infrastructures
(University of Florida, CloudLab, and Microsoft Azure).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08960</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08960</id><created>2015-09-29</created><authors><author><keyname>Khurana</keyname><forenames>Udayan</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author></authors><title>Storing and Analyzing Historical Graph Data at Scale</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work on large-scale graph analytics to date has largely focused on the
study of static properties of graph snapshots. However, a static view of
interactions between entities is often an oversimplification of several complex
phenomena like the spread of epidemics, information diffusion, formation of
online communities}, and so on. Being able to find temporal interaction
patterns, visualize the evolution of graph properties, or even simply compare
them across time, adds significant value in reasoning over graphs. However,
because of lack of underlying data management support, an analyst today has to
manually navigate the added temporal complexity of dealing with large evolving
graphs. In this paper, we present a system, called Historical Graph Store, that
enables users to store large volumes of historical graph data and to express
and run complex temporal graph analytical tasks against that data. It consists
of two key components: a Temporal Graph Index (TGI), that compactly stores
large volumes of historical graph evolution data in a partitioned and
distributed fashion; it provides support for retrieving snapshots of the graph
as of any timepoint in the past or evolution histories of individual nodes or
neighborhoods; and a Spark-based Temporal Graph Analysis Framework (TAF), for
expressing complex temporal analytical tasks and for executing them in an
efficient and scalable manner. Our experiments demonstrate our system's
efficient storage, retrieval and analytics across a wide variety of queries on
large volumes of historical graph data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08967</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08967</id><created>2015-09-29</created><updated>2016-01-23</updated><authors><author><keyname>Sercu</keyname><forenames>Tom</forenames></author><author><keyname>Puhrsch</keyname><forenames>Christian</forenames></author><author><keyname>Kingsbury</keyname><forenames>Brian</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Very Deep Multilingual Convolutional Neural Networks for LVCSR</title><categories>cs.CL cs.NE</categories><comments>Accepted for publication at ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) are a standard component of many current
state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR)
systems. However, CNNs in LVCSR have not kept pace with recent advances in
other domains where deeper neural networks provide superior performance. In
this paper we propose a number of architectural advances in CNNs for LVCSR.
First, we introduce a very deep convolutional network architecture with up to
14 weight layers. There are multiple convolutional layers before each pooling
layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture.
Then, we introduce multilingual CNNs with multiple untied layers. Finally, we
introduce multi-scale input features aimed at exploiting more context at
negligible computational cost. We evaluate the improvements first on a Babel
task for low resource speech recognition, obtaining an absolute 5.77% WER
improvement over the baseline PLP DNN by training our CNN on the combined data
of six different languages. We then evaluate the very deep CNNs on the Hub5'00
benchmark (using the 262 hours of SWB-1 training data) achieving a word error
rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6%
relative) over the best published CNN result so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08969</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08969</id><created>2015-09-29</created><authors><author><keyname>Vagharshakyan</keyname><forenames>Suren</forenames></author><author><keyname>Bregovic</keyname><forenames>Robert</forenames></author><author><keyname>Gotchev</keyname><forenames>Atanas</forenames></author></authors><title>Light Field Reconstruction Using Shearlet Transform</title><categories>cs.CV</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we develop an image based rendering technique based on light
field reconstruction from a limited set of perspective views acquired by
cameras. Our approach utilizes sparse representation of epipolar-plane images
in a directionally sensitive transform domain, obtained by an adapted discrete
shearlet transform. The used iterative thresholding algorithm provides
high-quality reconstruction results for relatively big disparities between
neighboring views. The generated densely sampled light field of a given 3D
scene is thus suitable for all applications which requires light field
reconstruction. The proposed algorithm is compared favorably against state of
the art depth image based rendering techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08970</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08970</id><created>2015-09-29</created><authors><author><keyname>Panda</keyname><forenames>Priyadarshini</forenames></author><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Venkataramani</keyname><forenames>Swagath</forenames></author><author><keyname>Raghunathan</keyname><forenames>Anand</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Object Detection using Semantic Decomposition for Energy-Efficient
  Neural Computing</title><categories>cs.CV</categories><comments>10 pages, 11 figures, 3 algorithms, Submitted to IEEE TPAMI(Under
  Review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-learning algorithms are used to solve classification problems across
a broad range of computing domains, from data centers to wearable technology,
and place significant demand on their computing abilities.In this paper, we
present a new approach to optimize energy efficiency of machine-learning using
semantic decomposition to build a hierarchical framework of classifiers. We
observe that certain semantic information like color or texture are common
across various images in real-world data-sets for object detection
applications.We exploit these common semantic features to distinguish the
objects of interest from the remaining inputs in a data-set at a lower
computational effort. We build a hierarchical framework of classifiers, with
increasing levels of complexity, trained to recognize the broad representative
semantic features of the input. The degree of confidence at the output of a
classifier is used to decide whether classification can be terminated at the
current stage. Our methodology thus allows us to transform a given
classification algorithm into a semantically decomposed hierarchical framework.
We use color and texture as distinctive traits to carry out several experiments
for object detection from the Caltech data-set. Our experiments show that the
proposed method yields 1.93 times improvement in average energy over the
traditional single classifier model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08971</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08971</id><created>2015-09-29</created><updated>2016-01-28</updated><authors><author><keyname>Panda</keyname><forenames>Priyadarshini</forenames></author><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Conditional Deep Learning for Energy-Efficient and Enhanced Pattern
  Recognition</title><categories>cs.CV</categories><comments>6 pages, 10 figures, 2 algorithms &lt; Accepted for Design and
  Automation Test in Europe (DATE) conference, 2016&gt;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning neural networks have emerged as one of the most powerful
classification tools for vision related applications. However, the
computational and energy requirements associated with such deep nets can be
quite high, and hence their energy-efficient implementation is of great
interest. Although traditionally the entire network is utilized for the
recognition of all inputs, we observe that the classification difficulty varies
widely across inputs in real-world datasets; only a small fraction of inputs
require the full computational effort of a network, while a large majority can
be classified correctly with very low effort. In this paper, we propose
Conditional Deep Learning (CDL) where the convolutional layer features are used
to identify the variability in the difficulty of input instances and
conditionally activate the deeper layers of the network. We achieve this by
cascading a linear network of output neurons for each convolutional layer and
monitoring the output of the linear network to decide whether classification
can be terminated at the current stage or not. The proposed methodology thus
enables the network to dynamically adjust the computational effort depending
upon the difficulty of the input data while maintaining competitive
classification accuracy. We evaluate our approach on the MNIST dataset. Our
experiments demonstrate that our proposed CDL yields 1.91x reduction in average
number of operations per input, which translates to 1.84x improvement in
energy. In addition, our results show an improvement in classification accuracy
from 97.5% to 98.9% as compared to the original network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08972</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08972</id><created>2015-09-29</created><authors><author><keyname>Ardakani</keyname><forenames>Arash</forenames></author><author><keyname>Leduc-Primeau</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Onizawa</keyname><forenames>Naoya</forenames></author><author><keyname>Hanyu</keyname><forenames>Takahiro</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>VLSI Implementation of Deep Neural Network Using Integral Stochastic
  Computing</title><categories>cs.NE cs.AR</categories><comments>11 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hardware implementation of deep neural networks (DNNs) has recently
received tremendous attention: many applications in fact require high-speed
operations that suit a hardware implementation. However, numerous elements and
complex interconnections are usually required, leading to a large area
occupation and copious power consumption. Stochastic computing has shown
promising results for low-power area-efficient hardware implementations, even
though existing stochastic algorithms require long streams that cause long
latencies. In this paper, we propose an integer form of stochastic computation
and introduce some elementary circuits. We then propose an efficient
implementation of a DNN based on integral stochastic computing. The proposed
architecture uses integer stochastic streams and a modified Finite State
Machine-based tanh function to perform computations and even reduce the latency
compared to conventional stochastic computation. The proposed architecture has
been implemented on a Virtex7 FPGA, resulting in 44.96% and 62.36% average
reductions in area and latency compared to the best reported architecture in
literature. We also synthesize the circuits in a 65 nm CMOS technology and show
that they can tolerate a fault rate of up to 20% on some computations when
timing violations are allowed to occur, resulting in power savings. The
fault-tolerance property of the proposed architectures make them suitable for
inherently unreliable advanced process technologies such as memristor
technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08973</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08973</id><created>2015-09-29</created><authors><author><keyname>Taniguchi</keyname><forenames>Tadahiro</forenames></author><author><keyname>Nagai</keyname><forenames>Takayuki</forenames></author><author><keyname>Nakamura</keyname><forenames>Tomoaki</forenames></author><author><keyname>Iwahashi</keyname><forenames>Naoto</forenames></author><author><keyname>Ogata</keyname><forenames>Tetsuya</forenames></author><author><keyname>Asoh</keyname><forenames>Hideki</forenames></author></authors><title>Symbol Emergence in Robotics: A Survey</title><categories>cs.AI cs.CL cs.CV cs.RO</categories><comments>submitted to Advanced Robotics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans can learn the use of language through physical interaction with their
environment and semiotic communication with other people. It is very important
to obtain a computational understanding of how humans can form a symbol system
and obtain semiotic skills through their autonomous mental development.
Recently, many studies have been conducted on the construction of robotic
systems and machine-learning methods that can learn the use of language through
embodied multimodal interaction with their environment and other systems.
Understanding human social interactions and developing a robot that can
smoothly communicate with human users in the long term, requires an
understanding of the dynamics of symbol systems and is crucially important. The
embodied cognition and social interaction of participants gradually change a
symbol system in a constructive manner. In this paper, we introduce a field of
research called symbol emergence in robotics (SER). SER is a constructive
approach towards an emergent symbol system. The emergent symbol system is
socially self-organized through both semiotic communications and physical
interactions with autonomous cognitive developmental agents, i.e., humans and
developmental robots. Specifically, we describe some state-of-art research
topics concerning SER, e.g., multimodal categorization, word discovery, and a
double articulation analysis, that enable a robot to obtain words and their
embodied meanings from raw sensory--motor information, including visual
information, haptic information, auditory information, and acoustic speech
signals, in a totally unsupervised manner. Finally, we suggest future
directions of research in SER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08979</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08979</id><created>2015-09-29</created><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>De Giacomo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Lenzerini</keyname><forenames>Maurizio</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>Node Selection Query Languages for Trees</title><categories>cs.DB cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of node selection query languages for (finite) trees has been a
major topic in the recent research on query languages for Web documents. On one
hand, there has been an extensive study of XPath and its various extensions. On
the other hand, query languages based on classical logics, such as first-order
logic (FO) or Monadic Second-Order Logic (MSO), have been considered. Results
in this area typically relate an XPath-based language to a classical logic.
What has yet to emerge is an XPath-related language that is as expressive as
MSO, and at the same time enjoys the computational properties of XPath, which
are linear time query evaluation and exponential time query-containment test.
In this paper we propose muXPath, which is the alternation-free fragment of
XPath extended with fixpoint operators. Using two-way alternating automata, we
show that this language does combine desired expressiveness and computational
properties, placing it as an attractive candidate for the definite
node-selection query language for trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08985</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08985</id><created>2015-09-29</created><updated>2015-10-09</updated><authors><author><keyname>Lee</keyname><forenames>Chen-Yu</forenames></author><author><keyname>Gallagher</keyname><forenames>Patrick W.</forenames></author><author><keyname>Tu</keyname><forenames>Zhuowen</forenames></author></authors><title>Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,
  Gated, and Tree</title><categories>stat.ML cs.LG cs.NE</categories><comments>Patent disclosure, UCSD Docket No. SD2015-184, &quot;Forest Convolutional
  Neural Network&quot;, filed on March 4, 2015. UCSD Docket No. SD2016-053,
  &quot;Generalizing Pooling Functions in Convolutional Neural Network&quot;, filed on
  Sept 23, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek to improve deep neural networks by generalizing the pooling
operations that play a central role in current architectures. We pursue a
careful exploration of approaches to allow pooling to learn and to adapt to
complex and variable patterns. The two primary directions lie in (1) learning a
pooling function via (two strategies of) combining of max and average pooling,
and (2) learning a pooling function in the form of a tree-structured fusion of
pooling filters that are themselves learned. In our experiments every
generalized pooling operation we explore improves performance when used in
place of average or max pooling. We experimentally demonstrate that the
proposed pooling operations provide a boost in invariance properties relative
to conventional pooling and set the state of the art on several widely adopted
benchmark datasets; they are also easy to implement, and can be applied within
various deep neural network architectures. These benefits come with only a
light increase in computational overhead during training and a very modest
increase in the number of model parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08990</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08990</id><created>2015-09-29</created><authors><author><keyname>Rahimian</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Learning without Recall: A Case for Log-Linear Learning</title><categories>cs.SI cs.LG cs.SY math.OC stat.ML</categories><comments>in 5th IFAC Workshop on Distributed Estimation and Control in
  Networked Systems, (NecSys 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a model of learning and belief formation in networks in which
agents follow Bayes rule yet they do not recall their history of past
observations and cannot reason about how other agents' beliefs are formed. They
do so by making rational inferences about their observations which include a
sequence of independent and identically distributed private signals as well as
the beliefs of their neighboring agents at each time. Fully rational agents
would successively apply Bayes rule to the entire history of observations. This
leads to forebodingly complex inferences due to lack of knowledge about the
global network structure that causes those observations. To address these
complexities, we consider a Learning without Recall model, which in addition to
providing a tractable framework for analyzing the behavior of rational agents
in social networks, can also provide a behavioral foundation for the variety of
non-Bayesian update rules in the literature. We present the implications of
various choices for time-varying priors of such agents and how this choice
affects learning and its rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08992</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08992</id><created>2015-09-29</created><updated>2015-10-30</updated><authors><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing
  Parameter Sets</title><categories>cs.LG stat.ML</categories><comments>Advances in Neural Information Processing Systems 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference is typically intractable in high-treewidth undirected graphical
models, making maximum likelihood learning a challenge. One way to overcome
this is to restrict parameters to a tractable set, most typically the set of
tree-structured parameters. This paper explores an alternative notion of a
tractable set, namely a set of &quot;fast-mixing parameters&quot; where Markov chain
Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the
stationary distribution. While it is common in practice to approximate the
likelihood gradient using samples obtained from MCMC, such procedures lack
theoretical guarantees. This paper proves that for any exponential family with
bounded sufficient statistics, (not just graphical models) when parameters are
constrained to a fast-mixing set, gradient descent with gradients approximated
by sampling will approximate the maximum likelihood solution inside the set
with high-probability. When unregularized, to find a solution epsilon-accurate
in log-likelihood requires a total amount of effort cubic in 1/epsilon,
disregarding logarithmic factors. When ridge-regularized, strong convexity
allows a solution epsilon-accurate in parameter distance with effort quadratic
in 1/epsilon. Both of these provide of a fully-polynomial time randomized
approximation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.08995</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.08995</id><created>2015-09-29</created><updated>2015-10-20</updated><authors><author><keyname>Ramezanali</keyname><forenames>Mohammad</forenames></author><author><keyname>Mitra</keyname><forenames>Partha P.</forenames></author><author><keyname>Sengupta</keyname><forenames>Anirvan M.</forenames></author></authors><title>Critical behavior and universality classes for an algorithmic phase
  transition in sparse reconstruction</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>18 pages, 8 figures, 3 tables. arXiv admin note: text overlap with
  arXiv:1501.03194</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization problems with sparsity-inducing penalties exhibit sharp
algorithmic phase transitions when the numbers of variables tend to infinity,
between regimes of `good' and `bad' performance. The nature of these phase
transitions and associated universality classes remain incompletely understood.
We analyze the mean field equations from the cavity method for two sparse
reconstruction algorithms, Basis Pursuit and Elastic Net. These algorithms are
associated with penalized regression problems $(\mathbf{y}-\mathbf{H}
\mathbf{x})^2+\lambda V(\mathbf{x})$, where $V(\mathbf{x})$ is an
algorithm-dependent penalty function. Here $\lambda$ is the penalty parameter,
$\mathbf{x}$ is the $N$-dimensional, $K$-sparse solution to be recovered,
$\mathbf{y}$ is the $M$ dimensional vector of measurements, and $\mathbf{H}$ is
a known `random' matrix. In the limit $\lambda \rightarrow 0$ and $N\rightarrow
\infty$, keeping $\rho=K/N$ fixed, exact recovery is possible for sufficiently
large values of fractional measurement number $\alpha=M/N$, with an algorithmic
phase transition occurring at a known critical value $\alpha_c = \alpha(\rho)$.
We find that Basis Pursuit $V(\mathbf{x})=||\mathbf{x}||_{1} $and Elastic Net
$V(\mathbf{x})=\lambda_1 ||\mathbf{x}||_{1} + \tfrac{\lambda_2}{2}
||\mathbf{x}||_{2}^2$ belong to two different universality classes. The Mean
Squared Error goes to zero as $MSE \sim(\alpha_c-\alpha)^2$ as $\alpha$
approaches $\alpha_c$ from below for both algorithms. However, for Basis
Pursuit, precisely on the phase transition line $\alpha=\alpha_c$, $MSE \sim
\lambda^{4/3}$ for Basis Pursuit, whereas $MSE\sim\lambda$ for Elastic Net. We
also find that in presence of additive noise, within the perfect reconstruction
phase $\alpha&gt;\alpha_c$ there is a non-zero setting for $\lambda$ that
minimizes MSE, whereas at $\alpha=\alpha_c$ a finite $\lambda$ always increases
the MSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09002</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09002</id><created>2015-09-29</created><updated>2016-01-04</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Convergence of Stochastic Gradient Descent for PCA</title><categories>cs.LG math.OC stat.ML</categories><comments>Added analysis of the positive eigengap scenario, with new results;
  Some minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of principal component analysis (PCA) in a streaming
stochastic setting, where our goal is to find a direction of approximate
maximal variance, based on a stream of i.i.d. data points in $\reals^d$. A
simple and computationally cheap algorithm for this is stochastic gradient
descent (SGD), which incrementally updates its estimate based on each new data
point. However, due to the non-convex nature of the problem, analyzing its
performance has been a challenge. In particular, existing guarantees rely on a
non-trivial eigengap assumption on the covariance matrix, which is intuitively
unnecessary. In this paper, we provide (to the best of our knowledge) the first
eigengap-free convergence guarantees for SGD in the context of PCA. This also
partially resolves an open problem posed in \cite{hardt2014noisy}. Moreover,
under an eigengap assumption, we show that the same techniques lead to new SGD
convergence guarantees with better dependence on the eigengap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09011</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09011</id><created>2015-09-30</created><authors><author><keyname>Komiyama</keyname><forenames>Junpei</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Nakagawa</keyname><forenames>Hiroshi</forenames></author></authors><title>Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial
  Monitoring</title><categories>stat.ML cs.LG</categories><comments>24 pages, to appear in NIPS2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial monitoring is a general model for sequential learning with limited
feedback formalized as a game between two players. In this game, the learner
chooses an action and at the same time the opponent chooses an outcome, then
the learner suffers a loss and receives a feedback signal. The goal of the
learner is to minimize the total loss. In this paper, we study partial
monitoring with finite actions and stochastic outcomes. We derive a logarithmic
distribution-dependent regret lower bound that defines the hardness of the
problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the
multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the
distribution-dependent regret. PM-DMED significantly outperforms
state-of-the-art algorithms in numerical experiments. To show the optimality of
PM-DMED with respect to the regret bound, we slightly modify the algorithm by
introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically
optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09014</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09014</id><created>2015-09-30</created><updated>2015-10-16</updated><authors><author><keyname>Behnam</keyname><forenames>Rofael Emil Fayez</forenames></author></authors><title>Stats-Calculus Pose Descriptor Feeding A Discrete HMM Low-latency
  Detection and Recognition System For 3D Skeletal Actions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of human actions, under low observational latency, is a growing
interest topic, nowadays. Many approaches have been represented based on a
provided set of 3D Cartesian coordinates system originated at a certain
specific point located on a root joint. In this paper, We will present a
statistical detection and recognition system using Hidden Markov Model using 7
types of pose descriptors. * Cartesian Calculus Pose descriptor. * Angular
Calculus Pose descriptor. * Mixed-mode Stats-Calculus Pose descriptor. *
Centro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus Pose
descriptor. * Rela-Centro-Stats-Calculus DCT Pose descriptor. *
Rela-Centro-Stats-Calculus DCT-AMDF Pose descriptor. Stats-Calculus is a
feature extracting technique, that is developed on Moving Pose descriptor , but
using a combination of Statistics measures and Calculus measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09030</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09030</id><created>2015-09-30</created><authors><author><keyname>Das</keyname><forenames>Ayan</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sourangshu</forenames></author></authors><title>Distributed Weighted Parameter Averaging for SVM Training on Big Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two popular approaches for distributed training of SVMs on big data are
parameter averaging and ADMM. Parameter averaging is efficient but suffers from
loss of accuracy with increase in number of partitions, while ADMM in the
feature space is accurate but suffers from slow convergence. In this paper, we
report a hybrid approach called weighted parameter averaging (WPA), which
optimizes the regularized hinge loss with respect to weights on parameters. The
problem is shown to be same as solving SVM in a projected space. We also
demonstrate an $O(\frac{1}{N})$ stability bound on final hypothesis given by
WPA, using novel proof techniques. Experimental results on a variety of toy and
real world datasets show that our approach is significantly more accurate than
parameter averaging for high number of partitions. It is also seen the proposed
method enjoys much faster convergence compared to ADMM in features space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09047</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09047</id><created>2015-09-30</created><updated>2015-10-20</updated><authors><author><keyname>Friedrichs</keyname><forenames>Stephan</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author></authors><title>Parallel Metric Tree Embedding based on an Algebraic View on
  Moore-Bellman-Ford</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \emph{metric tree embedding} of expected \emph{stretch $\alpha$} maps a
weighted $n$-node graph $G = (V, E, \omega)$ to a weighted tree $T = (V_T, E_T,
\omega_T)$ with $V \subseteq V_T$ such that $\operatorname{dist}(v, w, G) \leq
\operatorname{dist}(v, w, T)$ and $\E[\operatorname{dist}(v, w, T)] \leq \alpha
\operatorname{dist}(v, w, G)$ for all $v, w \in V$. Such embeddings are highly
useful for designing fast approximation algorithms, as many hard problems are
easy to solve on tree instances. However, to date the best parallel
$\operatorname{polylog} n$ depth algorithm that achieves an asymptotically
optimal expected stretch of $\alpha \in \operatorname{O}(\log n)$ requires
$\operatorname{\Omega}(n^2)$ work and requires a metric as input.
  In this paper, we show how to achieve the same guarantees using
$\operatorname{\tilde{O}}(m^{1+\epsilon})$ work, where $m$ is the number of
edges of $G$ and $\epsilon &gt; 0$ is an arbitrarily small constant. Moreover, one
may reduce the work further to $\operatorname{\tilde{O}}(m + n^{1+\epsilon})$,
at the expense of increasing the expected stretch $\alpha$ to
$\operatorname{O}(\epsilon^{-1} \log n)$. Our main tool in deriving these
parallel algorithms is an algebraic characterization of a generalization of the
classic Moore-Bellman-Ford algorithm. We consider this framework, which
subsumes a variety of previous &quot;Moore-Bellman-Ford-flavored&quot; algorithms, to be
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09055</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09055</id><created>2015-09-30</created><authors><author><keyname>Perret</keyname><forenames>Julien</forenames></author><author><keyname>Gribaudi</keyname><forenames>Maurizio</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author></authors><title>Roads and cities of $18^{th}$ century France</title><categories>physics.soc-ph cond-mat.dis-nn cs.CY</categories><comments>12 pages, 4 figures</comments><journal-ref>Scientific Data 2, Article number: 150048 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of infrastructure networks such as roads and streets are of
utmost importance to understand the evolution of urban systems. However,
datasets describing these spatial objects are rare and sparse. The database
presented here represents the road network at the french national level
described in the historical map of Cassini in the $18^{th}$ century. The
digitization of this historical map is based on a collaborative methodology
that we describe in detail. This dataset can be used for a variety of
interdisciplinary studies, covering multiple spatial resolutions and ranging
from history, geography, urban economics to network science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09057</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09057</id><created>2015-09-30</created><authors><author><keyname>Kablan</keyname><forenames>Murad</forenames></author><author><keyname>Joe-Won</keyname><forenames>Carlee</forenames></author><author><keyname>Ha</keyname><forenames>Sangtae</forenames></author><author><keyname>Jamjoom</keyname><forenames>Hani</forenames></author><author><keyname>Keller</keyname><forenames>Eric</forenames></author></authors><title>The Cloud Needs a Reputation System</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's cloud apps are built from many diverse services that are managed by
different parties. At the same time, these parties, which consume and/or
provide services, continue to rely on arcane static security and entitlements
models. In this paper, we introduce Seit, an inter-tenant framework that
manages the interactions between cloud services. Seit is a software-defined
reputation-based framework. It consists of two primary components: (1) a set of
integration and query interfaces that can be easily integrated into cloud and
service providers' management stacks, and (2) a controller that maintains
reputation information using a mechanism that is adaptive to the highly dynamic
environment of the cloud. We have fully implemented Seit, and integrated it
into an SDN controller, a load balancer, a cloud service broker, an intrusion
detection system, and a monitoring framework. We evaluate the efficacy of Seit
using both an analytical model and a Mininet-based emulated environment. Our
analytical model validate the isolation and stability properties of Seit. Using
our emulated environment, we show that Seit can provide improved security by
isolating malicious tenants, reduced costs by adapting the infrastructure
without compromising security, and increased revenues for high quality service
providers by enabling reputation to impact discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09059</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09059</id><created>2015-09-30</created><updated>2016-01-04</updated><authors><author><keyname>Wu</keyname><forenames>Sheng</forenames><affiliation>David</affiliation></author><author><keyname>Kuang</keyname><forenames>Linling</forenames><affiliation>David</affiliation></author><author><keyname>Ni</keyname><forenames>Zuyao</forenames><affiliation>David</affiliation></author><author><keyname>Defeng</keyname><affiliation>David</affiliation></author><author><keyname>Huang</keyname></author><author><keyname>Guo</keyname><forenames>Qinghua</forenames></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>Message-Passing Receiver for Joint Channel Estimation and Decoding in 3D
  Massive MIMO-OFDM Systems</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Wireless Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the message-passing receiver design for the 3D
massive MIMO-OFDM systems. With the aid of the central limit argument and
Taylor-series approximation, a computationally efficient receiver that performs
joint channel estimation and decoding is devised by the framework of
expectation propagation. Specially, the local belief defined at the channel
transition function is expanded up to the second order with Wirtinger calculus,
to transform the messages sent by the channel transition function to a
tractable form. As a result, the channel impulse response (CIR) between each
pair of antennas is estimated by Gaussian message passing. In addition, a
variational expectation-maximization (EM)-based method is derived to learn the
channel power-delay-profile (PDP). The proposed joint algorithm is assessed in
3D massive MIMO systems with spatially correlated channels, and the empirical
results corroborate its superiority in terms of performance and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09060</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09060</id><created>2015-09-30</created><updated>2015-10-01</updated><authors><author><keyname>Xu</keyname><forenames>Tao</forenames></author><author><keyname>He</keyname><forenames>Jun</forenames></author></authors><title>Multi-objective Differential Evolution with Helper Functions for
  Constrained Optimization</title><categories>cs.NE</categories><comments>Accepted by The 15th UK Workshop on Computational Intelligence (UKCI
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving constrained optimization problems by multi-objective evolutionary
algorithms has scored tremendous achievements in the last decade. Standard
multi-objective schemes usually aim at minimizing the objective function and
also the degree of constraint violation simultaneously. This paper proposes a
new multi-objective method for solving constrained optimization problems. The
new method keeps two standard objectives: the original objective function and
the sum of degrees of constraint violation. But besides them, four more
objectives are added. One is based on the feasible rule. The other three come
from the penalty functions. This paper conducts an initial experimental study
on thirteen benchmark functions. A simplified version of CMODE is applied to
solving multi-objective optimization problems. Our initial experimental results
confirm our expectation that adding more helper functions could be useful. The
performance of SMODE with more helper functions (four or six) is better than
that with only two helper functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09066</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09066</id><created>2015-09-30</created><authors><author><keyname>Kablan</keyname><forenames>Murad</forenames></author><author><keyname>Jamjoom</keyname><forenames>Hani</forenames></author><author><keyname>Keller</keyname><forenames>Eric</forenames></author></authors><title>Quality of Consumption: The Friendlier Side of Quality of Service</title><categories>cs.CY</categories><comments>arXiv admin note: text overlap with arXiv:1509.09057</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud services today are increasingly built using functionality from other
running services. In this paper, we question whether legacy Quality of Services
(QoS) metrics and enforcement techniques are sufficient as they are producer
centric. We argue that, similar to customer rating systems found in banking
systems and many sharing economy apps (e.g., Uber and Airbnb), Quality of
Consumption (QoC) should be introduced to capture different metrics about
service consumers. We show how the combination of QoS and QoC, dubbed QoX, can
be used by consumers and providers to improve the security and management of
their infrastructure. In addition, we demonstrate how sharing information among
other consumers and providers increase the value of QoX. To address the main
challenge with sharing information, namely sybil attacks and mis-information,
we describe how we can leverage cloud providers as vouching authorities to
ensure the integrity of information. We present initial results in prototyping
the appropriate abstractions and interfaces in a cloud environment, focusing on
the design impact on both service providers and consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09067</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09067</id><created>2015-09-30</created><authors><author><keyname>Benaben</keyname><forenames>Frederick</forenames></author><author><keyname>Boissel-Dallier</keyname><forenames>Nicolas</forenames></author><author><keyname>Pingaud</keyname><forenames>Herve</forenames></author><author><keyname>Lorre</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Semantic issues in model-driven management of information system
  interoperability</title><categories>cs.SE</categories><comments>http://www.tandfonline.com/toc/tcim20/current</comments><proxy>ccsd</proxy><journal-ref>International Journal of Computer Integrated Manufacturing
  (IJCIM), 2013, 26 (11), pp.1042-1053</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MISE Project (Mediation Information System Engineering) aims at providing
collaborating organizations with a Mediation Information System (MIS) in charge
of supporting interoperability of a collaborative network. MISE proposes an
overall MIS design method according to a model-driven approach, based on model
transformations. This MIS is in charge of managing (i) information, (ii)
functions and (iii) processes among the information systems (IS) of partner
organizations involved in the network. Semantic issues are accompanying this
triple objective: How to deal with information reconciliation? How to ensure
the matching between business functions and technical services? How to identify
workflows among business processes? This article aims first, at presenting the
MISE approach, second at defining the semantic gaps along the MISE approach and
third at describing some past, current and future research works that deal with
these issues. Finally and as a conclusion, the very &quot;design-oriented&quot; previous
considerations are confronted with &quot;run-time&quot; requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09088</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09088</id><created>2015-09-30</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Enhanced Bilingual Evaluation Understudy</title><categories>cs.CL stat.ML</categories><comments>machine translation evaluation, enchanced bleu. in Lecture Notes on
  Information Theory, ISSN: 2301-3788, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our research extends the Bilingual Evaluation Understudy (BLEU) evaluation
technique for statistical machine translation to make it more adjustable and
robust. We intend to adapt it to resemble human evaluation more. We perform
experiments to evaluate the performance of our technique against the primary
existing evaluation methods. We describe and show the improvements it makes
over existing methods as well as correlation to them. When human translators
translate a text, they often use synonyms, different word orders or style, and
other similar variations. We propose an SMT evaluation technique that enhances
the BLEU metric to consider variations such as those.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09089</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09089</id><created>2015-09-30</created><authors><author><keyname>Pang</keyname><forenames>Yanwei</forenames></author><author><keyname>Ye</keyname><forenames>Li</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author><author><keyname>Pan</keyname><forenames>Jing</forenames></author></authors><title>Moving Object Detection in Video Using Saliency Map and Subspace
  Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moving object detection is a key to intelligent video analysis. On the one
hand, what moves is not only interesting objects but also noise and cluttered
background. On the other hand, moving objects without rich texture are prone
not to be detected. So there are undesirable false alarms and missed alarms in
many algorithms of moving object detection. To reduce the false alarms and
missed alarms, in this paper, we propose to incorporate a saliency map into an
incremental subspace analysis framework where the saliency map makes estimated
background has less chance than foreground (i.e., moving objects) to contain
salient objects. The proposed objective function systematically takes account
into the properties of sparsity, low-rank, connectivity, and saliency. An
alternative minimization algorithm is proposed to seek the optimal solutions.
Experimental results on the Perception Test Images Sequences demonstrate that
the proposed method is effective in reducing false alarms and missed alarms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09090</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09090</id><created>2015-09-30</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Real-Time Statistical Speech Translation</title><categories>cs.CL stat.ML</categories><comments>machine translation, polish english</comments><journal-ref>Advances in Intelligent Systems and Computing volume 275,
  p.107-114, Publisher: Springer, ISSN 2194-5357, ISBN 978-3-319-05950-1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research investigates the Statistical Machine Translation approaches to
translate speech in real time automatically. Such systems can be used in a
pipeline with speech recognition and synthesis software in order to produce a
real-time voice communication system between foreigners. We obtained three main
data sets from spoken proceedings that represent three different types of human
speech. TED, Europarl, and OPUS parallel text corpora were used as the basis
for training of language models, for developmental tuning and testing of the
translation system. We also conducted experiments involving part of speech
tagging, compound splitting, linear language model interpolation, TrueCasing
and morphosyntactic analysis. We evaluated the effects of variety of data
preparations on the translation results using the BLEU, NIST, METEOR and TER
metrics and tried to give answer which metric is most suitable for PL-EN
language pair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09092</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09092</id><created>2015-09-30</created><authors><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Gonnord</keyname><forenames>Laure</forenames><affiliation>LIP</affiliation></author></authors><title>An encoding of array verification problems into array-free Horn clauses</title><categories>cs.PL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically verifying safety properties of programs is hard, and it is even
harder if the program acts upon arrays or other forms of maps. Many approaches
exist for verifying programs operating upon Boolean and integer values (e.g.
abstract interpretation, counterexample-guided abstraction refinement using
interpolants), but transposing them to array properties has been fraught with
difficulties.In contrast to most preceding approaches, we do not introduce a
new abstract domain or a new interpolation procedure for arrays. Instead, we
generate an abstraction as a scalar problem and feed it to a preexisting
solver, with tunable precision.Our transformed problem is expressed using Horn
clauses, a common format with clear and unambiguous logical semantics for
verification problems. An important characteristic of our encoding is that it
creates a nonlinear Horn problem, with tree unfoldings, even though following
&quot;flatly&quot; the control-graph structure ordinarily yields a linear Horn problem,
with linear unfoldings. That is, our encoding cannot be expressed by an
encoding into another control-flow graph problem, and truly leverages the
capacity of the Horn clause format.We illustrate our approach with a completely
automated proof of the functional correctness of selection sort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09093</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09093</id><created>2015-09-30</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>A Sentence Meaning Based Alignment Method for Parallel Text Corpora
  Preparation</title><categories>cs.CL cs.IR</categories><comments>corpora filtration, text alignement, corpora improvement. arXiv admin
  note: text overlap with arXiv:1509.08881</comments><journal-ref>Advances in Intelligent Systems and Computing volume 275,
  p.107-114, Publisher: Springer, ISSN 2194-5357, ISBN 978-3-319-05950-1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text alignment is crucial to the accuracy of Machine Translation (MT)
systems, some NLP tools or any other text processing tasks requiring bilingual
data. This research proposes a language independent sentence alignment approach
based on Polish (not position-sensitive language) to English experiments. This
alignment approach was developed on the TED Talks corpus, but can be used for
any text domain or language pair. The proposed approach implements various
heuristics for sentence recognition. Some of them value synonyms and semantic
text structure analysis as a part of additional information. Minimization of
data loss was ensured. The solution is compared to other sentence alignment
implementations. Also an improvement in MT system score with text processed
with described tool is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09097</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09097</id><created>2015-09-30</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Polish - English Speech Statistical Machine Translation Systems for the
  IWSLT 2013</title><categories>cs.CL stat.ML</categories><comments>statistical machine translation. arXiv admin note: substantial text
  overlap with arXiv:1509.08874, arXiv:1509.08909</comments><journal-ref>Proceedings of the 10th International Workshop on Spoken Language
  Translation, Heidelberg, Germany, p. 113-119, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research explores the effects of various training settings from Polish
to English Statistical Machine Translation system for spoken language. Various
elements of the TED parallel text corpora for the IWSLT 2013 evaluation
campaign were used as the basis for training of language models, and for
development, tuning and testing of the translation system. The BLEU, NIST,
METEOR and TER metrics were used to evaluate the effects of data preparations
on translation results. Our experiments included systems, which use stems and
morphological information on Polish words. We also conducted a deep analysis of
provided Polish data as preparatory work for the automatic data correction and
cleaning phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09113</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09113</id><created>2015-09-30</created><authors><author><keyname>Matsinos</keyname><forenames>Evangelos</forenames></author></authors><title>Processing of acoustical signals via a wavelet-based analysis</title><categories>cs.SD physics.data-an</categories><comments>27 pages, 8 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, details are given on the implementation of a
wavelet-based analysis tailored to the processing of acoustical signals. The
family of the suitable wavelets (`Reimann wavelets') are obtained in the time
domain from a Fourier transform, extracted in Ref.~\cite{r1} after invoking
theoretical principles and time-frequency localisation constraints. A scheme is
set forth to determine the optimal values of the parameters of this type of
wavelet on the basis of the goodness of the reproduction of a $30$-s audio file
containing harmonic signals corresponding to six successive $A$ notes of the
chromatic musical scale, from $A_2$ to $A_7$. The quality of the reproduction
over about six and a half octaves is investigated. Finally, details are given
on the incorporation of the re-assignment method in the analysis framework, as
the means a) to determine the important contributions of the wavelet transforms
and b) to suppress noise present in the signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09114</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09114</id><created>2015-09-30</created><authors><author><keyname>Hua</keyname><forenames>Yang</forenames></author><author><keyname>Alahari</keyname><forenames>Karteek</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Online Object Tracking with Proposal Selection</title><categories>cs.CV</categories><comments>ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking-by-detection approaches are some of the most successful object
trackers in recent years. Their success is largely determined by the detector
model they learn initially and then update over time. However, under
challenging conditions where an object can undergo transformations, e.g.,
severe rotation, these methods are found to be lacking. In this paper, we
address this problem by formulating it as a proposal selection task and making
two contributions. The first one is introducing novel proposals estimated from
the geometric transformations undergone by the object, and building a rich
candidate set for predicting the object location. The second one is devising a
novel selection strategy using multiple cues, i.e., detection score and
edgeness score computed from state-of-the-art object edges and motion
boundaries. We extensively evaluate our approach on the visual object tracking
2014 challenge and online tracking benchmark datasets, and show the best
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09121</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09121</id><created>2015-09-30</created><authors><author><keyname>Ashraf</keyname><forenames>Md Izhar</forenames></author><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author></authors><title>The &quot;handedness&quot; of language: Directional symmetry breaking of sign
  usage in words</title><categories>cs.CL</categories><comments>10 pages, 4 figures + Supplementary Information (8 pages, 3 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using large written corpora for many different scripts, we show that the
occurrence probability distributions of signs at the left and right ends of
words have a distinct heterogeneous nature. Characterizing this asymmetry using
quantitative inequality measures, we show that the beginning of a word is less
restrictive in sign usage than the end. The asymmetry is also seen in
undeciphered inscriptions and we use this to infer the direction of writing
which agrees with archaeological evidence. Unlike traditional investigations of
phonotactic constraints which focus on language-specific patterns, our study
reveals a property valid across languages and writing systems. As both language
and writing are unique aspects of our species, this universal signature may
reflect an innate feature of the human cognitive phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09130</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09130</id><created>2015-09-30</created><authors><author><keyname>Vernade</keyname><forenames>Claire</forenames><affiliation>LTCI</affiliation></author><author><keyname>Capp&#xe9;</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author></authors><title>Learning From Missing Data Using Selection Bias in Movie Recommendation</title><categories>stat.ML cs.IR cs.LG cs.SI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommending items to users is a challenging task due to the large amount of
missing information. In many cases, the data solely consist of ratings or tags
voluntarily contributed by each user on a very limited subset of the available
items, so that most of the data of potential interest is actually missing.
Current approaches to recommendation usually assume that the unobserved data is
missing at random. In this contribution, we provide statistical evidence that
existing movie recommendation datasets reveal a significant positive
association between the rating of items and the propensity to select these
items. We propose a computationally efficient variational approach that makes
it possible to exploit this selection bias so as to improve the estimation of
ratings from small populations of users. Results obtained with this approach
applied to neighborhood-based collaborative filtering illustrate its potential
for improving the reliability of the recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09132</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09132</id><created>2015-09-30</created><updated>2016-01-27</updated><authors><author><keyname>Hackett</keyname><forenames>A.</forenames></author><author><keyname>Cellai</keyname><forenames>D.</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>S.</forenames></author><author><keyname>Arenas</keyname><forenames>A.</forenames></author><author><keyname>Gleeson</keyname><forenames>J. P.</forenames></author></authors><title>Bond percolation on multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analytical approach for bond percolation on multiplex networks
and use it to determine the expected size of the giant connected component and
the value of the critical bond occupation probability in these networks. We
advocate the relevance of these tools to the modeling of multilayer robustness
and contribute to the debate on whether any benefit is to be yielded from
studying a full multiplex structure as opposed to its monoplex projection,
especially in the seemingly irrelevant case of a bond occupation probability
that does not depend on the layer. Although we find that in many cases the
predictions of our theory for multiplex networks coincide with previously
derived results for monoplex networks, we also uncover the remarkable result
that for a certain class of multiplex networks, well described by our theory,
new critical phenomena occur as multiple percolation phase transitions are
present. We provide an instance of this phenomenon in a multipex network
constructed from London rail and European air transportation datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09137</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09137</id><created>2015-09-30</created><authors><author><keyname>Nikou</keyname><forenames>Alexandros</forenames></author><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Cooperative Task Planning of Multi-Agent Systems Under Timed Temporal
  Specifications</title><categories>cs.SY</categories><comments>Submitted to American Control Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the problem of cooperative task planning of multi-agent systems
when timed constraints are imposed to the system is investigated. We consider
timed constraints given by Metric Interval Temporal Logic (MITL). We propose a
method for automatic control synthesis in a two-stage systematic procedure.
With this method we guarantee that all the agents satisfy their own individual
task specifications as well as that the team satisfies a team global task
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09138</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09138</id><created>2015-09-30</created><authors><author><keyname>Kumar</keyname><forenames>Manish</forenames></author><author><keyname>Kaul</keyname><forenames>Shubham</forenames></author></authors><title>Technical Report on Intruder Detection and Alert System</title><categories>cs.CY</categories><comments>Submitted to CII Innovation 2015, India and NI Engineering Impact
  Awards 2015, India. arXiv admin note: text overlap with arXiv:1508.03479</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a smart trespasser detection and alert system which aims
to increase the amount of security as well as the likelihood of positively
identifying or stopping trespassers and intruders as compared to other commonly
deployed home security system. Using multiple sensors, this system can gauge
the extent of danger exhibited by a person or animal in or around the home
premises, and can forward certain critical information regarding the same to
home owners as well as other specified persons such as relevant security
authorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09147</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09147</id><created>2015-09-30</created><authors><author><keyname>Cheung</keyname><forenames>Yun Kuen</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author><author><keyname>Starnberger</keyname><forenames>Martin</forenames></author></authors><title>Combinatorial Auctions with Conflict-Based Externalities</title><categories>cs.GT cs.DS</categories><comments>This is the full version of our WINE 2015 conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial auctions (CA) are a well-studied area in algorithmic mechanism
design. However, contrary to the standard model, empirical studies suggest that
a bidder's valuation often does not depend solely on the goods assigned to him.
For instance, in adwords auctions an advertiser might not want his ads to be
displayed next to his competitors' ads. In this paper, we propose and analyze
several natural graph-theoretic models that incorporate such negative
externalities, in which bidders form a directed conflict graph with maximum
out-degree $\Delta$. We design algorithms and truthful mechanisms for social
welfare maximization that attain approximation ratios depending on $\Delta$.
  For CA, our results are twofold: (1) A lottery that eliminates conflicts by
discarding bidders/items independent of the bids. It allows to apply any
truthful $\alpha$-approximation mechanism for conflict-free valuations and
yields an $\mathcal{O}(\alpha\Delta)$-approximation mechanism. (2) For
fractionally sub-additive valuations, we design a rounding algorithm via a
novel combination of a semi-definite program and a linear program, resulting in
a cone program; the approximation ratio is $\mathcal{O}((\Delta \log \log
\Delta)/\log \Delta)$. The ratios are almost optimal given existing hardness
results.
  For the prominent application of adwords auctions, we present several
algorithms for the most relevant scenario when the number of items is small. In
particular, we design a truthful mechanism with approximation ratio $o(\Delta)$
when the number of items is only logarithmic in the number of bidders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09149</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09149</id><created>2015-09-30</created><authors><author><keyname>Benaben</keyname><forenames>Frederick</forenames></author><author><keyname>Rajsiri</keyname><forenames>Vatcharaphun</forenames></author><author><keyname>Lorr&#xe9;</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Pingaud</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Knowledge-based system for collaborative process specification</title><categories>cs.SE cs.AI</categories><comments>\&amp;lt;10.1016/j.compind.2009.10.012\&amp;gt</comments><proxy>ccsd</proxy><journal-ref>Computers and Industrial Engineering, Elsevier, 2010, 61 (2),
  pp.161-175</journal-ref><doi>10.1016/j.compind.2009.10.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an ontology-based approach for the design of a
collaborative business process model (CBP). This CBP is considered as a
specification of needs in order to build a collaboration information system
(CIS) for a network of organisations. The study is a part of a model driven
engineering approach of the CIS in a specific enterprise interoperability
framework that will be summarised. An adaptation of the Business Process
Modeling Notation (BPMN) is used to represent the CBP model. We develop a
knowledge-based system (KbS) which is composed of three main parts: knowledge
gathering, knowledge representation and reasoning, and collaborative business
process modelling. The first part starts from a high abstraction level where
knowledge from business partners is captured. A collaboration ontology is
defined in order to provide a structure to store and use the knowledge
captured. In parallel, we try to reuse generic existing knowledge about
business processes from the MIT Process Handbook repository. This results in a
collaboration process ontology that is also described. A set of rules is
defined in order to extract knowledge about fragments of the CBP model from the
two previous ontologies. These fragments are finally assembled in the third
part of the KbS. A prototype of the KbS has been developed in order to
implement and support this approach. The prototype is a computer-aided design
tool of the CBP. In this paper, we will present the theoretical aspects of each
part of this KbS as well as the tools that we developed and used in order to
support its functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09152</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09152</id><created>2015-09-30</created><authors><author><keyname>Benaben</keyname><forenames>Frederick</forenames></author><author><keyname>Mu</keyname><forenames>Wenxin</forenames></author><author><keyname>Boissel-Dallier</keyname><forenames>Nicolas</forenames></author><author><keyname>Barthe-Delano&#xeb;</keyname><forenames>Anne-Marie</forenames></author><author><keyname>Zribi</keyname><forenames>Sarah</forenames></author><author><keyname>Pingaud</keyname><forenames>Herve</forenames></author></authors><title>Supporting interoperability of collaborative networks through
  engineering of a service-based Mediation Information System (MISE 2.0)</title><categories>cs.SE cs.AI</categories><comments>\&amp;lt;10.1080/17517575.2014.928949\&amp;gt</comments><proxy>ccsd</proxy><journal-ref>Enterprise Information Systems, Taylor \&amp; Francis: STM,
  Behavioural Science and Public Health Titles, 2015, 9 (5-6), pp.556-582</journal-ref><doi>10.1080/17517575.2014.928949</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Mediation Information System Engineering project is currently finishing
its second iteration (MISE 2.0). The main objective of this scientific project
is to provide any emerging collaborative situation with methods and tools to
deploy a Mediation Information System (MIS). MISE 2.0 aims at defining and
designing a service-based platform, dedicated to initiating and supporting the
interoperability of collaborative situations among potential partners. This
MISE 2.0 platform implements a model-driven engineering approach to the design
of a service-oriented MIS dedicated to supporting the collaborative situation.
This approach is structured in three layers, each providing their own key
innovative points: (i) the gathering of individual and collaborative knowledge
to provide appropriate collaborative business behaviour (key point: knowledge
management, including semantics, exploitation and capitalization), (ii)
deployment of a mediation information system able to computerize the previously
deduced collaborative processes (key point: the automatic generation of
collaborative workflows, including connection with existing devices or
services) (iii) the management of the agility of the obtained collaborative
network of organizations (key point: supervision of collaborative situations
and relevant exploitation of the gathered data). MISE covers business issues
(through BPM), technical issues (through an SOA) and agility issues of
collaborative situations (through EDA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09153</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09153</id><created>2015-09-30</created><authors><author><keyname>Benaben</keyname><forenames>Frederick</forenames></author><author><keyname>Barthe-Delano&#xeb;</keyname><forenames>Anne-Marie</forenames></author><author><keyname>Truptil</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Pingaud</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Event-driven agility of interoperability during the Run-time of
  collaborative processes</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>Decision Support Systems, Elsevier, 2014, 59, pp.171-179</journal-ref><doi>10.1016/j.dss.2013.11.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern business environment tends to involve a large network of
heterogeneous people, devices and organizations that engage in collaborative
processes among themselves. Given the nature of this type of collaboration and
the high degree of interoperability between partner Information Systems, these
processes need to be agile in order to respond to changes in context, which may
occur at any time during the collaborative situation.The objective is to build
a Mediation Information System (MIS), in support of collaborative situations,
whose architecture must be (i) built to be relevant to the collaborative
situation under consideration, (ii) more easily integrated into the existing
systems, and (iii) sufficiently agile, through its awareness of the environment
and of process events, and through the way it reacts to events detected as
being relevant.To apply agility mechanisms, it is crucial to detect the
significant events that will lead to a subsequent evolution of the situation
(detection step). Event-Driven Architecture (EDA) is used to design the
structure of the part of the system that is in charge of MIS agility. This
architecture takes the events into account, manages them and, if needed, uses
them to trigger the adaptation of the MIS.We have defined a means to monitor
the evolution of the situation. If relevant changes are detected, and if the
situation does not evolve in the expected way, an adaptation is proposed. It is
concluded that the principles of detection and adaptation, combined with the
responsiveness of the system (provided by the automation of transitions), and
based on Event Driven Architecture principles, together provide the agility
required for collaborative processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09157</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09157</id><created>2015-09-30</created><authors><author><keyname>Gogineni</keyname><forenames>Vinay Chakravarthi</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mrityunjoy</forenames></author></authors><title>Distributed Multi-task APA over Adaptive Networks Based on Partial
  Diffusion</title><categories>cs.SY</categories><comments>Under Communication. arXiv admin note: substantial text overlap with
  arXiv:1507.08566; text overlap with arXiv:1311.4894 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed multi-task adaptive strategies are useful to estimate multiple
parameter vectors simultaneously in a collaborative manner. The existed
distributed multi-task strategies use diffusion mode of cooperation in which
during adaptation step each node gets the cooperation from it neighborhood
nodes but not in the same cluster and during combining step each node combines
the intermediate estimates of it neighboring nodes that belong to the same
cluster. For this the nodes need to transmit the intermediate estimates to its
neighborhood. In this paper we propose an extension to the multi-task diffusion
affine projection algorithm by allowing partial sharing of the entries of the
intermediate estimates among the neighbors. The proposed algorithm is termed as
multi-task Partial diffusion Affine projection Algorithm (multi-task Pd-APA)
which can provide the trade-off between the communication performance and the
estimation performance. The performance analysis of the proposed multi-task
partial diffusion APA algorithm is studied in mean and mean square sense.
Simulations were conducted to verify the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09174</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09174</id><created>2015-09-30</created><authors><author><keyname>Fachada</keyname><forenames>Nuno</forenames></author><author><keyname>Lopes</keyname><forenames>Vitor V.</forenames></author><author><keyname>Martins</keyname><forenames>Rui C.</forenames></author><author><keyname>Rosa</keyname><forenames>Agostinho C.</forenames></author></authors><title>Model-independent comparison of simulation output</title><categories>cs.OH</categories><msc-class>68U20</msc-class><acm-class>D.2.4; I.2.2; I.5.2; I.6.4; I.6.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational models of complex systems are usually elaborate and sensitive
to implementation details, characteristics which often affect model
verification and validation. Model replication is a possible solution to this
problem, as it bypasses the biases associated with the language or toolkit used
to develop the original model, promoting model verification, model validation,
and improved model understanding. Some argue that a computational model is
untrustworthy until it has been successfully replicated. However, most models
have only been implemented by the original developer, and thus, have never been
replicated. Several reasons for this problem have been identified, namely: a)
lack of incentive; b) below par model communication; c) insufficient knowledge
of how to replicate; and, d) level of difficulty of the replication task. In
this paper, we present a model comparison technique, which uses principal
component analysis to convert simulation output into a set of linearly
uncorrelated statistical measures, analyzable in a consistent,
model-independent fashion. It is appropriate for ascertaining distributional
equivalence of a model replication with its original implementation. Besides
model-independence, this technique has three other desirable properties: a) it
automatically selects output features that best explain implementation
differences; b) it does not depend on the distributional properties of
simulation output; and, c) it simplifies the modelers' work, as it can be used
directly on simulation outputs. The proposed technique is shown to produce
similar results to classic comparison methods when applied to a well-studied
reference model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09187</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09187</id><created>2015-09-30</created><authors><author><keyname>Cheng</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Mallat</keyname><forenames>Stephane</forenames></author></authors><title>Deep Haar Scattering Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An orthogonal Haar scattering transform is a deep network, computed with a
hierarchy of additions, subtractions and absolute values, over pairs of
coefficients. It provides a simple mathematical model for unsupervised deep
network learning. It implements non-linear contractions, which are optimized
for classification, with an unsupervised pair matching algorithm, of polynomial
complexity. A structured Haar scattering over graph data computes permutation
invariant representations of groups of connected points in the graph. If the
graph connectivity is unknown, unsupervised Haar pair learning can provide a
consistent estimation of connected dyadic groups of points. Classification
results are given on image data bases, defined on regular grids or graphs, with
a connectivity which may be known or unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09188</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09188</id><created>2015-09-30</created><updated>2016-02-17</updated><authors><author><keyname>Kolev</keyname><forenames>Pavel</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author></authors><title>A Note on Spectral Clustering</title><categories>cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spectral clustering is a popular and successful approach for partitioning the
nodes of a graph into clusters for which the ratio of outside connections
compared to the volume (sum of degrees) is small. In order to partition into
$k$ clusters, one first computes an approximation of the first $k$ eigenvectors
of the (normalized) Laplacian of $G$, uses it to embed the vertices of $G$ into
$k$-dimensional Euclidean space $\mathbb{R}^k$, and then partitions the
resulting points via a $k$-means clustering algorithm. It is an important task
for theory to explain the success of spectral clustering.
  Peng et al. (COLT, 2015) made an important step in this direction. They
showed that spectral clustering provably works if the gap between the
$(k+1)$-th and the $k$-th eigenvalue of the normalized Laplacian is
sufficiently large. They prove a structural and an algorithmic result. The
algorithmic result needs a considerably stronger gap assumption and does not
analyze the standard spectral clustering paradigm; it replaces spectral
embedding by heat kernel embedding and $k$-means clustering by locality
sensitive hashing.
  We extend their work in two directions. Structurally, we improve the quality
guarantee for spectral clustering by a factor of $k$ and simultaneously weaken
the gap assumption. Algorithmically, we show that the standard paradigm for
spectral clustering works. Moreover, it even works with the same gap assumption
as required for the structural result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09199</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09199</id><created>2015-09-30</created><authors><author><keyname>Kulakov</keyname><forenames>Anton</forenames></author><author><keyname>Zwolinski</keyname><forenames>Mark</forenames></author><author><keyname>Reeve</keyname><forenames>Jeff</forenames></author></authors><title>Fault Tolerance in Distributed Neural Computing</title><categories>cs.NE cs.DC</categories><doi>10.13140/RG.2.1.1387.0800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing complexity of computing systems, complete hardware
reliability can no longer be guaranteed. We need, however, to ensure overall
system reliability. One of the most important features of artificial neural
networks is their intrinsic fault-tolerance. The aim of this work is to
investigate whether such networks have features that can be applied to wider
computational systems. This paper presents an analysis, in both the learning
and operational phases, of a distributed feed-forward neural network with
decentralised event-driven time management, which is insensitive to
intermittent faults caused by unreliable communication or faulty hardware
components. The learning rules used in the model are local in space and time,
which allows efficient scalable distributed implementation. We investigate the
overhead caused by injected faults and analyse the sensitivity to limited
failures in the computational hardware in different areas of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09207</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09207</id><created>2015-09-30</created><updated>2016-02-24</updated><authors><author><keyname>Yamauchi</keyname><forenames>Yukiko</forenames></author><author><keyname>Uehara</keyname><forenames>Taichi</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>Pattern Formation Problem for Synchronous Mobile Robots in the Three
  Dimensional Euclidean Space</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a swarm of autonomous mobile robots each of which is an anonymous
point in the three-dimensional Euclidean space (3D-space) and synchronously
executes a common distributed algorithm. We investigate the pattern formation
problem that requires the robots to form a given target pattern from an initial
configuration and characterize the problem by showing a necessary and
sufficient condition for the robots to form a given target pattern.
  The pattern formation problem in the two dimensional Euclidean space
(2D-space) has been investigated by Suzuki and Yamashita (SICOMP 1999, TCS
2010), and Fujinaga et al. (SICOMP 2015). The symmetricity $\rho(P)$ of a
configuration (i.e., the positions of robots) $P$ is intuitively the order of
the cyclic group that acts on $P$. It has been shown that fully-synchronous
(FSYNC) robots can form a target pattern $F$ from an initial configuration $P$
if and only if $\rho(P)$ divides $\rho(F)$.
  We extend the notion of symmetricity to 3D-space by using the rotation groups
each of which is defined by a set of rotation axes and their arrangement. We
define the symmetricity $\varrho(P)$ of configuration $P$ in 3D-space as the
set of rotation groups that acts on $P$ and whose rotation axes do not contain
any robot. We show the following necessary and sufficient condition for the
pattern formation problem which is a natural extension of the existing results
of the pattern formation problem in 2D-space: FSYNC robots in 3D-space can form
a target pattern $F$ from an initial configuration $P$ if and only if
$\varrho(P) \subseteq \varrho(F)$. For solvable instances, we present a pattern
formation algorithm for oblivious FSYNC robots. The insight of this paper is
that symmetry of mobile robots in 3D-space is sometimes lower than the symmetry
of their positions and the robots can show their symmetry by their movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09211</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09211</id><created>2015-09-30</created><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author></authors><title>Normalized rotation shape descriptors and lossy compression of molecular
  shape</title><categories>cs.CE</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a common need to search of molecular databases for compounds
resembling some shape, what suggests having similar biological activity while
searching for new drugs. The large size of the databases requires fast methods
for such initial screening, for example based on feature vectors constructed to
fulfill the requirement that similar molecules should correspond to close
vectors. Ultrafast Shape Recognition (USR) is a popular approach of this type.
It uses vectors of 12 real number as 3 first moments of distances from 4
emphasized points. These coordinates might contain unnecessary correlations and
does not allow to reconstruct the approximated shape. In contrast, spherical
harmonic (SH) decomposition uses orthogonal coordinates, suggesting their
independence and so lager informational content of the feature vector. There is
usually considered rotationally invariant SH descriptors, what means discarding
of some essential information.
  This article discusses framework for descriptors with normalized rotation,
for example by using principal component analysis (PCA-SH). As one of the most
interesting are ligands which have to slide into a protein, we will introduce
descriptors optimized for such flat elongated shapes. Bent deformed cylinder
(BDC) describes the molecule as a cylinder which was first bent, then deformed
such that its cross-sections became ellipses of evolving shape. Legendre
polynomials are used to describe the central axis of such bent cylinder.
Additional polynomials are used to define evolution of such elliptic
cross-section along the main axis. There will be also discussed bent
cylindrical harmonics (BCH), which uses cross-sections described by cylindrical
harmonics instead of ellipses. All these normalized rotation descriptors allow
to reconstruct (decode) the approximated representation of the shape, hence can
be also used for lossy compression purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09222</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09222</id><created>2015-09-30</created><authors><author><keyname>Amuru</keyname><forenames>SaiDhiraj</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>On Jamming Against Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>32 double-spaced pages, 18 figures. Submitted to IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study jamming attacks against wireless networks.
Specifically, we consider a network of base stations (BS) or access points (AP)
and investigate the impact of a fixed number of jammers that are randomly
deployed according to a Binomial point process. We shed light on the network
performance in terms of a) the outage probability and b) the error probability
of a victim receiver in the downlink of this wireless network. We derive
analytical expressions for both these metrics and discuss in detail how the
jammer network must adapt to the various wireless network parameters in order
to effectively attack the victim receivers. For instance, we will show that
with only 1 jammer per BS/AP a) the outage probability of the wireless network
can be increased from 1% (as seen in the non-jamming case) to 80% and b) when
retransmissions are used, the jammers cause the effective network activity
factor (and hence the interference among the BSs) to be doubled. Furthermore,
we show that the behavior of the jammer network as a function of the BS/AP
density is not obvious. In particular, an interesting concave-type behavior is
seen which indicates that the number of jammers required to attack the wireless
network must scale with the BS density only until a certain value beyond which
it decreases. In the context of error probability of the victim receiver, we
study whether or not some recent results related to jamming in the
point-to-point link scenario can be extended to the case of jamming against
wireless networks. Numerical results are presented to validate the theoretical
inferences presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09227</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09227</id><created>2015-09-30</created><authors><author><keyname>Molzahn</keyname><forenames>Daniel K</forenames></author><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author><author><keyname>Niemerg</keyname><forenames>Matthew</forenames></author></authors><title>Toward Topologically Based Upper Bounds on the Number of Power Flow
  Solutions</title><categories>math.OC cs.CE math.AG</categories><comments>6 pages, 5 figures. Submitted to special session at the IEEE American
  Control Conference</comments><report-no>ADP-15-34/T936</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power flow equations, which relate power injections and voltage phasors,
are at the heart of many electric power system computations. While Newton-based
methods typically find the &quot;high-voltage&quot; solution to the power flow equations,
which is of primary interest, there are potentially many &quot;low-voltage&quot;
solutions that are useful for certain analyses. This paper addresses the number
of solutions to the power flow equations. There exist upper bounds on the
number of power flow solutions; however, there is only limited work regarding
bounds that are functions of network topology. This paper empirically explores
the relationship between the network topology, as characterized by the maximal
cliques, and the number of power flow solutions. To facilitate this analysis,
we use a numerical polynomial homotopy continuation approach that is guaranteed
to find all complex solutions to the power flow equations. The number of
solutions obtained from this approach upper bounds the number of real
solutions. Testing with many small networks informs the development of upper
bounds that are functions of the network topology. Initial results include
empirically derived expressions for the maximum number of solutions for certain
classes of network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09228</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09228</id><created>2015-09-30</created><authors><author><keyname>Divakaran</keyname><forenames>Srikrishnan</forenames></author></authors><title>Fast Algorithms for Exact String Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pattern string $P$ of length $n$ and a query string $T$ of length
$m$, where the characters of $P$ and $T$ are drawn from an alphabet of size
$\Delta$, the {\em exact string matching} problem consists of finding all
occurrences of $P$ in $T$. For this problem, we present algorithms that in
$O(n\Delta^2)$ time pre-process $P$ to essentially identify $sparse(P)$, a
rarely occurring substring of $P$, and then use it to find occurrences of $P$
in $T$ efficiently. Our algorithms require a worst case search time of $O(m)$,
and expected search time of $O(m/min(|sparse(P)|, \Delta))$, where
$|sparse(P)|$ is at least $\delta$ (i.e. the number of distinct characters in
$P$), and for most pattern strings it is observed to be $\Omega(n^{1/2})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09235</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09235</id><created>2015-09-30</created><authors><author><keyname>Probst</keyname><forenames>Malte</forenames></author></authors><title>Generative Adversarial Networks in Estimation of Distribution Algorithms
  for Combinatorial Optimization</title><categories>cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1509.06535</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Generative Adversarial
Networks (GAN) are generative neural networks which can be trained to
implicitly model the probability distribution of given data, and it is possible
to sample this distribution. We integrate a GAN into an EDA and evaluate the
performance of this system when solving combinatorial optimization problems
with a single objective. We use several standard benchmark problems and compare
the results to state-of-the-art multivariate EDAs. GAN-EDA doe not yield
competitive results - the GAN lacks the ability to quickly learn a good
approximation of the probability distribution. A key reason seems to be the
large amount of noise present in the first EDA generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09236</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09236</id><created>2015-09-30</created><updated>2015-11-08</updated><authors><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen A.</forenames></author></authors><title>On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix
  Approximation</title><categories>cs.LG cs.CC math.NA math.OC</categories><comments>16 pages, new important references to the work of Poljak and Rohn,
  and Clarkson and Woodruff</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The low-rank matrix approximation problem with respect to the component-wise
$\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal
component analysis (PCA), has become a very popular tool in data mining and
machine learning. Robust PCA aims at recovering a low-rank matrix that was
perturbed with sparse noise, with applications for example in
foreground-background video separation. Although $\ell_1$-LRA is strongly
believed to be NP-hard, there is, to the best of our knowledge, no formal proof
of this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already in
the rank-one case, using a reduction from MAX CUT. Our derivations draw
interesting connections between $\ell_1$-LRA and several other well-known
problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a
particular densest bipartite subgraph problem, the computation of the cut norm
of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to
be NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09237</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09237</id><created>2015-09-30</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>K&#xf6;ppl</keyname><forenames>Dominik</forenames></author><author><keyname>Manea</keyname><forenames>Florin</forenames></author></authors><title>Efficiently Finding All Maximal $\alpha$-gapped Repeats</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For $\alpha\geq 1$, an $\alpha$-gapped repeat in a word $w$ is a factor $uvu$
of $w$ such that $|uv|\leq \alpha |u|$; the two factors $u$ in such a repeat
are called arms, while the factor $v$ is called gap. Such a repeat is called
maximal if its arms cannot be extended simultaneously with the same symbol to
the right or, respectively, to the left. In this paper we show that the number
of maximal $\alpha$-gapped repeats that may occur in a word is upper bounded by
$18\alpha n$. This allows us to construct an algorithm finding all the maximal
$\alpha$-gapped repeats of a word in $O(\alpha n)$; this is optimal, in the
worst case, as there are words that have $\Theta(\alpha n)$ maximal
$\alpha$-gapped repeats. Our techniques can be extended to get comparable
results in the case of $\alpha$-gapped palindromes, i.e., factors
$uvu^\mathrm{T}$ with $|uv|\leq \alpha |u|$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09240</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09240</id><created>2015-07-26</created><updated>2015-11-29</updated><authors><author><keyname>Luo</keyname><forenames>Chu</forenames></author></authors><title>Solving a Mathematical Problem in Square War: a Go-like Board Game</title><categories>cs.AI</categories><comments>8 pages</comments><msc-class>68R05</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a board game: Square War. The game definition of
Square War is similar to the classic Chinese board game Go. Then we propose a
mathematical problem of the game Square War. Finally, we show that the problem
can be solved by using a method of mixed mathematics and computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09241</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09241</id><created>2015-09-30</created><authors><author><keyname>Wu</keyname><forenames>Yuan</forenames><affiliation>Sherman</affiliation></author><author><keyname>He</keyname><forenames>Yanfei</forenames><affiliation>Sherman</affiliation></author><author><keyname>Qian</keyname><forenames>Liping</forenames><affiliation>Sherman</affiliation></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames><affiliation>Sherman</affiliation></author><author><keyname>Xuemin</keyname><affiliation>Sherman</affiliation></author><author><keyname>Shen</keyname></author></authors><title>Joint Scheduling and Power Allocations for Traffic Offloading via
  Dual-Connectivity</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of mobile traffic demand, a promising approach to
relieve cellular network congestion is to offload users' traffic to small-cell
networks. In this paper, we investigate how the mobile users (MUs) can
effectively offload traffic by taking advantage of the capability of
dual-connectivity, which enables an MU to simultaneously communicate with a
macro base station (BS) and a small-cell access point (AP) via two
radio-interfaces. Offloading traffic to the AP usually reduces the MUs' mobile
data cost, but often at the expense of suffering increased interferences from
other MUs at the same AP. We thus formulate an optimization problem that
jointly determines each MU's traffic schedule (between the BS and AP) and power
control (between two radio-interfaces). The system objective is to minimize all
MUs' total cost, while satisfying each MU's transmit-power constraints through
proper interference control. In spite of the non-convexity of the problem, we
design both a centralized algorithm and a distributed algorithm to solve the
joint optimization problem. Numerical results show that the proposed algorithms
can achieve the close-to-optimum results comparing with the ones achieved by
the LINGO (a commercial optimization software), but with significantly less
computational complexity. The results also show that the proposed adaptive
offloading can significantly reduce the MUs' cost, i.e., save more than 75% of
the cost without offloading traffic and 65% of the cost with a fixed
offloading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09243</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09243</id><created>2015-09-30</created><authors><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author><author><keyname>Gader</keyname><forenames>Paul</forenames></author></authors><title>A spatial compositional model (SCM) for linear unmixing and endmember
  uncertainty estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The normal compositional model (NCM) has been extensively used in
hyperspectral unmixing. However, most of the previous research has focused on
estimation of endmembers and/or their variability. Also, little work has
employed spatial information in NCM. In this paper, we show that NCM can be
used for calculating the uncertainty of the estimated endmembers with spatial
priors incorporated for better unmixing. This results in a spatial
compositional model (SCM) which features (i) spatial priors that force
neighboring abundances to be similar based on their pixel similarity and (ii) a
posterior that is obtained from a likelihood model which does not assume pixel
independence. The resulting algorithm turns out to be easy to implement and
efficient to run. We compared SCM with current state-of-the-art algorithms on
synthetic and real images. The results show that SCM can in the main provide
more accurate endmembers and abundances. Moreover, the estimated uncertainty
can serve as a prediction of endmember error under certain conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09249</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09249</id><created>2015-09-30</created><authors><author><keyname>Ghahroodi</keyname><forenames>Massoud Mokhtarpour</forenames></author><author><keyname>Zwolinski</keyname><forenames>Mark</forenames></author></authors><title>In-Field Logic Repair of Deep Sub-Micron CMOS Processors</title><categories>cs.AR</categories><doi>10.13140/RG.2.1.2435.6566</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra Deep-Sub-Micron CMOS chips have to function correctly and reliably, not
only during their early post-fabrication life, but also for their entire life
span. In this paper, we present an architectural-level in-field repair
technique. The key idea is to trade area for reliability by adding repair
features to the system while keeping the power and the performance overheads as
low as possible. In the case of permanent faults, spare blocks will replace the
faulty blocks on the fly. Meanwhile by shutting down the main logic blocks,
partial threshold voltage recovery can be achieved which will alleviate the
ageing-related delays and timing issues. The technique can avoid fatal
shut-downs in the system and will decrease the down-time, hence the
availability of such a system will be preserved. We have implemented the
proposed idea on a pipelined processor core using a conventional ASIC design
flow. The simulation results show that by tolerating about 70% area overhead
and less than 18% power overhead we can dramatically increase the reliability
and decrease the downtime of the processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09254</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09254</id><created>2015-09-30</created><authors><author><keyname>Crane</keyname><forenames>Harry</forenames></author><author><keyname>Dempsey</keyname><forenames>Walter</forenames></author></authors><title>Community detection for interaction networks</title><categories>cs.SI physics.soc-ph stat.ME</categories><comments>29 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, it is common practice to obtain a network from
interaction counts by thresholding each pairwise count at a prescribed value.
Our analysis calls attention to the dependence of certain methods, notably
Newman--Girvan modularity, on the choice of threshold. Essentially, the
threshold either separates the network into clusters automatically, making the
algorithm's job trivial, or erases all structure in the data, rendering
clustering impossible. By fitting the original interaction counts as given, we
show that minor modifications to classical statistical methods outperform the
prevailing approaches for community detection from interaction datasets. We
also introduce a new hidden Markov model for inferring community structures
that vary over time. We demonstrate each of these features on three real
datasets: the karate club dataset, voting data from the U.S.\ Senate
(2001--2003), and temporal voting data for the U.S. Supreme Court (1990--2004).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09257</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09257</id><created>2015-09-30</created><updated>2015-11-04</updated><authors><author><keyname>Bertsekas</keyname><forenames>Dimitri P.</forenames></author></authors><title>Incremental Aggregated Proximal and Augmented Lagrangian Algorithms</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider minimization of the sum of a large number of convex functions,
and we propose an incremental aggregated version of the proximal algorithm,
which bears similarity to the incremental aggregated gradient and subgradient
methods that have received a lot of recent attention. Under cost function
differentiability and strong convexity assumptions, we show linear convergence
for a sufficiently small constant stepsize. This result also applies to
distributed asynchronous variants of the method, involving bounded
interprocessor communication delays.
  We then consider dual versions of incremental proximal algorithms, which are
incremental augmented Lagrangian methods for separable equality-constrained
optimization problems. Contrary to the standard augmented Lagrangian method,
these methods admit decomposition in the minimization of the augmented
Lagrangian, and update the multipliers far more frequently. Our incremental
aggregated augmented Lagrangian methods bear similarity to several known
decomposition algorithms, including the alternating direction method of
multipliers (ADMM) and more recent variations. We compare these methods in
terms of their properties, and highlight their potential advantages and
limitations.
  We also address the solution of separable inequality-constrained optimization
problems through the use of nonquadratic augmented Lagrangiias such as the
exponential, and we dually consider a corresponding incremental aggregated
version of the proximal algorithm that uses nonquadratic regularization, such
as an entropy function. We finally propose a closely related linearly
convergent method for minimization of large differentiable sums subject to an
orthant constraint, which may be viewed as an incremental aggregated version of
the mirror descent method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09271</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09271</id><created>2015-09-30</created><updated>2016-03-01</updated><authors><author><keyname>Childs</keyname><forenames>Andrew M.</forenames></author><author><keyname>van Dam</keyname><forenames>Wim</forenames></author><author><keyname>Hung</keyname><forenames>Shih-Han</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Optimal quantum algorithm for polynomial interpolation</title><categories>quant-ph cs.CC cs.CR cs.DS</categories><comments>17 pages, minor improvements, added conjecture about multivariate
  interpolation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the number of quantum queries required to determine the
coefficients of a degree-d polynomial over GF(q). A lower bound shown
independently by Kane and Kutin and by Meyer and Pommersheim shows that d/2+1/2
quantum queries are needed to solve this problem with bounded error, whereas an
algorithm of Boneh and Zhandry shows that d quantum queries are sufficient. We
show that the lower bound is achievable: d/2+1/2 quantum queries suffice to
determine the polynomial with bounded error. Furthermore, we show that d/2+1
queries suffice to achieve probability approaching 1 for large q. These upper
bounds improve results of Boneh and Zhandry on the insecurity of cryptographic
protocols against quantum attacks. We also show that our algorithm's success
probability as a function of the number of queries is precisely optimal.
Furthermore, the algorithm can be implemented with gate complexity poly(log q)
with negligible decrease in the success probability. We end with a conjecture
about the quantum query complexity of multivariate polynomial interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09282</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09282</id><created>2015-09-30</created><authors><author><keyname>Zhu</keyname><forenames>Shanying</forenames></author><author><keyname>Soh</keyname><forenames>Yeng Chai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Distributed Inference for Relay-Assisted Sensor Networks With
  Intermittent Measurements Over Fading Channels</title><categories>cs.IT cs.DC math.IT</categories><comments>32 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a general distributed estimation problem in
relay-assisted sensor networks by taking into account time-varying asymmetric
communications, fading channels and intermittent measurements. Motivated by
centralized filtering algorithms, we propose a distributed innovation-based
estimation algorithm by combining the measurement innovation (assimilation of
new measurement) and local data innovation (incorporation of neighboring data).
Our algorithm is fully distributed which does not need a fusion center. We
establish theoretical results regarding asymptotic unbiasedness and consistency
of the proposed algorithm. Specifically, in order to cope with time-varying
asymmetric communications, we utilize an ordering technique and the generalized
Perron complement to manipulate the first and second moment analyses in a
tractable framework. Furthermore, we present a performance-oriented design of
the proposed algorithm for energy-constrained networks based on the theoretical
results. Simulation results corroborate the theoretical findings, thus
demonstrating the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09292</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09292</id><created>2015-09-30</created><updated>2015-11-03</updated><authors><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Maclaurin</keyname><forenames>Dougal</forenames></author><author><keyname>Aguilera-Iparraguirre</keyname><forenames>Jorge</forenames></author><author><keyname>G&#xf3;mez-Bombarelli</keyname><forenames>Rafael</forenames></author><author><keyname>Hirzel</keyname><forenames>Timothy</forenames></author><author><keyname>Aspuru-Guzik</keyname><forenames>Al&#xe1;n</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Convolutional Networks on Graphs for Learning Molecular Fingerprints</title><categories>cs.LG cs.NE stat.ML</categories><comments>9 pages, 5 figures. To appear in Neural Information Processing
  Systems (NIPS)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a convolutional neural network that operates directly on graphs.
These networks allow end-to-end learning of prediction pipelines whose inputs
are graphs of arbitrary size and shape. The architecture we present generalizes
standard molecular feature extraction methods based on circular fingerprints.
We show that these data-driven features are more interpretable, and have better
predictive performance on a variety of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09293</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09293</id><created>2015-09-30</created><updated>2016-01-22</updated><authors><author><keyname>Wiedermann</keyname><forenames>Marc</forenames></author><author><keyname>Donges</keyname><forenames>Jonathan F.</forenames></author><author><keyname>Kurths</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Donner</keyname><forenames>Reik V.</forenames></author></authors><title>Spatial network surrogates for disentangling complex system structure
  from spatial embedding of nodes</title><categories>physics.soc-ph cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks with nodes embedded in a metric space have gained increasing
interest in recent years. The effects of spatial embedding on the networks'
structural characteristics, however, are rarely taken into account when
studying their macroscopic properties. Here, we propose a hierarchy of null
models to generate random surrogates from a given spatially embedded network
that can preserve global and local statistics associated with the nodes'
embedding in a metric space. Comparing the original network's and the resulting
surrogates' global characteristics allows to quantify to what extent these
characteristics are already predetermined by the spatial embedding of the nodes
and links. We apply our framework to various real-world spatial networks and
show that the proposed models capture macroscopic properties of the networks
under study much better than standard random network models that do not account
for the nodes' spatial embedding. Depending on the actual performance of the
proposed null models, the networks are categorized into different classes.
Since many real-world complex networks are in fact spatial networks, the
proposed approach is relevant for disentangling underlying complex system
structure from spatial embedding of nodes in many fields, ranging from social
systems over infrastructure and neurophysiology to climatology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09294</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09294</id><created>2015-09-30</created><authors><author><keyname>Mustafa</keyname><forenames>Armin</forenames></author><author><keyname>Kim</keyname><forenames>Hansung</forenames></author><author><keyname>Guillemaut</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Hilton</keyname><forenames>Adrian</forenames></author></authors><title>General Dynamic Scene Reconstruction from Multiple View Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a general approach to dynamic scene reconstruction from
multiple moving cameras without prior knowledge or limiting constraints on the
scene structure, appearance, or illumination. Existing techniques for dynamic
scene reconstruction from multiple wide-baseline camera views primarily focus
on accurate reconstruction in controlled environments, where the cameras are
fixed and calibrated and background is known. These approaches are not robust
for general dynamic scenes captured with sparse moving cameras. Previous
approaches for outdoor dynamic scene reconstruction assume prior knowledge of
the static background appearance and structure. The primary contributions of
this paper are twofold: an automatic method for initial coarse dynamic scene
segmentation and reconstruction without prior knowledge of background
appearance or structure; and a general robust approach for joint segmentation
refinement and dense reconstruction of dynamic scenes from multiple
wide-baseline static or moving cameras. Evaluation is performed on a variety of
indoor and outdoor scenes with cluttered backgrounds and multiple dynamic
non-rigid objects such as people. Comparison with state-of-the-art approaches
demonstrates improved accuracy in both multiple view segmentation and dense
reconstruction. The proposed approach also eliminates the requirement for prior
knowledge of scene structure and appearance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09299</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09299</id><created>2015-09-30</created><authors><author><keyname>Andreev</keyname><forenames>Sergey</forenames></author><author><keyname>Galinina</keyname><forenames>Olga</forenames></author><author><keyname>Pyattaev</keyname><forenames>Alexander</forenames></author><author><keyname>Gerasimenko</keyname><forenames>Mikhail</forenames></author><author><keyname>Tirronen</keyname><forenames>Tuomas</forenames></author><author><keyname>Torsner</keyname><forenames>Johan</forenames></author><author><keyname>Sachs</keyname><forenames>Joachim</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Koucheryavy</keyname><forenames>Yevgeni</forenames></author></authors><title>Understanding the IoT Connectivity Landscape: A Contemporary M2M Radio
  Technology Roadmap</title><categories>cs.NI</categories><comments>9 pages, 4 figures, 15 references</comments><journal-ref>IEEE Communications Magazine, Volume: 53, Issue: 9, Pages: 32 --
  40, 2015</journal-ref><doi>10.1109/MCOM.2015.7263370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the market-changing phenomenon of the Internet of
Things (IoT), which relies on the underlying paradigm of machine-to-machine
(M2M) communications to integrate a plethora of various sensors, actuators, and
smart meters across a wide spectrum of businesses. The M2M landscape features
today an extreme diversity of available connectivity solutions which -- due to
the enormous economic promise of the IoT -- need to be harmonized across
multiple industries. To this end, we comprehensively review the most prominent
existing and novel M2M radio technologies, as well as share our first-hand
real-world deployment experiences, with the goal to provide a unified insight
into enabling M2M architectures, unique technology features, expected
performance, and related standardization developments. We pay particular
attention to the cellular M2M sector employing 3GPP LTE technology. This work
is a systematic recollection of our many recent research, industrial,
entrepreneurial, and standardization efforts within the contemporary M2M
ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09308</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09308</id><created>2015-09-30</created><updated>2015-11-10</updated><authors><author><keyname>Lavin</keyname><forenames>Andrew</forenames></author><author><keyname>Gray</keyname><forenames>Scott</forenames></author></authors><title>Fast Algorithms for Convolutional Neural Networks</title><categories>cs.NE cs.LG</categories><acm-class>I.2.6; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks take GPU days of compute time to train on
large data sets. Pedestrian detection for self driving cars requires very low
latency. Image recognition for mobile phones is constrained by limited
processing resources. The success of convolutional neural networks in these
situations is limited by how fast we can compute them. Conventional FFT based
convolution is fast for large filters, but state of the art convolutional
neural networks use small, 3x3 filters. We introduce a new class of fast
algorithms for convolutional neural networks using Winograd's minimal filtering
algorithms. The algorithms compute minimal complexity convolution over small
tiles, which makes them fast with small filters and small batch sizes. We
benchmark a GPU implementation of our algorithm with the VGG network and show
state of the art throughput at batch sizes from 1 to 64.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.09313</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.09313</id><created>2015-09-30</created><authors><author><keyname>Kannan</keyname><forenames>Ramakrishnan</forenames></author><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Park</keyname><forenames>Haesun</forenames></author></authors><title>A High-Performance Parallel Algorithm for Nonnegative Matrix
  Factorization</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative matrix factorization (NMF) is the problem of determining two
non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such
that $A \approx W H$. NMF is a useful tool for many applications in different
domains such as topic modeling in text mining, background separation in video
analysis, and community detection in social networks. Despite its popularity in
the data mining community, there is a lack of efficient parallel software to
solve the problem for big datasets. Existing distributed-memory algorithms are
limited in terms of performance and applicability, as they are implemented
using Hadoop and are designed only for sparse matrices.
  We propose a distributed-memory parallel algorithm that computes the
factorization by iteratively solving alternating non-negative least squares
(NLS) subproblems for $W$ and $H$. To our knowledge, our algorithm is the first
high-performance parallel algorithm for NMF. It maintains the data and factor
matrices in memory (distributed across processors), uses MPI for interprocessor
communication, and, in the dense case, provably minimizes communication costs
(under mild assumptions). As opposed to previous implementations, our algorithm
is also flexible: (1) it performs well for dense and sparse matrices, and (2)
it allows the user to choose from among multiple algorithms for solving local
NLS subproblems within the alternating iterations. We demonstrate the
scalability of our algorithm and compare it with baseline implementations,
showing significant performance improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00001</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00001</id><created>2015-09-30</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author></authors><title>Polish to English Statistical Machine Translation</title><categories>cs.CL stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1509.09097,
  arXiv:1509.08909, arXiv:1509.08874</comments><journal-ref>Polish to English Statistical Machine Translation., XV
  International Phd Workshop OWD 2013, Wis{\l}a, p.108-115, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research explores the effects of various training settings on a Polish
to English Statistical Machine Translation system for spoken language. Various
elements of the TED, Europarl, and OPUS parallel text corpora were used as the
basis for training of language models, for development, tuning and testing of
the translation system. The BLEU, NIST, METEOR and TER metrics were used to
evaluate the effects of the data preparations on the translation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00002</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00002</id><created>2015-09-30</created><authors><author><keyname>Mitharwal</keyname><forenames>Rajendra</forenames></author><author><keyname>Andriulli</keyname><forenames>Francesco P.</forenames></author></authors><title>A Regularized Boundary Element Formulation for Contactless SAR
  Evaluations within Homogeneous and Inhomogeneous Head Phantoms</title><categories>physics.med-ph cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a Boundary Element Method (BEM) formulation for
contactless electromagnetic field assessments. The new scheme is based on a
regularized BEM approach that requires the use of electric measurements only.
The regularization is obtained by leveraging on an extension of Calderon
techniques to rectangular systems leading to well-conditioned problems
independent of the discretization density. This enables the use of highly
discretized Huygens surfaces that can be consequently placed very near to the
radiating source. In addition, the new regularized scheme is hybridized with
both surfacic homogeneous and volumetric inhomogeneous forward BEM solvers
accelerated with fast matrix-vector multiplication schemes. This allows for
rapid and effective dosimetric assessments and permits the use of inhomogeneous
and realistic head phantoms. Numerical results corroborate the theory and
confirms the practical effectiveness of all newly proposed formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00012</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00012</id><created>2015-09-30</created><authors><author><keyname>Ye</keyname><forenames>Jianbo</forenames></author><author><keyname>Wu</keyname><forenames>Panruo</forenames></author><author><keyname>Wang</keyname><forenames>James Z.</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author></authors><title>Accelerated Discrete Distribution Clustering under Wasserstein Distance</title><categories>stat.CO cs.LG stat.ML</categories><comments>two-column, 14 pages, 3 figures, 4 tables, 29 equations, manuscript
  submitted to IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of research areas, the bag of weighted vectors and the histogram
are widely used descriptors for complex objects. Both can be expressed as
discrete distributions. D2-clustering pursues the minimum total within-cluster
variation for a set of discrete distributions subject to the
Kantorovich-Wasserstein metric. D2-clustering has a severe scalability issue,
the bottleneck being the computation of a centroid distribution that minimizes
its sum of squared distances to the cluster members. In this paper, we develop
three scalable optimization techniques, specifically, the subgradient descent
method, ADMM, and modified Bregman ADMM, for computing the centroids of large
clusters without compromising the objective function. The strengths and
weaknesses of these techniques are examined through experiments; and scenarios
for their respective usage are recommended. Moreover, we develop both serial
and parallelized versions of the algorithms, collectively named the
AD2-clustering. By experimenting with large-scale data, we demonstrate the
computational efficiency of the new methods and investigate their convergence
properties and numerical stability. The clustering results obtained on several
datasets in different domains are highly competitive in comparison with some
widely used methods' in the corresponding areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00026</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00026</id><created>2015-09-30</created><authors><author><keyname>Shao</keyname><forenames>Sihua</forenames></author><author><keyname>Khreishah</keyname><forenames>Abdallah</forenames></author><author><keyname>Khalil</keyname><forenames>Issa</forenames></author></authors><title>Joint Link Scheduling and Brightness Control for Greening VLC-based
  Indoor Access Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demands for broadband wireless access services is expected to outstrip the
spectrum capacity in the near-term - &quot;spectrum crunch&quot;. Deploying additional
femotocells to address this challenge is cost-inefficient, due to the backhaul
challenge and the exorbitant system maintenance. According to an Alcatel-Lucent
report, most of the mobile Internet access traffic happens indoor. Leveraging
power line communication and the available indoor infrastructure, visible light
communication (VLC) can be utilized with small one-time cost. VLC also
facilitates the great advantage of being able to jointly perform illumination
and communications, and little extra power beyond illumination is required to
empower communications, thus rendering wireless access with small power
consumption. In this study, we investigate the problem of minimizing total
power consumption of a general multi-user VLC indoor network while satisfying
users' traffic demands and maintaining an acceptable level of illumination. We
utilize the column generation method to obtain an $\epsilon$-bounded solution.
Several practical implementation issues are integrated with the proposed
algorithm, including different configurations of light source and ways of
resolving the interference among VLC links. Through extensive simulations, we
show that our approach reduces the power consumption of the state-of-art
VLC-based scheduling algorithms by more than 60\% while maintaining the
required illumination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00037</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00037</id><created>2015-09-30</created><authors><author><keyname>Golo</keyname><forenames>Natasa</forenames></author></authors><title>A case study of conspiracy theories about Charlie Hebdo terrorist attack</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 7 figures, submitted to Royal Society Open Science as a
  brief report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results of the public opinion poll performed in January 2015, just after
the terrorist attack on the French satirical weekly magazine Charlie Hebdo and
the kosher supermarket in Paris, when 17 people were killed, showed that a
significant number of French citizens held conspiratorial beliefs about it (17
%). This gave reason to an alternative analysis of public opinion, presented in
this paper. We collected 990 on-line articles mentioning Charlie Hebdo from Le
Monde web site (one of the leading French news agencies), and looked at the
ones that contained words related with conspiracy (in French: `complot',
`conspiration' or `conjuration'). Then we analyzed the readers response,
performing a semantic analysis of the 16490 comments posted on-line as reaction
to the above articles. We identified 2 attempts to launch a conspiratorial
rumour. A more recent Le Monde article, which reflects on those early
conspiratorial attempts from a rational perspective, and the commentary
thereon, showed that the readers have more interest in understanding the
possible causes for the onset of conspiratorial beliefs then to delve into the
arguments that the conspiracists previously brought up to the public. We
discuss the results of the above semantic analysis and give interpretation of
the opinion dynamics measured in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00041</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00041</id><created>2015-09-30</created><authors><author><keyname>Arnold</keyname><forenames>Taylor</forenames></author><author><keyname>Kane</keyname><forenames>Michael</forenames></author><author><keyname>Urbanek</keyname><forenames>Simon</forenames></author></authors><title>iotools: High-Performance I/O Tools for R</title><categories>stat.CO cs.PF</categories><comments>8 pages, 2 figures</comments><msc-class>03-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The iotools package provides a set of tools for Input/Output (I/O) intensive
datasets processing in R (R Core Team, 2014). Efficent parsing methods are
included which minimize copying and avoid the use of intermediate string
representations whenever possible. Functions for applying chunk-wise operations
allow for computing on streaming input as well as arbitrarily large files. We
present a set of example use cases for iotools, as well as extensive benchmarks
comparing comparable functions provided in both core-R as well as other
contributed packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00050</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00050</id><created>2015-09-30</created><authors><author><keyname>Kumar</keyname><forenames>Rajesh</forenames></author><author><keyname>Guck</keyname><forenames>Dennis</forenames></author><author><keyname>Stoelinga</keyname><forenames>Marielle</forenames></author></authors><title>Time Dependent Analysis with Dynamic Counter Measure Trees</title><categories>cs.CR</categories><comments>5 pages, 2 Figures, Workshop paper(QAPL 2015)-Extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of a security attack crucially depends on time: the more time
available to the attacker, the higher the probability of a successful attack.
Formalisms such as Reliability block diagrams, Reliability graphs and Attack
Countermeasure trees provide quantitative information about attack scenarios,
but they are provably insufficient to model dependent actions which involve
costs, skills, and time. In this presentation, we extend the Attack
Countermeasure trees with a notion of time; inspired by the fact that there is
a strong correlation between the amount of resources in which the attacker
invests (in this case time) and probability that an attacker succeeds. This
allows for an effective selection of countermeasures and rank them according to
their resource consumption in terms of costs/skills of installing them and
effectiveness in preventing an attack
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00055</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00055</id><created>2015-09-30</created><authors><author><keyname>Setlur</keyname><forenames>Pawan</forenames></author><author><keyname>Rangaswamy</keyname><forenames>Muralidhar</forenames></author></authors><title>Joint Filter and Waveform Design for Radar STAP in Signal Dependent
  Interference</title><categories>cs.SY</categories><comments>AFRL tech. treport, 2014. DTIC (www.dtic.mil) number unassigned</comments><doi>10.1109/TSP.2015.2451114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Waveform design is a pivotal component of the fully adaptive radar construct.
In this paper we consider waveform design for radar space time adaptive
processing (STAP), accounting for the waveform dependence of the clutter
correlation matrix. Due to this dependence, in general, the joint problem of
receiver filter optimization and radar waveform design becomes an intractable,
non-convex optimization problem, Nevertheless, it is however shown to be
individually convex either in the filter or in the waveform variables. We
derive constrained versions of: a) the alternating minimization algorithm, b)
proximal alternating minimization, and c) the constant modulus alternating
minimization, which, at each step, iteratively optimizes either the STAP filter
or the waveform independently. A fast and slow time model permits waveform
design in radar STAP but the primary bottleneck is the computational complexity
of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00056</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00056</id><created>2015-09-30</created><authors><author><keyname>McSweeney</keyname><forenames>John K.</forenames></author></authors><title>Single-Seed Cascades on Clustered Networks</title><categories>physics.soc-ph cs.SI math.PR</categories><comments>14 pages, 1 figure</comments><msc-class>60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic network cascade process developed by Watts applied to a
random networks with a specified amount of clustering, belonging to a class of
random networks developed by Newman. We adapt existing tree-based methods to
formulate an appropriate two-type branching process to describe the spread of a
cascade started with a single active node, and obtain a fixed-point equation to
implicitly express the extinction probability of such a cascade. In so doing,
we also recover a special case of a formula of Hackett et al. giving conditions
for certain extinction of the cascade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00059</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00059</id><created>2015-09-30</created><authors><author><keyname>Gao</keyname><forenames>Xiaobin</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>On Remote Estimation with Multiple Communication Channels</title><categories>cs.IT cs.SY math.IT</categories><comments>Submitted to 2016 American Control Conference (ACC 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a sequential estimation and sensor scheduling problem in
the presence of multiple communication channels. As opposed to the classical
remote estimation problem that involves one perfect (noiseless) channel and one
extremely noisy channel (which corresponds to not transmitting the observed
state), a more realistic additive noise channel with fixed power constraint
along with a more costly perfect channel is considered. It is shown, via a
counter-example, that the common folklore of applying symmetric threshold
policy, which is well known to be optimal (for unimodal state densities) in the
classical two-channel remote estimation problem, can be suboptimal for the
setting considered. Next, in order to make the problem tractable, a side
channel which signals the sign of the underlying state is considered. It is
shown that, under some technical assumptions, threshold-in-threshold
communication scheduling is optimal for this setting. The impact of the
presence of a noisy channel is analyzed numerically based on dynamic
programming. This numerical analysis uncovers some rather surprising results
inheriting known properties from the noisy and noiseless settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00064</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00064</id><created>2015-09-30</created><authors><author><keyname>Gao</keyname><forenames>Xiaobin</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Optimal Sensor Scheduling and Remote Estimation over an Additive Noise
  Channel</title><categories>cs.SY cs.IT math.IT</categories><comments>In Proceedings of American Control Conference (ACC), pages 2723-2728,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a sensor scheduling and remote estimation problem with one sensor
and one estimator. At each time step, the sensor makes an observation on the
state of a source, and then decides whether to transmit its observation to the
estimator or not. The sensor is charged a cost for each transmission. The
remote estimator generates a real-time estimate on the state of the source
based on the messages received from the sensor. The estimator is charged for
estimation error. As compared with previous works from the literature, we
further assume that there is an additive communication channel noise. As a
consequence, the sensor needs to encode the message before transmitting it to
the estimator. For some specific distributions of the underlying random
variables, we obtain the optimal solution to the problem of minimizing the
expected value of the sum of communication cost and estimation cost over the
time horizon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00072</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00072</id><created>2015-09-30</created><authors><author><keyname>Mohammad</keyname><forenames>Ahmad Shaban</forenames></author></authors><title>On Boosting the Throughput with Minimal Emitted No. of Molecules for the
  Diffusion-Based Molecular Communication networks: Prospective and Challenges</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is assumed that each nano-robot has a 0.1{\mu}m3 tankage which is far
smaller than the volume of a human red blood cell[6]. Thus, the nano-machines
are non-invasive for biomedical applications, such as drug delivery and disease
diagnosis. Since the nano-machine has an initial source of 108 molecules in its
own tankage and since the throughput of the DMC systems is significantly low;
we propose and discuss some ideas to boost the throughput with emphasize of
reducing the emitted no. of molecules per message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00073</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00073</id><created>2015-09-30</created><authors><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author><author><keyname>Molzahn</keyname><forenames>Daniel K</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Recent Advances in Computational Methods for the Power Flow Equations</title><categories>math.OC cs.CE cs.SY math.AG</categories><comments>13 pages, 2 figures. Submitted to the Tutorial Session at IEEE 2016
  American Control Conference</comments><report-no>ADP-15-35/T937</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power flow equations are at the core of most of the computations for
designing and operating electric power systems. The power flow equations are a
system of multivariate nonlinear equations which relate the power injections
and voltages in a power system. A plethora of methods have been devised to
solve these equations, starting from Newton-based methods to homotopy
continuation and other optimization-based methods. While many of these methods
often efficiently find a high-voltage, stable solution due to its large basin
of attraction, most of the methods struggle to find low-voltage solutions which
play significant role in certain stability-related computations. While we do
not claim to have exhausted the existing literature on all related methods,
this tutorial paper introduces some of the recent advances in methods for
solving power flow equations to the wider power systems community as well as
bringing attention from the computational mathematics and optimization
communities to the power systems problems. After briefly reviewing some of the
traditional computational methods used to solve the power flow equations, we
focus on three emerging methods: the numerical polynomial homotopy continuation
method, Groebner basis techniques, and moment/sum-of-squares relaxations using
semidefinite programming. In passing, we also emphasize the importance of an
upper bound on the number of solutions of the power flow equations and review
the current status of research in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00077</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00077</id><created>2015-09-30</created><authors><author><keyname>Gabbay</keyname><forenames>Dov</forenames></author><author><keyname>Gabbay</keyname><forenames>Michael</forenames></author></authors><title>The Attack as Intuitionistic Negation</title><categories>cs.LO</categories><comments>34 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We translate the argumentation networks ${\cal A}=(S, R)$ into a theory $D$
of intuitionistic logic, retaining $S$ as the domain and using intuitionistic
negation to model the attack $R$ in ${\cal A}$: the attack $xRy$ is translated
to $x\to\neg y$. The intuitionistic models of $D$ characterise the complete
extensions of ${\cal A}$.
  The reduction of argumentation networks to intuitionistic logic yields, in
addition to a representation theorem, some additional benefits: it allows us to
give semantics to higher level attacks, where an attack &quot;$xRy$&quot; can itself
attack another attack &quot;$uRv$&quot;; one can make higher level meta-statements $W$ on
$(S, R)$ and such meta-statements can attack and be attacked in the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00082</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00082</id><created>2015-09-30</created><updated>2015-12-26</updated><authors><author><keyname>Yao</keyname><forenames>Jianping</forenames></author><author><keyname>Feng</keyname><forenames>Suili</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Secure Routing in Multihop Wireless Ad-hoc Networks with
  Decode-and-Forward Relaying</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, 3 tables. Accepted for publication in the IEEE
  Transactions on Communications. It overlaps with the former version
  (arXiv:1510.00082)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of secure routing in a multihop wireless
ad-hoc network in the presence of randomly distributed eavesdroppers.
Specifically, the locations of the eavesdroppers are modeled as a homogeneous
Poisson point process (PPP) and the source-destination pair is assisted by
intermediate relays using the decode-and-forward (DF) strategy. We analytically
characterize the physical layer security performance of any chosen multihop
path using the end-to-end secure connection probability (SCP) for both
colluding and non-colluding eavesdroppers. To facilitate finding an efficient
solution to secure routing, we derive accurate approximations of the SCP. Based
on the SCP approximations, we study the secure routing problem which is defined
as finding the multihop path having the highest SCP. A revised Bellman-Ford
algorithm is adopted to find the optimal path in a distributed manner.
Simulation results demonstrate that the proposed secure routing scheme achieves
nearly the same performance as exhaustive search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00083</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00083</id><created>2015-09-30</created><updated>2016-02-05</updated><authors><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Liu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Coskun</keyname><forenames>Ayse K.</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>Optimizing Energy Storage Participation in Emerging Power Markets</title><categories>cs.SY</categories><comments>The full (longer and extended) version of the paper accepted in IGSC
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing amount of intermittent renewables in power generation creates
challenges for real-time matching of supply and demand in the power grid.
Emerging ancillary power markets provide new incentives to consumers (e.g.,
electrical vehicles, data centers, and others) to perform demand response to
help stabilize the electricity grid. A promising class of potential demand
response providers includes energy storage systems (ESSs). This paper evaluates
the benefits of using various types of novel ESS technologies for a variety of
emerging smart grid demand response programs, such as regulation services
reserves (RSRs), contingency reserves, and peak shaving. We model, formulate
and solve optimization problems to maximize the net profit of ESSs in providing
each demand response. Our solution selects the optimal power and energy
capacities of the ESS, determines the optimal reserve value to provide as well
as the ESS real-time operational policy for program participation. Our results
highlight that applying ultra-capacitors and flywheels in RSR has the potential
to be up to 30 times more profitable than using common battery technologies
such as LI and LA batteries for peak shaving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00086</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00086</id><created>2015-09-30</created><authors><author><keyname>Assadi</keyname><forenames>Sepehr</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Vohra</keyname><forenames>Rakesh</forenames></author></authors><title>Fast Convergence in the Double Oral Auction</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical trading experiment consists of a set of unit demand buyers and
unit supply sellers with identical items. Each agent's value or opportunity
cost for the item is their private information and preferences are
quasi-linear. Trade between agents employs a double oral auction (DOA) in which
both buyers and sellers call out bids or offers which an auctioneer recognizes.
Transactions resulting from accepted bids and offers are recorded. This
continues until there are no more acceptable bids or offers. Remarkably, the
experiment consistently terminates in a Walrasian price. The main result of
this paper is a mechanism in the spirit of the DOA that converges to a
Walrasian equilibrium in a polynomial number of steps, thus providing a
theoretical basis for the above-described empirical phenomenon. It is
well-known that computation of a Walrasian equilibrium for this market
corresponds to solving a maximum weight bipartite matching problem. The
uncoordinated but rational responses of agents thus solve in a distributed
fashion a maximum weight bipartite matching problem that is encoded by their
private valuations. We show, furthermore, that every Walrasian equilibrium is
reachable by some sequence of responses. This is in contrast to the well known
auction algorithms for this problem which only allow one side to make offers
and thus essentially choose an equilibrium that maximizes the surplus for the
side making offers. Our results extend to the setting where not every agent
pair is allowed to trade with each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00087</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00087</id><created>2015-09-30</created><authors><author><keyname>Weller</keyname><forenames>Adrian</forenames></author><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Clamping Improves TRW and Mean Field Approximations</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the effect of clamping variables for approximate inference in
undirected graphical models with pairwise relationships and discrete variables.
For any number of variable labels, we demonstrate that clamping and summing
approximate sub-partition functions can lead only to a decrease in the
partition function estimate for TRW, and an increase for the naive mean field
method, in each case guaranteeing an improvement in the approximation and
bound. We next focus on binary variables, add the Bethe approximation to
consideration and examine ways to choose good variables to clamp, introducing
new methods. We show the importance of identifying highly frustrated cycles,
and of checking the singleton entropy of a variable. We explore the value of
our methods by empirical analysis and draw lessons to guide practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00090</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00090</id><created>2015-09-30</created><authors><author><keyname>Atighehchi</keyname><forenames>Kevin</forenames></author><author><keyname>Ballet</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Bonnecaze</keyname><forenames>Alexis</forenames></author><author><keyname>Rolland</keyname><forenames>Robert</forenames></author></authors><title>On Chudnovsky-Based Arithmetic Algorithms in Finite Fields</title><categories>cs.DM math.AG</categories><msc-class>68R99, 11G20, 14-XX</msc-class><acm-class>G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to a new construction of the so-called Chudnovsky-Chudnovsky
multiplication algorithm, we design efficient algorithms for both the
exponentiation and the multiplication in finite fields. They are tailored to
hardware implementation and they allow computations to be parallelized while
maintaining a low number of bilinear multiplications. We give an example with
the finite field ${\mathbb F}_{16^{13}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00095</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00095</id><created>2015-09-30</created><authors><author><keyname>Li</keyname><forenames>Wenfa</forenames></author><author><keyname>Liu</keyname><forenames>Hongzhe</forenames></author><author><keyname>Yang</keyname><forenames>Peng</forenames></author><author><keyname>Xie</keyname><forenames>Wei</forenames></author></authors><title>Supporting Regularized Logistic Regression Privately and Efficiently</title><categories>cs.LG cs.CR q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of the most popular statistical and machine learning models, logistic
regression with regularization has found wide adoption in biomedicine, social
sciences, information technology, and so on. These domains often involve data
of human subjects that are contingent upon strict privacy regulations.
Increasing concerns over data privacy make it more and more difficult to
coordinate and conduct large-scale collaborative studies, which typically rely
on cross-institution data sharing and joint analysis. Our work here focuses on
safeguarding regularized logistic regression, a widely-used machine learning
model in various disciplines while at the same time has not been investigated
from a data security and privacy perspective. We consider a common use scenario
of multi-institution collaborative studies, such as in the form of research
consortia or networks as widely seen in genetics, epidemiology, social
sciences, etc. To make our privacy-enhancing solution practical, we demonstrate
a non-conventional and computationally efficient method leveraging distributing
computing and strong cryptography to provide comprehensive protection over
individual-level and summary data. Extensive empirical evaluation on several
studies validated the privacy guarantees, efficiency and scalability of our
proposal. We also discuss the practical implications of our solution for
large-scale studies and applications from various disciplines, including
genetic and biomedical studies, smart grid, network analysis, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00098</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00098</id><created>2015-09-30</created><updated>2016-02-27</updated><authors><author><keyname>Xie</keyname><forenames>Michael</forenames></author><author><keyname>Jean</keyname><forenames>Neal</forenames></author><author><keyname>Burke</keyname><forenames>Marshall</forenames></author><author><keyname>Lobell</keyname><forenames>David</forenames></author><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author></authors><title>Transfer Learning from Deep Features for Remote Sensing and Poverty
  Mapping</title><categories>cs.CV cs.CY</categories><comments>In Proc. 30th AAAI Conference on Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lack of reliable data in developing countries is a major obstacle to
sustainable development, food security, and disaster relief. Poverty data, for
example, is typically scarce, sparse in coverage, and labor-intensive to
obtain. Remote sensing data such as high-resolution satellite imagery, on the
other hand, is becoming increasingly available and inexpensive. Unfortunately,
such data is highly unstructured and currently no techniques exist to
automatically extract useful insights to inform policy decisions and help
direct humanitarian efforts. We propose a novel machine learning approach to
extract large-scale socioeconomic indicators from high-resolution satellite
imagery. The main challenge is that training data is very scarce, making it
difficult to apply modern techniques such as Convolutional Neural Networks
(CNN). We therefore propose a transfer learning approach where nighttime light
intensities are used as a data-rich proxy. We train a fully convolutional CNN
model to predict nighttime lights from daytime imagery, simultaneously learning
features that are useful for poverty prediction. The model learns filters
identifying different terrains and man-made structures, including roads,
buildings, and farmlands, without any supervision beyond nighttime lights. We
demonstrate that these learned features are highly informative for poverty
mapping, even approaching the predictive performance of survey data collected
in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00102</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00102</id><created>2015-09-30</created><authors><author><keyname>Coudron</keyname><forenames>Matthew</forenames></author><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author></authors><title>Interactive proofs with approximately commuting provers</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class $\MIP^*$ of promise problems that can be decided through an
interactive proof system with multiple entangled provers provides a
complexity-theoretic framework for the exploration of the nonlocal properties
of entanglement. Little is known about the power of this class. The only
proposed approach for establishing upper bounds is based on a hierarchy of
semidefinite programs introduced independently by Pironio et al. and Doherty et
al. This hierarchy converges to a value that is only known to coincide with the
provers' maximum success probability in a given proof system under a plausible
but difficult mathematical conjecture, Connes' embedding conjecture. No bounds
on the rate of convergence are known.
  We introduce a rounding scheme for the hierarchy, establishing that any
solution to its $N$-th level can be mapped to a strategy for the provers in
which measurement operators associated with distinct provers have pairwise
commutator bounded by $O(\ell^2/\sqrt{N})$ in operator norm, where $\ell$ is
the number of possible answers per prover.
  Our rounding scheme motivates the introduction of a variant of $\MIP^*$,
called $\MIP_\delta^*$, in which the soundness property is required to hold as
long as the commutator of operations performed by distinct provers has norm at
most $\delta$. Our rounding scheme implies the upper bound $\MIP_\delta^*
\subseteq \DTIME(\exp(\exp(\poly)/\delta^2))$. In terms of lower bounds we
establish that $\MIP^*_{2^{-\poly}}$, with completeness $1$ and soundness
$1-2^{-\poly}$, contains $\NEXP$. The relationship of $\MIP_\delta^*$ to
$\MIPstar$ has connections with the mathematical literature on approximate
commutation. Our rounding scheme gives an elementary proof that the Strong
Kirchberg Conjecture implies that $\MIPstar$ is computable. We discuss
applications to device-independent cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00109</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00109</id><created>2015-10-01</created><authors><author><keyname>Elamvazhuthi</keyname><forenames>Karthik</forenames></author><author><keyname>Wilson</keyname><forenames>Sean</forenames></author><author><keyname>Berman</keyname><forenames>Spring</forenames></author></authors><title>Confinement Control of Double Integrators using Partially Periodic
  Leader Trajectories</title><categories>cs.MA cs.SY math.OC</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-agent confinement control problem in which a single
leader has a purely repulsive effect on follower agents with double-integrator
dynamics. By decomposing the leader's control inputs into periodic and
aperiodic components, we show that the leader can be driven so as to guarantee
confinement of the followers about a time-dependent trajectory in the plane. We
use tools from averaging theory and an input-to-state stability type argument
to derive conditions on the model parameters that guarantee confinement of the
followers about the trajectory. For the case of a single follower, we show that
if the follower starts at the origin, then the error in trajectory tracking can
be made arbitrarily small depending on the frequency of the periodic control
components and the rate of change of the trajectory. We validate our approach
using simulations and experiments with a small mobile robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00112</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00112</id><created>2015-10-01</created><updated>2015-10-29</updated><authors><author><keyname>Dowty</keyname><forenames>James G.</forenames></author></authors><title>Higher-order asymptotics for the parametric complexity</title><categories>cs.IT math.IT stat.ME stat.ML</categories><comments>Version 3: Fixed a minor error in the introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parametric complexity is the key quantity in the minimum description
length (MDL) approach to statistical model selection. Rissanen and others have
shown that the parametric complexity of a statistical model approaches a simple
function of the Fisher information volume of the model as the sample size $n$
goes to infinity. This paper derives higher-order asymptotic expansions for the
parametric complexity, in the case of exponential families and independent and
identically distributed data. These higher-order approximations are calculated
for some examples and are shown to have better finite-sample behaviour than
Rissanen's approximation. The higher-order terms are given as expressions
involving cumulants (or, more naturally, the Amari-Chentsov tensors), and these
terms are likely to be interesting in themselves since they arise naturally
from the general information-theoretic principles underpinning MDL. The
derivation given here specializes to an alternative and arguably simpler proof
of Rissanen's result (for the case considered here), proving for the first time
that his approximation is $O(n^{-1})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00115</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00115</id><created>2015-10-01</created><authors><author><keyname>Gunawan</keyname><forenames>Andreas D. M.</forenames></author><author><keyname>Zhang</keyname><forenames>Louxin</forenames></author></authors><title>Bounding the Size of a Network Defined By Visibility Property</title><categories>q-bio.PE cs.DM</categories><comments>23 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phylogenetic networks are mathematical structures for modeling and
visualization of reticulation processes in the study of evolution. Galled
networks, reticulation visible networks, nearly-stable networks and
stable-child networks are the four classes of phylogenetic networks that are
recently introduced to study the topological and algorithmic aspects of
phylogenetic networks. We prove the following results.
  (1) A binary galled network with n leaves has at most 2(n-1) reticulation
nodes. (2) A binary nearly-stable network with n leaves has at most 3(n-1)
reticulation nodes. (3) A binary stable-child network with n leaves has at most
7(n-1) reticulation nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00116</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00116</id><created>2015-10-01</created><authors><author><keyname>Goel</keyname><forenames>Seep</forenames></author><author><keyname>Aggarwal</keyname><forenames>Pooja</forenames></author><author><keyname>Sarangi</keyname><forenames>Smruti R.</forenames></author></authors><title>A Wait-Free Stack</title><categories>cs.DC</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a novel algorithm to create a con- current
wait-free stack. To the best of our knowledge, this is the first wait-free
algorithm for a general purpose stack. In the past, researchers have proposed
restricted wait-free implementations of stacks, lock-free implementations, and
efficient universal constructions that can support wait-free stacks. The crux
of our wait-free implementation is a fast pop operation that does not modify
the stack top; instead, it walks down the stack till it finds a node that is
unmarked. It marks it but does not delete it. Subsequently, it is lazily
deleted by a cleanup operation. This operation keeps the size of the stack in
check by not allowing the size of the stack to increase beyond a factor of W as
compared to the actual size. All our operations are wait-free and linearizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00132</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00132</id><created>2015-10-01</created><authors><author><keyname>Hushchyn</keyname><forenames>Mikhail</forenames></author><author><keyname>Charpentier</keyname><forenames>Philippe</forenames></author><author><keyname>Ustyuzhanin</keyname><forenames>Andrey</forenames></author></authors><title>Disk storage management for LHCb based on Data Popularity estimator</title><categories>cs.DC cs.LG physics.data-an</categories><doi>10.1088/1742-6596/664/4/042026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm providing recommendations for optimizing the
LHCb data storage. The LHCb data storage system is a hybrid system. All
datasets are kept as archives on magnetic tapes. The most popular datasets are
kept on disks. The algorithm takes the dataset usage history and metadata
(size, type, configuration etc.) to generate a recommendation report. This
article presents how we use machine learning algorithms to predict future data
popularity. Using these predictions it is possible to estimate which datasets
should be removed from disk. We use regression algorithms and time series
analysis to find the optimal number of replicas for datasets that are kept on
disk. Based on the data popularity and the number of replicas optimization, the
algorithm minimizes a loss function to find the optimal data distribution. The
loss function represents all requirements for data distribution in the data
storage system. We demonstrate how our algorithm helps to save disk space and
to reduce waiting times for jobs using this data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00140</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00140</id><created>2015-10-01</created><authors><author><keyname>Bre&#x161;ar</keyname><forenames>Bo&#x161;tjan</forenames></author><author><keyname>Dorbec</keyname><forenames>Paul</forenames></author><author><keyname>Klav&#x17e;ar</keyname><forenames>Sandi</forenames></author><author><keyname>Ko&#x161;mrlj</keyname><forenames>Ga&#x161;per</forenames></author><author><keyname>Renault</keyname><forenames>Gabriel</forenames></author></authors><title>Complexity of the Game Domination Problem</title><categories>math.CO cs.CC</categories><msc-class>05C57, 68Q15, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game domination number is a graph invariant that arises from a game,
which is related to graph domination in a similar way as the game chromatic
number is related to graph coloring. In this paper we show that verifying
whether the game domination number of a graph is bounded by a given integer is
PSPACE-complete. This contrasts the situation of the game coloring problem
whose complexity is still unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00143</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00143</id><created>2015-10-01</created><updated>2016-02-02</updated><authors><author><keyname>Zhao</keyname><forenames>Ningning</forenames></author><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Basarab</keyname><forenames>Adrian</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Kouame</keyname><forenames>Denis</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Fast Single Image Super-Resolution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of single image super-resolution (SR), which
consists of recovering a high resolution image from its blurred, decimated and
noisy version. The existing algorithms for single image SR use different
strategies to handle the decimation and blurring operators. In addition to the
traditional first-order gradient methods, recent techniques investigate
splitting-based methods dividing the SR problem into up-sampling and
deconvolution steps that can be easily solved. Instead of following this
splitting strategy, we propose to deal with the decimation and blurring
operators simultaneously by taking advantage of their particular properties in
the frequency domain, leading to a new fast SR approach. Specifically, an
analytical solution can be obtained and implemented efficiently for the
Gaussian prior or any other regularization that can be formulated into an
l2-regularized quadratic model. Furthermore, the flexibility of the proposed SR
scheme is shown through the use of various priors/regularizations, ranging from
generic image priors to learning-based approaches. In the case of non-Gaussian
priors, we show how the analytical solution derived from the Gaussian case can
be embedded into traditional splitting frameworks, allowing the computation
cost of existing algorithms to be decreased significantly. Simulation results
conducted on several images with different priors illustrate the effectiveness
of our fast SR approach compared with the existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00149</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00149</id><created>2015-10-01</created><updated>2016-02-15</updated><authors><author><keyname>Han</keyname><forenames>Song</forenames></author><author><keyname>Mao</keyname><forenames>Huizi</forenames></author><author><keyname>Dally</keyname><forenames>William J.</forenames></author></authors><title>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained
  Quantization and Huffman Coding</title><categories>cs.CV cs.NE</categories><comments>Published as a conference paper at ICLR 2016 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems with limited hardware
resources. To address this limitation, we introduce &quot;deep compression&quot;, a three
stage pipeline: pruning, trained quantization and Huffman coding, that work
together to reduce the storage requirement of neural networks by 35x to 49x
without affecting their accuracy. Our method first prunes the network by
learning only the important connections. Next, we quantize the weights to
enforce weight sharing, finally, we apply Huffman coding. After the first two
steps we retrain the network to fine tune the remaining connections and the
quantized centroids. Pruning, reduces the number of connections by 9x to 13x;
Quantization then reduces the number of bits that represent each connection
from 32 to 5. On the ImageNet dataset, our method reduced the storage required
by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method
reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of
accuracy. This allows fitting the model into on-chip SRAM cache rather than
off-chip DRAM memory. Our compression method also facilitates the use of
complex neural networks in mobile applications where application size and
download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,
compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00165</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00165</id><created>2015-10-01</created><authors><author><keyname>Schweizer</keyname><forenames>Daniel</forenames></author><author><keyname>Zehnder</keyname><forenames>Michael</forenames></author><author><keyname>Wache</keyname><forenames>Holger</forenames></author><author><keyname>Witschel</keyname><forenames>Hans-Friedrich</forenames></author><author><keyname>Zanatta</keyname><forenames>Danilo</forenames></author><author><keyname>Rodriguez</keyname><forenames>Miguel</forenames></author></authors><title>Using consumer behavior data to reduce energy consumption in smart homes</title><categories>cs.CY stat.ML</categories><comments>To be presented at IEEE International Conference of Machine Learning
  and Applications (ICMLA, Dec. 2015). arXiv admin note: text overlap with
  arXiv:1509.05722</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses how usage patterns and preferences of inhabitants can be
learned efficiently to allow smart homes to autonomously achieve energy
savings. We propose a frequent sequential pattern mining algorithm suitable for
real-life smart home event data. The performance of the proposed algorithm is
compared to existing algorithms regarding completeness/correctness of the
results, run times as well as memory consumption and elaborates on the
shortcomings of the different solutions. We also present a recommender system
based on the developed algorithm that provides recommendations to the users to
reduce their energy consumption. The recommender system was deployed to a set
of test homes. The test participants rated the impact of the recommendations on
their comfort. We used this feedback to adjust the system parameters and make
it more accurate during a second test phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00177</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00177</id><created>2015-10-01</created><authors><author><keyname>Kari</keyname><forenames>Jarkko</forenames></author><author><keyname>Szabados</keyname><forenames>Michal</forenames></author></authors><title>An Algebraic Geometric Approach to Nivat's Conjecture</title><categories>cs.DM math.CO</categories><comments>ICALP 2015, Kyoto, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study multidimensional configurations (infinite words) and subshifts of
low pattern complexity using tools of algebraic geometry. We express the
configuration as a multivariate formal power series over integers and
investigate the setup when there is a non-trivial annihilating polynomial: a
non-zero polynomial whose formal product with the power series is zero. Such
annihilator exists, for example, if the number of distinct patterns of some
finite shape D in the configuration is at most the size |D| of the shape. This
is our low pattern complexity assumption. We prove that the configuration must
be a sum of periodic configurations over integers, possibly with unbounded
values. As a specific application of the method we obtain an asymptotic version
of the well-known Nivat's conjecture: we prove that any two-dimensional,
non-periodic configuration can satisfy the low pattern complexity assumption
with respect to only finitely many distinct rectangular shapes D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00184</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00184</id><created>2015-10-01</created><authors><author><keyname>Mirkin</keyname><forenames>Leonid</forenames></author></authors><title>Intermittent Redesign of Analog Controllers via the Youla Parameter</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies digital redesign of linear time-invariant analog
controllers under intermittent sampling. It proves, constructively, that every
stabilizing analog controller can be redesigned to preserve closed-loop
stability under any sampling pattern with uniformly bounded sampling intervals.
Performance-preserving schemes are also proposed and the optimality of the
uniform sampling, under a fixed sampling density, is proved in both the $H^2$
and the $H^\infty$ cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00203</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00203</id><created>2015-10-01</created><authors><author><keyname>Alampay</keyname><forenames>R.</forenames></author><author><keyname>Teknomo</keyname><forenames>K.</forenames></author></authors><title>Data Association for an Adaptive Multi-target Particle Filter Tracking
  System</title><categories>cs.CV</categories><acm-class>I.5.4</acm-class><journal-ref>Philippine Computing Journal Vol 7 (1) August 2012, p. 16-25</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to improve the accuracy of tracking
multiple objects in a static scene using a particle filter system by
introducing a data association step, a state queue for the collection of
tracked objects and adaptive parameters to the system. The data association
step makes use of the object detection phase and appearance model to determine
if the approximated targets given by the particle filter step match the given
set of detected objects. The remaining detected objects are used as information
to instantiate new objects for tracking. State queues are also used for each
tracked object to deal with occlusion events and occlusion recovery. Finally we
present how the parameters adjust to occlusion events. The adaptive property of
the system is also used for possible occlusion recovery. Results of the system
are then compared to a ground truth data set for performance evaluation. Our
system produced accurate results and was able to handle partially occluded
objects as well as proper occlusion recovery from tracking multiple objects
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00208</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00208</id><created>2015-10-01</created><updated>2015-10-05</updated><authors><author><keyname>F&#xfc;zesdi</keyname><forenames>Mark</forenames></author></authors><title>Boolean-type Retractable State-finite Automata Without Outputs</title><categories>cs.FL</categories><comments>12 pages</comments><msc-class>68Q70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automaton $\bf A$ is called a retractable automaton if, for every
subautomaton $\bf B$ of $\bf A$, there is at least one homomorphism of $\bf A$
onto $\bf B$ which leaves the elements of $B$ fixed (such homomorphism is
called a retract homomorphism of $\bf A$ onto $\bf B$). We say that a
retractable automaton ${\bf A}$=(A,X,$\delta$) is Boolean-type if there exists
a family $\{\lambda_B \mid \textrm{ B is a subautomaton of A } \}$ of retract
homomorphisms $\lambda _B$ of $\bf A$ such that, for arbitrary subautomata
${\bf B}_1$ and ${\bf B}_2$ of $\bf A$, the condition $B_1\subseteq B_2$
implies $Ker\lambda _{B_2}\subseteq Ker\lambda _{B_1}$. In this paper we
describe the Boolean-type retractable state-finite automata without outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00215</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00215</id><created>2015-10-01</created><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author></authors><title>FPT Approximation Schemes for Maximizing Submodular Functions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the existence of approximation algorithms for maximization of
submodular functions, that run in fixed parameter tractable (FPT) time. Given a
non-decreasing submodular set function $v: 2^X \to \mathbb{R}$ the goal is to
select a subset $S$ of $K$ elements from $X$ such that $v(S)$ is maximized. We
identify three properties of set functions, referred to as $p$-separability
properties, and we argue that many real-life problems can be expressed as
maximization of submodular, $p$-separable functions, with low values of the
parameter $p$. We present FPT approximation schemes for the minimization and
maximization variants of the problem, for several parameters that depend on
characteristics of the optimized set function, such as $p$ and $K$. We confirm
that our algorithms are applicable to a broad class of problems, in particular
to problems from computational social choice, such as item selection or winner
determination under several multiwinner election systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00216</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00216</id><created>2015-10-01</created><authors><author><keyname>Williams</keyname><forenames>Ashley</forenames></author></authors><title>A comparison of the performance and scalability of relational and
  document-based web-systems for large scale applications in a rehabilitation
  context</title><categories>cs.CY</categories><comments>Unpublished MSc thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The Virtual Rehabilitation Environment (VRE) provides patients of
long term neurological conditions with a platform to review their previous
physiotherapy sessions, as well as see their goals and any treatments or
exercises that their clinician has set for them to practice before their next
session.
  Objective: The initial application implemented 21 of the 27 core features
using the Microsoft ASP.NET MVC stack. However, the two core, non-functional
requirements were negated from the project due to lack of experience and strict
time constraints. This project aimed to investigate whether the application
would be more suited to a non-relational solution.
  Method: The application was re-written using the MEAN stack (MongoDB,
ExpressJS, AngularJS, NodeJS), an open source, fully JavaScript stack and then
performance tests were carried out to compare the two applications. A
scalability review was also conducted to assess the benefits and drawbacks of
each technology in this aspect.
  Results: The investigation proved that the non-relational solution was much
more efficient and performed faster. However, the choice of database was only a
small part of the increase in efficiency and it was an all-round better design
that gave the new application its performance upper hand.
  Conclusion: A proposal for a new application design is given that follows the
microservice architecture used by companies such as Amazon and Netflix. The
application is to be split up into four parts; database, client application,
server application and content delivery network. These four, independently
scalable and manageable services offer the greatest flexibility for future
development at the low costs necessary for a start-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00217</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00217</id><created>2015-10-01</created><authors><author><keyname>Aoki</keyname><forenames>Takaaki</forenames></author><author><keyname>Rocha</keyname><forenames>Luis E. C.</forenames></author><author><keyname>Gross</keyname><forenames>Thilo</forenames></author></authors><title>Temporal and structural heterogeneities emerging in adaptive temporal
  networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model of adaptive temporal networks whose evolution is
regulated by an interplay between node activity and dynamic exchange of
information through links. We study the model by using a master equation
approach. Starting from a homogeneous initial configuration, we show that
temporal and structural heterogeneities, characteristic of real-world networks,
spontaneously emerge. This theoretically tractable model thus contributes to
the understanding of the dynamics of human activity and interaction networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00220</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00220</id><created>2015-10-01</created><authors><author><keyname>Chandran</keyname><forenames>Sandeep</forenames></author><author><keyname>Peter</keyname><forenames>Eldhose</forenames></author><author><keyname>Panda</keyname><forenames>Preeti Ranjan</forenames></author><author><keyname>Sarangi</keyname><forenames>Smruti R.</forenames></author></authors><title>Fundamental Results for a Generic Implementation of Barriers using
  Optical Interconnects</title><categories>cs.DC</categories><comments>3 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we report some fundamental results and bounds on the number
of messages and storage required to implement barriers using futuristic on-chip
optical and RF networks. We prove that it is necessary to maintain a count to
at least N (number of threads) in memory, broadcast the barrier id at least
once, and if we elect a co-ordinator, we can reduce the number of messages by a
factor of O(N ).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00225</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00225</id><created>2015-10-01</created><authors><author><keyname>Lauras</keyname><forenames>Matthieu</forenames><affiliation>UL2</affiliation></author><author><keyname>Benaben</keyname><forenames>Frederick</forenames><affiliation>UL2</affiliation></author><author><keyname>Truptil</keyname><forenames>Sebastien</forenames><affiliation>UL2</affiliation></author><author><keyname>Charles</keyname><forenames>Aurelie</forenames><affiliation>UL2</affiliation></author></authors><title>Event-Cloud Platform to Support Decision- Making in Emergency Management</title><categories>cs.CY</categories><proxy>ccsd</proxy><journal-ref>Information Systems Frontiers, Springer Verlag (Germany), 2015, 17
  (4), pp.857-869</journal-ref><doi>10.1007/s10796-013-9475-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of this paper is to underline the capability of an Event-Cloud
Platform to support efficiently an emergency situation. We chose to focus on a
nuclear crisis use case. The proposed approach consists in modeling the
business processes of crisis response on the one hand, and in supporting the
orchestration and execution of these processes by using an Event-Cloud Platform
on the other hand. This paper shows how the use of Event-Cloud techniques can
support crisis management stakeholders by automatizing non-value added tasks
and by directing decision- makers on what really requires their capabilities of
choice. If Event-Cloud technology is a very interesting and topical subject,
very few research works have considered this to improve emergency management.
This paper tries to fill this gap by considering and applying these
technologies on a nuclear crisis use-case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00226</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00226</id><created>2015-10-01</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Jan</keyname><forenames>Zahoor</forenames></author><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Khan</keyname><forenames>Zahid</forenames></author></authors><title>An Adaptive Secret Key-directed Cryptographic Scheme for Secure
  Transmission in Wireless Sensor Networks</title><categories>cs.CR</categories><comments>A short paper of 6 pages, proposing an adaptive cryptographic
  technique for secure transmission in WSN. The original paper can be found on
  this link at page 48.
  http://web.uettaxila.edu.pk/techJournal/2015/No3/TECHNICAL_JOURNAL_VOL_20_NO_3.pdf</comments><journal-ref>Technical Journal, University of Engineering and Technology
  Taxila, Pakistan, vol.20, pp.48-53, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are memory and bandwidth limited networks
whose main goals are to maximize the network lifetime and minimize the energy
consumption and transmission cost. To achieve these goals, dif ferent
techniques of compression and clustering have been used. However, security is
an open and major issue in WSNs for which different approaches are used, both
in centralized and distributed WSNs' environments. This paper presents an
adaptive cryptographic scheme for secure transmission of various sensitive
parameters, sensed by wireless sensors to the fusion center for further
processing in WSNs such as military networks. The proposed method encrypts the
sensitive captured data of sensor nodes using various encryption procedures
(bitxor operation, bits shuffling, and secret key based encryption) and then
sends it to the fusion center. At the fusion center, the received encrypted
data is decrypted for taking further necessary actions. The experimental
results with complexity analysis, validate the effectiveness and feasibility of
the proposed method in terms of security in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00229</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00229</id><created>2015-10-01</created><authors><author><keyname>Yang</keyname><forenames>Jiannan</forenames></author><author><keyname>Wang</keyname><forenames>Hanpin</forenames></author><author><keyname>Cao</keyname><forenames>Yongzhi</forenames></author></authors><title>Making problems tractable on big data via preprocessing with
  polylog-size output</title><categories>cs.CC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To provide a dichotomy between those queries that can be made feasible on big
data after appropriate preprocessing and those for which preprocessing does not
help, Fan et al. developed the $\sqcap$-tractability theory. This theory
provides a formal foundation for understanding the tractability of query
classes in the context of big data. Along this line, we introduce a novel
notion of $\sqcap'$-tractability in this paper. Inspired by some technologies
used to deal big data, we place a restriction on preprocessing function, which
limits the function to produce a relatively small database as output, at most
polylog-size of the input database. At the same time, we bound the redundancy
information when re-factorizing data and queries for preprocessing. These
changes aim to make our theory more closely linked to practice. We set two
complexity classes to denote the classes of Boolean queries that are
$\sqcap'$-tractable themselves and that can be made $\sqcap'$-tractable,
respectively. Based on a new factorization in our complexity classes, we
investigate two reductions, which differ from whether allowing re-factorizing
data and query parts. We verify the transitive and compatible properties of the
reductions and analysis the complete problems and sizes of the complexity
classes. We conclude that all PTIME classes of Boolean queries can be made
$\sqcap'$-tractable, similar to that of the $\sqcap$-tractability theory. With
a little surprise, we prove that the set of all $\sqcap'$-tractable queries is
strictly smaller than that of all $\sqcap$-tractable queries, and thus the set
of $\sqcap'$-tractable queries is properly contained in that of PTIME queries.
In this way, we attain a new complexity class inside the complexity class of
PTIME queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00240</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00240</id><created>2015-10-01</created><authors><author><keyname>Potapova</keyname><forenames>Rodmonga</forenames></author><author><keyname>Gordeev</keyname><forenames>Denis</forenames></author></authors><title>Determination of the Internet Anonymity Influence on the Level of
  Aggression and Usage of Obscene Lexis</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the analysis of the semantic content of the anonymous
Russian-speaking forum 2ch.hk, different verbal means of expressing of the
emotional state of aggression are revealed for this site, and aggression is
classified by its directions. The lexis of different Russian-and English-
speaking anonymous forums (2ch.hk and iichan.hk, 4chan.org) and public
community &quot;MDK&quot; of the Russian-speaking social network VK is analyzed and
compared with the Open Corpus of the Russian language (Opencorpora.org and
Brown corpus). The analysis shows that anonymity has no influence on the amount
of invective items usage. The effectiveness of moderation was shown for
anonymous forums. It was established that Russian obscene lexis was used to
express the emotional state of aggression only in 60.4% of cases for 2ch.hk.
These preliminary results show that the Russian obscene lexis on the Internet
does not have direct dependence on the emotional state of aggression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00244</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00244</id><created>2015-10-01</created><authors><author><keyname>Kerdjoudj</keyname><forenames>Fadhela</forenames></author><author><keyname>Cur&#xe9;</keyname><forenames>Olivier</forenames></author></authors><title>RDF Knowledge Graph Visualization From a Knowledge Extraction System</title><categories>cs.HC cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a system to visualize RDF knowledge graphs. These
graphs are obtained from a knowledge extraction system designed by
GEOLSemantics. This extraction is performed using natural language processing
and trigger detection. The user can visualize subgraphs by selecting some
ontology features like concepts or individuals. The system is also
multilingual, with the use of the annotated ontology in English, French, Arabic
and Chinese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00245</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00245</id><created>2015-10-01</created><authors><author><keyname>Casad</keyname><forenames>Madeleine</forenames></author><author><keyname>Rieger</keyname><forenames>Oya Y.</forenames></author><author><keyname>Alexander</keyname><forenames>Desiree</forenames></author></authors><title>Enduring Access to Rich Media Content: Understanding Use and Usability
  Requirements</title><categories>cs.DL cs.HC</categories><journal-ref>D-Lib Magazine, Volume 21, Number 9/10, September/October 2015</journal-ref><doi>10.1045/september2015-casad</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through an NEH-funded initiative, Cornell University Library is creating a
technical, curatorial, and managerial framework for preserving access to
complex born-digital new media objects. The Library's Rose Goldsen Archive of
New Media Art provides the testbed for this project. This collection of complex
interactive born-digital artworks are used by students, faculty, and artists
from various disciplines. Interactive digital assets are far more complex to
preserve and manage than single uniform digital media files. The preservation
model developed will apply not merely to new media artworks, but to other rich
digital media environments. This article describes the project's findings and
discoveries, focusing on a user survey conducted with the aim of creating user
profiles and use cases for born-digital assets like those in the testbed
collection. The project's ultimate goal is to create a preservation and access
practice grounded in thorough and practical understanding of the
characteristics of digital objects and their access requirements, seen from the
perspectives of collection curators and users alike. We discuss how the survey
findings informed the development of an artist questionnaire to support
creation of user-centric and cost-efficient preservation strategies. Although
this project focuses on new media art, our methodologies and findings will
inform other kinds of complex born-digital collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00249</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00249</id><created>2015-10-01</created><authors><author><keyname>Maity</keyname><forenames>Suman Kalyan</forenames></author><author><keyname>Saraf</keyname><forenames>Ritvik</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>#Bieber + #Blast = #BieberBlast: Early Prediction of Popular Hashtag
  Compounds</title><categories>cs.SI</categories><comments>14 pages, 4 figures, 9 tables, published in CSCW (Computer-Supported
  Cooperative Work and Social Computing) 2016. in Proceedings of 19th ACM
  conference on Computer-Supported Cooperative Work and Social Computing (CSCW
  2016)</comments><acm-class>H.4; J.4</acm-class><doi>10.1145/2818048.2820019</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Compounding of natural language units is a very common phenomena. In this
paper, we show, for the first time, that Twitter hashtags which, could be
considered as correlates of such linguistic units, undergo compounding. We
identify reasons for this compounding and propose a prediction model that can
identify with 77.07% accuracy if a pair of hashtags compounding in the near
future (i.e., 2 months after compounding) shall become popular. At longer times
T = 6, 10 months the accuracies are 77.52% and 79.13% respectively. This
technique has strong implications to trending hashtag recommendation since
newly formed hashtag compounds can be recommended early, even before the
compounding has taken place. Further, humans can predict compounds with an
overall accuracy of only 48.7% (treated as baseline). Notably, while humans can
discriminate the relatively easier cases, the automatic framework is successful
in classifying the relatively harder cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00252</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00252</id><created>2015-10-01</created><authors><author><keyname>Kwon</keyname><forenames>Taehoon</forenames></author><author><keyname>Lim</keyname><forenames>Yeon-Geun</forenames></author><author><keyname>Min</keyname><forenames>Byung-Wook</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>RF Lens-Embedded Massive MIMO Systems: Fabrication Issues and Codebook
  Design</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a radio frequency (RF) lens-embedded massive
multiple-input multiple-output (MIMO) system and evaluate the performance of
limited feedback by utilizing a technique for generating a suitable codebook
for the system. We fabricate an RF lens that operates at a 77 GHz (mmWave)
band. Experimental results show a proper value of amplitude gain and an
appropriate focusing property. In addition, using a simple numerical
method---beam propagation method (BPM)---we estimate the power profile of the
RF lens and verify its accordance with experimental results. Numerical results
confirm that the proposed system shows significant performance enhancement over
a conventional massive MIMO system without an RF lens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00258</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00258</id><created>2015-09-29</created><updated>2016-01-10</updated><authors><author><keyname>Ellis</keyname><forenames>David</forenames></author><author><keyname>Friedgut</keyname><forenames>Ehud</forenames></author><author><keyname>Kindler</keyname><forenames>Guy</forenames></author><author><keyname>Yehudayoff</keyname><forenames>Amir</forenames></author></authors><title>Geometric stability via information theory</title><categories>math.MG cs.IT math.CO math.IT</categories><comments>27 pages. Our stability result for the Uniform Cover inequality now
  has \Theta(\epsilon)-dependence, which is best-possible; the proof is also a
  little shortened</comments><msc-class>52C07, 05D99</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Loomis-Whitney inequality, and the more general Uniform Cover inequality,
bound the volume of a body in terms of a product of the volumes of
lower-dimensional projections of the body. In this paper, we prove stability
versions of these inequalities, showing that when they are close to being
tight, the body in question is close in symmetric difference to a 'box'. Our
results are best possible up to a constant factor depending upon the dimension
alone. Our approach is information theoretic.
  We use our stability result for the Loomis-Whitney inequality to obtain a
stability result for the edge-isoperimetric inequality in the infinite
$d$-dimensional lattice. Namely, we prove that a subset of $\mathbb{Z}^d$ with
small edge-boundary must be close in symmetric difference to a $d$-dimensional
cube. Our bound is, again, best possible up to a constant factor depending upon
$d$ alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00259</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00259</id><created>2015-10-01</created><updated>2015-12-03</updated><authors><author><keyname>Hyland</keyname><forenames>Stephanie L.</forenames></author><author><keyname>Karaletsos</keyname><forenames>Theofanis</forenames></author><author><keyname>R&#xe4;tsch</keyname><forenames>Gunnar</forenames></author></authors><title>A Generative Model of Words and Relationships from Multiple Sources</title><categories>cs.CL cs.LG stat.ML</categories><comments>8 pages, 5 figures; incorporated feedback from reviewers; to appear
  in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural language models are a powerful tool to embed words into semantic
vector spaces. However, learning such models generally relies on the
availability of abundant and diverse training examples. In highly specialised
domains this requirement may not be met due to difficulties in obtaining a
large corpus, or the limited range of expression in average use. Such domains
may encode prior knowledge about entities in a knowledge base or ontology. We
propose a generative model which integrates evidence from diverse data sources,
enabling the sharing of semantic information. We achieve this by generalising
the concept of co-occurrence from distributional semantics to include other
relationships between entities or words, which we model as affine
transformations on the embedding space. We demonstrate the effectiveness of
this approach by outperforming recent models on a link prediction task and
demonstrating its ability to profit from partially or fully unobserved data
training labels. We further demonstrate the usefulness of learning from
different data sources with overlapping vocabularies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00266</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00266</id><created>2015-10-01</created><authors><author><keyname>Sch&#xfc;ldt</keyname><forenames>Christian</forenames></author><author><keyname>H&#xe4;ndel</keyname><forenames>Peter</forenames></author></authors><title>Noise robust integration for blind and non-blind reverberation time
  estimation</title><categories>cs.SD</categories><comments>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brisbane, Australia, April 19-24, 2015</comments><doi>10.1109/ICASSP.2015.7177931</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of the decay rate of a signal section is an integral component
of both blind and non-blind reverberation time estimation methods. Several
decay rate estimators have previously been proposed, based on, e.g., linear
regression and maximum-likelihood estimation. Unfortunately, most approaches
are sensitive to background noise, and/or are fairly demanding in terms of
computational complexity. This paper presents a low complexity decay rate
estimator, robust to stationary noise, for reverberation time estimation.
Simulations using artificial signals, and experiments with speech in
ventilation noise, demonstrate the performance and noise robustness of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00268</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00268</id><created>2015-10-01</created><authors><author><keyname>Mousa</keyname><forenames>Amr El-Desoky</forenames></author><author><keyname>Marchi</keyname><forenames>Erik</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>The ICSTM+TUM+UP Approach to the 3rd CHIME Challenge: Single-Channel
  LSTM Speech Enhancement with Multi-Channel Correlation Shaping
  Dereverberation and LSTM Language Models</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our contribution to the 3rd CHiME Speech Separation and
Recognition Challenge. Our system uses Bidirectional Long Short-Term Memory
(BLSTM) Recurrent Neural Networks (RNNs) for Single-channel Speech Enhancement
(SSE). Networks are trained to predict clean speech as well as noise features
from noisy speech features. In addition, the system applies two methods of
dereverberation on the 6-channel recordings of the challenge. The first is the
Phase-Error based Filtering (PEF) that uses time-varying phase-error filters
based on estimated time-difference of arrival of the speech source and the
phases of the microphone signals. The second is the Correlation Shaping (CS)
that applies a reduction of the long-term correlation energy in reverberant
speech. The Linear Prediction (LP) residual is processed to suppress the
long-term correlation. Furthermore, the system employs a LSTM Language Model
(LM) to perform N-best rescoring of recognition hypotheses. Using the proposed
methods, an improved Word Error Rate (WER) of 24.38% is achieved over the real
eval test set. This is around 25% relative improvement over the challenge
baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00277</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00277</id><created>2015-10-01</created><authors><author><keyname>Gerlach</keyname><forenames>Martin</forenames></author><author><keyname>Font-Clos</keyname><forenames>Francesc</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author></authors><title>On the similarity of symbol frequency distributions with heavy tails</title><categories>physics.soc-ph cs.CL physics.data-an</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantifying the similarity between symbolic sequences is a traditional
problem in Information Theory which requires comparing the frequencies of
symbols in different sequences. In numerous modern applications, ranging from
DNA over music to texts, the distribution of symbol frequencies is
characterized by heavy-tailed distributions (e.g., Zipf's law). The large
number of low-frequency symbols in these distributions poses major difficulties
to the estimation of the similarity between sequences, e.g., they hinder an
accurate finite-size estimation of entropies. Here we show how the accuracy of
estimations depend on the sample size~$N$, not only for the Shannon entropy
$(\alpha=1)$ and its corresponding similarity measures (e.g., the Jensen-Shanon
divergence) but also for measures based on the generalized entropy of order
$\alpha$. For small $\alpha$'s, including $\alpha=1$, the bias and fluctuations
in the estimations decay slower than the $1/N$ decay observed in short-tailed
distributions. For $\alpha$ larger than a critical value $\alpha^*\leq 2$, the
$1/N$-scaling is recovered. We show the practical significance of our results
by quantifying the evolution of the English language over the last two
centuries using a complete $\alpha$-spectrum of measures. We find that frequent
words change more slowly than less frequent words and that $\alpha=2$ provides
the most robust measure to quantify language change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00293</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00293</id><created>2015-10-01</created><authors><author><keyname>Guo</keyname><forenames>Chuan</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author></authors><title>A tight bound on the size of certain separating hash families</title><categories>cs.IT cs.DM math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new lower bound on the size of separating hash
families of type {w_1^{q-1},w_2} where w_1 &lt; w_2. Our result extends the paper
by Guo et al. on binary frameproof codes. This bound compares well against
known general bounds, and is especially useful when trying to bound the size of
strong separating hash families. We also show that our new bound is tight by
constructing hash families that meet the new bound with equality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00295</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00295</id><created>2015-10-01</created><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Vetta</keyname><forenames>Adrian</forenames></author></authors><title>Welfare and Rationality Guarantees for the Simultaneous Multiple-Round
  Ascending Auction</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simultaneous multiple-round auction (SMRA) and the combinatorial clock
auction (CCA) are the two primary mechanisms used to sell bandwidth. Under
truthful bidding, the SMRA is known to output a Walrasian equilibrium that
maximizes social welfare provided the bidder valuation functions satisfy the
gross substitutes property. Recently, it was shown that the combinatorial clock
auction (CCA) provides good welfare guarantees for general classes of valuation
functions. This motivates the question of whether similar welfare guarantees
hold for the SMRA in the case of general valuation functions.
  We show the answer is no. But we prove that good welfare guarantees still
arise if the degree of complementarities in the bidder valuations are bounded.
In particular, if bidder valuations functions are $\alpha$-near-submodular
then, under truthful bidding, the SMRA has a welfare ratio (the worst case
ratio between the social welfare of the optimal allocation and the auction
allocation) of at most $(1+\alpha)$. The special case of submodular valuations,
namely $\alpha=1$, and produces individually rational solutions. However, for
$\alpha&gt;1$, this is a bicriteria guarantee, to obtain good welfare under
truthful bidding requires relaxing individual rationality.
  Finally, we examine what strategies are required to ensure individual
rationality in the SMRA with general valuation functions. First, we provide a
weak characterization, namely \emph{secure bidding}, for individual
rationality. We then show that if the bidders use a profit-maximizing secure
bidding strategy the welfare ratio is at most $1+\alpha$. Consequently, by
bidding securely, it is possible to obtain the same welfare guarantees as
truthful bidding without the loss of individual rationality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00297</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00297</id><created>2015-10-01</created><updated>2016-03-07</updated><authors><author><keyname>Anis</keyname><forenames>Aamir</forenames></author><author><keyname>Gadde</keyname><forenames>Akshay</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Efficient Sampling Set Selection for Bandlimited Graph Signals Using
  Graph Spectral Proxies</title><categories>cs.IT math.IT</categories><comments>14 pages, 3 figures, 4 tables, Accepted for publication in IEEE
  Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of selecting the best sampling set for bandlimited
reconstruction of signals on graphs. A frequency domain representation for
graph signals can be defined using the eigenvectors and eigenvalues of
variation operators that take into account the underlying graph connectivity.
Smoothly varying signals defined on the nodes are of particular interest in
various applications, and tend to be approximately bandlimited in the frequency
basis. Sampling theory for graph signals deals with the problem of choosing the
best subset of nodes for reconstructing a bandlimited signal from its samples.
Most approaches to this problem require a computation of the frequency basis
(i.e., the eigenvectors of the variation operator), followed by a search
procedure using the basis elements. This can be impractical, in terms of
storage and time complexity, for real datasets involving very large graphs. We
circumvent this issue in our formulation by introducing quantities called graph
spectral proxies, defined using the powers of the variation operator, in order
to approximate the spectral content of graph signals. This allows us to
formulate a direct sampling set selection approach that does not require the
computation and storage of the basis elements. We show that our approach also
provides stable reconstruction when the samples are noisy or when the original
signal is only approximately bandlimited. Furthermore, the proposed approach is
valid for any choice of the variation operator, thereby covering a wide range
of graphs and applications. We demonstrate its effectiveness through various
numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00304</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00304</id><created>2015-10-01</created><authors><author><keyname>Nasr</keyname><forenames>Imen</forenames></author><author><keyname>Atallah</keyname><forenames>Leila Najjar</forenames></author><author><keyname>Cherif</keyname><forenames>Sofiane</forenames></author><author><keyname>Yang</keyname><forenames>Jianxiao</forenames></author><author><keyname>Wang</keyname><forenames>Kulun</forenames></author></authors><title>On a Hybrid Preamble/Soft-Output Demapper Approach for Time
  Synchronization for IEEE 802.15.6 Narrowband WBAN</title><categories>cs.IT math.IT</categories><journal-ref>China Communications, vol.12, no.2, pp.1-10, Feb. 2015</journal-ref><doi>10.1109/CC.2015.7084397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a maximum likelihood (ML) based time
synchronization algorithm for Wireless Body Area Networks (WBAN). The proposed
technique takes advantage of soft information retrieved from the soft demapper
for the time delay estimation. This algorithm has a low complexity and is
adapted to the frame structure specified by the IEEE 802.15.6 standard for the
narrowband systems. Simulation results have shown good performance which
approach the theoretical mean square error limit bound represented by the
Cramer Rao Bound (CRB).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00322</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00322</id><created>2015-10-01</created><authors><author><keyname>Rieger</keyname><forenames>Oya Y.</forenames></author></authors><title>Sustainability: Scholarly Repository as an Enterprise</title><categories>cs.DL</categories><journal-ref>ASIS&amp;T Bulletin, October/November 2012, Volume 39, Number 1</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expanding need for an open information sharing infrastructure to promote
scholarly communication led to the pioneering establishment of arXiv.org, now
maintained by the Cornell University Library. To be sustainable, the repository
requires careful, long term planning for services, management and funding. The
library is developing a sustainability model for arXiv.org, based on voluntary
contributions and the ongoing participation and support of 200 libraries and
research laboratories around the world. The sustainability initiative is based
on a membership model and builds on arXiv's technical, service, financial and
policy infrastructure. Five principles for sustainability drive development,
starting with deep integration into the scholarly community. Also key are a
clearly defined mandate and governance structure, a stable yet innovative
technology platform, systematic creation of content policies and strong
business planning strategies. Repositories like arXiv must consider usability
and life cycle alongside values and trends in scholarly communication. To
endure, they must also support and enhance their service by securing and
managing resources and demonstrating responsible stewardship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00331</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00331</id><created>2015-10-01</created><updated>2016-01-14</updated><authors><author><keyname>Taniguchi</keyname><forenames>Tadahiro</forenames></author><author><keyname>Takano</keyname><forenames>Toshiaki</forenames></author><author><keyname>Yoshino</keyname><forenames>Ryo</forenames></author></authors><title>Multimodal Hierarchical Dirichlet Process-based Active Perception</title><categories>cs.RO cs.AI stat.ML</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an active perception method for recognizing object
categories based on the multimodal hierarchical Dirichlet process (MHDP). The
MHDP enables a robot to form object categories using multimodal information,
e.g., visual, auditory, and haptic information, which can be observed by
performing actions on an object. However, performing many actions on a target
object requires a long time. In a real-time scenario, i.e., when the time is
limited, the robot has to determine the set of actions that is most effective
for recognizing a target object. We propose an MHDP-based active perception
method that uses the information gain (IG) maximization criterion and lazy
greedy algorithm. We show that the IG maximization criterion is optimal in the
sense that the criterion is equivalent to a minimization of the expected
Kullback--Leibler divergence between a final recognition state and the
recognition state after the next set of actions. However, a straightforward
calculation of IG is practically impossible. Therefore, we derive an efficient
Monte Carlo approximation method for IG by making use of a property of the
MHDP. We also show that the IG has submodular and non-decreasing properties as
a set function because of the structure of the graphical model of the MHDP.
Therefore, the IG maximization problem is reduced to a submodular maximization
problem. This means that greedy and lazy greedy algorithms are effective and
have a theoretical justification for their performance. We conducted an
experiment using an upper-torso humanoid robot and a second one using synthetic
data. The experimental results show that the method enables the robot to select
a set of actions that allow it to recognize target objects quickly and
accurately. The results support our theoretical outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00347</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00347</id><created>2015-10-01</created><authors><author><keyname>Mittelbach</keyname><forenames>Martin</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Coding Theorem and Converse for Abstract Channels with Time Structure
  and Memory</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coding theorem and converse are proved for abstract channels with time
structure that contain continuous-time continuous-valued channels and the
result by Kadota and Wyner (1972) as special cases. As main contribution the
coding theorem is proved for a significantly weaker condition on the channel
output memory and without imposing extra measurability requirements to the
channel. These improvements are achieved by introducing a suitable
characterization of information rate capacity. It is shown that the previously
used $\psi$-mixing condition is quite restrictive, in particular for the
important class of Gaussian channels. In fact, it is proved that for Gaussian
(e.g., fading or additive noise) channels the $\psi$-mixing condition is
equivalent to finite output memory. Moreover, a weak converse is derived for
all stationary channels with time structure. Intersymbol interference as well
as input constraints are taken into account in a general and flexible way,
including amplitude and average power constraints as special case. Formulated
in rigorous mathematical terms complete, explicit, and transparent proofs are
presented. As a side product a gap is closed in the proof of Kadota and Wyner
regarding a lemma on the monotonicity of some sequence of normalized mutual
informations. An operational perspective is taken and an abstract framework is
established, which allows to treat discrete- and continuous-time channels with
(possibly infinite) memory and arbitrary alphabets simultaneously in a unified
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00354</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00354</id><created>2015-10-01</created><authors><author><keyname>Biderman</keyname><forenames>Joshua</forenames></author><author><keyname>Cuddy</keyname><forenames>Kevin</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Song</keyname><forenames>Min Jae</forenames></author></authors><title>On the Sensitivity of k-Uniform Hypergraph Properties</title><categories>cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a graph property with sensitivity
$\Theta(\sqrt{n})$, where $n={v\choose2}$ is the number of variables, and
generalize it to a $k$-uniform hypergraph property with sensitivity
$\Theta(\sqrt{n})$, where $n={v\choose k}$ is again the number of variables.
This yields the smallest sensitivity yet achieved for a $k$-uniform hypergraph
property. We then show that, for even $k$, there is a $k$-uniform hypergraph
property that demonstrates a quadratic gap between sensitivity and block
sensitivity. This matches the previously known largest gap found by Rubinstein
(1995) for a general Boolean function and Chakraborty (2005) for a cyclically
invariant Boolean function, and is the first known example of such a gap for a
graph or hypergraph property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00359</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00359</id><created>2015-10-01</created><updated>2015-10-15</updated><authors><author><keyname>Salah</keyname><forenames>Mohamed</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Mohasseb</keyname><forenames>Yahya</forenames></author><author><keyname>Nafie</keyname><forenames>Mohammed</forenames></author></authors><title>Achievable Degrees of Freedom on K-user MIMO Multi-way Relay Channel
  with Common and Private Messages</title><categories>cs.IT math.IT</categories><comments>5 double-column pages, 2 figures, 1 table, the paper will be
  presented at the IEEE Asilomar Conference Nov. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the achievable total degrees of freedom (DoF) of the
MIMO multi-way relay channel that consists of K users, where each user is
equipped with M antennas, and a decode-and-forward relay equipped with N
antennas. In this channel, each user wants to convey K-1 private messages to
the other users in addition to a common message to all of them. Due to the
absence of direct links between the users, communication occurs through the
relay in two phases; a multiple access channel phase (MAC) and a broadcast (BC)
phase. We drive cut-set bounds on the total DoF of the network, and show that
the network has DoF less than or equal to K min(N,M). Achievability of the
upper bound is shown by using signal space alignment for network coding in the
MAC phase, and zero-forcing precoding in the BC phase. We show that introducing
the common messages besides the private messages leads to achieving higher
total DoF than using the private messages only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00377</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00377</id><created>2015-10-01</created><authors><author><keyname>Parent</keyname><forenames>Alex</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Reversible circuit compilation with space constraints</title><categories>quant-ph cs.ET</categories><comments>32 pages, 15 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for resource efficient compilation of higher-level
programs into lower-level reversible circuits. Our main focus is on optimizing
the memory footprint of the resulting reversible networks. This is motivated by
the limited availability of qubits for the foreseeable future. We apply three
main techniques to keep the number of required qubits small when computing
classical, irreversible computations by means of reversible networks: first,
wherever possible we allow the compiler to make use of in-place functions to
modify some of the variables. Second, an intermediate representation is
introduced that allows to trace data dependencies within the program, allowing
to clean up qubits early. This realizes an analog to &quot;garbage collection&quot; for
reversible circuits. Third, we use the concept of so-called pebble games to
transform irreversible programs into reversible programs under space
constraints, allowing for data to be erased and recomputed if needed.
  We introduce REVS, a compiler for reversible circuits that can translate a
subset of the functional programming language F# into Toffoli networks which
can then be further interpreted for instance in LIQui|&gt;, a domain-specific
language for quantum computing and which is also embedded into F#. We discuss a
number of test cases that illustrate the advantages of our approach including
reversible implementations of SHA-2 and other cryptographic hash-functions,
reversible integer arithmetic, as well as a test-bench of combinational
circuits used in classical circuit synthesis. Compared to Bennett's method,
REVS can reduce space complexity by a factor of $4$ or more, while having an
only moderate increase in circuit size as well as in the time it takes to
compile the reversible networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00383</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00383</id><created>2015-10-01</created><authors><author><keyname>Eaton</keyname><forenames>James</forenames></author><author><keyname>Gaubitch</keyname><forenames>Nikolay D.</forenames></author><author><keyname>Moore</keyname><forenames>Alastair H.</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick A.</forenames></author></authors><title>Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA (2015)</title><categories>cs.SD</categories><comments>New Paltz, New York, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several established parameters and metrics have been used to characterize the
acoustics of a room. The most important are the Direct-To-Reverberant Ratio
(DRR), the Reverberation Time (T60) and the reflection coefficient. The
acoustic characteristics of a room based on such parameters can be used to
predict the quality and intelligibility of speech signals in that room.
Recently, several important methods in speech enhancement and speech
recognition have been developed that show an increase in performance compared
to the predecessors but do require knowledge of one or more fundamental
acoustical parameters such as the T60. Traditionally, these parameters have
been estimated using carefully measured Acoustic Impulse Responses (AIRs).
However, in most applications it is not practical or even possible to measure
the acoustic impulse response. Consequently, there is increasing research
activity in the estimation of such parameters directly from speech and audio
signals. The aim of this challenge was to evaluate state-of-the-art algorithms
for blind acoustic parameter estimation from speech and to promote the emerging
area of research in this field. Participants evaluated their algorithms for T60
and DRR estimation against the 'ground truth' values provided with the
data-sets and presented the results in a paper describing the method used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00384</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00384</id><created>2015-10-01</created><authors><author><keyname>Ongie</keyname><forenames>Greg</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Off-the-Grid Recovery of Piecewise Constant Images from Few Fourier
  Samples</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method to recover a continuous domain representation of a
piecewise constant two-dimensional image from few low-pass Fourier samples.
Assuming the edge set of the image is localized to the zero set of a
trigonometric polynomial, we show the Fourier coefficients of the partial
derivatives of the image satisfy a linear annihilation relation. We present
necessary and sufficient conditions for unique recovery of the image from
finite low-pass Fourier samples using the annihilation relation. We also
propose a practical two-stage recovery algorithm which is robust to
model-mismatch and noise. In the first stage we estimate a continuous domain
representation of the edge set of the image. In the second stage we perform an
extrapolation in Fourier domain by a least squares two-dimensional linear
prediction, which recovers the exact Fourier coefficients of the underlying
image. We demonstrate our algorithm on the super-resolution recovery of MRI
phantoms and real MRI data from low-pass Fourier samples, and show the
superiority of the method over standard approaches for single-image
super-resolution MRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00419</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00419</id><created>2015-10-01</created><authors><author><keyname>Arkhipov</keyname><forenames>Viktor</forenames></author><author><keyname>Buzdalov</keyname><forenames>Maxim</forenames></author><author><keyname>Shalyto</keyname><forenames>Anatoly</forenames></author></authors><title>An Asynchronous Implementation of the Limited Memory CMA-ES</title><categories>cs.NE</categories><comments>9 pages, 4 figures, 4 tables; this is a full version of a paper which
  has been accepted as a poster to IEEE ICMLA conference 2015</comments><msc-class>90C56</msc-class><acm-class>G.1.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our asynchronous implementation of the LM-CMA-ES algorithm, which
is a modern evolution strategy for solving complex large-scale continuous
optimization problems. Our implementation brings the best results when the
number of cores is relatively high and the computational complexity of the
fitness function is also high. The experiments with benchmark functions show
that it is able to overcome its origin on the Sphere function, reaches certain
thresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly
performs much better than the original version on the Rastrigin function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00432</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00432</id><created>2015-10-01</created><updated>2015-11-13</updated><authors><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Banerjee</keyname><forenames>Aparajita</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Hybrid Spintronic-CMOS Spiking Neural Network With On-Chip Learning:
  Devices, Circuits and Systems</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade Spiking Neural Networks (SNN) have emerged as one of the
popular architectures to emulate the brain. In SNN, information is temporally
encoded and communication between neurons is accomplished by means of spikes.
In such networks, spike-timing dependent plasticity mechanisms require the
online programming of synapses based on the temporal information of spikes
transmitted by spiking neurons. In this work, we propose a spintronic synapse
with decoupled spike transmission and programming current paths. The spintronic
synapse consists of a ferromagnet-heavy metal heterostructure where programming
current through the heavy metal generates spin-orbit torque to modulate the
device conductance. Ultra-low programming energy and fast programming times
demonstrate the efficacy of the proposed device as a nanoelectronic synapse. We
demonstrate the interfacing of such spintronic synapses with CMOS neurons and
learning circuits operating in transistor sub-threshold region to form a
network of spiking neurons that can be utilized for pattern recognition
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00436</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00436</id><created>2015-10-01</created><authors><author><keyname>Futrell</keyname><forenames>Richard</forenames></author><author><keyname>Mahowald</keyname><forenames>Kyle</forenames></author><author><keyname>Gibson</keyname><forenames>Edward</forenames></author></authors><title>Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and
  G\'omez-Rodr\'iguez (2015) on Dependency Length Minimization</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and
G\'omez-Rodr\'iguez, 2015) of our work on empirical evidence of dependency
length minimization across languages (Futrell et al., 2015). First, we
acknowledge error in failing to acknowledge Liu (2008)'s previous work on
corpora of 20 languages with similar aims. A correction will appear in PNAS.
Nevertheless, we argue that our work provides novel, strong evidence for
dependency length minimization as a universal quantitative property of
languages, beyond this previous work, because it provides baselines which focus
on word order preferences. Second, we argue that our choices of baselines were
appropriate because they control for alternative theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00440</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00440</id><created>2015-10-01</created><authors><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Panda</keyname><forenames>Priyadarshini</forenames></author><author><keyname>Wijesinghe</keyname><forenames>Parami</forenames></author><author><keyname>Kim</keyname><forenames>Yusung</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Magnetic Tunnel Junction Mimics Stochastic Cortical Spiking Neurons</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain-inspired computing architectures attempt to mimic the computations
performed in the neurons and the synapses in the human brain in order to
achieve its efficiency in learning and cognitive tasks. In this work, we
demonstrate the mapping of the probabilistic spiking nature of pyramidal
neurons in the cortex to the stochastic switching behavior of a Magnetic Tunnel
Junction in presence of thermal noise. We present results to illustrate the
efficiency of neuromorphic systems based on such probabilistic neurons for
pattern recognition tasks in presence of lateral inhibition and homeostasis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00450</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00450</id><created>2015-10-01</created><authors><author><keyname>Mehmetoglu</keyname><forenames>Mustafa Said</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Analog Multiple Descriptions: A Zero-Delay Source-Channel Coding
  Approach</title><categories>cs.IT math.IT</categories><comments>Submitted to ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the well-known source coding problem of multiple
descriptions, in its general and basic setting, to analog source-channel coding
scenarios. Encoding-decoding functions that optimally map between the (possibly
continuous valued) source and the channel spaces are numerically derived. The
main technical tool is a non-convex optimization method, namely, deterministic
annealing, which has recently been successfully used in other mapping
optimization problems. The obtained functions exhibit several interesting
structural properties, map multiple source intervals to the same interval in
the channel space, and consistently outperform the known competing mapping
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00452</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00452</id><created>2015-10-01</created><updated>2016-02-25</updated><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>Optimal Binary Classifier Aggregation for General Losses</title><categories>cs.LG stat.ML</categories><comments>NIPS 2015, &quot;Learning Faster from Easy Data II&quot; Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of aggregating an ensemble of binary classifiers in a
semi-supervised setting. Recently, this problem was solved optimally using a
game-theoretic approach, but that analysis was specific to the 0-1 loss. In
this paper, we generalize the minimax optimal algorithm of the previous work to
a very general, novel class of loss functions, including but not limited to all
convex surrogates, while extending its performance and efficiency guarantees.
  The result is a family of parameter-free ensemble aggregation algorithms
which use labeled and unla- beled data; these are as efficient as linear
learning and prediction for convex risk minimization, but work without any
relaxations on many non-convex loss functions. The prediction algorithms take a
form familiar in decision theory, applying sigmoid functions to a generalized
notion of ensemble margin, but without the assumptions typically made in
margin-based learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00455</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00455</id><created>2015-10-01</created><authors><author><keyname>Maidens</keyname><forenames>John</forenames></author><author><keyname>Arcak</keyname><forenames>Murat</forenames></author></authors><title>Semidefinite relaxations in optimal experiment design with application
  to substrate injection for hyperpolarized MRI</title><categories>cs.SY math.OC</categories><comments>6 pages, Submitted to ACC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimal input design for estimating uncertain
parameters in a discrete-time linear state space model, subject to simultaneous
amplitude and l1/l2-norm constraints on the admissible inputs. We formulate
this problem as the maximization of a (non-concave) quadratic function over the
space of inputs, and use semidefinite relaxation techniques to efficiently find
the global solution or to provide an upper bound. This investigation is
motivated by a problem in medical imaging, specifically designing a substrate
injection profile for in vivo metabolic parameter mapping using magnetic
resonance imaging (MRI) with hyperpolarized carbon-13 pyruvate. In the
l2-norm-constrained case, we show that the relaxation is tight, allowing us to
efficiently compute a globally optimal injection profile. In the
l1-norm-constrained case the relaxation is no longer tight, but can be used to
prove that the boxcar injection currently used in practice achieves at least
98.7% of the global optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00459</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00459</id><created>2015-10-01</created><updated>2016-02-03</updated><authors><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Shim</keyname><forenames>Yong</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Proposal for an All-Spin Artificial Neural Network: Emulating Neural and
  Synaptic Functionalities Through Domain Wall Motion in Ferromagnets</title><categories>cs.ET</categories><comments>The article will appear in a future issue of IEEE Transactions on
  Biomedical Circuits and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Boolean computing based on emerging post-CMOS technologies can
potentially pave the way for low-power neural computing platforms. However,
existing work on such emerging neuromorphic architectures have either focused
on solely mimicking the neuron, or the synapse functionality. While memristive
devices have been proposed to emulate biological synapses, spintronic devices
have proved to be efficient at performing the thresholding operation of the
neuron at ultra-low currents. In this work, we propose an All-Spin Artificial
Neural Network where a single spintronic device acts as the basic building
block of the system. The device offers a direct mapping to synapse and neuron
functionalities in the brain while inter-layer network communication is
accomplished via CMOS transistors. To the best of our knowledge, this is the
first demonstration of a neural architecture where a single nanoelectronic
device is able to mimic both neurons and synapses. The ultra-low voltage
operation of low resistance magneto-metallic neurons enables the low-voltage
operation of the array of spintronic synapses, thereby leading to ultra-low
power neural architectures. Device-level simulations, calibrated to
experimental results, was used to drive the circuit and system level
simulations of the neural network for a standard pattern recognition problem.
Simulation studies indicate energy savings by ~ 100x in comparison to a
corresponding digital/ analog CMOS neuron implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00460</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00460</id><created>2015-10-01</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Characterizing SW-Efficiency in the Social Choice Domain</title><categories>cs.GT</categories><comments>Short Note</comments><msc-class>C63, C70, C71, C78</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Dogan, Dogan and Yildiz (2015) presented a new efficiency notion
for the random assignment setting called SW (social welfare)-efficiency and
characterized it. In this note, we generalize the characterization for the more
general domain of randomized social choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00466</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00466</id><created>2015-10-01</created><updated>2016-01-04</updated><authors><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author></authors><title>Parallel proximal methods for total variation minimization</title><categories>cs.IT math.IT math.OC</categories><comments>To be presented at ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total variation (TV) is a widely used regularizer for stabilizing the
solution of ill-posed inverse problems. In this paper, we propose a novel
proximal-gradient algorithm for minimizing TV regularized least-squares cost
functional. Our method replaces the standard proximal step of TV by a simpler
alternative that computes several independent proximals. We prove that the
proposed parallel proximal method converges to the TV solution, while requiring
no sub-iterations. The results in this paper could enhance the applicability of
TV for solving very large scale imaging inverse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00473</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00473</id><created>2015-10-01</created><authors><author><keyname>Johnson</keyname><forenames>Thor</forenames></author><author><keyname>Robertson</keyname><forenames>Neil</forenames></author><author><keyname>Seymour</keyname><forenames>Paul</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Excluding A Grid Minor In Planar Digraphs</title><categories>math.CO cs.DM</categories><comments>19 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [Directed tree-width, J. Combin. Theory Ser. B 82 (2001), 138-154] we
introduced the notion of tree-width of directed graphs and presented a
conjecture, formulated during discussions with Noga Alon and Bruce Reed,
stating that a digraph of huge tree-width has a large &quot;cylindrical grid&quot; minor.
Here we prove the conjecture for planar digraphs, but many steps of the proof
work in general.
  This is an unedited and unpolished manuscript from October 2001. Since many
people asked for copies we are making it available in the hope that it may be
useful. The conjecture was proved by Kawarabayashi and Kreutzer in
arXiv:1411.5681.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00474</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00474</id><created>2015-10-01</created><authors><author><keyname>Wang</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>Microwave Surveillance based on Ghost Imaging and Distributed Antennas</title><categories>stat.AP cs.IT math.IT</categories><comments>4 pages, 11 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we proposed a ghost imaging (GI) and distributed antennas
based microwave surveillance scheme. By analyzing its imaging resolution and
sampling requirement, the potential of employing microwave GI to achieve
high-quality surveillance performance with low system complexity has been
demonstrated. The theoretical analysis and effectiveness of the proposed
microwave surveillance method are also validated via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00477</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00477</id><created>2015-10-01</created><authors><author><keyname>Zhu</keyname><forenames>Jun-Yan</forenames></author><author><keyname>Kr&#xe4;henb&#xfc;hl</keyname><forenames>Philipp</forenames></author><author><keyname>Shechtman</keyname><forenames>Eli</forenames></author><author><keyname>Efros</keyname><forenames>Alexei A.</forenames></author></authors><title>Learning a Discriminative Model for the Perception of Realism in
  Composite Images</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What makes an image appear realistic? In this work, we are answering this
question from a data-driven perspective by learning the perception of visual
realism directly from large amounts of data. In particular, we train a
Convolutional Neural Network (CNN) model that distinguishes natural photographs
from automatically generated composite images. The model learns to predict
visual realism of a scene in terms of color, lighting and texture
compatibility, without any human annotations pertaining to it. Our model
outperforms previous works that rely on hand-crafted heuristics, for the task
of classifying realistic vs. unrealistic photos. Furthermore, we apply our
learned model to compute optimal parameters of a compositing method, to
maximize the visual realism score predicted by our CNN model. We demonstrate
its advantage against existing methods via a human perception study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00479</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00479</id><created>2015-10-01</created><authors><author><keyname>Jindal</keyname><forenames>Ishan</forenames></author><author><keyname>Raman</keyname><forenames>Shanmuganathan</forenames></author></authors><title>Effective Object Tracking in Unstructured Crowd Scenes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are presenting a rotation variant Oriented Texture Curve
(OTC) descriptor based mean shift algorithm for tracking an object in an
unstructured crowd scene. The proposed algorithm works by first obtaining the
OTC features for a manually selected object target, then a visual vocabulary is
created by using all the OTC features of the target. The target histogram is
obtained using codebook encoding method which is then used in mean shift
framework to perform similarity search. Results are obtained on different
videos of challenging scenes and the comparison of the proposed approach with
several state-of-the-art approaches are provided. The analysis shows the
advantages and limitations of the proposed approach for tracking an object in
unstructured crowd scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00482</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00482</id><created>2015-10-01</created><updated>2015-10-11</updated><authors><author><keyname>Chan</keyname><forenames>Ka Ching</forenames></author></authors><title>Integration of physical equipment and simulators for on-campus and
  online delivery of practical networking labs</title><categories>cs.NI cs.CY</categories><comments>10 pages, 6 figures, 2 tables</comments><report-no>Technical Report CSIT 20151002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design and development of a networking laboratory
that integrates physical networking equipment with the open source GNS3 network
simulators for delivery of practical networking classes simultaneously to both
on-campus and online students. This transformation work has resulted in
significant increase in laboratory capacity, reducing repeating classes. The
integrated platform offers students the real world experience of using real
equipment, and the convenience of easy setup and reconfiguration by using
simulators. A practical exercise in setting up an OSPF/BGP network is presented
as an example to illustrate the experimental design before and after the
integration of GNS3 simulators. In summary, we report our experiences with the
integrated platform, from infrastructure, network design to experiment design;
and the learning and teaching experiences of using GNS3 in classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00504</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00504</id><created>2015-10-02</created><updated>2015-10-28</updated><authors><author><keyname>Traonmilin</keyname><forenames>Yann</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Stable recovery of low-dimensional cones in Hilbert spaces: One RIP to
  rule them all</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many inverse problems in signal processing deal with the robust estimation of
unknown data from underdetermined linear observations. Low dimensional models,
when combined with appropriate regularizers, have been shown to be efficient at
performing this task. Sparse models with the 1-norm or low rank models with the
nuclear norm are examples of such successful combinations. Stable recovery
guarantees in these settings have been established using a common tool adapted
to each case: the notion of restricted isometry property (RIP). In this paper,
we establish generic RIP-based guarantees for the stable recovery of cones
(positively homogeneous model sets) with arbitrary regularizers. These
guarantees are illustrated on selected examples. For block structured sparsity
in the infinite dimensional setting, we use the guarantees for a family of
regularizers which efficiency in terms of RIP constant can be controlled,
leading to stronger and sharper guarantees than the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00519</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00519</id><created>2015-10-02</created><authors><author><keyname>Andreyev</keyname><forenames>Sergey</forenames></author></authors><title>Interface Between Market and Science</title><categories>cs.HC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the beginning, programming was inspired by the search of the best
solutions. At that time some fundamental stones like famous languages and
object oriented and structured programming were laid. It was found later that
applications could generate huge profits; after it marketing departments
started to decide what was right and wrong. Programs are ruled by developers
but declared user-friendly; millions of users are going mad trying to get the
needed results from these applications. Research goes on and new results can be
opposite to business view. History shows that not science has to adjust to
business, but eventually business will have to adapt to the results of the
research work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00523</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00523</id><created>2015-10-02</created><authors><author><keyname>Toda</keyname><forenames>Takahisa</forenames></author><author><keyname>Soh</keyname><forenames>Takehide</forenames></author></authors><title>Implementing Efficient All Solutions SAT Solvers</title><categories>cs.DS cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All solutions SAT (AllSAT for short) is a variant of propositional
satisfiability problem. Despite its significance, AllSAT has been relatively
unexplored compared to other variants. We thus survey and discuss major
techniques of AllSAT solvers. We faithfully implement them and conduct
comprehensive experiments using a large number of instances and various types
of solvers including one of the few public softwares. The experiments reveal
solver's characteristics. Our implemented solvers are made publicly available
so that other researchers can easily develop their solver by modifying our
codes and compare it with existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00542</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00542</id><created>2015-10-02</created><authors><author><keyname>Sharma</keyname><forenames>Gaurav</forenames></author><author><keyname>Jurie</keyname><forenames>Frederic</forenames></author></authors><title>Local Higher-Order Statistics (LHS) describing images with statistics of
  local non-binarized pixel patterns</title><categories>cs.CV</categories><comments>CVIU preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new image representation for texture categorization and facial
analysis, relying on the use of higher-order local differential statistics as
features. It has been recently shown that small local pixel pattern
distributions can be highly discriminative while being extremely efficient to
compute, which is in contrast to the models based on the global structure of
images. Motivated by such works, we propose to use higher-order statistics of
local non-binarized pixel patterns for the image description. The proposed
model does not require either (i) user specified quantization of the space (of
pixel patterns) or (ii) any heuristics for discarding low occupancy volumes of
the space. We propose to use a data driven soft quantization of the space, with
parametric mixture models, combined with higher-order statistics, based on
Fisher scores. We demonstrate that this leads to a more expressive
representation which, when combined with discriminatively learned classifiers
and metrics, achieves state-of-the-art performance on challenging texture and
facial analysis datasets, in low complexity setup. Further, it is complementary
to higher complexity features and when combined with them improves performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00549</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00549</id><created>2015-10-02</created><updated>2016-01-15</updated><authors><author><keyname>&#xc1;brego</keyname><forenames>Bernardo M.</forenames></author><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Fern&#xe1;ndez-Merchant</keyname><forenames>Silvia</forenames></author><author><keyname>McQuillan</keyname><forenames>Dan</forenames></author><author><keyname>Mohar</keyname><forenames>Bojan</forenames></author><author><keyname>Mutzel</keyname><forenames>Petra</forenames></author><author><keyname>Ramos</keyname><forenames>Pedro</forenames></author><author><keyname>Richter</keyname><forenames>R. Bruce</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Bishellable drawings of $K_n$</title><categories>math.CO cs.CG</categories><comments>10 pages, 4 figures. Fixed a typo on the list of authors</comments><msc-class>05C10, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Harary-Hill conjecture, still open after more than 50 years, asserts that
the crossing number of the complete graph $K_n$ is \[ H(n) = \frac 1 4
\left\lfloor\frac{\mathstrut n}{\mathstrut 2}\right\rfloor
\left\lfloor\frac{\mathstrut n-1}{\mathstrut 2}\right\rfloor
\left\lfloor\frac{\mathstrut n-2}{\mathstrut 2}\right\rfloor
\left\lfloor\frac{\mathstrut n-3}{\mathstrut 2}\right\rfloor\,.\] \'Abrego et
al.~\cite{shell1} introduced the notion of shellability of a drawing $D$ of
$K_n$. They proved that if $D$ is $s$-shellable for some
$s\geq\lfloor\frac{n}{2}\rfloor$, then $D$ has at least $H(n)$ crossings. This
is the first combinatorial condition on a drawing that guarantees at least
$H(n)$ crossings.
  In this work, we generalize the concept of $s$-shellability to
bishellability, where the former implies the latter in the sense that every
$s$-shellable drawing is, for any $b \leq s-2$, also $b$-bishellable. Our main
result is that $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellability also
guarantees, with a simpler proof than for $s$-shellability, that a drawing has
at least $H(n)$ crossings. We exhibit a drawing of $K_{11}$ that has $H(11)$
crossings, is 3-bishellable, and is not $s$-shellable for any $s\geq5$. This
shows that we have properly extended the class of drawings for which the
Harary-Hill Conjecture is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00552</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00552</id><created>2015-10-02</created><updated>2015-10-05</updated><authors><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Hajian</keyname><forenames>Sara</forenames></author><author><keyname>Mishra</keyname><forenames>Bud</forenames></author><author><keyname>Ramazzotti</keyname><forenames>Daniele</forenames></author></authors><title>Exposing the Probabilistic Causal Structure of Discrimination</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrimination discovery from data is an important task aiming at identifying
patterns of illegal and unethical discriminatory activities against
protected-by-law groups, e.g., ethnic minorities. While any legally-valid proof
of discrimination requires evidence of causality, the state-of-the-art methods
are essentially correlation-based, albeit, as it is well known, correlation
does not imply causation.
  In this paper we take a principled causal approach to the data mining problem
of discrimination detection in databases. Following Suppes' probabilistic
causation theory, we define a method to extract, from a dataset of historical
decision records, the causal structures existing among the attributes in the
data. The result is a type of constrained Bayesian network, which we dub
Suppes-Bayes Causal Network (SBCN). Next, we develop a toolkit of methods based
on random walks on top of the SBCN, addressing different anti-discrimination
legal concepts, such as direct and indirect discrimination, group and
individual discrimination, genuine requirement, and favoritism. Our experiments
on real-world datasets confirm the inferential power of our approach in all
these different tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00556</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00556</id><created>2015-10-02</created><updated>2016-02-08</updated><authors><author><keyname>Zidan</keyname><forenames>M.</forenames></author><author><keyname>Sagheer</keyname><forenames>A.</forenames></author><author><keyname>Metwally</keyname><forenames>N.</forenames></author></authors><title>Autonomous Perceptron Neural Network Inspired from Quantum computing</title><categories>quant-ph cs.NE</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This abstract will be modified after correcting the minor error in Eq.(2)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00561</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00561</id><created>2015-10-02</created><authors><author><keyname>Katsigiannis</keyname><forenames>Stamos</forenames></author><author><keyname>Papaioannou</keyname><forenames>Georgios</forenames></author><author><keyname>Maroulis</keyname><forenames>Dimitris</forenames></author></authors><title>CVC: The Contourlet Video Compression algorithm for real-time
  applications</title><categories>cs.MM</categories><comments>22 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, real-time video communication over the internet through video
conferencing applications has become an invaluable tool in everyone's
professional and personal life. This trend underlines the need for video coding
algorithms that provide acceptable quality on low bitrates and can support
various resolutions inside the same stream in order to cope with limitations on
computational resources and network bandwidth. In this work, a novel scalable
video coding algorithm based on the contourlet transform is presented. The
algorithm utilizes both lossy and lossless methods in order to achieve
compression. One of its most notable features is that due to the transform
utilised, it does not suffer from blocking artifacts that occur with many
widely adopted compression algorithms. The proposed algorithm takes advantage
of the vast computational capabilities of modern GPUs, in order to achieve
real-time performance and provide satisfactory encoding and decoding times at
relatively low cost, making it suitable for applications like video
conferencing. Experiments show that the proposed algorithm performs
satisfactorily in terms of compression ratio and speed, while it outperforms
standard methods in terms of perceptual quality on lower bitrates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00562</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00562</id><created>2015-10-02</created><authors><author><keyname>Sun</keyname><forenames>Lin</forenames></author><author><keyname>Jia</keyname><forenames>Kui</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author><author><keyname>Shi</keyname><forenames>Bertram E.</forenames></author></authors><title>Human Action Recognition using Factorized Spatio-Temporal Convolutional
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human actions in video sequences are three-dimensional (3D) spatio-temporal
signals characterizing both the visual appearance and motion dynamics of the
involved humans and objects. Inspired by the success of convolutional neural
networks (CNN) for image classification, recent attempts have been made to
learn 3D CNNs for recognizing human actions in videos. However, partly due to
the high complexity of training 3D convolution kernels and the need for large
quantities of training videos, only limited success has been reported. This has
triggered us to investigate in this paper a new deep architecture which can
handle 3D signals more effectively. Specifically, we propose factorized
spatio-temporal convolutional networks (FstCN) that factorize the original 3D
convolution kernel learning as a sequential process of learning 2D spatial
kernels in the lower layers (called spatial convolutional layers), followed by
learning 1D temporal kernels in the upper layers (called temporal convolutional
layers). We introduce a novel transformation and permutation operator to make
factorization in FstCN possible. Moreover, to address the issue of sequence
alignment, we propose an effective training and inference strategy based on
sampling multiple video clips from a given action video sequence. We have
tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51).
Without using auxiliary training videos to boost the performance, FstCN
outperforms existing CNN based methods and achieves comparable performance with
a recent method that benefits from using auxiliary training videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00563</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00563</id><created>2015-10-02</created><authors><author><keyname>Svensson</keyname><forenames>Andreas</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Solin</keyname><forenames>Arno</forenames></author><author><keyname>S&#xe4;rkk&#xe4;</keyname><forenames>Simo</forenames></author></authors><title>Nonlinear State Space Model Identification Using a Regularized Basis
  Function Expansion</title><categories>stat.CO cs.SY</categories><comments>Accepted to the 6th IEEE international workshop on computational
  advances in multi-sensor adaptive processing (CAMSAP), Cancun, Mexico,
  December 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with black-box identification of nonlinear state
space models. By using a basis function expansion within the state space model,
we obtain a flexible structure. The model is identified using an expectation
maximization approach, where the states and the parameters are updated
iteratively in such a way that a maximum likelihood estimate is obtained. We
use recent particle methods with sound theoretical properties to infer the
states, whereas the model parameters can be updated using closed-form
expressions by exploiting the fact that our model is linear in the parameters.
Not to over-fit the flexible model to the data, we also propose a
regularization scheme without increasing the computational burden. Importantly,
this opens up for systematic use of regularization in nonlinear state space
models. We conclude by evaluating our proposed approach on one simulation
example and two real-data problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00571</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00571</id><created>2015-10-02</created><authors><author><keyname>Chang</keyname><forenames>Hsien-Chih</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author></authors><title>Electrical Reduction, Homotopy Moves, and Defect</title><categories>cs.CG math.GT</categories><comments>27 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the first nontrivial worst-case lower bounds for two closely related
problems. First, $\Omega(n^{3/2})$ degree-1 reductions, series-parallel
reductions, and $\Delta$Y transformations are required in the worst case to
reduce an $n$-vertex plane graph to a single vertex or edge. The lower bound is
achieved by any planar graph with treewidth $\Theta(\sqrt{n})$. Second,
$\Omega(n^{3/2})$ homotopy moves are required in the worst case to reduce a
closed curve in the plane with $n$ self-intersection points to a simple closed
curve. For both problems, the best upper bound known is $O(n^2)$, and the only
lower bound previously known was the trivial $\Omega(n)$.
  The first lower bound follows from the second using medial graph techniques
ultimately due to Steinitz, together with more recent arguments of Noble and
Welsh [J. Graph Theory 2000]. The lower bound on homotopy moves follows from an
observation by Haiyashi et al. [J. Knot Theory Ramif. 2012] that the standard
projections of certain torus knots have large defect, a topological invariant
of generic closed curves introduced by Aicardi and Arnold. Finally, we prove
that every closed curve in the plane with $n$ crossings has defect
$O(n^{3/2})$, which implies that better lower bounds for our algorithmic
problems will require different techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00574</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00574</id><created>2015-10-02</created><authors><author><keyname>Ouyang</keyname><forenames>Xing</forenames></author><author><keyname>Antony</keyname><forenames>Cleitus</forenames></author><author><keyname>Gunning</keyname><forenames>Fatima</forenames></author><author><keyname>Zhang</keyname><forenames>Hongyu</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Discrete Fresnel Transform and Its Circular Convolution</title><categories>cs.IT math.IT physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete trigonometric transformations, such as the discrete Fourier and
cosine/sine transforms, are important in a variety of applications due to their
useful properties. For example, one well-known property is the convolution
theorem for Fourier transform. In this letter, we derive a discrete Fresnel
transform (DFnT) from the infinitely periodic optical gratings, as a linear
trigonometric transform. Compared to the previous formulations of DFnT, the
DFnT in this letter has no degeneracy, which hinders its mathematic
applications, due to destructive interferences. The circular convolution
property of the DFnT is studied for the first time. It is proved that the DFnT
of a circular convolution of two sequences equals either one circularly
convolving with the DFnT of the other. As circular convolution is a fundamental
process in discrete systems, the DFnT not only gives the coefficients of the
Talbot image, but can also be useful for optical and digital signal processing
and numerical evaluation of the Fresnel transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00585</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00585</id><created>2015-10-02</created><authors><author><keyname>Singh</keyname><forenames>Ranveer</forenames></author><author><keyname>Patra</keyname><forenames>Bidyut Kr.</forenames></author><author><keyname>Adhikari</keyname><forenames>Bibhas</forenames></author></authors><title>A Complex Network Approach for Collaborative Recommendation</title><categories>cs.IR</categories><comments>22 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering (CF) is the most widely used and successful approach
for personalized service recommendations. Among the collaborative
recommendation approaches, neighborhood based approaches enjoy a huge amount of
popularity, due to their simplicity, justifiability, efficiency and stability.
Neighborhood based collaborative filtering approach finds K nearest neighbors
to an active user or K most similar rated items to the target item for
recommendation. Traditional similarity measures use ratings of co-rated items
to find similarity between a pair of users. Therefore, traditional similarity
measures cannot compute effective neighbors in sparse dataset. In this paper,
we propose a two-phase approach, which generates user-user and item-item
networks using traditional similarity measures in the first phase. In the
second phase, two hybrid approaches HB1, HB2, which utilize structural
similarity of both the network for finding K nearest neighbors and K most
similar items to a target items are introduced. To show effectiveness of the
measures, we compared performances of neighborhood based CFs using
state-of-the-art similarity measures with our proposed structural similarity
measures based CFs. Recommendation results on a set of real data show that
proposed measures based CFs outperform existing measures based CFs in various
evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00598</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00598</id><created>2015-10-02</created><authors><author><keyname>Bougeret</keyname><forenames>Marin</forenames></author><author><keyname>Bessy</keyname><forenames>Stephane</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>Paul</keyname><forenames>Cristophe</forenames></author></authors><title>On independent set on B1-EPG graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the Maximum Independent Set problem (MIS) on
$B_1$-EPG graphs. EPG (for Edge intersection graphs of Paths on a Grid) was
introduced in ~\cite{edgeintersinglebend} as the class of graphs whose vertices
can be represented as simple paths on a rectangular grid so that two vertices
are adjacent if and only if the corresponding paths share at least one edge of
the underlying grid. The restricted class $B_k$-EPG denotes EPG-graphs where
every path has at most $k$ bends. The study of MIS on $B_1$-EPG graphs has been
initiated in~\cite{wadsMIS} where authors prove that MIS is NP-complete on
$B_1$-EPG graphs, and provide a polynomial $4$-approximation. In this article
we study the approximability and the fixed parameter tractability of MIS on
$B_1$-EPG. We show that there is no PTAS for MIS on $B_1$-EPG unless P$=$NP,
even if there is only one shape of path, and even if each path has its vertical
part or its horizontal part of length at most $3$. This is optimal, as we show
that if all paths have their horizontal part bounded by a constant, then MIS
admits a PTAS. Finally, we show that MIS is FPT in the standard
parameterization on $B_1$-EPG restricted to only three shapes of path, and
$W_1$-hard on $B_2$-EPG. The status for general $B_1$-EPG (with the four
shapes) is left open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00600</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00600</id><created>2015-10-02</created><authors><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Mart&#xed;nez-Sandoval</keyname><forenames>Leonardo</forenames></author><author><keyname>Alfons&#xed;n</keyname><forenames>Jorge Luis Ram&#xed;rez</forenames></author></authors><title>A Tutte polynomial inequality for lattice path matroids</title><categories>math.CO cs.DM</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $M$ be a matroid without loops or coloops and let $T_M$ be its Tutte
polynomial. In 1999 Merino and Welsh conjectured that $\max(T_M(2,0),
T_M(0,2))\geq T_M(1,1)$ for graphic matroids. Ten years later, Conde and Merino
proposed a multiplicative version of the conjecture which implies the original
one. In this paper we show the validity of the multiplicative conjecture when
$M$ is a lattice path matroid.
  In order to do this, we introduce and study a particular class of lattice
path matroids, called {\em snakes}. We present a characterization showing that
snakes are the only graphic lattice path matroids and provide explicit formulas
for the number of trees, acyclic orientations and totally cyclic orientations
in this case. Snakes are used as building bricks to establish a stronger
inequality implying the above multiplicative conjecture as well as to
characterize the cases in which equality holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00604</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00604</id><created>2015-10-02</created><authors><author><keyname>Steinert</keyname><forenames>Laura</forenames></author><author><keyname>Hoefinghoff</keyname><forenames>Jens</forenames></author><author><keyname>Pauli</keyname><forenames>Josef</forenames></author></authors><title>Online Vision- and Action-Based Object Classification Using Both
  Symbolic and Subsymbolic Knowledge Representations</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a robot is supposed to roam an environment and interact with objects, it
is often necessary to know all possible objects in advance, so that a database
with models of all objects can be generated for visual identification. However,
this constraint cannot always be fulfilled. Due to that reason, a model based
object recognition cannot be used to guide the robot's interactions. Therefore,
this paper proposes a system that analyzes features of encountered objects and
then uses these features to compare unknown objects to already known ones. From
the resulting similarity appropriate actions can be derived. Moreover, the
system enables the robot to learn object categories by grouping similar objects
or by splitting existing categories. To represent the knowledge a hybrid form
is used, consisting of both symbolic and subsymbolic representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00609</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00609</id><created>2015-10-02</created><authors><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Frequency Selective Hybrid Precoding for Limited Feedback Millimeter
  Wave Systems</title><categories>cs.IT math.IT</categories><comments>37 pages, 6 figures, submitted to IEEE Transactions on Communications
  (Invited Paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid analog/digital precoding offers a compromise between hardware
complexity and system performance in millimeter wave (mmWave) systems. This
type of precoding allows mmWave systems to leverage large antenna array gains
that are necessary for sufficient link margin, while permitting low cost and
power consumption hardware. Most prior work has focused on hybrid precoding for
narrowband mmWave systems, with perfect or estimated channel knowledge at the
transmitter. MmWave systems, however, will likely operate on wideband channels
with frequency selectivity. Therefore, this paper considers wideband mmWave
systems with a limited feedback channel between the transmitter and receiver.
First, the optimal hybrid precoding design for a given RF codebook is derived.
This provides a benchmark for any other heuristic algorithm and gives useful
insights into codebook designs. Second, efficient hybrid analog/digital
codebooks are developed for spatial multiplexing in wideband mmWave systems.
Finally, a low-complexity yet near-optimal greedy frequency selective hybrid
precoding algorithm is proposed based on Gram-Schmidt orthogonalization.
Simulation results show that the developed hybrid codebooks and precoder
designs achieve very good performance compared with the unconstrained solutions
while requiring much less complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00618</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00618</id><created>2015-10-02</created><updated>2015-10-05</updated><authors><author><keyname>Fernandez-Fernandez</keyname><forenames>Miguel</forenames></author><author><keyname>Gayo-Avello</keyname><forenames>Daniel</forenames></author></authors><title>Automatic Taxonomy Extraction from Query Logs with no Additional Sources
  of Information</title><categories>cs.CL</categories><comments>21 pages, 4 figures, 5 tables. Old (2012) unpublished manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engine logs store detailed information on Web users interactions.
Thus, as more and more people use search engines on a daily basis, important
trails of users common knowledge are being recorded in those files. Previous
research has shown that it is possible to extract concept taxonomies from full
text documents, while other scholars have proposed methods to obtain similar
queries from query logs. We propose a mixture of both lines of research, that
is, mining query logs not to find related queries nor query hierarchies, but
actual term taxonomies that could be used to improve search engine
effectiveness and efficiency. As a result, in this study we have developed a
method that combines lexical heuristics with a supervised classification model
to successfully extract hyponymy relations from specialization search patterns
revealed from log missions, with no additional sources of information, and in a
language independent way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00620</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00620</id><created>2015-10-02</created><authors><author><keyname>Centenaro</keyname><forenames>Marco</forenames></author><author><keyname>Vangelista</keyname><forenames>Lorenzo</forenames></author><author><keyname>Zanella</keyname><forenames>Andrea</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Long-Range Communications in Unlicensed Bands: the Rising Stars in the
  IoT and Smart City Scenarios</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectivity is probably the most basic building block of the Internet of
Things (IoT) paradigm. Up to know, the two main approaches to provide data
access to the \emph{things} have been based either on multi-hop mesh networks
using short-range communication technologies in the unlicensed spectrum, or on
long-range, legacy cellular technologies, mainly 2G/GSM, operating in the
corresponding licensed frequency bands. Recently, these reference models have
been challenged by a new type of wireless connectivity, characterized by
low-rate, long-range transmission technologies in the unlicensed sub-GHz
frequency bands, used to realize access networks with star topology which are
referred to a \emph{Low-Power Wide Area Networks} (LPWANs). In this paper, we
introduce this new approach to provide connectivity in the IoT scenario,
discussing its advantages over the established paradigms in terms of
efficiency, effectiveness, and architectural design, in particular for the
typical Smart Cities applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00627</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00627</id><created>2015-10-02</created><authors><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Multi-armed Bandits with Application to 5G Small Cells</title><categories>cs.LG cs.DC cs.NI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the pervasive demand for mobile services, next generation wireless
networks are expected to be able to deliver high date rates while wireless
resources become more and more scarce. This requires the next generation
wireless networks to move towards new networking paradigms that are able to
efficiently support resource-demanding applications such as personalized mobile
services. Examples of such paradigms foreseen for the emerging fifth generation
(5G) cellular networks include very densely deployed small cells and
device-to-device communications. For 5G networks, it will be imperative to
search for spectrum and energy-efficient solutions to the resource allocation
problems that i) are amenable to distributed implementation, ii) are capable of
dealing with uncertainty and lack of information, and iii) can cope with users'
selfishness. The core objective of this article is to investigate and to
establish the potential of multi-armed bandit (MAB) framework to address this
challenge. In particular, we provide a brief tutorial on bandit problems,
including different variations and solution approaches. Furthermore, we discuss
recent applications as well as future research directions. In addition, we
provide a detailed example of using an MAB model for energy-efficient small
cell planning in 5G networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00633</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00633</id><created>2015-10-02</created><authors><author><keyname>Wang</keyname><forenames>Jialei</forenames></author><author><keyname>Kolar</keyname><forenames>Mladen</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Distributed Multitask Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed multi-task learning, where each
machine learns a separate, but related, task. Specifically, each machine learns
a linear predictor in high-dimensional space,where all tasks share the same
small support. We present a communication-efficient estimator based on the
debiased lasso and show that it is comparable with the optimal centralized
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00634</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00634</id><created>2015-10-02</created><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Lecroq</keyname><forenames>Thierry</forenames></author><author><keyname>Lefebvre</keyname><forenames>Arnaud</forenames></author><author><keyname>Prieur-Gaston</keyname><forenames>&#xc9;lise</forenames></author><author><keyname>Smyth</keyname><forenames>William F.</forenames></author></authors><title>A Note on Easy and Efficient Computation of Full Abelian Periods of a
  Word</title><categories>cs.DS cs.DM cs.FL</categories><comments>Accepted for publication in Discrete Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constantinescu and Ilie (Bulletin of the EATCS 89, 167-170, 2006) introduced
the idea of an Abelian period with head and tail of a finite word. An Abelian
period is called full if both the head and the tail are empty. We present a
simple and easy-to-implement $O(n\log\log n)$-time algorithm for computing all
the full Abelian periods of a word of length $n$ over a constant-size alphabet.
Experiments show that our algorithm significantly outperforms the $O(n)$
algorithm proposed by Kociumaka et al. (Proc. of STACS, 245-256, 2013) for the
same problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00649</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00649</id><created>2015-10-02</created><authors><author><keyname>Hossain</keyname><forenames>M. M. Aftab</forenames></author><author><keyname>Cavdar</keyname><forenames>Cicek</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author></authors><title>Energy-Efficient Load-Adaptive Massive MIMO</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1411.1998</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a promising technique to meet the exponential growth of
global mobile data traffic demand. However, contrary to the current systems,
energy consumption of next generation networks is required to be load adaptive
as the network load varies significantly throughout the day. In this paper, we
propose a load adaptive massive MIMO system that varies the number of antennas
following the daily load profile (DLP) in order to maximize the downlink energy
efficiency (EE). A multi-cell system is considered where each base station (BS)
is equipped with a large number of antennas to serve many single antenna users.
In order to incorporate DLP, each BS is modeled as an M/G/m/m state dependent
queue under the assumption that the network is dimensioned to serve a maximum
number of users at the peak load. For a given number of users in a cell, the
optimum number of active antennas maximizing EE is derived. The EE maximization
problem is formulated in a game theoretic framework where the number of
antennas to be used by a BS is determined through best response iteration. This
load adaptive system achieves overall 19% higher EE compared to a baseline
system where the BSs always run with the fixed number of antennas that is most
energy efficient at peak load and that can be switched-off when there is no
traffic
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00651</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00651</id><created>2015-10-02</created><authors><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author></authors><title>Project Maelstrom: Forensic Analysis of the BitTorrent-Powered Browser</title><categories>cs.CR</categories><journal-ref>Journal of Digital Forensics, Security and Law (Proc. of 10th
  International Conference on Systematic Approaches to Digital Forensic
  Engineering, SADFE 2015)</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In April 2015, BitTorrent Inc. released their distributed peer-to-peer
powered browser, Project Maelstrom, into public beta. The browser facilitates a
new alternative website distribution paradigm to the traditional HTTP-based,
client-server model. This decentralised web is powered by each of the visitors
accessing each Maelstrom hosted website. Each user shares their copy of the
website's source code and multimedia content with new visitors. As a result, a
Maelstrom hosted website cannot be taken offline by law enforcement or any
other parties. Due to this open distribution model, a number of interesting
censorship, security and privacy considerations are raised. This paper explores
the application, its protocol, sharing Maelstrom content and its new visitor
powered &quot;web-hosting&quot; paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00661</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00661</id><created>2015-10-02</created><authors><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Kohlmann</keyname><forenames>Stephen</forenames></author><author><keyname>Khac</keyname><forenames>Nhien-An Le</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author></authors><title>HTML5 Zero Configuration Covert Channels: Security Risks and Challenges</title><categories>cs.CR cs.CY cs.NI</categories><comments>15 pages; Proc. of Tenth ADFSL Conference on Digital Forensics,
  Security and Law (CDFSL 2015)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In recent months there has been an increase in the popularity and public
awareness of secure, cloudless file transfer systems. The aim of these services
is to facilitate the secure transfer of files in a peer-to- peer (P2P) fashion
over the Internet without the need for centralised authentication or storage.
These services can take the form of client installed applications or entirely
web browser based interfaces. Due to their P2P nature, there is generally no
limit to the file sizes involved or to the volume of data transmitted - and
where these limitations do exist they will be purely reliant on the capacities
of the systems at either end of the transfer. By default, many of these
services provide seamless, end-to-end encryption to their users. The
cybersecurity and cyberforensic consequences of the potential criminal use of
such services are significant. The ability to easily transfer encrypted data
over the Internet opens up a range of opportunities for illegal use to
cybercriminals requiring minimal technical know-how. This paper explores a
number of these services and provides an analysis of the risks they pose to
corporate and governmental security. A number of methods for the forensic
investigation of such transfers are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00664</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00664</id><created>2015-10-02</created><authors><author><keyname>Schut</keyname><forenames>Hessel</forenames></author><author><keyname>Scanlon</keyname><forenames>Mark</forenames></author><author><keyname>Farina</keyname><forenames>Jason</forenames></author><author><keyname>Le-Khac</keyname><forenames>Nhien-An</forenames></author></authors><title>Towards the Forensic Identification and Investigation of Cloud Hosted
  Servers through Noninvasive Wiretaps</title><categories>cs.DC cs.CR cs.NI</categories><comments>Proceedings of 10th International Conference on Availability,
  Reliability and Security (ARES 2015)</comments><doi>10.1109/ARES.2015.77</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When conducting modern cybercrime investigations, evidence has often to be
gathered from computer systems located at cloud-based data centres of hosting
providers. In cases where the investigation cannot rely on the cooperation of
the hosting provider, or where documentation is not available, investigators
can often find the identification of which distinct server among many is of
interest difficult and extremely time consuming. To address the problem of
identifying these servers, in this paper a new approach to rapidly and reliably
identify these cloud hosting computer systems is presented. In the outlined
approach, a handheld device composed of an embedded computer combined with a
method of undetectable interception of Ethernet based communications is
presented. This device is tested and evaluated, and a discussion is provided on
its usefulness in identifying of server of interest to an investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00684</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00684</id><created>2015-09-30</created><authors><author><keyname>Pergel</keyname><forenames>Martin</forenames></author></authors><title>A Note on Graphs with 2 Bends</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show NP-completeness for the recognition problem of 2-line-bend graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00697</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00697</id><created>2015-10-02</created><authors><author><keyname>Juglaret</keyname><forenames>Yannis</forenames></author><author><keyname>Hritcu</keyname><forenames>Catalin</forenames></author><author><keyname>de Amorim</keyname><forenames>Arthur Azevedo</forenames></author><author><keyname>Pierce</keyname><forenames>Benjamin C.</forenames></author><author><keyname>Spector-Zabusky</keyname><forenames>Antal</forenames></author><author><keyname>Tolmach</keyname><forenames>Andrew</forenames></author></authors><title>Towards a Fully Abstract Compiler Using Micro-Policies: Secure
  Compilation for Mutually Distrustful Components</title><categories>cs.PL cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure compilation prevents all low-level attacks on compiled code and allows
for sound reasoning about security in the source language. In this work we
propose a new attacker model for secure compilation that extends the well-known
notion of full abstraction to ensure protection for mutually distrustful
components. We devise a compiler chain (compiler, linker, and loader) and a
novel security monitor that together defend against this strong attacker model.
The monitor is implemented using a recently proposed, generic tag-based
protection framework called micro-policies, which comes with hardware support
for efficient caching and with a formal verification methodology. Our monitor
protects the abstractions of a simple object-oriented language---class
isolation, the method call discipline, and type safety---against arbitrary
low-level attackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00701</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00701</id><created>2015-10-02</created><authors><author><keyname>Sambasivan</keyname><forenames>Abhinav V.</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis D.</forenames></author></authors><title>Minimax Lower Bounds for Noisy Matrix Completion Under Sparse Factor
  Models</title><categories>cs.IT math.IT stat.ML</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines fundamental error characteristics for a general class of
matrix completion problems, where matrix of interest is a product of two a
priori unknown matrices, one of which is sparse, and the observations are
noisy. Our main contributions come in the form of minimax lower bounds for the
expected per-element squared error for these problems under several
noise/corruption models; specifically, we analyze scenarios where the
corruptions are characterized by additive Gaussian noise or additive
heavier-tailed (Laplace) noise, Poisson-distributed observations, and
highly-quantized (e.g., one-bit) observations. Our results establish that the
error bounds derived in (Soni et al., 2014) for complexity-regularized maximum
likelihood estimators achieve, up to multiplicative constant and logarithmic
factors, the minimax error rates in each of these noise scenarios, provided the
sparse factor exhibits linear sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00722</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00722</id><created>2015-10-02</created><authors><author><keyname>Guih&#xe9;neuf</keyname><forenames>Pierre-Antoine</forenames></author></authors><title>Discretizations of isometries</title><categories>cs.DM math.DS</categories><comments>12 pages + 6 pages of annex</comments><msc-class>52C23, 37M05, 37C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the dynamics of discretizations of isometries of
$\mathbf R^n$, and more precisely the density of the successive images of
$\mathbf Z^n$ by the discretizations of a generic sequence of isometries. We
show that this density tends to 0 as the time goes to infinity. Thus, in
general, all the information of a numerical image will be lost by applying many
times a naive algorithm of rotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00723</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00723</id><created>2015-10-02</created><authors><author><keyname>Guih&#xe9;neuf</keyname><forenames>Pierre-Antoine</forenames></author></authors><title>Degree of recurrence of generic diffeomorphisms</title><categories>math.DS cs.DM</categories><comments>35 pages. A part of this article is repeated in the appendices of
  arXiv:1510.00720</comments><msc-class>37M05, 37A05, 37A45, 37C20, 52C23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the spatial discretizations of dynamical systems: can we recover
some dynamical features of a system from numerical simulations? Here, we tackle
this issue for the simplest algorithm possible: we compute long segments of
orbits with a fixed number of digits. We show that the dynamics of the
discretizations of a $C^1$ generic conservative diffeomorphism of the torus is
very different from that observed in the $C^0$ regularity. The proof of our
results involves in particular a local-global formula for discretizations, as
well as a study of the corresponding linear case, which uses ideas from the
theory of quasicrystals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00726</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00726</id><created>2015-10-02</created><authors><author><keyname>Goldberg</keyname><forenames>Yoav</forenames></author></authors><title>A Primer on Neural Network Models for Natural Language Processing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, neural networks have re-emerged as powerful
machine-learning models, yielding state-of-the-art results in fields such as
image recognition and speech processing. More recently, neural network models
started to be applied also to textual natural language signals, again with very
promising results. This tutorial surveys neural network models from the
perspective of natural language processing research, in an attempt to bring
natural-language researchers up to speed with the neural techniques. The
tutorial covers input encoding for natural language tasks, feed-forward
networks, convolutional networks, recurrent networks and recursive networks, as
well as the computation graph abstraction for automatic gradient computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00738</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00738</id><created>2015-10-02</created><authors><author><keyname>Freund</keyname><forenames>Daniel</forenames></author><author><keyname>Williamson</keyname><forenames>David P.</forenames></author></authors><title>Rank Aggregation: New Bounds for MCx</title><categories>cs.DM cs.DS</categories><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank aggregation problem has received significant recent attention within
the computer science community. Its applications today range far beyond the
original aim of building metasearch engines to problems in machine learning,
recommendation systems and more. Several algorithms have been proposed for
these problems, and in many cases approximation guarantees have been proven for
them. However, it is also known that some Markov chain based algorithms (MC1,
MC2, MC3, MC4) perform extremely well in practice, yet had no known performance
guarantees. We prove supra-constant lower bounds on approximation guarantees
for all of them. We also raise the lower bound for sorting by Copeland score
from 3/2 to 2 and prove an upper bound of 11, before showing that in particular
ways, MC4 can nevertheless be seen as a generalization of Copeland score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00740</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00740</id><created>2015-10-02</created><authors><author><keyname>Shao</keyname><forenames>Sihua</forenames></author><author><keyname>Khreishah</keyname><forenames>Abdallah</forenames></author><author><keyname>Ayyash</keyname><forenames>Moussa</forenames></author></authors><title>Delay Analysis of Hybrid WiFi-LiFi System</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous wireless networks are capable of effectively leveraging
different access technologies to provide a wide variety of coverage areas. In
this paper, the coexistence of WiFi and visible light communication (VLC) is
investigated as a paradigm. The delay of two configurations of such
heterogeneous system has been evaluated. In the first configuration, the
non-aggregated system, any request is either allocated to WiFi or VLC. While in
the second configuration, the aggregated system, each request is split into two
pieces, one is forwarded to WiFi and the other is forwarded to VLC. Under the
assumptions of Poisson arrival process of requests and the exponential
distribution of requests size, it is mathematically proved that the aggregated
system provides lower minimum average system delay than that of the
non-aggregated system. For the non-aggregated system, the optimal traffic
allocation ratio is derived. For the aggregated system, an efficient solution
for the splitting ratio is proposed. Empirical results show that the solution
proposed here incurs a delay penalty (less than 3\%) over the optimal result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00745</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00745</id><created>2015-10-02</created><authors><author><keyname>Orenstein</keyname><forenames>Eric C.</forenames></author><author><keyname>Beijbom</keyname><forenames>Oscar</forenames></author><author><keyname>Peacock</keyname><forenames>Emily E.</forenames></author><author><keyname>Sosik</keyname><forenames>Heidi M.</forenames></author></authors><title>WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark
  Dataset for Plankton Classification</title><categories>cs.CV</categories><comments>2 pages, 1 figure, presented at the Third Workshop on Fine-Grained
  Visual Categorization at CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planktonic organisms are of fundamental importance to marine ecosystems: they
form the basis of the food web, provide the link between the atmosphere and the
deep ocean, and influence global-scale biogeochemical cycles. Scientists are
increasingly using imaging-based technologies to study these creatures in their
natural habit. Images from such systems provide an unique opportunity to model
and understand plankton ecosystems, but the collected datasets can be enormous.
The Imaging FlowCytobot (IFCB) at Woods Hole Oceanographic Institution, for
example, is an \emph{in situ} system that has been continuously imaging
plankton since 2006. To date, it has generated more than 700 million samples.
Manual classification of such a vast image collection is impractical due to the
size of the data set. In addition, the annotation task is challenging due to
the large space of relevant classes, intra-class variability, and inter-class
similarity. Methods for automated classification exist, but the accuracy is
often below that of human experts. Here we introduce WHOI-Plankton: a large
scale, fine-grained visual recognition dataset for plankton classification,
which comprises over 3.4 million expert-labeled images across 70 classes. The
labeled image set is complied from over 8 years of near continuous data
collection with the IFCB at the Martha's Vineyard Coastal Observatory (MVCO).
We discuss relevant metrics for evaluation of classification performance and
provide results for a traditional method based on hand-engineered features and
two methods based on convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00755</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00755</id><created>2015-10-02</created><authors><author><keyname>Arnold</keyname><forenames>Taylor</forenames></author></authors><title>Sparse Density Representations for Simultaneous Inference on Large
  Spatial Datasets</title><categories>stat.CO cs.DS</categories><comments>9 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large spatial datasets often represent a number of spatial point processes
generated by distinct entities or classes of events. When crossed with
covariates, such as discrete time buckets, this can quickly result in a data
set with millions of individual density estimates. Applications that require
simultaneous access to a substantial subset of these estimates become resource
constrained when densities are stored in complex and incompatible formats. We
present a method for representing spatial densities along the nodes of sparsely
populated trees. Fast algorithms are provided for performing set operations and
queries on the resulting compact tree structures. The speed and simplicity of
the approach is demonstrated on both real and simulated spatial data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00756</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00756</id><created>2015-10-02</created><authors><author><keyname>De Sa</keyname><forenames>Christopher</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using
  Hierarchy Width</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gibbs sampling on factor graphs is a widely used inference technique, which
often produces good empirical results. Theoretical guarantees for its
performance are weak: even for tree structured graphs, the mixing time of Gibbs
may be exponential in the number of variables. To help understand the behavior
of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy
width. We show that under suitable conditions on the weights, bounded hierarchy
width ensures polynomial mixing time. Our study of hierarchy width is in part
motivated by a class of factor graph templates, hierarchical templates, which
have bounded hierarchy width---regardless of the data used to instantiate them.
We demonstrate a rich application from natural language processing in which
Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human
volunteers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00757</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00757</id><created>2015-10-02</created><updated>2015-11-03</updated><authors><author><keyname>Burtini</keyname><forenames>Giuseppe</forenames></author><author><keyname>Loeppky</keyname><forenames>Jason</forenames></author><author><keyname>Lawrence</keyname><forenames>Ramon</forenames></author></authors><title>A Survey of Online Experiment Design with the Stochastic Multi-Armed
  Bandit</title><categories>stat.ML cs.LG</categories><comments>49 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive and sequential experiment design is a well-studied area in numerous
domains. We survey and synthesize the work of the online statistical learning
paradigm referred to as multi-armed bandits integrating the existing research
as a resource for a certain class of online experiments. We first explore the
traditional stochastic model of a multi-armed bandit, then explore a taxonomic
scheme of complications to that model, for each complication relating it to a
specific requirement or consideration of the experiment design context.
Finally, at the end of the paper, we present a table of known upper-bounds of
regret for all studied algorithms providing both perspectives for future
theoretical work and a decision-making tool for practitioners looking for
theoretical guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00759</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00759</id><created>2015-10-02</created><authors><author><keyname>Rahimi</keyname><forenames>Afshin</forenames></author><author><keyname>Eslami</keyname><forenames>Moharram</forenames></author><author><keyname>Vazirnezhad</keyname><forenames>Bahram</forenames></author></authors><title>It is not all downhill from here: Syllable Contact Law in Persian</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syllable contact pairs crosslinguistically tend to have a falling sonority
slope a constraint which is called the Syllable Contact Law SCL In this study
the phonotactics of syllable contacts in 4202 CVCCVC words of Persian lexicon
is investigated The consonants of Persian were divided into five sonority
categories and the frequency of all possible sonority slopes is computed both
in lexicon type frequency and in corpus token frequency Since an unmarked
phonological structure has been shown to diachronically become more frequent we
expect to see the same pattern for syllable contact pairs with falling sonority
slope The correlation of sonority categories of the two consonants in a
syllable contact pair is measured using Pointwise Mutual Information
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00760</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00760</id><created>2015-10-02</created><authors><author><keyname>Rahimi</keyname><forenames>Afshin</forenames></author><author><keyname>Vazirnezhad</keyname><forenames>Bahram</forenames></author><author><keyname>Eslami</keyname><forenames>Moharram</forenames></author></authors><title>P-trac Procedure: The Dispersion and Neutralization of Contrasts in
  Lexicon</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive acoustic cues have an important role in shaping the phonological
structure of language as a means to optimal communication. In this paper we
introduced P-trac procedure in order to track dispersion of contrasts in
different contexts in lexicon. The results of applying P-trac procedure to the
case of dispersion of contrasts in pre- consonantal contexts and in consonantal
positions of CVCC sequences in Persian provide Evidence in favor of phonetic
basis of dispersion argued by Licensing by Cue hypothesis and the Dispersion
Theory of Contrast. The P- trac procedure is proved to be very effective in
revealing the dispersion of contrasts in lexicon especially when comparing the
dispersion of contrasts in different contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00761</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00761</id><created>2015-10-02</created><authors><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>On the Rate of Convergence of Mean-Field Models: Stein's Method Meets
  the Perturbation Theory</title><categories>cs.PF cs.SY math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the rate of convergence of a family of continuous-time
Markov chains (CTMC) to a mean-field model. When the mean-field model is a
finite-dimensional dynamical system with a unique equilibrium point, an
analysis based on Stein's method and the perturbation theory shows that under
some mild conditions, the stationary distributions of CTMCs converge (in the
mean-square sense) to the equilibrium point of the mean-field model if the
mean-field model is globally asymptotically stable and locally exponentially
stable. In particular, the mean square difference between the $M$th CTMC in the
steady state and the equilibrium point of the mean-field system is $O(1/M),$
where $M$ is the size of the $M$th CTMC. This approach based on Stein's method
provides a new framework for studying the convergence of CTMCs to their
mean-field limit by mainly looking into the stability of the mean-field model,
which is a deterministic system and is often easier to analyze than the CTMCs.
More importantly, this approach quantifies the rate of convergence, which
reveals the approximation error of using mean-field models for approximating
finite-size systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00764</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00764</id><created>2015-10-02</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Langbort</keyname><forenames>Cedric</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Strategic Compression and Transmission of Information: Crawford-Sobel
  Meet Shannon</title><categories>cs.IT cs.GT math.IT math.OC</categories><comments>submitted to the Proceedings of IEEE, Special Issue on Science of
  Information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the well-known strategic information transmission (SIT)
concept of Crawford and Sobel in information economics, from the lens of
information theory. SIT differs from the conventional communication paradigms
in information theory since it involves different objectives for the encoder
and the decoder, which are aware of this mismatch and act accordingly. This
leads to a game whose equilibrium solutions are studied here. The problem is
modeled as a Stackelberg game-as opposed to the Nash model used in prior work
in economics. The transmitter is the leader, and the receiver is the follower.
As leader, the transmitter announces an encoding strategy with full commitment,
and its distortion measure depends on a private information sequence which is
non-causally available --only to the transmitter. Three problem settings are
considered, focusing on the quadratic distortion measures and jointly Gaussian
source and private information: compression, communication, and the simple
equilibrium conditions without any compression or communication. The
equilibrium strategies and associated costs are characterized. The analysis is
then extended to the receiver side information setting. Finally, several
applications of the results within the broader context of decision theory are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00771</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00771</id><created>2015-10-02</created><authors><author><keyname>Jaramillo</keyname><forenames>Carlos</forenames><affiliation>City University of New York, Graduate Center</affiliation></author></authors><title>Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor
  Micro Aerial Vehicles (MAVs)</title><categories>cs.CV cs.RO cs.SY</categories><comments>49 pages, 22 figures, journal article draft</comments><journal-ref>Sensors 16 (2016) 217</journal-ref><doi>10.3390/s16020217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the design and 3D sensing performance of an omnidirectional
stereo-vision system (omnistereo) as applied to Micro Aerial Vehicles (MAVs).
The proposed omnistereo model employs a monocular camera that is co-axially
aligned with a pair of hyperboloidal mirrors (folded catadioptric
configuration). We show that this arrangement is practical for performing
stereo-vision when mounted on top of propeller-based MAVs characterized by low
payloads. The theoretical single viewpoint (SVP) constraint helps us derive
analytical solutions for the sensor's projective geometry and generate
SVP-compliant panoramic images to compute 3D information from stereo
correspondences (in a truly synchronous fashion). We perform an extensive
analysis on various system characteristics such as its size, catadioptric
spatial resolution, field-of-view. In addition, we pose a probabilistic model
for uncertainty estimation of the depth from triangulation for skew
back-projection rays. We expect to motivate the reproducibility of our solution
since it can be adapted (optimally) to other catadioptric-based omnistereo
vision applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00772</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00772</id><created>2015-10-02</created><authors><author><keyname>Choi</keyname><forenames>Sou-Cheng T.</forenames></author></authors><title>Machine Learning for Machine Data from a CATI Network</title><categories>cs.LG</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a machine learning application paper involving big data. We present
high-accuracy prediction methods of rare events in semi-structured machine log
files, which are produced at high velocity and high volume by NORC's
computer-assisted telephone interviewing (CATI) network for conducting surveys.
We judiciously apply natural language processing (NLP) techniques and
data-mining strategies to train effective learning and prediction models for
classifying uncommon error messages in the log---without access to source code,
updated documentation or dictionaries. In particular, our simple but effective
approach of features preallocation for learning from imbalanced data coupled
with naive Bayes classifiers can be conceivably generalized to supervised or
semi-supervised learning and prediction methods for other critical events such
as cyberattack detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00773</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00773</id><created>2015-10-02</created><authors><author><keyname>Ye</keyname><forenames>Junjie</forenames></author></authors><title>A Note on Finding Dual Feedback Vertex Set</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an edge-bicolored graph $G$ where each edge is colored either red or
blue, a vertex set $S$ is a dual feedback vertex set if $S$ hits all blue
cycles and red cycles of $G$. In this paper, we show that a dual feedback
vertex set of size at most $k$ can be found in time $O^*(c_1^k)$ and all
minimal dual feedback vertex set of size at most $k$ can be enumerated in time
$O^*(c_2^{k^2 + k})$ by compact representations for constants $c_1$ and $c_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00781</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00781</id><created>2015-10-03</created><authors><author><keyname>Yang</keyname><forenames>Yingxiang</forenames></author><author><keyname>Park</keyname><forenames>Leonard T.</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author><author><keyname>Glass</keyname><forenames>Arnold</forenames></author><author><keyname>Sinha</keyname><forenames>Neha</forenames></author></authors><title>Prospect Pricing in Cognitive Radio Networks</title><categories>cs.GT cs.IT math.IT</categories><comments>To appear in IEEE TCCN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in cognitive radio networks have primarily focused on the design of
spectrally agile radios and novel spectrum sharing techniques that are founded
on Expected Utility Theory (EUT). In this paper, we consider the development of
novel spectrum sharing algorithms in such networks taking into account human
psychological behavior of the end-users, which often deviates from EUT.
Specifically, we consider the impact of end-user decision making on pricing and
management of radio resources in a cognitive radio enabled network when there
is uncertainty in the Quality of Service (QoS) guarantees offered by the
Service Provider (SP). Using Prospect Theory (a Nobel-Prize-winning behavioral
economic theory that captures human decision making and its deviation from
EUT), we design data pricing and channel allocation algorithms for use in
cognitive radio networks by formulating a game theoretic analysis of the
interplay between the price offerings, bandwidth allocation by the SP and the
service choices made by end-users. We show that, when the end-users
under-weight the service guarantee, they tend to reject the offer which results
in under-utilization of radio resources and revenue loss. We propose prospect
pricing, a pricing mechanism that can make the system robust to decision making
and improve radio resource management. We present analytical results as well as
preliminary human subject studies with video QoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00783</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00783</id><created>2015-10-03</created><authors><author><keyname>Almishari</keyname><forenames>Mishari</forenames></author><author><keyname>Oguz</keyname><forenames>Ekin</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>Trilateral Large-Scale OSN Account Linkability Study</title><categories>cs.CY cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, Online Social Networks (OSNs) have taken the world by
storm. They range from superficial to professional, from focused to
general-purpose, and, from free-form to highly structured. Numerous people have
multiple accounts within the same OSN and even more people have an account on
more than one OSN. Since all OSNs involve some amount of user input, often in
written form, it is natural to consider whether multiple incarnations of the
same person in various OSNs can be effectively correlated or linked. One
intuitive means of linking accounts is by using stylometric analysis.
  This paper reports on (what we believe to be) the first trilateral
large-scale stylometric OSN linkability study. Its outcome has important
implications for OSN privacy. The study is trilateral since it involves three
OSNs with very different missions: (1) Yelp, known primarily for its
user-contributed reviews of various venues, e.g, dining and entertainment, (2)
Twitter, popular for its pithy general-purpose micro-blogging style, and (3)
Flickr, used exclusively for posting and labeling (describing) photographs. As
our somewhat surprising results indicate, stylometric linkability of accounts
across these heterogeneous OSNs is both viable and quite effective. The main
take-away of this work is that, despite OSN heterogeneity, it is very
challenging for one person to maintain privacy across multiple active accounts
on different OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00797</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00797</id><created>2015-10-03</created><authors><author><keyname>Mio</keyname><forenames>Matteo</forenames></author><author><keyname>Simpson</keyname><forenames>Alex</forenames></author></authors><title>{\L}ukasiewicz {\mu}-calculus</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper explores properties of the {\L}ukasiewicz {\mu}-calculus, or
{\L}{\mu} for short, an extension of {\L}ukasiewicz logic with scalar
multiplication and least and greatest fixed-point operators (for monotone
formulas). We observe that {\L}{\mu} terms, with $n$ variables, define monotone
piecewise linear functions from $[0, 1]^n$ to $[0, 1]$. Two effective
procedures for calculating the output of {\L}{\mu} terms on rational inputs are
presented. We then consider the {\L}ukasiewicz modal {\mu}-calculus, which is
obtained by adding box and diamond modalities to {\L}{\mu}. Alternatively, it
can be viewed as a generalization of Kozen's modal {\mu}-calculus adapted to
probabilistic nondeterministic transition systems (PNTS's). We show how
properties expressible in the well-known logic PCTL can be encoded as
{\L}ukasiewicz modal {\mu}-calculus formulas. We also show that the algorithms
for computing values of {\L}ukasiewicz {\mu}-calculus terms provide automatic
(albeit impractical) methods for verifying {\L}ukasiewicz modal {\mu}-calculus
properties of finite rational PNTS's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00798</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00798</id><created>2015-10-03</created><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Delay-optimal Data Transmission in Renewable Energy Aided Cognitive
  Radio Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewable energy powered cognitive radio (CR) network has gained much
attention due to its combination of the CR's spectrum efficiency and the
renewable energy's &quot;green&quot; nature. In the paper, we investigate the
delay-optimal data transmission in the renewable energy aided CR networks.
Specifically, a primary user (PU) and a secondary user (SU) share the same
frequency in an area. The SU's interference to the PU is controlled by
interference-signal-ratio (ISR) constraint, which means that the ISR at the PU
receiver (Rx) should be less than a threshold. Under this constraint, the
renewable energy powered SU aims to minimize the average data buffer delay by
scheduling the renewable allocations in each slot. A constrained stochastic
optimization problem is formulated when the randomness of the renewable
arrival, the uncertainty of the SU's data generation, and the variability of
the fading channel are taken into account. By analyzing the formulated problem,
we propose two practical algorithms that is optimal for two special scenarios.
And the two algorithms respectively give an upper and a lower bound for the
general scenario. In addition, the availability of the PU's private information
at the SU is discussed. Finally, numerical simulations verify the effectiveness
of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00805</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00805</id><created>2015-10-03</created><authors><author><keyname>Kharaji</keyname><forenames>Morteza Yousefi</forenames></author><author><keyname>Rizi</keyname><forenames>Fatemeh Salehi</forenames></author></authors><title>A Fast Survey on Methods for Classification Anonymity Requirements</title><categories>cs.CR</categories><comments>5 pages, 2 figures, 1 table, (IJCSIS) International Journal of
  Computer Science and Information Security, Vol. 12, No. 4, April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymity has become a significant issue in security field by recent advances
in information technology and internet. The main objective of anonymity is
hiding and concealing entities privacy inside a system. Many methods and
protocols have been proposed with different anonymity services to provide
anonymity requirements in various fields until now. Each anonymity method or
protocol is developed using particular approach. In this paper, first, accurate
and perfect definitions of privacy and anonymity are presented then most
important problems in anonymity field are investigated. Afterwards, the numbers
of main anonymity protocols are described with necessary details. Finally, all
findings are concluded and some more future perspectives are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00817</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00817</id><created>2015-10-03</created><authors><author><keyname>Li</keyname><forenames>Qi</forenames></author></authors><title>Distributed Parameter Map-Reduce</title><categories>cs.DC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes how to convert a machine learning problem into a series
of map-reduce tasks. We study logistic regression algorithm. In logistic
regression algorithm, it is assumed that samples are independent and each
sample is assigned a probability. Parameters are obtained by maxmizing the
product of all sample probabilities. Rapid expansion of training samples brings
challenges to machine learning method. Training samples are so many that they
can be only stored in distributed file system and driven by map-reduce style
programs. The main step of logistic regression is inference. According to
map-reduce spirit, each sample makes inference through a separate map
procedure. But the premise of inference is that the map procedure holds
parameters for all features in the sample. In this paper, we propose
Distributed Parameter Map-Reduce, in which not only samples, but also
parameters are distributed in nodes of distributed filesystem. Through a series
of map-reduce tasks, we assign each sample parameters for its features, make
inference for the sample and update paramters of the model. The above processes
are excuted looply until convergence. We test the proposed algorithm in actual
hadoop production environment. Experiments show that the acceleration of the
algorithm is in linear relationship with the number of cluster nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00819</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00819</id><created>2015-10-03</created><authors><author><keyname>Manral</keyname><forenames>Jai</forenames></author></authors><title>Intelligent Search Optimization using Artificial Fuzzy Logics</title><categories>cs.AI cs.IR</categories><comments>Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information on the web is prodigious; searching relevant information is
difficult making web users to rely on search engines for finding relevant
information on the web. Search engines index and categorize web pages according
to their contents using crawlers and rank them accordingly. For given user
query they retrieve millions of webpages and display them to users according to
web-page rank. Every search engine has their own algorithms based on certain
parameters for ranking web-pages. Search Engine Optimization (SEO) is that
technique by which webmasters try to improve ranking of their websites by
optimizing it according to search engines ranking parameters. It is the aim of
this research to identify the most popular SEO techniques used by search
engines for ranking web-pages and to establish their importance for indexing
and categorizing web data. The research tries to establish that using more SEO
parameters in ranking algorithms helps in retrieving better search results thus
increasing user satisfaction.
  In the accomplished research, a web based Meta search engine is proposed to
aggregates search results from different search engines and rank web-pages
based on new page ranking algorithm which will assign heuristic page rank to
web-pages based on SEO parameters such as title tag, Meta description, sitemap
etc. The research also provides insight into techniques which webmasters can
use for better ranking their websites in Google and Bing.
  Initial results has shown that using certain SEO parameters in present
ranking algorithm helps in retrieving more useful results for user queries.
These results generated from Meta search engine outperformed existing search
engines in terms of better retrieved search results and high precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00831</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00831</id><created>2015-10-03</created><authors><author><keyname>Wibral</keyname><forenames>Michael</forenames></author><author><keyname>Priesemann</keyname><forenames>Viola</forenames></author><author><keyname>Kay</keyname><forenames>Jim W.</forenames></author><author><keyname>Lizier</keyname><forenames>Joseph T.</forenames></author><author><keyname>Phillips</keyname><forenames>William A.</forenames></author></authors><title>Partial Information Decomposition as a Unified Approach to the
  Specification of Neural Goal Functions</title><categories>q-bio.NC cs.IT math.IT nlin.AO physics.data-an</categories><comments>21 pages, 4 figures, appendix</comments><doi>10.1016/j.bandc.2015.09.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many neural systems anatomical motifs are present repeatedly, but despite
their structural similarity they can serve very different tasks. A prime
example for such a motif is the canonical microcircuit of six-layered
neo-cortex, which is repeated across cortical areas, and is involved in a
number of different tasks (e.g.sensory, cognitive, or motor tasks). This
observation has spawned interest in finding a common underlying principle, a
'goal function', of information processing implemented in this structure. By
definition such a goal function, if universal, cannot be cast in
processing-domain specific language (e.g. 'edge filtering', 'working memory').
Thus, to formulate such a principle, we have to use a domain-independent
framework. Information theory offers such a framework. However, while the
classical framework of information theory focuses on the relation between one
input and one output (Shannon's mutual information), we argue that neural
information processing crucially depends on the combination of
\textit{multiple} inputs to create the output of a processor. To account for
this, we use a very recent extension of Shannon Information theory, called
partial information decomposition (PID). PID allows to quantify the information
that several inputs provide individually (unique information), redundantly
(shared information) or only jointly (synergistic information) about the
output. First, we review the framework of PID. Then we apply it to reevaluate
and analyze several earlier proposals of information theoretic neural goal
functions (predictive coding, infomax, coherent infomax, efficient coding). We
find that PID allows to compare these goal functions in a common framework, and
also provides a versatile approach to design new goal functions from first
principles. Building on this, we design and analyze a novel goal function,
called 'coding with synergy'. [...]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00832</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00832</id><created>2015-10-03</created><authors><author><keyname>Lim</keyname><forenames>Sung Hoon</forenames></author><author><keyname>Kim</keyname><forenames>Kwang Taik</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Distributed Decode-Forward for Relay Networks</title><categories>cs.IT math.IT</categories><comments>33 pages, 2 figures, submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new coding scheme for general N-node relay networks is presented for
unicast, multicast, and broadcast. The proposed distributed decode-forward
scheme combines and generalizes Marton coding for single-hop broadcast channels
and the Cover-El Gamal partial decode-forward coding scheme for 3-node relay
channels. The key idea of the scheme is to precode all the codewords of the
entire network at the source by multicoding over multiple blocks. This encoding
step allows these codewords to carry partial information of the messages
implicitly, which is then recovered at the relay nodes and forwarded further.
For N-node Gaussian unicast, multicast, and broadcast relay networks, the
scheme achieves within 0.5N bits from the cutset bound and thus from the
capacity (region), regardless of the network topology, channel gains, or power
constraints. Roughly speaking, distributed decode-forward is dual to noisy
network coding, which generalized compress-forward to unicast, multicast, and
multiple access relay networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00839</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00839</id><created>2015-10-03</created><updated>2015-11-08</updated><authors><author><keyname>Evra</keyname><forenames>Shai</forenames></author><author><keyname>Kaufman</keyname><forenames>Tali</forenames></author></authors><title>Bounded Degree Cosystolic Expanders of Every Dimension</title><categories>math.CO cs.CC cs.DM math.GR math.GT</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years a high dimensional theory of expanders has emerged. The
notion of combinatorial expansion of graphs (i.e. the Cheeger constant of a
graph) has seen two generalizations to high dimensional simplicial complexes.
One generalization, known as coboundary expansion, is due to Linial and
Meshulem; the other, which we term here cosystolic expansion, is due to Gromov,
who showed that cosystolic expanders have the topological overlapping property.
No construction (either random or explicit) of bounded degree combinational
expanders (according to either definition) were known until a recent work
of~\cite{KKL}. The work of~\cite{KKL} provided the first bounded degree
cosystolic expanders of dimension two. No bounded degree combinatorial
expanders are known in higher dimensions.
  In this work we present explicit {\em bounded degree} cosystolic expanders of
every dimension. This solves affirmatively an open question raised by Gromov,
who asked whether there exist bounded degree complexes with the topological
overlapping property in every dimension.
  Moreover, we provide a local to global criterion on a complex that implies
cosystolic expansion: Namely, if the $1$-skeleton graph underlying a
$d$-dimensional complex $X$ is a good expander graph and all its links are both
coboundary expanders and good expander graphs, then the $(d-1)$-dimensional
skeleton of the complex is a cosystolic expander.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00844</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00844</id><created>2015-10-03</created><authors><author><keyname>Azad</keyname><forenames>Ariful</forenames></author><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Buluc</keyname><forenames>Aydin</forenames></author><author><keyname>Demmel</keyname><forenames>James</forenames></author><author><keyname>Grigori</keyname><forenames>Laura</forenames></author><author><keyname>Schwartz</keyname><forenames>Oded</forenames></author><author><keyname>Toledo</keyname><forenames>Sivan</forenames></author><author><keyname>Williams</keyname><forenames>Samuel</forenames></author></authors><title>Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix
  Multiplication</title><categories>cs.DC cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many
high-performance graph algorithms as well as for some linear solvers, such as
algebraic multigrid. The scaling of existing parallel implementations of SpGEMM
is heavily bound by communication. Even though 3D (or 2.5D) algorithms have
been proposed and theoretically analyzed in the flat MPI model on Erdos-Renyi
matrices, those algorithms had not been implemented in practice and their
complexities had not been analyzed for the general case. In this work, we
present the first ever implementation of the 3D SpGEMM formulation that also
exploits multiple (intra-node and inter-node) levels of parallelism, achieving
significant speedups over the state-of-the-art publicly available codes at all
levels of concurrencies. We extensively evaluate our implementation and
identify bottlenecks that should be subject to further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00857</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00857</id><created>2015-10-03</created><authors><author><keyname>Cinbis</keyname><forenames>Ramazan Gokberk</forenames></author><author><keyname>Verbeek</keyname><forenames>Jakob</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Approximate Fisher Kernels of non-iid Image Models for Image
  Categorization</title><categories>cs.CV cs.LG</categories><comments>IEEE Transactions on Pattern Analysis and Machine Intelligence, in
  press, 2015</comments><doi>10.1109/TPAMI.2015.2484342</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bag-of-words (BoW) model treats images as sets of local descriptors and
represents them by visual word histograms. The Fisher vector (FV)
representation extends BoW, by considering the first and second order
statistics of local descriptors. In both representations local descriptors are
assumed to be identically and independently distributed (iid), which is a poor
assumption from a modeling perspective. It has been experimentally observed
that the performance of BoW and FV representations can be improved by employing
discounting transformations such as power normalization. In this paper, we
introduce non-iid models by treating the model parameters as latent variables
which are integrated out, rendering all local regions dependent. Using the
Fisher kernel principle we encode an image by the gradient of the data
log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate
discounting effects in the representations; suggesting that such
transformations have proven successful because they closely correspond to the
representations obtained for non-iid models. To enable tractable computation,
we rely on variational free-energy bounds to learn the hyper-parameters and to
compute approximate Fisher kernels. Our experimental evaluation results
validate that our models lead to performance improvements comparable to using
power normalization, as employed in state-of-the-art feature aggregation
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00878</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00878</id><created>2015-10-03</created><updated>2016-01-11</updated><authors><author><keyname>Alexandre</keyname><forenames>Claudio</forenames></author><author><keyname>Balsa</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Client Profiling for an Anti-Money Laundering System</title><categories>cs.LG cs.AI stat.ML</categories><comments>7 pages, 15 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a data mining approach for profiling bank clients in order to
support the process of detection of anti-money laundering operations. We first
present the overall system architecture, and then focus on the relevant
component for this paper. We detail the experiments performed on real world
data from a financial institution, which allowed us to group clients in
clusters and then generate a set of classification rules. We discuss the
relevance of the founded client profiles and of the generated classification
rules. According to the defined overall agent-based architecture, these rules
will be incorporated in the knowledge base of the intelligent agents
responsible for the signaling of suspicious transactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00886</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00886</id><created>2015-10-03</created><authors><author><keyname>Guan</keyname><forenames>Yonghui</forenames></author></authors><title>Flattenings and Koszul Young flattenings arising in complexity theory</title><categories>math.AG cs.CC</categories><comments>14 pages</comments><msc-class>14Q20, 68Q15, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I find new equations for Chow varieties, their secant varieties, and an
additional variety that arises in the study of depth 5 circuits by flattenings
and Koszul Young flattenings. This enables a new lower bound for symmetric
border rank of $x_1x_2\cdots x_d$ when $d$ is odd, and a lower bound on the
size of depth 5 circuits that compute the permanent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00888</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00888</id><created>2015-10-03</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Jiao</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Wenzhong</forenames></author><author><keyname>Fu</keyname><forenames>Xiaoming</forenames></author></authors><title>Efficient Multi-User Computation Offloading for Mobile-Edge Cloud
  Computing</title><categories>cs.NI</categories><comments>The paper has been accepted by IEEE/ACM Transactions on Networking,
  Sept. 2015. arXiv admin note: substantial text overlap with arXiv:1404.3200</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile-edge cloud computing is a new paradigm to provide cloud computing
capabilities at the edge of pervasive radio access networks in close proximity
to mobile users. In this paper, we first study the multi-user computation
offloading problem for mobile-edge cloud computing in a multi-channel wireless
interference environment. We show that it is NP-hard to compute a centralized
optimal solution, and hence adopt a game theoretic approach for achieving
efficient computation offloading in a distributed manner. We formulate the
distributed computation offloading decision making problem among mobile device
users as a multi-user computation offloading game. We analyze the structural
property of the game and show that the game admits a Nash equilibrium and
possesses the finite improvement property. We then design a distributed
computation offloading algorithm that can achieve a Nash equilibrium, derive
the upper bound of the convergence time, and quantify its efficiency ratio over
the centralized optimal solutions in terms of two important performance
metrics. We further extend our study to the scenario of multi-user computation
offloading in the multi-channel wireless contention environment. Numerical
results corroborate that the proposed algorithm can achieve superior
computation offloading performance and scale well as the user size increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00889</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00889</id><created>2015-10-03</created><authors><author><keyname>Teknomo</keyname><forenames>Kardi</forenames></author><author><keyname>Fernandez</keyname><forenames>Proceso</forenames></author></authors><title>Background Image Generation Using Boolean Operations</title><categories>cs.CV</categories><acm-class>I.4.6</acm-class><journal-ref>Philippine Computing Journal Vol 4 No 2, December 2009, pp. 43-49</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking moving objects from a video sequence requires segmentation of these
objects from the background image. However, getting the actual background image
automatically without object detection and using only the video is difficult.
In this paper, we describe a novel algorithm that generates background from
real world images without foreground detection. The algorithm assumes that the
background image is shown in the majority of the video. Given this simple
assumption, the method described in this paper is able to accurately generate,
with high probability, the background image from a video using only a small
number of binary operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00911</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00911</id><created>2015-10-04</created><authors><author><keyname>Nagy</keyname><forenames>Attila</forenames></author></authors><title>Retractable state-finite automata without outputs</title><categories>cs.FL</categories><comments>12 pages</comments><msc-class>68Q70</msc-class><journal-ref>Acta Cybernetica 16(3): 399-409 (2004),</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A homomorphism of an automaton ${\bf A}$ without outputs onto a subautomaton
${\bf B}$ of ${\bf A}$ is called a retract homomorphism if it leaves the
elements of $B$ fixed. An automaton ${\bf A}$ is called a retractable automaton
if, for every subautomaton ${\bf B}$ of ${\bf A}$, there is a retract
homomorphism of ${\bf A}$ onto ${\bf B}$. In [1] and [3], special retractable
automata are examined. The purpose of this paper is to give a complete
description of state-finite retractable automata without outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00914</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00914</id><created>2015-10-04</created><updated>2015-11-24</updated><authors><author><keyname>Sasidevan</keyname><forenames>V.</forenames></author><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author></authors><title>Co-action provides rational basis for the evolution of cooperation</title><categories>physics.soc-ph cs.GT</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How cooperation can originate and be maintained in a society of selfish
individuals is one of the central questions in evolutionary biology and social
sciences. Formally, it can be posed in the context of the Iterated Prisoners
Dilemma (IPD) game where agents interact by either cooperating or defecting in
each round, based on the information about their choices in previous rounds of
the game. It has been recently shown that social dilemmas arising in
single-stage games can be resolved in the co-action framework when the agents
are in a completely symmetric situation. Here we examine the IPD from a
co-action perspective and show that it allows cooperation to emerge and
subsequently be sustained even in the presence of noise. Specifically, we show
that the co-action equilibrium for an N -player IPD is a state in which a
majority of cooperators coexist with defectors, the exact composition being
determined by the ratio of the payoffs. For an IPD between two players, we show
that the co-action solution corresponds to a win-stay, lose-shift behavioral
rule, thereby providing a rational basis for this Pavlovian strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00917</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00917</id><created>2015-10-04</created><authors><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author><author><keyname>D'Acquisto</keyname><forenames>Giuseppe</forenames></author></authors><title>Differential Privacy: An Estimation Theory-Based Method for Choosing
  Epsilon</title><categories>cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is achieved by the introduction of Laplacian noise in
the response to a query, establishing a precise trade-off between the level of
differential privacy and the accuracy of the database response (via the amount
of noise introduced). However, the amount of noise to add is typically defined
through the scale parameter of the Laplace distribution, whose use may not be
so intuitive. In this paper we propose to use two parameters instead, related
to the notion of interval estimation, which provide a more intuitive picture of
how precisely the true output of a counting query may be gauged from the
noise-polluted one (hence, how much the individual's privacy is protected).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00918</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00918</id><created>2015-10-04</created><authors><author><keyname>Sarkar</keyname><forenames>Koushiki</forenames></author><author><keyname>Mukherjee</keyname><forenames>Diganta</forenames></author></authors><title>Testing for Characteristics of Attribute Linked Infinite Networks based
  on Small Samples</title><categories>stat.AP cs.SI</categories><comments>Working Paper (To be presented in LSCNA, in conjunction with IEEE
  ANTS, ISI Kolkata)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to study the characteristics (geometric and
otherwise) of very large attribute based undirected networks. Real-world
networks are often very large and fast evolving. Their analysis and
understanding present a great challenge. An Attribute based network is a graph
in which the edges depend on certain properties of the vertices on which they
are incident. In context of a social network, the existence of links between
two individuals may depend on certain attributes of the two of them. We use the
Lovasz type sampling strategy of observing a certain random process on a graph
locally , i.e., in the neighborhood of a node, and deriving information about
global properties of the graph. The corresponding adjacency matrix is our
primary object of interest. We study the efficiency of recently proposed
sampling strategies, modified to our set up, to estimate the degree
distribution, centrality measures, planarity etc. The limiting distributions
are derived using recently developed probabilistic techniques for random
matrices and hence we devise relevant test statistics and confidence intervals
for different parameters / hypotheses of interest. We hope that our work will
be useful for social and computer scientists for designing sampling strategies
and computational algorithms appropriate to their respective domains of
inquiry. Extensive simulations studies are done to empirically verify the
probabilistic statements made in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00920</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00920</id><created>2015-10-04</created><authors><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>A Least Squares Approach for Stable Phase Retrieval from Short-Time
  Fourier Transform Magnitude</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of recovering a signal (up to global phase) from its
short-time Fourier transform (STFT) magnitude measurements. This problem arises
in several applications, including optical imaging and speech processing. In
this paper we suggest three interrelated algorithms. The first algorithm
estimates the signal efficiently from noisy measurements by solving a simple
least-squares (LS) problem. In contrast to previously proposed algorithms, the
LS approach has stability guarantees and does not require any prior knowledge
on the sought signal. However, the recovery is guaranteed under relatively
strong restrictions on the STFT window. The second approach is guaranteed to
recover a non-vanishing signal efficiently from noise-free measurements, under
very moderate conditions on the STFT window. Finally, the third method
estimates the signal robustly from noisy measurements by solving a
semi-definite program (SDP). The proposed SDP algorithm contains an inherent
trade-off between its robustness and the restrictions on the STFT windows that
can be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00921</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00921</id><created>2015-10-04</created><authors><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Cross-convolutional-layer Pooling for Generic Visual Recognition</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.7466</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that a Deep Convolutional Neural Network (DCNN)
pretrained on a large image dataset can be used as a universal image
descriptor, and that doing so leads to impressive performance for a variety of
image classification tasks. Most of these studies adopt activations from a
single DCNN layer, usually the fully-connected layer, as the image
representation. In this paper, we proposed a novel way to extract image
representations from two consecutive convolutional layers: one layer is
utilized for local feature extraction and the other serves as guidance to pool
the extracted features. By taking different viewpoints of convolutional layers,
we further develop two schemes to realize this idea. The first one directly
uses convolutional layers from a DCNN. The second one applies the pretrained
CNN on densely sampled image regions and treats the fully-connected activations
of each image region as convolutional feature activations. We then train
another convolutional layer on top of that as the pooling-guidance
convolutional layer. By applying our method to three popular visual
classification tasks, we find our first scheme tends to perform better on the
applications which need strong discrimination on subtle object patterns within
small regions while the latter excels in the cases that require discrimination
on category-level patterns. Overall, the proposed method achieves superior
performance over existing ways of extracting image representations from a DCNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00925</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00925</id><created>2015-10-04</created><authors><author><keyname>Guha</keyname><forenames>Arjun</forenames></author><author><keyname>Saftoiu</keyname><forenames>Claudiu</forenames></author><author><keyname>Krishnamurthi</keyname><forenames>Shriram</forenames></author></authors><title>The Essence of JavaScript</title><categories>cs.PL</categories><comments>European Conference on Object-Oriented Programming (ECOOP) 2010</comments><doi>10.1007/978-3-642-14107-2_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce JavaScript to a core calculus structured as a small-step
operational semantics. We present several peculiarities of the language and
show that our calculus models them. We explicate the desugaring process that
turns JavaScript programs into ones in the core. We demonstrate faithfulness to
JavaScript using real-world test suites. Finally, we illustrate utility by
defining a security property, implementing it as a type system on the core, and
extending it to the full language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00931</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00931</id><created>2015-10-04</created><authors><author><keyname>Touati</keyname><forenames>Mikael</forenames></author><author><keyname>El-Azouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kelif</keyname><forenames>Eitan Altmanand Jean-Marc</forenames></author></authors><title>Controlled Matching Game for User Association and Resource Allocation in
  Multi-Rate WLANs</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of IEEE 802.11 based WLANs in populated areas is such that
many mobile terminals are covered by several Access Points (APs). These mobiles
have the possibility to associate to the AP with the strongest signal
(best-RSSI association scheme).This can lead to poor performances and
overloaded APs. Moreover, the well known anomaly in the protocol at the MAC
layer may also lead to very unpredictable performances and affect the system
throughput due to the presence of heterogeneous data rate nodes and the shared
nature of the 802.11 medium. The goal of this paper is to propose an
alternative approach for the association. We model the joint resource
allocation and mobile user association as a matching game with
complementarities, peer effects and selfish players\footnote{Merely interested
in maximizing their own individual throughput}. We focus on the throughput
fairness allocation induced by the saturated regime with equal packet sizes. We
propose a novel three-stages mechanism for the modeling and control of load
balancing, resource allocation and user association. We show that the proposed
mechanism can greatly improve the efficiency of 802.11 with heterogeneous nodes
and reduce the negative impact of peer effects such as the anomaly in IEEE
802.11. The mechanism can be used at the connectivity management layer to
achieve efficient APs-mobile user associations without modification of the MAC
layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00936</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00936</id><created>2015-10-04</created><updated>2015-11-18</updated><authors><author><keyname>Zarezade</keyname><forenames>Ali</forenames></author><author><keyname>Khodadadi</keyname><forenames>Ali</forenames></author><author><keyname>Farajtabar</keyname><forenames>Mehrdad</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>Correlated Cascades: Compete or Cooperate</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real world social networks, there are multiple diffusion processes which
are rarely independent. They usually interact with each other in a competitive
or cooperative manner. In this paper, motivated by the reinforcement theory in
sociology, we model the adoption behavior of users in social networks by a
multivariate marked Hawkes process. According to this model, the intensity of a
user to adopt any behavior is modeled by aggregation of behaviors of its
neighbors. The type of adopted behavior is also defined probabilistically from
the ratio of different product usages among neighbors in which a user can
exhibit a competitive or cooperative behavior. In other words, users tend to
conduct the mostly adopted behavior in a competitive environment. The resulting
inference problem is proved to be convex and is solved by using the barrier
method. The advantage of the proposed model is twofold; besides modeling
dependent cascades, it also learns the underlying latent diffusion network.
Experimental results on synthetic and two real datasets gathered from Twitter,
URL shortening and music streaming services, illustrate the superior
performance of the proposed model over alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00945</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00945</id><created>2015-10-04</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author><author><keyname>Mkrtchyan</keyname><forenames>Vahan V.</forenames></author><author><keyname>Kamalian</keyname><forenames>Rafayel R.</forenames></author></authors><title>Graph Theory</title><categories>math.CO cs.DM</categories><comments>215 pages (in Armenian)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This book is based on Graph Theory courses taught by P.A. Petrosyan, V.V.
Mkrtchyan and R.R. Kamalian at Yerevan State University.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00958</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00958</id><created>2015-10-04</created><authors><author><keyname>Varma</keyname><forenames>Akshar</forenames></author></authors><title>Tree Realization Problem: Depth, Height and Subtree Size Sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rooted tree is an important data structure, and the height, depth and
subtree size are naturally defined attributes of every node. We consider the
problem of the realization of a tree given a sequence of one of these
attributes. We also consider the problem of the existence of a tree when
multiple sequences of attributes are given or when a sequence of tuples of
attributes is given. Our most significant results are the NP-Completeness of
problems related to subtree size sequences, which we prove by non-trivial
reductions from Numerical Matching with Target Sums. By contrast we give
polynomial time algorithms for depth and height sequences. Even for the subtree
size problem, we show that the realization of some classes of trees can be
solved in O(log(n)) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00981</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00981</id><created>2015-10-04</created><updated>2015-10-17</updated><authors><author><keyname>Kang</keyname><forenames>Byeongkeun</forenames></author><author><keyname>Lee</keyname><forenames>Yeejin</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Q.</forenames></author></authors><title>Efficient Hand Articulations Tracking using Adaptive Hand Model and
  Depth map</title><categories>cs.CV</categories><comments>Advances in Visual Computing: 11th International Symposium on Visual
  Computing (ISVC'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time hand articulations tracking is important for many applications such
as interacting with virtual / augmented reality devices or tablets. However,
most of existing algorithms highly rely on expensive and high power-consuming
GPUs to achieve real-time processing. Consequently, these systems are
inappropriate for mobile and wearable devices. In this paper, we propose an
efficient hand tracking system which does not require high performance GPUs. In
our system, we track hand articulations by minimizing discrepancy between depth
map from sensor and computer-generated hand model. We also initialize hand pose
at each frame using finger detection and classification. Our contributions are:
(a) propose adaptive hand model to consider different hand shapes of users
without generating personalized hand model; (b) improve the highly efficient
frame initialization for robust tracking and automatic initialization; (c)
propose hierarchical random sampling of pixels from each depth map to improve
tracking accuracy while limiting required computations. To the best of our
knowledge, it is the first system that achieves both automatic hand model
adjustment and real-time tracking without using GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00983</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00983</id><created>2015-10-04</created><authors><author><keyname>Singh</keyname><forenames>Rahul</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author><author><keyname>Xie</keyname><forenames>Le</forenames></author></authors><title>The ISO Problem: Decentralized Stochastic Control via Bidding Schemes</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a smart-grid connecting several agents, modeled as stochastic
dynamical systems, who may be electricity consumers/producers. At each discrete
time instant, which may represent a 15 minute interval, each agent may
consume/generate some quantity of electrical energy. The Independent System
Operator (ISO) is given the task of assigning consumptions/generations to the
agents so as to maximize the sum of the utilities accrued to the agents,
subject to the constraint that energy generation equals consumption at each
time.
  This task of coordinating generation and demand has to be accomplished by the
ISO without the agents revealing their system states, dynamics, or utility/cost
functions. We show how and when a simple iterative procedure converges to the
optimal solution. The ISO iteratively obtains electricity bids by the agents,
and declares the tentative market clearing prices. In response to these prices,
the agents submit new bids.
  On the demand side, the solution yields an optimal demand response for
dynamic and stochastic loads. On the generation side, it provides the optimal
utilization of stochastically varying renewables such as solar/wind, and
generation with fossil fuel based generation with dynamic constraints such as
ramping rates. Thereby we solve a decentralized stochastic control problem,
without agents sharing any information about their system models, states or
utility functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00984</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00984</id><created>2015-10-04</created><authors><author><keyname>Plata-Chaves</keyname><forenames>Jorge</forenames></author><author><keyname>Bahari</keyname><forenames>Mohamad Hasan</forenames></author><author><keyname>Moonen</keyname><forenames>Marc</forenames></author><author><keyname>Bertrand</keyname><forenames>Alexander</forenames></author></authors><title>Unsupervised diffusion-based LMS for node-specific parameter estimation
  over wireless sensor networks</title><categories>cs.SY</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a distributed node-specific parameter estimation problem where each
node in a wireless sensor network is interested in the simultaneous estimation
of different vectors of parameters that can be of local interest, of common
interest to a subset of nodes, or of global interest to the whole network. We
assume a setting where the nodes do not know which other nodes share the same
estimation interests. First, we conduct a theoretical analysis on the
asymptotic bias that results in case the nodes blindly process all the local
estimates of all their neighbors to solve their own node-specific parameter
estimation problem. Next, we propose an unsupervised diffusion-based LMS
algorithm that allows each node to obtain unbiased estimates of its
node-specific vector of parameters by continuously identifying which of the
neighboring local estimates correspond to each of its own estimation tasks.
Finally, simulation experiments illustrate the efficiency of the proposed
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.00994</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.00994</id><created>2015-10-04</created><authors><author><keyname>Bidokhti</keyname><forenames>Shirin Saeedi</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Capacity Bounds for Diamond Networks with an Orthogonal Broadcast
  Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of diamond networks is studied where the broadcast component is
orthogonal and modeled by two independent bit-pipes. New upper and lower bounds
on the capacity are derived. The proof technique for the upper bound
generalizes bounding techniques of Ozarow for the Gaussian multiple description
problem (1981) and Kang and Liu for the Gaussian diamond network (2011). The
lower bound is based on Marton's coding technique and superposition coding. The
bounds are evaluated for Gaussian and binary adder multiple access channels
(MACs). For Gaussian MACs, both the lower and upper bounds strengthen the
Kang-Liu bounds and establish capacity for interesting ranges of bit-pipe
capacities. For binary adder MACs, the capacity is established for all ranges
of bit-pipe capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01006</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01006</id><created>2015-10-04</created><updated>2016-01-14</updated><authors><author><keyname>Correia</keyname><forenames>Rion Brattig</forenames></author><author><keyname>Li</keyname><forenames>Lang</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Monitoring Potential Drug Interactions and Reactions via Network
  Analysis of Instagram User Timelines</title><categories>cs.SI cs.CY cs.IR q-bio.QM stat.ML</categories><comments>Pacific Symposium on Biocomputing. 21:492-503</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Much recent research aims to identify evidence for Drug-Drug Interactions
(DDI) and Adverse Drug reactions (ADR) from the biomedical scientific
literature. In addition to this &quot;Bibliome&quot;, the universe of social media
provides a very promising source of large-scale data that can help identify DDI
and ADR in ways that have not been hitherto possible. Given the large number of
users, analysis of social media data may be useful to identify under-reported,
population-level pathology associated with DDI, thus further contributing to
improvements in population health. Moreover, tapping into this data allows us
to infer drug interactions with natural products--including cannabis--which
constitute an array of DDI very poorly explored by biomedical research thus
far. Our goal is to determine the potential of Instagram for public health
monitoring and surveillance for DDI, ADR, and behavioral pathology at large.
Using drug, symptom, and natural product dictionaries for identification of the
various types of DDI and ADR evidence, we have collected ~7000 timelines. We
report on 1) the development of a monitoring tool to easily observe user-level
timelines associated with drug and symptom terms of interest, and 2)
population-level behavior via the analysis of co-occurrence networks computed
from user timelines at three different scales: monthly, weekly, and daily
occurrences. Analysis of these networks further reveals 3) drug and symptom
direct and indirect associations with greater support in user timelines, as
well as 4) clusters of symptoms and drugs revealed by the collective behavior
of the observed population. This demonstrates that Instagram contains much
drug- and pathology specific data for public health monitoring of DDI and ADR,
and that complex network analysis provides an important toolbox to extract
health-related associations and their support from large-scale social media
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01008</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01008</id><created>2015-10-04</created><authors><author><keyname>de la Cruz</keyname><forenames>Javier</forenames></author><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Lopez</keyname><forenames>Hiram H.</forenames></author><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Rank distribution of Delsarte codes</title><categories>cs.IT math.IT</categories><msc-class>2010: 94B60, 94C99, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analogy with the Singleton defect for classical codes, we propose a
definition of rank defect for Delsarte rank-metric codes. We characterize codes
whose rank defect and dual rank defect are both zero, and prove that the rank
distribution of such codes is determined by their parameters. This extends a
result by Delsarte on the rank distribution of MRD codes. In the general case
of codes of positive defect, we show that the rank distribution is determined
by the parameters of the code, together the number of codewords of small rank.
Moreover, we prove that if the rank defect of a code and its dual are both one,
and the dimension satisfies a divisibility condition, then the number of
minimum-rank codewords and dual minimum-rank codewords is the same. Finally, we
discuss how our results specialize to Gabidulin codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01018</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01018</id><created>2015-10-04</created><authors><author><keyname>Lu</keyname><forenames>Huimin</forenames></author><author><keyname>Li</keyname><forenames>Yujie</forenames></author><author><keyname>Nakashima</keyname><forenames>Shota</forenames></author><author><keyname>Serikawa</keyname><forenames>Seiichi</forenames></author></authors><title>Single Image Dehazing through Improved Atmospheric Light Estimation</title><categories>cs.CV</categories><comments>Multimedia Tools and Applications (2015)</comments><doi>10.1007/s11042-015-2977-7</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Image contrast enhancement for outdoor vision is important for smart car
auxiliary transport systems. The video frames captured in poor weather
conditions are often characterized by poor visibility. Most image dehazing
algorithms consider to use a hard threshold assumptions or user input to
estimate atmospheric light. However, the brightest pixels sometimes are objects
such as car lights or streetlights, especially for smart car auxiliary
transport systems. Simply using a hard threshold may cause a wrong estimation.
In this paper, we propose a single optimized image dehazing method that
estimates atmospheric light efficiently and removes haze through the estimation
of a semi-globally adaptive filter. The enhanced images are characterized with
little noise and good exposure in dark regions. The textures and edges of the
processed images are also enhanced significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01022</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01022</id><created>2015-10-05</created><authors><author><keyname>Yan</keyname><forenames>Tongjiang</forenames></author><author><keyname>Liu</keyname><forenames>Yanyan</forenames></author><author><keyname>Sun</keyname><forenames>Yuhua</forenames></author></authors><title>Cyclic Codes from Two-Prime Generalized Cyclotomic Sequences of Order 6</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes have wide applications in data storage systems and communication
systems. Employing two-prime Whiteman generalized cyclotomic sequences of order
6, we construct several classes of cyclic codes over the finite field GF}(q)
and give their generator polynomials. And we also calculate the minimum
distance of some cyclic codes and give lower bounds of the minimum distance for
some other cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01025</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01025</id><created>2015-10-05</created><authors><author><keyname>Liu</keyname><forenames>Huikang</forenames></author><author><keyname>Wu</keyname><forenames>Weijie</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author></authors><title>Quadratic Optimization with Orthogonality Constraints: Explicit
  Lojasiewicz Exponent and Linear Convergence of Line-Search Methods</title><categories>math.OC cs.LG cs.NA math.NA</categories><msc-class>90C26, 90C46, 90C52</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental class of matrix optimization problems that arise in many areas
of science and engineering is that of quadratic optimization with orthogonality
constraints. Such problems can be solved using line-search methods on the
Stiefel manifold, which are known to converge globally under mild conditions.
To determine the convergence rate of these methods, we give an explicit
estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set
of critical points of the aforementioned class of problems. By combining such
an estimate with known arguments, we are able to establish the linear
convergence of a large class of line-search methods. A key step in our proof is
to establish a local error bound for the set of critical points, which may be
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01026</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01026</id><created>2015-10-05</created><authors><author><keyname>Febres</keyname><forenames>Gerardo</forenames></author><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Calculating entropy at different scales among diverse communication
  systems</title><categories>cs.IT cs.CL math.IT</categories><comments>27 pages, 6 Figures, 6 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluated the impact of changing the observation scale over the entropy
measures for text descriptions. MIDI coded Music, computer code and two human
natural languages were studied at the scale of characters, words, and at the
Fundamental Scale resulting from adjusting the symbols length used to interpret
each text-description until it produced minimum entropy. The results show that
the Fundamental Scale method is comparable with the use of words when measuring
entropy levels in written texts. However, this method can also be used in
communication systems lacking words such as music. Measuring symbolic entropy
at the fundamental scale allows to calculate quantitatively, relative levels of
complexity for different communication systems. The results open novel vision
on differences among the structure of the communication systems studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01027</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01027</id><created>2015-10-05</created><authors><author><keyname>Wang</keyname><forenames>Xinggang</forenames></author><author><keyname>Zhu</keyname><forenames>Zhuotun</forenames></author><author><keyname>Yao</keyname><forenames>Cong</forenames></author><author><keyname>Bai</keyname><forenames>Xiang</forenames></author></authors><title>Relaxed Multiple-Instance SVM with Application to Object Discovery</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-instance learning (MIL) has served as an important tool for a wide
range of vision applications, for instance, image classification, object
detection, and visual tracking. In this paper, we propose a novel method to
solve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM).
We treat the positiveness of instance as a continuous variable, use Noisy-OR
model to enforce the MIL constraints, and jointly optimize the bag label and
instance label in a unified framework. The optimization problem can be
efficiently solved using stochastic gradient decent. The extensive experiments
demonstrate that RMI-SVM consistently achieves superior performance on various
benchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision
task, common object discovery. The state-of-the-art results of object discovery
on Pascal VOC datasets further confirm the advantages of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01031</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01031</id><created>2015-10-05</created><authors><author><keyname>Xu</keyname><forenames>Guangkui</forenames></author><author><keyname>Cao</keyname><forenames>Xiwang</forenames></author></authors><title>Linear Codes With Two or Three Weights From Some Functions With Low
  Walsh Spectrum in Odd Characteristic</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes with few weights have applications in authentication codes,
secrete sharing schemes, association schemes, consumer electronics and data
storage system. In this paper, several classes of linear codes with two or
three weights are obtained from some functions with low Walsh spectrum in odd
characteristic. Numerical results show that some of the linear codes obtained
are optimal or almost optimal in the sense that they meet certain bounds on
linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01032</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01032</id><created>2015-10-05</created><updated>2016-01-08</updated><authors><author><keyname>Kamper</keyname><forenames>Herman</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author></authors><title>Deep convolutional acoustic word embeddings using word-pair side
  information</title><categories>cs.CL</categories><comments>5 pages, 3 figures; added reference, acknowledgement and link to code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have been revisiting whole words as the basic modelling unit
in speech recognition and query applications, instead of phonetic units. Such
whole-word segmental systems rely on a function that maps a variable-length
speech segment to a vector in a fixed-dimensional space; the resulting acoustic
word embeddings need to allow for accurate discrimination between different
word types, directly in the embedding space. We compare several old and new
approaches in a word discrimination task. Our best approach uses side
information in the form of known word pairs to train a Siamese convolutional
neural network (CNN): a pair of tied networks that take two speech segments as
input and produce their embeddings, trained with a hinge loss that separates
same-word pairs and different-word pairs by some margin. A word classifier CNN
performs similarly, but requires much stronger supervision. Both types of CNNs
yield large improvements over the best previously published results on the word
discrimination task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01039</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01039</id><created>2015-10-05</created><updated>2016-03-04</updated><authors><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>Moon</keyname><forenames>Eunyoung</forenames></author></authors><title>Dynamical complexity in the perception-based network formation model</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many link formation mechanisms for the evolution of social networks have been
successful to reproduce various empirical findings in social networks. However,
they have largely ignored the fact that individuals make decisions on whether
to create links to other individuals based on cost and benefit of linking, and
the fact that individuals may use perception of the network in their decision
making. In this paper, we study the evolution of social networks in terms of
perception-based strategic link formation. Here each individual has her own
perception of the actual network, and use it to decide whether to create a link
to another individual. An individual with the least perception accuracy can
benefit from updating her perception using that of the most accurate individual
via a new link. This benefit is compared to the cost of linking in decision
making. Once a new link is created, it affects the accuracies of other
individuals' perceptions, leading to a further evolution of the actual network.
As for initial actual networks, we consider homogeneous and heterogeneous
cases. The homogeneous initial actual network is modeled by Erd\H{o}s-R\'enyi
(ER) random networks, while we take a star network for the heterogeneous case.
In any cases, individual perceptions of the actual network are modeled by ER
random networks with controllable linking probability. Then the stable link
density of the actual network is found to show discontinuous transitions or
jumps according to the cost of linking. As the number of jumps is the
consequence of the dynamical complexity, we discuss the effect of initial
conditions on the number of jumps to find that the dynamical complexity
strongly depends on how much individuals initially overestimate or
underestimate the link density of the actual network. For the heterogeneous
case, the role of the highly connected individual as an information spreader is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01041</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01041</id><created>2015-10-05</created><authors><author><keyname>Shapira</keyname><forenames>Gil</forenames></author><author><keyname>Hassner</keyname><forenames>Tal</forenames></author></authors><title>GPU-Based Computation of 2D Least Median of Squares with Applications to
  Fast and Robust Line Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 2D Least Median of Squares (LMS) is a popular tool in robust regression
because of its high breakdown point: up to half of the input data can be
contaminated with outliers without affecting the accuracy of the LMS estimator.
The complexity of 2D LMS estimation has been shown to be $\Omega(n^2)$ where
$n$ is the total number of points. This high theoretical complexity along with
the availability of graphics processing units (GPU) motivates the development
of a fast, parallel, GPU-based algorithm for LMS computation. We present a CUDA
based algorithm for LMS computation and show it to be much faster than the
optimal state of the art single threaded CPU algorithm. We begin by describing
the proposed method and analyzing its performance. We then demonstrate how it
can be used to modify the well-known Hough Transform (HT) in order to
efficiently detect image lines in noisy images. Our method is compared with
standard HT-based line detection methods and shown to overcome their
shortcomings in terms of both efficiency and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01044</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01044</id><created>2015-10-05</created><authors><author><keyname>Borgstr&#xf6;m</keyname><forenames>Johannes</forenames></author><author><keyname>Gutkovas</keyname><forenames>Ram&#x16b;nas</forenames></author><author><keyname>Parrow</keyname><forenames>Joachim</forenames></author><author><keyname>Victor</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Pohjola</keyname><forenames>Johannes &#xc5;man</forenames></author></authors><title>A Sorted Semantic Framework for Applied Process Calculi</title><categories>cs.LO</categories><comments>50 pages</comments><acm-class>D.2.2; D.1.3; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applied process calculi include advanced programming constructs such as type
systems, communication with pattern matching, encryption primitives, concurrent
constraints, nondeterminism, process creation, and dynamic connection
topologies. Several such formalisms, e.g. the applied pi calculus, are
extensions of the the pi-calculus; a growing number is geared towards
particular applications or computational paradigms. Our goal is a unified
framework to represent different process calculi and notions of computation. To
this end, we extend our previous work on psi-calculi with novel abstract
patterns and pattern matching, and add sorts to the data term language, giving
sufficient criteria for subject reduction to hold. Our framework can directly
represent several existing process calculi; the resulting transition systems
are isomorphic to the originals up to strong bisimulation. We also demonstrate
different notions of computation on data terms, including cryptographic
primitives and a lambda-calculus with erratic choice. Finally, we prove
standard congruence and structural properties of bisimulation; the proof has
been machine-checked using Nominal Isabelle in the case of a single name sort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01050</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01050</id><created>2015-10-05</created><authors><author><keyname>Coutaz</keyname><forenames>Joelle</forenames><affiliation>PRIMA</affiliation></author><author><keyname>Crowley</keyname><forenames>James L.</forenames><affiliation>PRIMA</affiliation></author></authors><title>Learning about End-User Development for Smart Homes by &quot;Eating Our Own
  Dog Food&quot;</title><categories>cs.HC</categories><proxy>ccsd</proxy><journal-ref>CHI 2015, Conference on Computer Human Interaction, Apr 2015,
  Seoul, South Korea. Workshop on End-User Development for IOT Era,.
  \&amp;lt;https://chi2015.acm.org/\&amp;gt;</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SPOK is an End-User Development Environment that permits people to monitor,
control, and configure smart home services and devices. SPOK has been deployed
for more than 4 months in the homes of 5 project team members for testing and
refinement, prior to longitudinal experiments in the homes of families not
involved in the project. This article reports on the lessons learned in this
initial deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01064</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01064</id><created>2015-10-05</created><authors><author><keyname>Li</keyname><forenames>Alexander Hanbo</forenames></author><author><keyname>Bradic</keyname><forenames>Jelena</forenames></author></authors><title>Boosting in the presence of outliers: adaptive classification with
  non-convex loss functions</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the role and efficiency of the non-convex loss functions
for binary classification problems. In particular, we investigate how to design
a simple and effective boosting algorithm that is robust to the outliers in the
data. The analysis of the role of a particular non-convex loss for prediction
accuracy varies depending on the diminishing tail properties of the gradient of
the loss -- the ability of the loss to efficiently adapt to the outlying data,
the local convex properties of the loss and the proportion of the contaminated
data. In order to use these properties efficiently, we propose a new family of
non-convex losses named $\gamma$-robust losses. Moreover, we present a new
boosting framework, {\it Arch Boost}, designed for augmenting the existing work
such that its corresponding classification algorithm is significantly more
adaptable to the unknown data contamination. Along with the Arch Boosting
framework, the non-convex losses lead to the new class of boosting algorithms,
named adaptive, robust, boosting (ARB). Furthermore, we present theoretical
examples that demonstrate the robustness properties of the proposed algorithms.
In particular, we develop a new breakdown point analysis and a new influence
function analysis that demonstrate gains in robustness. Moreover, we present
new theoretical results, based only on local curvatures, which may be used to
establish statistical and optimization properties of the proposed Arch boosting
algorithms with highly non-convex loss functions. Extensive numerical
calculations are used to illustrate these theoretical properties and reveal
advantages over the existing boosting methods when data exhibits a number of
outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01072</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01072</id><created>2015-10-05</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Roditty</keyname><forenames>Liam</forenames></author><author><keyname>Seiferth</keyname><forenames>Paul</forenames></author></authors><title>Routing in Unit Disk Graphs</title><categories>cs.CG cs.DS</categories><comments>16 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a set of $n$ sites in the plane. The unit disk graph UD$(S)$ on
$S$ has vertex set $S$ and an edge between two distinct sites $s,t \in S$ if
and only if $s$ and $t$ have Euclidean distance $|st| \leq 1$.
  A routing scheme $R$ for UD$(S)$ assigns to each site $s \in S$ a label
$\ell(s)$ and a routing table $\rho(s)$. For any two sites $s, t \in S$, the
scheme $R$ must be able to route a packet from $s$ to $t$ in the following way:
given a current site $r$ (initially, $r = s$), a header $h$ (initially empty),
and the target label $\ell(t)$, the scheme $R$ may consult the current routing
table $\rho(r)$ to compute a new site $r'$ and a new header $h'$, where $r'$ is
a neighbor of $r$. The packet is then routed to $r'$, and the process is
repeated until the packet reaches $t$. The resulting sequence of sites is
called the routing path. The stretch of $R$ is the maximum ratio of the
(Euclidean) length of the routing path produced by $R$ and the shortest path in
UD$(S)$, over all pairs of distinct sites in $S$.
  For any given $\varepsilon &gt; 0$, we show how to construct a routing scheme
for UD$(S)$ with stretch $1+\varepsilon$ using labels of $O(\log n)$ bits and
routing tables of $O(\varepsilon^{-5}\log^2 n \log^2 D)$ bits, where $D$ is the
(Euclidean) diameter of UD$(S)$. The header size is $O(\log n \log D)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01077</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01077</id><created>2015-10-05</created><authors><author><keyname>Gilboa</keyname><forenames>Guy</forenames></author><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Burger</keyname><forenames>Martin</forenames></author></authors><title>Nonlinear Spectral Analysis via One-homogeneous Functionals - Overview
  and Future Prospects</title><categories>math.SP cs.CV cs.NA math.NA</categories><msc-class>35A15, 35A22, 68U10, 35P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper the motivation and theory of nonlinear spectral
representations, based on convex regularizing functionals. Some comparisons and
analogies are drawn to the fields of signal processing, harmonic analysis and
sparse representations. The basic approach, main results and initial
applications are shown. A discussion of open problems and future directions
concludes this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01083</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01083</id><created>2015-10-05</created><authors><author><keyname>Sergii</keyname><forenames>Kushch</forenames></author></authors><title>Alternative forms of representation of Boolean functions in
  Cryptographic Information Security Facilities</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><journal-ref>Scientific Journal of NSTU. 1(2014) 72-78</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work offers a new approach to the formation of functions which are used
in cryptography and cryptanalysis. It will use alternative forms of
representation of Boolean functions, that is, those which are different from
the classical form, which is formed in a Boolean basis AND-OR-NOT. An example
of this is, in particular, the formation of cryptographic functions with the
use of alternative forms of representation, namely Cognate-representation of
Boolean functions. This form is, by definition, multivariant and allows you to
choose the best variant from a plurality of possible and permissible forms.
Moreover, criteria of admissibility can be also selected depending on the
particular situation, since it is known that an improvement of one criterion
usually leads to deterioration of others. The methods suggested in the project,
exemplified by Cognate-form representations of Boolean functions, show that the
use of alternative forms of representation of Boolean functions in forming of
cryptographic functions, algorithms and devices can significantly improve their
parameters and properties. And their use in cryptographic means of protection
allow to optimize the process of logical design of cryptographic devices and
improve the safety performance of information and communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01088</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01088</id><created>2015-10-05</created><authors><author><keyname>Van</keyname><forenames>Eric Nguyen</forenames></author><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Schnez</keyname><forenames>Stephan</forenames></author></authors><title>Autonomous take-off and landing of a tethered aircraft: a simulation
  study</title><categories>cs.SY math.OC</categories><comments>This is the longer version of a paper submitted to the 2016 American
  Control Conference 2016, with more details on the simulation parameters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of autonomous launch and landing of a tethered rigid aircraft for
airborne wind energy generation is addressed. The system operates with
ground-based power conversion and pumping cycles, where the tether is
repeatedly reeled in and out of a winch installed on the ground and linked to
an electric motor/generator. In order to accelerate the aircraft to take-off
speed, the ground station is augmented with a linear motion system composed by
a slide translating on rails and controlled by a second motor. An onboard
propeller is used to sustain the forward velocity during the ascend of the
aircraft. During landing, a slight tension on the line is kept, while the
onboard control surfaces are used to align the aircraft with the rails and to
land again on them. A model-based, decentralized control approach is proposed,
capable to carry out a full cycle of launch, low-tension flight, and landing
again on the rails. The derived controller is tested via numerical simulations
with a realistic dynamical model of the system, in presence of different wind
speeds and turbulence, and its performance in terms of landing accuracy is
assessed. This study is part of a project aimed to experimentally verify the
launch and landing approach on a small-scale prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01091</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01091</id><created>2015-10-05</created><authors><author><keyname>Antonakaki</keyname><forenames>Despoina</forenames></author><author><keyname>Ioannidis</keyname><forenames>Sotiris</forenames></author><author><keyname>Fragopoulou</keyname><forenames>Paraskevi</forenames></author></authors><title>Evolving Twitter: an experimental analysis of graph properties of the
  social graph</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is one of the most prominent Online Social Networks. It covers a
significant part of the online worldwide population~20% and has impressive
growth rates. The social graph of Twitter has been the subject of numerous
studies since it can reveal the intrinsic properties of large and complex
online communities. Despite the plethora of these studies, there is a limited
cover on the properties of the social graph while they evolve over time.
Moreover, due to the extreme size of this social network (millions of nodes,
billions of edges), there is a small subset of possible graph properties that
can be efficiently measured in a reasonable timescale. In this paper we propose
a sampling framework that allows the estimation of graph properties on large
social networks. We apply this framework to a subset of Twitter's social
network that has 13.2 million users, 8.3 billion edges and covers the complete
Twitter timeline (from April 2006 to January 2015). We derive estimation on the
time evolution of 24 graph properties many of which have never been measured on
large social networks. We further discuss how these estimations shed more light
on the inner structure and growth dynamics of Twitter's social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01098</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01098</id><created>2015-10-05</created><updated>2016-01-25</updated><authors><author><keyname>Rajaei</keyname><forenames>Boshra</forenames></author><author><keyname>Tramel</keyname><forenames>Eric W.</forenames></author><author><keyname>Gigan</keyname><forenames>Sylvain</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author></authors><title>Intensity-only optical compressive imaging using a multiply scattering
  material and a double phase retrieval approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of compressive imaging is addressed using natural
randomization by means of a multiply scattering medium. To utilize the medium
in this way, its corresponding transmission matrix must be estimated. To
calibrate the imager, we use a digital micromirror device (DMD) as a simple,
cheap, and high-resolution binary intensity modulator. We propose a phase
retrieval algorithm which is well adapted to intensity-only measurements on the
camera, and to the input binary intensity patterns, both to estimate the
complex transmission matrix as well as image reconstruction. We demonstrate
promising experimental results for the proposed algorithm using the MNIST
dataset of handwritten digits as example images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01113</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01113</id><created>2015-10-05</created><updated>2015-10-06</updated><authors><author><keyname>Guerrero</keyname><forenames>Paul</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy J.</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author></authors><title>RAID: A Relation-Augmented Image Descriptor</title><categories>cs.GR cs.CV</categories><comments>Fixed affiliation and email address of first author</comments><acm-class>I.4.8; I.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As humans, we regularly interpret images based on the relations between image
regions. For example, a person riding object X, or a plank bridging two
objects. Current methods provide limited support to search for images based on
such relations. We present RAID, a relation-augmented image descriptor that
supports queries based on inter-region relations. The key idea of our
descriptor is to capture the spatial distribution of simple point-to-region
relationships to describe more complex relationships between two image regions.
We evaluate the proposed descriptor by querying into a large subset of the
Microsoft COCO database and successfully extract nontrivial images
demonstrating complex inter-region relations, which are easily missed or
erroneously classified by existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01116</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01116</id><created>2015-10-05</created><updated>2015-11-24</updated><authors><author><keyname>Barucca</keyname><forenames>Paolo</forenames></author><author><keyname>Tantari</keyname><forenames>Daniele</forenames></author><author><keyname>Lillo</keyname><forenames>Fabrizio</forenames></author></authors><title>Centrality metrics and localization in core-periphery networks</title><categories>cs.SI physics.soc-ph</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two concepts of centrality have been defined in complex networks. The first
considers the centrality of a node and many different metrics for it has been
defined (e.g. eigenvector centrality, PageRank, non-backtracking centrality,
etc). The second is related to a large scale organization of the network, the
core-periphery structure, composed by a dense core plus an outlying and
loosely-connected periphery. In this paper we investigate the relation between
these two concepts. We consider networks generated via the Stochastic Block
Model, or its degree corrected version, with a strong core-periphery structure
and we investigate the centrality properties of the core nodes and the ability
of several centrality metrics to identify them. We find that the three measures
with the best performance are marginals obtained with belief propagation,
PageRank, and degree centrality, while non-backtracking and eigenvector
centrality (or MINRES}, showed to be equivalent to the latter in the large
network limit) perform worse in the investigated networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01118</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01118</id><created>2015-10-05</created><authors><author><keyname>Ray</keyname><forenames>Nicolas</forenames><affiliation>ALICE</affiliation></author><author><keyname>Dmitry</keyname><forenames>Sokolov</forenames><affiliation>ALICE</affiliation></author></authors><title>Illustration of iterative linear solver behavior on simple 1D and 2D
  problems</title><categories>cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In geometry processing, numerical optimization methods often involve solving
sparse linear systems of equations. These linear systems have a structure that
strongly resembles to adjacency graphs of the underlying mesh. We observe how
classic linear solvers behave on this specific type of problems. For the sake
of simplicity, we minimise either the squared gradient or the squared
Laplacian, evaluated by finite differences on a regular 1D or 2D grid. We
observed the evolution of the solution for both energies, in 1D and 2D, and
with different solvers: Jacobi, Gauss-Seidel, SSOR (Symmetric successive
over-relaxation) and CG (conjugate gradient [She94]). Plotting results at
different iterations allows to have an intuition of the behavior of these
classic solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01122</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01122</id><created>2015-10-05</created><authors><author><keyname>Rupp</keyname><forenames>K.</forenames></author><author><keyname>Balay</keyname><forenames>S.</forenames></author><author><keyname>Brown</keyname><forenames>J.</forenames></author><author><keyname>Knepley</keyname><forenames>M.</forenames></author><author><keyname>McInnes</keyname><forenames>L. C.</forenames></author><author><keyname>Smith</keyname><forenames>B.</forenames></author></authors><title>On The Evolution Of User Support Topics in Computational Science and
  Engineering Software</title><categories>cs.OH cs.MS</categories><comments>2 pages, 1 figure, whitepaper for the workshop &quot;Computational Science
  &amp; Engineering Software Sustainability and Productivity Challenges&quot;</comments><msc-class>68N01</msc-class><acm-class>D.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate ten years of user support emails in the large-scale solver
library PETSc in order to identify changes in user requests. For this purpose
we assign each email thread to one or several categories describing the type of
support request. We find that despite several changes in hardware architecture
as well programming models, the relative share of emails for the individual
categories does not show a notable change over time. This is particularly
remarkable as the total communication volume has increased four-fold in the
considered time frame, indicating a considerable growth of the user base. Our
data also demonstrates that user support cannot be substituted with what is
often referred to as 'better documentation' and that the involvement of core
developers in user support is essential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01125</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01125</id><created>2015-10-05</created><authors><author><keyname>Marzouk</keyname><forenames>Fatma</forenames><affiliation>SAMOVAR</affiliation></author><author><keyname>Zagrouba</keyname><forenames>Rachid</forenames><affiliation>SAMOVAR</affiliation></author><author><keyname>Laouiti</keyname><forenames>Anis</forenames><affiliation>SAMOVAR</affiliation></author><author><keyname>Muhlethaler</keyname><forenames>Paul</forenames><affiliation>EVA</affiliation></author></authors><title>An empirical study of Unfairness and Oscillation in ETSI DCC</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Performance Evalutaion and Modeling in
  Wired and Wireless networks PEMWN' 2015, Nov 2015, Hammamet, Tunisia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -Performance of Vehicular Adhoc Networks (VANETs) in high node density
situation has long been a major field of studies. Particular attention has been
paid to the frequent exchange of Cooperative Awareness Messages (CAMs) on which
many road safety applications rely. In the present paper, se focus on the
European Telecommunications Standard Institute (ETSI) Decentralized Congestion
Control (DCC) mechanism, particularly on the evaluation of its facility layers
component when applied in the context of dense networks. For this purpose, a
set of simulations has been conducted over several scenarios, considering rural
highway and urban mobility in order to investigate unfairness and oscillation
issues, and analyze the triggering factors. The experimental results show that
the latest technical specification of the ETSI DCC presents a significant
enhancement in terms of fairness. In contrast, the stability criterion leaves
room for improvement as channel load measurement presents (i) considerable
fluctuations when only the facility layer control is applied and (i.i) severe
state oscillation when different DCC control methods are combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01127</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01127</id><created>2015-10-05</created><authors><author><keyname>Gallet</keyname><forenames>Matteo</forenames></author><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Liaison Linkages</title><categories>cs.RO cs.SC math.AG</categories><comments>40 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complete classification of hexapods - also known as Stewart Gough
platforms - of mobility one is still open. To tackle this problem, we can
associate to each hexapod of mobility one an algebraic curve, called the
configuration curve. In this paper we establish an upper bound for the degree
of this curve, assuming the hexapod is general enough. Moreover, we provide a
construction of hexapods with curves of maximal degree, which is based on
liaison, a technique used in the theory of algebraic curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01130</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01130</id><created>2015-10-05</created><authors><author><keyname>Hoeltgen</keyname><forenames>Laurent</forenames></author><author><keyname>Breu&#xdf;</keyname><forenames>Michael</forenames></author></authors><title>Bregman Iteration for Correspondence Problems: A Study of Optical Flow</title><categories>math.OC cs.CV</categories><comments>27 pages, 2 Figures,</comments><msc-class>65Kxx, 65Nxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bregman iterations are known to yield excellent results for denoising,
deblurring and compressed sensing tasks, but so far this technique has rarely
been used for other image processing problems. In this paper we give a thorough
description of the Bregman iteration, unifying thereby results of different
authors within a common framework. Then we show how to adapt the split Bregman
iteration, originally developed by Goldstein and Osher for image restoration
purposes, to optical flow which is a fundamental correspondence problem in
computer vision. We consider some classic and modern optical flow models and
present detailed algorithms that exhibit the benefits of the Bregman iteration.
By making use of the results of the Bregman framework, we address the issues of
convergence and error estimation for the algorithms. Numerical examples
complement the theoretical part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01134</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01134</id><created>2015-10-05</created><authors><author><keyname>Bachhuber</keyname><forenames>Christoph</forenames></author><author><keyname>Steinbach</keyname><forenames>Eckehard</forenames></author></authors><title>A System for Precise End-to-End Delay Measurements in Video
  Communication</title><categories>cs.MM</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low delay video transmission is becoming increasingly important. Delay
critical, video enabled applications range from teleoperation scenarios such as
controlling drones or telesurgery to autonomous control through computer vision
algorithms applied on real-time video. To judge the quality of the video
transmission in such a system, it is important to be able to precisely measure
the end-to-end (E2E) delay of the transmitted video. We present a
low-complexity system that automatically takes pairwise independent
measurements of E2E delay. The precision can be far below the millisecond
order, mainly limited by the sampling rate of the measurement system. In our
implementation, we achieve a precision of 0.5 milliseconds with a sampling rate
of 2kHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01135</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01135</id><created>2015-10-05</created><authors><author><keyname>Kerrigan</keyname><forenames>Eric C.</forenames></author></authors><title>Feedback and Time are Essential for the Optimal Control of Computing
  Systems</title><categories>cs.SY</categories><journal-ref>Proc. 5th IFAC Conference on Nonlinear Model Predictive Control
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance, reliability, cost, size and energy usage of computing
systems can be improved by one or more orders of magnitude by the systematic
use of modern control and optimization methods. Computing systems rely on the
use of feedback algorithms to schedule tasks, data and resources, but the
models that are used to design these algorithms are validated using open-loop
metrics. By using closed-loop metrics instead, such as the gap metric developed
in the control community, it should be possible to develop improved scheduling
algorithms and computing systems that have not been over-engineered.
Furthermore, scheduling problems are most naturally formulated as constraint
satisfaction or mathematical optimization problems, but these are seldom
implemented using state of the art numerical methods, nor do they explicitly
take into account the fact that the scheduling problem itself takes time to
solve. This paper makes the case that recent results in real-time model
predictive control, where optimization problems are solved in order to control
a process that evolves in time, are likely to form the basis of scheduling
algorithms of the future. We therefore outline some of the research problems
and opportunities that could arise by explicitly considering feedback and time
when designing optimal scheduling algorithms for computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01139</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01139</id><created>2015-10-05</created><authors><author><keyname>Shapin</keyname><forenames>Alexey</forenames></author><author><keyname>Kleyko</keyname><forenames>Denis</forenames></author><author><keyname>Lyamin</keyname><forenames>Nikita</forenames></author><author><keyname>Osipov</keyname><forenames>Evgeny</forenames></author><author><keyname>Melentyev</keyname><forenames>Oleg</forenames></author></authors><title>Performance Peculiarities of Viterbi Decoder in Mathworks Simulink, GNU
  Radio and Other Systems with Likewise Implementation</title><categories>cs.IT math.IT</categories><comments>4 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of convolutional codes decoding by the Viterbi algorithm
should not depend on the particular distribution of zeros and ones in the input
messages, as they are linear. However, it was identified that specific
implementations of Add-Compare-Select unit for the Viterbi Algorithm
demonstrate the decoding performance that depends on proportion of elements in
the input message. It is conjectured that the modern commercial hard- and
software defined communication equipment may also feature similar
implementation and as such their decoding performance could also vary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01140</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01140</id><created>2015-08-24</created><authors><author><keyname>Situngkir</keyname><forenames>Hokky</forenames></author></authors><title>Deconstructing Bataknese Gorga Computationally</title><categories>cs.CG cs.CY</categories><comments>10 pages, 4 figures</comments><journal-ref>BFI Working Paper Series, WP072012, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The carved and painted decorations in traditional Batak houses and buildings,
gorga, are the source of their exoticism. There are no identical patterns of
the ornaments within Batak houses and the drawings are closely related to the
way ancient Batak capture the dynamicity of the growing 'tree of life', one of
central things within their cosmology and mythology. The survey of ornaments of
Batak houses and buildings in Northern Sumatera Indonesia has made us possible
to observe the complex pattern. The fractal dimensions of the geometrical
shapes in gorga are calculated and they are conjectured into 1.5-1.6, between
the dimensional of a line and a plane. The way gorga is drawn is captured by
using some modification to the turtle geometry of L-System model, a popular
model to model the dynamics of growing plants. The result is a proposal to see
Bataknese gorga as one of traditional heritage that may enrich the studies to
the generative art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01145</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01145</id><created>2015-10-05</created><authors><author><keyname>Zhang</keyname><forenames>Yaqi</forenames></author><author><keyname>Nathan</keyname><forenames>Ralph</forenames></author><author><keyname>Sorin</keyname><forenames>Daniel J.</forenames></author></authors><title>Reduced Precision Checking to Detect Errors in Floating Point Arithmetic</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use reduced precision checking (RPC) to detect errors in
floating point arithmetic. Prior work explored RPC for addition and
multiplication. In this work, we extend RPC to a complete floating point unit
(FPU), including division and square root, and we present precise analyses of
the errors undetectable with RPC that show bounds that are smaller than prior
work. We implement RPC for a complete FPU in RTL and experimentally evaluate
its error coverage and cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01148</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01148</id><created>2015-10-05</created><updated>2015-10-29</updated><authors><author><keyname>Liu</keyname><forenames>Fanghui</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Gu</keyname><forenames>Irene Y. H.</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author></authors><title>Visual Tracking via Nonnegative Regularization Multiple Locality Coding</title><categories>cs.CV</categories><comments>8 pages, 5 figures, ICCVW</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel object tracking method based on approximated
Locality-constrained Linear Coding (LLC). Rather than using a non-negativity
constraint on encoding coefficients to guarantee these elements nonnegative, in
this paper, the non-negativity constraint is substituted for a conventional
$\ell_2$ norm regularization term in approximated LLC to obtain the similar
nonnegative effect. And we provide a detailed and adequate explanation in
theoretical analysis to clarify the rationality of this replacement. Instead of
specifying fixed K nearest neighbors to construct the local dictionary, a
series of different dictionaries with pre-defined numbers of nearest neighbors
are selected. Weights of these various dictionaries are also learned from
approximated LLC in the similar framework. In order to alleviate tracking
drifts, we propose a simple and efficient occlusion detection method. The
occlusion detection criterion mainly depends on whether negative templates are
selected to represent the severe occluded target. Both qualitative and
quantitative evaluations on several challenging sequences show that the
proposed tracking algorithm achieves favorable performance compared with other
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01153</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01153</id><created>2015-10-05</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Mauroy</keyname><forenames>Alexandre</forenames></author></authors><title>Estimation of Isostables and Basins of Attraction of Monotone Systems</title><categories>math.OC cs.SY math.DS</categories><comments>7 pages, contains material submitted to American Control Conference
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study spectral properties of the so-called monotone systems
and link these results with the celebrated Perron-Frobenius theorem for linear
positive systems. Using these spectral properties we study the geometry of
basins of attraction of monotone systems. Additionally, we show that under
certain conditions we can bound the variations in these basins under parametric
uncertainty in the vector field. We also provide a computational algorithm to
estimate the basins of attraction and illustrate the results on two and three
state monotone systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01155</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01155</id><created>2015-10-05</created><authors><author><keyname>Keuper</keyname><forenames>Janis</forenames></author><author><keyname>Pfreundt</keyname><forenames>Franz-Josef</forenames></author></authors><title>Balancing the Communication Load of Asynchronously Parallelized Machine
  Learning Algorithms</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.04956</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is the standard numerical method used to
solve the core optimization problem for the vast majority of machine learning
(ML) algorithms. In the context of large scale learning, as utilized by many
Big Data applications, efficient parallelization of SGD is in the focus of
active research. Recently, we were able to show that the asynchronous
communication paradigm can be applied to achieve a fast and scalable
parallelization of SGD. Asynchronous Stochastic Gradient Descent (ASGD)
outperforms other, mostly MapReduce based, parallel algorithms solving large
scale machine learning problems. In this paper, we investigate the impact of
asynchronous communication frequency and message size on the performance of
ASGD applied to large scale ML on HTC cluster and cloud environments. We
introduce a novel algorithm for the automatic balancing of the asynchronous
communication load, which allows to adapt ASGD to changing network bandwidths
and latencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01158</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01158</id><created>2015-10-05</created><authors><author><keyname>Gavalas</keyname><forenames>Damianos</forenames></author><author><keyname>Konstantopoulos</keyname><forenames>Charalampos</forenames></author><author><keyname>Pantziou</keyname><forenames>Grammati</forenames></author></authors><title>Design and Management of Vehicle Sharing Systems: A Survey of
  Algorithmic Approaches</title><categories>cs.DS</categories><comments>38 pages, 3 figures, 4 tables, In: &quot;Smart Cities and Homes: Key
  Enabling Technologies&quot;, M.S. Obaidat and P. Nicopolitidis (Eds.), Elsevier
  Science, in press</comments><acm-class>F.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle (bike or car) sharing represents an emerging transportation scheme
which may comprise an important link in the green mobility chain of smart city
environments. This chapter offers a comprehensive review of algorithmic
approaches for the design and management of vehicle sharing systems. Our focus
is on one-way vehicle sharing systems (wherein customers are allowed to pick-up
a vehicle at any location and return it to any other station) which best suits
typical urban journey requirements. Along this line, we present methods dealing
with the so-called asymmetric demand-offer problem (i.e. the unbalanced offer
and demand of vehicles) typically experienced in one-way sharing systems which
severely affects their economic viability as it implies that considerable human
(and financial) resources should be engaged in relocating vehicles to satisfy
customer demand. The chapter covers all planning aspects that affect the
effectiveness and viability of vehicle sharing systems: the actual system
design (e.g. number and location of vehicle station facilities, vehicle fleet
size, vehicles distribution among stations); customer incentivisation schemes
to motivate customer-based distribution of bicycles/cars (such schemes offer
meaningful incentives to users so as to leave their vehicle to a station
different to that originally intended and satisfy future user demand);
cost-effective solutions to schedule operator-based repositioning of
bicycles/cars (by employees explicitly enrolled in vehicle relocation) based on
the current and future (predicted) demand patterns (operator-based and
customer-based relocation may be thought as complementary methods to achieve
the intended distribution of vehicles among stations).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01165</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01165</id><created>2015-10-05</created><authors><author><keyname>Groshaus</keyname><forenames>Marina</forenames></author><author><keyname>Montero</keyname><forenames>Leandro</forenames></author></authors><title>Tight lower bounds on the number of bicliques in false-twin-free graphs</title><categories>cs.DM</categories><comments>16 pages, 4 figues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \emph{biclique} is a maximal bipartite complete induced subgraph of $G$.
Bicliques have been studied in the last years motivated by the large number of
applications. In particular, enumeration of the maximal bicliques has been of
interest in data analysis. Associated with this issue, bounds on the maximum
number of bicliques were given. In this paper we study bounds on the minimun
number of bicliques of a graph. Since adding false-twin vertices to $G$ does
not change the number of bicliques, we restrict to false-twin-free graphs. We
give a tight lower bound on the minimum number bicliques for a subclass of
$\{C_4$,false-twin$\}$-free graphs and for the class of
$\{K_3$,false-twin$\}$-free graphs. Finally we discuss the problem for general
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01171</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01171</id><created>2015-10-05</created><authors><author><keyname>Lafond</keyname><forenames>Jean</forenames></author><author><keyname>Wai</keyname><forenames>Hoi-To</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author></authors><title>Convergence Analysis of a Stochastic Projection-free Algorithm</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents and analyzes a stochastic version of the Frank-Wolfe
algorithm (a.k.a. conditional gradient method or projection-free algorithm) for
constrained convex optimization. We first prove that when the quality of
gradient estimate improves as ${\cal O}( \sqrt{ \eta_t^{\Delta} / t } )$, where
$t$ is the iteration index and $\eta_t^{\Delta}$ is an increasing sequence,
then the objective value of the stochastic Frank-Wolfe algorithm converges in
at least the same order. When the optimal solution lies in the interior of the
constraint set, the convergence rate is accelerated to ${\cal
O}(\eta_t^{\Delta} /t)$. Secondly, we study how the stochastic Frank-Wolfe
algorithm can be applied to a few practical machine learning problems. Tight
bounds on the gradient estimate errors for these examples are established.
Numerical simulations support our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01175</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01175</id><created>2015-10-05</created><authors><author><keyname>D&#xed;az-Morales</keyname><forenames>Roberto</forenames></author></authors><title>Cross-Device Tracking: Matching Devices and Cookies</title><categories>cs.LG cs.CY</categories><doi>10.1109/ICDMW.2015.244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of computers, tablets and smartphones is increasing rapidly, which
entails the ownership and use of multiple devices to perform online tasks. As
people move across devices to complete these tasks, their identities becomes
fragmented. Understanding the usage and transition between those devices is
essential to develop efficient applications in a multi-device world. In this
paper we present a solution to deal with the cross-device identification of
users based on semi-supervised machine learning methods to identify which
cookies belong to an individual using a device. The method proposed in this
paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections
challenge proving its good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01176</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01176</id><created>2015-10-05</created><authors><author><keyname>Zhou</keyname><forenames>Qing</forenames></author><author><keyname>Liu</keyname><forenames>Nan</forenames></author></authors><title>Energy-Efficient Data Transmission with Non-FIFO Packets</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of energy-efficient packet transmission
with arbitrary arrival instants and deadline constraints over a point-to-point
Additive White Gaussian Noise (AWGN) channel. This is different from previous
work where it is assumed that the packets follow a First-In-First-Out (FIFO)
order in that the packets that arrive earlier will have a deadline that is also
earlier. We first investigate the necessary and sufficient conditions of the
optimal transmission scheduler. We then propose an algorithm which finds the
transmission schedule of each packet in the order of the packets with the
largest transmission rate to the packets with the smallest transmission rate.
Finally, we show that our algorithm satisfies the sufficient conditions of the
optimal transmission scheduler and thus, is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01179</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01179</id><created>2015-09-23</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author></authors><title>Evolvable Autonomic Management</title><categories>cs.OH</categories><comments>arXiv admin note: text overlap with arXiv:1508.03975</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Autonomic management is aimed at adapting to uncertainty. Hence, it is
devised as m-connected k-dominating set problem, resembled by dominator and
dominate, such that dominators are resilient up to m-1 uncertainty among them
and dominate are resilient up to k-1 uncertainty on their way to dominators.
Therefore, an evolutionary algorithm GENESIS is proposed, which resolves
uncertainty by evolving population of solutions, while considering uncertain
constraints as sub-problems, started by initial populations by a greedy
algorithm AVIDO. Theoretical analysis first justifies original problem as
NP-hard problem. Eventually, the absence of polynomial time approximation
scheme necessitates justification of original problem as multiobjective
optimization problem. Furthermore, approximation to Pareto front is verified to
be decomposed into scalar optimization sub-problems, which lays out the
theoretical foundation for decomposition based evolutionary solution. Finally,
case-study, feasibility analysis and exemplary implication are presented for
evolvable autonomic management in combined cancer treatment with in-vivo sensor
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01193</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01193</id><created>2015-10-05</created><authors><author><keyname>Eaton</keyname><forenames>James</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick A.</forenames></author></authors><title>Reverberation time estimation on the ACE corpus using the SDD method</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reverberation Time (T60) is an important measure for characterizing the
properties of a room. The author's T60 estimation algorithm was previously
tested on simulated data where the noise is artificially added to the speech
after convolution with a impulse responses simulated using the image method. We
test the algorithm on speech convolved with real recorded impulse responses and
noise from the same rooms from the Acoustic Characterization of Environments
(ACE) corpus and achieve results comparable results to those using simulated
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01201</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01201</id><created>2015-10-05</created><authors><author><keyname>&#xdc;&#xe7;&#xfc;nc&#xfc;</keyname><forenames>Ali Bulut</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>Ali &#xd6;zg&#xfc;r</forenames></author></authors><title>Out-of-Band Radiation Comparison of GFDM, WCP-COQAM and OFDM at Equal
  Spectral Efficiency</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GFDM and WCP-COQAM are amongst the candidate physical layer modulation
formats to be used in 5G, whose claimed lower out-of-band (OOB) emissions are
important with respect to cognitive radio based dynamic spectrum access
solutions. In this study, we compare OFDM, GFDM and WCP-COQAM in terms of OOB
emissions in a fair manner such that their spectral efficiencies are the same
and OOB emission reduction techniques are applied to all of the modulation
types. Analytical PSD expressions are also correlated with the simulation based
OOB emission results. Maintaining the same spectral efficiency, carrier
frequency offset immunities will also be compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01210</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01210</id><created>2015-10-01</created><updated>2016-02-08</updated><authors><author><keyname>Fleiner</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Jank&#xf3;</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Tamura</keyname><forenames>Akihisa</forenames></author><author><keyname>Teytelboym</keyname><forenames>Alexander</forenames></author></authors><title>Trading Networks with Bilateral Contracts</title><categories>cs.GT q-fin.EC</categories><acm-class>J.4; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider general networks of bilateral contracts that include supply
chains. We define a new stability concept, called trail stability, and show
that any network of bilateral contracts has a trail-stable outcome whenever
agents' preferences satisfy full substitutability. Trail stability is a natural
extension of chain stability, but is a stronger solution concept in general
contract networks. Trail-stable outcomes are not immune to deviations of
arbitrary sets of firms. In fact, we show that outcomes satisfying an even more
demanding stability property -- full trail stability -- always exist. We pin
down conditions under which trail-stable and fully trail-stable outcomes have a
lattice structure. We then completely describe the relationships between all
stability concepts. When contracts specify trades and prices, we also show that
competitive equilibrium exists in networked markets even in the absence of
fully transferrable utility. The competitive equilibrium outcome is
trail-stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01225</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01225</id><created>2015-10-05</created><authors><author><keyname>Ardeshiri</keyname><forenames>Tohid</forenames></author><author><keyname>Orguner</keyname><forenames>Umut</forenames></author><author><keyname>Gustafsson</keyname><forenames>Fredrik</forenames></author></authors><title>Bayesian Inference via Approximation of Log-likelihood for Priors in
  Exponential Family</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a Bayesian inference technique based on Taylor series
approximation of the logarithm of the likelihood function is presented. The
proposed approximation is devised for the case, where the prior distribution
belongs to the exponential family of distributions. The logarithm of the
likelihood function is linearized with respect to the sufficient statistic of
the prior distribution in exponential family such that the posterior obtains
the same exponential family form as the prior. Similarities between the
proposed method and the extended Kalman filter for nonlinear filtering are
illustrated. Furthermore, an extended target measurement update for target
models where the target extent is represented by a random matrix having an
inverse Wishart distribution is derived. The approximate update covers the
important case where the spread of measurement is due to the target extent as
well as the measurement noise in the sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01234</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01234</id><created>2015-10-05</created><authors><author><keyname>Soorat</keyname><forenames>Ram</forenames></author><author><keyname>K.</keyname><forenames>Madhuri</forenames></author><author><keyname>Vudayagiri</keyname><forenames>Ashok</forenames></author></authors><title>Hardware Random number Generator for cryptography</title><categories>physics.comp-ph cs.CR physics.ins-det</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  One of the key requirement of many schemes is that of random numbers.
Sequence of random numbers are used at several stages of a standard
cryptographic protocol. A simple example is of a Vernam cipher, where a string
of random numbers is added to massage string to generate the encrypted code. It
is represented as $C=M \oplus K $ where $M$ is the message, $K$ is the key and
$C$ is the ciphertext. It has been mathematically shown that this simple scheme
is unbreakable is key K as long as M and is used only once. For a good
cryptosystem, the security of the cryptosystem is not be based on keeping the
algorithm secret but solely on keeping the key secret. The quality and
unpredictability of secret data is critical to securing communication by modern
cryptographic techniques. Generation of such data for cryptographic purposes
typically requires an unpredictable physical source of random data. In this
manuscript, we present studies of three different methods for producing random
number. We have tested them by studying its frequency, correlation as well as
using the test suit from NIST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01240</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01240</id><created>2015-10-05</created><updated>2016-02-19</updated><authors><author><keyname>Caluwaerts</keyname><forenames>Ken</forenames></author><author><keyname>Bruce</keyname><forenames>Jonathan</forenames></author><author><keyname>Friesen</keyname><forenames>Jeffrey M.</forenames></author><author><keyname>SunSpiral</keyname><forenames>Vytas</forenames></author></authors><title>State Estimation for Tensegrity Robots</title><categories>cs.RO</categories><comments>accepted for publication at the IEEE International Conference on
  Robotics and Automation (ICRA) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensegrity robots are a class of compliant robots that have many desirable
traits when designing mass efficient systems that must interact with uncertain
environments. Various promising control approaches have been proposed for
tensegrity systems in simulation. Unfortunately, state estimation methods for
tensegrity robots have not yet been thoroughly studied.
  In this paper, we present the design and evaluation of a state estimator for
tensegrity robots. This state estimator will enable existing and future control
algorithms to transfer from simulation to hardware. Our approach is based on
the unscented Kalman filter (UKF) and combines inertial measurements, ultra
wideband time-of-flight ranging measurements, and actuator state information.
  We evaluate the effectiveness of our method on the SUPERball, a tensegrity
based planetary exploration robotic prototype. In particular, we conduct tests
for evaluating both the robot's success in estimating global position in
relation to fixed ranging base stations during rolling maneuvers as well as
local behavior due to small-amplitude deformations induced by cable actuation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01257</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01257</id><created>2015-10-05</created><authors><author><keyname>Lu</keyname><forenames>Yongxi</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Efficient Object Detection for High Resolution Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient generation of high-quality object proposals is an essential step in
state-of-the-art object detection systems based on deep convolutional neural
networks (DCNN) features. Current object proposal algorithms are
computationally inefficient in processing high resolution images containing
small objects, which makes them the bottleneck in object detection systems. In
this paper we present effective methods to detect objects for high resolution
images. We combine two complementary strategies. The first approach is to
predict bounding boxes based on adjacent visual features. The second approach
uses high level image features to guide a two-step search process that
adaptively focuses on regions that are likely to contain small objects. We
extract features required for the two strategies by utilizing a pre-trained
DCNN model known as AlexNet. We demonstrate the effectiveness of our algorithm
by showing its performance on a high-resolution image subset of the SUN 2012
object detection dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01258</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01258</id><created>2015-10-05</created><authors><author><keyname>Babaheidarian</keyname><forenames>Parisa</forenames></author><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panos</forenames></author></authors><title>Finite-SNR Regime Analysis of The Gaussian Wiretap Multiple-Access
  Channel</title><categories>cs.IT math.IT</categories><comments>8 pages, 4 figures, accepted to Allerton 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a K-user Gaussian wiretap multiple-access channel
(GW-MAC) in which each transmitter has an independent confidential message for
the receiver. There is also an external eavesdropper who intercepts the
communications. The goal is to transmit the messages reliably while keeping
them confidential from the eavesdropper. To accomplish this goal, two different
approaches have been proposed in prior works, namely, i.i.d. Gaussian random
coding and real alignment. However, the former approach fails at moderate and
high SNR regimes as its achievable result does not grow with SNR. On the other
hand, while the latter approach gives a promising result at the infinite SNR
regime, its extension to the finite-SNR regime is a challenging task. To fill
the gap between the performance of the existing approaches, in this work, we
establish a new scheme in which, at the receiver's side, it utilizes an
extension of the compute-and-forward decoding strategy and at the transmitters'
side it exploits lattice alignment, cooperative jamming, and i.i.d. random
codes. For the proposed scheme, we derive a new achievable bound on sum secure
rate which scales with log(SNR) and hence it outperforms the i.i.d. Gaussian
codes in moderate and high SNR regimes. We evaluate the performance of our
scheme, both theoretically and numerically. Furthermore, we show that our sum
secure rate achieves the optimal sum secure degrees of freedom in the
infinite-SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01261</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01261</id><created>2015-10-05</created><authors><author><keyname>Zhou</keyname><forenames>Yuchen</forenames></author><author><keyname>Maity</keyname><forenames>Dipankar</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author></authors><title>Optimal Mission Planner with Timed Temporal Logic Constraints</title><categories>cs.SY cs.RO math.LO</categories><comments>European Control Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an optimization based method for path planning of a
mobile robot subject to time bounded temporal constraints, in a dynamic
environment. Temporal logic (TL) can address very complex task specification
such as safety, coverage, motion sequencing etc. We use metric temporal logic
(MTL) to encode the task specifications with timing constraints. We then
translate the MTL formulae into mixed integer linear constraints and solve the
associated optimization problem using a mixed integer linear program solver.
This approach is different from the automata based methods which generate a
finite abstraction of the environment and dynamics, and use an automata
theoretic approach to formally generate a path that satisfies the TL. We have
applied our approach on several case studies in complex dynamical environments
subjected to timed temporal specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01270</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01270</id><created>2015-10-05</created><authors><author><keyname>Kajdanowicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Musia&#x142;</keyname><forenames>Katarzyna</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Learning in Unlabeled Networks - An Active Learning and Inference
  Approach</title><categories>stat.ML cs.LG cs.SI</categories><journal-ref>AI Communications, Vol. 29, No. 1, 2016, IOS Press</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of determining labels of all network nodes based on the knowledge
about network structure and labels of some training subset of nodes is called
the within-network classification. It may happen that none of the labels of the
nodes is known and additionally there is no information about number of classes
to which nodes can be assigned. In such a case a subset of nodes has to be
selected for initial label acquisition. The question that arises is: &quot;labels of
which nodes should be collected and used for learning in order to provide the
best classification accuracy for the whole network?&quot;. Active learning and
inference is a practical framework to study this problem.
  A set of methods for active learning and inference for within network
classification is proposed and validated. The utility score calculation for
each node based on network structure is the first step in the process. The
scores enable to rank the nodes. Based on the ranking, a set of nodes, for
which the labels are acquired, is selected (e.g. by taking top or bottom N from
the ranking). The new measure-neighbour methods proposed in the paper suggest
not obtaining labels of nodes from the ranking but rather acquiring labels of
their neighbours. The paper examines 29 distinct formulations of utility score
and selection methods reporting their impact on the results of two collective
classification algorithms: Iterative Classification Algorithm and Loopy Belief
Propagation.
  We advocate that the accuracy of presented methods depends on the structural
properties of the examined network. We claim that measure-neighbour methods
will work better than the regular methods for networks with higher clustering
coefficient and worse than regular methods for networks with low clustering
coefficient. According to our hypothesis, based on clustering coefficient we
are able to recommend appropriate active learning and inference method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01276</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01276</id><created>2015-10-01</created><authors><author><keyname>Yba&#xf1;ez</keyname><forenames>Michael</forenames></author><author><keyname>Teknomo</keyname><forenames>Kardi</forenames></author><author><keyname>Fernandez</keyname><forenames>Proceso</forenames></author></authors><title>Hadamard Product Decomposition and Mutually Exclusive Matrices on
  Network Structure and Utilization</title><categories>cs.DM</categories><comments>Proceeding of the International Conference on Innovation Challenges
  in Multidisciplinary Research and Practice (ICMRP 2013), Kuala Lumpur, Dec
  13-14, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are very important mathematical structures used in many applications,
one of which is transportation science. When dealing with transportation
networks, one deals not only with the network structure, but also with
information related to the utilization of the elements of the network, which
can be shown using flow and origin-destination matrices. This paper extends an
algebraic model used to relate all these components by deriving additional
relationships and constructing a more structured understanding of the model.
Specifically, the paper introduces the concept of mutually exclusive matrices,
and shows their effect when decomposing the components of a Hadamard product on
matrices
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01288</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01288</id><created>2015-10-05</created><updated>2015-10-08</updated><authors><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Sachs</keyname><forenames>Joachim</forenames></author></authors><title>5G Ultra-Reliable Vehicular Communication</title><categories>cs.IT math.IT</categories><comments>Fixed typos. Last paragraph of Sec. 3 modified to stress
  applicability to V2X comm. Submitted to IEEE Communications Magazine, July
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications enabled by Cooperative Intelligent Transport Systems (C-ITS)
represent a major step towards making the road transport system safer and more
efficient (green), and thus suited for a sustainable future. Wireless
communication between vehicles and road infrastructure is an enabler for
high-performance C-ITS applications. State-of-the-art communication systems for
supporting low-latency C-ITS applications are based on IEEE 802.11 medium
access control (MAC) and physical (PHY) layers. In this paper, we argue that a
well-designed 5G system can complement or even replace these systems. We will
review the C-ITS application requirements and explain how these are well
aligned with the foreseen generic 5G service of ultra-reliable machine-type
communication (uMTC). Key technology components suitable for constructing the
uMTC service are identified: reliable service composition (RSC) and
device-to-device (D2D) links for all-to-all broadcast communication,
operational at high mobility and with varying degree of network assistance.
Important problems for future studies, including radio-resource management,
medium access control, and physical layer challenges, are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01291</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01291</id><created>2015-10-05</created><updated>2015-10-07</updated><authors><author><keyname>Fang</keyname><forenames>Dongping</forenames></author><author><keyname>Oberlin</keyname><forenames>Elizabeth</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author><author><keyname>Kounaves</keyname><forenames>Samuel P.</forenames></author></authors><title>A Common-Factor Approach for Multivariate Data Cleaning with an
  Application to Mars Phoenix Mission Data</title><categories>cs.AI</categories><comments>12 pages, 10 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data quality is fundamentally important to ensure the reliability of data for
stakeholders to make decisions. In real world applications, such as scientific
exploration of extreme environments, it is unrealistic to require raw data
collected to be perfect. As data miners, when it is infeasible to physically
know the why and the how in order to clean up the data, we propose to seek the
intrinsic structure of the signal to identify the common factors of
multivariate data. Using our new data driven learning method, the common-factor
data cleaning approach, we address an interdisciplinary challenge on
multivariate data cleaning when complex external impacts appear to interfere
with multiple data measurements. Existing data analyses typically process one
signal measurement at a time without considering the associations among all
signals. We analyze all signal measurements simultaneously to find the hidden
common factors that drive all measurements to vary together, but not as a
result of the true data measurements. We use common factors to reduce the
variations in the data without changing the base mean level of the data to
avoid altering the physical meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01292</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01292</id><created>2015-10-05</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Tongtong</forenames></author><author><keyname>Ren</keyname><forenames>Jian</forenames></author></authors><title>Beyond the MDS Bound in Distributed Cloud Storage</title><categories>cs.CR cs.IT math.IT</categories><comments>38 pages, 2 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage plays a crucial role in the current cloud computing
framework. After the theoretical bound for distributed storage was derived by
the pioneer work of the regenerating code, Reed-Solomon code based regenerating
codes were developed. The RS code based minimum storage regeneration code
(RS-MSR) and the minimum bandwidth regeneration code (RS-MBR) can achieve
theoretical bounds on the MSR point and the MBR point respectively in code
regeneration. They can also maintain the MDS property in code reconstruction.
However, in the hostile network where the storage nodes can be compromised and
the packets can be tampered with, the storage capacity of the network can be
significantly affected. In this paper, we propose a Hermitian code based
minimum storage regenerating (H-MSR) code and a minimum bandwidth regenerating
(H-MBR) code. We first prove that our proposed Hermitian code based
regenerating codes can achieve the theoretical bounds for MSR point and MBR
point respectively. We then propose data regeneration and reconstruction
algorithms for the H-MSR code and the H-MBR code in both error-free network and
hostile network. Theoretical evaluation shows that our proposed schemes can
detect the erroneous decodings and correct more errors in hostile network than
the RS-MSR code and the RS-MBR code with the same code rate. Our analysis also
demonstrates that the proposed H-MSR and H-MBR codes have lower computational
complexity than the RS-MSR/RS-MBR codes in both code regeneration and code
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01308</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01308</id><created>2015-10-05</created><authors><author><keyname>Hsu</keyname><forenames>Lun-Kai</forenames></author><author><keyname>Achim</keyname><forenames>Tudor</forenames></author><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author></authors><title>Tight Variational Bounds via Random Projections and I-Projections</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information projections are the key building block of variational inference
algorithms and are used to approximate a target probabilistic model by
projecting it onto a family of tractable distributions. In general, there is no
guarantee on the quality of the approximation obtained. To overcome this issue,
we introduce a new class of random projections to reduce the dimensionality and
hence the complexity of the original model. In the spirit of random
projections, the projection preserves (with high probability) key properties of
the target distribution. We show that information projections can be combined
with random projections to obtain provable guarantees on the quality of the
approximation obtained, regardless of the complexity of the original model. We
demonstrate empirically that augmenting mean field with a random projection
step dramatically improves partition function and marginal probability
estimates, both on synthetic and real world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01315</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01315</id><created>2015-10-05</created><authors><author><keyname>Deng</keyname><forenames>Weibing</forenames></author><author><keyname>Allahverdyan</keyname><forenames>Armen E.</forenames></author></authors><title>Rank-frequency relations of phonemes uncover an author-dependency of
  their usage</title><categories>cs.CL nlin.AO</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study rank-frequency relations for phonemes in texts written by different
authors. We show that they can be described by generating phonemes via random
probabilities governed by the (one-parameter) Dirichlet density, the simplest
density for random probabilities. This description allows us to demonstrate
that the rank-frequency relations for phonemes of a text do depend on the
author. The author-dependency effect is not caused by common words used in
different texts. This suggests that it is directly related to phonemes or/and
syllables. These features contrast to rank-frequency relations for words, which
are both author and text independent and are governed by the Zipf's law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01344</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01344</id><created>2015-10-05</created><authors><author><keyname>Havaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Poulin</keyname><forenames>Philippe</forenames></author><author><keyname>Jodoin</keyname><forenames>Pierre-Marc</forenames></author></authors><title>Within-Brain Classification for Brain Tumor Segmentation</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: In this paper, we investigate a framework for interactive brain
tumor segmentation which, at its core, treats the problem of interactive brain
tumor segmentation as a machine learning problem.
  Methods: This method has an advantage over typical machine learning methods
for this task where generalization is made across brains. The problem with
these methods is that they need to deal with intensity bias correction and
other MRI-specific noise. In this paper, we avoid these issues by approaching
the problem as one of within brain generalization. Specifically, we propose a
semi-automatic method that segments a brain tumor by training and generalizing
within that brain only, based on some minimum user interaction.
  Conclusion: We investigate how adding spatial feature coordinates (i.e. $i$,
$j$, $k$) to the intensity features can significantly improve the performance
of different classification methods such as SVM, kNN and random forests. This
would only be possible within an interactive framework. We also investigate the
use of a more appropriate kernel and the adaptation of hyper-parameters
specifically for each brain.
  Results: As a result of these experiments, we obtain an interactive method
whose results reported on the MICCAI-BRATS 2013 dataset are the second most
accurate compared to published methods, while using significantly less memory
and processing power than most state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01363</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01363</id><created>2015-10-05</created><updated>2015-11-02</updated><authors><author><keyname>Maya</keyname><forenames>Juan Augusto</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia G.</forenames></author></authors><title>Cooperative spectrum sensing schemes with partial statistics knowledge</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we analyze the problem of detecting spectrum holes in
cognitive radio systems. We consider that a group of unlicensed users can sense
the radio signal energy, perform some simple processing and transmit the result
to a central entity, where the decision about the presence or not of licensed
users is made. We show that the proposed cooperative schemes present good
performances even without any knowledge about the measurements statistics in
the unlicensed users and with only partial knowledge of them in the central
entity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01364</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01364</id><created>2015-09-30</created><authors><author><keyname>Horgue</keyname><forenames>Pierre</forenames><affiliation>IMFT</affiliation></author><author><keyname>Franc</keyname><forenames>Jacques</forenames><affiliation>IMFT</affiliation></author><author><keyname>Guibert</keyname><forenames>Romain</forenames><affiliation>IMFT</affiliation></author><author><keyname>Debenest</keyname><forenames>G&#xe9;rald</forenames><affiliation>IMFT</affiliation></author></authors><title>An extension of the open-source porousMultiphaseFoam toolbox dedicated
  to groundwater flows solving the Richards' equation</title><categories>cs.CE physics.class-ph</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, the existing porousMultiphaseFoam toolbox, developed initially
for any two-phase flow in porous media is extended to the specific case of the
Richards' equation which neglect the pressure gradient of the non-wetting
phase. This model is typically used for saturated and unsaturated groundwater
flows. A Picard's algorithm is implemented to linearize and solve the Richards'
equation developed in the pressure head based form. This new solver of the
porousMultiphaseFoam toolbox is named groundwaterFoam. The validation of
thesolver is achieved by a comparison between numerical simulations and results
obtained from the literature. Finally, a parallel efficiency test is performed
on a large unstructured mesh and exhibits a super-linear behavior as observed
for the other solvers of the toolbox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01367</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01367</id><created>2015-10-05</created><authors><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Cooperation Alignment for Distributed Interference Management</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cooperative Gaussian interference channel in which each
receiver must decode its intended message locally, with the help of cooperation
either at the receivers side or at the transmitter side. In the case of
receiver cooperation, the receivers can process and share information through
limited capacity backhaul links. In contrast to various previously considered
distributed antenna architectures, where processing is utterly performed in a
centralized fashion, the model considered in this paper aims to capture the
essence of decentralized processing, allowing for a more general class of
&quot;interactive&quot; interference management strategies. Focusing on the three-user
case, we characterize the fundamental tradeoff between the achievable
communication rates and the corresponding backhaul cooperation rate, in terms
of degrees of freedom (DoF). Surprisingly, we show that the optimum
communication-cooperation tradeoff per user remains the same when we move from
two-user to three-user interference channels. In the absence of cooperation,
this is due to interference alignment, which keeps the fraction of
communication dimensions wasted for interference unchanged. When backhaul
cooperation is available, we develop a new idea that we call cooperation
alignment, which guarantees that the average (per user) backhaul load remains
the same as we increase the number of users. In the case of transmitter
cooperation, the transmitters can form their jointly precoded signals through
an interactive protocol over the backhaul. In this case, we show that the
optimal (per user) tradeoff between the achievable communication rates and the
corresponding backhaul cooperation rate in the three-user case is the same as
for receiver cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01370</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01370</id><created>2015-10-05</created><authors><author><keyname>Latif</keyname><forenames>Mohd Azman Abdul</forenames></author><author><keyname>Ali</keyname><forenames>Noohul Basheer Zain</forenames></author><author><keyname>Hussin</keyname><forenames>Fawnizu Azmadi</forenames></author><author><keyname>Zwolinski</keyname><forenames>Mark</forenames></author></authors><title>Implications of Burn-In Stress on NBTI Degradation</title><categories>cs.OH</categories><doi>10.13140/RG.2.1.5057.0965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Burn-in is accepted as a way to evaluate ageing effects in an accelerated
manner. It has been suggested that burn-in stress may have a significant effect
on the Negative Bias Temperature Instability (NBTI) of subthreshold CMOS
circuits. This paper analyses the effect of burn-in on NBTI in the context of a
Digital to Analogue Converter (DAC) circuit. Analogue circuits require matched
device pairs; NBTI may cause mismatches and hence circuit failure. The NBTI
degradation observed in the simulation analysis indicates that under severe
stress conditions, a significant voltage threshold mismatch in the DAC beyond
the design specification of 2 mV limit can result. Experimental results confirm
the sensitivity of the DAC circuit design to NBTI resulting from burn-in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01374</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01374</id><created>2015-10-05</created><authors><author><keyname>Fadaee</keyname><forenames>Saber Shokat</forenames></author><author><keyname>Farajtabar</keyname><forenames>Mehrdad</forenames></author><author><keyname>Sundaram</keyname><forenames>Ravi</forenames></author><author><keyname>Aslam</keyname><forenames>Javed A.</forenames></author><author><keyname>Passas</keyname><forenames>Nikos</forenames></author></authors><title>On The Network You Keep: Analyzing Persons of Interest using Cliqster</title><categories>cs.SI</categories><comments>The final publication is available at Springer via
  http://dx.doi.org/10.1007/s13278-015-0302-0 A preliminary version of this
  paper appeared in Proceedings of the 2014 IEEE/ACM International Conference
  on Advances in Social Networks Analysis and Mining (ASONAM)</comments><doi>10.1007/s13278-015-0302-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to determine the structural differences between different
categories of networks and to use these differences to predict the network
category. Existing work on this topic has looked at social networks such as
Facebook, Twitter, co-author networks etc. We, instead, focus on a novel data
set that we have assembled from a variety of sources, including law-enforcement
agencies, financial institutions, commercial database providers and other
similar organizations. The data set comprises networks of &quot;persons of interest&quot;
with each network belonging to different categories such as suspected
terrorists, convicted individuals etc. We demonstrate that such &quot;anti-social&quot;
networks are qualitatively different from the usual social networks and that
new techniques are required to identify and learn features of such networks for
the purposes of prediction and classification.
  We propose Cliqster, a new generative Bernoulli process-based model for
unweighted networks. The generating probabilities are the result of a
decomposition which reflects a network's community structure. Using a maximum
likelihood solution for the network inference leads to a least-squares problem.
By solving this problem, we are able to present an efficient algorithm for
transforming the network to a new space which is both concise and
discriminative. This new space preserves the identity of the network as much as
possible. Our algorithm is interpretable and intuitive. Finally, by comparing
our research against the baseline method (SVD) and against a state-of-the-art
Graphlet algorithm, we show the strength of our algorithm in discriminating
between different categories of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01378</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01378</id><created>2015-10-05</created><authors><author><keyname>Laurent</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>Pereyra</keyname><forenames>Gabriel</forenames></author><author><keyname>Brakel</keyname><forenames>Phil&#xe9;mon</forenames></author><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Batch Normalized Recurrent Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) are powerful models for sequential data that
have the potential to learn long-term dependencies. However, they are
computationally expensive to train and difficult to parallelize. Recent work
has shown that normalizing intermediate representations of neural networks can
significantly improve convergence rates in feedforward neural networks . In
particular, batch normalization, which uses mini-batch statistics to
standardize features, was shown to significantly reduce training time. In this
paper, we show that applying batch normalization to the hidden-to-hidden
transitions of our RNNs doesn't help the training procedure. We also show that
when applied to the input-to-hidden transitions, batch normalization can lead
to a faster convergence of the training criterion but doesn't seem to improve
the generalization performance on both our language modelling and speech
recognition tasks. All in all, applying batch normalization to RNNs turns out
to be more challenging than applying it to feedforward networks, but certain
variants of it can still be beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01385</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01385</id><created>2015-10-05</created><authors><author><keyname>Cheung</keyname><forenames>Kent Tsz Kan</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Distributed Energy Spectral Efficiency Optimization for Partial/Full
  Interference Alignment in Multi-User Multi-Relay Multi-Cell MIMO Systems</title><categories>cs.IT math.IT</categories><comments>15 pages, 8 figures, 2 tables, accepted to appear on IEEE
  Transactions on Signal Processing, Oct. 2015</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 64, no. 4, pp.
  882-896, Feb. 2016</journal-ref><doi>10.1109/TSP.2015.2488579</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy spectral efficiency maximization (ESEM) problem of a multi-user,
multi-relay, multi-cell system is considered, where all the network nodes are
equipped with multi-antenna transceivers. To deal with the potentially
excessive interference originating from a plethora of geographically
distributed transmission sources, a pair of transmission protocols based on
interference alignment (IA) are conceived. The first, termed the full-IA,
avoids all intra-cell interference (ICI) and other-cell interference by finding
the perfect interference-nulling receive beamforming matrices (RxBFMs). The
second protocol, termed partial-IA, only attempts to null the ICI. Employing
the RxBFMs computed by either of these protocols mathematically decomposes the
channel into a multiplicity of non-interfering multiple-input--single-output
channels, which we term as spatial multiplexing components (SMCs). The problem
of finding the optimal SMCs as well as their power control variables for the
ESEM problem considered is formally defined and converted into a convex
optimization form with carefully selected variable relaxations and
transformations. Thus, the optimal SMCs and power control variables can be
distributively computed using both the classic dual decomposition and
subgradient methods. Our results indicate that indeed, the ESEM algorithm
performs better than the baseline equal power allocation algorithm in terms of
its ESE. Furthermore, surprisingly the partial-IA outperforms the full-IA in
all cases considered, which is because the partial-IA is less restrictive in
terms of the number of available transmit dimensions at the transmitters. Given
the typical cell sizes considered in this paper, the path-loss sufficiently
attenuates the majority of the interference, and thus the full-IA
over-compensates, when trying to avoid all possible sources of interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01391</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01391</id><created>2015-10-05</created><authors><author><keyname>Horsman</keyname><forenames>Dominic C.</forenames></author></authors><title>Abstraction/Representation Theory for Heterotic Physical Computing</title><categories>cs.LO cs.ET cs.SI</categories><comments>Dominic Horsman published previously as Clare Horsman</comments><journal-ref>Phil. Trans. R. Soc. A 2015 373 20140224 (June 2015)</journal-ref><doi>10.1098/rsta.2014.0224</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a rigorous framework for the interaction of physical computing
devices with abstract computation. Device and program are mediated by the
non-logical 'representation relation'; we give the conditions under which
representation and device theory give rise to commuting diagrams between
logical and physical domains, and the conditions for computation to occur. We
give the interface of this new framework with currently existing formal
methods, showing in particular its close relationship to refinement theory, and
the implications for questions of meaning and reference in theoretical computer
science. The case of hybrid computing is considered in detail, addressing in
particular the example of an internet-mediated 'social machine', and the
abstraction/representation framework used to provide a formal distinction
between heterotic and hybrid computing. This forms the basis for future use of
the framework in formal treatments of nonstandard physical computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01392</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01392</id><created>2015-10-05</created><updated>2016-02-19</updated><authors><author><keyname>Li</keyname><forenames>Yingzhe</forenames></author><author><keyname>Baccelli</keyname><forenames>Francois</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Novlan</keyname><forenames>Thomas D.</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong Charlie</forenames></author></authors><title>Modeling and Analyzing the Coexistence of Wi-Fi and LTE in Unlicensed
  Spectrum</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We leverage stochastic geometry to characterize key performance metrics for
neighboring Wi-Fi and LTE networks in unlicensed spectrum. Our analysis focuses
on a single unlicensed frequency band, where the locations for the Wi-Fi access
points (APs) and LTE eNodeBs (eNBs) are modeled as two independent homogeneous
Poisson point processes. Three LTE coexistence mechanisms are investigated: (1)
LTE with continuous transmission and no protocol modifications; (2) LTE with
discontinuous transmission; and (3) LTE with listen-before-talk (LBT) and
random back-off (BO). For each scenario, we have derived the medium access
probability (MAP), the signal-to-interference-plus-noise ratio (SINR) coverage
probability, the density of successful transmissions (DST), and the rate
coverage probability for both Wi-Fi and LTE. Compared to the baseline scenario
where one Wi-Fi network coexists with an additional Wi-Fi network, our results
show that Wi-Fi performance is severely degraded when LTE transmits
continuously. However, LTE is able to improve the DST and rate coverage
probability of Wi-Fi while maintaining acceptable data rate performance when it
adopts one or more of the following coexistence features: a shorter
transmission duty cycle, lower channel access priority, or more sensitive clear
channel assessment (CCA) thresholds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01397</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01397</id><created>2015-10-05</created><authors><author><keyname>Moll&#xe9;n</keyname><forenames>Christopher</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Waveforms for the Massive MIMO Downlink: Amplifier Efficiency,
  Distortion and Performance</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In massive MIMO, most precoders result in downlink signals that suffer from
high PAR, independently of modulation order and whether single-carrier or OFDM
transmission is used. The high PAR lowers the power efficiency of the base
station amplifiers. To increase power efficiency, low-PAR precoders have been
proposed. In this article, we compare different transmission schemes for
massive MIMO in terms of the power consumed by the amplifiers. It is found that
(i) OFDM and single-carrier transmission have the same performance over a
hardened massive MIMO channel and (ii) when the higher amplifier power
efficiency of low-PAR precoding is taken into account, conventional and low-PAR
precoders lead to approximately the same power consumption. Since downlink
signals with low PAR allow for simpler and cheaper hardware, than signals with
high PAR, therefore, the results suggest that low-PAR precoding with either
single-carrier or OFDM transmission should be used in a massive MIMO base
station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01401</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01401</id><created>2015-10-05</created><authors><author><keyname>Agarwal</keyname><forenames>Sameer</forenames></author><author><keyname>Lee</keyname><forenames>Hon-Leung</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author><author><keyname>Thomas</keyname><forenames>Rekha R.</forenames></author></authors><title>On the Existence of Epipolar Matrices</title><categories>cs.CV math.AG</categories><comments>19 pages, 2 figures; This paper is related to our previous paper
  arXiv:1407.5367. However, the two papers differ enough in their focus and
  results that they merit being archived separately</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the foundational question of the existence of a
fundamental (resp. essential) matrix given $m$ point correspondences in two
views. We present a complete answer for the existence of fundamental matrices
for any value of $m$. Using examples we disprove the widely held beliefs that
fundamental matrices always exist whenever $m \leq 7$. At the same time, we
prove that they exist unconditionally when $m \leq 5$. Under a mild genericity
condition, we show that an essential matrix always exists when $m \leq 4$. We
also characterize the six and seven point configurations in two views for which
all matrices satisfying the epipolar constraint have rank at most one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01413</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01413</id><created>2015-10-05</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Abbasi</keyname><forenames>Ehsan</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>BER Analysis of the box relaxation for BPSK Signal Recovery</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recovering an $n$-dimensional vector of $\{\pm1\}^n$
(BPSK) signals from $m$ noise corrupted measurements
$\mathbf{y}=\mathbf{A}\mathbf{x}_0+\mathbf{z}$. In particular, we consider the
box relaxation method which relaxes the discrete set $\{\pm1\}^n$ to the convex
set $[-1,1]^n$ to obtain a convex optimization algorithm followed by hard
thresholding. When the noise $\mathbf{z}$ and measurement matrix $\mathbf{A}$
have iid standard normal entries, we obtain an exact expression for the
bit-wise probability of error $P_e$ in the limit of $n$ and $m$ growing and
$\frac{m}{n}$ fixed. At high SNR our result shows that the $P_e$ of box
relaxation is within 3dB of the matched filter bound MFB for square systems,
and that it approaches MFB as $m $ grows large compared to $n$. Our results
also indicates that as $m,n\rightarrow\infty$, for any fixed set of size $k$,
the error events of the corresponding $k$ bits in the box relaxation method are
independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01419</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01419</id><created>2015-10-05</created><authors><author><keyname>Razaghpanah</keyname><forenames>Abbas</forenames></author><author><keyname>Vallina-Rodriguez</keyname><forenames>Narseo</forenames></author><author><keyname>Sundaresan</keyname><forenames>Srikanth</forenames></author><author><keyname>Kreibich</keyname><forenames>Christian</forenames></author><author><keyname>Gill</keyname><forenames>Phillipa</forenames></author><author><keyname>Allman</keyname><forenames>Mark</forenames></author><author><keyname>Paxson</keyname><forenames>Vern</forenames></author></authors><title>Haystack: In Situ Mobile Traffic Analysis in User Space</title><categories>cs.NI</categories><comments>13 pages incl. figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite our growing reliance on mobile phones for a wide range of daily
tasks, we remain largely in the dark about the operation and performance of our
devices, including how (or whether) they protect the information we entrust to
them, and with whom they share it. The absence of easy, device-local access to
the traffic of our mobile phones presents a fundamental impediment to improving
this state of affairs. To develop detailed visibility, we devise Haystack, a
system for unobtrusive and comprehensive monitoring of network communications
on mobile phones, entirely from user-space. Haystack correlates disparate
contextual information such as app identifiers and radio state with specific
traffic flows destined to remote services, even if encrypted. Haystack
facilitates user-friendly, large-scale deployment of mobile traffic
measurements and services to illuminate mobile app performance, privacy and
security. We discuss the design of Haystack and demonstrate its feasibility
with an implementation that provides 26-55 Mbps throughput with less than 5%
CPU overhead. Our system and results highlight the potential for client-side
traffic analysis to help understand the mobile ecosystem at scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01421</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01421</id><created>2015-10-05</created><authors><author><keyname>Du</keyname><forenames>Miao</forenames></author><author><keyname>Versteeg</keyname><forenames>Steve</forenames></author><author><keyname>Schneider</keyname><forenames>Jean-Guy</forenames></author><author><keyname>Grundy</keyname><forenames>John</forenames></author><author><keyname>Han</keyname><forenames>Jun</forenames></author></authors><title>From Network Traces to System Responses: Opaquely Emulating Software
  Services</title><categories>cs.SE</categories><comments>Technical Report. Swinburne University of Technology, Faculty of
  Information and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise software systems make complex interactions with other services in
their environment. Developing and testing for production-like conditions is
therefore a challenging task. Prior approaches include emulations of the
dependency services using either explicit modelling or record-and-replay
approaches. Models require deep knowledge of the target services while
record-and-replay is limited in accuracy. We present a new technique that
improves the accuracy of record-and-replay approaches, without requiring prior
knowledge of the services. The approach uses multiple sequence alignment to
derive message prototypes from recorded system interactions and a scheme to
match incoming request messages against message prototypes to generate response
messages. We introduce a modified Needleman-Wunsch algorithm for distance
calculation during message matching, wildcards in message prototypes for high
variability sections, and entropy-based weightings in distance calculations for
increased accuracy. Combined, our new approach has shown greater than 99%
accuracy for four evaluated enterprise system messaging protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01422</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01422</id><created>2015-10-05</created><authors><author><keyname>Matloff</keyname><forenames>Norman</forenames></author></authors><title>Improved Estimation of Class Prior Probabilities through Unlabeled Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Work in the classification literature has shown that in computing a
classification function, one need not know the class membership of all
observations in the training set; the unlabeled observations still provide
information on the marginal distribution of the feature set, and can thus
contribute to increased classification accuracy for future observations. The
present paper will show that this scheme can also be used for the estimation of
class prior probabilities, which would be very useful in applications in which
it is difficult or expensive to determine class membership. Both parametric and
nonparametric estimators are developed. Asymptotic distributions of the
estimators are derived, and it is proven that the use of the unlabeled
observations does reduce asymptotic variance. This methodology is also extended
to the estimation of subclass probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01425</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01425</id><created>2015-10-06</created><updated>2015-10-11</updated><authors><author><keyname>Campbell</keyname><forenames>Julie Ann</forenames></author><author><keyname>Aragon</keyname><forenames>Cecilia</forenames></author><author><keyname>Davis</keyname><forenames>Katie</forenames></author><author><keyname>Evans</keyname><forenames>Sarah</forenames></author><author><keyname>Evans</keyname><forenames>Abigail</forenames></author><author><keyname>Randall</keyname><forenames>David P.</forenames></author></authors><title>Thousands of Positive Reviews: Distributed Mentoring in Online Fan
  Communities</title><categories>cs.HC cs.CY cs.SI</categories><comments>14 pages, to be published in Proceedings of the ACM Conference on
  Computer Supported Cooperative Work 2016</comments><acm-class>H.5.3</acm-class><doi>10.1145/2818048.2819934</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Young people worldwide are participating in ever-increasing numbers in online
fan communities. Far from mere shallow repositories of pop culture, these sites
are accumulating significant evidence that sophisticated informal learning is
taking place online in novel and unexpected ways. In order to understand and
analyze in more detail how learning might be occurring, we conducted an
in-depth nine-month ethnographic investigation of online fanfiction
communities, including participant observation and fanfiction author
interviews. Our observations led to the development of a theory we term
distributed mentoring, which we present in detail in this paper. Distributed
mentoring exemplifies one instance of how networked technology affords new
extensions of behaviors that were previously bounded by time and space.
Distributed mentoring holds potential for application beyond the spontaneous
mentoring observed in this investigation and may help students receive diverse,
thoughtful feedback in formal learning environments as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01429</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01429</id><created>2015-10-06</created><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author><author><keyname>Bespalov</keyname><forenames>Evgeny</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>Distance-2 MDS codes and latin colorings in the Doob graphs</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>18pp</comments><msc-class>05B30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum independent sets in the Doob graphs D(m,n) are analogs of the
distance-2 MDS codes in Hamming graphs and of the latin hypercubes. We prove
the characterization of these sets stating that every such set is semilinear or
reducible. As related objects, we study vertex sets with maximum cut (edge
boundary) in D(m,n) and prove some facts on their structure. We show that the
considered two classes (the maximum independent sets and the maximum-cut sets)
can be defined as classes of completely regular sets with specified 2-by-2
quotient matrices. It is notable that for a set from the considered classes,
the eigenvalues of the quotient matrix are the maximum and the minimum
eigenvalues of the graph. For D(m,0), we show the existence of a third,
intermediate, class of completely regular sets with the same property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01431</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01431</id><created>2015-10-06</created><updated>2015-12-13</updated><authors><author><keyname>Mathews</keyname><forenames>Alexander</forenames></author><author><keyname>Xie</keyname><forenames>Lexing</forenames></author><author><keyname>He</keyname><forenames>Xuming</forenames></author></authors><title>SentiCap: Generating Image Descriptions with Sentiments</title><categories>cs.CV cs.CL</categories><acm-class>I.2.10; I.2.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent progress on image recognition and language modeling is making
automatic description of image content a reality. However, stylized,
non-factual aspects of the written description are missing from the current
systems. One such style is descriptions with emotions, which is commonplace in
everyday communication, and influences decision-making and interpersonal
relationships. We design a system to describe an image with emotions, and
present a model that automatically generates captions with positive or negative
sentiments. We propose a novel switching recurrent neural network with
word-level regularization, which is able to produce emotional image captions
using only 2000+ training sentences containing sentiments. We evaluate the
captions with different automatic and crowd-sourcing metrics. Our model
compares favourably in common quality metrics for image captioning. In 84.6% of
cases the generated positive captions were judged as being at least as
descriptive as the factual captions. Of these positive captions 88% were
confirmed by the crowd-sourced workers as having the appropriate sentiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01434</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01434</id><created>2015-10-06</created><updated>2016-01-31</updated><authors><author><keyname>Leng</keyname><forenames>Shiyang</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Objective Resource Allocation in Full-Duplex SWIPT Systems</title><categories>cs.IT math.IT</categories><comments>accepted for presentation at the IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the resource allocation algorithm design for
full-duplex simultaneous wireless information and power transfer (FD-SWIPT)
systems. The considered system comprises a FD radio base station, multiple
single-antenna half-duplex (HD) users, and multiple energy harvesters equipped
with multiple antennas. We propose a multi-objective optimization framework to
study the trade-off between uplink transmit power minimization, downlink
transmit power minimization, and total harvested energy maximization. The
considered optimization framework takes into account heterogeneous quality of
service requirements for uplink and downlink communication and wireless power
transfer. The non-convex multi-objective optimization problem is transformed
into an equivalent rank-constrained semidefinite program (SDP) and solved
optimally by SDP relaxation. The solution of the proposed framework results in
a set of Pareto optimal resource allocation policies. Numerical results unveil
an interesting trade-off between the considered conflicting system design
objectives and reveal the improved power efficiency facilitated by FD in SWIPT
systems compared to traditional HD systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01439</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01439</id><created>2015-10-06</created><updated>2015-10-17</updated><authors><author><keyname>Nagargoje</keyname><forenames>Vishvajeet</forenames></author></authors><title>Codes That Achieve Capacity on Symmetric Channels</title><categories>cs.IT math.IT</categories><comments>Survey done under the guidance of Prof. Prahladh Harsha as part of
  the Visiting Students' Research Programme 2015 at the School of Technology
  and Computer Science, Tata Institute of Fundamental Research, Mumbai.
  Keywords : capacity achieving codes, polar codes, reed muller codes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission of information reliably and efficiently across channels is one
of the fundamental goals of coding and information theory. In this respect,
efficiently decodable deterministic coding schemes which achieve capacity
provably have been elusive until as recent as 2008, even though schemes which
come close to it in practice existed. This survey tries to give the interested
reader an overview of the area.
  Erdal Arikan came up with his landmark polar coding shemes which achieve
capacity on symmetric channels subject to the constraint that the input
codewords are equiprobable. His idea is to convert any B-DMC into efficiently
encodable-decodable channels which have rates 0 and 1, while conserving
capacity in this transformation. An exponentially decreasing probability of
error which independent of code rate is achieved for all rates lesser than the
symmetric capacity. These codes perform well in practice since encoding and
decoding complexity is O(N log N). Guruswami et al. improved the above results
by showing that error probability can be made to decrease doubly exponentially
in the block length.
  We also study recent results by Urbanke et al. which show that 2-transitive
codes also achieve capacity on erasure channels under MAP decoding. Urbanke and
his group use complexity theoretic results in boolean function analysis to
prove that EXIT functions, which capture the error probability, have a sharp
threshold at 1-R, thus proving that capacity is achieved. One of the oldest and
most widely used codes - Reed Muller codes are 2-transitive. Polar codes are
2-transitive too and we thus have a different proof of the fact that they
achieve capacity, though the rate of polarization would be better as found out
by Guruswami.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01440</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01440</id><created>2015-10-06</created><authors><author><keyname>Wu</keyname><forenames>Ruobing</forenames></author><author><keyname>Wang</keyname><forenames>Baoyuan</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>Harvesting Discriminative Meta Objects with Deep CNN Features for Scene
  Classification</title><categories>cs.CV</categories><comments>To Appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on scene classification still makes use of generic CNN features
in a rudimentary manner. In this ICCV 2015 paper, we present a novel pipeline
built upon deep CNN features to harvest discriminative visual objects and parts
for scene classification. We first use a region proposal technique to generate
a set of high-quality patches potentially containing objects, and apply a
pre-trained CNN to extract generic deep features from these patches. Then we
perform both unsupervised and weakly supervised learning to screen these
patches and discover discriminative ones representing category-specific objects
and parts. We further apply discriminative clustering enhanced with local CNN
fine-tuning to aggregate similar objects and parts into groups, called meta
objects. A scene image representation is constructed by pooling the feature
response maps of all the learned meta objects at multiple spatial scales. We
have confirmed that the scene image representation obtained using this new
pipeline is capable of delivering state-of-the-art performance on two popular
scene benchmark datasets, MIT Indoor 67~\cite{MITIndoor67} and
Sun397~\cite{Sun397}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01442</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01442</id><created>2015-10-06</created><authors><author><keyname>Yang</keyname><forenames>Huan</forenames></author><author><keyname>Wang</keyname><forenames>Baoyuan</forenames></author><author><keyname>Lin</keyname><forenames>Stephen</forenames></author><author><keyname>Wipf</keyname><forenames>David</forenames></author><author><keyname>Guo</keyname><forenames>Minyi</forenames></author><author><keyname>Guo</keyname><forenames>Baining</forenames></author></authors><title>Unsupervised Extraction of Video Highlights Via Robust Recurrent
  Auto-encoders</title><categories>cs.CV</categories><comments>To Appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing popularity of short-form video sharing platforms such as
\em{Instagram} and \em{Vine}, there has been an increasing need for techniques
that automatically extract highlights from video. Whereas prior works have
approached this problem with heuristic rules or supervised learning, we present
an unsupervised learning approach that takes advantage of the abundance of
user-edited videos on social media websites such as YouTube. Based on the idea
that the most significant sub-events within a video class are commonly present
among edited videos while less interesting ones appear less frequently, we
identify the significant sub-events via a robust recurrent auto-encoder trained
on a collection of user-edited videos queried for each particular class of
interest. The auto-encoder is trained using a proposed shrinking exponential
loss function that makes it robust to noise in the web-crawled training data,
and is configured with bidirectional long short term memory
(LSTM)~\cite{LSTM:97} cells to better model the temporal structure of highlight
segments. Different from supervised techniques, our method can infer highlights
using only a set of downloaded edited videos, without also needing their
pre-edited counterparts which are rarely available online. Extensive
experiments indicate the promise of our proposed solution in this challenging
unsupervised settin
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01443</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01443</id><created>2015-10-06</created><authors><author><keyname>Fan</keyname><forenames>Bo</forenames></author><author><keyname>Lee</keyname><forenames>Siu Wa</forenames></author><author><keyname>Tian</keyname><forenames>Xiaohai</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Dong</keyname><forenames>Minghui</forenames></author></authors><title>A Waveform Representation Framework for High-quality Statistical
  Parametric Speech Synthesis</title><categories>cs.SD cs.LG</categories><comments>accepted and will appear in APSIPA2015; keywords: speech synthesis,
  LSTM-RNN, vocoder, phase, waveform, modeling</comments><msc-class>68T10</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  State-of-the-art statistical parametric speech synthesis (SPSS) generally
uses a vocoder to represent speech signals and parameterize them into features
for subsequent modeling. Magnitude spectrum has been a dominant feature over
the years. Although perceptual studies have shown that phase spectrum is
essential to the quality of synthesized speech, it is often ignored by using a
minimum phase filter during synthesis and the speech quality suffers. To bypass
this bottleneck in vocoded speech, this paper proposes a phase-embedded
waveform representation framework and establishes a magnitude-phase joint
modeling platform for high-quality SPSS. Our experiments on waveform
reconstruction show that the performance is better than that of the widely-used
STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms
a leading-edge, vocoded, deep bidirectional long short-term memory recurrent
neural network (DBLSTM-RNN)-based baseline system in various objective
evaluation metrics conducted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01444</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01444</id><created>2015-10-06</created><updated>2015-12-10</updated><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author></authors><title>Stochastic subGradient Methods with Linear Convergence for Polyhedral
  Convex Optimization</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that simple {Stochastic} subGradient Decent methods
with multiple Restarting, named {\bf RSGD}, can achieve a \textit{linear
convergence rate} for a class of non-smooth and non-strongly convex
optimization problems where the epigraph of the objective function is a
polyhedron, to which we refer as {\bf polyhedral convex optimization}. Its
applications in machine learning include $\ell_1$ constrained or regularized
piecewise linear loss minimization and submodular function minimization. To the
best of our knowledge, this is the first result on the linear convergence rate
of stochastic subgradient methods for non-smooth and non-strongly convex
optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01446</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01446</id><created>2015-10-06</created><authors><author><keyname>Liu</keyname><forenames>Wenhao</forenames></author><author><keyname>Strangio</keyname><forenames>Maurizio Adriano</forenames></author><author><keyname>Wang</keyname><forenames>Shengbao</forenames></author></authors><title>Efficient Certificateless Signcryption Tag-KEMs for Resource-constrained
  Devices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient certificateless one-pass session key establishment protocols can be
constructed from key encapsulation mechanisms (KEMs) by making use of tags and
signcryption schemes. The resulting primitives are referred to as
Certificateless Signcryption Tag Key Encapsulation Mechanisms (CLSC-TKEMs). In
this paper we propose two novel CLSC-TKEM protocols, the first, named
LSW-CLSC-TKEM, makes use of the signature scheme of Liu et al., the second,
named DKTUTS-CLSC-TKEM, is based on the direct key transport using a timestamp
(DKTUTS) protocol first described by Zheng. In order to achieve greater
efficiency both schemes are instantiated on elliptic curves without making use
of pairings and are therefore good candidates for deployment on resource
constrained devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01455</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01455</id><created>2015-10-06</created><updated>2016-02-23</updated><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Lang</keyname><forenames>Kevin</forenames></author><author><keyname>Rhodes</keyname><forenames>Lee</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>A Framework for Estimating Stream Expression Cardinalities</title><categories>cs.DS</categories><acm-class>G.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $m$ distributed data streams $A_1, \dots, A_m$, we consider the problem
of estimating the number of unique identifiers in streams defined by set
expressions over $A_1, \dots, A_m$. We identify a broad class of algorithms for
solving this problem, and show that the estimators output by any algorithm in
this class are perfectly unbiased and satisfy strong variance bounds. Our
analysis unifies and generalizes a variety of earlier results in the
literature. To demonstrate its generality, we describe several novel sampling
algorithms in our class, and show that they achieve a novel tradeoff between
accuracy, space usage, update speed, and applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01463</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01463</id><created>2015-10-06</created><authors><author><keyname>Lei</keyname><forenames>Yunwen</forenames></author><author><keyname>Ding</keyname><forenames>Lixin</forenames></author><author><keyname>Bi</keyname><forenames>Yingzhou</forenames></author></authors><title>Local Rademacher Complexity Bounds based on Covering Numbers</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a general result on controlling local Rademacher
complexities, which captures in an elegant form to relate the complexities with
constraint on the expected norm to the corresponding ones with constraint on
the empirical norm. This result is convenient to apply in real applications and
could yield refined local Rademacher complexity bounds for function classes
satisfying general entropy conditions. We demonstrate the power of our
complexity bounds by applying them to derive effective generalization error
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01485</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01485</id><created>2015-10-06</created><authors><author><keyname>Kaufmann</keyname><forenames>Dinu</forenames></author><author><keyname>Parbhoo</keyname><forenames>Sonali</forenames></author><author><keyname>Wieczorek</keyname><forenames>Aleksander</forenames></author><author><keyname>Keller</keyname><forenames>Sebastian</forenames></author><author><keyname>Adametz</keyname><forenames>David</forenames></author><author><keyname>Roth</keyname><forenames>Volker</forenames></author></authors><title>Bayesian Markov Blanket Estimation</title><categories>stat.ML cs.LG</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a Bayesian view for estimating a sub-network in a Markov
random field. The sub-network corresponds to the Markov blanket of a set of
query variables, where the set of potential neighbours here is big. We
factorize the posterior such that the Markov blanket is conditionally
independent of the network of the potential neighbours. By exploiting this
blockwise decoupling, we derive analytic expressions for posterior
conditionals. Subsequently, we develop an inference scheme which makes use of
the factorization. As a result, estimation of a sub-network is possible without
inferring an entire network. Since the resulting Gibbs sampler scales linearly
with the number of variables, it can handle relatively large neighbourhoods.
The proposed scheme results in faster convergence and superior mixing of the
Markov chain than existing Bayesian network estimation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01490</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01490</id><created>2015-10-06</created><authors><author><keyname>Thai</keyname><forenames>Duy Hoang</forenames></author><author><keyname>Gottschlich</keyname><forenames>Carsten</forenames></author></authors><title>Directional Global Three-part Image Decomposition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of image decomposition and we introduce a new model
coined directional global three-part decomposition (DG3PD) for solving it. As
key ingredients of the DG3PD model, we introduce a discrete multi-directional
total variation norm and a discrete multi-directional G-norm. Using these novel
norms, the proposed discrete DG3PD model can decompose an image into two parts
or into three parts. Existing models for image decomposition by Vese and Osher,
by Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich are
included as special cases in the new model. Decomposition of an image by DG3PD
results in a cartoon image, a texture image and a residual image. Advantages of
the DG3PD model over existing ones lie in the properties enforced on the
cartoon and texture images. The geometric objects in the cartoon image have a
very smooth surface and sharp edges. The texture image yields oscillating
patterns on a defined scale which is both smooth and sparse. Moreover, the
DG3PD method achieves the goal of perfect reconstruction by summation of all
components better than the other considered methods. Relevant applications of
DG3PD are a novel way of image compression as well as feature extraction for
applications such as latent fingerprint processing and optical character
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01495</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01495</id><created>2015-10-06</created><authors><author><keyname>Martius</keyname><forenames>Georg</forenames></author><author><keyname>Olbrich</keyname><forenames>Eckehard</forenames></author></authors><title>Quantifying Emergent Behavior of Autonomous Robots</title><categories>cs.IT cs.LG cs.RO math.DS math.IT</categories><comments>24 pages, 10 figures, submitted Entropy Journal</comments><msc-class>68P30, 68T05, 68T40, 70E60, 93C10, 94A17</msc-class><acm-class>H.1.1; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantifying behaviors of robots which were generated autonomously from
task-independent objective functions is an important prerequisite for objective
comparisons of algorithms and movements of animals. The temporal sequence of
such a behavior can be considered as a time series and hence complexity
measures developed for time series are natural candidates for its
quantification. The predictive information and the excess entropy are such
complexity measures. They measure the amount of information the past contains
about the future and thus quantify the nonrandom structure in the temporal
sequence. However, when using these measures for systems with continuous states
one has to deal with the fact that their values will depend on the resolution
with which the systems states are observed. For deterministic systems both
measures will diverge with increasing resolution. We therefore propose a new
decomposition of the excess entropy in resolution dependent and resolution
independent parts and discuss how they depend on the dimensionality of the
dynamics, correlations and the noise level. For the practical estimation we
propose to use estimates based on the correlation integral instead of the
direct estimation of the mutual information using the algorithm by Kraskov et
al. (2004) which is based on next neighbor statistics because the latter allows
less control of the scale dependencies. Using our algorithm we are able to show
how autonomous learning generates behavior of increasing complexity with
increasing learning duration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01509</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01509</id><created>2015-10-06</created><authors><author><keyname>Cameron</keyname><forenames>Peter J.</forenames></author><author><keyname>Kusuma</keyname><forenames>Josephine</forenames></author></authors><title>$Z_4$-codes and their Gray map images as orthogonal arrays</title><categories>math.CO cs.IT math.IT</categories><msc-class>05 B 15, 94 B 05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classic result of Delsarte connects the strength (as orthogonal array) of a
linear code with the minimum weight of its dual: the former is one less than
the latter.
  Since the paper of Hammons \emph{et al.}, there is a lot of interest in codes
over rings, especially in codes over $Z_4$ and their (usually non-linear)
binary Gray map images.
  We show that Delsarte's observation extends to codes over arbitrary finite
rings. However, the connection between the strength of a $Z_4$-code and that of
its Gray map image is more problematic. We conjecture that the strength of the
Gray map image of $C$ is one less than the minimum Lee weight of $C^\perp$ and
give some evidence for this conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01518</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01518</id><created>2015-10-06</created><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Hall</keyname><forenames>Georgina</forenames></author></authors><title>DC Decomposition of Nonconvex Polynomials with Algebraic Techniques</title><categories>math.OC cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decomposing a multivariate polynomial as the
difference of two convex polynomials. We introduce algebraic techniques which
reduce this task to linear, second order cone, and semidefinite programming.
This allows us to optimize over subsets of valid difference of convex
decompositions (dcds) and find ones that speed up the convex-concave procedure
(CCP). We prove, however, that optimizing over the entire set of dcds is
NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01520</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01520</id><created>2015-10-06</created><authors><author><keyname>Chan</keyname><forenames>T-H. Hubert</forenames></author><author><keyname>Tang</keyname><forenames>Zhihao Gavin</forenames></author><author><keyname>Zhang</keyname><forenames>Chenzi</forenames></author></authors><title>Spectral Properties of Laplacian and Stochastic Diffusion Process for
  Edge Expansion in Hypergraphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been recent work [Louis STOC 2015] to analyze the spectral
properties of hypergraphs with respect to edge expansion. In particular, a
diffusion process is defined on a hypergraph such that within each hyperedge,
measure flows from nodes having maximum weighted measure to those having
minimum. The diffusion process determines a Laplacian, whose spectral
properties are related to the edge expansion properties of the hypergraph.
  It is suggested that in the above diffusion process, within each hyperedge,
measure should flow uniformly in the complete bipartite graph from nodes with
maximum weighted measure to those with minimum. However, we discover that this
method has some technical issues. First, the diffusion process would not be
well-defined. Second, the resulting Laplacian would not have the claimed
spectral properties.
  In this paper, we show that the measure flow between the above two sets of
nodes must be coordinated carefully over different hyperedges in order for the
diffusion process to be well-defined, from which a Laplacian can be uniquely
determined. Since the Laplacian is non-linear, we have to exploit other
properties of the diffusion process to recover a spectral property concerning
the &quot;second eigenvalue&quot; of the resulting Laplacian. Moreover, we show that
higher order spectral properties cannot hold in general using the current
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01537</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01537</id><created>2015-10-06</created><authors><author><keyname>Rivi&#xe8;re</keyname><forenames>Lionel</forenames></author><author><keyname>Najm</keyname><forenames>Zakaria</forenames></author><author><keyname>Rauzy</keyname><forenames>Pablo</forenames></author><author><keyname>Danger</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Bringer</keyname><forenames>Julien</forenames></author><author><keyname>Sauvage</keyname><forenames>Laurent</forenames></author></authors><title>High Precision Fault Injections on the Instruction Cache of ARMv7-M
  Architectures</title><categories>cs.CR</categories><comments>HOST 2015: IEEE International Symposium on Hardware-Oriented Security
  and Trust, May 2015, Washington, United States</comments><proxy>ccsd</proxy><doi>10.1109/HST.2015.7140238</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware and software of secured embedded systems are prone to physical
attacks. In particular, fault injection attacks revealed vulnerabilities on the
data and the control flow allowing an attacker to break cryptographic or
secured algorithms implementations. While many research studies concentrated on
successful attacks on the data flow, only a few targets the instruction flow.
In this paper, we focus on electromagnetic fault injection (EMFI) on the
control flow, especially on the instruction cache. We target the very
widespread (smartphones, tablets, settop-boxes, health-industry monitors and
sensors, etc.) ARMv7-M architecture. We describe a practical EMFI platform and
present a methodology providing high control level and high reproducibility
over fault injections. Indeed, we observe that a precise fault model occurs in
up to 96% of the cases. We then characterize and exhibit this practical fault
model on the cache that is not yet considered in the literature. We
comprehensively describe its effects and show how it can be used to reproduce
well known fault attacks. Finally, we describe how it can benefits attackers to
mount new powerful attacks or simplify existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01541</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01541</id><created>2015-10-06</created><updated>2015-10-30</updated><authors><author><keyname>Turner</keyname><forenames>Jacob</forenames></author></authors><title>Tensors Masquerading as Matchgates: Relaxing Planarity Restrictions on
  Pfaffian Circuits</title><categories>math.AG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Holographic algorithms, alternatively known as Pfaffian circuits, have
received a great deal of attention for giving polynomial-time algorithms of
$\#\mathsf{P}$-hard problems. Much work has been done to determine the extent
of what this machinery can do and the expressiveness of these circuits. One
aspect of interest is the fact that these circuits must be planar. Work has
been done to try and relax the planarity conditions and extend these algorithms
further. We show that an approach based on orbit closures does not work, but
give a different technique for allowing the SWAP gate to be used in a Pfaffian
circuit given a suitable basis and restricted type of graph. This is done by
exploiting the fact that the set of Pfaffian (co)gates always lies in a
hyperplane. We then give a variety of bases that can be chosen such that the
SWAP gate acts like a Pfaffian cogate and discuss how many SWAP gates can be
implemented in a Pfaffian circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01544</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01544</id><created>2015-10-06</created><authors><author><keyname>Gavves</keyname><forenames>Efstratios</forenames></author><author><keyname>Mensink</keyname><forenames>Thomas</forenames></author><author><keyname>Tommasi</keyname><forenames>Tatiana</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets
  for Future Tasks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we reuse existing knowledge, in the form of available datasets, when
solving a new and apparently unrelated target task from a set of unlabeled
data? In this work we make a first contribution to answer this question in the
context of image classification. We frame this quest as an active learning
problem and use zero-shot classifiers to guide the learning process by linking
the new task to the existing classifiers. By revisiting the dual formulation of
adaptive SVM, we reveal two basic conditions to choose greedily only the most
relevant samples to be annotated. On this basis we propose an effective active
learning algorithm which learns the best possible target classification model
with minimum human labeling effort. Extensive experiments on two challenging
datasets show the value of our approach compared to the state-of-the-art active
learning methodologies, as well as its potential to reuse past datasets with
minimal effort for future tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01545</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01545</id><created>2015-10-06</created><authors><author><keyname>Heo</keyname><forenames>Eunhye</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Park</keyname><forenames>Hyuncheol</forenames></author></authors><title>Optimal Fronthaul Compression for Synchronization in the Uplink of Cloud
  Radio Access Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem in the design of cloud radio access networks (CRANs) is that of
devising effective baseband compression strategies for transmission on the
fronthaul links connecting a remote radio head (RRH) to the managing central
unit (CU). Most theoretical works on the subject implicitly assume that the
RRHs, and hence the CU, are able to perfectly recover time synchronization from
the baseband signals received in the uplink, and focus on the compression of
the data fields. This paper instead dose not assume a priori synchronization of
RRHs and CU, and considers the problem of fronthaul compression design at the
RRHs with the aim of enhancing the performance of time and phase
synchronization at the CU. The problem is tackled by analyzing the impact of
the synchronization error on the performance of the link and by adopting
information and estimationtheoretic performance metrics such as the
rate-distortion function and the Cramer-Rao bound (CRB). The proposed algorithm
is based on the Charnes-Cooper transformation and on the Difference of Convex
(DC) approach, and is shown via numerical results to outperform conventional
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01553</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01553</id><created>2015-10-06</created><authors><author><keyname>Xu</keyname><forenames>Dan</forenames></author><author><keyname>Ricci</keyname><forenames>Elisa</forenames></author><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Song</keyname><forenames>Jingkuan</forenames></author><author><keyname>Sebe</keyname><forenames>Nicu</forenames></author></authors><title>Learning Deep Representations of Appearance and Motion for Anomalous
  Event Detection</title><categories>cs.CV</categories><comments>Oral paper in BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel unsupervised deep learning framework for anomalous event
detection in complex video scenes. While most existing works merely use
hand-crafted appearance and motion features, we propose Appearance and Motion
DeepNet (AMDN) which utilizes deep neural networks to automatically learn
feature representations. To exploit the complementary information of both
appearance and motion patterns, we introduce a novel double fusion framework,
combining both the benefits of traditional early fusion and late fusion
strategies. Specifically, stacked denoising autoencoders are proposed to
separately learn both appearance and motion features as well as a joint
representation (early fusion). Based on the learned representations, multiple
one-class SVM models are used to predict the anomaly scores of each input,
which are then integrated with a late fusion strategy for final anomaly
detection. We evaluate the proposed method on two publicly available video
surveillance datasets, showing competitive performance with respect to state of
the art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01554</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01554</id><created>2015-10-06</created><authors><author><keyname>Bajones</keyname><forenames>Markus</forenames></author><author><keyname>Wolf</keyname><forenames>Daniel</forenames></author><author><keyname>Prankl</keyname><forenames>Johann</forenames></author><author><keyname>Vincze</keyname><forenames>Markus</forenames></author></authors><title>Where to look first? Behaviour control for fetch-and-carry missions of
  service robots</title><categories>cs.RO</categories><comments>Part of the Austrian Robotics Workshop 2014 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the behaviour control of a service robot for intelligent
object search in a domestic environment. A major challenge in service robotics
is to enable fetch-and-carry missions that are satisfying for the user in terms
of efficiency and human-oriented perception. The proposed behaviour controller
provides an informed intelligent search based on a semantic segmentation
framework for indoor scenes and integrates it with object recognition and
grasping. Instead of manually annotating search positions in the environment,
the framework automatically suggests likely locations to search for an object
based on contextual information, e.g. next to tables and shelves. In a
preliminary set of experiments we demonstrate that this behaviour control is as
efficient as using manually annotated locations. Moreover, we argue that our
approach will reduce the intensity of labour associated with programming
fetch-and-carry tasks for service robots and that it will be perceived as more
human-oriented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01557</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01557</id><created>2015-10-06</created><authors><author><keyname>Agrawal</keyname><forenames>Akanksha</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Simultaneous Feedback Vertex Set: A Parameterized Perspective</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a family of graphs $\mathcal{F}$, a graph $G$, and a positive integer
$k$, the $\mathcal{F}$-Deletion problem asks whether we can delete at most $k$
vertices from $G$ to obtain a graph in $\mathcal{F}$. $\mathcal{F}$-Deletion
generalizes many classical graph problems such as Vertex Cover, Feedback Vertex
Set, and Odd Cycle Transversal. A graph $G = (V, \cup_{i=1}^{\alpha} E_{i})$,
where the edge set of $G$ is partitioned into $\alpha$ color classes, is called
an $\alpha$-edge-colored graph. A natural extension of the
$\mathcal{F}$-Deletion problem to edge-colored graphs is the
$\alpha$-Simultaneous $\mathcal{F}$-Deletion problem. In the latter problem, we
are given an $\alpha$-edge-colored graph $G$ and the goal is to find a set $S$
of at most $k$ vertices such that each graph $G_i \setminus S$, where $G_i =
(V, E_i)$ and $1 \leq i \leq \alpha$, is in $\mathcal{F}$. In this work, we
study $\alpha$-Simultaneous $\mathcal{F}$-Deletion for $\mathcal{F}$ being the
family of forests. In other words, we focus on the $\alpha$-Simultaneous
Feedback Vertex Set ($\alpha$-SimFVS) problem. Algorithmically, we show that,
like its classical counterpart, $\alpha$-SimFVS parameterized by $k$ is
fixed-parameter tractable (FPT) and admits a polynomial kernel, for any fixed
constant $\alpha$. In particular, we give an algorithm running in $2^{O(\alpha
k)}n^{O(1)}$ time and a kernel with $O(\alpha k^{3(\alpha + 1)})$ vertices. The
running time of our algorithm implies that $\alpha$-SimFVS is FPT even when
$\alpha \in o(\log n)$. We complement this positive result by showing that for
$\alpha \in O(\log n)$, where $n$ is the number of vertices in the input graph,
$\alpha$-SimFVS becomes W[1]-hard. Our positive results answer one of the open
problems posed by Cai and Ye (MFCS 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01560</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01560</id><created>2015-10-06</created><authors><author><keyname>Avdis</keyname><forenames>Alexandros</forenames></author><author><keyname>Jacobs</keyname><forenames>Christian T.</forenames></author><author><keyname>Hill</keyname><forenames>Jon</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D.</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author></authors><title>Shoreline and Bathymetry Approximation in Mesh Generation for Tidal
  Renewable Simulations</title><categories>cs.CG physics.ao-ph physics.comp-ph physics.flu-dyn</categories><comments>Pre-print of conference publication accepted in the Proceedings of
  11th European Wave &amp; Tidal Energy Conference (EWTEC 2015,
  http://www.ewtec.org/ewtec2015/ ). This paper was presented at the EWTEC 2015
  conference on Tuesday 8 September 2015 in Nantes, France. Number of pages: 7.
  Number of figures: 6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the fractal nature of the domain geometry in geophysical flow
simulations, a completely accurate description of the domain in terms of a
computational mesh is frequently deemed infeasible. Shoreline and bathymetry
simplification methods are used to remove small scale details in the geometry,
particularly in areas away from the region of interest. To that end, a novel
method for shoreline and bathymetry simplification is presented. Existing
shoreline simplification methods typically remove points if the resultant
geometry satisfies particular geometric criteria. Bathymetry is usually
simplified using traditional filtering techniques, that remove unwanted Fourier
modes. Principal Component Analysis (PCA) has been used in other fields to
isolate small-scale structures from larger scale coherent features in a robust
way, underpinned by a rigorous but simple mathematical framework. Here we
present a method based on principal component analysis aimed towards
simplification of shorelines and bathymetry. We present the algorithm in detail
and show simplified shorelines and bathymetry in the wider region around the
North Sea. Finally, the methods are used in the context of unstructured mesh
generation aimed at tidal resource assessment simulations in the coastal
regions around the UK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01562</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01562</id><created>2015-10-06</created><authors><author><keyname>Piwowarski</keyname><forenames>Benjamin</forenames></author><author><keyname>Lamprier</keyname><forenames>Sylvain</forenames></author><author><keyname>Despres</keyname><forenames>Nicolas</forenames></author></authors><title>Parameterized Neural Network Language Models for Information Retrieval</title><categories>cs.IR cs.CL</categories><acm-class>H.3.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Retrieval (IR) models need to deal with two difficult issues,
vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to
the difficulty of retrieving relevant documents that do not contain exact query
terms but semantically related terms. Term dependencies refers to the need of
considering the relationship between the words of the query when estimating the
relevance of a document. A multitude of solutions has been proposed to solve
each of these two problems, but no principled model solve both. In parallel, in
the last few years, language models based on neural networks have been used to
cope with complex natural language processing tasks like emotion and paraphrase
detection. Although they present good abilities to cope with both term
dependencies and vocabulary mismatch problems, thanks to the distributed
representation of words they are based upon, such models could not be used
readily in IR, where the estimation of one language model per document (or
query) is required. This is both computationally unfeasible and prone to
over-fitting. Based on a recent work that proposed to learn a generic language
model that can be modified through a set of document-specific parameters, we
explore use of new neural network models that are adapted to ad-hoc IR tasks.
Within the language model IR framework, we propose and study the use of a
generic language model as well as a document-specific language model. Both can
be used as a smoothing component, but the latter is more adapted to the
document at hand and has the potential of being used as a full document
language model. We experiment with such models and analyze their results on
TREC-1 to 8 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01570</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01570</id><created>2015-10-06</created><authors><author><keyname>Alfter</keyname><forenames>David</forenames></author></authors><title>Analyzer and generator for Pali</title><categories>cs.CL</categories><comments>Bachelor Thesis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work describes a system that performs morphological analysis and
generation of Pali words. The system works with regular inflectional paradigms
and a lexical database. The generator is used to build a collection of
inflected and derived words, which in turn is used by the analyzer. Generating
and storing morphological forms along with the corresponding morphological
information allows for efficient and simple look up by the analyzer. Indeed, by
looking up a word and extracting the attached morphological information, the
analyzer does not have to compute this information. As we must, however, assume
the lexical database to be incomplete, the system can also work without the
dictionary component, using a rule-based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01574</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01574</id><created>2015-10-06</created><authors><author><keyname>Boasson</keyname><forenames>Luc</forenames></author><author><keyname>Bonizzoni</keyname><forenames>Paola</forenames></author><author><keyname>De Felice</keyname><forenames>Clelia</forenames></author><author><keyname>Fagnot</keyname><forenames>Isabelle</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Zaccagnino</keyname><forenames>Rocco</forenames></author><author><keyname>Zizza</keyname><forenames>Rosalba</forenames></author></authors><title>Splicing Systems from Past to Future: Old and New Challenges</title><categories>cs.FL cs.DM</categories><comments>Appeared in: Discrete Mathematics and Computer Science. Papers in
  Memoriam Alexandru Mateescu (1952-2005). The Publishing House of the Romanian
  Academy, 2014. arXiv admin note: text overlap with arXiv:1112.4897 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A splicing system is a formal model of a recombinant behaviour of sets of
double stranded DNA molecules when acted on by restriction enzymes and ligase.
In this survey we will concentrate on a specific behaviour of a type of
splicing systems, introduced by P\u{a}un and subsequently developed by many
researchers in both linear and circular case of splicing definition. In
particular, we will present recent results on this topic and how they stimulate
new challenging investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01576</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01576</id><created>2015-10-06</created><authors><author><keyname>Castro</keyname><forenames>Daniel</forenames></author><author><keyname>Hickson</keyname><forenames>Steven</forenames></author><author><keyname>Bettadapura</keyname><forenames>Vinay</forenames></author><author><keyname>Thomaz</keyname><forenames>Edison</forenames></author><author><keyname>Abowd</keyname><forenames>Gregory</forenames></author><author><keyname>Christensen</keyname><forenames>Henrik</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Predicting Daily Activities From Egocentric Images Using Deep Learning</title><categories>cs.CV</categories><comments>8 pages</comments><acm-class>I.5; J.4; J.3</acm-class><journal-ref>ISWC '15 Proceedings of the 2015 ACM International Symposium on
  Wearable Computers - Pages 75-82</journal-ref><doi>10.1145/2802083.2808398</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to analyze images taken from a passive egocentric
wearable camera along with the contextual information, such as time and day of
week, to learn and predict everyday activities of an individual. We collected a
dataset of 40,103 egocentric images over a 6 month period with 19 activity
classes and demonstrate the benefit of state-of-the-art deep learning
techniques for learning and predicting daily activities. Classification is
conducted using a Convolutional Neural Network (CNN) with a classification
method we introduce called a late fusion ensemble. This late fusion ensemble
incorporates relevant contextual information and increases our classification
accuracy. Our technique achieves an overall accuracy of 83.07% in predicting a
person's activity across the 19 activity classes. We also demonstrate some
promising results from two additional users by fine-tuning the classifier with
one day of training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01577</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01577</id><created>2015-10-06</created><authors><author><keyname>Rozewski</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Jankowski</keyname><forenames>Jaroslaw</forenames></author></authors><title>Model of Multilayer Knowledge Diffusion for Competence Development in an
  Organization</title><categories>cs.SI</categories><journal-ref>Mathematical Problems in Engineering, Article ID 529256, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Growing role of intellectual capital within organizations is affecting new
strategies related to knowledge management and competence development. Among
different aspects related to this field, knowledge diffusion has become one of
interesting areas from both practitioner and researchers perspective. Several
models were proposed with main goal to simulate diffusion and to explain the
nature of these processes. Existing models are focused on knowledge diffusion
and they assume diffusion within a single layer using knowledge representation.
From the organizational perspective connecting several types of knowledge and
modelling changes of competence can bring additional value. In the article we
extended existing approaches by using multilayer diffusion model and focused on
analysis of competence development process. The proposed model describes
competence development process in a new way through horizontal and vertical
knowledge diffusion in multilayer network. In the network, agents collaborate
and interchange various kind of knowledge through different layers and this
mutual activities affect the competences in a positive or negative way. Taking
under consideration workers cognitive and social abilities and the previous
level of competence the new competence level can be estimated. The model is
developed to support competence management in different organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01591</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01591</id><created>2015-09-04</created><authors><author><keyname>Dertli</keyname><forenames>Abdullah</forenames></author><author><keyname>Cengellenmis</keyname><forenames>Yasein</forenames></author><author><keyname>Eren</keyname><forenames>Senol</forenames></author></authors><title>On the codes over the Z_3+vZ_3+v^2Z_3</title><categories>cs.IT math.IT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the structure of cyclic, quasi-cyclic, constacyclic
codes and their skew codes over the finite ring R=Z_3+vZ_3+v^2Z_3, v^3=v. The
Gray images of cyclic, quasi-cyclic, skew cyclic, skew quasi-cyclic and skew
constacyclic codes over R are obtained. A necessary and sufficient condition
for cyclic (negacyclic) codes over R that contains its dual has been given. The
parameters of quantum error correcting codes are obtained from both cyclic and
negacyclic codes over R. It is given some examples. Firstly, quasi-constacyclic
and skew quasi-constacyclic codes are introduced. By giving two product, it is
investigated their duality. A sufficient condition for 1-generator skew
quasi-constacyclic codes to be free is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01595</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01595</id><created>2015-10-06</created><authors><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author></authors><title>Ridgelet transform on the sphere</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures, Code available at http://www.s2let.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first revisit the spherical Radon transform, also called the Funk-Radon
transform, viewing it as an axisymmetric convolution on the sphere. Viewing the
spherical Radon transform in this manner leads to a straightforward derivation
of its spherical harmonic representation, from which we show the spherical
Radon transform can be inverted exactly for signals exhibiting antipodal
symmetry. We then construct a spherical ridgelet transform by composing the
spherical Radon and scale-discretised wavelet transforms on the sphere. The
resulting spherical ridgelet transform also admits exact inversion for
antipodal signals. The restriction to antipodal signals is expected since the
spherical Radon and ridgelet transforms themselves result in signals that
exhibit antipodal symmetry. Our ridgelet transform is defined natively on the
sphere, probes signal content globally along great circles, does not exhibit
blocking artefacts, does not rely of any ad hoc parameters and exhibits an
explicit inverse transform. No alternative ridgelet construction on the sphere
satisfies all of these properties. Our implementation of the spherical Radon
and ridgelet transforms is made publicly available. Finally, we illustrate the
effectiveness of spherical ridgelets for diffusion magnetic resonance imaging
of white matter fibers in the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01597</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01597</id><created>2015-10-06</created><updated>2016-01-28</updated><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Hall</keyname><forenames>Georgina</forenames></author></authors><title>Sum of Squares Basis Pursuit with Linear and Second Order Cone
  Programming</title><categories>math.OC cs.DM cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a scheme for solving an iterative sequence of linear programs (LPs)
or second order cone programs (SOCPs) to approximate the optimal value of any
semidefinite program (SDP) or sum of squares (SOS) program. The first LP and
SOCP-based bounds in the sequence come from the recent work of Ahmadi and
Majumdar on diagonally dominant sum of squares (DSOS) and scaled diagonally
dominant sum of squares (SDSOS) polynomials. We then iteratively improve on
these bounds by pursuing better bases in which more relevant SOS polynomials
admit a DSOS or SDSOS representation. Different interpretations of the
procedure from primal and dual perspectives are given. While the approach is
applicable to SDP relaxations of general polynomial programs, we apply it to
two problems of discrete optimization: the maximum independent set problem and
the partition problem. We further show that some completely trivial instances
of the partition problem lead to strictly positive polynomials on the boundary
of the sum of squares cone and hence make the SOS relaxation fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01599</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01599</id><created>2015-10-06</created><authors><author><keyname>Brochenin</keyname><forenames>Remi</forenames></author><author><keyname>Lierler</keyname><forenames>Yuliya</forenames></author><author><keyname>Maratea</keyname><forenames>Marco</forenames></author></authors><title>Disjunctive Answer Set Solvers via Templates</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><doi>10.1017/S1471068415000411</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer set programming is a declarative programming paradigm oriented towards
difficult combinatorial search problems. A fundamental task in answer set
programming is to compute stable models, i.e., solutions of logic programs.
Answer set solvers are the programs that perform this task. The problem of
deciding whether a disjunctive program has a stable model is
$\Sigma^P_2$-complete. The high complexity of reasoning within disjunctive
logic programming is responsible for few solvers capable of dealing with such
programs, namely DLV, GnT, Cmodels, CLASP and WASP. In this paper we show that
transition systems introduced by Nieuwenhuis, Oliveras, and Tinelli to model
and analyze satisfiability solvers can be adapted for disjunctive answer set
solvers. Transition systems give a unifying perspective and bring clarity in
the description and comparison of solvers. They can be effectively used for
analyzing, comparing and proving correctness of search algorithms as well as
inspiring new ideas in the design of disjunctive answer set solvers. In this
light, we introduce a general template, which accounts for major techniques
implemented in disjunctive solvers. We then illustrate how this general
template captures solvers DLV, GnT and Cmodels. We also show how this framework
provides a convenient tool for designing new solving algorithms by means of
combinations of techniques employed in different solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01624</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01624</id><created>2015-10-06</created><updated>2016-01-07</updated><authors><author><keyname>Krause</keyname><forenames>Oswin</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author><author><keyname>Igel</keyname><forenames>Christian</forenames></author></authors><title>Population-Contrastive-Divergence: Does Consistency help with RBM
  training?</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the log-likelihood gradient with respect to the parameters of a
Restricted Boltzmann Machine (RBM) typically requires sampling using Markov
Chain Monte Carlo (MCMC) techniques. To save computation time, the Markov
chains are only run for a small number of steps, which leads to a biased
estimate. This bias can cause RBM training algorithms such as Contrastive
Divergence (CD) learning to deteriorate. We adopt the idea behind Population
Monte Carlo (PMC) methods to devise a new RBM training algorithm termed
Population-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a
consistent estimate and may have a significantly lower bias. Its computational
overhead is negligible compared to CD. However, the variance of the gradient
estimate increases. We experimentally show that pop-CD can significantly
outperform CD. In many cases, we observed a smaller bias and achieved higher
log-likelihood values. However, when the RBM distribution has many hidden
neurons, the consistent estimate of pop-CD may still have a considerable bias
and the variance of the gradient estimate requires a smaller learning rate.
Thus, despite its superior theoretical properties, it is not advisable to use
pop-CD in its current form on large problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01625</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01625</id><created>2015-10-06</created><authors><author><keyname>Pardo</keyname><forenames>Diego</forenames></author><author><keyname>Neunert</keyname><forenames>Michael</forenames></author><author><keyname>Winkler</keyname><forenames>Alexander W.</forenames></author><author><keyname>Buchli</keyname><forenames>Jonas</forenames></author></authors><title>Projection based whole body motion planning for legged robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new approach for dynamic motion planning for
legged robots. We formulate a trajectory optimization problem based on a
compact form of the robot dynamics. Such a form is obtained by projecting the
rigid body dynamics onto the null space of the Constraint Jacobian. As
consequence of the projection, contact forces are removed from the model but
their effects are still taken into account. This approach permits to solve the
optimal control problem of a floating base constrained multibody system while
avoiding the use of an explicit contact model. We use direct transcription to
numerically solve the optimization. As the contact forces are not part of the
decision variables the size of the resultant discrete mathematical program is
reduced and therefore solutions can be obtained in a tractable time. Using a
predefined sequence of contact configurations (phases), our approach solves
motions where contact switches occur. Transitions between phases are
automatically resolved without using a model for switching dynamics. We present
results on a hydraulic quadruped robot (HyQ), including single phase (standing,
crouching) as well as multiple phase (rearing, diagonal leg balancing and
stepping) dynamic motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01628</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01628</id><created>2015-10-06</created><authors><author><keyname>Traganitis</keyname><forenames>Panagiotis A.</forenames></author><author><keyname>Slavakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Large-scale subspace clustering using sketching and validation</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nowadays massive amounts of generated and communicated data present major
challenges in their processing. While capable of successfully classifying
nonlinearly separable objects in various settings, subspace clustering (SC)
methods incur prohibitively high computational complexity when processing
large-scale data. Inspired by the random sampling and consensus (RANSAC)
approach to robust regression, the present paper introduces a randomized scheme
for SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale
data. At the heart of SkeVa-SC lies a randomized scheme for approximating the
underlying probability density function of the observed data by kernel
smoothing arguments. Sparsity in data representations is also exploited to
reduce the computational burden of SC, while achieving high clustering
accuracy. Performance analysis as well as extensive numerical tests on
synthetic and real data corroborate the potential of SkeVa-SC and its
competitive performance relative to state-of-the-art scalable SC approaches.
Keywords: Subspace clustering, big data, kernel smoothing, randomization,
sketching, validation, sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01640</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01640</id><created>2015-10-06</created><authors><author><keyname>Michalewski</keyname><forenames>Henryk</forenames></author><author><keyname>Mio</keyname><forenames>Matteo</forenames></author></authors><title>On the Problem of Computing the Probability of Regular Sets of Trees</title><categories>cs.FL</categories><acm-class>F.1.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the probability of regular languages of
infinite trees with respect to the natural coin-flipping measure. We propose an
algorithm which computes the probability of languages recognizable by
\emph{game automata}. In particular this algorithm is applicable to all
deterministic automata. We then use the algorithm to prove through examples
three properties of measure: (1) there exist regular sets having irrational
probability, (2) there exist comeager regular sets having probability $0$ and
(3) the probability of \emph{game languages} $W_{i,k}$, from automata theory,
is $0$ if $k$ is odd and is $1$ otherwise.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="84000" completeListSize="102538">1122234|85001</resumptionToken>
</ListRecords>
</OAI-PMH>
