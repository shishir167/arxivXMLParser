<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:01:02Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|89001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05135</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05135</id><created>2015-12-16</created><authors><author><keyname>Lee</keyname><forenames>Byunghan</forenames></author><author><keyname>Lee</keyname><forenames>Taehoon</forenames></author><author><keyname>Na</keyname><forenames>Byunggook</forenames></author><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author></authors><title>DNA-Level Splice Junction Prediction using Deep Recurrent Neural
  Networks</title><categories>cs.LG q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A eukaryotic gene consists of multiple exons (protein coding regions) and
introns (non-coding regions), and a splice junction refers to the boundary
between a pair of exon and intron. Precise identification of spice junctions on
a gene is important for deciphering its primary structure, function, and
interaction. Experimental techniques for determining exon/intron boundaries
include RNA-seq, which is often accompanied by computational approaches.
Canonical splicing signals are known, but computational junction prediction
still remains challenging because of a large number of false positives and
other complications. In this paper, we exploit deep recurrent neural networks
(RNNs) to model DNA sequences and to detect splice junctions thereon. We test
various RNN units and architectures including long short-term memory units,
gated recurrent units, and recently proposed iRNN for in-depth design space
exploration. According to our experimental results, the proposed approach
significantly outperforms not only conventional machine learning-based methods
but also a recent state-of-the-art deep belief network-based technique in terms
of prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05141</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05141</id><created>2015-12-16</created><authors><author><keyname>Spyrou</keyname><forenames>Evangelos D.</forenames></author><author><keyname>Mitrakos</keyname><forenames>Dimitrios K.</forenames></author></authors><title>Approximating Nash Equilibrium Uniqueness of Power Control In Practical
  WSNs</title><categories>cs.NI cs.GT</categories><comments>16 pages, 10 figures, Game theory, Wireless Sensor Networks,
  International Journal of Computer Networks &amp; Communications (IJCNC) Vol.7,
  No.6, November 2015</comments><msc-class>91A10, 91A80, 91A40</msc-class><acm-class>C.2.1, G.1.6</acm-class><doi>10.5121/ijcnc.2015.7604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission power has a major impact on link and communication reliability
and network lifetime in Wireless Sensor Networks. We study power control in a
multi-hop Wireless Sensor Network where nodes' communication interfere with
each other. Our objective is to determine each node's transmission power level
that will reduce the communication interference and keep energy consumption to
a minimum. We propose a potential game approach to obtain the unique
equilibrium of the network transmission power allocation. The unique
equilibrium is located in a continuous domain. However, radio transceivers
accept only discrete values for transmission power level setting. We study the
viability and performance of mapping the continuous solution from the potential
game to the discrete domain required by the radio. We demonstrate the success
of our approach through TOSSIM simulation when nodes use the Collection Tree
Protocol for routing the data. Also, we show results of our method from the
Indriya testbed. We compare it with the case where the motes use Collection
Tree Protocol with the maximum transmission power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05152</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05152</id><created>2015-12-16</created><authors><author><keyname>Parsa</keyname><forenames>Salman</forenames></author></authors><title>Small Model $2$-Complexes in $4$-space and Applications</title><categories>cs.CG cs.DS math.GT</categories><msc-class>57Q35, 55Q05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study finite $2$-complexes built from group presentations, called model
$2$-complexes. Model complexes have fundamental group isomorphic with the group
presented. We show that there are model complexes of size in the order of
bit-complexity of the presentation that can be realized linearly in
$\mathbb{R}^4$. We further derive some applications of this result regarding
embeddability problems in the euclidean $4$-space, and complexity of computing
integral homology groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05160</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05160</id><created>2015-12-16</created><authors><author><keyname>Qureshi</keyname><forenames>Jalaluddin</forenames></author><author><keyname>Malik</keyname><forenames>Adeel</forenames></author></authors><title>Throughput Bound of XOR Coded Wireless Multicasting to Three Clients</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper appears in the proceedings of 20th IEEE International
  Workshop on Computer Aided Modelling and Design of Communication Links and
  Networks (CAMAD), 7-9 September 2015, University of Surrey, Guildford, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a well-known result that constructing codewords over $GF(2)$ to
minimize the number of transmissions for a single-hop wireless multicasting is
an NP-complete problem. Linearly independent codewords can be constructed in
polynomial time for all the $n$ clients, known as maximum distance separable
(MDS) code, when the finite field size $q$ is larger than or equal to the
number of clients, $q\geq n$. In this paper we quantify the exact minimum
number of transmissions for a multicast network using erasure code when $q=2$
and $n=3$, such that $q&lt;n$. We first show that the use of Markov chain model to
derive the minimum number of transmissions for such a network is limited for
very small number of input packets. We then use combinatorial approach to
derive an upper bound on the exact minimum number of transmissions. Our results
show that the difference between the expected number of transmissions using XOR
coding and MDS coding is negligible for $n=3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05164</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05164</id><created>2015-12-16</created><updated>2016-03-03</updated><authors><author><keyname>Parsa</keyname><forenames>Salman</forenames></author></authors><title>On links of vertices in simplicial $d$-complexes embeddable in the
  euclidean $2d$-space</title><categories>cs.CG cs.DM math.CO</categories><msc-class>52B05</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider $d$-dimensional simplicial complexes which can be PL embedded in
the $2d$-dimensional euclidean space. In short, we show that in any such
complex, for any three vertices, the intersection of the link-complexes of the
vertices is linklessly embeddable in the $(2d-1)$-dimensional euclidean space.
These considerations lead us to a new upper bound on the total number of
$d$-simplices in an embeddable complex in $2d$-space with $n$ vertices,
improving known upper bounds, for all $d \geq 2$. Moreover, the bound is also
true for the size of $d$-complexes linklessly embeddable in the
$(2d+1)$-dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05172</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05172</id><created>2015-12-16</created><updated>2015-12-21</updated><authors><author><keyname>An</keyname><forenames>Ni</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>On the performance overhead tradeoff of distributed principal component
  analysis via data partitioning</title><categories>cs.DC cs.NI cs.PF</categories><comments>6 pages, 6 figures, submitted to CISS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) is not only a fundamental dimension
reduction method, but is also a widely used network anomaly detection
technique. Traditionally, PCA is performed in a centralized manner, which has
poor scalability for large distributed systems, on account of the large network
bandwidth cost required to gather the distributed state at a fusion center.
Consequently, several recent works have proposed various distributed PCA
algorithms aiming to reduce the communication overhead incurred by PCA without
losing its inferential power. This paper evaluates the tradeoff between
communication cost and solution quality of two distributed PCA algorithms on a
real domain name system (DNS) query dataset from a large network. We also apply
the distributed PCA algorithm in the area of network anomaly detection and
demonstrate that the detection accuracy of both distributed PCA-based methods
has little degradation in quality, yet achieves significant savings in
communication bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05177</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05177</id><created>2015-12-16</created><authors><author><keyname>Weinert</keyname><forenames>Alexander</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Visibly Linear Dynamic Logic</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Visibly Linear Dynamic Logic (VLDL), which is an extension of
Linear Dynamic Logic (LDL) with temporal operators that are guarded by
nondeterministic visibly pushdown automata. We prove that VLDL describes
exactly the visibly pushdown languages over infinite words, which makes it
strictly more powerful than LTL and LDL and able to express properties of
recursive programs. The main technical contribution of this work is a
translation of VLDL formulas into nondeterministic visibly pushdown automata
with B\&quot;uchi acceptance of exponential size via one-way alternating jumping
automata.
  This translation yields exponential-time algorithms for satisfiability,
validity and model checking. We also show that visibly pushdown games with VLDL
winning conditions are solvable in triply-exponential time. We show all of
these problems to be complete for their respective complexity classes.
Furthermore, we prove that using deterministic pushdown automata as guards
yields undecidable decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05185</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05185</id><created>2015-12-16</created><updated>2016-01-09</updated><authors><author><keyname>Wang</keyname><forenames>Bin</forenames></author><author><keyname>Sun</keyname><forenames>Kai</forenames></author></authors><title>Power System Differential-Algebraic Equations</title><categories>cs.SY</categories><comments>4 pages. For researchers in power systems or control field who want
  to simulate the power system dynamics, this document gives details about the
  power system differential-algebraic equations and an example on the IEEE
  9-bus power system</comments><msc-class>37-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document presents an introduction of two commonly used power system
differential algebraic equations (DAEs) for studying power system dynamics like
electromechanical oscillation and angle stability: the second-order classical
model and the fourth-order detailed generator model. An example is provided on
the IEEE 9-bus system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05193</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05193</id><created>2015-12-16</created><updated>2015-12-29</updated><authors><author><keyname>Yin</keyname><forenames>Wenpeng</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>ABCNN: Attention-Based Convolutional Neural Network for Modeling
  Sentence Pairs</title><categories>cs.CL</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to model a pair of sentences is a critical issue in many natural language
processing (NLP) tasks such as answer selection (AS), paraphrase identification
(PI) and textual entailment (TE). Most prior work (i) deals with one individual
task by fine-tuning a specific system; (ii) models each sentence separately,
without considering the impact of the other sentence; or (iii) relies fully on
manually designed, task-specific linguistic features. This work presents a
general Attention Based Convolutional Neural Network (ABCNN) for modeling a
pair of sentences. We make three contributions. (i) ABCNN can be applied to a
wide variety of tasks that require modeling of sentence pairs. (ii) We propose
three attention schemes that integrate mutual influence between sentences into
CNN; thus, the representation of each sentence takes into consideration its
counterpart. These interdependent sentence pair representations are more
powerful than isolated sentence representations. (iii) ABCNN achieves
state-of-the-art performance on AS, PI and TE tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05199</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05199</id><created>2015-12-16</created><updated>2015-12-20</updated><authors><author><keyname>Kayama</keyname><forenames>Yoshihiko</forenames></author></authors><title>Extension of cellular automata by introducing an algorithm of recursive
  estimation of neighbors</title><categories>cs.CC nlin.CG</categories><comments>5 pages,10 figures, 21st International Symposium on Artificial Life
  and Robotics</comments><msc-class>68Q80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study focuses on an extended model of a standard cellular automaton (CA)
that includes an extra index consisting of a radius that defines a perception
area for each cell in addition to the radius defined by the CA rule. Extended
standard CA rules form a sequence ordered by this index, which includes the CA
rule as its first term. This extension aims at constructing a model that can be
used within the CA framework to study the relationship between information
processing and pattern formation in collective systems. Although the extension
presented here is merely an extrapolation to a CA with a larger rule
neighborhood, the extra radius can be interpreted as an individual difference
of each cell, which provides a new perspective to CA. Some pattern formations
in extended one-dimensional elementary CAs and two-dimensional Life-like CAs
are presented. It is expected that the extended CA can be applied to various
simulations of complex systems and other fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05201</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05201</id><created>2015-12-16</created><authors><author><keyname>Farsani</keyname><forenames>Reza K.</forenames></author><author><keyname>Khandani</keyname><forenames>Amir K.</forenames></author></authors><title>Novel Outer Bounds and Capacity Results for the Interference Channel
  with Conferencing Receivers</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, capacity bounds for the two-user Interference Channel (IC)
with cooperative receivers via conferencing links of finite capacities is
investigated. The capacity results known for this communication scenario are
limited to a very few special cases of the one-sided IC. One of the major
challenges in analyzing such cooperative networks is how to establish efficient
capacity outer bounds for them. In this paper, by applying new techniques,
novel capacity outer bounds are presented for the ICs with conferencing users.
Using the outer bounds, several new capacity results are proved for interesting
channels with unidirectional cooperation in strong and mixed interference
regimes. A fact is that the conferencing link (between receivers) may be
utilized to provide one receiver with information about its corresponding
signal or its non-corresponding signal (interference signal). As a remarkable
consequence, it is demonstrated that both strategies can be helpful to achieve
the capacity of the channel. Finally, for the case of Gaussian IC, it is
mathematically shown that our outer bound is strictly tighter than the previous
one derived by Wang and Tse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05207</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05207</id><created>2015-12-16</created><authors><author><keyname>Ehlers</keyname><forenames>Ruediger</forenames></author></authors><title>Computing the Complete Pareto Front</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an efficient algorithm to enumerate all elements of a Pareto front in
a multi-objective optimization problem in which the space of values is finite
for all objectives. Our algorithm uses a feasibility check for a search space
element as an oracle and minimizes the number of oracle calls that are
necessary to identify the Pareto front of the problem. Given a $k$-dimensional
search space in which each dimension has $n$ elements, it needs $p \cdot (k
\cdot \lceil \log_2 n \rceil + 1) + \psi(p)$ oracle calls, where $p$ is the
size of the Pareto front and $\psi(p)$ is the number of greatest elements of
the part of the search space that is not dominated by the Pareto front
elements. We show that this number of oracle calls is essentially optimal as
approximately $p \cdot k \cdot \log_2 n$ oracle calls are needed to identify
the Pareto front elements in sparse Pareto sets and $\psi(p)$ calls are needed
to show that no element is missing in the set of Pareto front elements found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05212</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05212</id><created>2015-12-16</created><authors><author><keyname>Chen</keyname><forenames>Yuanfang</forenames></author><author><keyname>Crespi</keyname><forenames>Noel</forenames></author><author><keyname>Lee</keyname><forenames>Gyu Myoung</forenames></author></authors><title>Reality Mining with Mobile Big Data: Understanding the Impact of Network
  Structure on Propagation Dynamics</title><categories>cs.SI physics.soc-ph stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information and epidemic propagation dynamics in complex networks is truly
important to discover and control terrorist attack and disease spread. How to
track, recognize and model such dynamics is a big challenge. With the
popularity of intellectualization and the rapid development of Internet of
Things (IoT), massive mobile data is automatically collected by millions of
wireless devices (e.g., smart phone and tablet). In this article, as a typical
use case, the impact of network structure on epidemic propagation dynamics is
investigated by using the mobile data collected from the smart phones carried
by the volunteers of Ebola outbreak areas. On this basis, we propose a model to
recognize the dynamic structure of a network. Then, we introduce and discuss
the open issues and future work for developing the proposed recognition model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05220</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05220</id><created>2015-12-16</created><authors><author><keyname>Bashford</keyname><forenames>Luke</forenames></author><author><keyname>Mehring</keyname><forenames>Carsten</forenames></author></authors><title>Ownership and Agency of an Independent Supernumerary Hand Induced by an
  Imitation Brain-Computer Interface</title><categories>q-bio.NC cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To study body ownership and control, illusions that elicit these feelings in
non-body objects are widely used. Classically introduced with the Rubber Hand
Illusion, these illusions have been replicated more recently in virtual reality
and by using brain-computer interfaces. Traditionally these illusions
investigate the replacement of a body part by an artificial counterpart,
however as brain-computer interface research develops it offers us the
possibility to explore the case where non-body objects are controlled in
addition to movements of our own limbs. Therefore we propose a new illusion
designed to test the feeling of ownership and control of an independent
supernumerary hand. Subjects are under the impression they control a virtual
reality hand via a brain-computer interface, but in reality there is no causal
connection between brain activity and virtual hand movement but correct
movements are observed with 80% probability. These imitation brain-computer
interface trials are interspersed with movements in both the subjects' real
hands, which are in view throughout the experiment. We show that subjects
develop strong feelings of ownership and control over the third hand, despite
only receiving visual feedback with no causal link to the actual brain signals.
Our illusion is crucially different from previously reported studies as we
demonstrate independent ownership and control of the third hand without loss of
ownership in the real hands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05222</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05222</id><created>2015-12-16</created><authors><author><keyname>Herman</keyname><forenames>Ivo</forenames></author><author><keyname>Martinec</keyname><forenames>Dan</forenames></author><author><keyname>Sebek</keyname><forenames>Michael</forenames></author></authors><title>Transfer functions in consensus systems with higher-order dynamics and
  external inputs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers transfer functions in consensus systems where agents
have identical SISO dynamics of arbitrary order. The interconnecting structure
is a directed graph. The transfer functions for various inputs and outputs are
presented in simple product forms with a similar structure of the numerator and
the denominator. This structure combines the network properties and the agent
model in an explicit way. The link between a higher-order and a
single-integrator dynamics is shown and the polynomials of the transfer
function in the single-integrator system are related to the graph properties.
These properties also allow to generalize a result on the minimal dimension of
the controllable subspace to the directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05223</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05223</id><created>2015-12-16</created><authors><author><keyname>Faria</keyname><forenames>Luerbio</forenames></author><author><keyname>Klein</keyname><forenames>Sulamita</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Sucupira</keyname><forenames>Rubens</forenames></author></authors><title>Improved kernels for Signed Max Cut parameterized above lower bound on
  (r,l)-graphs</title><categories>cs.DS</categories><comments>18 pages, 3 figures</comments><msc-class>05C85, 05C10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is signed if each edge is assigned $+$ or $-$. A signed graph is
balanced if there is a bipartition of its vertex set such that an edge has sign
$-$ if and only if its endpoints are in different parts. The Edwards-Erd\&quot;os
bound states that every graph with $n$ vertices and $m$ edges has a balanced
subgraph with at least $\frac{m}{2}+\frac{n-1}{4}$ edges. In the Signed Max Cut
Above Tight Lower Bound (Signed Max Cut ATLB) problem, given a signed graph $G$
and a parameter $k$, the question is whether $G$ has a balanced subgraph with
at least $\frac{m}{2}+\frac{n-1}{4}+\frac{k}{4}$ edges. This problem
generalizes Max Cut Above Tight Lower Bound, for which a kernel with $O(k^5)$
vertices was given by Crowston et al. [ICALP 2012, Algorithmica 2015]. Crowston
et al. [TCS 2013] improved this result by providing a kernel with $O(k^3)$
vertices for the more general Signed Max Cut ATLB problem. In this article we
are interested in improving the size of the kernels for Signed Max Cut ATLB on
restricted graph classes for which the problem remains hard. For two integers
$r,\ell \geq 0$, a graph $G$ is an $(r,\ell)$-graph if $V(G)$ can be
partitioned into $r$ independent sets and $\ell$ cliques. Building on the
techniques of Crowston et al. [TCS 2013], we provide a kernel with $O(k^2)$
vertices on $(r,\ell)$-graphs for any fixed $r,\ell \geq 0$, and a simple
linear kernel on subclasses of split graphs for which we prove that the problem
is still NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05227</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05227</id><created>2015-12-16</created><authors><author><keyname>Cui</keyname><forenames>Yin</forenames></author><author><keyname>Zhou</keyname><forenames>Feng</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author></authors><title>Fine-grained Categorization and Dataset Bootstrapping using Deep Metric
  Learning with Humans in the Loop</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing fine-grained visual categorization methods often suffer from three
challenges: lack of training data, large number of fine-grained categories, and
high intraclass vs. low inter-class variance. In this work we propose a generic
iterative framework for fine-grained categorization and dataset bootstrapping
that handles these three challenges. Using deep metric learning with humans in
the loop, we learn a low dimensional feature embedding with anchor points on
manifolds for each category. These anchor points capture intra-class variances
and remain discriminative between classes. In each round, images with high
confidence scores from our model are sent to humans for labeling. By comparing
with exemplar images, labelers will mark each candidate image as either a &quot;true
positive&quot; or a &quot;false positive&quot;. True positives are added into our current
dataset and false positives are regarded as &quot;hard negatives&quot; for our metric
learning model. Then the model is retrained with an expanded dataset and hard
negatives for the next round. To demonstrate the effectiveness of the proposed
framework, we bootstrap a fine-grained flower dataset with 620 categories from
Instagram images. The proposed deep metric learning scheme is evaluated on both
our dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show
significant performance gain using dataset bootstrapping and demonstrate
state-of-the-art results achieved by the proposed deep metric learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05228</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05228</id><created>2015-12-16</created><authors><author><keyname>Yu</keyname><forenames>Jihong</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Wang</keyname><forenames>Kehao</forenames></author></authors><title>Finding Needles in a Haystack: Missing Tag Detection in Large RFID
  Systems</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio frequency identification (RFID) technology has been widely used in
missing tag detection to reduce and avoid inventory shrinkage. In this
application, promptly finding out the missing event is of paramount importance.
However, existing missing tag detection protocols cannot efficiently handle the
presence of a large number of unexpected tags whose IDs are not known to the
reader, which shackles the time efficiency. To deal with the problem of
detecting missing tags in the presence of unexpected tags, this paper
introduces a two-phase Bloom filter-based missing tag detection protocol
(BMTD). The proposed BMTD exploits Bloom filter in sequence to first deactivate
the unexpected tags and then test the membership of the expected tags, thus
dampening the interference from the unexpected tags and considerably reducing
the detection time. Moreover, the theoretical analysis of the protocol
parameters is performed to minimize the detection time of the proposed BMTD and
achieve the required reliability simultaneously. Extensive experiments are then
conducted to evaluate the performance of the proposed BMTD. The results
demonstrate that the proposed BMTD significantly outperforms the
state-of-the-art solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05244</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05244</id><created>2015-12-16</created><updated>2016-02-12</updated><authors><author><keyname>Nock</keyname><forenames>Richard</forenames></author></authors><title>Learning Games and Rademacher Observations Losses</title><categories>cs.LG</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has recently been shown that supervised learning with the popular logistic
loss is equivalent to optimizing the exponential loss over sufficient
statistics about the class: Rademacher observations (rados). We first show that
this unexpected equivalence can actually be generalized to other example / rado
losses, with necessary and sufficient conditions for the equivalence,
exemplified on four losses that bear popular names in various fields:
exponential (boosting), mean-variance (finance), Linear Hinge (on-line
learning), ReLU (deep learning), and unhinged (statistics). Second, we show
that the generalization unveils a surprising new connection to regularized
learning, and in particular a sufficient condition under which regularizing the
loss over examples is equivalent to regularizing the rados (with Minkowski
sums) in the equivalent rado loss. This brings simple and powerful rado-based
learning algorithms for sparsity-controlling regularization, that we exemplify
on a boosting algorithm for the regularized exponential rado-loss, which
formally boosts over four types of regularization, including the popular ridge
and lasso, and the recently coined slope --- we obtain the first proven
boosting algorithm for this last regularization. Through our first contribution
on the equivalence of rado and example-based losses, Omega-R.AdaBoost~appears
to be an efficient proxy to boost the regularized logistic loss over examples
using whichever of the four regularizers. Experiments display that
regularization consistently improves performances of rado-based learning, and
may challenge or beat the state of the art of example-based learning even when
learning over small sets of rados. Finally, we connect regularization to
differential privacy, and display how tiny budgets can be afforded on big
domains while beating (protected) example-based learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05245</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05245</id><created>2015-12-16</created><authors><author><keyname>Byrne</keyname><forenames>Fergal</forenames></author></authors><title>Symphony from Synapses: Neocortex as a Universal Dynamical Systems
  Modeller using Hierarchical Temporal Memory</title><categories>cs.NE cs.AI q-bio.NC</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Reverse engineering the brain is proving difficult, perhaps impossible. While
many believe that this is just a matter of time and effort, a different
approach might help. Here, we describe a very simple idea which explains the
power of the brain as well as its structure, exploiting complex dynamics rather
than abstracting it away. Just as a Turing Machine is a Universal Digital
Computer operating in a world of symbols, we propose that the brain is a
Universal Dynamical Systems Modeller, evolved bottom-up (itself using nested
networks of interconnected, self-organised dynamical systems) to prosper in a
world of dynamical systems.
  Recent progress in Applied Mathematics has produced startling evidence of
what happens when abstract Dynamical Systems interact. Key latent information
describing system A can be extracted by system B from very simple signals, and
signals can be used by one system to control and manipulate others. Using these
facts, we show how a region of the neocortex uses its dynamics to intrinsically
&quot;compute&quot; about the external and internal world.
  Building on an existing &quot;static&quot; model of cortical computation (Hawkins'
Hierarchical Temporal Memory - HTM), we describe how a region of neocortex can
be viewed as a network of components which together form a Dynamical Systems
modelling module, connected via sensory and motor pathways to the external
world, and forming part of a larger dynamical network in the brain.
  Empirical modelling and simulations of Dynamical HTM are possible with simple
extensions and combinations of currently existing open source software. We list
a number of relevant projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05246</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05246</id><created>2015-12-16</created><authors><author><keyname>Murdock</keyname><forenames>Calvin</forenames></author><author><keyname>Li</keyname><forenames>Zhen</forenames></author><author><keyname>Zhou</keyname><forenames>Howard</forenames></author><author><keyname>Duerig</keyname><forenames>Tom</forenames></author></authors><title>Blockout: Dynamic Model Selection for Hierarchical Deep Networks</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most deep architectures for image classification--even those that are trained
to classify a large number of diverse categories--learn shared image
representations with a single model. Intuitively, however, categories that are
more similar should share more information than those that are very different.
While hierarchical deep networks address this problem by learning separate
features for subsets of related categories, current implementations require
simplified models using fixed architectures specified via heuristic clustering
methods. Instead, we propose Blockout, a method for regularization and model
selection that simultaneously learns both the model architecture and
parameters. A generalization of Dropout, our approach gives a novel
parametrization of hierarchical architectures that allows for structure
learning via back-propagation. To demonstrate its utility, we evaluate Blockout
on the CIFAR and ImageNet datasets, demonstrating improved classification
accuracy, better regularization performance, faster training, and the clear
emergence of hierarchical network structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05247</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05247</id><created>2015-12-16</created><authors><author><keyname>De Clercq</keyname><forenames>Sofie</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author><author><keyname>De Cock</keyname><forenames>Martine</forenames></author><author><keyname>Now&#xe9;</keyname><forenames>Ann</forenames></author></authors><title>Solving stable matching problems using answer set programming</title><categories>cs.AI</categories><comments>Under consideration in Theory and Practice of Logic Programming
  (TPLP). arXiv admin note: substantial text overlap with arXiv:1302.7251</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the introduction of the stable marriage problem (SMP) by Gale and
Shapley (1962), several variants and extensions have been investigated. While
this variety is useful to widen the application potential, each variant
requires a new algorithm for finding the stable matchings. To address this
issue, we propose an encoding of the SMP using answer set programming (ASP),
which can straightforwardly be adapted and extended to suit the needs of
specific applications. The use of ASP also means that we can take advantage of
highly efficient off-the-shelf solvers. To illustrate the flexibility of our
approach, we show how our ASP encoding naturally allows us to select optimal
stable matchings, i.e. matchings that are optimal according to some
user-specified criterion. To the best of our knowledge, our encoding offers the
first exact implementation to find sex-equal, minimum regret, egalitarian or
maximum cardinality stable matchings for SMP instances in which individuals may
designate unacceptable partners and ties between preferences are allowed.
  This paper is under consideration in Theory and Practice of Logic Programming
(TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05256</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05256</id><created>2015-12-16</created><authors><author><keyname>Samanvi</keyname><forenames>Kanigalpula</forenames></author><author><keyname>Sivadasan</keyname><forenames>Naveen</forenames></author></authors><title>Subgraph Similarity Search in Large Graphs</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges in applications related to social networks,
computational biology, collaboration networks etc., is to efficiently search
for similar patterns in their underlying graphs. These graphs are typically
noisy and contain thousands of vertices and millions of edges. In many cases,
the graphs are unlabeled and the notion of similarity is also not well defined.
We study the problem of searching an induced subgraph in a large target graph
that is most similar to the given query graph. We assume that the query graph
and target graph are undirected and unlabeled. We use graphlet kernels
\cite{shervashidze2009efficient} to define graph similarity. Graphlet kernels
are known to perform better than other kernels in different applications.
  Our algorithm maps topological neighborhood information of vertices in the
query and target graphs to vectors. These local topological informations are
then combined to find a target subgraph having highly similar global topology
with the given query graph. We tested our algorithm on several real world
networks such as facebook network, google plus network, youtube network, amazon
network etc. Most of them contain thousands of vertices and million edges. Our
algorithm is able to detect highly similar matches when queried in these
networks. Our multi-threaded implementation takes about one second to find the
match on a 32 core machine, excluding the time for one time preprocessing.
Computationally expensive parts of our algorithm can be further scaled to
standard parallel and distributed frameworks like map-reduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05264</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05264</id><created>2015-12-16</created><authors><author><keyname>Pastorelli</keyname><forenames>Elena</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Martinelli</keyname><forenames>Michele</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>Impact of exponential long range and Gaussian short range lateral
  connectivity on the distributed simulation of neural networks including up to
  30 billion synapses</title><categories>cs.DC q-bio.NC</categories><comments>6 pages, 4 figures, 1 table</comments><acm-class>C.2.4; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent experimental neuroscience studies are pointing out the role of
long-range intra-areal connectivity that can be modeled by a distance dependent
exponential decay of the synaptic probability distribution. This short report
provides a preliminary measure of the impact of exponentially decaying lateral
connectivity compared to that of shorter-range Gaussian decays on the scaling
behaviour and memory occupation of a distributed spiking neural network
simulator (DPSNN). Two-dimensional grids of cortical columns composed by
point-like spiking neurons have been connected by up to 30 billion synapses
using exponential and Gaussian connectivity models. Up to 1024 hardware cores,
hosted on a 64 nodes server platform, executed the MPI processes composing the
distributed simulator. The hardware platform was a cluster of IBM NX360 M5
16-core compute nodes, each one containing two Intel Xeon Haswell 8-core
E5-2630 v3 processors, with a clock of 2.40GHz, interconnected through an
InfiniBand network. This study is conducted in the framework of the CORTICONIC
FET project, also in view of the next -to-start activities foreseen as part of
the Human Brain Project (HBP), SubProject 3 Cognitive and Systems Neuroscience,
WaveScalES work-package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05272</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05272</id><created>2015-12-16</created><authors><author><keyname>Djorgovski</keyname><forenames>S. G.</forenames></author><author><keyname>Mahabal</keyname><forenames>A. A.</forenames></author><author><keyname>Crichton</keyname><forenames>D.</forenames></author><author><keyname>Chaudhry</keyname><forenames>B.</forenames></author></authors><title>From Stars to Patients: Lessons from Space Science and Astrophysics for
  Health Care Informatics</title><categories>cs.SY astro-ph.IM</categories><comments>3 pages, to appear in refereed Proc. IEEE Big Data 2015, IEEE press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data are revolutionizing nearly every aspect of the modern society. One
area where this can have a profound positive societal impact is the field of
Health Care Informatics (HCI), which faces many challenges. The key idea behind
this study is: can we use some of the experience and technical and
methodological solutions from the fields that have successfully adapted to the
Big Data era, namely astronomy and space science, to help accelerate the
progress of HCI? We illustrate this with examples from the Virtual Observatory
framework, and the NCI EDRN project. An effective sharing and reuse of tools,
methods, and experiences from different fields can save a lot of effort, time,
and expense. HCI can thus benefit from the proven solutions to big data
challenges from other domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05278</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05278</id><created>2015-12-16</created><authors><author><keyname>Hui</keyname><forenames>Zhuo</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C</forenames></author></authors><title>Shape and Spatially-Varying Reflectance Estimation From Virtual
  Exemplars</title><categories>cs.CV</categories><comments>PAMI submission. arXiv admin note: substantial text overlap with
  arXiv:1503.04265</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of estimating the shape of objects that
exhibit spatially-varying reflectance. We assume that multiple images of the
object are obtained under a fixed view-point and varying illumination, i.e.,
the setting of photometric stereo. At the core of our techniques is the
assumption that the BRDF at each pixel lies in the non-negative span of a known
BRDF dictionary.This assumption enables a per-pixel surface normal and BRDF
estimation framework that is computationally tractable and requires no
initialization in spite of the underlying problem being non-convex. Our
estimation framework first solves for the surface normal at each pixel using a
variant of example-based photometric stereo. We design an efficient multi-scale
search strategy for estimating the surface normal and subsequently, refine this
estimate using a gradient descent procedure. Given the surface normal estimate,
we solve for the spatially-varying BRDF by constraining the BRDF at each pixel
to be in the span of the BRDF dictionary, here, we use additional priors to
further regularize the solution. A hallmark of our approach is that it does not
require iterative optimization techniques nor the need for careful
initialization, both of which are endemic to most state-of-the-art techniques.
We showcase the performance of our technique on a wide range of simulated and
real scenes where we outperform competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05279</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05279</id><created>2015-12-16</created><authors><author><keyname>Gold</keyname><forenames>Omer</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Improved Bounds for 3SUM, K-SUM, and Linear Degeneracy</title><categories>cs.DS cs.CC cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1404.0799 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of $n$ real numbers, the 3SUM problem is to decide whether there
are three of them that sum to zero. Until a recent breakthrough by Gr{\o}nlund
and Pettie [FOCS'14], a simple $\Theta(n^2)$-time deterministic algorithm for
this problem was conjectured to be optimal. Over the years many algorithmic
problems have been shown to be reducible from the 3SUM problem or its variants,
including the more generalized forms of the problem, such as k-SUM and
k-variate linear degeneracy testing. The conjectured hardness of these problems
have become extremely popular for basing conditional lower bounds for numerous
algorithmic problems in P. Thus, a better understanding of the complexity of
the 3SUM problem and its variants, might shed more light on the complexity of a
wide class of problems in P.
  In this paper we show the following:
  1. A deterministic algorithm for 3SUM that runs in $O(n^2 \log\log n / \log
n)$ time.
  2. The randomized decision tree complexity of 3SUM is $O(n^{3/2})$.
  3. The randomized decision tree complexity of $k$-variate linear degeneracy
testing (k-LDT) is $O(n^{k/2})$, for any odd $k\ge 3$.
  These bounds improve the ones obtained by Gr{\o}nlund and Pettie, giving a
faster deterministic algorithm and new randomized decision tree bounds for this
archetypal algorithmic problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05281</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05281</id><created>2015-12-16</created><authors><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author><author><keyname>Veltri</keyname><forenames>Luca</forenames></author><author><keyname>Davoli</keyname><forenames>Luca</forenames></author><author><keyname>Ventre</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Siracusano</keyname><forenames>Giuseppe</forenames></author></authors><title>PMSR - Poor Man's Segment Routing, a minimalistic approach to Segment
  Routing and a Traffic Engineering use case</title><categories>cs.NI</categories><comments>September 2015 - Paper accepted to the Mini-conference track of NOMS
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current specification of the Segment Routing (SR) architecture requires
enhancements to the intra-domain routing protocols (e.g. OSPF and IS-IS) so
that the nodes can advertise the Segment Identifiers (SIDs). We propose a
simpler solution called PMSR (Poor Man's Segment Routing), that does not
require any enhancement to routing protocol. We compare the procedures of PMSR
with traditional SR, showing that PMSR can reduce the operation and management
complexity. We analyze the set of use cases in the current SR drafts and we
claim that PMSR can support the large majority of them. Thanks to the drastic
simplification of the Control Plane, we have been able to develop an Open
Source prototype of PMSR. In the second part of the paper, we consider a
Traffic Engineering use case, starting from a traditional flow assignment
optimization problem which allocates hop-by-hop paths to flows. We propose a SR
path assignment algorithm and prove that it is optimal with respect to the
number of segments allocated to a flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05294</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05294</id><created>2015-12-16</created><updated>2016-02-07</updated><authors><author><keyname>Suresh</keyname><forenames>Harini</forenames></author></authors><title>Feature Representation for ICU Mortality</title><categories>cs.AI cs.LG stat.ML</categories><comments>This article has been withdrawn due by the author due to the need for
  more testing to verify results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good predictors of ICU Mortality have the potential to identify high-risk
patients earlier, improve ICU resource allocation, or create more accurate
population-level risk models. Machine learning practitioners typically make
choices about how to represent features in a particular model, but these
choices are seldom evaluated quantitatively. This study compares the
performance of different representations of clinical event data from MIMIC II
in a logistic regression model to predict 36-hour ICU mortality. The most
common representations are linear (normalized counts) and binary (yes/no).
These, along with a new representation termed &quot;hill&quot;, are compared using both
L1 and L2 regularization. Results indicate that the introduced &quot;hill&quot;
representation outperforms both the binary and linear representations, the hill
representation thus has the potential to improve existing models of ICU
mortality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05300</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05300</id><created>2015-12-16</created><updated>2015-12-18</updated><authors><author><keyname>Ustinova</keyname><forenames>Evgeniya</forenames></author><author><keyname>Ganin</keyname><forenames>Yaroslav</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Multiregion Bilinear Convolutional Neural Networks for Person
  Re-Identification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore the applicability of the recently proposed CNN
architecture, called Bilinear CNN, and its new modification that we call
multi-region Bilinear CNN to the person re-identification problem. Originally,
Bilinear CNNs were introduced for fine-grained classification and proved to be
both simple and high-performing architectures. Bilinear CNN allows to build an
orderless descriptor for an image using outer product of features outputted
from two separate feature extractors. Based on this approach, Multiregion
Bilinear CNN, apply bilinear pooling over multiple regions for extracting rich
and useful descriptors that retain some spatial information.
  We show than when embedded into a standard &quot;siamese&quot; type learning, bilinear
CNNs and in particular their multi-region variants can improve
re-identification performance compared to standard CNNs and achieve
state-of-the-art accuracy on the largest person re-identification datasets
available at the moment, namely CUHK03 and Market-1501.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05306</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05306</id><created>2015-12-16</created><authors><author><keyname>Di Luna</keyname><forenames>Giuseppe Antonio</forenames></author><author><keyname>Dobrev</keyname><forenames>Stefan</forenames></author><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Santoro</keyname><forenames>Nicola</forenames></author></authors><title>Live Exploration of Dynamic Rings</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost all the vast literature on graph exploration assumes that the graph is
static: its topology does not change during the exploration, except for
occasional faults. To date, very little is known on exploration of dynamic
graphs, where the topology is continously changing. The few studies have been
limited to the centralized (or post-mortem) case, assuming complete a priori
knowledge of the changes and the times of their occurrence, and have only
considered fully synchronous systems. In this paper, we start the study of the
decentralized (or live) exploration of dynamic graphs, i.e. when the agents
operate in the graph unaware of the location and timing of the changes. We
consider dynamic rings under the weak 1-interval-connected restriction, and
investigate the feasibility of their exploration, in both the fully synchronous
and semi-synchronous cases. When exploration is possible we examine at what
cost, focusing on the minimum number of agents capable of exploring the ring.
We establish several results highlighting the impact that anonymity and
structural knowledge have on the feasibility and complexity of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05313</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05313</id><created>2015-12-16</created><updated>2015-12-30</updated><authors><author><keyname>Blot</keyname><forenames>Valentin</forenames><affiliation>University of Bath</affiliation></author></authors><title>Typed realizability for first-order classical analysis</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:22) 2015</journal-ref><doi>10.2168/LMCS-11(4:22)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a realizability framework for classical first-order logic in
which realizers live in (a model of) typed {\lambda}{\mu}-calculus. This allows
a direct interpretation of classical proofs, avoiding the usual negative
translation to intuitionistic logic. We prove that the usual terms of G\&quot;odel's
system T realize the axioms of Peano arithmetic, and that under some
assumptions on the computational model, the bar recursion operator realizes the
axiom of dependent choice. We also perform a proper analysis of relativization,
which allows for less technical proofs of adequacy. Extraction of algorithms
from proofs of {\Pi}02 formulas relies on a novel implementation of Friedman's
trick exploiting the control possibilities of the language. This allows to have
extracted programs with simpler types than in the case of negative translation
followed by intuitionistic realizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05325</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05325</id><created>2015-12-16</created><authors><author><keyname>P&#xf6;ll&#xe4;nen</keyname><forenames>Antti</forenames></author></authors><title>Locally Repairable Codes and Matroid Theory</title><categories>cs.IT math.CO math.IT</categories><comments>Bachelor thesis, Aalto University School of Science, 2015. Instructor
  Ph.D. Thomas Westerb\&quot;ack, supervisor Prof. Camilla Hollanti. This thesis
  builds on the article arXiv:1501.00153. Main results: The class of optimal
  matroids with ceil(k/r)=2 is extended. An improved lower bound for d_max is
  presented. This bound is proven to be tight for a certain important class of
  matroids</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally repairable codes (LRCs) are error correcting codes used in
distributed data storage. A traditional approach is to look for codes which
simultaneously maximize error tolerance and minimize storage space consumption.
However, this tends to yield codes for which error correction requires an
unrealistic amount of communication between storage nodes. LRCs solve this
problem by allowing errors to be corrected locally.
  This thesis reviews previous results on the subject presented in [1]. These
include that every almost affine LRC induces a matroid such that the essential
properties of the code are determined by the matroid. Also, the generalized
Singleton bound for LRCs can be extended to matroids as well. Then, matroid
theory can be used to find classes of matroids that either achieve the bound,
meaning they are optimal in a certain sense, or at least come close to the
bound. This thesis presents an improvement to the results of [1] in both of
these cases.
  [1] T. Westerb\&quot;ack, R. Freij, T. Ernvall and C. Hollanti, &quot;On the
Combinatorics of Locally Repairable Codes via Matroid Theory&quot;, arXiv:1501.00153
[cs.IT], 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05338</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05338</id><created>2015-12-15</created><updated>2016-02-29</updated><authors><author><keyname>Yu</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>He</keyname><forenames>Zhengyou</forenames></author></authors><title>Two improved normalized subband adaptive filter algorithms with good
  robustness against impulsive interferences</title><categories>cs.OH</categories><comments>14 pages,8 figures,accepted by Circuits, Systems, and Signal
  Processing on Feb 23, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the robustness of subband adaptive filter (SAF) against impulsive
interferences, we propose two modified SAF algorithms with an individual scale
function for each subband, which are derived by maximizing correntropy-based
cost function and minimizing logarithm-based cost function, respectively,
called MCC-SAF and LC-SAF. Whenever the impulsive interference happens, the
subband scale functions can sharply drop the step size, which eliminate the
influence of outliers on the tap-weight vector update. Therefore, the proposed
algorithms are robust against impulsive interferences, and exhibit the faster
convergence rate and better tracking capability than the sign SAF (SSAF)
algorithm. Besides, in impulse-free interference environments, the proposed
algorithms achieve similar convergence performance as the normalized SAF (NSAF)
algorithm. Simulation results have demonstrated the performance of our proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05382</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05382</id><created>2015-12-16</created><authors><author><keyname>Hanel</keyname><forenames>Paul H. P.</forenames></author></authors><title>Why scientific publications should be anonymous</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous studies have revealed biases within the scientific communication
system and across all scientific fields. For example, already prominent
researchers receive disproportional credit compared to their (almost) equally
qualified colleagues -- because of their prominence. However, none of those
studies has offered a solution as to how to decrease the incidence of these
biases. In this paper I argue that by publishing anonymously, we can decrease
the incidence of inaccurate heuristics in the current scientific communication
system. Specific suggestions are made as to how to implement the changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05402</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05402</id><created>2015-12-16</created><authors><author><keyname>Ahmadi</keyname><forenames>Amir Ali</forenames></author><author><keyname>Dash</keyname><forenames>Sanjeeb</forenames></author><author><keyname>Hall</keyname><forenames>Georgina</forenames></author></authors><title>Optimization over Structured Subsets of Positive Semidefinite Matrices
  via Column Generation</title><categories>math.OC cs.DM cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop algorithms for inner approximating the cone of positive
semidefinite matrices via linear programming and second order cone programming.
Starting with an initial linear algebraic approximation suggested recently by
Ahmadi and Majumdar, we describe an iterative process through which our
approximation is improved at every step. This is done using ideas from column
generation in large-scale linear and integer programming. We then apply these
techniques to approximate the sum of squares cone in a nonconvex polynomial
optimization setting, and the copositive cone for a discrete optimization
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05403</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05403</id><created>2015-12-16</created><authors><author><keyname>Morales-Escalante</keyname><forenames>Jose</forenames></author><author><keyname>Gamba</keyname><forenames>Irene M.</forenames></author><author><keyname>Cheng</keyname><forenames>Yingda</forenames></author><author><keyname>Majorana</keyname><forenames>Armando</forenames></author><author><keyname>Shu</keyname><forenames>Chi-Wang</forenames></author><author><keyname>Chelikowsky</keyname><forenames>James</forenames></author></authors><title>Discontinuous Galerkin Deterministic Solvers for a Boltzmann-Poisson
  Model of Hot Electron Transport by Averaged Empirical Pseudopotential Band
  Structures</title><categories>cs.CE cond-mat.mes-hall math.NA</categories><comments>submission to CMAME (Computer Methods in Applied Mechanics and
  Engineering) Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this work is to incorporate numerically, in a discontinuous
Galerkin (DG) solver of a Boltzmann-Poisson model for hot electron transport,
an electronic conduction band whose values are obtained by the spherical
averaging of the full band structure given by a local empirical pseudopotential
method (EPM) around a local minimum of the conduction band for silicon, as a
midpoint between a radial band model and an anisotropic full band, in order to
provide a more accurate physical description of the electron group velocity and
conduction energy band structure in a semiconductor. This gives a better
quantitative description of the transport and collision phenomena that
fundamentally define the behaviour of the Boltzmann - Poisson model for
electron transport used in this work. The numerical values of the derivatives
of this conduction energy band, needed for the description of the electron
group velocity, are obtained by means of a cubic spline interpolation. The
EPM-Boltzmann-Poisson transport with this spherically averaged EPM calculated
energy surface is numerically simulated and compared to the output of
traditional analytic band models such as the parabolic and Kane bands,
numerically implemented too, for the case of 1D $n^+-n-n^+$ silicon diodes with
400nm and 50nm channels. Quantitative differences are observed in the kinetic
moments related to the conduction energy band used, such as mean velocity,
average energy, and electric current (momentum).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05405</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05405</id><created>2015-12-16</created><updated>2016-02-09</updated><authors><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Varma</keyname><forenames>Rohan</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Signal Recovery on Graphs: Fundamental Limits of Sampling Strategies</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper builds theoretical foundations for the recovery of a newly
proposed class of smooth graph signals, approximately bandlimited graph
signals, under three sampling strategies: uniform sampling, experimentally
designed sampling and active sampling. We then state minimax lower bounds on
the maximum risk for the approximately bandlimited class under three sampling
strategies and show that active sampling cannot fundamentally outperform
experimentally designed sampling. We then propose a recovery strategy to
compare uniform sampling with experimentally designed sampling. As the proposed
recovery strategy lends itself well to statistical analysis, we derive the
exact mean square error of each sampling strategy. To study convergence rates,
we introduce two types of graphs and find that (1) the proposed recovery
strategies achieve the optimal rates; and (2) the experimentally designed
sampling fundamentally outperforms uniform sampling for type-2 class of graphs.
To validate our proposed recovery strategies, we test them on five specific
graphs: a ring graph with $k$ nearest neighbors, an Erd\H{o}s-R\'enyi graph
graph, a random geometric graph, a small-world graph and a power-law graph and
find that experimental results match the proposed theory well. This work shows
a comprehensive explanation for when and why sampling for semi-supervised
learning with graphs works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05406</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05406</id><created>2015-12-16</created><authors><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Varma</keyname><forenames>Rohan</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Signal Representations on Graphs: Tools and Applications</title><categories>cs.AI cs.IT cs.SI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for representing and modeling data on graphs. Based on
this framework, we study three typical classes of graph signals: smooth graph
signals, piecewise-constant graph signals, and piecewise-smooth graph signals.
For each class, we provide an explicit definition of the graph signals and
construct a corresponding graph dictionary with desirable properties. We then
study how such graph dictionary works in two standard tasks: approximation and
sampling followed with recovery, both from theoretical as well as algorithmic
perspectives. Finally, for each class, we present a case study of a real-world
problem by using the proposed methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05411</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05411</id><created>2015-12-16</created><authors><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Non-Local Probes Do Not Help with Graph Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work bridges the gap between distributed and centralised models of
computing in the context of sublinear-time graph algorithms. A priori, typical
centralised models of computing (e.g., parallel decision trees or centralised
local algorithms) seem to be much more powerful than distributed
message-passing algorithms: centralised algorithms can directly probe any part
of the input, while in distributed algorithms nodes can only communicate with
their immediate neighbours. We show that for a large class of graph problems,
this extra freedom does not help centralised algorithms at all: for example,
efficient stateless deterministic centralised local algorithms can be simulated
with efficient distributed message-passing algorithms. In particular, this
enables us to transfer existing lower bound results from distributed algorithms
to centralised local algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05413</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05413</id><created>2015-12-16</created><authors><author><keyname>Liu</keyname><forenames>Lihua</forenames></author><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author></authors><title>A Note on &quot;Efficient Algorithms for Secure Outsourcing of Bilinear
  Pairings&quot;</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the verifying equations in the scheme [Theoretical Computer
Science, 562 (2015), 112-121] cannot filter out some malformed values returned
by the malicious servers. We also remark that the two untrusted programs model
adopted in the scheme is somewhat artificial, and discuss some reasonable
scenarios for outsourcing computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05417</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05417</id><created>2015-12-16</created><updated>2016-03-02</updated><authors><author><keyname>Chow</keyname><forenames>Shui-Nee</forenames></author><author><keyname>Ye</keyname><forenames>Xiaojing</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author><author><keyname>Zhou</keyname><forenames>Haomin</forenames></author></authors><title>Influence Prediction for Continuous-Time Information Propagation on
  Networks</title><categories>cs.SI math.NA physics.soc-ph</categories><comments>25 pages, 14 figures</comments><msc-class>65C40, 65Y10, 68U35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of predicting the time evolution of influence, the
expected number of activated nodes, given a set of initially active nodes on a
propagation network. To address the significant computational challenges of
this problem on large-scale heterogeneous networks, we establish a system of
differential equations governing the dynamics of probability mass functions on
the state graph where the nodes each lumps a number of activation states of the
network, which can be considered as an analogue to the Fokker-Planck equation
in continuous space. We provides several methods to estimate the system
parameters which depend on the identities of the initially active nodes,
network topology, and activation rates etc. The influence is then estimated by
the solution of such a system of differential equations. This approach gives
rise to a class of novel and scalable algorithms that work effectively for
large-scale and dense networks. Numerical results are provided to show the very
promising performance in terms of prediction accuracy and computational
efficiency of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05421</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05421</id><created>2015-12-16</created><authors><author><keyname>Deglint</keyname><forenames>Jason</forenames></author><author><keyname>Kazemzadeh</keyname><forenames>Farnoud</forenames></author><author><keyname>Cho</keyname><forenames>Daniel</forenames></author><author><keyname>Clausi</keyname><forenames>David A.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Numerical Demultiplexing of Color Image Sensor Measurements via
  Non-linear Random Forest Modeling</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simultaneous capture of imaging data at multiple wavelengths across the
electromagnetic spectrum is highly challenging, requiring complex and costly
multispectral image sensors. In this study, we introduce a comprehensive
framework for performing simultaneous multispectral imaging using conventional
image sensors with color filter arrays via numerical demultiplexing of the
color image sensor measurements. A numerical forward model characterizing the
formation of sensor measurements from light spectra hitting the sensor is
constructed based on a comprehensive spectral characterization of the sensor. A
numerical demultiplexer is then learned via non-linear random forest modeling
based on the forward model. Given the learned numerical demultiplexer, one can
then demultiplex simultaneously-acquired measurements made by the image sensor
into reflectance intensities at discrete selectable wavelengths, resulting in a
higher resolution reflectance spectrum. Simulation and real-world experimental
results demonstrate the efficacy of such a method for simultaneous
multispectral imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05423</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05423</id><created>2015-12-16</created><authors><author><keyname>D&#xf6;rpinghaus</keyname><forenames>Meik</forenames></author></authors><title>A Proof of the Hyperplane Conjecture for a Large Class of Probability
  Density Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For log-concave probability density functions fulfilling certain regularity
conditions we present a proof of the famous hyperplane conjecture, also known
as slicing problem, in convex geometry originally stated by J. Bourgain. The
proof is based on an entropic formulation of the hyperplane conjecture given by
Bobkov and Madiman and uses a recent result on bounding the Kullback-Leibler
divergence by the Wasserstein distance given by Polyanskiy and Wu. Moreover, we
describe an application of the result for bounding entropy rates of stationary
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05427</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05427</id><created>2015-12-16</created><authors><author><keyname>Benavides</keyname><forenames>Fernando</forenames></author><author><keyname>Rajsbaum</keyname><forenames>Sergio</forenames></author></authors><title>The read/write protocol complex is collapsible</title><categories>cs.DC</categories><comments>Full version of Springer LNCS Proceedings of LATIN 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The celebrated \emph{asynchronous computability theorem} provides a
characterization of the class of decision tasks that can be solved in a
wait-free manner by asynchronous processes that communicate by writing and
taking atomic snapshots of a shared memory. Several variations of the model
have been proposed (immediate snapshots and iterated immediate snapshots), all
equivalent for wait-free solution of decision tasks, in spite of the fact that
the protocol complexes that arise from the different models are structurally
distinct. The topological and combinatorial properties of these snapshot
protocol complexes have been studied in detail, providing explanations for why
the asynchronous computability theorem holds in all the models.
  In reality concurrent systems do not provide processes with snapshot
operations. Instead, snapshots are implemented (by a wait-free protocol) using
operations that write and read individual shared memory locations. Thus,
read/write protocols are also computationally equivalent to snapshot protocols.
However, the structure of the read/write protocol complex has not been studied.
In this paper we show that the read/write iterated protocol complex is
collapsible (and hence contractible). Furthermore, we show that a distributed
protocol that wait-free implements atomic snapshots in effect is performing the
collapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05429</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05429</id><created>2015-12-16</created><authors><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Perez</keyname><forenames>David Lopez</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>DNA-GA: A New Approach of Network Performance Analysis</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages, 4 figures, submitted to ICC'16. arXiv admin note: text
  overlap with arXiv:1508.02808</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new approach of network performance analysis,
which is based on our previous works on the deterministic network analysis
using the Gaussian approximation (DNA-GA). First, we extend our previous works
to a signal-to-interference ratio (SIR) analysis, which makes our DNA-GA
analysis a formal microscopic analysis tool. Second, we show two approaches for
upgrading the DNA-GA analysis to a macroscopic analysis tool. Finally, we
perform a comparison between the proposed DNA-GA analysis and the existing
macroscopic analysis based on stochastic geometry. Our results show that the
DNA-GA analysis possesses a few special features: (i) shadow fading is
naturally considered in the DNAGA analysis; (ii) the DNA-GA analysis can handle
non-uniform user distributions and any type of multi-path fading; (iii) the
shape and/or the size of cell coverage areas in the DNA-GA analysis can be made
arbitrary for the treatment of hotspot network scenarios. Thus, DNA-GA analysis
is very useful for the network performance analysis of the 5th generation (5G)
systems with general cell deployment and user distribution, both on a
microscopic level and on a macroscopic level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05430</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05430</id><created>2015-12-16</created><updated>2016-02-02</updated><authors><author><keyname>Yu</keyname><forenames>Qian</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Stumpe</keyname><forenames>Martin C.</forenames></author><author><keyname>Yatziv</keyname><forenames>Liron</forenames></author><author><keyname>Shet</keyname><forenames>Vinay</forenames></author><author><keyname>Ibarz</keyname><forenames>Julian</forenames></author><author><keyname>Arnoud</keyname><forenames>Sacha</forenames></author></authors><title>Large Scale Business Discovery from Street Level Imagery</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search with local intent is becoming increasingly useful due to the
popularity of the mobile device. The creation and maintenance of accurate
listings of local businesses worldwide is time consuming and expensive. In this
paper, we propose an approach to automatically discover businesses that are
visible on street level imagery. Precise business store front detection enables
accurate geo-location of businesses, and further provides input for business
categorization, listing generation, etc. The large variety of business
categories in different countries makes this a very challenging problem.
Moreover, manual annotation is prohibitive due to the scale of this problem. We
propose the use of a MultiBox based approach that takes input image pixels and
directly outputs store front bounding boxes. This end-to-end learning approach
instead preempts the need for hand modeling either the proposal generation
phase or the post-processing phase, leveraging large labelled training
datasets. We demonstrate our approach outperforms the state of the art
detection techniques with a large margin in terms of performance and run-time
efficiency. In the evaluation, we show this approach achieves human accuracy in
the low-recall settings. We also provide an end-to-end evaluation of business
discovery in the real world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05437</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05437</id><created>2015-12-16</created><authors><author><keyname>Jong</keyname><forenames>Man-Hung</forenames></author><author><keyname>Ri</keyname><forenames>Chong-Han</forenames></author><author><keyname>Choe</keyname><forenames>Hyok-Chol</forenames></author><author><keyname>Hwang</keyname><forenames>Chol-Jun</forenames></author></authors><title>A Method of Passage-Based Document Retrieval in Question Answering
  System</title><categories>cs.IR</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for using the scoring values of passages to effectively
retrieve documents in a Question Answering system.
  For this, we suggest evaluation function that considers proximity between
each question terms in passage. And using this evaluation function , we extract
a documents which involves scoring values in the highest collection, as a
suitable document for question.
  The proposed method is very effective in document retrieval of Korean
question answering system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05448</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05448</id><created>2015-12-16</created><authors><author><keyname>Oliveira</keyname><forenames>Danilo Elias</forenames></author><author><keyname>Wolkowicz</keyname><forenames>Henry</forenames></author><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author></authors><title>ADMM for the SDP relaxation of the QAP</title><categories>math.OC cs.DS math.CO</categories><comments>12 pages, 1 table</comments><msc-class>90C22, 90B80, 90C46, 90-08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semidefinite programming (SDP) relaxation has proven to be extremely
strong for many hard discrete optimization problems. This is in particular true
for the quadratic assignment problem (QAP), arguably one of the hardest NP-hard
discrete optimization problems. There are several difficulties that arise in
efficiently solving the SDP relaxation, e.g.,~increased dimension; inefficiency
of the current primal-dual interior point solvers in terms of both time and
accuracy; and difficulty and high expense in adding cutting plane constraints.
  We propose using the alternating direction method of multipliers (ADMM) to
solve the SDP relaxation. This first order approach allows for inexpensive
iterations, a method of cheaply obtaining low rank solutions, as well a trivial
way of adding cutting plane inequalities. When compared to current approaches
and current best available bounds we obtain remarkable robustness, efficiency
and improved bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05449</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05449</id><created>2015-12-16</created><updated>2015-12-24</updated><authors><author><keyname>Du</keyname><forenames>Wei</forenames></author><author><keyname>Leung</keyname><forenames>Sunney Yung Sun</forenames></author><author><keyname>Tang</keyname><forenames>Yang</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Differential Evolution with Event-Triggered Impulsive Control</title><categories>cs.NE cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential evolution (DE) is a simple but powerful evolutionary algorithm,
which has been widely and successfully used in various areas. In this paper, an
event-triggered impulsive control scheme (ETI) is introduced to improve the
performance of DE. Impulsive control, the concept of which derives from control
theory, aims at regulating the states of a network by instantly adjusting the
states of a fraction of nodes at certain instants, and these instants are
determined by event-triggered mechanism (ETM). By introducing impulsive control
and ETM into DE, we hope to change the search performance of the population in
a positive way after revising the positions of some individuals at certain
moments. At the end of each generation, the impulsive control operation is
triggered when the update rate of the population declines or equals to zero. In
detail, inspired by the concepts of impulsive control, two types of impulses
are presented within the framework of DE in this paper: stabilizing impulses
and destabilizing impulses. Stabilizing impulses help the individuals with
lower rankings instantly move to a desired state determined by the individuals
with better fitness values. Destabilizing impulses randomly alter the positions
of inferior individuals within the range of the current population. By means of
intelligently modifying the positions of a part of individuals with these two
kinds of impulses, both exploitation and exploration abilities of the whole
population can be meliorated. In addition, the proposed ETI is flexible to be
incorporated into several state-of-the-art DE variants. Experimental results
over the CEC 2014 benchmark functions exhibit that the developed scheme is
simple yet effective, which significantly improves the performance of the
considered DE algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05457</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05457</id><created>2015-12-16</created><updated>2016-01-20</updated><authors><author><keyname>Li</keyname><forenames>Yixuan</forenames></author><author><keyname>Martinez</keyname><forenames>Oscar</forenames></author><author><keyname>Chen</keyname><forenames>Xing</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Hopcroft</keyname><forenames>John</forenames></author></authors><title>In a World That Counts: Clustering and Detecting Fake Social Engagement
  at Scale</title><categories>cs.SI</categories><comments>accepted to the International Conference on World Wide Web (WWW'16)</comments><doi>10.1145/2872427.2882972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can web services that depend on user generated content discern fake
social engagement activities by spammers from legitimate ones? In this paper,
we focus on the social site of YouTube and the problem of identifying bad
actors posting inorganic contents and inflating the count of social engagement
metrics. We propose an effective method, Leas (Local Expansion at Scale), and
show how the fake engagement activities on YouTube can be tracked over time by
analyzing the temporal graph based on the engagement behavior pattern between
users and YouTube videos. With the domain knowledge of spammer seeds, we
formulate and tackle the problem in a semi-supervised manner --- with the
objective of searching for individuals that have similar pattern of behavior as
the known seeds --- based on a graph diffusion process via local spectral
subspace. We offer a fast, scalable MapReduce deployment adapted from the
localized spectral clustering algorithm. We demonstrate the effectiveness of
our deployment at Google by achieving an manual review accuracy of 98% on
YouTube Comments graph in practice. Comparing with the state-of-the-art
algorithm CopyCatch, Leas achieves 10 times faster running time. Leas is
actively in use at Google, searching for daily deceptive practices on YouTube's
engagement graph spanning over a billion users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05463</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05463</id><created>2015-12-16</created><authors><author><keyname>Cui</keyname><forenames>Yuwei</forenames></author><author><keyname>Surpur</keyname><forenames>Chetan</forenames></author><author><keyname>Ahmad</keyname><forenames>Subutai</forenames></author><author><keyname>Hawkins</keyname><forenames>Jeff</forenames></author></authors><title>Continuous online sequence learning with an unsupervised neural network
  model</title><categories>cs.NE q-bio.NC</categories><comments>Preprint of journal submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability to recognize and predict temporal sequences of sensory inputs is
vital for survival in natural environments. Based on many known properties of
cortical neurons, a recent study proposed hierarchical temporal memory (HTM)
sequence memory as a theoretical framework for sequence learning in the cortex.
In this paper, we analyze properties of HTM sequence memory and apply it to
various sequence learning and prediction problems. We show the model is able to
continuously learn a large number of variable-order temporal sequences using an
unsupervised Hebbian-like learning rule. The sparse temporal codes formed by
the model can robustly handle branching temporal sequences by maintaining
multiple predictions until there is sufficient disambiguating evidence. We
compare the HTM sequence memory and other sequence learning algorithms,
including the autoregressive integrated moving average (ARIMA) model and long
short-term memory (LSTM), on sequence prediction problems with both artificial
and real-world data. The HTM model not only achieves comparable or better
accuracy than state-of-the-art algorithms, but also exhibits a set of
properties that is critical for sequence learning. These properties include
continuous online learning, the ability to handle multiple predictions and
branching sequences, robustness to sensor noise and fault tolerance, and good
performance without task-specific hyper-parameters tuning. Therefore the HTM
sequence memory not only advances our understanding of how the brain may solve
the sequence learning problem, but is also applicable to a wide range of
real-world problems such as discrete and continuous sequence prediction,
anomaly detection, and sequence classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05467</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05467</id><created>2015-12-17</created><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Lallich</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Unsupervised Feature Construction for Improving Data Representation and
  Semantics</title><categories>cs.AI cs.LG</categories><journal-ref>Journal of Intelligent Information Systems, vol. 40, iss. 3, pp.
  501-527, 2013</journal-ref><doi>10.1007/s10844-013-0235-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature-based format is the main data representation format used by machine
learning algorithms. When the features do not properly describe the initial
data, performance starts to degrade. Some algorithms address this problem by
internally changing the representation space, but the newly-constructed
features are rarely comprehensible. We seek to construct, in an unsupervised
way, new features that are more appropriate for describing a given dataset and,
at the same time, comprehensible for a human user. We propose two algorithms
that construct the new features as conjunctions of the initial primitive
features or their negations. The generated feature sets have reduced
correlations between features and succeed in catching some of the hidden
relations between individuals in a dataset. For example, a feature like $sky
\wedge \neg building \wedge panorama$ would be true for non-urban images and is
more informative than simple features expressing the presence or the absence of
an object. The notion of Pareto optimality is used to evaluate feature sets and
to obtain a balance between total correlation and the complexity of the
resulted feature set. Statistical hypothesis testing is used in order to
automatically determine the values of the parameters used for constructing a
data-dependent feature set. We experimentally show that our approaches achieve
the construction of informative feature sets for multiple datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05471</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05471</id><created>2015-12-17</created><authors><author><keyname>Sandor</keyname><forenames>Christian</forenames></author><author><keyname>Fuchs</keyname><forenames>Martin</forenames></author><author><keyname>Cassinelli</keyname><forenames>Alvaro</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Newcombe</keyname><forenames>Richard</forenames></author><author><keyname>Yamamoto</keyname><forenames>Goshiro</forenames></author><author><keyname>Feiner</keyname><forenames>Steven</forenames></author></authors><title>Breaking the Barriers to True Augmented Reality</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained
considerable commercial traction, with Facebook acquiring Oculus VR for \$2
billion, Magic Leap attracting more than \$500 million of funding, and
Microsoft announcing their HoloLens head-worn computer. Where is humanity
headed: a brave new dystopia-or a paradise come true?
  In this article, we present discussions, which started at the symposium
&quot;Making Augmented Reality Real&quot;, held at Nara Institute of Science and
Technology in August 2014. Ten scientists were invited to this three-day event,
which started with a full day of public presentations and panel discussions
(video recordings are available at the event web page), followed by two days of
roundtable discussions addressing the future of AR and VR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05475</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05475</id><created>2015-12-17</created><authors><author><keyname>Caron</keyname><forenames>Pascal</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Miklarz</keyname><forenames>Cl&#xe9;ment</forenames></author></authors><title>On the Hierarchy of Block Deterministic Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A regular language is $k$-lookahead deterministic (resp. $k$-block
deterministic) if it is specified by a $k$-lookahead deterministic (resp.
$k$-block deterministic) regular expression. These two subclasses of regular
languages have been respectively introduced by Han and Wood ($k$-lookahead
determinism) and by Giammarresi et al. ($k$-block determinism) as a possible
extension of one-unambiguous languages defined and characterized by
Br\&quot;uggemann-Klein and Wood. In this paper, we study the hierarchy and the
inclusion links of these families. We first show that each $k$-block
deterministic language is the alphabetic image of some one-unambiguous
language. Moreover, we show that the conversion from a minimal DFA of a
$k$-block deterministic regular language to a $k$-block deterministic automaton
not only requires state elimination, and that the proof given by Han and Wood
of a proper hierarchy in $k$-block deterministic languages based on this result
is erroneous. Despite these results, we show by giving a parameterized family
that there is a proper hierarchy in $k$-block deterministic regular languages.
We also prove that there is a proper hierarchy in $k$-lookahead deterministic
regular languages by studying particular properties of unary regular
expressions. Finally, using our valid results, we confirm that the family of
$k$-block deterministic regular languages is strictly included into the one of
$k$-lookahead deterministic regular languages by showing that any $k$-block
deterministic unary language is one-unambiguous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05483</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05483</id><created>2015-12-17</created><authors><author><keyname>Jain</keyname><forenames>Atish</forenames></author><author><keyname>Dedhia</keyname><forenames>Ronak</forenames></author><author><keyname>Patil</keyname><forenames>Abhijit</forenames></author></authors><title>Enhancing the security of caesar cipher substitution method using a
  randomized approach for more secure communication</title><categories>cs.CR</categories><comments>6 pages, 7 figures</comments><journal-ref>International Journal of Computer Applications, Volume 129, Number
  13, pages 6-11, November 2015</journal-ref><doi>10.5120/ijca2015907062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caesar cipher is an ancient, elementary method of encrypting plain text
message to protect it from adversaries. However, with the advent of powerful
computers there is a need for increasing the complexity of such algorithms. In
this paper, we contribute in the area of classical cryptography by providing a
modified approach and expanded version for Caesar cipher using knowledge of
mathematics and computer science. To increase the strength of this classical
encryption technique we use the concepts of affine ciphers, transposition
ciphers and randomized substitution techniques to create a cipher text which is
nearly impossible to decode. We also increase the domain of characters which
Caesar cipher Algorithm can encrypt by including all ASCII and extended ASCII
characters in addition to alphabets. A complex key generation technique which
generates two keys from a single key is used to provide enhanced security. We
aim to propose a modified version of Caesar cipher substitution technique which
can overcome all the limitations faced by classical Caesar Cipher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05484</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05484</id><created>2015-12-17</created><authors><author><keyname>Malmir</keyname><forenames>Mohsen</forenames></author><author><keyname>Sikka</keyname><forenames>Karan</forenames></author><author><keyname>Forster</keyname><forenames>Deborah</forenames></author><author><keyname>Fasel</keyname><forenames>Ian</forenames></author><author><keyname>Movellan</keyname><forenames>Javier R.</forenames></author><author><keyname>Cottrell</keyname><forenames>Garrison W.</forenames></author></authors><title>Deep Active Object Recognition by Joint Label and Action Prediction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An active object recognition system has the advantage of being able to act in
the environment to capture images that are more suited for training and that
lead to better performance at test time. In this paper, we propose a deep
convolutional neural network for active object recognition that simultaneously
predicts the object label, and selects the next action to perform on the object
with the aim of improving recognition performance. We treat active object
recognition as a reinforcement learning problem and derive the cost function to
train the network for joint prediction of the object label and the action. A
generative model of object similarities based on the Dirichlet distribution is
proposed and embedded in the network for encoding the state of the system. The
training is carried out by simultaneously minimizing the label and action
prediction errors using gradient descent. We empirically show that the proposed
network is able to predict both the object label and the actions on GERMS, a
dataset for active object recognition. We compare the test label prediction
accuracy of the proposed model with Dirichlet and Naive Bayes state encoding.
The results of experiments suggest that the proposed model equipped with
Dirichlet state encoding is superior in performance, and selects images that
lead to better training and higher accuracy of label prediction at test time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05485</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05485</id><created>2015-12-17</created><authors><author><keyname>Liu</keyname><forenames>Jin-Hu</forenames></author><author><keyname>Zhu</keyname><forenames>Yu-Xiao</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Improving personalized link prediction by hybrid diffusion</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Physica A 447 (2016) 199-207</journal-ref><doi>10.1016/j.physa.2015.12.036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by traditional link prediction and to solve the problem of
recommending friends in social networks, we introduce the personalized link
prediction in this paper, in which each individual will get equal number of
diversiform predictions. While the performances of many classical algorithms
are not satisfactory under this framework, thus new algorithms are in urgent
need. Motivated by previous researches in other fields, we generalize heat
conduction process to the framework of personalized link prediction and find
that this method outperforms many classical similarity-based algorithms,
especially in the performance of diversity. In addition, we demonstrate that
adding one ground node who is supposed to connect all the nodes in the system
will greatly benefit the performance of heat conduction. Finally, better hybrid
algorithms composed of local random walk and heat conduction have been
proposed. Numerical results show that the hybrid algorithms can outperform
other algorithms simultaneously in all four adopted metrics: AUC, precision,
recall and hamming distance. In a word, this work may shed some light on the
in-depth understanding of the effect of physical processes in personalized link
prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05486</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05486</id><created>2015-12-17</created><authors><author><keyname>Serhii</keyname><forenames>Dyshko</forenames></author></authors><title>When the extension property does not hold</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complete extension theorem for linear codes over a module alphabet and the
symmetrized weight composition is proved. It is shown that an extension
property with respect to arbitrary weight function does not hold for module
alphabets with a noncyclic socle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05497</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05497</id><created>2015-12-17</created><updated>2016-02-16</updated><authors><author><keyname>B&#xe6;kgaard</keyname><forenames>Per</forenames></author><author><keyname>Petersen</keyname><forenames>Michael Kai</forenames></author><author><keyname>Larsen</keyname><forenames>Jakob Eg</forenames></author></authors><title>Assessing Levels of Attention using Low Cost Eye Tracking</title><categories>cs.HC q-bio.NC</categories><comments>12 pages, 6 figures, 2 tables. The final publication will be
  available at Springer via http://dx.doi.org/DOIxxx, when published as part of
  the HCI International 2016 Conference Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of mobile eye trackers embedded in next generation smartphones
or VR displays will make it possible to trace not only what objects we look at
but also the level of attention in a given situation. Exploring whether we can
quantify the engagement of a user interacting with a laptop, we apply mobile
eye tracking in an in-depth study over 2 weeks with nearly 10.000 observations
to assess pupil size changes, related to attentional aspects of alertness,
orientation and conflict resolution. Visually presenting conflicting cues and
targets we hypothesize that it's feasible to measure the allocated effort when
responding to confusing stimuli. Although such experiments are normally carried
out in a lab, we are able to differentiate between sustained alertness and
complex decision making even with low cost eye tracking &quot;in the wild&quot;. From a
quantified self perspective of individual behavioral adaptation, the
correlations between the pupil size and the task dependent reaction time and
error rates may longer term provide a foundation for modifying smartphone
content and interaction to the users perceived level of attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05499</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05499</id><created>2015-12-17</created><authors><author><keyname>Mutiara</keyname><forenames>A. B.</forenames></author><author><keyname>Refianti</keyname><forenames>R.</forenames></author><author><keyname>Karamoy</keyname><forenames>J. S. K.</forenames></author></authors><title>On A Testing and Implementation of Quantum Gate and Measurement Emulator
  (QGAME)</title><categories>cs.OH</categories><comments>10 pages, 6 figures</comments><journal-ref>International Journal of Engineering and Technology (IJET), Vol. 5
  No. 3 Jun-Jul 2013, pp. 2186-2195</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, people are looking forward to get an awesome computational power. This
kind of desire can be answered by quantum computing. By adopting quantum
mechanics theory, it can generate a very fast computation result. As known,
quantum mechanics can establish that particle can also become wave; it shows
that electron can be in duality. Through this theory, even a human
teleportation is issued can be really happened in the future. However, it needs
a high requirement of hardware support to implement the real quantum computing.
That is why it is difficult to bring quantum computing into reality. This
research presents a study about quantum computing. Here it is studied, a
specialty of quantum computing, like superposition, as if the classical
computer can do it. Since there was a marvellous research about quantum
computer simulation that runs on classical computer, this research provides an
analysis about our testing and implementation of Quantum Gate and Measurement
Emulator (QGAME). Our analysis, testing and implementation are based on a
method that always use in the software engineering field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05500</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05500</id><created>2015-12-17</created><authors><author><keyname>Yang</keyname><forenames>Wenjie Kristo</forenames></author><author><keyname>Wang</keyname><forenames>Mao</forenames></author><author><keyname>Zou</keyname><forenames>Jun Kingsley</forenames></author><author><keyname>Hua</keyname><forenames>Min</forenames></author><author><keyname>Zhang</keyname><forenames>Jingjing</forenames></author></authors><title>Power- and Spectrally-Efficient Network Access for Cellular Machine-Type
  Communications</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-type communication (MTC) is the key technology to support data
transfer among devices (sensors and actuators). Cellular communication
technologies are developed mainly for &quot;human-type&quot; communications, while
enabling MTC with cellular networks not only improves the connectivity,
accessibility, and availability of an MTC network but also has the potential to
further drive down the operation cost. However, cellular MTC poses some unique
challenges due to a huge mismatch between the cellular user equipment and the
MTC device, caused mainly by the physical limitations, deployment environment
and density, and application behavior of MTC devices. One of the most
challenging issues is to provide an efficient way for an MTC device to access
the network. We address the issues in the existing random access scheme in a
cellular system (e.g., LTE), and propose an effective spectrum concept enabling
a power and spectrally optimized design specifically-tailored for MTC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05504</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05504</id><created>2015-12-17</created><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Sekar</keyname><forenames>Shreyas</forenames></author></authors><title>Blind, Greedy, and Random: Ordinal Approximation Algorithms for Graph
  Problems</title><categories>cs.GT cs.AI</categories><comments>This paper contains the results that appeared in a AAAI'16 paper
  along with several new results for other problems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Matching, Clustering, and related problems in a partial information
setting, where the agents' true utilities are hidden, and the algorithm only
has access to ordinal preference information. Our model is motivated by the
fact that in many settings, agents cannot express the numerical values of their
utility for different outcomes, but are still able to rank the outcomes in
their order of preference. Specifically, we study problems where the ground
truth exists in the form of a weighted graph of agent utilities, but the
algorithm receives as input only a preference ordering for each agent induced
by the underlying weights. Against this backdrop, we design algorithms to
approximate the true optimum solution with respect to the hidden weights.
Perhaps surprisingly, such algorithms are possible for many important problems,
as we show using our framework based on greedy and random techniques. Our
framework yields a 1.6-approximation algorithm for the maximum weighted
matching problem, a 2-approximation for the problem of clustering agents into
equal sized partitions, a 4-approximation algorithm for Densest $k$-subgraph,
and a 1.88-approximation algorithm for Max TSP as long as the hidden weights
constitute a metric. Our results are the first non-trivial ordinal
approximation algorithms for such problems, and indicate that in many
situations, we can design robust algorithms even when we are agnostic to the
precise agent utilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05509</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05509</id><created>2015-12-17</created><authors><author><keyname>Steckelmacher</keyname><forenames>Denis</forenames></author><author><keyname>Vrancx</keyname><forenames>Peter</forenames></author></authors><title>An Empirical Comparison of Neural Architectures for Reinforcement
  Learning in Partially Observable Environments</title><categories>cs.NE cs.AI cs.LG</categories><comments>Presented at the 27th Benelux Conference on Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the performance of fitted neural Q iteration for
reinforcement learning in several partially observable environments, using
three recurrent neural network architectures: Long Short-Term Memory, Gated
Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of
several thousands candidate architectures. A variant of fitted Q iteration,
based on Advantage values instead of Q values, is also explored. The results
show that GRU performs significantly better than LSTM and MUT1 for most of the
problems considered, requiring less training episodes and less CPU time before
learning a very good policy. Advantage learning also tends to produce better
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05511</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05511</id><created>2015-12-17</created><authors><author><keyname>Mu&#xf1;oz</keyname><forenames>Pablo</forenames></author><author><keyname>Vortmeier</keyname><forenames>Nils</forenames></author><author><keyname>Zeume</keyname><forenames>Thomas</forenames></author></authors><title>Dynamic Graph Queries</title><categories>cs.LO cs.DB</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph databases in many applications---semantic web, transport or biological
networks among others---are not only large, but also frequently modified.
Evaluating graph queries in this dynamic context is a challenging task, as
those queries often combine first-order and navigational features.
  Motivated by recent results on maintaining dynamic reachability, we study the
dynamic evaluation of traditional query languages for graphs in the descriptive
complexity framework. Our focus is on maintaining regular path queries, and
extensions thereof, by first-order formulas. In particular we are interested in
path queries defined by non-regular languages and in extended conjunctive
regular path queries (which allow to compare labels of paths based on word
relations). Further we study the closely related problems of maintaining
distances in graphs and reachability in product graphs.
  In this preliminary study we obtain upper bounds for those problems in
restricted settings, such as undirected and acyclic graphs, or under insertions
only, and negative results regarding quantifier-free update formulas. In
addition we point out interesting directions for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05526</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05526</id><created>2015-12-17</created><authors><author><keyname>Arnau</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>Single-Pole IIR Channel Power Prediction with Variable Delays</title><categories>cs.IT math.IT</categories><comments>Paper presented at IEEE GLOBECOM 2015, San Diego, California</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting outdated channel quality indicators is crucial in most adaptive
wireless communication systems. This is often done through channel prediction
based on previous received indicators. In this paper, we analyze the case where
the feedback delay experienced by the quality indicators is not constant, but
random. Focusing on a single-pole IIR predictor, we obtain analytical
expressions for the MSE and the filter parameters, and study the throughput
behavior through Monte Carlo simulations. Results show that prediction provides
a performance advantage for average delays smaller than 30 ms for low terminal
speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05550</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05550</id><created>2015-12-17</created><authors><author><keyname>Garimella</keyname><forenames>Kiran</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author><author><keyname>Mathioudakis</keyname><forenames>Michael</forenames></author></authors><title>Exploring Controversy in Twitter</title><categories>cs.SI</categories><comments>Accepted as demo at CSCW 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the topics discussed on social media, some spark more heated debate
than others. For example, experience suggests that major political events, such
as a vote for healthcare law in the US, would spark more debate between
opposing sides than other events, such as a concert of a popular music band.
Exploring the topics of discussion on Twitter and understanding which ones are
controversial is extremely useful for a variety of purposes, such as for
journalists to understand what issues divide the public, or for social
scientists to understand how controversy is manifested in social interactions.
  The system we present processes the daily trending topics discussed on the
platform, and assigns to each topic a controversy score, which is computed
based on the interactions among Twitter users, and a visualization of these
interactions, which provides an intuitive visual cue regarding the controversy
of the topic. The system also allows users to explore the messages (tweets)
associated with each topic, and sort and explore the topics by different
criteria (e.g., by controversy score, time, or related keywords).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05558</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05558</id><created>2015-12-17</created><authors><author><keyname>Fowkes</keyname><forenames>Jaroslav</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Parameter-Free Probabilistic API Mining at GitHub Scale</title><categories>cs.SE</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing API mining algorithms are not yet practical to use as they require
expensive parameter tuning and the returned set of API calls can be large,
highly redundant and difficult to understand. In an attempt to remedy these
shortcomings we present PAM (Probabilistic API Miner), a near parameter-free
probabilistic algorithm for mining the most informative API call patterns. We
show that PAM significantly outperforms both MAPO and UPMiner, achieving 70%
test-set precision, at retrieving relevant API call sequences from GitHub.
Moreover, we focus on libraries for which the developers have explicitly
provided code examples, yielding over 300,000 LOC of hand-written API example
code from the 967 client projects in the data set. This evaluation suggests
that the hand-written examples actually have limited coverage of real API
usages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05568</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05568</id><created>2015-12-17</created><authors><author><keyname>Brenguier</keyname><forenames>Romain</forenames></author><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Randour</keyname><forenames>Mickael</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames></author><author><keyname>Sassolas</keyname><forenames>Mathieu</forenames></author></authors><title>Non-Zero Sum Games for Reactive Synthesis</title><categories>cs.LO cs.FL cs.GT</categories><comments>LATA'16 invited paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this invited contribution, we summarize new solution concepts useful for
the synthesis of reactive systems that we have introduced in several recent
publications. These solution concepts are developed in the context of non-zero
sum games played on graphs. They are part of the contributions obtained in the
inVEST project funded by the European Research Council.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05569</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05569</id><created>2015-12-17</created><authors><author><keyname>Verma</keyname><forenames>Mohit</forenames></author><author><keyname>Rajasankar</keyname><forenames>J.</forenames></author></authors><title>A thermodynamical approach towards multi-criteria decision making (MCDM)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-criteria decision making (MCDM) problems, ratings are assigned to
the alternatives on different criteria by the expert group. In this paper, we
propose a thermodynamically consistent model for MCDM using the analogies for
thermodynamical indicators - energy, exergy and entropy. The most commonly used
method for analysing MCDM problem is Technique for Order of Preference by
Similarity to Ideal Solution (TOPSIS). The conventional TOPSIS method uses a
measure similar to that of energy for the ranking of alternatives. We
demonstrate that the ranking of the alternatives is more meaningful if we use
exergy in place of energy. The use of exergy is superior due to the inclusion
of a factor accounting for the quality of the ratings by the expert group. The
unevenness in the ratings by the experts is measured by entropy. The procedure
for the calculation of the thermodynamical indicators is explained in both
crisp and fuzzy environment. Finally, two case studies are carried out to
demonstrate effectiveness of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05578</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05578</id><created>2015-12-17</created><authors><author><keyname>Brauer</keyname><forenames>Peter</forenames><affiliation>Ericsson AB, G&#xf6;teborg, Sweden</affiliation></author><author><keyname>Lundqvist</keyname><forenames>Martin</forenames><affiliation>Ericsson AB, G&#xf6;teborg, Sweden</affiliation></author><author><keyname>M&#xe4;llo</keyname><forenames>Aare</forenames><affiliation>Ericsson AB, G&#xf6;teborg, Sweden</affiliation></author></authors><title>Improving Latency in a Signal Processing System on the Epiphany
  Architecture</title><categories>cs.DC</categories><comments>Draft paper submitted to and accepted by PDP 2016, 24th Euromicro
  International Conference on Parallel, Distributed and Network-Based
  Processing. Heraklion Crete, Greece, 17th-19th February 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use the Adapteva Epiphany manycore chip to demonstrate how
the throughput and the latency of a baseband signal processing chain, typically
found in LTE or WiFi, can be optimized by a combination of task- and data
parallelization, and data pipelining. The parallelization and data pipelining
are facilitated by the shared memory architecture of the Epiphany, and the fact
that a processor on one core can write directly into the memory of any other
core on the chip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05582</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05582</id><created>2015-12-17</created><updated>2016-01-18</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>Kauffman's adjacent possible in word order evolution</title><categories>cs.CL cs.IT math.IT physics.data-an physics.soc-ph</categories><comments>Minor corrections (small errors concerning the parameters of model 1,
  language, style,...) except for the mathematical arguments at the end of
  section &quot;Further details about Model 2&quot; of the supplementary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word order evolution has been hypothesized to be constrained by a word order
permutation ring: transitions involving orders that are closer in the
permutation ring are more likely. The hypothesis can be seen as a particular
case of Kauffman's adjacent possible in word order evolution. Here we consider
the problem of the association of the six possible orders of S, V and O to
yield a couple of primary alternating orders as a window to word order
evolution. We evaluate the suitability of various competing hypotheses to
predict one member of the couple from the other with the help of information
theoretic model selection. Our ensemble of models includes a six-way model that
is based on the word order permutation ring (Kauffman's adjacent possible) and
another model based on the dual two-way of standard typology, that reduces word
order to basic orders preferences (e.g., a preference for SV over VS and
another for SO over OS). Our analysis indicates that the permutation ring
yields the best model when favoring parsimony strongly, providing support for
Kauffman's general view and a six-way typology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05586</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05586</id><created>2015-12-17</created><authors><author><keyname>Chen</keyname><forenames>Zhouye</forenames></author><author><keyname>Basarab</keyname><forenames>Adrian</forenames></author><author><keyname>Kouam&#xe9;</keyname><forenames>Denis</forenames></author></authors><title>Reconstruction of Enhanced Ultrasound Images From Compressed
  Measurements Using Simultaneous Direction Method of Multipliers</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High resolution ultrasound image reconstruction from a reduced number of
measurements is of great interest in ultrasound imaging, since it could enhance
both the frame rate and image resolution. Compressive deconvolution, combining
compressed sensing and image deconvolution, represents an interesting
possibility to consider this challenging task. The model of compressive
deconvolution includes, in addition to the compressive sampling matrix, a 2D
convolution operator carrying the information on the system point spread
function. Through this model, the resolution of reconstructed ultrasound images
from compressed measurements mainly depends on three aspects: the acquisition
setup, i.e. the incoherence of the sampling matrix, the image regularization,
i.e. the sparsity prior, and the optimization technique. In this paper, we
mainly focused on the last two aspects. We proposed a novel simultaneous
direction method of multipliers-based optimization scheme to invert the linear
model, including two regularization terms expressing the sparsity of the RF
images in a given basis and the generalized Gaussian statistical assumption on
tissue reflectivity functions. The performance of the method is evaluated on
both simulated and in vivo data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05596</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05596</id><created>2015-12-17</created><authors><author><keyname>Tamura</keyname><forenames>Shinsuke</forenames></author><author><keyname>Haddad</keyname><forenames>Hazim A.</forenames></author><author><keyname>Islam</keyname><forenames>Nazmul</forenames></author><author><keyname>Alam</keyname><forenames>Kazi Md. Rokibul</forenames></author></authors><title>An Incoercible E-Voting Scheme Based on Revised Simplified Verifiable
  Re-encryption Mix-nets</title><categories>cs.CR</categories><journal-ref>Information Security and Computer Fraud 3 (2015) 32-38</journal-ref><doi>10.12691/iscf-3-2-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simplified verifiable re-encryption mix-net (SVRM) is revised and a scheme
for e-voting systems is developed based on it. The developed scheme enables
e-voting systems to satisfy all essential requirements of elections. Namely,
they satisfy requirements about privacy, verifiability, fairness and
robustness. It also successfully protects voters from coercers except cases
where the coercers force voters to abstain from elections. In detail, voters
can conceal correspondences between them and their votes, anyone can verify the
accuracy of election results, and interim election results are concealed from
any entity. About incoercibility, provided that erasable-state voting booths
which disable voters to memorize complete information exchanged between them
and election authorities for constructing votes are available, coercer C cannot
know candidates that voters coerced by C had chosen even if the candidates are
unique to the voters. In addition, elections can be completed without
reelections even when votes were handled illegitimately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05612</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05612</id><created>2015-12-17</created><authors><author><keyname>Sun</keyname><forenames>Ye</forenames></author><author><keyname>Ma</keyname><forenames>Long</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author></authors><title>The spreading ability of nodes towards localized targets in complex
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important type of dynamics on complex networks, spreading is widely
used to model many real processes such as the epidemic contagion and
information propagation. One of the most significant research questions in
spreading is to rank the spreading ability of nodes in the network. To this
end, substantial effort has been made and a variety of effective methods have
been proposed. These methods usually define the spreading ability of a node as
the number of finally infected nodes given that the spreading is initialized
from the node. However, in many real cases such as advertising and medicine
science the spreading only aims to cover a specific group of nodes. Therefore,
it is necessary to study the spreading ability of nodes towards localized
targets in complex networks. In this paper, we propose a reversed local path
algorithm for this problem. Simulation results show that our method outperforms
the existing methods in identifying the influential nodes with respect to these
localized targets. Moreover, the influential spreaders identified by our method
can effectively avoid infecting the non-target nodes in the spreading process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05616</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05616</id><created>2015-12-17</created><authors><author><keyname>Beltramelli</keyname><forenames>Tony</forenames></author><author><keyname>Risi</keyname><forenames>Sebastian</forenames></author></authors><title>Deep-Spying: Spying using Smartwatch and Deep Learning</title><categories>cs.CR cs.CY cs.LG</categories><comments>Security, Side-Channel Attack, Keystroke Inference, Motion Sensors,
  Deep Learning, Recurrent Neural Network, Wearable Computing</comments><acm-class>K.4.1; K.6.5; F.1.1; I.2.6; I.5.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Wearable technologies are today on the rise, becoming more common and broadly
available to mainstream users. In fact, wristband and armband devices such as
smartwatches and fitness trackers already took an important place in the
consumer electronics market and are becoming ubiquitous. By their very nature
of being wearable, these devices, however, provide a new pervasive attack
surface threatening users privacy, among others.
  In the meantime, advances in machine learning are providing unprecedented
possibilities to process complex data efficiently. Allowing patterns to emerge
from high dimensional unavoidably noisy data.
  The goal of this work is to raise awareness about the potential risks related
to motion sensors built-in wearable devices and to demonstrate abuse
opportunities leveraged by advanced neural network architectures.
  The LSTM-based implementation presented in this research can perform
touchlogging and keylogging on 12-keys keypads with above-average accuracy even
when confronted with raw unprocessed data. Thus demonstrating that deep neural
networks are capable of making keystroke inference attacks based on motion
sensors easier to achieve by removing the need for non-trivial pre-processing
pipelines and carefully engineered feature extraction strategies. Our results
suggest that the complete technological ecosystem of a user can be compromised
when a wearable wristband device is worn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05632</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05632</id><created>2015-12-17</created><authors><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>G&#xf6;bel</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Lapinskas</keyname><forenames>John</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author></authors><title>Amplifiers for the Moran Process</title><categories>math.PR cs.DM cs.SI q-bio.PE</categories><comments>98 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Moran process, as studied by Lieberman, Hauert and Nowak, is a stochastic
process modelling the spread of genetic mutations in populations. It has an
underlying graph in which vertices correspond to individuals. Initially, one
individual (chosen uniformly at random) possesses a mutation, with fitness
$r&gt;1$. All other individuals have fitness 1. At each step of the discrete-time
process, an individual is chosen with probability proportional to its fitness,
and its state (mutant or non-mutant) is passed on to an out-neighbour chosen
u.a.r. If the underlying graph is strongly connected, the process will
eventually reach fixation (all individuals are mutants) or extinction (no
individuals are mutants). We define an infinite family of directed graphs to be
strongly amplifying if, for every $r&gt;1$, the extinction probability tends to 0
as the number $n$ of vertices increases. Strong amplification is a rather
surprising property - the initial mutant only has a fixed selective advantage,
independent of $n$, which is &quot;amplified&quot; to give a fixation probability tending
to 1. Strong amplifiers have received quite a bit of attention. Lieberman et
al. proposed two potential families of them: superstars and metafunnels. It has
been argued heuristically that some infinite families of superstars are
strongly amplifying. The same has been claimed for metafunnels. We give the
first rigorous proof that there is a strongly amplifying family of directed
graphs which we call &quot;megastars&quot;. We show that the extinction probability of
$n$-vertex graphs in this family of megastars is roughly $n^{-1/2}$, up to
logarithmic factors, and that all infinite families of superstars and
metafunnels have larger extinction probabilities as a function of $n$. Our
analysis of megastars is tight, up to logarithmic factors. We also clarify the
literature on the isothermal theorem of Lieberman et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05648</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05648</id><created>2015-12-17</created><authors><author><keyname>Guth</keyname><forenames>Larry</forenames></author><author><keyname>Zahl</keyname><forenames>Joshua</forenames></author></authors><title>Curves in $\mathbb{R}^4$ and two-rich points</title><categories>math.CO cs.CG</categories><comments>20 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain a new bound on the number of two-rich points spanned by an
arrangement of low degree algebraic curves in $\mathbb{R}^4$. Specifically, we
show that an arrangement of $n$ algebraic curves determines at most $C_\epsilon
n^{4/3+3\epsilon}$ two-rich points, provided at most $n^{2/3-2\epsilon}$ curves
lie in any low degree hypersurface and at most $n^{1/3-\epsilon}$ curves lie in
any low degree surface. This result follows from a structure theorem about
arrangements of curves that determine many two-rich points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05653</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05653</id><created>2015-12-16</created><authors><author><keyname>Sparavigna</keyname><forenames>A. C.</forenames></author><author><keyname>Marazzato</keyname><forenames>R.</forenames></author></authors><title>Effects of GIMP Retinex Filtering Evaluated by the Image Entropy</title><categories>cs.CV</categories><comments>Keywords: Image Processing, Foggy Images, Retinex, Shannon Entropy,
  Generalized Entropies, Kaniadakis Entropy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A GIMP Retinex filtering can be used for enhancing images, with good results
on foggy images, as recently discussed. Since this filter has some parameters
that can be adjusted to optimize the output image, several approaches can be
decided according to desired results. Here, as a criterion for optimizing the
filtering parameters, we consider the maximization of the image entropy. We
use, besides the Shannon entropy, also a generalized entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05656</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05656</id><created>2015-12-17</created><authors><author><keyname>Albrecht</keyname><forenames>Benjamin</forenames></author></authors><title>Fast computation of all maximum acyclic agreement forests for two rooted
  binary phylogenetic trees</title><categories>q-bio.PE cs.DS</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary scenarios displaying reticulation events are often represented
by rooted phylogenetic networks. Due to biological reasons, those events occur
very rarely, and, thus, networks containing a minimum number of such events,
so-called minimum hybridization networks, are of particular interest for
research. Moreover, to study reticulate evolution, biologist need not only a
subset but all of those networks. To achieve this goal, the less complex
concept of rooted phylogenetic trees can be used as building block. Here, as a
first important step, the trees are disjoint into common parts, so-called
maximum acyclic agreement forests, which can then be turned into minimum
hybridization networks by applying further network building algorithms. In this
paper, we present two modifications of the first non-naive algorithm --- called
allMAAFs --- computing all maximum acyclic agreement forests for two rooted
binary phylogenetic trees on the same set of taxa. By a simulation study, we
indicate that through these modifications the algorithm is on average 8 times
faster than the original algorithm making this algorithm accessible to larger
input trees and, thus, to a wider range of biological problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05665</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05665</id><created>2015-12-17</created><updated>2016-01-05</updated><authors><author><keyname>Schaechtle</keyname><forenames>Ulrich</forenames></author><author><keyname>Zinberg</keyname><forenames>Ben</forenames></author><author><keyname>Radul</keyname><forenames>Alexey</forenames></author><author><keyname>Stathis</keyname><forenames>Kostas</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash K.</forenames></author></authors><title>Probabilistic Programming with Gaussian Process Memoization</title><categories>cs.LG cs.AI stat.ML</categories><comments>36 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Processes (GPs) are widely used tools in statistics, machine
learning, robotics, computer vision, and scientific computation. However,
despite their popularity, they can be difficult to apply; all but the simplest
classification or regression applications require specification and inference
over complex covariance functions that do not admit simple analytical
posteriors. This paper shows how to embed Gaussian processes in any
higher-order probabilistic programming language, using an idiom based on
memoization, and demonstrates its utility by implementing and extending classic
and state-of-the-art GP applications. The interface to Gaussian processes,
called gpmem, takes an arbitrary real-valued computational process as input and
returns a statistical emulator that automatically improve as the original
process is invoked and its input-output behavior is recorded. The flexibility
of gpmem is illustrated via three applications: (i) robust GP regression with
hierarchical hyper-parameter learning, (ii) discovering symbolic expressions
from time-series data by fully Bayesian structure learning over kernels
generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian
optimization with automatic inference and action selection. All applications
share a single 50-line Python library and require fewer than 20 lines of
probabilistic code each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05667</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05667</id><created>2015-12-17</created><updated>2016-01-04</updated><authors><author><keyname>Je&#x159;&#xe1;bek</keyname><forenames>Emil</forenames></author></authors><title>Proof complexity of intuitionistic implicational formulas</title><categories>cs.LO math.LO</categories><comments>45 pages, 1 figure; improved results in Appendix B</comments><msc-class>03F20 (Primary), 03B55 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study implicational formulas in the context of proof complexity of
intuitionistic propositional logic (IPC). On the one hand, we give an efficient
transformation of tautologies to implicational tautologies that preserves the
lengths of intuitionistic extended Frege (EF) or substitution Frege (SF) proofs
up to a polynomial. On the other hand, EF proofs in the implicational fragment
of IPC polynomially simulate full intuitionistic logic for implicational
tautologies. The results also apply to other fragments of other
superintuitionistic logics under certain conditions.
  In particular, the exponential lower bounds on the length of intuitionistic
EF proofs by Hrube\v{s} \cite{hru:lbint}, generalized to exponential separation
between EF and SF systems in superintuitionistic logics of unbounded branching
by Je\v{r}\'abek \cite{ej:sfef}, can be realized by implicational tautologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05670</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05670</id><created>2015-12-17</created><updated>2015-12-22</updated><authors><author><keyname>Krishna</keyname><forenames>Amrith</forenames></author><author><keyname>Goyal</keyname><forenames>Pawan</forenames></author></authors><title>Towards automating the generation of derivative nouns in Sanskrit by
  simulating Panini</title><categories>cs.CL</categories><comments>16th World Sanskrit Conference, Bangkok June 28th - July 02 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal with
generation of derivative nouns, making it one of the largest topical sections
in Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76.
This section is a systematic arrangement of rules that enumerates various
affixes that are used in the derivation under specific semantic relations. We
propose a system that automates the process of generation of derivative nouns
as per the rules in Astadhyayi. The proposed system follows a completely object
oriented approach, that models each rule as a class of its own and then groups
them as rule groups. The rule groups are decided on the basis of selective
grouping of rules by virtue of anuvrtti. The grouping of rules results in an
inheritance network of rules which is a directed acyclic graph. Every rule
group has a head rule and the head rule notifies all the direct member rules of
the group about the environment which contains all the details about data
entities, participating in the derivation process. The system implements this
mechanism using multilevel inheritance and observer design patterns. The system
focuses not only on generation of the desired final form, but also on the
correctness of sequence of rules applied to make sure that the derivation has
taken place in strict adherence to Astadhyayi. The proposed system's design
allows to incorporate various conflict resolution methods mentioned in
authentic texts and hence the effectiveness of those rules can be validated
with the results from the system. We also present cases where we have checked
the applicability of the system with the rules which are not specifically
applicable to derivation of derivative nouns, in order to see the effectiveness
of the proposed schema as a generic system for modeling Astadhyayi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05671</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05671</id><created>2015-12-17</created><authors><author><keyname>Olteanu</keyname><forenames>Alexandra</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Gatica-Perez</keyname><forenames>Daniel</forenames></author></authors><title>Characterizing the Demographics Behind the #BlackLivesMatter Movement</title><categories>cs.SI</categories><comments>9 pages, 7 figures, accepted to AAAI Spring Symposia on Observational
  Studies through Social Media and Other Human-Generated Content, Stanford, US,
  March 2016</comments><acm-class>K.4.2; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The debates on minority issues are often dominated by or held among the
concerned minority: gender equality debates have often failed to engage men,
while those about race fail to effectively engage the dominant group. To test
this observation, we study the #BlackLivesMatter}movement and hashtag on
Twitter--which has emerged and gained traction after a series of events
typically involving the death of African-Americans as a result of police
brutality--and aim to quantify the population biases across user types
(individuals vs. organizations), and (for individuals) across various
demographics factors (race, gender and age). Our results suggest that more
African-Americans engage with the hashtag, and that they are also more active
than other demographic groups. We also discuss ethical caveats with broader
implications for studies on sensitive topics (e.g. discrimination, mental
health, or religion) that focus on users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05685</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05685</id><created>2015-12-17</created><updated>2016-01-11</updated><authors><author><keyname>Schaible</keyname><forenames>Johann</forenames></author><author><keyname>Gottron</keyname><forenames>Thomas</forenames></author><author><keyname>Scherp</keyname><forenames>Ansgar</forenames></author></authors><title>TermPicker: Enabling the Reuse of Vocabulary Terms by Exploiting Data
  from the Linked Open Data Cloud - An Extended Technical Report</title><categories>cs.DB cs.IR</categories><comments>17 pages, 3 figures, extended technical report for a Conference Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding which vocabulary terms to use when modeling data as Linked Open Data
(LOD) is far from trivial. Choosing too general vocabulary terms, or terms from
vocabularies that are not used by other LOD datasets, is likely to lead to a
data representation, which will be harder to understand by humans and to be
consumed by Linked data applications. In this technical report, we propose
TermPicker: a novel approach for vocabulary reuse by recommending RDF types and
properties based on exploiting the information on how other data providers on
the LOD cloud use RDF types and properties to describe their data. To this end,
we introduce the notion of so-called schema-level patterns (SLPs). They capture
how sets of RDF types are connected via sets of properties within some data
collection, e.g., within a dataset on the LOD cloud. TermPicker uses such SLPs
and generates a ranked list of vocabulary terms for reuse. The lists of
recommended terms are ordered by a ranking model which is computed using the
machine learning approach Learning To Rank (L2R). TermPicker is evaluated based
on the recommendation quality that is measured using the Mean Average Precision
(MAP) and the Mean Reciprocal Rank at the first five positions (MRR@5). Our
results illustrate an improvement of the recommendation quality by 29% - 36%
when using SLPs compared to the beforehand investigated baselines of
recommending solely popular vocabulary terms or terms from the same vocabulary.
The overall best results are achieved using SLPs in conjunction with the
Learning To Rank algorithm Random Forests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05691</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05691</id><created>2015-12-17</created><authors><author><keyname>Dutta</keyname><forenames>Sourjya</forenames></author><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Ford</keyname><forenames>Russell</forenames></author><author><keyname>Zhang</keyname><forenames>Menglei</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Frame Structure Design and Analysis for Millimeter Wave Cellular Systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave (mmWave) frequencies between 30 and 300 GHz have attracted
considerable attention for fifth generation (5G) cellular communication as they
offer orders of magnitude greater bandwidth than current cellular systems.
However, the MAC layer may need to be significantly redesigned to support the
highly directional transmissions, very low latencies and high peak rates
inherent in mmWave communication. This paper analyzes two aspects of the mmWave
MAC design.Firstly, this paper analyzes radio frame design options for a mmWave
cellular airlink under the assumption of an LTE-like OFDM frame structure.
Simple analytic formulae are derived to estimate the utilization as a function
of key parameters such as the control periodicity, number of users, and channel
and traffic statistics. It is found that certain flexible frame structures can
offer dramatically improved utilization under various assumptions on the
traffic pattern. Secondly, the beamforming choices are analyzed based on the
control channel overhead that the link incurs. Analytical expressions for the
overhead due to the physical layer control messages are derived as a function
of control periodicity, signal-to-noise ratio, antenna gains and the frame
design choice. It is shown that fully digital beamforming architectures offer
significantly lower overhead compared to analog and hybrid beamforming under
equivalent power budgets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05693</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05693</id><created>2015-12-17</created><updated>2016-02-18</updated><authors><author><keyname>Bulteau</keyname><forenames>Laurent</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Hartung</keyname><forenames>Sepp</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Co-Clustering Under the Maximum Norm</title><categories>cs.DM</categories><comments>Journal version to appear in Algorithms</comments><msc-class>68Q25, 68R05, 62H30</msc-class><acm-class>F.2.2; G.2.1; H.3.3; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Co-clustering, that is, partitioning a numerical matrix into homogeneous
submatrices, has many applications ranging from bioinformatics to election
analysis. Many interesting variants of co-clustering are NP-hard. We focus on
the basic variant of co-clustering where the homogeneity of a submatrix is
defined in terms of minimizing the maximum distance between two entries. In
this context, we spot several NP-hard as well as a number of relevant
polynomial-time solvable special cases, thus charting the border of tractabil-
ity for this challenging data clustering problem. For instance, we provide
polynomial-time solvability when having to partition the rows and columns into
two subsets each (meaning that one obtains four submatrices). When partitioning
rows and columns into three subsets each, however, we encounter NP-hardness
even for input matrices containing only values from {0, 1, 2}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05702</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05702</id><created>2015-12-17</created><authors><author><keyname>Trischler</keyname><forenames>Adam P</forenames></author><author><keyname>D'Eleuterio</keyname><forenames>Gabriele MT</forenames></author></authors><title>Synthesis of recurrent neural networks for dynamical system simulation</title><categories>cs.NE</categories><msc-class>68T01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review several of the most widely used techniques for training recurrent
neural networks to approximate dynamical systems, then describe a novel
algorithm for this task. The algorithm is based on an earlier theoretical
result that guarantees the quality of the network approximation. We show that a
feedforward neural network can be trained on the vector field representation of
a given dynamical system using backpropagation, then recast, using matrix
manipulations, as a recurrent network that replicates the original system's
dynamics. After detailing this algorithm and its relation to earlier
approaches, we present numerical examples that demonstrate its capabilities.
One of the distinguishing features of our approach is that both the original
dynamical systems and the recurrent networks that simulate them operate in
continuous time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05703</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05703</id><created>2015-12-17</created><authors><author><keyname>Albrecht</keyname><forenames>Benjamin</forenames></author></authors><title>Computing a Relevant Set of Nonbinary Maximum Acyclic Agreement Forests</title><categories>q-bio.PE cs.DS</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exist several methods dealing with the reconstruction of rooted
phylogenetic networks explaining different evolutionary histories given by
rooted binary phylogenetic trees. In practice, however, due to insufficient
information of the underlying data, phylogenetic trees are in general not
completely resolved and, thus, those methods can often not be applied to
biological data. In this work, we make a first important step to approach this
goal by presenting the first algorithm --- called allMulMAAFs --- that enables
the computation of all relevant nonbinary maximum acyclic agreement forests for
two rooted (nonbinary) phylogenetic trees on the same set of taxa. Notice that
our algorithm is part of the freely available software Hybroscale computing
minimum hybridization networks for a set of rooted (nonbinary) phylogenetic
trees on an overlapping set of taxa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05705</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05705</id><created>2015-12-17</created><updated>2016-01-21</updated><authors><author><keyname>Triki</keyname><forenames>Imen</forenames></author><author><keyname>El-Azouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Haddad</keyname><forenames>Majed</forenames></author></authors><title>NEWCAST: Anticipating Resource Management and QoE Provisioning for
  Mobile Video Streaming</title><categories>cs.MM cs.NI</categories><comments>11 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The knowledge of future throughput variation in wireless networks using
smartphone becomes more and more possible by exploiting the rich contextual
information from smartphone sensors through mobile applications and services.
Contextual information may include the traffic, mobility and radio conditions.
Inspired by the attractive features and potential advantages of this agile
resource management, several approaches have been proposed during the last
period. However, agile resource management also comes with its own challenges,
and there are significant technical issues that still need to be addressed for
successful rollout and operation of this technique. In this paper, we propose
an approach for anticipating throughput variation for mobile video streaming
services. The solution of the optimization problem realizes a fundamental
trade-offs among critical metrics that impact the user's perceptual quality of
the experience (QoE) and system utilization. Both simulated and real-world
traces are carried out to evaluate the performance of the proposed approach. It
is shown that our approach provides the accuracy, efficiency and robustness
that the new 5G architectures require.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05726</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05726</id><created>2015-12-17</created><authors><author><keyname>Lei</keyname><forenames>Tao</forenames></author><author><keyname>Joshi</keyname><forenames>Hrishikesh</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author><author><keyname>Tymoshenko</keyname><forenames>Katerina</forenames></author><author><keyname>Moschitti</keyname><forenames>Alessandro</forenames></author><author><keyname>Marquez</keyname><forenames>Lluis</forenames></author></authors><title>Denoising Bodies to Titles: Retrieving Similar Questions with Recurrent
  Convolutional Models</title><categories>cs.CL cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Question answering forums are rapidly growing in size with no automated
ability to refer to and reuse existing answers. In this paper, we develop a
methodology for finding semantically related questions. The task is difficult
since 1) key pieces of information are often buried in extraneous detail in the
question body and 2) available annotations are scarce and fragmented, driven by
participants. We design a novel combination of recurrent and convolutional
models (gated convolutions) to effectively map questions to their semantic
representations. The models are pre-trained within an encoder-decoder framework
(from body to title) on the basis of the entire raw corpus, and fine-tuned
discriminatively from limited annotations. Our evaluation demonstrates that our
model yields 10\% gain over a standard IR baseline, and 6\% over standard
neural network architectures (including CNNs and LSTMs) trained analogously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05732</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05732</id><created>2015-12-17</created><authors><author><keyname>Pinals</keyname><forenames>Lisa</forenames></author><author><keyname>Haija</keyname><forenames>Ahmad Abu Al</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Link Regime and Power Savings of Decode-Forward Relaying in Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>16 pages, 11 figures, Related conference papers 'Relay Power Savings
  Through Independent Coding' and 'Outage Analysis and Power Savings for
  Independent and Coherent Decode-Forward Relaying' presented at IEEE Globecom
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we re-examine the relay channel under the decode-forward (DF)
strategy. Contrary to the established belief that block Markov coding is always
the rate-optimal DF strategy, under certain channel conditions (a link regime),
independent signaling between the source and relay achieves the same
transmission rate without requiring coherent channel phase information.
Further, this independent signaling regime allows the relay to conserve power.
As such, we design a composite DF relaying strategy that achieves the same rate
as block Markov DF but with less required relay power. The finding is
attractive from the link adaptation perspective to adapt relay coding and relay
power according to the link state. We examine this link adaptation in fading
under both perfect channel state information (CSI) and practical CSI, in which
nodes have perfect receive and long-term transmit CSI, and derive the
corresponding relay power savings in both cases. We also derive the outage
probability of the composite relaying scheme which adapts the signaling to the
link regime. Through simulation, we expose a novel trade-off for relay
placement showing that the relay conserves the most power when closer to the
destination but achieves the most rate gain when closer to the source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05741</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05741</id><created>2015-12-17</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author><author><keyname>Bar-Ilan</keyname><forenames>Judit</forenames></author><author><keyname>Halevi</keyname><forenames>Gali</forenames></author></authors><title>Comparing source coverage, citation counts and speed of indexing in
  Google Scholar and Scopus</title><categories>cs.DL</categories><comments>Version 16 Dec 2015 submitted to Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An analysis of 1,200 target articles in 12 journals in 6 subject fields and
of 7,000 citations to 36 top cited articles found in virology and chemistry a
ratio of Google Scholar (GS) over Scopus citation counts between 0.8 and 1.5,
in Chinese studies between 1.8 and 2.8, in computational linguistics between 2
and 4, and in political science journals between 3 and 4. Open access journals
show higher ratios than their non-OA counterparts. Unique GS sources come from
Google Books and/or from large book publishers, and from large disciplinary and
institutional repositories. Unique Scopus sources are mainly books and Chinese
journals. There is a huge dispersion in GS source titles and web links. The
citation impact of documents in surplus sources covered in GS but not in Scopus
and vice versa is some 80 per cent lower than that of documents in sources
indexed in both. Pearson R between GS and Scopus citation counts at the article
level are in all 12 target journals above 0.8, and for 8 journals above 0.9.
Effect of double citation counts due to multiple citations with identical or
substantially similar meta data occurs in less than 2 per cent of cases. In GS,
the trade-off between data quality and indexing speed seems to be in favor of
the latter. A median Scopus indexing delay of two months compared to GS is
largely though not exclusively due to missing cited references in articles in
press. Pros and cons of article-based and concept-based citation indexes are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05742</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05742</id><created>2015-12-17</created><updated>2015-12-21</updated><authors><author><keyname>Serban</keyname><forenames>Iulian Vlad</forenames></author><author><keyname>Lowe</keyname><forenames>Ryan</forenames></author><author><keyname>Charlin</keyname><forenames>Laurent</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>A Survey of Available Corpora for Building Data-Driven Dialogue Systems</title><categories>cs.CL cs.AI cs.HC cs.LG stat.ML</categories><comments>46 pages including references, 5 tables and 1 figure; Under review
  for the Dialogue &amp; Discourse journal. Update: includes Facebook's 'Movie
  Dialog Dataset'</comments><msc-class>68T01, 68T05, 68T35, 68T50</msc-class><acm-class>I.2.6; I.2.7; I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the past decade, several areas of speech and language understanding
have witnessed substantial breakthroughs from the use of data-driven models. In
the area of dialogue systems, the trend is less obvious, and most practical
systems are still built through significant engineering and expert knowledge.
Nevertheless, several recent results suggest that data-driven approaches are
feasible and quite promising. To facilitate research in this area, we have
carried out a wide survey of publicly available datasets suitable for
data-driven learning of dialogue systems. We discuss important characteristics
of these datasets and how they can be used to learn diverse dialogue
strategies. We also describe other potential uses of these datasets, such as
methods for transfer learning between datasets and the use of external
knowledge, and discuss appropriate choice of evaluation metrics for the
learning objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05808</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05808</id><created>2015-12-17</created><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Zhao</keyname><forenames>Zheng</forenames></author><author><keyname>Zhang</keyname><forenames>Ruiwen</forenames></author></authors><title>Successive Ray Refinement and Its Application to Coordinate Descent for
  LASSO</title><categories>cs.LG</categories><comments>26 pages, 6 figures, 6 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Coordinate descent is one of the most popular approaches for solving Lasso
and its extensions due to its simplicity and efficiency. When applying
coordinate descent to solving Lasso, we update one coordinate at a time while
fixing the remaining coordinates. Such an update, which is usually easy to
compute, greedily decreases the objective function value. In this paper, we aim
to improve its computational efficiency by reducing the number of coordinate
descent iterations. To this end, we propose a novel technique called Successive
Ray Refinement (SRR). SRR makes use of the following ray continuation property
on the successive iterations: for a particular coordinate, the value obtained
in the next iteration almost always lies on a ray that starts at its previous
iteration and passes through the current iteration. Motivated by this
ray-continuation property, we propose that coordinate descent be performed not
directly on the previous iteration but on a refined search point that has the
following properties: on one hand, it lies on a ray that starts at a history
solution and passes through the previous iteration, and on the other hand, it
achieves the minimum objective function value among all the points on the ray.
We propose two schemes for defining the search point and show that the refined
search point can be efficiently obtained. Empirical results for real and
synthetic data sets show that the proposed SRR can significantly reduce the
number of coordinate descent iterations, especially for small Lasso
regularization parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05811</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05811</id><created>2015-12-17</created><authors><author><keyname>Harandi</keyname><forenames>Negar M.</forenames></author><author><keyname>Aalto</keyname><forenames>Daniel</forenames></author><author><keyname>Hannukainen</keyname><forenames>Antti</forenames></author><author><keyname>Malinen</keyname><forenames>Jarmo</forenames></author><author><keyname>Fels</keyname><forenames>Sidney</forenames></author></authors><title>Spectral Study of the Vocal Tract in Vowel Synthesis: A Comparison
  between 1D and 3D Acoustic Analysis</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A state-of-the-art 1D acoustic synthesizer has been previously developed, and
coupled to speaker-specific biomechanical models of oropharynx in ArtiSynth. As
expected, the formant frequencies of the synthesized vowel sounds were shown to
be different from those of the recorded audio. Such discrepancy was
hypothesized to be due to the simplified geometry of the vocal tract model as
well as the one dimensional implementation of Navier-Stokes equations. In this
paper, we calculate Helmholtz resonances of our vocal tract geometries using 3D
finite element method (FEM), and compare them with the formant frequencies
obtained from the 1D method and audio. We hope such comparison helps with
clarifying the limitations of our current models and/or speech synthesizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05813</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05813</id><created>2015-12-17</created><authors><author><keyname>Cho</keyname><forenames>Kenta</forenames></author><author><keyname>Jacobs</keyname><forenames>Bart</forenames></author><author><keyname>Westerbaan</keyname><forenames>Bas</forenames></author><author><keyname>Westerbaan</keyname><forenames>Abraham</forenames></author></authors><title>An Introduction to Effectus Theory</title><categories>cs.LO quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effectus theory is a new branch of categorical logic that aims to capture the
essentials of quantum logic, with probabilistic and Boolean logic as special
cases. Predicates in effectus theory are not subobjects having a Heyting
algebra structure, like in topos theory, but `characteristic' functions,
forming effect algebras. Such effect algebras are algebraic models of
quantitative logic, in which double negation holds. Effects in quantum theory
and fuzzy predicates in probability theory form examples of effect algebras.
  This text is an account of the basics of effectus theory. It includes the
fundamental duality between states and effects, with the associated Born rule
for validity of an effect (predicate) in a particular state. A basic result
says that effectuses can be described equivalently in both `total' and
`partial' form. So-called `commutative' and `Boolean' effectuses are
distinguished, for probabilistic and classical models. It is shown how these
Boolean effectuses are essentially extensive categories. A large part of the
theory is devoted to the logical notions of comprehension and quotient, which
are described abstractly as right adjoint to truth, and as left adjoint to
falisity, respectively. It is illustrated how comprehension and quotients are
closely related to measurement. The paper closes with a section on
`non-commutative' effectus theory, where the appropriate formalisation is not
entirely clear yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05814</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05814</id><created>2015-12-17</created><authors><author><keyname>Sahin</keyname><forenames>Cem S.</forenames></author><author><keyname>Lychev</keyname><forenames>Robert</forenames></author><author><keyname>Wagner</keyname><forenames>Neal</forenames></author></authors><title>General Framework for Evaluating Password Complexity and Strength</title><categories>cs.CR</categories><comments>11 pages and 4 figures</comments><acm-class>D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although it is common for users to select bad passwords that can be easily
cracked by attackers, password-based authentication remains the most
widely-used method. To encourage users to select good passwords, enterprises
often enforce policies. Such policies have been proven to be ineffectual in
practice. Accurate assessment of a password's resistance to cracking attacks is
still an unsolved problem, and our work addresses this challenge. Although the
best way to determine how difficult it may be to crack a user-selected password
is to check its resistance to cracking attacks employed by attackers in the
wild, implementing such a strategy at an enterprise would be infeasible in
practice. We first formalize the concepts of password complexity and strength
with concrete definitions emphasizing their differences. Our framework captures
human biases and many known techniques attackers use to recover stolen
credentials in real life, such as brute-force attacks. Building on our
definitions, we develop a general framework for calculating password complexity
and strength that could be used in practice. Our approach is based on the key
insight that an attacker's success at cracking a password must be defined by
its available computational resources, time, function used to store that
password, as well as the topology that bounds that attacker's search space
based on that attacker's available inputs, transformations it can use to tweak
and explore its inputs, and the path of exploration which can be based on the
attacker's perceived probability of success. We also provide a general
framework for assessing the accuracy of password complexity and strength
estimators that can be used to compare other tools available in the wild.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05819</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05819</id><created>2015-12-17</created><authors><author><keyname>Daggitt</keyname><forenames>Matthew</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Shaw</keyname><forenames>Blake</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Tracking Urban Activity Growth Globally with Big Location Data</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent decades the world has experienced rates of urban growth
unparalleled in any other period of history and this growth is shaping the
environment in which an increasing proportion of us live. In this paper we use
a longitudinal dataset from Foursquare, a location-based social network, to
analyse urban growth across 100 major cities worldwide.
  Initially we explore how urban growth differs in cities across the world. We
show that there exists a strong spatial correlation, with nearby pairs of
cities more likely to share similar growth profiles than remote pairs of
cities. Subsequently we investigate how growth varies inside cities and
demonstrate that, given the existing local density of places,
higher-than-expected growth is highly localised while lower-than-expected
growth is more diffuse. Finally we attempt to use the dataset to characterise
competition between new and existing venues. By defining a measure based on the
change in throughput of a venue before and after the opening of a new nearby
venue, we demonstrate which venue types have a positive effect on venues of the
same type and which have a negative effect. For example, our analysis confirms
the hypothesis that there is large degree of competition between bookstores, in
the sense that existing bookstores normally experience a notable drop in
footfall after a new bookstore opens nearby. Other place categories however,
such as Airport Gates or Museums, have a cooperative effect and their presence
fosters higher traffic volumes to nearby places of the same type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05827</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05827</id><created>2015-12-17</created><authors><author><keyname>Swisher</keyname><forenames>Matthew</forenames></author></authors><title>HALO: Report and Predicted Response Times</title><categories>cs.DC</categories><comments>3 pages, 2 charts, Report on HALO: Heterogeneity-Aware Load Balancing
  for independent study at O.S.U</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HALO: Heterogeneity-Aware Load Balancing is a paper that proposes a class of
heterogeneity-aware Load Balancers (LBs) for cluster systems. LBs that are
heterogeneity-aware are able to detect when servers differ in speeds and in
number of cores. Response times for heterogeneous systems are calculated and
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05830</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05830</id><created>2015-12-17</created><authors><author><keyname>Shen</keyname><forenames>Li</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Huang</keyname><forenames>Qingming</forenames></author></authors><title>Learning Deep Convolutional Neural Networks for Places2 Scene
  Recognition</title><categories>cs.CV cs.LG</categories><comments>The brief technical report for our submissions to the ILSVRC 2015
  Scene Classification Challenge, where we won the first place. The updated
  version with more details is coming soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the optimization algorithm and methodologies we used in the
ILSVRC 2015 Scene Classification Challenge, which we won the first place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05832</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05832</id><created>2015-12-17</created><authors><author><keyname>Evans</keyname><forenames>Owain</forenames></author><author><keyname>Stuhlmueller</keyname><forenames>Andreas</forenames></author><author><keyname>Goodman</keyname><forenames>Noah D.</forenames></author></authors><title>Learning the Preferences of Ignorant, Inconsistent Agents</title><categories>cs.AI</categories><comments>AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important use of machine learning is to learn what people value. What
posts or photos should a user be shown? Which jobs or activities would a person
find rewarding? In each case, observations of people's past choices can inform
our inferences about their likes and preferences. If we assume that choices are
approximately optimal according to some utility function, we can treat
preference inference as Bayesian inverse planning. That is, given a prior on
utility functions and some observed choices, we invert an optimal
decision-making process to infer a posterior distribution on utility functions.
However, people often deviate from approximate optimality. They have false
beliefs, their planning is sub-optimal, and their choices may be temporally
inconsistent due to hyperbolic discounting and other biases. We demonstrate how
to incorporate these deviations into algorithms for preference inference by
constructing generative models of planning for agents who are subject to false
beliefs and time inconsistency. We explore the inferences these models make
about preferences, beliefs, and biases. We present a behavioral experiment in
which human subjects perform preference inference given the same observations
of choices as our model. Results show that human subjects (like our model)
explain choices in terms of systematic deviations from optimal behavior and
suggest that they take such deviations into account when inferring preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05839</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05839</id><created>2015-12-17</created><updated>2016-01-17</updated><authors><author><keyname>Chiribella</keyname><forenames>Giulio</forenames></author><author><keyname>Yang</keyname><forenames>Yuxiang</forenames></author></authors><title>Quantum superreplication of states and gates</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the no-cloning theorem forbids the perfect replication of quantum
information, it is sometimes possible to produce large numbers of replicas with
vanishingly small error. This phenomenon, known as quantum superreplication,
can take place both for quantum states and quantum gates. The aim of this paper
is to review the central features of quantum superreplication, providing a
unified view on the existing results. The paper also includes new results. In
particular, we show that, when quantum superreplication can be achieved, it can
be achieved through estimation, up to an error vanishing with a power law.
Quantum strategies still offer an advantage for superreplication, in that they
allow for an exponentially faster reduction of the error. Using the relation
with estimation, we provide i) an alternative proof of the optimality of the
Heisenberg scaling of quantum metrology, ii) a strategy to estimate arbitrary
unitary gates with Heisenberg scaling, up to a logarithmic overhead, and iii) a
protocol that generates M nearly perfect copies of a generic pure state with a
number of queries to the corresponding unitary gate scaling as the square root
of M. Finally, we point out that superreplication can be achieved using
interactions among k systems, provided that k is large compared to square of
the ratio between the numbers of input and output copies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05840</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05840</id><created>2015-12-17</created><authors><author><keyname>Kung</keyname><forenames>Pau Perng-Hwa</forenames></author></authors><title>Deep Poisson Factorization Machines: factor analysis for mapping
  behaviors in journalist ecosystem</title><categories>cs.CY cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Newsroom in online ecosystem is difficult to untangle. With prevalence of
social media, interactions between journalists and individuals become visible,
but lack of understanding to inner processing of information feedback loop in
public sphere leave most journalists baffled. Can we provide an organized view
to characterize journalist behaviors on individual level to know better of the
ecosystem? To this end, I propose Poisson Factorization Machine (PFM), a
Bayesian analogue to matrix factorization that assumes Poisson distribution for
generative process. The model generalizes recent studies on Poisson Matrix
Factorization to account temporal interaction which involves tensor-like
structure, and label information. Two inference procedures are designed, one
based on batch variational EM and another stochastic variational inference
scheme that efficiently scales with data size. An important novelty in this
note is that I show how to stack layers of PFM to introduce a deep
architecture. This work discusses some potential results applying the model and
explains how such latent factors may be useful for analyzing latent behaviors
for data exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05841</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05841</id><created>2015-12-17</created><authors><author><keyname>Strey</keyname><forenames>Eleonesio</forenames></author><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author></authors><title>Lattices from codes over $\mathbb{Z}_n$: Generalization of Constructions
  $D$, $D'$ and $\overline{D}$</title><categories>cs.IT math.IT</categories><comments>Submitted to Designs, Codes and Cryptography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the lattice Constructions $D$, $D'$ and
$\overline{D}$ $($this latter is also known as Forney's code formula$)$ from
codes over $\mathbb{F}_p$ to linear codes over $\mathbb{Z}_q$, where $q \in
\mathbb{N}$. We define an operation in $\mathbb{Z}_q^n$ called zero-one
addition, which coincides with the Schur product when restricted to
$\mathbb{Z}_2^n$ and show that the extended Construction $\overline{D}$
produces a lattice if and only if the nested codes are closed under this
addition. A generalization to the real case of the recently developed
Construction $A'$ is also derived and we show that this construction produces a
lattice if and only if the corresponding code over $\mathbb{Z}_q[X]/X^a$ is
closed under a shifted zero-one addition. One of the motivations for this work
is the recent use of $q$-ary lattices in cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05844</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05844</id><created>2015-12-17</created><authors><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Siva</keyname><forenames>Parthipan</forenames></author><author><keyname>Fieguth</keyname><forenames>Paul</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Domain Adaptation and Transfer Learning in StochasticNets</title><categories>cs.CV stat.ML</categories><journal-ref>Vision Letters, Vol. 1, No. 1, pp. VL115, 2015</journal-ref><doi>10.15353/vsnl.v1i1.44</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transfer learning is a recent field of machine learning research that aims to
resolve the challenge of dealing with insufficient training data in the domain
of interest. This is a particular issue with traditional deep neural networks
where a large amount of training data is needed. Recently, StochasticNets was
proposed to take advantage of sparse connectivity in order to decrease the
number of parameters that needs to be learned, which in turn may relax training
data size requirements. In this paper, we study the efficacy of transfer
learning on StochasticNet frameworks. Experimental results show ~7% improvement
on StochasticNet performance when the transfer learning is applied in training
step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05849</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05849</id><created>2015-12-17</created><authors><author><keyname>Brundage</keyname><forenames>Miles</forenames></author></authors><title>Modeling Progress in AI</title><categories>cs.AI</categories><comments>AAAI 2016 Workshop on AI, Ethics, and Society</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Participants in recent discussions of AI-related issues ranging from
intelligence explosion to technological unemployment have made diverse claims
about the nature, pace, and drivers of progress in AI. However, these theories
are rarely specified in enough detail to enable systematic evaluation of their
assumptions or to extrapolate progress quantitatively, as is often done with
some success in other technological domains. After reviewing relevant
literatures and justifying the need for more rigorous modeling of AI progress,
this paper contributes to that research program by suggesting ways to account
for the relationship between hardware speed increases and algorithmic
improvements in AI, the role of human inputs in enabling AI capabilities, and
the relationships between different sub-fields of AI. It then outlines ways of
tailoring AI progress models to generate insights on the specific issue of
technological unemployment, and outlines future directions for research on AI
progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05857</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05857</id><created>2015-12-18</created><authors><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Transmission and Scheduling Aspects of Distributed Storage and Their
  Connections with Index Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Index coding is often studied with the assumption that a single source has
all the messages requested by the receivers. We refer to this as
\emph{centralized} index coding. In contrast, this paper focuses on
\emph{distributed} index coding and addresses the following question: How does
the availability of messages at distributed sources (storage nodes) affect the
solutions and achievable rates of index coding? An extension to the work of
Arbabjolfaei et al. in ISIT 2013 is presented when distributed sources
communicate via a semi-deterministic multiple access channel (MAC) to
simultaneous receivers. A numbers of examples are discussed that show the
effect of message distribution and redundancy across the network on achievable
rates of index coding and motivate future research on designing practical
network storage codes that offer high index coding rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05864</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05864</id><created>2015-12-18</created><updated>2016-01-04</updated><authors><author><keyname>Atighehchi</keyname><forenames>Kevin</forenames></author><author><keyname>Rolland</keyname><forenames>Robert</forenames></author></authors><title>Optimization of Tree Modes for Parallel Hash Functions</title><categories>cs.DC cs.CR</categories><comments>Typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on parallel hash functions based on tree modes of
operation for a compression function. We discuss the various forms of
optimality that can be obtained when designing such parallel hash functions.
The first result is a scheme which optimizes the tree topology in order to
decrease at best the running time. Then, without affecting the optimal running
time we show that we can slightly change the corresponding tree topology so as
to decrease at best the number of required processors as well. Consequently,
the resulting scheme optimizes in the first place the running time and in the
second place the number of required processors. The present work is of
independent interest if we consider the problem of parallelizing the evaluation
of an expression where the operator used is neither associative nor
commutative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05868</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05868</id><created>2015-12-18</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Golomb</keyname><forenames>Iddan</forenames></author></authors><title>On Voting and Facility Location</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study mechanisms for candidate selection that seek to minimize the social
cost, where voters and candidates are associated with points in some underlying
metric space. The social cost of a candidate is the sum of its distances to
each voter. Some of our work assumes that these points can be modeled on a real
line, but other results of ours are more general.
  A question closely related to candidate selection is that of minimizing the
sum of distances for facility location. The difference is that in our setting
there is a fixed set of candidates, whereas the large body of work on facility
location seems to consider every point in the metric space to be a possible
candidate. This gives rise to three types of mechanisms which differ in the
granularity of their input space (voting, ranking and location mechanisms). We
study the relationships between these three classes of mechanisms.
  While it may seem that Black's 1948 median algorithm is optimal for candidate
selection on the line, this is not the case. We give matching upper and lower
bounds for a variety of settings. In particular, when candidates and voters are
on the line, our universally truthful spike mechanism gives a [tight]
approximation of two. When assessing candidate selection mechanisms, we seek
several desirable properties: (a) efficiency (minimizing the social cost) (b)
truthfulness (dominant strategy incentive compatibility) and (c) simplicity (a
smaller input space). We quantify the effect that truthfulness and simplicity
impose on the efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05875</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05875</id><created>2015-12-18</created><updated>2016-01-06</updated><authors><author><keyname>Fontana</keyname><forenames>Alessandro</forenames></author></authors><title>The Quadripolar Relational Model: an Artificial Intelligence framework
  for the description of personality disorders</title><categories>q-bio.NC cs.AI</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The borderline and narcissistic personality disorders are important
nosographic entities and have been subject of intensive investigation. The
currently prevailing psychodynamic theory for mental disorders is based on the
repertoire of defense mechanisms employed. Another fruitful line of research is
concerned with the study of psychological traumas and with dissociation as a
defensive response. Both theories can be used to shed light on some aspects of
pathological mental functioning, and have many points of contact. This work
starts from a model of emotional behaviour, inspired by an approach derived
from the field of artificial intelligence, and based on neural networks. The
model is then used to overcome the dichotomy between the two aforementioned
psychological theories, and conceive a common framework for the description of
personality disorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05876</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05876</id><created>2015-12-18</created><authors><author><keyname>Kobayashi</keyname><forenames>Yasuaki</forenames></author><author><keyname>Tamaki</keyname><forenames>Hisao</forenames></author></authors><title>A faster fixed parameter algorithm for two-layer crossing minimization</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm that decides whether the bipartite crossing number of a
given graph is at most $k$. The running time of the algorithm is upper bounded
by $2^{O(k)} + n^{O(1)}$, where $n$ is the number of vertices of the input
graph, which improves the previously known algorithm due to Kobayashi et al.
(TCS 2014) that runs in $2^{O(k \log k)} + n^{O(1)}$ time. This result is based
on a combinatorial upper bound on the number of two-layer drawings of a
connected bipartite graph with a bounded crossing number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05881</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05881</id><created>2015-12-18</created><authors><author><keyname>H&#xe9;am</keyname><forenames>Pierre-Cyrille</forenames><affiliation>FEMTO-ST, CASSIS</affiliation></author><author><keyname>Joly</keyname><forenames>Jean-Luc</forenames><affiliation>FEMTO-ST, CASSIS</affiliation></author></authors><title>Random Generation and Enumeration of Accessible Determinisitic Real-time
  Pushdown Automata</title><categories>cs.FL</categories><comments>Frank Drewes. CIAA 2015, Aug 2015, Umea, Sweden. Springer, 9223,
  pp.12, 2015, Implementation and Application of Automata - 20th International
  Conference</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This papers presents a general framework for the uniform random generation of
deterministic real-time accessible pushdown au-tomata. A polynomial time
algorithm to randomly generate a pushdown automaton having a fixed stack
operations total size is proposed. The influence of the accepting condition
(empty stack, final state) on the reachability of the generated automata is
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05882</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05882</id><created>2015-12-18</created><authors><author><keyname>Merezeanu</keyname><forenames>Daniel Marian</forenames></author><author><keyname>Andone</keyname><forenames>Daniela</forenames></author></authors><title>Tandem Queueing Systems Maximum Throughput Problem</title><categories>cs.PF</categories><journal-ref>SINTES 10, Proc. of SINTES 10, Craiova, Romania, pp. 120-123 May
  25-26, 2000</journal-ref><doi>10.13140/RG.2.1.2601.7362</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of maximum throughput for tandem
queueing system. We modeled this system as a Quasi-Birth-Death process. In
order to do this we named level the number of customers waiting in the first
buffer (including the customer in service) and we called phase the state of the
remining servers. Using this model we studied the problem of maximum throughput
of the system: the maximum arrival rate that a given system could support
before becoming saturated, or unstable. We considered different particular
cases of such systems, which were obtained by modifying the capacity of the
intermediary buffers, the arrival rate and the service rates. The results of
the simulations are presented in our paper and can be summed up in the
following conclusions: 1. The analytic formula for the maximum throughput of
the system tends to become rather complicated when the number of servers
increase 2. The maximum throughput of the system converges as the number of
servers increases 3. The homogeneous case reveals an interesting
characteristic: if we reverse the order of the servers, maximum thruoughput of
the system remains unchanged The QBD process used for the case of Poisson
arrivals can be extended to model more general arrival processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05904</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05904</id><created>2015-12-18</created><authors><author><keyname>Veetil</keyname><forenames>Sreejith T.</forenames></author><author><keyname>Kuchi</keyname><forenames>Kiran</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author></authors><title>Performance of Cloud Radio Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud radio networks coordinate transmission among base stations (BSs) to
reduce the interference effects, particularly for the cell-edge users. In this
paper, we analyze the performance of a cloud network with static clustering
where geographically close BSs form a cloud network of cooperating BSs.
Because, of finite cooperation, the interference in a practical cloud radio
cannot be removed and in this paper, the distance based interference is taken
into account in the analysis. In particular, we consider centralized zero
forcing equalizer and dirty paper precoding for cancelling the interference.
Bounds are developed on the signal-to-interference ratio distribution and
achievable rate with full and limited channel feedback from the cluster users.
The adverse effect of finite clusters on the achievable rate is quantified. We
show that, the number of cooperating BSs is more crucial than the cluster area
when full channel state information form the cluster is available for
precoding. Also, we study the impact of limiting the channel state information
on the achievable rate. We show that even with a practically feasible feedback
of about five to six channel states from each user, significant gain in mean
rate and cell edge rate compared to conventional cellular systems can be
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05915</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05915</id><created>2015-12-18</created><authors><author><keyname>Wang</keyname><forenames>Lifeng</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Heath,</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author></authors><title>Millimeter Wave Power Transfer and Information Transmission</title><categories>cs.IT math.IT</categories><comments>6 pages, Accepted in Proc. IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to the existing lower frequency wireless power transfer, millimeter
wave (mmWave) power transfer takes advantage of the high-dimensional
multi-antenna and narrow beam transmission. In this paper we introduce wireless
power transfer for mmWave cellular networks. Here, we consider users with large
energy storage that are recharged by the mmWave base stations prior to uplink
information transmission, and analyze the average harvested energy and average
achievable rate. Numerical results corroborate our analysis and show that the
serving base station plays a dominant role in wireless power transfer, and the
contribution of the interference power from the interfering base stations is
negligible, even when the interfering base stations are dense. By examining the
average achievable rate in the uplink, when increasing the base station
density, a transition from a noise-limited regime to an interference-limited
regime is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05919</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05919</id><created>2015-12-18</created><updated>2016-01-06</updated><authors><author><keyname>Qin</keyname><forenames>Bing</forenames></author><author><keyname>Tang</keyname><forenames>Duyu</forenames></author><author><keyname>Geng</keyname><forenames>Xinwei</forenames></author><author><keyname>Ning</keyname><forenames>Dandan</forenames></author><author><keyname>Liu</keyname><forenames>Jiahao</forenames></author><author><keyname>Liu</keyname><forenames>Ting</forenames></author></authors><title>A Planning based Framework for Essay Generation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating an article automatically with computer program is a challenging
task in artificial intelligence and natural language processing. In this paper,
we target at essay generation, which takes as input a topic word in mind and
generates an organized article under the theme of the topic. We follow the idea
of text planning \cite{Reiter1997} and develop an essay generation framework.
The framework consists of three components, including topic understanding,
sentence extraction and sentence reordering. For each component, we studied
several statistical algorithms and empirically compared between them in terms
of qualitative or quantitative analysis. Although we run experiments on Chinese
corpus, the method is language independent and can be easily adapted to other
language. We lay out the remaining challenges and suggest avenues for future
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05944</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05944</id><created>2015-12-18</created><authors><author><keyname>Brinkmann</keyname><forenames>Gunnar</forenames></author><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author></authors><title>Generation of cubic graphs and snarks with large girth</title><categories>math.CO cs.DM</categories><comments>27 pages (including appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two new algorithms for the generation of all non-isomorphic cubic
graphs with girth at least $k\ge 5$ which are very efficient for $5\le k \le 7$
and show how these algorithms can be efficiently restricted to generate snarks
with girth at least $k$.
  Our implementation of these algorithms is more than 30, respectively 40 times
faster than the previously fastest generator for cubic graphs with girth at
least 6 and 7, respectively.
  Using these generators we have also generated all non-isomorphic snarks with
girth at least 6 up to 38 vertices and show that there are no snarks with girth
at least 7 up to 42 vertices. We present and analyse the new list of snarks
with girth 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05947</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05947</id><created>2015-12-18</created><authors><author><keyname>Bl&#xf6;mer</keyname><forenames>Johannes</forenames></author><author><keyname>Brauer</keyname><forenames>Sascha</forenames></author><author><keyname>Bujna</keyname><forenames>Kathrin</forenames></author></authors><title>Complexity and Approximation of the Fuzzy K-Means Problem</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fuzzy $K$-means problem is a generalization of the classical $K$-means
problem to soft clusterings, i.e. clusterings where each points belongs to each
cluster to some degree. Although popular in practice, prior to this work the
fuzzy $K$-means problem has not been studied from a complexity theoretic or
algorithmic perspective. We show that optimal solutions for fuzzy $K$-means
cannot, in general, be expressed by radicals over the input points.
Surprisingly, this already holds for very simple inputs in one-dimensional
space. Hence, one cannot expect to compute optimal solutions exactly. We give
the first $(1+\epsilon)$-approximation algorithms for the fuzzy $K$-means
problem. First, we present a deterministic approximation algorithm whose
runtime is polynomial in $N$ and linear in the dimension $D$ of the input set,
given that $K$ is constant, i.e. a polynomial time approximation algorithm
given a fixed $K$. We achieve this result by showing that for each soft
clustering there exists a hard clustering with comparable properties. Second,
by using techniques known from coreset constructions for the $K$-means problem,
we develop a deterministic approximation algorithm that runs in time almost
linear in $N$ but exponential in the dimension $D$. We complement these results
with a randomized algorithm which imposes some natural restrictions on the
input set and whose runtime is comparable to some of the most efficient
approximation algorithms for $K$-means, i.e. linear in the number of points and
the dimension, but exponential in the number of clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05948</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05948</id><created>2015-12-18</created><authors><author><keyname>Bartholdi</keyname><forenames>Laurent</forenames></author><author><keyname>Dudko</keyname><forenames>Dzmitry</forenames></author></authors><title>Algorithmic aspects of branched coverings</title><categories>cs.CC math.DS math.GR</categories><comments>55-page announcement of 5-part text, to follow</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the announcement, and the long summary, of a series of articles on
the algorithmic study of Thurston maps. We describe branched coverings of the
sphere in terms of group-theoretical objects called bisets, and develop a
theory of decompositions of bisets.
  We introduce a canonical &quot;Levy&quot; decomposition of an arbitrary Thurston map
into homeomorphisms, metrically-expanding maps and maps doubly covered by torus
endomorphisms. The homeomorphisms decompose themselves into finite-order and
pseudo-Anosov maps, and the expanding maps decompose themselves into rational
maps.
  As an outcome, we prove that it is decidable when two Thurston maps are
equivalent. We also show that the decompositions above are computable, both in
theory and in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05949</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05949</id><created>2015-12-18</created><authors><author><keyname>Jungnickel</keyname><forenames>Tim</forenames></author><author><keyname>Herb</keyname><forenames>Tobias</forenames></author></authors><title>TP1-valid Transformation Functions for Operations on ordered n-ary Trees</title><categories>cs.LO cs.HC</categories><comments>Extension/Report for the work &quot;Simultaneous Editing of JSON Objects
  via Operational Transformation&quot; in ACM SAC '16</comments><doi>10.1145/2851613.2852003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative work on shared documents was revolutionized by web services
like Google Docs or Etherpad. Multiple users can work on the same document in a
comfortable and distributed way. For the synchronization of the changes a
replication system named Operational Transformation is used. Such a system
consists of a control algorithm and a transformation function. In essence, a
transformation function solves the conflicts that arise when multiple users
change the document at the same time. In this work we investigate on the
correctness of such transformation functions. We introduce transformation
functions n-ary trees that we designed especially for the purpose of
synchronization changes on JSON objects. We provide a detailed proof of the
necessary property: the Transformation Property 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05974</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05974</id><created>2015-12-18</created><authors><author><keyname>Darwish</keyname><forenames>Omar</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author></authors><title>Improved Balanced Flow Computation Using Parametric Flow</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for computing balanced flows in equality networks
arising in market equilibrium computations. The current best time bound for
computing balanced flows in such networks requires $O(n)$ maxflow computations,
where $n$ is the number of nodes in the network [Devanur et al. 2008]. Our
algorithm requires only a single parametric flow computation. The best
algorithm for computing parametric flows [Gallo et al. 1989] is only by a
logarithmic factor slower than the best algorithms for computing maxflows.
Hence, the running time of the algorithms in [Devanur et al. 2008] and [Duan
and Mehlhorn 2015] for computing market equilibria in linear Fisher and
Arrow-Debreu markets improve by almost a factor of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05979</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05979</id><created>2015-12-18</created><authors><author><keyname>Bansal</keyname><forenames>Anshul</forenames></author><author><keyname>Rompikuntla</keyname><forenames>Susheel Kaushik</forenames></author><author><keyname>Gopinadhan</keyname><forenames>Jaganadh</forenames></author><author><keyname>Kaur</keyname><forenames>Amanpreet</forenames></author><author><keyname>Kazi</keyname><forenames>Zahoor Ahamed</forenames></author></authors><title>Energy Consumption Forecasting for Smart Meters</title><categories>cs.OH</categories><comments>Presented at BAI Conference 2015 at IIM Bangalore, India</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Earth, water, air, food, shelter and energy are essential factors required
for human being to survive on the planet. Among this energy plays a key role in
our day to day living including giving lighting, cooling and heating of
shelter, preparation of food. Due to this interdependency, energy, specifically
electricity, production and distribution became a high tech industry. Unlike
other industries, the key differentiator of electricity industry is the product
itself. It can be produced but cannot be stored for future; production and
consumption happen almost in near real-time. This particular peculiarity of the
industry is the key driver for Machine Learning and Data Science based
innovations in this industry. There is always a gap between the demand and
supply in the electricity market across the globe. To fill the gap and improve
the service efficiency through providing necessary supply to the market,
commercial as well as federal electricity companies employ forecasting
techniques to predict the future demand and try to meet the demand and provide
curtailment guidelines to optimise the electricity consumption/demand. In this
paper the authors examine the application of Machine Learning algorithms,
specifically Boosted Decision Tree Regression, to the modelling and forecasting
of energy consumption for smart meters. The data used for this exercise is
obtained from DECC data website. Along with this data, the methodology has been
tested in Smart Meter data obtained from EMA Singapore. This paper focuses on
feature engineering for time series forecasting using regression algorithms and
deriving a methodology to create personalised electricity plans offers for
household users based on usage history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05980</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05980</id><created>2015-12-18</created><updated>2015-12-21</updated><authors><author><keyname>Garner</keyname><forenames>Richard</forenames><affiliation>LAMA</affiliation></author><author><keyname>Hirschowitz</keyname><forenames>Tom</forenames><affiliation>LAMA</affiliation></author></authors><title>Shapely monads and analytic functors</title><categories>cs.LO math.AT math.CT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give precise mathematical form to the idea of a structure
whose data and axioms are faithfully represented by a graphical calculus; some
prominent examples are operads, polycategories, properads, and PROPs. Building
on the established presentation of such structures as algebras for monads on
presheaf categories, we describe a characteristic property of the associated
monads---the \emph{shapeliness} of the title---which says that &quot;any two
operations of the same shape agree&quot;. An important part of this work is the
study of \emph{analytic} functors between presheaf categories, which are a
common generalisation of Joyal's analytic endofunctors on sets and of the
parametric right adjoint functors on presheaf categories introduced by Diers
and studied by Carboni--Johnstone, Leinster and Weber. Our shapely monads will
be found among the analytic endofunctors, and may be characterised as the
submonads of a \emph{universal} analytic monad with &quot;exactly one operation of
each shape&quot;. In fact, shapeliness also gives a way to \emph{define} the data
and axioms of a structure directly from its graphical calculus, by generating a
free shapely monad on the basic operations of the calculus. In this paper we do
this for some of the examples listed above; in future work, we intend to do so
for graphical calculi such as Milner's bigraphs, Lafont's interaction nets, or
Girard's multiplicative proof nets, thereby obtaining canonical notions of
denotational model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05986</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05986</id><created>2015-12-18</created><authors><author><keyname>Menkovski</keyname><forenames>Vlado</forenames></author><author><keyname>Aleksovski</keyname><forenames>Zharko</forenames></author><author><keyname>Saalbach</keyname><forenames>Axel</forenames></author><author><keyname>Nickisch</keyname><forenames>Hannes</forenames></author></authors><title>Can Pretrained Neural Networks Detect Anatomy?</title><categories>cs.CV cs.AI cs.NE</categories><comments>NIPS 2015 Workshop on Machine Learning in Healthcare</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks demonstrated outstanding empirical results in
computer vision and speech recognition tasks where labeled training data is
abundant. In medical imaging, there is a huge variety of possible imaging
modalities and contrasts, where annotated data is usually very scarce. We
present two approaches to deal with this challenge. A network pretrained in a
different domain with abundant data is used as a feature extractor, while a
subsequent classifier is trained on a small target dataset; and a deep
architecture trained with heavy augmentation and equipped with sophisticated
regularization methods. We test the approaches on a corpus of X-ray images to
design an anatomy detection system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05990</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05990</id><created>2015-12-18</created><authors><author><keyname>Ma</keyname><forenames>Andy J</forenames></author><author><keyname>Yuen</keyname><forenames>Pong C</forenames></author><author><keyname>Saria</keyname><forenames>Suchi</forenames></author></authors><title>Deformable Distributed Multiple Detector Fusion for Multi-Person
  Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses fully automated multi-person tracking in complex
environments with challenging occlusion and extensive pose variations. Our
solution combines multiple detectors for a set of different regions of interest
(e.g., full-body and head) for multi-person tracking. The use of multiple
detectors leads to fewer miss detections as it is able to exploit the
complementary strengths of the individual detectors. While the number of false
positives may increase with the increased number of bounding boxes detected
from multiple detectors, we propose to group the detection outputs by bounding
box location and depth information. For robustness to significant pose
variations, deformable spatial relationship between detectors are learnt in our
multi-person tracking system. On RGBD data from a live Intensive Care Unit
(ICU), we show that the proposed method significantly improves multi-person
tracking performance over state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05996</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05996</id><created>2015-12-18</created><authors><author><keyname>Komm</keyname><forenames>Dennis</forenames></author><author><keyname>Kr&#xe1;lovi&#x10d;</keyname><forenames>Rastislav</forenames></author><author><keyname>Kr&#xe1;lovi&#x10d;</keyname><forenames>Richard</forenames></author><author><keyname>Kudahl</keyname><forenames>Christian</forenames></author></authors><title>Advice Complexity of the Online Induced Subgraph Problem</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several well-studied graph problems aim to select a largest (or smallest)
induced subgraph with a given property of the input graph. Examples of such
problems include maximum independent set, maximum planar graph, and many
others. We consider these problems, where the vertices are presented online.
With each vertex, the online algorithm must decide whether to include it into
the constructed subgraph, based only on the subgraph induced by the vertices
presented so far. We study the properties that are common to all these problems
by investigating the generalized problem: for a hereditary property \pty, find
some maximal induced subgraph having \pty. We study this problem from the point
of view of advice complexity. Using a result from Boyar et al. [STACS 2015], we
give a tight trade-off relationship stating that for inputs of length n roughly
n/c bits of advice are both needed and sufficient to obtain a solution with
competitive ratio c, regardless of the choice of \pty, for any c (possibly a
function of n). Surprisingly, a similar result cannot be obtained for the
symmetric problem: for a given cohereditary property \pty, find a minimum
subgraph having \pty. We show that the advice complexity of this problem varies
significantly with the choice of \pty.
  We also consider preemptive online model, where the decision of the algorithm
is not completely irreversible. In particular, the algorithm may discard some
vertices previously assigned to the constructed set, but discarded vertices
cannot be reinserted into the set again. We show that, for the maximum induced
subgraph problem, preemption cannot help much, giving a lower bound of
$\Omega(n/(c^2\log c))$ bits of advice needed to obtain competitive ratio $c$,
where $c$ is any increasing function bounded by \sqrt{n/log n}. We also give a
linear lower bound for c close to 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06000</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06000</id><created>2015-12-18</created><authors><author><keyname>D'Acquisto</keyname><forenames>Giuseppe</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Kikiras</keyname><forenames>Panayiotis</forenames></author><author><keyname>Torra</keyname><forenames>Vicen&#xe7;</forenames></author><author><keyname>de Montjoye</keyname><forenames>Yves-Alexandre</forenames></author><author><keyname>Bourka</keyname><forenames>Athena</forenames></author></authors><title>Privacy by design in big data: An overview of privacy enhancing
  technologies in the era of big data analytics</title><categories>cs.CR</categories><comments>80 pages. European Union Agency for Network and Information Security
  (ENISA) report, December 2015, ISBN 978-92-9204-160-1.
  https://www.enisa.europa.eu/activities/identity-and-trust/library/deliverables/big-data-protection/</comments><msc-class>94A60</msc-class><acm-class>K.4.1; D.4.6; H.2.0</acm-class><doi>10.2824/641480</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extensive collection and processing of personal information in big data
analytics has given rise to serious privacy concerns, related to wide scale
electronic surveillance, profiling, and disclosure of private data. To reap the
benefits of analytics without invading the individuals' private sphere, it is
essential to draw the limits of big data processing and integrate data
protection safeguards in the analytics value chain. ENISA, with the current
report, supports this approach and the position that the challenges of
technology (for big data) should be addressed by the opportunities of
technology (for privacy).
  We first explain the need to shift from &quot;big data versus privacy&quot; to &quot;big
data with privacy&quot;. In this respect, the concept of privacy by design is key to
identify the privacy requirements early in the big data analytics value chain
and in subsequently implementing the necessary technical and organizational
measures.
  After an analysis of the proposed privacy by design strategies in the
different phases of the big data value chain, we review privacy enhancing
technologies of special interest for the current and future big data landscape.
In particular, we discuss anonymization, the &quot;traditional&quot; analytics technique,
the emerging area of encrypted search and privacy preserving computations,
granular access control mechanisms, policy enforcement and accountability, as
well as data provenance issues. Moreover, new transparency and access tools in
big data are explored, together with techniques for user empowerment and
control.
  Achieving &quot;big data with privacy&quot; is no easy task and a lot of research and
implementation is still needed. Yet, it remains a possible task, as long as all
the involved stakeholders take the necessary steps to integrate privacy and
data protection safeguards in the heart of big data, by design and by default.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06009</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06009</id><created>2015-12-18</created><authors><author><keyname>Farrugia</keyname><forenames>Reuben</forenames></author><author><keyname>Guillemot</keyname><forenames>Christine</forenames></author></authors><title>Face Hallucination using Linear Models of Coupled Sparse Support</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most face super-resolution methods assume that low-resolution and
high-resolution manifolds have similar local geometrical structure, hence learn
local models on the lowresolution manifolds (e.g. sparse or locally linear
embedding models), which are then applied on the high-resolution manifold.
However, the low-resolution manifold is distorted by the oneto-many
relationship between low- and high- resolution patches. This paper presents a
method which learns linear models based on the local geometrical structure on
the high-resolution manifold rather than on the low-resolution manifold. For
this, in a first step, the low-resolution patch is used to derive a globally
optimal estimate of the high-resolution patch. The approximated solution is
shown to be close in Euclidean space to the ground-truth but is generally
smooth and lacks the texture details needed by state-ofthe-art face
recognizers. This first estimate allows us to find the support of the
high-resolution manifold using sparse coding (SC), which are then used as
support for learning a local projection (or upscaling) model between the
low-resolution and the highresolution manifolds using Multivariate Ridge
Regression (MRR). Experimental results show that the proposed method
outperforms six face super-resolution methods in terms of both recognition and
quality. These results also reveal that the recognition and quality are
significantly affected by the method used for stitching all super-resolved
patches together, where quilting was found to better preserve the texture
details which helps to achieve higher recognition rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06014</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06014</id><created>2015-12-18</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Nandan</keyname><forenames>Sanket</forenames></author><author><keyname>Kurmi</keyname><forenames>Indrajit</forenames></author></authors><title>Multiclass Classification of Cervical Cancer Tissues by Hidden Markov
  Model</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we report a hidden Markov model based multiclass
classification of cervical cancer tissues. This model has been validated
directly over time series generated by the medium refractive index fluctuations
extracted from differential interference contrast images of healthy and
different stages of cancer tissues. The method shows promising results for
multiclass classification with higher accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06017</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06017</id><created>2015-12-18</created><authors><author><keyname>Zubov</keyname><forenames>Dmytro</forenames></author></authors><title>Cloud Computation and Google Earth Visualization of Heat/Cold Waves: A
  Nonanticipative Long-Range Forecasting Case Study</title><categories>cs.OH</categories><comments>10 pages, 2 figures, 4 tables, 30 references. arXiv admin note: text
  overlap with arXiv:1507.03283</comments><msc-class>68U35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long-range forecasting of heat/cold waves is a topical issue nowadays. High
computational complexity of the design of numerical and statistical models is a
bottleneck for the forecast process. In this work, Windows Server 2012 R2
virtual machines are used as a high-performance tool for the speed-up of the
computational process. Six D-series and one standard tier A-series virtual
machines were hosted in Microsoft Azure public cloud for this purpose.
Visualization of the forecasted data is based on the Google Earth Pro virtual
globe in ASP.NET web-site against http://gearth.azurewebsites.net (prototype),
where KMZ file represents geographic placemarks. The long-range predictions of
the heat/cold waves are computed for several specifically located places based
on nonanticipative analog algorithm. The arguments of forecast models are
datasets from around the world, which reflects the concept of teleconnections.
This methodology does not require the probability distribution to design the
forecast models and/or calculate the predictions. Heat weaves at Annaba
(Algeria) are discussed in detail. Up to 36.4% of heat waves are specifically
predicted. Up to 33.3% of cold waves are specifically predicted for other four
locations around the world. The proposed approach is 100% accurate if the signs
of predicted and actual values are compared according to climatological
baseline. These high-accuracy predictions were achieved due to the
interdisciplinary approach, but advanced computer science techniques, public
cloud computing and Google Earth Pro virtual globe mainly, form the major part
of the work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06021</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06021</id><created>2015-12-18</created><authors><author><keyname>Wang</keyname><forenames>Jia</forenames></author><author><keyname>Chang</keyname><forenames>Kevin Chen-Chuan</forenames></author><author><keyname>Sundaram</keyname><forenames>Hari</forenames></author></authors><title>Network Cartography: Seeing the Forest and the Trees</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world networks are often complex and large with millions of nodes,
posing a great challenge for analysts to quickly see the big picture for more
productive subsequent analysis. We aim at facilitating exploration of
node-attributed networks by creating representations with conciseness,
expressiveness, interpretability, and multi-resolution views. We develop such a
representation as a {\it map} --- among the first to explore principled network
cartography for general networks. In parallel with common maps, ours has
landmarks, which aggregate nodes homogeneous in their traits and interactions
with nodes elsewhere, and roads, which represent the interactions between the
landmarks. We capture such homogeneity by the similar roles the nodes played.
Next, to concretely model the landmarks, we propose a probabilistic generative
model of networks with roles as latent factors. Furthermore, to enable
interactive zooming, we formulate novel model-based constrained optimization.
Then, we design efficient linear-time algorithms for the optimizations.
Experiments using real-world and synthetic networks show that our method
produces more expressive maps than existing methods, with up to 10 times
improvement in network reconstruction quality. We also show that our method
extracts landmarks with more homogeneous nodes, with up to 90\% improvement in
the average attribute/link entropy among the nodes over each landmark.
Sense-making of a real-world network using a map computed by our method
qualitatively verify the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06026</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06026</id><created>2015-12-18</created><authors><author><keyname>Gibson</keyname><forenames>Travis E.</forenames></author></authors><title>Sign Stability via Root Locus Analysis</title><categories>math.OC cs.SY physics.bio-ph q-bio.MN</categories><comments>Expository</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of network science old topics in ecology and economics are
resurfacing. One such topic is structural stability (often referred to as
qualitative stability or sign stability). A system is deemed structurally
stable if the system remains stable for all possible parameter variations so
long as the parameters do not change sign. This type of stability analysis is
appealing when studying real systems as the underlying stability result only
requires the scientist or engineer to know the sign of the parameters in the
model and not the specific values. The necessary and sufficient conditions for
qualitative stability however are opaque. In order to shed light on those
conditions root locus analysis is employed. This technique allows us to
illustrate the necessary conditions for qualitative stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06034</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06034</id><created>2015-12-18</created><authors><author><keyname>Adrian</keyname><forenames>Weronika T.</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Manna</keyname><forenames>Marco</forenames></author></authors><title>Ontology-driven Information Extraction</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homogeneous unstructured data (HUD) are collections of unstructured documents
that share common properties, such as similar layout, common file format, or
common domain of values. Building on such properties, it would be desirable to
automatically process HUD to access the main information through a semantic
layer -- typically an ontology -- called semantic view. Hence, we propose an
ontology-based approach for extracting semantically rich information from HUD,
by integrating and extending recent technologies and results from the fields of
classical information extraction, table recognition, ontologies, text
annotation, and logic programming. Moreover, we design and implement a system,
named KnowRex, that has been successfully applied to curriculum vitae in the
Europass style to offer a semantic view of them, and be able, for example, to
select those which exhibit required skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06050</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06050</id><created>2015-12-18</created><authors><author><keyname>Ye</keyname><forenames>Hongxing</forenames></author><author><keyname>Li</keyname><forenames>Zuyi</forenames></author></authors><title>Pricing the Ramping Reserve and Capacity Reserve in Real Time Markets</title><categories>math.OC cs.SY</categories><comments>We presented related content in 2014 IEEE Power and Energy Society
  General Meeting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing penetration of renewable energy in recent years has led to
more uncertainties in power systems. In order to maintain system reliability
and security, electricity market operators need to keep certain reserves in the
Security-Constrained Economic Dispatch (SCED) problems. A new concept,
deliverable generation ramping reserve, is proposed in this paper. The prices
of generation ramping reserves and generation capacity reserves are derived in
the Affine Adjustable Robust Optimization framework. With the help of these
prices, the valuable reserves can be identified among the available reserves.
These prices provide crucial information on the values of reserve resources,
which are critical for the long-term flexibility investment. The market
equilibrium based on these prices is analyzed. Simulations on a 3-bus system
and the IEEE 118-bus system are performed to illustrate the concept of ramping
reserve price and capacity reserve price. The impacts of the reserve credit on
market participants are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06061</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06061</id><created>2015-12-18</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh</forenames></author></authors><title>Asymptotic Behavior of Mean Partitions in Consensus Clustering</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although consistency is a minimum requirement of any estimator, little is
known about consistency of the mean partition approach in consensus clustering.
This contribution studies the asymptotic behavior of mean partitions. We show
that under normal assumptions, the mean partition approach is consistent and
asymptotic normal. To derive both results, we represent partitions as points of
some geometric space, called orbit space. Then we draw on results from the
theory of Fr\'echet means and stochastic programming. The asymptotic properties
hold for continuous extensions of standard cluster criteria (indices). The
results justify consensus clustering using finite but sufficiently large sample
sizes. Furthermore, the orbit space framework provides a mathematical
foundation for studying further statistical, geometrical, and analytical
properties of sets of partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06065</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06065</id><created>2015-12-18</created><authors><author><keyname>Serhii</keyname><forenames>Dyshko</forenames></author></authors><title>When the extension property does not hold for vector space alphabets</title><categories>cs.IT math.CO math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent paper we characterized the extension property for symmetrized
weight composition for linear codes over a module alphabet. Several
improvements for the case of vector space alphabets are given in this paper. A
detailed description of the property of $G$-pseudo-injectivity for vector
spaces is made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06073</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06073</id><created>2015-12-18</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Doignon</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Merckx</keyname><forenames>Keno</forenames></author></authors><title>On the shelling antimatroids of split graphs</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike poset antimatroids, chordal graph shelling antimatroids have received
little attention as regard their structures, optimization properties and
associated circuits. Here we consider a special case of those antimatroids,
namely the split graph shelling antimatroids. We establish a connection between
the structure of split graph shelling antimatroids and poset shelling
antimatroids. We discuss some applications of this new connection, in
particular, we give a simple polynomial time algorithm to find a maximum (or
minimum) weight feasible set in split graph shelling antimatroids and list all
the circuits and free sets for this class of antimatroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06075</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06075</id><created>2015-12-18</created><authors><author><keyname>Yacoob</keyname><forenames>Yaser</forenames></author></authors><title>Modeling Colors of Single Attribute Variations with Application to Food
  Appearance</title><categories>cs.CV</categories><comments>9 Pages. Paper does not reference recent food-classification papers.
  It is intended for wider scope</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the intra-image color-space of an object or a scene when
these are subject to a dominant single-source of variation. The source of
variation can be intrinsic or extrinsic (i.e., imaging conditions) to the
object. We observe that the quantized colors for such objects typically lie on
a planar subspace of RGB, and in some cases linear or polynomial curves on this
plane are effective in capturing these color variations. We also observe that
the inter-image color sub-spaces are robust as long as drastic illumination
change is not involved.
  We illustrate the use of this analysis for: discriminating between
shading-change and reflectance-change for patches, and object detection,
segmentation and recognition based on a single exemplar. We focus on images of
food items to illustrate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06080</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06080</id><created>2015-12-18</created><authors><author><keyname>Etcheverry</keyname><forenames>Lorena</forenames></author><author><keyname>Gomez</keyname><forenames>Silvia Silvia</forenames></author><author><keyname>Vaisman</keyname><forenames>Alejandro</forenames></author></authors><title>Modeling and Querying Data Cubes on the Semantic Web</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web is changing the way in which data warehouses are designed, used, and
queried. With the advent of initiatives such as Open Data and Open Government,
organizations want to share their multidimensional data cubes and make them
available to be queried online. The RDF data cube vocabulary (QB), the W3C
standard to publish statistical data in RDF, presents several limitations to
fully support the multidimensional model. The QB4OLAP vocabulary extends QB to
overcome these limitations, allowing to im- plement the typical OLAP
operations, such as rollup, slice, dice, and drill-across using standard SPARQL
queries. In this paper we introduce a formal data model where the main object
is the data cube, and define OLAP operations using this model, independent of
the underlying representation of the cube. We show then that a cube expressed
using our model can be represented using the QB4OLAP vocabulary, and finally we
provide a SPARQL implementation of OLAP operations over data cubes in QB4OLAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06110</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06110</id><created>2015-12-18</created><updated>2015-12-31</updated><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>Tsvetkov</keyname><forenames>Yulia</forenames></author><author><keyname>Neubig</keyname><forenames>Graham</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Morphological Inflection Generation Using Character Sequence to Sequence
  Learning</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphological inflection generation is the task of generating the inflected
form of a given lemma corresponding to a particular linguistic transformation.
We model the problem of inflection generation as a character sequence to
sequence learning problem and present a variant of the neural encoder-decoder
model for solving it. Our model is language independent and can be trained in
both supervised and semi-supervised settings. We evaluate our system on seven
datasets of morphologically rich languages and achieve either better or
comparable results to existing state-of-the-art models of inflection
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06112</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06112</id><created>2015-12-18</created><authors><author><keyname>Rok</keyname><forenames>Alexandre</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Coloring curves intersecting a fixed line</title><categories>math.CO cs.CG cs.DM</categories><msc-class>05C62, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{F}$ be a family of curves in the plane with the following
properties: (1) each member of $\mathcal{F}$ intersects a fixed straight line
$L$ in at least one and at most $t$ points, (2) any two members of
$\mathcal{F}$ intersect in at most one point, (3) the intersection graph of
$\mathcal{F}$ is triangle-free. We prove that the chromatic number
$\chi(\mathcal{F})$ of the intersection graph of $\mathcal{F}$ is bounded by a
function of $t$.
  Dependence on $t$ is crucial. It follows easily from the existence of
triangle-free segment intersection graphs with arbitrarily large chromatic
number that $\chi(\mathcal{F})$ can be arbitrarily large as $t$ grows. It has
been conjectured that the intersection graphs of families of curves
$\mathcal{F}$ satisfying just condition 1 have chromatic number bounded in
terms of $t$ and the clique number, which would generalize the recent result
that the class of outerstring graphs is $\chi$-bounded. We also show that it is
enough to establish the case $t=2$ in order to prove the conjecture for any
$t$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06136</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06136</id><created>2015-12-18</created><authors><author><keyname>Engwer</keyname><forenames>Christian</forenames></author><author><keyname>Gr&#xe4;ser</keyname><forenames>Carsten</forenames></author><author><keyname>M&#xfc;thing</keyname><forenames>Steffen</forenames></author><author><keyname>Sander</keyname><forenames>Oliver</forenames></author></authors><title>The interface for functions in the dune-functions module</title><categories>cs.MS</categories><comments>The C++ source code of tests is attached to pdf file of the paper</comments><msc-class>68N99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dune-functions dune module introduces a new programmer interface for
discrete and non-discrete functions. Unlike the previous interfaces considered
in the existing dune modules, it is based on overloading operator(), and
returning values by-value. This makes user code much more readable, and allows
the incorporation of newer C++ features such as lambda expressions. Run-time
polymorphism is implemented not by inheritance, but by type erasure,
generalizing the ideas of the std::function class from the C++11 standard
library. We describe the new interface, show its possibilities, and measure the
performance impact of type erasure and return-by-value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06141</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06141</id><created>2015-12-18</created><authors><author><keyname>Craig</keyname><forenames>Alison</forenames></author><author><keyname>Cranmer</keyname><forenames>Skyler J.</forenames></author><author><keyname>Desmarais</keyname><forenames>Bruce A.</forenames></author><author><keyname>Clark</keyname><forenames>Christopher J.</forenames></author><author><keyname>Moscardelli</keyname><forenames>Vincent G.</forenames></author></authors><title>The Role of Race, Ethnicity, and Gender in the Congressional
  Cosponsorship Network</title><categories>stat.AP cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous research indicates that race, ethnicity, and gender influence
legislative behavior in important ways. The bulk of this research, however,
focuses on the way these characteristics shape an individual legislator's
behavior, making it less clear how they account for relationships between
legislators. We study the cosponsorship process in order to understand the race
and gender based dynamics underlying the relational component of
representation. Using a temporal exponential random graph model, we examine the
U.S. House cosponsorship network from 1981 through 2004. We find that Black and
Latino members of Congress are at a comparative disadvantage as a result of
race-based assortative mixing in the cosponsorship process, yet this
disadvantage is mitigated by the electoral pressures that all members face.
Members representing districts with significant racial and ethnic minority
populations are more likely to support their minority colleagues. We also find
that women members do not appear to face a similar disadvantage as a result of
their minority status. We argue that these race and gender dynamics in the
cosponsorship network are the result of both the inherent tendency towards
intra-group homophily in social networks and the electoral connection, which is
manifested here as members supporting minority colleagues to broaden their own
electoral base of support among minority constituencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06143</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06143</id><created>2015-12-18</created><authors><author><keyname>Assadi</keyname><forenames>Sepehr</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Tannen</keyname><forenames>Val</forenames></author></authors><title>Algorithms for Provisioning Queries and Analytics</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provisioning is a technique for avoiding repeated expensive computations in
what-if analysis. Given a query, an analyst formulates $k$ hypotheticals, each
retaining some of the tuples of a database instance, possibly overlapping, and
she wishes to answer the query under scenarios, where a scenario is defined by
a subset of the hypotheticals that are &quot;turned on&quot;. We say that a query admits
compact provisioning if given any database instance and any $k$ hypotheticals,
one can create a poly-size (in $k$) sketch that can then be used to answer the
query under any of the $2^{k}$ possible scenarios without accessing the
original instance.
  In this paper, we focus on provisioning complex queries that combine
relational algebra (the logical component), grouping, and statistics/analytics
(the numerical component). We first show that queries that compute quantiles or
linear regression (as well as simpler queries that compute count and
sum/average of positive values) can be compactly provisioned to provide
(multiplicative) approximate answers to an arbitrary precision. In contrast,
exact provisioning for each of these statistics requires the sketch size to be
exponential in $k$. We then establish that for any complex query whose logical
component is a positive relational algebra query, as long as the numerical
component can be compactly provisioned, the complex query itself can be
compactly provisioned. On the other hand, introducing negation or recursion in
the logical component again requires the sketch size to be exponential in $k$.
While our positive results use algorithms that do not access the original
instance after a scenario is known, we prove our lower bounds even for the case
when, knowing the scenario, limited access to the instance is allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06161</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06161</id><created>2015-12-18</created><authors><author><keyname>Blaum</keyname><forenames>Mario</forenames></author></authors><title>On Locally Recoverable (LRC) Codes</title><categories>cs.IT math.IT</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present simple constructions of optimal erasure-correcting LRC codes by
exhibiting their parity-check matrices. When the number of local parities in a
parity group plus the number of global parities is smaller than the size of the
parity group, the constructed codes are optimal with a field of size at least
the length of the code. We can reduce the size of the field to at least the
size of the parity groups when the number of global parities equals the number
of local parities in a parity group plus one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06168</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06168</id><created>2015-12-18</created><updated>2016-01-05</updated><authors><author><keyname>Ren</keyname><forenames>Kun</forenames></author><author><keyname>Faleiro</keyname><forenames>Jose M.</forenames></author><author><keyname>Abadi</keyname><forenames>Daniel J.</forenames></author></authors><title>Design Principles for Scaling Multi-core OLTP Under High Contention</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although significant recent progress has been made in improving the
multi-core scalability of high throughput transactional database systems,
modern systems still fail to achieve scalable throughput for workloads
involving frequent access to highly contended data. Most of this inability to
achieve high throughput is explained by the fundamental constraints involved in
guaranteeing ACID --- the addition of cores results in more concurrent
transactions accessing the same contended data for which access must be
serialized in order to guarantee isolation. Thus, linear scalability for
contended workloads is impossible. However, there exist flaws in many modern
architectures that exacerbate their poor scalability, and result in throughput
that is much worse than fundamentally required by the workload.
  In this paper we identify two prevalent design principles that limit the
multi-core scalability of many (but not all) transactional database systems on
contended workloads: the multi-purpose nature of execution threads in these
systems, and the lack of advanced planning of data access. We demonstrate the
deleterious results of these design principles by implementing a prototype
system, ORTHRUS, that is motivated by the principles of separation of database
component functionality and advanced planning of transactions. We find that
these two principles alone result in significantly improved scalability on
high-contention workloads, and an order of magnitude increase in throughput for
a non-trivial subset of these contended workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06173</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06173</id><created>2015-12-18</created><authors><author><keyname>Dang</keyname><forenames>Xuan Hong</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author><author><keyname>Bogdanov</keyname><forenames>Petko</forenames></author><author><keyname>You</keyname><forenames>Hongyuan</forenames></author><author><keyname>Hsu</keyname><forenames>Bayyuan</forenames></author></authors><title>Discriminative Subnetworks with Regularized Spectral Learning for
  Global-state Network Data</title><categories>cs.LG</categories><comments>manuscript for the ECML 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining practitioners are facing challenges from data with network
structure. In this paper, we address a specific class of global-state networks
which comprises of a set of network instances sharing a similar structure yet
having different values at local nodes. Each instance is associated with a
global state which indicates the occurrence of an event. The objective is to
uncover a small set of discriminative subnetworks that can optimally classify
global network values. Unlike most existing studies which explore an
exponential subnetwork space, we address this difficult problem by adopting a
space transformation approach. Specifically, we present an algorithm that
optimizes a constrained dual-objective function to learn a low-dimensional
subspace that is capable of discriminating networks labelled by different
global states, while reconciling with common network topology sharing across
instances. Our algorithm takes an appealing approach from spectral graph
learning and we show that the globally optimum solution can be achieved via
matrix eigen-decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06176</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06176</id><created>2015-12-18</created><updated>2016-02-17</updated><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Jiang</keyname><forenames>Dongdong</forenames></author><author><keyname>Wu</keyname><forenames>Yueping</forenames></author></authors><title>Analysis and Optimization of Caching and Multicasting in Large-Scale
  Cache-Enabled Wireless Networks</title><categories>cs.IT math.IT</categories><comments>31 pages, 6 figures, 1 table. Transactions on Wireless Communication
  (submitted in July 2015, now under 2nd revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching and multicasting at base stations are two promising approaches to
support massive content delivery over wireless networks. However, existing
analysis and designs do not fully explore and exploit the potential advantages
of the two approaches. In this paper, we consider the analysis and optimization
of caching and multicasting in a large-scale cache-enabled wireless network. We
propose a random caching and multicasting scheme with a design parameter. By
carefully handling different types of interferers and adopting appropriate
approximations, we derive a tractable expression for the successful
transmission probability in the general region, utilizing tools from stochastic
geometry. We also obtain a closed-form expression for the successful
transmission probability in the high signal-to-noise ratio (SNR) and user
density region. Then, we consider the successful transmission probability
maximization, which is a very complex non-convex problem in general. Using
optimization techniques, we develop an iterative numerical algorithm to obtain
a local optimal caching and multicasting design in the general region. To
reduce complexity and maintain superior performance, we also derive an
asymptotically optimal caching and multicasting design in the asymptotic
region, based on a two-step optimization framework. Finally, numerical
simulations show that the asymptotically optimal design achieves a significant
gain in successful transmission probability over some baseline schemes in the
general region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06178</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06178</id><created>2015-12-18</created><authors><author><keyname>Navarro</keyname><forenames>Marisa</forenames><affiliation>UPV/EHU</affiliation></author></authors><title>Proceedings XV Jornadas sobre Programaci\'on y Lenguajes</title><categories>cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 200, 2015</journal-ref><doi>10.4204/EPTCS.200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains a selection of the papers presented at the XV Jornadas
sobre Programaci\'on y Lenguajes (PROLE 2015), held at Santander, Spain, during
September 15th-17th, 2015. Previous editions of the workshop were held in
C\'adiz (2014), Madrid (2013), Almer\'ia (2012), A Coru\~na (2011), Val\`encia
(2010), San Sebasti\'an (2009), Gij\'on (2008), Zaragoza (2007), Sitges (2006),
Granada (2005), M\'alaga (2004), Alicante (2003), El Escorial (2002), and
Almagro (2001). Programming languages provide a conceptual framework which is
necessary for the development, analysis, optimization and understanding of
programs and programming tasks. The aim of the PROLE series of conferences
(PROLE stems from PROgramaci\'on y LEnguajes) is to serve as a meeting point
for Spanish research groups which develop their work in the area of programming
and programming languages. The organization of this series of events aims at
fostering the exchange of ideas, experiences and results among these groups.
Promoting further collaboration is also one of its main goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06184</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06184</id><created>2015-12-18</created><updated>2016-01-19</updated><authors><author><keyname>Aleem</keyname><forenames>Saad A.</forenames></author><author><keyname>Nowzari</keyname><forenames>Cameron</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Self-triggered Pursuit of a Single Evader with Uncertain Information</title><categories>cs.SY</categories><comments>12 pages, 12 figures. Preprint for submission in Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a pursuit-evasion problem involving a single pursuer and a
single evader, where we are interested in developing a pursuit strategy that
doesn't require continuous, or even periodic, information about the position of
the evader. We propose a self-triggered control strategy that allows the
pursuer to sample the evader's position autonomously, while satisfying desired
performance metric of evader capture. The work in this paper builds on the
previously proposed self-triggered pursuit strategy which guarantees capture of
the evader in finite time with a finite number of evader samples. However, this
algorithm relied on the unrealistic assumption that the evader's exact position
was available to the pursuer. Instead, we extend our previous framework to
develop an algorithm which allows for uncertainties in sampling the information
about the evader, and derive tolerable upper-bounds on the error such that the
pursuer can guarantee capture of the evader. In addition, we outline the
advantages of retaining the evader's history in improving the current estimate
of the true location of the evader that can be used to capture the evader with
even less samples. Our approach is in sharp contrast to the existing works in
literature and our results ensure capture without sacrificing any performance
in terms of guaranteed time-to-capture, as compared to classic algorithms that
assume continuous availability of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06195</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06195</id><created>2015-12-19</created><authors><author><keyname>Aturban</keyname><forenames>Mohamed</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author><author><keyname>Weigle</keyname><forenames>Michele C.</forenames></author></authors><title>Quantifying Orphaned Annotations in Hypothes.is</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web annotation has been receiving increased attention recently with the
organization of the Open Annotation Collaboration and new tools for open
annotation, such as Hypothes.is. We investigate the prevalence of orphaned
annotations, where neither the live Web page nor an archived copy of the Web
page contains the text that had previously been annotated in the Hypothes.is
annotation system (containing 20,953 highlighted text annotations). We found
that about 22% of highlighted text annotations can no longer be attached to
their live Web pages. Unfortunately, only about 12% of these annotations can be
reattached using the holdings of current public web archives, leaving the
remaining 88% of these annotations orphaned. For those annotations that are
still attached, 53% are in danger of becoming orphans if the live Web page
changes. This points to the need for archiving the target of an annotation at
the time the annotation is created.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06197</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06197</id><created>2015-12-19</created><authors><author><keyname>Lyu</keyname><forenames>Jiangbin</forenames></author><author><keyname>Chew</keyname><forenames>Yong Huat</forenames></author><author><keyname>Wong</keyname><forenames>Wai-Choong</forenames></author></authors><title>A Stackelberg Game Model for Overlay D2D Transmission with Heterogeneous
  Rate Requirements</title><categories>cs.GT</categories><comments>32 pages, 11 figures, accepted for publication in IEEE Transactions
  on Vehicular Technology, Dec 11, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of overlay device-to-device (D2D)
communication links via carrier sense multiple access (CSMA) protocols. We
assume that the D2D links have heterogeneous rate requirements and different
willingness to pay, and each of them acts non-altruistically to achieve its
target rate while maximizing its own payoff. Spatial reuse is allowed if the
links are not interfering with each other. A non-cooperative game model is used
to address the resource allocation among the D2D links, at the same time
leveraging on the ideal CSMA network (ICN) model to address the physical
channel access issue. We propose a Stackelberg game in which the base station
in the cellular network acts as a Stackelberg leader to regulate the individual
payoff by modifying the unit service price so that the total D2D throughput is
maximized. The problem is shown to be quasi-convex and can be solved by a
sequence of equivalent convex optimization problems. The pricing strategies are
designed so that the network always operates within the feasible throughput
region. The results are verified by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06211</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06211</id><created>2015-12-19</created><authors><author><keyname>Keet</keyname><forenames>C. Maria</forenames></author><author><keyname>Lawrynowicz</keyname><forenames>Agnieszka</forenames></author></authors><title>Test-Driven Development of ontologies (extended version)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging ontology authoring methods to add knowledge to an ontology focus on
ameliorating the validation bottleneck. The verification of the newly added
axiom is still one of trying and seeing what the reasoner says, because a
systematic testbed for ontology authoring is missing. We sought to address this
by introducing the approach of test-driven development for ontology authoring.
We specify 36 generic tests, as TBox queries and TBox axioms tested through
individuals, and structure their inner workings in an `open box'-way, which
cover the OWL 2 DL language features. This is implemented as a Protege plugin
so that one can perform a TDD test as a black box test. We evaluated the two
test approaches on their performance. The TBox queries were faster, and that
effect is more pronounced the larger the ontology is. We provide a general
sequence of a TDD process for ontology engineering as a foundation for a TDD
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06216</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06216</id><created>2015-12-19</created><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Hu</keyname><forenames>Zhiting</forenames></author><author><keyname>Wei</keyname><forenames>Jinliang</forenames></author><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Kim</keyname><forenames>Gunhee</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Poseidon: A System Architecture for Efficient GPU-based Deep Learning on
  Multiple Machines</title><categories>cs.LG cs.CV cs.DC</categories><comments>14 pages, 8 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning (DL) has achieved notable successes in many machine learning
tasks. A number of frameworks have been developed to expedite the process of
designing and training deep neural networks (DNNs), such as Caffe, Torch and
Theano. Currently they can harness multiple GPUs on a single machine, but are
unable to use GPUs that are distributed across multiple machines; as even
average-sized DNNs can take days to train on a single GPU with 100s of GBs to
TBs of data, distributed GPUs present a prime opportunity for scaling up DL.
However, the limited bandwidth available on commodity Ethernet networks
presents a bottleneck to distributed GPU training, and prevents its trivial
realization.
  To investigate how to adapt existing frameworks to efficiently support
distributed GPUs, we propose Poseidon, a scalable system architecture for
distributed inter-machine communication in existing DL frameworks. We integrate
Poseidon with Caffe and evaluate its performance at training DNNs for object
recognition. Poseidon features three key contributions that accelerate DNN
training on clusters: (1) a three-level hybrid architecture that allows
Poseidon to support both CPU-only and GPU-equipped clusters, (2) a distributed
wait-free backpropagation (DWBP) algorithm to improve GPU utilization and to
balance communication, and (3) a structure-aware communication protocol (SACP)
to minimize communication overheads. We empirically show that Poseidon
converges to same objectives as a single machine, and achieves state-of-art
training speedup across multiple models and well-established datasets using a
commodity GPU cluster of 8 nodes (e.g. 4.5x speedup on AlexNet, 4x on
GoogLeNet, 4x on CIFAR-10). On the much larger ImageNet22K dataset, Poseidon
with 8 nodes achieves better speedup and competitive accuracy to recent
CPU-based distributed systems such as Adam and Le et al., which use 10s to
1000s of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06222</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06222</id><created>2015-12-19</created><authors><author><keyname>Kari</keyname><forenames>Dariush</forenames></author><author><keyname>Sayin</keyname><forenames>Muhammed Omer</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman Serdar</forenames></author></authors><title>A new robust adaptive algorithm for underwater acoustic channel
  equalization</title><categories>cs.SD cs.IT cs.LG math.IT</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel family of adaptive robust equalizers for highly
challenging underwater acoustic (UWA) channel equalization. Since the
underwater environment is highly non-stationary and subjected to impulsive
noise, we use adaptive filtering techniques based on a relative logarithmic
cost function inspired by the competitive methods from the online learning
literature. To improve the convergence performance of the conventional linear
equalization methods, while mitigating the stability issues, we intrinsically
combine different norms of the error in the cost function, using logarithmic
functions. Hence, we achieve a comparable convergence performance to least mean
fourth (LMF) equalizer, while significantly enhancing the stability performance
in such an adverse communication medium. We demonstrate the performance of our
algorithms through highly realistic experiments performed on accurately
simulated underwater acoustic channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06223</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06223</id><created>2015-12-19</created><authors><author><keyname>Platero</keyname><forenames>Carlos</forenames></author><author><keyname>Tobar</keyname><forenames>M. Carmen</forenames></author></authors><title>Combining patch-based strategies and non-rigid registration-based label
  fusion methods</title><categories>cs.CV</categories><comments>33 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this study is to develop a patch-based labeling method that
cooperates with a label fusion using non-rigid registrations. We present a
novel patch-based label fusion method, whose selected patches and their weights
are calculated from a combination of similarity measures between patches using
intensity-based distances and labeling-based distances, where a previous
labeling of the target image is inferred through a label fusion method using
non-rigid registrations. These combined similarity measures result in better
selection of the patches, and their weights are more robust, which improves the
segmentation results compared to other label fusion methods, including the
conventional patch-based labeling method. To evaluate the performance and the
robustness of the proposed label fusion method, we employ two available
databases of T1-weighted (T1W) magnetic resonance imaging (MRI) of human
brains. We compare our approach with other label fusion methods in the
automatic hippocampal segmentation from T1W-MRI.
  Our label fusion method yields mean Dice coefficients of 0.847 and 0.798 for
the two databases used with mean times of approximately 180 and 320 seconds,
respectively. The collaboration between the patch-based labeling method and the
label fusion using non-rigid registrations is given in the several levels: (a)
The pre-selection of the patches in the atlases are improved, (b) The weights
of our selected patches are also more robust, (c) our approach imposes
geometrical restrictions, such as shape priors, and (d) the work-flow is very
efficient. We show that the proposed approach is very competitive with respect
to recently reported methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06227</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06227</id><created>2015-12-19</created><authors><author><keyname>Bouzid</keyname><forenames>Zohir</forenames></author><author><keyname>Imbs</keyname><forenames>Damien</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author></authors><title>A Necessary Condition for Byzantine $k$-Set Agreement</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper presents a necessary condition for Byzantine $k$-set
agreement in (synchronous or asynchronous) message-passing systems and
asynchronous shared memory systems where the processes communicate through
atomic single-writer multi-reader registers. It gives a proof, which is
particularly simple, that $k$-set agreement cannot be solved $t$-resiliently in
an $n$-process system when $n \leq 2t + \frac{t}{k}$. This bound is tight for
the case $k=1$ (Byzantine consensus) in synchronous message-passing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06228</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06228</id><created>2015-12-19</created><authors><author><keyname>Sharang</keyname><forenames>Abhijit</forenames></author><author><keyname>Rao</keyname><forenames>Chetan</forenames></author></authors><title>Using machine learning for medium frequency derivative portfolio trading</title><categories>q-fin.TR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use machine learning for designing a medium frequency trading strategy for
a portfolio of 5 year and 10 year US Treasury note futures. We formulate this
as a classification problem where we predict the weekly direction of movement
of the portfolio using features extracted from a deep belief network trained on
technical indicators of the portfolio constituents. The experimentation shows
that the resulting pipeline is effective in making a profitable trade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06230</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06230</id><created>2015-12-19</created><authors><author><keyname>Cid-Fuentes</keyname><forenames>Raul G.</forenames></author><author><keyname>Naderi</keyname><forenames>M. Yousof</forenames></author><author><keyname>Doost-Mohammady</keyname><forenames>Rahman</forenames></author><author><keyname>Chowdhury</keyname><forenames>Kaushik R.</forenames></author><author><keyname>Cabellos-Aparicio</keyname><forenames>Albert</forenames></author><author><keyname>Alarcon</keyname><forenames>Eduard</forenames></author></authors><title>Leveraging Deliberately Generated Interferences for Multi-sensor
  Wireless RF Power Transmission</title><categories>cs.NI</categories><comments>IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless RF power transmission promises battery-less, resilient, and
perpetual wireless sensor networks. Through the action of controllable Energy
Transmitters (ETs) that operate at-a-distance, the sensors can be re-charged by
harvesting the radiated RF energy. However, both the charging rate and
effective charging range of the ETs are limited, and thus multiple ETs are
required to cover large areas. While this action increases the amount of
wireless energy injected into the network, there are certain areas where the RF
energy combines destructively. To address this problem, we propose a
duty-cycled random-phase multiple access (DRAMA). Non-intuitively, our approach
relies on deliberately generating random interferences, both destructive and
constructive, at the destination nodes. We demonstrate that DRAMA optimizes the
power conversion efficiency, and the total amount of energy harvested. Through
real-testbed experiments, we prove that our proposed scheme provides
significant advantages over the current state of the art in our considered
scenario, as it requires up to 70\% less input RF power to recharge the energy
buffer of the sensor in the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06233</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06233</id><created>2015-12-19</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Process Realizability</title><categories>cs.LO</categories><comments>Appeared in Foundations of Secure Computation: Proceedings of the
  1999 Marktoberdorf Summer School, F. L. Bauer and R. Steinbruggen, eds. (IOS
  Press) 2000, 167-180</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a notion of realizability for Classical Linear Logic based on a
concurrent process calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06235</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06235</id><created>2015-12-19</created><authors><author><keyname>Shah</keyname><forenames>Rajvi</forenames></author><author><keyname>Deshpande</keyname><forenames>Aditya</forenames></author><author><keyname>Narayanan</keyname><forenames>P J</forenames></author></authors><title>Multistage SFM: A Coarse-to-Fine Approach for 3D Reconstruction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several methods have been proposed for large-scale 3D reconstruction from
large, unorganized image collections. A large reconstruction problem is
typically divided into multiple components which are reconstructed
independently using structure from motion (SFM) and later merged together.
Incremental SFM methods are most popular for the basic structure recovery of a
single component. They are robust and effective but are strictly sequential in
nature. We present a multistage approach for SFM reconstruction of a single
component that breaks the sequential nature of the incremental SFM methods. Our
approach begins with quickly building a coarse 3D model using only a fraction
of features from given images. The coarse model is then enriched by localizing
remaining images and matching and triangulating remaining features in
subsequent stages. These stages are made efficient and highly parallel by
leveraging the geometry of the coarse model. Our method produces similar
quality models as compared to incremental SFM methods while being notably fast
and parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06237</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06237</id><created>2015-12-19</created><authors><author><keyname>Lipi&#x144;ski</keyname><forenames>Zbigniew</forenames></author></authors><title>On the solutions of the minimum energy problem in one dimensional sensor
  networks</title><categories>cs.NI</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss solutions of the minimum energy problem in one dimensional
wireless sensor networks for the data transmission cost function $E(x_i,x_j) =
d(x_i,x_j)^a + \lambda \;d(x_i,x_j)^b$ with any exponent $a,b\in R$ and
$\lambda \geq 0$, where $d(x_i,x_j)$ is a distance between transmitter and
receiver. We define the minimum energy problem in terms of sensors signal
power, transmission time and capacities of a transmission channels. We prove,
that for the point-to-point data transmission method utilized by the sensors in
the physical layer, when the transmitter adjust the power of its radio signal
to the distance to the receiver, the solutions of the minimum energy problem
written in terms of data transmission cost function and in terms of the sensors
signal power coincide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06238</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06238</id><created>2015-12-19</created><authors><author><keyname>Balkanski</keyname><forenames>Eric</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author><author><keyname>Singer</keyname><forenames>Yaron</forenames></author></authors><title>The Limitations of Optimization from Samples</title><categories>cs.DS cs.DM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we grow highly dependent on data for making predictions, we translate
these predictions into models that help us make informed decisions. But what
are the guarantees we have? Can we optimize decisions on models learned from
data and be guaranteed that we achieve desirable outcomes? In this paper we
formalize this question through a novel model called optimization from samples
(OPS). In the OPS model, we are given sampled values of a function drawn from
some distribution and our objective is to optimize the function under some
constraint. Our main interest is in the following question: are functions that
are learnable (from samples) and approximable (given oracle access to the
function) also optimizable from samples?
  We show that there are classes of submodular functions which have desirable
approximation and learnability guarantees and for which no reasonable
approximation for optimizing from samples is achievable. In particular, our
main result shows that even for maximization of coverage functions under a
cardinality constraint $k$, there exists a hypothesis class of functions that
cannot be approximated within a factor of $n^{-1/4 + \epsilon}$ (for any
constant $\epsilon &gt; 0$) of the optimal solution, from samples drawn from the
uniform distribution over all sets of size at most $k$. In the general case of
monotone submodular functions, we show an $n^{-1/3 + \epsilon}$ lower bound and
an almost matching $\tilde{\Omega}(n^{-1/3})$-optimization from samples
algorithm. Additive and unit-demand functions can be optimized from samples to
within arbitrarily good precision. Finally, we also consider a corresponding
notion of additive approximation for continuous optimization from samples, and
show near-optimal hardness for concave maximization and convex minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06244</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06244</id><created>2015-12-19</created><authors><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Trumpf</keyname><forenames>Jochen</forenames></author></authors><title>Convergence and State Reconstruction of Time-varying Multi-agent Systems
  from Complete Observability Theory</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study continuous-time consensus dynamics for multi-agent systems with
undirected switching interaction graphs. We establish a necessary and
sufficient condition for exponential asymptotic consensus based on the
classical theory of complete observability. The proof is remarkably simple
compared to similar results in the literature and the conditions for consensus
are mild. This observability-based method can also be applied to the case where
negatively weighted edges are present. Additionally, as a by-product of the
observability based arguments, we show that the nodes' initial value can be
recovered from the signals on the edges up to a shift of the network average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06246</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06246</id><created>2015-12-19</created><authors><author><keyname>Geck</keyname><forenames>Gaetano</forenames></author><author><keyname>Ketsman</keyname><forenames>Bas</forenames></author><author><keyname>Neven</keyname><forenames>Frank</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author></authors><title>Parallel-Correctness and Containment for Conjunctive Queries with Union
  and Negation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-round multiway join algorithms first reshuffle data over many servers
and then evaluate the query at hand in a parallel and communication-free way. A
key question is whether a given distribution policy for the reshuffle is
adequate for computing a given query, also referred to as parallel-correctness.
This paper extends the study of the complexity of parallel-correctness and its
constituents, parallel-soundness and parallel-completeness, to unions of
conjunctive queries with and without negation. As a by-product it is shown that
the containment problem for conjunctive queries with negation is
coNEXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06257</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06257</id><created>2015-12-19</created><updated>2015-12-29</updated><authors><author><keyname>Yao</keyname><forenames>Lina</forenames></author><author><keyname>Sheng</keyname><forenames>Quan Z.</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author><author><keyname>Dustdar</keyname><forenames>Schahram</forenames></author><author><keyname>Shemshadi</keyname><forenames>Ali</forenames></author><author><keyname>Wang</keyname><forenames>Xianzhi</forenames></author><author><keyname>Ngu</keyname><forenames>Anne H. H.</forenames></author></authors><title>Up in the Air: When Homes Meet the Web of Things</title><categories>cs.CY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging Web of Things (WoT) will comprise billions of Web-enabled
objects (or &quot;things&quot;) where such objects can sense, communicate, compute and
potentially actuate. WoT is essentially the embodiment of the evolution from
systems linking digital documents to systems relating digital information to
real-world physical items. It is widely understood that significant technical
challenges exist in developing applications in the WoT environment. In this
paper, we report our practical experience in the design and development of a
smart home system in a WoT environment. Our system provides a layered framework
for managing and sharing the information produced by physical things as well as
the residents. We particularly focus on a research prototype named WITS, that
helps the elderly live independently and safely in their own homes, with
minimal support from the decreasing number of individuals in the working-age
population. WITS enables an unobtrusive monitoring of elderly people in a
real-world, inhabituated home environment, by leveraging WoT technologies in
building context-aware, personalized services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06264</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06264</id><created>2015-12-19</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Flexible Widely-Linear Multi-Branch Decision Feedback Detection
  Algorithms for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>3 figures, 9 pages. arXiv admin note: text overlap with
  arXiv:1308.2725</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents widely-linear multi-branch decision feedback detection
techniques for large-scale multiuser multiple-antenna systems. We consider a
scenario with impairments in the radio-frequency chain in which the in-phase
(I) and quadrature (Q) components exhibit an imbalance, which degrades the
receiver performance and originates non-circular signals. A widely-linear
multi-branch decision feedback receiver is developed to mitigate both the
multiuser interference and the I/Q imbalance effects. An iterative detection
and decoding scheme with the proposed receiver and convolutional codes is also
devised. Simulation results show that the proposed techniques outperform
existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06271</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06271</id><created>2015-12-19</created><authors><author><keyname>Guruganesh</keyname><forenames>Guru Prashanth</forenames></author><author><keyname>Singla</keyname><forenames>Sahil</forenames></author></authors><title>Online Matroid Intersection: Beating Half for Random Arrival</title><categories>cs.DS</categories><comments>26 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the online matroid intersection problem, which is related to the
well-studied online bipartite matching problem in the vertex arrival model. For
two matroids $\mathcal{M}_1$ and $\mathcal{M}_2$ defined on the same ground set
$E$, the problem is to design an algorithm that constructs a large common
independent set in an online fashion. The algorithm is presented with the
ground set elements one-by-one in a uniformly random order. At each step, the
algorithm must irrevocably decide whether to pick the element, while always
maintaining a common independent set. Since the greedy algorithm --- pick the
element whenever possible --- has a competitive ratio of half, the natural
question is whether we can beat half. This problem generalizes online bipartite
matching in the edge arrival model where a random edge is presented at each
step; nothing better than half competitiveness was previously known.
  In this paper, we present a simple randomized algorithm that has a $\frac12 +
\delta$ competitive ratio in expectation, for a constant $\delta&gt;0$. The
expectation is over the randomness of the input order and the coin tosses of
the algorithm. We also extend our result to intersection of $k$ matroids and to
general graphs, cases not captured by intersection of two matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06282</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06282</id><created>2015-12-19</created><authors><author><keyname>Kelly</keyname><forenames>Philip</forenames></author><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Contributions to the compositional semantics of first-order predicate
  logic</title><categories>cs.LO</categories><comments>14 pages, 1 figure</comments><report-no>DCS-356-IR</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Henkin, Monk and Tarski gave a compositional semantics for first-order
predicate logic. We extend this work by including function symbols in the
language and by giving the denotation of the atomic formula as a composition of
the denotations of its predicate symbol and of its tuple of arguments. In
addition we give the denotation of a term as a composition of the denotations
of its function symbol and of its tuple of arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06283</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06283</id><created>2015-12-19</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Chinese Postman Problem on Edge-Colored Multigraphs</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the Chinese postman problem on undirected and directed
graphs is polynomial-time solvable. We extend this result to edge-colored
multigraphs. Our result is in sharp contrast to the Chinese postman problem on
mixed graphs, i.e., graphs with directed and undirected edges, for which the
problem is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06285</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06285</id><created>2015-12-19</created><authors><author><keyname>Xian</keyname><forenames>Min</forenames></author><author><keyname>Zhang</keyname><forenames>Yingtao</forenames></author><author><keyname>Cheng</keyname><forenames>H. D.</forenames></author><author><keyname>Xu</keyname><forenames>Fei</forenames></author><author><keyname>Ding</keyname><forenames>Jianrui</forenames></author></authors><title>Neutro-Connectedness Cut</title><categories>cs.CV</categories><comments>30 pages, 10 figures, 3 tables, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive image segmentation is a challenging task and received increasing
attention recently; however, two major drawbacks exist in interactive
segmentation approaches. First, the segmentation performance of ROI-based
methods is sensitive to the initial ROI: different ROIs may produce results
with great difference. Second, most seed-based methods need intense
interactions, and are not applicable in many cases. In this work, we generalize
the Neutro-Connectedness (NC) to be independent of top-down priors of objects
and to model image topology with indeterminacy measurement on image regions,
propose a novel method for determining object and background regions, which is
applied to exclude isolated background regions and enforce label consistency,
and put forward a hybrid interactive segmentation method, Neutro-Connectedness
Cut (NC-Cut), which can overcome the above two problems by utilizing both
pixel-wise appearance information and region-based NC properties. We evaluate
the proposed NC-Cut by employing two image datasets (265 images), and
demonstrate that the proposed approach outperforms state-of-the-art interactive
image segmentation methods (Grabcut, MILCut, One-Cut, {{GC}_max}^sum and pPBC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06291</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06291</id><created>2015-12-19</created><authors><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Zhao</keyname><forenames>Wanyao</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>Secure Degrees of Freedom of the Gaussian Diamond-Wiretap Channel</title><categories>cs.IT math.IT</categories><comments>29 pages, 5 figures, Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Gaussian diamond-wiretap channel that consists
of an orthogonal broadcast channel from a source to two relays and a Gaussian
fast-fading multiple access-wiretap channel from the two relays to a legitimate
destination and an eavesdropper. For the multiple access part, we consider both
the case with full channel state information (CSI) and the case with no
eavesdropper's CSI, at the relays and the legitimate destination. For both the
cases, we establish the exact secure degrees of freedom and generalize the
results for multiple relays.
  For the converse part, we introduce a new technique of capturing the
trade-off between the message rate and the amount of individual randomness
injected at each relay. In the achievability part, we show (i) how to strike a
balance between sending message symbols and common noise symbols from the
source to the relays in the broadcast component and (ii) how to combine
artificial noise-beamforming and noise-alignment techniques at the relays in
the multiple access component. In the case with full CSI, we propose a scheme
where the relays simultaneously beamform common noise signals in the null space
of the legitimate destination's channel, and align them with the message
signals at the eavesdropper. In the case with no eavesdropper's CSI, we present
a scheme that efficiently utilizes the broadcast links by incorporating
computation between the message and common noise symbols at the source.
Finally, most of our achievability and converse techniques can also be adapted
to the Gaussian (non-fading) channel model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06293</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06293</id><created>2015-12-19</created><authors><author><keyname>Wiatowski</keyname><forenames>Thomas</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>A Mathematical Theory of Deep Convolutional Neural Networks for Feature
  Extraction</title><categories>cs.IT cs.AI cs.LG math.FA math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks have led to breakthrough results in
practical feature extraction applications. The mathematical analysis of such
networks was initiated by Mallat, 2012. Specifically, Mallat considered
so-called scattering networks based on semi-discrete shift-invariant wavelet
frames and modulus non-linearities in each network layer, and proved
translation invariance (asymptotically in the wavelet scale parameter) and
deformation stability of the corresponding feature extractor. The purpose of
this paper is to develop Mallat's theory further by allowing for general
convolution kernels, or in more technical parlance, general semi-discrete
shift-invariant frames (including Weyl-Heisenberg, curvelet, shearlet,
ridgelet, and wavelet frames) and general Lipschitz-continuous non-linearities
(e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents,
and modulus functions), as well as pooling through sub-sampling, all of which
can be different in different network layers. The resulting generalized network
enables extraction of significantly wider classes of features than those
resolved by Mallat's wavelet-modulus scattering network. We prove deformation
stability for a larger class of deformations than those considered by Mallat,
and we establish a new translation invariance result which is of vertical
nature in the sense of the network depth determining the amount of invariance.
Moreover, our results establish that deformation stability and vertical
translation invariance are guaranteed by the network structure per se rather
than the specific convolution kernels and non-linearities. This offers an
explanation for the tremendous success of deep convolutional neural networks in
a wide variety of practical feature extraction applications. The mathematical
techniques we employ are based on continuous frame theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06298</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06298</id><created>2015-12-19</created><authors><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>Streaming Data Transmission in the Moderate Deviations and Central Limit
  Regimes</title><categories>cs.IT math.IT</categories><comments>36 pages, 3 figures, Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider streaming data transmission over a discrete memoryless channel. A
new message is given to the encoder at the beginning of each block and the
decoder decodes each message sequentially, after a delay of $T$ blocks. In this
streaming setup, we study the fundamental interplay between the rate and error
probability in the central limit and moderate deviations regimes and show that
i) in the moderate deviations regime, the moderate deviations constant improves
over the block coding or non-streaming setup by a factor of $T$ and ii) in the
central limit regime, the second-order coding rate improves by a factor of
approximately $\sqrt{T}$ for a wide range of channel parameters. For both
regimes, we propose coding techniques that incorporate a joint encoding of
fresh and previous messages. In particular, for the central limit regime, we
propose a coding technique with truncated memory to ensure that a summation of
constants, which arises as a result of applications of the central limit
theorem, does not diverge in the error analysis.
  Furthermore, we explore interesting variants of the basic streaming setup in
the moderate deviations regime. We first consider a scenario with an erasure
option at the decoder and show that both the exponents of the total error and
the undetected error probabilities improve by factors of $T$. Next, by
utilizing the erasure option, we show that the exponent of the total error
probability can be improved to that of the undetected error probability (in the
order sense) at the expense of a variable decoding delay. Finally, we also
extend our results to the case where the message rate is not fixed but
alternates between two values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06303</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06303</id><created>2015-12-19</created><authors><author><keyname>Elkouri</keyname><forenames>Andrew</forenames></author></authors><title>Predicting the Sentiment Polarity and Rating of Yelp Reviews</title><categories>cs.IR</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online reviews of businesses have become increasingly important in recent
years, as customers and even competitors use them to judge the quality of a
business. Yelp is one of the most popular websites for users to write such
reviews, and it would be useful for them to be able to predict the sentiment or
even the star rating of a review. In this paper, we develop two classifiers to
perform positive/negative classification and 5-star classification. We use
Naive Bayes, Support Vector Machines, and Logistic Regression as models, and
achieved the best accuracy with Logistic Regression: 92.90% for
positive/negative classification, and 63.92% for 5-star classification. These
results demonstrate the quality of the Logistic Regression model using only the
text of the review, yet there is a promising opportunity for improvement with
more data, more features, and perhaps different models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06307</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06307</id><created>2015-12-19</created><authors><author><keyname>Arachchilage</keyname><forenames>Nalin Asanka Gamagedara</forenames></author><author><keyname>Namiluko</keyname><forenames>Cornelius</forenames></author><author><keyname>Martin</keyname><forenames>Andrew</forenames></author></authors><title>Developing a Trust Domain Taxonomy for Securely Sharing Information
  Among Others</title><categories>cs.CR</categories><comments>10, International Journal for Information Security Research (IJISR),
  Volume 3 Issues 1/2, ISSN 2042-4639 (2013). arXiv admin note: text overlap
  with arXiv:1511.04541</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In any given collaboration, information needs to flow from one participant to
another. While participants may be interested in sharing information with one
another, it is often necessary for them to establish the impact of sharing
certain kinds of information. This is because certain information could have
detrimental effects when it ends up in wrong hands. For this reason, any
would-be participant in a collaboration may need to establish the guarantees
that the collaboration provides, in terms of protecting sensitive information,
before joining the collaboration as well as evaluating the impact of sharing a
given piece of information with a given set of entities. The concept of a trust
domains aims at managing trust-related issues in information sharing. It is
essential for enabling efficient collaborations. Therefore, this research
attempts to develop a taxonomy for trust domains with measurable trust
characteristics, which provides security-enhanced, distributed containers for
the next generation of composite electronic services for supporting
collaboration and data exchange within and across multiple organisations. Then
the developed taxonomy is applied to possible scenarios (e.g. Health Care
Service Scenario and ConfiChair Scenario), in which the concept of trust
domains could be useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06314</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06314</id><created>2015-12-19</created><authors><author><keyname>Leinster</keyname><forenames>Tom</forenames></author><author><keyname>Meckes</keyname><forenames>Mark W.</forenames></author></authors><title>Maximizing diversity in biology and beyond</title><categories>cs.IT math.IT q-bio.PE q-bio.QM</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy, under a variety of names, has long been used as a measure of
diversity in ecology, as well as in genetics, economics and other fields. There
is a spectrum of viewpoints on diversity, indexed by a real parameter q giving
greater or lesser importance to rare species. Leinster and Cobbold proposed a
one-parameter family of diversity measures taking into account both this
variation and the varying similarities between species. Because of this latter
feature, diversity is not maximized by the uniform distribution on species. So
it is natural to ask: which distributions maximize diversity, and what is its
maximum value?
  In principle, both answers depend on q, but our main theorem is that neither
does. Thus, there is a single distribution that maximizes diversity from all
viewpoints simultaneously, and any list of species has an unambiguous maximum
diversity value. Furthermore, the maximizing distribution(s) can be computed in
finite time, and any distribution maximizing diversity from some particular
viewpoint q &gt; 0 actually maximizes diversity for all q.
  Although we phrase our results in ecological terms, they apply very widely,
with applications in graph theory and metric geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06317</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06317</id><created>2015-12-19</created><authors><author><keyname>Lemanski</keyname><forenames>Matthew</forenames></author></authors><title>A Survey of Digital Privacy Rights Under CISA</title><categories>cs.CR cs.CY</categories><comments>CS6740 assignment; 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent passing of the Cybersecurity Information Sharing Act of 2015
introduces a new framework for information sharing between private and US
government entities with the expressed intent to identify cybersecurity
threats. This is the latest in a series of similar bills that have been
introduced to Congress over the last several years. While each of the previous
standalone bills were defeated following widespread public resistance, the
latest edition was included as an amendment to the United States' 2016 spending
bill. This means that any dissenting congressmen unwilling to pass the spending
bill with the CISA rider would be willing to risk another government shutdown
due to the inability to come to terms on the budget measures. This paper seeks
to explore the potential impacts of the measures introduced or enabled by CISA,
and consider the formalization of digital privacy rights in an increasingly
online and monitored world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06337</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06337</id><created>2015-12-20</created><authors><author><keyname>Wu</keyname><forenames>Dan</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Jiang</keyname><forenames>Longyu</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Kernel principal component analysis network for image classification</title><categories>cs.LG cs.CV</categories><comments>7 pages, 1 figure, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to classify the nonlinear feature with linear classifier and improve
the classification accuracy, a deep learning network named kernel principal
component analysis network (KPCANet) is proposed. First, mapping the data into
higher space with kernel principal component analysis to make the data linearly
separable. Then building a two-layer KPCANet to obtain the principal components
of image. Finally, classifying the principal components with linearly
classifier. Experimental results show that the proposed KPCANet is effective in
face recognition, object recognition and hand-writing digits recognition, it
also outperforms principal component analysis network (PCANet) generally as
well. Besides, KPCANet is invariant to illumination and stable to occlusion and
slight deformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06338</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06338</id><created>2015-12-20</created><updated>2016-01-04</updated><authors><author><keyname>Song</keyname><forenames>Yinglei</forenames></author></authors><title>Lower Bounds for the Domination Numbers of Connected Graphs without
  Short Cycles</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain lower bounds for the domination numbers of connected
graphs with girth at least $7$. We show that the domination number of a
connected graph with girth at least $7$ is either $1$ or at least
$\frac{1}{2}(3+\sqrt{8(m-n)+9})$, where $n$ is the number of vertices in the
graph and $m$ is the number of edges in the graph. For graphs with minimum
degree $2$ and girth at least $7$, the lower bound can be improved to
$\max{\{\sqrt{n}, \sqrt{\frac{2m}{3}}\}}$, where $n$ and $m$ are the numbers of
vertices and edges in the graph respectively. In cases where the graph is of
minimum degree $2$ and its girth $g$ is at least $12$, the lower bound can be
further improved to $\max{\{\sqrt{n}, \sqrt{\frac{\lfloor \frac{g}{3}
\rfloor-1}{3}m}\}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06348</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06348</id><created>2015-12-20</created><authors><author><keyname>Xu</keyname><forenames>Zhongqi</forenames></author><author><keyname>Pu</keyname><forenames>Cunlai</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>Link prediction based on path entropy</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory has been taken as a prospective tool for quantifying the
complexity of complex networks. In this paper, we first study the information
entropy or uncertainty of a path using the information theory. Then we apply
the path entropy to the link prediction problem in real-world networks.
Specifically, we propose a new similarity index, namely Path Entropy (PE)
index, which considers the information entropies of shortest paths between node
pairs with penalization to long paths. Empirical experiments demonstrate that
PE index outperforms the mainstream link predictors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06352</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06352</id><created>2015-12-20</created><updated>2016-01-11</updated><authors><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author></authors><title>Vector Network Coding Based on Subspace Codes Outperforms Scalar Linear
  Network Coding</title><categories>cs.IT math.IT</categories><comments>a shorter version submitted to ISIT2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers vector network coding based on rank-metric codes and
subspace codes. The main result of this paper is that vector network coding can
significantly reduce the required field size compared to scalar linear network
coding in the same multicast network. The achieved gap between the field size
of scalar and vector network coding is in the order of
$q^{(\ell-1)t^2/\ell}-q^t$, for any $q \geq 2$, where $t$ denotes the length of
the vectors in the vector solution, and the number of inputs is $2 \ell$, $\ell
\geq 2$. Previously, only a gap of constant size had been shown. This implies
also the same gap between the field size in linear and nonlinear scalar network
coding for multicast networking. Several networks are considered which are
variations of the well-known combination network. Further, for all these
networks, including the unmodified combination network, we show that our vector
coding solution reduces the decoding complexity at the receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06353</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06353</id><created>2015-12-20</created><authors><author><keyname>Gamzu</keyname><forenames>Iftah</forenames></author><author><keyname>Segev</keyname><forenames>Danny</forenames></author></authors><title>A Polynomial-Time Approximation Scheme for The Airplane Refueling
  Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the airplane refueling problem which was introduced by the
physicists Gamow and Stern in their classical book Puzzle-Math (1958). Sticking
to the original story behind this problem, suppose we have to deliver a bomb in
some distant point of the globe, the distance being much greater than the range
of any individual airplane at our disposal. Therefore, the only feasible option
to carry out this mission is to better utilize our fleet via mid-air refueling.
Starting with several airplanes that can refuel one another, and gradually drop
out of the flight until the single plane carrying the bomb reaches the target,
how would you plan the refueling policy?
  The main contribution of Gamow and Stern was to provide a complete
characterization of the optimal refueling policy for the special case of
identical airplanes. In spite of their elegant and easy-to-analyze solution,
the computational complexity of the general airplane refueling problem, with
arbitrary tank volumes and consumption rates, has remained widely open ever
since, as recently pointed out by Woeginger (Open Problems in Scheduling,
Dagstuhl 2010, page 24). To our knowledge, other than a logarithmic
approximation, which can be attributed to folklore, it is not entirely obvious
even if constant-factor performance guarantees are within reach.
  In this paper, we propose a polynomial-time approximation scheme for the
airplane refueling problem in its utmost generality. Our approach builds on a
novel combination of ideas related to parametric pruning, efficient guessing
tricks, reductions to well-structured instances of generalized assignment, and
additional insight into how LP-rounding algorithms in this context actually
work. We complement this result by presenting a fast and easy-to-implement
algorithm that approximates the optimal refueling policy to within a constant
factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06362</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06362</id><created>2015-12-20</created><authors><author><keyname>Abdo</keyname><forenames>Nichola</forenames></author><author><keyname>Stachniss</keyname><forenames>Cyrill</forenames></author><author><keyname>Spinello</keyname><forenames>Luciano</forenames></author><author><keyname>Burgard</keyname><forenames>Wolfram</forenames></author></authors><title>Collaborative Filtering for Predicting User Preferences for Organizing
  Objects</title><categories>cs.RO cs.IR</categories><comments>Submission to The International Journal of Robotics Research.
  Relevant material can be found at
  http://www2.informatik.uni-freiburg.de/~abdon/task_preferences.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As service robots become more and more capable of performing useful tasks for
us, there is a growing need to teach robots how we expect them to carry out
these tasks. However, different users typically have their own preferences, for
example with respect to arranging objects on different shelves. As many of
these preferences depend on a variety of factors including personal taste,
cultural background, or common sense, it is challenging for an expert to
pre-program a robot in order to accommodate all potential users. At the same
time, it is impractical for robots to constantly query users about how they
should perform individual tasks. In this work, we present an approach to learn
patterns in user preferences for the task of tidying up objects in containers,
e.g., shelves or boxes. Our method builds upon the paradigm of collaborative
filtering for making personalized recommendations and relies on data from
different users that we gather using crowdsourcing. To deal with novel objects
for which we have no data, we propose a method that compliments standard
collaborative filtering by leveraging information mined from the Web. When
solving a tidy-up task, we first predict pairwise object preferences of the
user. Then, we subdivide the objects in containers by modeling a spectral
clustering problem. Our solution is easy to update, does not require complex
modeling, and improves with the amount of user data. We evaluate our approach
using crowdsourcing data from over 1,200 users and demonstrate its
effectiveness for two tidy-up scenarios. Additionally, we show that a real
robot can reliably predict user preferences using our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06372</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06372</id><created>2015-12-20</created><authors><author><keyname>Cordasco</keyname><forenames>Gennaro</forenames></author><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Rescigno</keyname><forenames>Adele A.</forenames></author><author><keyname>Vaccaro</keyname><forenames>Ugo</forenames></author></authors><title>Optimizing Spread of Influence in Weighted Social Networks via Partial
  Incentives</title><categories>cs.DS cs.SI math.CO physics.soc-ph</categories><comments>An extended abstract of a preliminary version of this paper appeared
  in: Proceedings of 22nd International Colloquium on Structural Information
  and Communication Complexity (SIROCCO 2015), Lectures Notes in Computer
  Science vol. 9439, C. Scheideler (Ed.), pp. 119-134, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A widely studied process of influence diffusion in social networks posits
that the dynamics of influence diffusion evolves as follows: Given a graph
$G=(V,E)$, representing the network, initially \emph{only} the members of a
given $S\subseteq V$ are influenced; subsequently, at each round, the set of
influenced nodes is augmented by all the nodes in the network that have a
sufficiently large number of already influenced neighbors. The general problem
is to find a small initial set of nodes that influences the whole network. In
this paper we extend the previously described basic model in the following
ways: firstly, we assume that there are non negative values $c(v)$ associated
to each node $v\in V$, measuring how much it costs to initially influence node
$v$, and the algorithmic problem is to find a set of nodes of \emph{minimum
total cost} that influences the whole network; successively, we study the
consequences of giving \emph{incentives} to member of the networks, and we
quantify how this affects (i.e., reduces) the total costs of starting process
that influences the whole network. For the two above problems we provide both
hardness and algorithmic results. We also experimentally validate our
algorithms via extensive simulations on real life networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06383</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06383</id><created>2015-12-20</created><authors><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author></authors><title>Tracking Angles of Departure and Arrival in a Mobile Millimeter Wave
  Channel</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, submitted to ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave provides a very promising approach for meeting the
ever-growing traffic demand in next generation wireless networks. To utilize
this band, it is crucial to obtain the channel state information in order to
perform beamforming and combining to compensate for severe path loss. In
contrast to lower frequencies, a typical millimeter wave channel consists of a
few dominant paths. Thus it is generally sufficient to estimate the path gains,
angles of departure (AoDs), and angles of arrival (AoAs) of those paths.
Proposed in this paper is a dual timescale model to characterize abrupt channel
changes (e.g., blockage) and slow variations of AoDs and AoAs. This work
focuses on tracking the slow variations and detecting abrupt changes. A Kalman
filter based tracking algorithm and an abrupt change detection method are
proposed. The tracking algorithm is compared with the adaptive algorithm due to
Alkhateeb, Ayach, Leus and Heath (2014) in the case with single radio frequency
chain. Simulation results show that to achieve the same tracking performance,
the proposed algorithm requires much lower signal-to-noise-ratio (SNR) and much
fewer pilots than the other algorithm. Moreover, the change detection method
can always detect abrupt changes with moderate number of pilots and SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06388</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06388</id><created>2015-12-20</created><authors><author><keyname>Wu</keyname><forenames>Xi</forenames></author><author><keyname>Fredrikson</keyname><forenames>Matthew</forenames></author><author><keyname>Wu</keyname><forenames>Wentao</forenames></author><author><keyname>Jha</keyname><forenames>Somesh</forenames></author><author><keyname>Naughton</keyname><forenames>Jeffrey F.</forenames></author></authors><title>Revisiting Differentially Private Regression: Lessons From Learning
  Theory and their Consequences</title><categories>cs.CR cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Private regression has received attention from both database and security
communities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzed
the functional mechanism (Zhang et al. VLDB 2012) for training linear
regression models over medical data. Unfortunately, they found that model
accuracy is already unacceptable with differential privacy when $\varepsilon =
5$. We address this issue, presenting an explicit connection between
differential privacy and stable learning theory through which a substantially
better privacy/utility tradeoff can be obtained. Perhaps more importantly, our
theory reveals that the most basic mechanism in differential privacy, output
perturbation, can be used to obtain a better tradeoff for all
convex-Lipschitz-bounded learning tasks. Since output perturbation is simple to
implement, it means that our approach is potentially widely applicable in
practice. We go on to apply it on the same medical data as used by Fredrikson
et al. Encouragingly, we achieve accurate models even for $\varepsilon = 0.1$.
In the last part of this paper, we study the impact of our improved
differentially private mechanisms on model inversion attacks, a privacy attack
introduced by Fredrikson et al. We observe that the improved tradeoff makes the
resulting differentially private model more susceptible to inversion attacks.
We analyze this phenomenon formally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06389</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06389</id><created>2015-12-20</created><updated>2016-01-15</updated><authors><author><keyname>Brown</keyname><forenames>Russell A.</forenames></author></authors><title>Building Balanced k-d Tree with MapReduce</title><categories>cs.DS</categories><comments>7 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original description of the k-d tree recognized that rebalancing
techniques, such as are used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is
necessary to obtain all of the data prior to building the tree then to build
the tree via recursive subdivision of the data. One algorithm for building a
balanced k-d tree finds the median of the data for each recursive subdivision
of the data and builds the tree in O(n log n) time. A new algorithm builds a
balanced k-d tree by presorting the data in each of k dimensions prior to
building the tree, then preserves the order of the k presorts during recursive
subdivision of the data and builds the tree in O(kn log n) time. This new
algorithm is amenable to execution via MapReduce and permits building and
searching a k-d tree that is represented as a distributed graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06395</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06395</id><created>2015-12-20</created><updated>2016-02-03</updated><authors><author><keyname>Kargar</keyname><forenames>Mehdi</forenames></author><author><keyname>Golab</keyname><forenames>Lukasz</forenames></author><author><keyname>Szlichta</keyname><forenames>Jaroslaw</forenames></author></authors><title>Effective Keyword Search in Graphs</title><categories>cs.DB</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword search in node-labeled graphs finds subtrees of the graph whose nodes
contain all of the input keywords. Previous work ranks answer trees using
combinations of structural and content-based metrics, such as path lengths
between keywords or relevance of the labels in the answer tree to the query
keywords. We propose two new ways to rank keyword search results over graphs.
The first takes node importance into account while the second is a bi-objective
optimization of edge weights and node importance. Since both of these problems
are NP-hard, we propose greedy algorithms to solve them, and experimentally
verify their effectiveness and efficiency on a real dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06404</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06404</id><created>2015-12-20</created><updated>2016-01-11</updated><authors><author><keyname>Combi</keyname><forenames>Carlo</forenames></author><author><keyname>Vigan&#xf3;</keyname><forenames>Luca</forenames></author><author><keyname>Zavatteri</keyname><forenames>Matteo</forenames></author></authors><title>Security Constraints in Temporal Role-Based Access-Controlled Workflows
  (Extended Version)</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Workflows and role-based access control models need to be suitably merged, in
order to allow users to perform processes in a correct way, according to the
given data access policies and the temporal constraints. Given a mapping
between workflow models and simple temporal networks with uncertainty, we
discuss a mapping between role temporalities and simple temporal networks, and
how to connect the two resulting networks to make explicit who can do what,
when. If the connected network is still executable, we show how to compute the
set of authorized users for each task. Finally, we define security constraints
(to prevent users from doing unauthorized actions) and security constraint
propagation rules (to propagate security constraints at runtime). We also
provide an algorithm to check whether a set of propagation rules is safe, and
we extend an existing execution algorithm to take into account these new
security aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06423</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06423</id><created>2015-12-20</created><authors><author><keyname>Jajodia</keyname><forenames>Sushil</forenames></author><author><keyname>Litwin</keyname><forenames>Witold</forenames></author><author><keyname>Schwarz</keyname><forenames>Thomas</forenames></author></authors><title>On-the fly AES Decryption/Encryption for Cloud SQL Databases</title><categories>cs.DB cs.CR</categories><comments>12 pages</comments><report-no>Lamsade Res. Report 06-15-2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the client-side AES256 encryption for a cloud SQL DB. A column
ciphertext is deterministic or probabilistic. We trust the cloud DBMS for
security of its run-time values, e.g., through a moving target defense. The
client may send AES key(s) with the query. These serve the on-the-fly
decryption of selected ciphertext into plaintext for query evaluation. The DBMS
clears the key(s) and the plaintext at the query end at latest. It may deliver
ciphertext to decryption enabled clients or plaintext otherwise, e.g., to
browsers/navigators. The scheme functionally offers to a cloud DBMS
capabilities of a plaintext SQL DBMS. AES processing overhead appears
negligible for a modern CPU, e.g., a popular Intel I5. The determin-istic
encryption may have no storage overhead. The probabilistic one doubles the DB
storage. The scheme seems the first generally practical for an outsourced
encrypted SQL DB. An implementation sufficient to practice with appears easy.
An existing cloud SQL DBMS with UDF support should do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06425</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06425</id><created>2015-12-20</created><authors><author><keyname>Shafique</keyname><forenames>Muhammad</forenames></author></authors><title>Content-based Multi-path Routing in Structured Cyclic Overlays</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acyclic overlays used in broker-based Publish/subscribe (PS) systems provide
only one path for content-based routing between publishers and subscribers.
This poses serious challenges in handling network conditions like congestion,
and link or broker failures. A cyclic overlay may provide multiple paths
between publishers and subscribers; however, there is always one subscription
tree available for sending a notification to an interested subscriber, which
makes content-based dynamic routing a difficult task. This paper introduces a
structured cyclic topology that provides multiple paths between publishers and
subscribers. The subscription forwarding algorithm exploits the structured
nature of the proposed overlay topology and uses a clustering technique to
generate shortest-lengths subscription trees without generating duplicate
messages. We implemented static and intra-cluster dynamic routing algorithms in
the proposed overlay topology for our subscription-based PS system, called
Octopi. Experiments on a cluster testbed show that our approach generates fewer
inter-broker messages, and is scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06427</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06427</id><created>2015-12-20</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards Integrated Glance To Restructuring in Combinatorial Optimization</title><categories>cs.AI cs.DS cs.SY math.OC</categories><comments>31 pages, 34 figures, 10 tables</comments><msc-class>68T20, 93A13, 93B51, 90Bxx, 90B50</msc-class><acm-class>I.2.8; J.6; K.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on a new class of combinatorial problems which consists in
restructuring of solutions (as sets/structures) in combinatorial optimization.
Two main features of the restructuring process are examined: (i) a cost of the
restructuring, (ii) a closeness to a goal solution. Three types of the
restructuring problems are under study: (a) one-stage structuring, (b)
multi-stage structuring, and (c) structuring over changed element set.
One-criterion and multicriteria problem formulations can be considered. The
restructuring problems correspond to redesign (improvement, upgrade) of modular
systems or solutions. The restructuring approach is described and illustrated
(problem statements, solving schemes, examples) for the following combinatorial
optimization problems: knapsack problem, multiple choice problem, assignment
problem, spanning tree problems, clustering problem, multicriteria ranking
(sorting) problem, morphological clique problem. Numerical examples illustrate
the restructuring problems and solving schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06428</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06428</id><created>2015-12-20</created><updated>2015-12-28</updated><authors><author><keyname>Yu</keyname><forenames>Haoran</forenames></author><author><keyname>Cheung</keyname><forenames>Man Hon</forenames></author><author><keyname>Huang</keyname><forenames>Longbo</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Power-Delay Tradeoff with Predictive Scheduling in Integrated Cellular
  and Wi-Fi Networks</title><categories>cs.NI</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth of global mobile traffic has lead to a rapid growth in
the energy consumption in communication networks. In this paper, we focus on
the energy-aware design of the network selection, subchannel, and power
allocation in cellular and Wi-Fi networks, while taking into account the
traffic delay of mobile users. The problem is particularly challenging due to
the two-timescale operations for the network selection (large timescale) and
subchannel and power allocation (small timescale). Based on the two-timescale
Lyapunov optimization technique, we first design an online Energy-Aware Network
Selection and Resource Allocation (ENSRA) algorithm. The ENSRA algorithm yields
a power consumption within O(1/V) bound of the optimal value, and guarantees an
O(V) traffic delay for any positive control parameter V. Motivated by the
recent advancement in the accurate estimation and prediction of user mobility,
channel conditions, and traffic demands, we further develop a novel predictive
Lyapunov optimization technique to utilize the predictive information, and
propose a Predictive Energy-Aware Network Selection and Resource Allocation
(P-ENSRA) algorithm. We characterize the performance bounds of P-ENSRA in terms
of the power-delay tradeoff theoretically. To reduce the computational
complexity, we finally propose a Greedy Predictive Energy-Aware Network
Selection and Resource Allocation (GP-ENSRA) algorithm, where the operator
solves the problem in P-ENSRA approximately and iteratively. Numerical results
show that GP-ENSRA significantly improves the power-delay performance over
ENSRA in the large delay regime. For a wide range of system parameters,
GP-ENSRA reduces the traffic delay over ENSRA by 20~30% under the same power
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06429</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06429</id><created>2015-12-20</created><authors><author><keyname>Calmon</keyname><forenames>Flavio P.</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author></authors><title>Strong Data Processing Inequalities for Input Constrained Additive Noise
  Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper quantifies the intuitive observation that adding noise reduces
available information by means of non-linear strong data processing
inequalities. Consider the random variables $W\to X\to Y$ forming a Markov
chain, where $Y=X+Z$ with $X$ and $Z$ real-valued, independent and $X$ bounded
in $L_p$-norm. It is shown that $I(W;Y) \le F_I(I(W;X))$ with $F_I(t)&lt;t$
whenever $t&gt;0$, if and only if $Z$ has a density whose support is not disjoint
from any translate of itself.
  A related question is to characterize for what couplings $(W,X)$ the mutual
information $I(W;Y)$ is close to maximum possible. To that end we show that in
order to saturate the channel, i.e. for $I(W;Y)$ to approach capacity, it is
mandatory that $I(W;X)\to\infty$ (under suitable conditions on the channel). A
key ingredient for this result is a deconvolution lemma which shows that
post-convolution total variation distance bounds the pre-convolution
Kolmogorov-Smirnov distance.
  Explicit bounds are provided for the special case of the additive Gaussian
noise channel with quadratic cost constraint. These bounds are shown to be
order-optimal. For this case simplified proofs are provided leveraging
Gaussian-specific tools such as the connection between information and
estimation (I-MMSE) and Talagrand's information-transportation inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06430</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06430</id><created>2015-12-20</created><authors><author><keyname>Khan</keyname><forenames>Muhammad R.</forenames></author><author><keyname>Manoj</keyname><forenames>Johua</forenames></author><author><keyname>Singh</keyname><forenames>Anikate</forenames></author><author><keyname>Blumenstock</keyname><forenames>Joshua</forenames></author></authors><title>Behavioral Modeling for Churn Prediction: Early Indicators and Accurate
  Predictors of Custom Defection and Loyalty</title><categories>cs.LG</categories><doi>10.1109/BigDataCongress.2015.107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Churn prediction, or the task of identifying customers who are likely to
discontinue use of a service, is an important and lucrative concern of firms in
many different industries. As these firms collect an increasing amount of
large-scale, heterogeneous data on the characteristics and behaviors of
customers, new methods become possible for predicting churn. In this paper, we
present a unified analytic framework for detecting the early warning signs of
churn, and assigning a &quot;Churn Score&quot; to each customer that indicates the
likelihood that the particular individual will churn within a predefined amount
of time. This framework employs a brute force approach to feature engineering,
then winnows the set of relevant attributes via feature selection, before
feeding the final feature-set into a suite of supervised learning algorithms.
Using several terabytes of data from a large mobile phone network, our method
identifies several intuitive - and a few surprising - early warning signs of
churn, and our best model predicts whether a subscriber will churn with 89.4%
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06432</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06432</id><created>2015-12-20</created><updated>2015-12-25</updated><authors><author><keyname>Yao</keyname><forenames>Zhihao</forenames></author><author><keyname>Papapanagiotou</keyname><forenames>Ioannis</forenames></author><author><keyname>Griffith</keyname><forenames>Rean</forenames></author></authors><title>Serifos: Workload Consolidation and Load Balancing for SSD Based Cloud
  Storage Systems</title><categories>cs.DC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving high performance in virtualized data centers requires both
deploying high throughput storage clusters, i.e. based on Solid State Disks
(SSDs), as well as optimally consolidating the workloads across storage nodes.
Nowadays, the only practical solution for cloud storage providers to offer
guaranteed performance is to grossly over-provision the storage nodes. The
current workload scheduling mechanisms used in production do not have the
intelligence to optimally allocate block storage volumes based on the
performance of SSDs. In this paper, we introduce Serifos, an autonomous
performance modeling and load balancing system designed for SSD-based cloud
storage. Serifos takes into account the characteristics of the SSD storage
units and constructs hardware dependent workload consolidation models. Thus
Serifos is able to predict the latency caused by workload interference and the
average latency of concurrent workloads. Furthermore, Serifos leverages an I/O
load balancing algorithm to dynamically balance the volumes across the cluster.
Experimental results indicate that Serifos consolidation model is able to
maintain the mean prediction error of around 10% for heterogeneous hardware. As
a result of Serifos load balancing, we found that the variance and the maximum
average latency are reduced by 82% and 52%, respectively. The supported Service
Level Objectives (SLOs) on the testbed improve 43% on average latency, 32% on
the maximum read and 63% on the maximum write latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06440</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06440</id><created>2015-12-20</created><authors><author><keyname>Boyac&#x131;</keyname><forenames>Arman</forenames></author><author><keyname>Ekim</keyname><forenames>T&#x131;naz</forenames></author><author><keyname>Shalom</keyname><forenames>Mordechai</forenames></author><author><keyname>Zaks</keyname><forenames>Shmuel</forenames></author></authors><title>Graphs of Edge-Intersecting and Non-Splitting One Bend Paths in a Grid</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The families EPT (resp. EPG) Edge Intersection Graphs of Paths in a tree
(resp. in a grid) are well studied graph classes. Recently we introduced the
graph classes Edge-Intersecting and Non-Splitting Paths in a Tree ENPT, and in
a Grid (ENPG). It was shown that ENPG contains an infinite hierarchy of
subclasses that are obtained by restricting the number of bends in the paths.
Motivated by this result, in this work we focus on one bend {ENPG} graphs. We
show that one bend ENPG graphs are properly included in two bend ENPG graphs.
We also show that trees and cycles are one bend ENPG graphs, and characterize
the split graphs and co-bipartite graphs that are one bend ENPG. We prove that
the recognition problem of one bend ENPG split graphs is NP-complete even in a
very restricted subfamily of split graphs. Last we provide a linear time
recognition algorithm for one bend ENPG co-bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06447</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06447</id><created>2015-12-20</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Leung</keyname><forenames>Christophe</forenames></author></authors><title>Botnets Drilling Away Privacy Infrastructure</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore various technologies and their roles in subverting
the privacy infrastructure of the Internet. We also provide mitigation
techniques on the attack vectors the technologies provide, and assess the
overall severity of these threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06448</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06448</id><created>2015-12-20</created><authors><author><keyname>Sajnani</keyname><forenames>Hitesh</forenames></author><author><keyname>Saini</keyname><forenames>Vaibhav</forenames></author><author><keyname>Svajlenko</keyname><forenames>Jeffrey</forenames></author><author><keyname>Roy</keyname><forenames>Chanchal K.</forenames></author><author><keyname>Lopes</keyname><forenames>Cristina V.</forenames></author></authors><title>SourcererCC: Scaling Code Clone Detection to Big Code</title><categories>cs.SE</categories><comments>Accepted for publication at ICSE'16 (preprint, unrevised)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite a decade of active research, there is a marked lack in clone
detectors that scale to very large repositories of source code, in particular
for detecting near-miss clones where significant editing activities may take
place in the cloned code. We present SourcererCC, a token-based clone detector
that targets three clone types, and exploits an index to achieve scalability to
large inter-project repositories using a standard workstation. SourcererCC uses
an optimized inverted-index to quickly query the potential clones of a given
code block. Filtering heuristics based on token ordering are used to
significantly reduce the size of the index, the number of code-block
comparisons needed to detect the clones, as well as the number of required
token-comparisons needed to judge a potential clone.
  We evaluate the scalability, execution time, recall and precision of
SourcererCC, and compare it to four publicly available and state-of-the-art
tools. To measure recall, we use two recent benchmarks, (1) a large benchmark
of real clones, BigCloneBench, and (2) a Mutation/Injection-based framework of
thousands of fine-grained artificial clones. We find SourcererCC has both high
recall and precision, and is able to scale to a large inter-project repository
(250MLOC) using a standard workstation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06452</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06452</id><created>2015-12-20</created><authors><author><keyname>Soleimani</keyname><forenames>Hossein</forenames></author><author><keyname>Miller</keyname><forenames>David J.</forenames></author></authors><title>ATD: Anomalous Topic Discovery in High Dimensional Discrete Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm for detecting patterns exhibited by anomalous
clusters in high dimensional discrete data. Unlike most anomaly detection (AD)
methods, which detect individual anomalies, our proposed method detects groups
(clusters) of anomalies; i.e. sets of points which collectively exhibit
abnormal patterns. In many applications this can lead to better understanding
of the nature of the atypical behavior and to identifying the sources of the
anomalies. Moreover, we consider the case where the atypical patterns exhibit
on only a small (salient) subset of the very high dimensional feature space.
Individual AD techniques and techniques that detect anomalies using all the
features typically fail to detect such anomalies, but our method can detect
such instances collectively, discover the shared anomalous patterns exhibited
by them, and identify the subsets of salient features. In this paper, we focus
on detecting anomalous topics in a batch of text documents, developing our
algorithm based on topic models. Results of our experiments show that our
method can accurately detect anomalous topics and salient features (words)
under each such topic in a synthetic data set and two real-world text corpora
and achieves better performance compared to both standard group AD and
individual AD techniques. All required code to reproduce our experiments is
available from https://github.com/hsoleimani/ATD
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06457</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06457</id><created>2015-12-20</created><updated>2016-01-21</updated><authors><author><keyname>Sizemore</keyname><forenames>Ann</forenames></author><author><keyname>Giusti</keyname><forenames>Chad</forenames></author><author><keyname>Bassett</keyname><forenames>Danielle</forenames></author></authors><title>Classification of weighted networks through mesoscale homological
  features</title><categories>math.CO cs.DM cs.SI</categories><comments>18 pages, 8 figures</comments><msc-class>55U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As complex networks find applications in a growing range of disciplines, the
diversity of naturally occurring and model networks being studied is exploding.
The adoption of a well-developed collection of network taxonomies is a natural
method for both organizing this data and understanding deeper relationships
between networks. Most existing metrics for network structure rely on classical
graph-theoretic measures, extracting characteristics primarily related to
individual vertices or paths between them, and thus classify networks from the
perspective of local features. Here, we describe an alternative approach to
studying structure in networks that relies on an algebraic-topological metric
called persistent homology, which studies intrinsically mesoscale structures
called cycles, constructed from cliques in the network. We present a
classification of 14 commonly studied weighted network models into four groups
or classes, and discuss the structural themes arising in each class. Finally,
we compute the persistent homology of two real-world networks and one network
constructed by a common dynamical systems model, and we compare the results
with the three classes to obtain a better understanding of those networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06466</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06466</id><created>2015-12-20</created><authors><author><keyname>Chen</keyname><forenames>Zhe</forenames></author></authors><title>Machine Ruling</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging technologies, such as big data, Internet of things, cloud computing,
mobile Internet, and robotics, breed and expedite new applications and fields.
In the mean while, the long-term prosperity and happiness of human race demands
advanced technologies. In this paper, the aforementioned emerging technologies
are applied to management and governance for the long-term prosperity and
happiness of human race. The term &quot;machine ruling&quot; is coined, introduced, and
justified. Moreover, the framework and architecture of machine ruling are
proposed. Enabling technologies and challenges are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06468</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06468</id><created>2015-12-20</created><authors><author><keyname>Wang</keyname><forenames>Sheng</forenames></author><author><keyname>Chu</keyname><forenames>Chunliang</forenames></author></authors><title>Geometry-covering Jammer Localization based on Distance Comprehension in
  Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jamming attacks could cause severe damage to Wireless Sensor Networks (WSNs).
Once jamming attack occurs, the most urgent work is to get the position
information of the jammer. Then safety measures to eliminate the jamming
effects can be devised. In this paper, the jammer localization is conducted by
geometric covering method to achieve a low energy consumption. And utilizing
the power of the jamming signal received by the boundary nodes, a compensating
method is composed to reduce the estimating error of the jamming area. At last
the localization is conducted by extracting the minimum covering circle of the
compensated victim area. Simulations are conducted to test the localization
accuracy with the impact of node density, jamming region and radius. Results
show that this localization method achieves both good precision and low energy
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06469</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06469</id><created>2015-12-20</created><authors><author><keyname>Bhattacharya</keyname><forenames>Prasanta</forenames></author><author><keyname>Phan</keyname><forenames>Tuan Q.</forenames></author><author><keyname>Bai</keyname><forenames>Xue</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo</forenames></author></authors><title>Investigating The Impact Of Network Effects On Content Generation:
  Evidence From A Large Online Student Network</title><categories>cs.SI physics.soc-ph stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of online social network sites (SNS), it has become
imperative for platform owners and online marketers to investigate what drives
content production on these platforms. However, previous research has found it
difficult to statistically model these factors using observational data due to
the inability to separate the effects of network formation from those of
network influence. The inability to successfully separate these two mechanisms
makes it difficult to interpret whether the observed behavior is a result of
peer influence or merely indicative of a selection bias due to homophily. In
this paper, we propose an actor-oriented continuous-time model to jointly
estimate the co-evolution of the users' social network structure and their
content production behavior using a Markov Chain Monte Carlo (MCMC) based
simulation approach. Specifically, we offer a method to analyze non-stationary
and continuous behavior with network effects, similar to what is observed in
social media ecosystems. Leveraging a unique dataset contributed by Facebook,
we apply our model to data on university students across six months to find
that users tend to connect with others that have similar posting behavior.
However, after doing so, users tend to diverge in posting behavior. Further, we
also discover that homophilous friend selection as well as susceptibility to
peer influence are sensitive to the strength of the posting behaviour. Our
results provide insights and recommendations for SNS platforms to sustain an
active and viable community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06473</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06473</id><created>2015-12-20</created><authors><author><keyname>Wu</keyname><forenames>Jiaxiang</forenames></author><author><keyname>Leng</keyname><forenames>Cong</forenames></author><author><keyname>Wang</keyname><forenames>Yuhang</forenames></author><author><keyname>Hu</keyname><forenames>Qinghao</forenames></author><author><keyname>Cheng</keyname><forenames>Jian</forenames></author></authors><title>Quantized Convolutional Neural Networks for Mobile Devices</title><categories>cs.CV</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, convolutional neural networks (CNN) have demonstrated impressive
performance in various computer vision tasks. However, high performance
hardware is typically indispensable for the application of CNN models due to
the high computation complexity, which prohibits their further extensions. In
this paper, we propose an efficient framework, namely Quantized CNN, to
simultaneously speed-up the computation and reduce the storage and memory
overhead of CNN models. Both filter kernels in convolutional layers and
weighting matrices in fully-connected layers are quantized, aiming at
minimizing the estimation error of each layer's response. Extensive experiments
on the ILSVRC-12 benchmark demonstrate $4 \sim 6 \times$ speed-up and $15 \sim
20 \times$ compression with merely one percentage loss of classification
accuracy. With our quantized CNN model, even mobile devices can accurately
classify images within one second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06474</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06474</id><created>2015-12-20</created><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Rekatsinas</keyname><forenames>Theodoros</forenames></author><author><keyname>Garcia-Molina</keyname><forenames>Hector</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Exploiting Features for Data Source Quality Estimation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating the quality of data sources in data fusion
settings. In contrast to existing models that rely only on conflicting
observations across sources to infer quality (internal signals), we propose a
data fusion model, called FUSE, that combines internal signals with external
data-source features. We show both theoretically and empirically, that FUSE
yields better quality estimates with rigorous guarantees; in contrast, models
which utilize only internal signals have weaker or no guarantees. We study
different approaches for learning FUSE's parameters, (i) empirical risk
minimization (ERM), which utilizes ground truth and relies on fast convex
optimization methods, and (ii) expectation maximization (EM), which assumes no
ground truth and uses slow iterative optimization procedures. EM is the
standard approach used in most existing methods. An implication of our
theoretical analysis is that features allow FUSE to obtain low-error estimates
with limited ground truth on the correctness of source observations. We study
the tradeoff between the statistical efficiency and the runtime of data fusion
models along two directions: (i) whether or not the model uses features (ii)
the amount of ground truth available. We empirically show that features allow
FUSE with ERM to obtain estimates of similar or better quality than
feature-less models, and also FUSE with EM, with only a few training examples
(in some cases as few as $50$) while being much faster; in our experiments we
observe speedups of $27\times$. We evaluate FUSE on real data and show that it
outperforms feature-less baselines, and can yield reductions of more than
$30\%$ in the source accuracy estimation error and improvements of more than
$10\%$ in the F1-score when resolving conflicts across sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06479</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06479</id><created>2015-12-20</created><authors><author><keyname>James</keyname><forenames>Ryan G.</forenames></author><author><keyname>Barnett</keyname><forenames>Nix</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Information Flows? A Critique of Transfer Entropies</title><categories>cond-mat.stat-mech cs.IT math.IT math.ST nlin.AO q-bio.MN stat.TH</categories><comments>6 pages, 2 figures; http://csc.ucdavis.edu/~cmg/compmech/pubs/if.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central task in analyzing complex dynamics is to determine the loci of
information storage and the communication topology of information flows within
a system. Over the last decade and a half, diagnostics for the latter have come
to be dominated by the transfer entropy. Via straightforward examples, we show
that it and a derivative quantity, the causation entropy, do not, in fact,
quantify the flow of information. At one and the same time they can
overestimate flow or underestimate influence. We isolate why this is the case
and propose alternate measures for information flow. An auxiliary consequence
reveals that the proliferation of networks as a now-common theoretical model
for large-scale systems in concert with the use of transfer-like entropies has
shoehorned dyadic relationships into our structural interpretation of the
organization and behavior of complex systems, despite the occurrence of
polyadic dependencies. The net result is that much of the sophisticated
organization of complex systems goes undetected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06488</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06488</id><created>2015-12-20</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Hirschberg</keyname><forenames>Daniel S.</forenames></author></authors><title>From Discrepancy to Majority</title><categories>cs.DS</categories><comments>15 pages, 3 figures. Extended version of a paper to appear at LATIN
  2016</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to select an item with the majority color from $n$ two-colored
items, given access to the items only through an oracle that returns the
discrepancy of subsets of $k$ items. We use $n/\lfloor\tfrac{k}{2}\rfloor+O(k)$
queries, improving a previous method by De Marco and Kranakis that used
$n-k+k^2/2$ queries. We also prove a lower bound of $n/(k-1)-O(n^{1/3})$ on the
number of queries needed, improving a lower bound of $\lfloor n/k\rfloor$ by De
Marco and Kranakis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06492</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06492</id><created>2015-12-20</created><authors><author><keyname>Wang</keyname><forenames>Qifei</forenames></author><author><keyname>Kurillo</keyname><forenames>Gregorij</forenames></author><author><keyname>Ofli</keyname><forenames>Ferda</forenames></author><author><keyname>Bajcsy</keyname><forenames>Ruzena</forenames></author></authors><title>Remote Health Coaching System and Human Motion Data Analysis for
  Physical Therapy with Microsoft Kinect</title><categories>cs.CV cs.AI</categories><comments>6 pages, Computer Vision for Accessible and Affordable HealthCare
  Workshop (ICCV2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the recent progress we have made for the computer
vision technologies in physical therapy with the accessible and affordable
devices. We first introduce the remote health coaching system we build with
Microsoft Kinect. Since the motion data captured by Kinect is noisy, we
investigate the data accuracy of Kinect with respect to the high accuracy
motion capture system. We also propose an outlier data removal algorithm based
on the data distribution. In order to generate the kinematic parameter from the
noisy data captured by Kinect, we propose a kinematic filtering algorithm based
on Unscented Kalman Filter and the kinematic model of human skeleton. The
proposed algorithm can obtain smooth kinematic parameter with reduced noise
compared to the kinematic parameter generated from the raw motion data from
Kinect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06498</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06498</id><created>2015-12-21</created><updated>2015-12-23</updated><authors><author><keyname>Murthy</keyname><forenames>O. V. Ramana</forenames></author><author><keyname>Goecke</keyname><forenames>Roland</forenames></author></authors><title>Harnessing the Deep Net Object Models for Enhancing Human Action
  Recognition</title><categories>cs.CV</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1411.4006 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, the influence of objects is investigated in the scenario of
human action recognition with large number of classes. We hypothesize that the
objects the humans are interacting will have good say in determining the action
being performed. Especially, if the objects are non-moving, such as objects
appearing in the background, features such as spatio-temporal interest points,
dense trajectories may fail to detect them. Hence we propose to detect objects
using pre-trained object detectors in every frame statically. Trained Deep
network models are used as object detectors. Information from different layers
in conjunction with different encoding techniques is extensively studied to
obtain the richest feature vectors. This technique is observed to yield
state-of-the-art performance on HMDB51 and UCF101 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06499</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06499</id><created>2015-12-21</created><authors><author><keyname>Kuzuoka</keyname><forenames>Shigeaki</forenames></author></authors><title>On the Smooth Renyi Entropy and Variable-Length Source Coding Allowing
  Errors</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of variable-length source coding
allowing errors. The exponential moment of the codeword length is analyzed in
the non-asymptotic regime and in the asymptotic regime. Our results show that
the smooth Renyi entropy characterizes the optimal exponential moment of the
codeword length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06500</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06500</id><created>2015-12-21</created><authors><author><keyname>Wu</keyname><forenames>Gang</forenames></author><author><keyname>Feng</keyname><forenames>Ting-ting</forenames></author><author><keyname>Zhang</keyname><forenames>Li-jia</forenames></author><author><keyname>Yang</keyname><forenames>Meng</forenames></author></authors><title>Inexact Krylov Subspace Algorithms for Large Matrix Exponential
  Eigenproblem from Dimensionality Reduction</title><categories>math.NA cs.NA</categories><comments>24 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Matrix exponential discriminant analysis (EDA) is a generalized discriminant
analysis method based on matrix exponential. It can essentially overcome the
intrinsic difficulty of small sample size problem that exists in the classical
linear discriminant analysis (LDA). However, for data with high dimension, one
has to solve a large matrix exponential eigenproblem in this method, and the
time complexity is dominated by the computation of exponential of large
matrices. In this paper, we propose two inexact Krylov subspace algorithms for
solving the large matrix exponential eigenproblem effectively. The contribution
of this work is threefold. First, we consider how to compute matrix
exponential-vector products efficiently, which is the key step in the Krylov
subspace method. Second, we compare the discriminant analysis criterion of EDA
and that of LDA from a theoretical point of view. Third, we establish a
relationship between the accuracy of the approximate eigenvectors and the
distance to nearest neighbour classifier, and show why the matrix exponential
eigenproblem can be solved approximately in practice. Numerical experiments on
some real-world databases show superiority of our new algorithms over many
state-of-the-art algorithms for face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06502</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06502</id><created>2015-12-21</created><authors><author><keyname>Feng</keyname><forenames>Qi</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Ye</keyname><forenames>Zhihui</forenames></author><author><keyname>Zhang</keyname><forenames>Naitong</forenames></author></authors><title>Sparse Multipath Channel Estimation and Decoding for Broadband Vector
  OFDM Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector orthogonal frequency division multiplexing (V-OFDM) is a general
system that builds a bridge between OFDM and single-carrier frequency domain
equalization in terms of intersymbol interference and receiver complexity. In
this paper, we investigate the sparse multipath channel estimation and decoding
for broadband V-OFDM systems. Unlike the non-sparse channel estimation, sparse
channel estimation only needs to recover the nonzero taps with reduced
complexity. Consider the pilot signals are transmitted through a sparse channel
that has only a few nonzero taps with and without additive white Gaussian
noise, respectively. The exactly and approximately sparse inverse fast Fourier
transform (SIFFT) can be employed for these two cases. The SIFFT-based
algorithm recovers the nonzero channel coefficients and their corresponding
coordinates directly, which is significant to the proposed partial intersection
sphere (PIS) decoding approach. Unlike the maximum likelihood (ML) decoding
that enumerates symbol constellation and estimates the transmitted symbols with
the minimum distance, the PIS decoding first generates the set of possible
transmitted symbols and then chooses the transmitted symbols only from this set
with the minimum distance. The diversity order of the PIS decoding is
determined by not only the number of nonzero taps, but also the coordinates of
nonzero taps, and the bit error rate (BER) is also influenced by vector block
size to some extent but roughly independent of the maximum time delay.
Simulation results indicate that by choosing appropriate sphere radius, the BER
performance of the PIS decoding outperforms the conventional zero-forcing
decoding and minimum mean square error decoding, and approximates to the ML
decoding with the increase of signal-to-noise ratio, but reduces the
computational complexity significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06520</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06520</id><created>2015-12-21</created><authors><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author></authors><title>Fast Operations on Linearized Polynomials and their Applications in
  Coding Theory</title><categories>cs.SC cs.IT math.IT</categories><comments>30 pages, submitted to Journal of Symbolic Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers fast algorithms for operations on linearized
polynomials. Among these results are fast algorithms for division of linearized
polynomials, $q$-transform, multi-point evaluation, computing minimal subspace
polynomials and interpolation. The complexity of all these operations is now
sub-quadratic in the $q$-degree of the linearized polynomials, while being at
least quadratic before. This leads to the first error and erasure decoding
algorithm for Gabidulin codes with sub-quadratic complexity. Moreover, we show
how close our results are to an optimal solution. In particular, we prove that
a quasi-linear algorithm for multiplying two linearized polynomials is highly
unlikely to exist since it is equivalent to matrix multiplication with
quadratic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06528</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06528</id><created>2015-12-21</created><updated>2015-12-22</updated><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Chen</keyname><forenames>Hsiao-Hwa</forenames></author></authors><title>Secrecy Wireless Information and Power Transfer: Challenges and
  Opportunities</title><categories>cs.IT math.IT</categories><comments>19 pages, 6 figures, IEEE Wireless Communications, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless information and power transfer (WIPT) enables more sustainable and
resilient communications owing to the fact that it avoids frequent battery
charging and replacement. However, it also suffers from possible information
interception due to the open nature of wireless channels. Compared to
traditional secure communications, secrecy wireless information and power
transfer (SWIPT) carries several distinct characteristics. On one hand,
wireless power transfer may increase the vulnerability of eavesdropping, since
a power receiver, as a potential eavesdropper, usually has a shorter access
distance than an information receiver. On the other hand, wireless power
transfer can be exploited to enhance wireless security. This article reviews
the security issues in various SWIPT scenarios, with an emphasis on revealing
the corresponding challenges and opportunities for implementing SWIPT.
Furthermore, we provide a survey on a variety of physical layer security
techniques to improve secrecy performance. In particular, we propose to use
massive multiple-input multiple-output (MIMO) techniques to enhance power
transfer efficiency and secure information transmission simultaneously.
Finally, we discuss several potential research directions to further enhance
the security in SWIPT systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06532</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06532</id><created>2015-12-21</created><authors><author><keyname>Lamali</keyname><forenames>Mohamed Lamine</forenames><affiliation>PRISM</affiliation></author><author><keyname>Pouyllau</keyname><forenames>H&#xe9;lia</forenames><affiliation>PRISM</affiliation></author><author><keyname>Barth</keyname><forenames>Dominique</forenames><affiliation>PRISM</affiliation></author></authors><title>Path computation in multi-layer multi-domain networks: A language
  theoretic approach</title><categories>cs.DS cs.FL cs.NI</categories><comments>Journal on Computer Communications, 2013</comments><proxy>ccsd</proxy><doi>10.1016/j.comcom.2012.11.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-layer networks are networks in which several protocols may coexist at
different layers. The Pseudo-Wire architecture provides encapsulation and
de-capsulation functions of protocols over Packet-Switched Networks. In a
multi-domain context, computing a path to support end-to-end services requires
the consideration of encapsulation and decapsulation capabilities. It appears
that graph models are not expressive enough to tackle this problem. In this
paper, we propose a new model of heterogeneous networks using Automata Theory.
A network is modeled as a Push-Down Automaton (PDA) which is able to capture
the encapsulation and decapsulation capabilities, the PDA stack corresponding
to the stack of encapsulated protocols. We provide polynomial algorithms that
compute the shortest path either in hops or in the number of encapsulations and
decapsulations along the inter-domain path, the latter reducing manual
configurations and possible loops in the path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06539</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06539</id><created>2015-12-21</created><authors><author><keyname>Tadano</keyname><forenames>Ryuichi</forenames></author><author><keyname>Pediredla</keyname><forenames>Adithya Kumar</forenames></author><author><keyname>Mitra</keyname><forenames>Kaushik</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author></authors><title>Spatial Phase-Sweep: Increasing temporal resolution of transient imaging
  using a light source array</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transient imaging or light-in-flight techniques capture the propagation of an
ultra-short pulse of light through a scene, which in effect captures the
optical impulse response of the scene. Recently, it has been shown that we can
capture transient images using commercially available Time-of-Flight (ToF)
systems such as Photonic Mixer Devices (PMD). In this paper, we propose
`spatial phase-sweep', a technique that exploits the speed of light to increase
the temporal resolution beyond the 100 picosecond limit imposed by current
electronics. Spatial phase-sweep uses a linear array of light sources with
spatial separation of about 3 mm between them, thereby resulting in a time
shift of about 10 picoseconds, which translates into 100 Gfps of transient
imaging in theory. We demonstrate a prototype and transient imaging results
using spatial phase-sweep.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06559</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06559</id><created>2015-12-21</created><authors><author><keyname>Favali</keyname><forenames>Marta</forenames></author><author><keyname>Abbasi-Sureshjani</keyname><forenames>Samaneh</forenames></author><author><keyname>Romeny</keyname><forenames>Bart ter Haar</forenames></author><author><keyname>Sarti</keyname><forenames>Alessandro</forenames></author></authors><title>Analysis of Vessel Connectivities in Retinal Images by Cortically
  Inspired Spectral Clustering</title><categories>cs.CV</categories><comments>submitted to JMIV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retinal images provide early signs of diabetic retinopathy, glaucoma and
hypertension. These signs can be investigated based on microaneurysms or
smaller vessels. The diagnostic biomarkers are the change of vessel widths and
angles especially at junctions, which are investigated using the vessel
segmentation or tracking. Vessel paths may also be interrupted, crossings and
bifurcations may be disconnected. This paper addresses a novel contextual
method based on the geometry of the primary visual cortex (V1) to study these
difficulties. We have analysed the specific problems at junctions with a
connectivity kernel obtained as the fundamental solution of the Fokker-Planck
equation, which is usually used to represent the geometrical structure of
multi-orientation cortical connectivity. By using the spectral clustering on a
large local affinity matrix constructed by both the connectivity kernel and the
feature of intensity, the vessels are identified successfully in a hierarchical
topology each representing an individual perceptual unit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06566</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06566</id><created>2015-12-21</created><authors><author><keyname>Favali</keyname><forenames>Marta</forenames></author><author><keyname>Citti</keyname><forenames>Giovanna</forenames></author><author><keyname>Sarti</keyname><forenames>Alessandro</forenames></author></authors><title>Local and global gestalt laws: A neurally based spectral approach</title><categories>cs.CV</categories><comments>submitted to Neural Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mathematical model of figure-ground articulation is presented, taking into
account both local and global gestalt laws. The model is compatible with the
functional architecture of the primary visual cortex (V1). Particularly the
local gestalt law of good continuity is described by means of suitable
connectivity kernels, that are derived from Lie group theory and are neurally
implemented in long range connectivity in V1. Different kernels are compatible
with the geometric structure of cortical connectivity and they are derived as
the fundamental solutions of the Fokker Planck, the Sub-Riemannian Laplacian
and the isotropic Laplacian equations. The kernels are used to construct
matrices of connectivity among the features present in a visual stimulus.
Global gestalt constraints are then introduced in terms of spectral analysis of
the connectivity matrix, showing that this processing can be cortically
implemented in V1 by mean field neural equations. This analysis performs
grouping of local features and individuates perceptual units with the highest
saliency. Numerical simulations are performed and results are obtained applying
the technique to a number of stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06578</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06578</id><created>2015-12-21</created><authors><author><keyname>Qin</keyname><forenames>Bo</forenames></author><author><keyname>Deng</keyname><forenames>Hua</forenames></author><author><keyname>Wu</keyname><forenames>Qianhong</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Naccache</keyname><forenames>David</forenames></author><author><keyname>Zhou</keyname><forenames>Yunya</forenames></author></authors><title>Flexible Attribute-Based Encryption Applicable to Secure E-Healthcare
  Records</title><categories>cs.CR</categories><journal-ref>International Journal of Information Security, Vol. 14, no. 6, pp.
  499-511, 2015</journal-ref><doi>10.1007/s10207-014-0272-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In e-healthcare record systems (EHRS), attribute-based encryption (ABE)
appears as a natural way to achieve fine-grained access control on health
records. Some proposals exploit key-policy ABE (KP-ABE) to protect privacy in
such a way that all users are associated with specific access policies and only
the ciphertexts matching the users' access policies can be decrypted. An issue
with KP-ABE is that it requires an a priori formulation of access policies
during key generation, which is not always practicable in EHRS because the
policies to access health records are sometimes determined after key
generation. In this paper, we revisit KPABE and propose a dynamic ABE paradigm,
referred to as access policy redefinable ABE (APR-ABE). To address the above
issue, APR-ABE allows users to redefine their access policies and delegate keys
for the redefined ones; hence a priori precise policies are no longer
mandatory. We construct an APR-ABE scheme with short ciphertexts and prove its
full security in the standard model under several static assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06581</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06581</id><created>2015-12-21</created><authors><author><keyname>Xu</keyname><forenames>P.</forenames></author><author><keyname>Wu</keyname><forenames>Q.</forenames></author><author><keyname>Wang</keyname><forenames>W.</forenames></author><author><keyname>Susilo</keyname><forenames>W.</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>J.</forenames></author><author><keyname>Jin</keyname><forenames>H.</forenames></author></authors><title>Generating Searchable Public-Key Ciphertexts with Hidden Structures for
  Fast Keyword Search</title><categories>cs.CR</categories><journal-ref>IEEE Transactions on Information Forensics and Security, Vol. 10,
  no. 9, pp. 1993-2006, 2015</journal-ref><doi>10.1109/TIFS.2015.2442220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing semantically secure public-key searchable encryption schemes take
search time linear with the total number of the ciphertexts. This makes
retrieval from large-scale databases prohibitive. To alleviate this problem,
this paper proposes Searchable Public-Key Ciphertexts with Hidden Structures
(SPCHS) for keyword search as fast as possible without sacrificing semantic
security of the encrypted keywords. In SPCHS, all keyword-searchable
ciphertexts are structured by hidden relations, and with the search trapdoor
corresponding to a keyword, the minimum information of therelations is
disclosed to a search algorithm as the guidance to find all matching
ciphertexts efficiently. We construct a simple SPCHS scheme from scratch in
which the ciphertexts have a hidden star-like structure. We prove our scheme to
be semantically secure based on the decisional bilinear Diffie-Hellman
assumption in the Random Oracle (RO) model. The search complexity of our scheme
is dependent on the actual number of the ciphertexts containing the queried
keyword, rather than the number of all ciphertexts. Finally, we present a
generic SPCHS construction from anonymous identity-based encryption and
collision-free full-identity malleable Identity-Based Key Encapsulation
Mechanism (IBKEM) with anonymity. We illustrate two collision-free
full-identity malleable IBKEM instances, which are semantically secure and
anonymous, respectively, in the RO and standard models. The latter instance
enables us to construct an SPCHS scheme with semantic security in the standard
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06585</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06585</id><created>2015-12-21</created><authors><author><keyname>Cui</keyname><forenames>Heng</forenames></author><author><keyname>Karame</keyname><forenames>Ghassan O.</forenames></author><author><keyname>Klaedtke</keyname><forenames>Felix</forenames></author><author><keyname>Bifulco</keyname><forenames>Roberto</forenames></author></authors><title>Fingerprinting Software-defined Networks</title><categories>cs.CR cs.NI</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-defined networking (SDN) eases network management by centralizing
the control plane and separating it from the data plane. The separation of
planes in SDN, however, introduces new vulnerabilities in SDN networks since
the difference in processing packets at each plane allows an adversary to
fingerprint the network's packet-forwarding logic. In this paper, we study the
feasibility of fingerprinting the controller-switch interactions by a remote
adversary, whose aim is to acquire knowledge about specific flow rules that are
installed at the switches. This knowledge empowers the adversary with a better
understanding of the network's packet-forwarding logic and exposes the network
to a number of threats. In our study, we collect measurements from hosts
located across the globe using a realistic SDN network comprising of OpenFlow
hardware and software switches. We show that, by leveraging information from
the RTT and packet-pair dispersion of the exchanged packets, fingerprinting
attacks on SDN networks succeed with overwhelming probability. We also show
that these attacks are not restricted to active adversaries, but can be equally
mounted by passive adversaries that only monitor traffic exchanged with the SDN
network. Finally, we discuss the implications of these attacks on the security
of SDN networks, and we present and evaluate an efficient countermeasure to
strengthen SDN networks against fingerprinting. Our results demonstrate the
effectiveness of our countermeasure in deterring fingerprinting attacks on SDN
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06593</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06593</id><created>2015-12-21</created><authors><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author><author><keyname>Setzer</keyname><forenames>Alexander</forenames></author><author><keyname>Strothmann</keyname><forenames>Thim</forenames></author></authors><title>Towards Establishing Monotonic Searchability in Self-Stabilizing Data
  Structures (full version)</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed applications are commonly based on overlay networks
interconnecting their sites so that they can exchange information. For these
overlay networks to preserve their functionality, they should be able to
recover from various problems like membership changes or faults. Various
self-stabilizing overlay networks have already been proposed in recent years,
which have the advantage of being able to recover from any illegal state, but
none of these networks can give any guarantees on its functionality while the
recovery process is going on. We initiate research on overlay networks that are
not only self-stabilizing but that also ensure that searchability is maintained
while the recovery process is going on, as long as there are no corrupted
messages in the system. More precisely, once a search message from node $u$ to
another node $v$ is successfully delivered, all future search messages from $u$
to $v$ succeed as well. We call this property monotonic searchability. We show
that in general it is impossible to provide monotonic searchability if
corrupted messages are present in the system, which justifies the restriction
to system states without corrupted messages. Furthermore, we provide a
self-stabilizing protocol for the line for which we can also show monotonic
searchability. It turns out that even for the line it is non-trivial to achieve
this property. Additionally, we extend our protocol to deal with node
departures in terms of the Finite Departure Problem of Foreback et. al (SSS
2014). This makes our protocol even capable of handling node dynamics.
  This is the full version of a correspondent paper published at OPODIS'15.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06612</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06612</id><created>2015-12-21</created><updated>2016-01-03</updated><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Yan</keyname><forenames>Rui</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Backward and Forward Language Modeling for Constrained Sentence
  Generation</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent language models, especially those based on recurrent neural networks
(RNNs), make it possible to generate natural language from a learned
probability. Language generation has wide applications including machine
translation, summarization, question answering, conversation systems, etc.
Existing methods typically learn a joint probability of words conditioned on
additional information, which is (either statically or dynamically) fed to
RNN's hidden layer. In many applications, we are likely to impose hard
constraints on the generated texts, i.e., a particular word must appear in the
sentence. Unfortunately, existing approaches could not solve this problem. In
this paper, we propose a novel backward and forward language model. Provided a
specific word, we use RNNs to generate previous words and future words, either
simultaneously or asynchronously, resulting in two model variants. In this way,
the given word could appear at any position in the sentence. Experimental
results show that the generated texts are comparable to sequential LMs in
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06618</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06618</id><created>2015-12-21</created><updated>2016-01-11</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author></authors><title>The Dispersion of Nearest-Neighbor Decoding for Additive Non-Gaussian
  Channels</title><categories>cs.IT math.IT</categories><comments>17 pages, 3 figures, Submitted to ISIT 2016 and to be presented at
  the 2016 International Zurich Seminar on Communications (IZS); Submitted to
  the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the second-order asymptotics of information transmission using
random Gaussian codebooks and nearest neighbor (NN) decoding over a
power-limited stationary memoryless additive non-Gaussian channel. We show that
the dispersion term depends on the non-Gaussian noise only through its second
and fourth moments. Furthermore, we characterize the second-order asymptotics
of point-to-point codes over Gaussian interference networks. Specifically, we
assume that each user's codebook is Gaussian and that NN decoding is employed,
i.e., that interference from unintended users is treated as noise at each
decoder. We show that whereas the first-order term in the asymptotic expansion
of the maximum number of messages depends on the power on the interfering
codewords only through their sum, this does not hold for the second-order term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06626</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06626</id><created>2015-12-21</created><authors><author><keyname>Jani</keyname><forenames>M.</forenames></author><author><keyname>Babolian</keyname><forenames>E.</forenames></author><author><keyname>Javadi</keyname><forenames>S.</forenames></author><author><keyname>Bhatta</keyname><forenames>D.</forenames></author></authors><title>Banded operational matrices for Bernstein polynomials and application to
  the fractional advection-dispersion equation</title><categories>math.NA cs.NA</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the papers dealing with derivation and applications of operational
matrices of Bernstein polynomials, a basis transformation, commonly a
transformation to power basis, is used. The main disadvantage of this method is
that the transformation may be ill-conditioned. Moreover, when applied to the
numerical simulation of a functional differential equation, it leads to dense
operational matrices and so a dense coefficient matrix is obtained. In this
paper, we present a new property for Bernstein polynomials. Using this
property, we build exact banded operational matrices for derivatives of
Bernstein polynomials. Next, as an application, we propose a new numerical
method based on a Petrov-Galerkin variational formulation and the new
operational matrices utilizing the dual Bernstein basis for the time-fractional
advection-dispersion equation. Finally, we show that the proposed method leads
to a narrow-banded linear system and so less computational effort is required
to obtain the desired accuracy for the approximate solution. Some numerical
examples are provided to demonstrate the efficiency of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06629</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06629</id><created>2015-12-21</created><authors><author><keyname>Javadi</keyname><forenames>S.</forenames></author><author><keyname>Babolian</keyname><forenames>E.</forenames></author><author><keyname>Jani</keyname><forenames>M.</forenames></author></authors><title>A numerical scheme for space-time fractional advection-dispersion
  equation</title><categories>math.NA cs.NA physics.comp-ph</categories><comments>Submitted to Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a numerical resolution of the space-time fractional
advection-dispersion equation. After time discretization, we utilize
collocation technique and implement a product integration method in order to
simplify the evaluation of the terms involving spatial fractional order
derivatives. Then utilizing Bernstein polynomials as basis, the problem is
transformed into a linear system of algebraic equations. Error analysis and
order of convergence for the proposed method are also discussed. Some numerical
experiments are presented to demonstrate the effectiveness of the proposed
method and to confirm the analytic results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06632</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06632</id><created>2015-12-21</created><updated>2016-01-01</updated><authors><author><keyname>Toffano</keyname><forenames>Zeno</forenames></author></authors><title>Eigenlogic in the spirit of George Boole</title><categories>cs.LO math.LO</categories><comments>12 pages, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an operational and geometric approach to logic. It starts
from the multilinear elective decomposition of boolean functions in the
original form introduced by George Boole. It is then shown that this algebraic
polynomial formulation can be naturally extended to operators in finite vector
spaces. Logical operators will appear as commuting projectors and the truth
values, which take the binary values {0,1}, are the respective eigenvalues. In
this view the solution of a logical proposition resulting from the operation on
a combination of arguments will appear as a selection where the outcome can
only be one of the eigenvalues. In this way propositional logic can be
formalised in linear algebra by using elective developments which correspond
here to combinations of tensored elementary projectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06633</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06633</id><created>2015-12-21</created><authors><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe</forenames></author><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Fremont</keyname><forenames>Daniel J.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author><author><keyname>Fried</keyname><forenames>Dror</forenames></author><author><keyname>Ivrii</keyname><forenames>Alexander</forenames></author><author><keyname>Malik</keyname><forenames>Sharad</forenames></author></authors><title>Constrained Sampling and Counting: Universal Hashing Meets SAT Solving</title><categories>cs.AI cs.LO</categories><comments>Appears in proceedings of AAAI-16 Workshop on Beyond NP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained sampling and counting are two fundamental problems in artificial
intelligence with a diverse range of applications, spanning probabilistic
reasoning and planning to constrained-random verification. While the theory of
these problems was thoroughly investigated in the 1980s, prior work either did
not scale to industrial size instances or gave up correctness guarantees to
achieve scalability. Recently, we proposed a novel approach that combines
universal hashing and SAT solving and scales to formulas with hundreds of
thousands of variables without giving up correctness guarantees. This paper
provides an overview of the key ingredients of the approach and discusses
challenges that need to be overcome to handle larger real-world instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06635</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06635</id><created>2015-12-21</created><authors><author><keyname>Golenberg</keyname><forenames>Konstantin</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author></authors><title>A Practically Efficient Algorithm for Generating Answers to Keyword
  Search over Data Graphs</title><categories>cs.DB</categories><comments>Full version of ICDT'16 paper</comments><acm-class>H.3.3; H.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In keyword search over a data graph, an answer is a non-redundant subtree
that contains all the keywords of the query. A naive approach to producing all
the answers by increasing height is to generalize Dijkstra's algorithm to
enumerating all acyclic paths by increasing weight. The idea of freezing is
introduced so that (most) non-shortest paths are generated only if they are
actually needed for producing answers. The resulting algorithm for generating
subtrees, called GTF, is subtle and its proof of correctness is intricate.
Extensive experiments show that GTF outperforms existing systems, even ones
that for efficiency's sake are incomplete (i.e., cannot produce all the
answers). In particular, GTF is scalable and performs well even on large data
graphs and when many answers are needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06637</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06637</id><created>2015-12-21</created><authors><author><keyname>B&#xed;r&#xf3;</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Barnaf&#xf6;ldi</keyname><forenames>Gergely G&#xe1;bor</forenames></author><author><keyname>Fut&#xf3;</keyname><forenames>Endre</forenames></author></authors><title>On the Way to Future's High Energy Particle Physics Transport Code</title><categories>cs.DC physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High Energy Physics (HEP) needs a huge amount of computing resources. In
addition data acquisition, transfer, and analysis require a well developed
infrastructure too. In order to prove new physics disciplines it is required to
higher the luminosity of the accelerator facilities, which produce
more-and-more data in the experimental detectors. Both testing new theories and
detector R&amp;D are based on complex simulations. Today have already reach that
level, the Monte Carlo detector simulation takes much more time than real data
collection. This is why speed up of the calculations and simulations became
important in the HEP community. The Geant Vector Prototype (GeantV) project
aims to optimize the most-used particle transport code applying parallel
computing and to exploit the capabilities of the modern CPU and GPU
architectures as well. With the maximized concurrency at multiple levels the
GeantV is intended to be the successor of the Geant4 particle transport code
that has been used since two decades successfully. Here we present our latest
result on the GeantV tests performances, comparing CPU/GPU based vectorized
GeantV geometrical code to the Geant4 version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06643</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06643</id><created>2015-12-21</created><authors><author><keyname>Saz</keyname><forenames>Oscar</forenames></author><author><keyname>Doulaty</keyname><forenames>Mortaza</forenames></author><author><keyname>Deena</keyname><forenames>Salil</forenames></author><author><keyname>Milner</keyname><forenames>Rosanna</forenames></author><author><keyname>Ng</keyname><forenames>Raymond W. M.</forenames></author><author><keyname>Hasan</keyname><forenames>Madina</forenames></author><author><keyname>Liu</keyname><forenames>Yulan</forenames></author><author><keyname>Hain</keyname><forenames>Thomas</forenames></author></authors><title>The 2015 Sheffield System for Transcription of Multi-Genre Broadcast
  Media</title><categories>cs.CL</categories><comments>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU
  2015), 13-17 Dec 2015, Scottsdale, Arizona, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the University of Sheffield system for participation in the 2015
Multi-Genre Broadcast (MGB) challenge task of transcribing multi-genre
broadcast shows. Transcription was one of four tasks proposed in the MGB
challenge, with the aim of advancing the state of the art of automatic speech
recognition, speaker diarisation and automatic alignment of subtitles for
broadcast media. Four topics are investigated in this work: Data selection
techniques for training with unreliable data, automatic speech segmentation of
broadcast media shows, acoustic modelling and adaptation in highly variable
environments, and language modelling of multi-genre shows. The final system
operates in multiple passes, using an initial unadapted decoding stage to
refine segmentation, followed by three adapted passes: a hybrid DNN pass with
input features normalised by speaker-based cepstral normalisation, another
hybrid stage with input features normalised by speaker feature-MLLR
transformations, and finally a bottleneck-based tandem stage with noise and
speaker factorisation. The combination of these three system outputs provides a
final error rate of 27.5% on the official development set, consisting of 47
multi-genre shows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06645</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06645</id><created>2015-12-21</created><authors><author><keyname>Wiese</keyname><forenames>Moritz</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panos</forenames></author></authors><title>Frequency hopping does not increase anti-jamming resilience of wireless
  channels</title><categories>cs.IT math.IT</categories><comments>A shorter version of this will be presented at the 2016 International
  Zurich Seminar on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effectiveness of frequency hopping for anti-jamming protection of
wireless channels is analyzed from an information-theoretic perspective. The
sender can input its symbols into one of several frequency subbands at a time.
Each subband channel is modeled as an additive noise channel. No common
randomness between sender and receiver is assumed. It is shown that capacity is
positive, and then equals the common randomness assisted (CR) capacity, if and
only if the sender power strictly exceeds the jammer power. Thus compared to
transmission over any fixed frequency subband, frequency hopping is not more
resilient towards jamming, but it does increase the capacity. Upper and lower
bounds on the CR capacity are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06649</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06649</id><created>2015-12-21</created><authors><author><keyname>Cambazard</keyname><forenames>Hadrien</forenames></author><author><keyname>Catusse</keyname><forenames>Nicolas</forenames></author></authors><title>Fixed-Parameter Algorithms for Rectilinear Steiner tree and Rectilinear
  Traveling Salesman Problem in the plane</title><categories>cs.DS cs.CG cs.DM</categories><comments>10 pages, 14 figures, 5 tables</comments><msc-class>Primary: 90C39, secondary: 90C27</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ points with their pairwise distances, the traveling
salesman problem (TSP) asks for a shortest tour that visits each point exactly
once. We consider the rectilinear instances of the TSP, i.e. TSP instances
where the points lie in the plane and the distance between two points is the
$l_1$ distance. We propose a fixed-parameter algorithm for the Rectilinear TSP
running in $O\left(nh7^h\right)$ where $h \leq n$ denotes the number of
horizontal lines containing the points of $P$. Our approach can be directly
applied to the problem of finding a shortest rectilinear Steiner tree that
interconnects the points of $P$. It provides a $O\left(nh5^h\right)$ algorithm
which improves significantly over the best known existing fixed-parameter
algorithm for rectilinear Steiner tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06657</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06657</id><created>2015-12-17</created><authors><author><keyname>Gao</keyname><forenames>Pu</forenames></author><author><keyname>Molloy</keyname><forenames>Michael</forenames></author></authors><title>Inside the clustering window for random linear equations</title><categories>cs.CC math.CO math.PR</categories><comments>23 pages. A major part of this paper has appeared in the preprint
  arXiv:1309.6651</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a random system of cn linear equations over n variables in GF(2),
where each equation contains exactly r variables; this is equivalent to
r-XORSAT. Previous work has established a clustering threshold, c^*_r for this
model: if c=c_r^*-\epsilon for any constant \epsilon&gt;0 then with high
probability all solutions form a well-connected cluster; whereas if
c=c^*_r+\epsilon, then with high probability the solutions partition into
well-connected, well-separated clusters (with probability tending to 1 as n
goes to infinity). This is part of a general clustering phenomenon which is
hypothesized to arise in most of the commonly studied models of random
constraint satisfaction problems, via sophisticated but mostly non-rigorous
techniques from statistical physics. We extend that study to the range
c=c^*_r+o(1), and prove that the connectivity parameters of the r-XORSAT
clusters undergo a smooth transition around the clustering threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06658</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06658</id><created>2015-12-21</created><authors><author><keyname>Zheng</keyname><forenames>Haitian</forenames></author><author><keyname>Fang</keyname><forenames>Lu</forenames></author><author><keyname>Ji</keyname><forenames>Mengqi</forenames></author><author><keyname>Strese</keyname><forenames>Matti</forenames></author><author><keyname>Ozer</keyname><forenames>Yigitcan</forenames></author><author><keyname>Steinbach</keyname><forenames>Eckehard</forenames></author></authors><title>Deep Learning for Surface Material Classification Using Haptic And
  Visual Information</title><categories>cs.RO cs.CV cs.LG</categories><comments>8 pages, under review as a paper at Transactions on Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a user scratches a hand-held rigid tool across an object surface, an
acceleration signal can be captured, which carries relevant information about
the surface. More importantly, such a haptic signal is complementary to the
visual appearance of the surface, which suggests the combination of both
modalities for the recognition of the surface material. In this paper, we
present a novel deep learning method dealing with the surface material
classification problem based on a Fully Convolutional Network (FCN), which
takes as input the aforementioned acceleration signal and a corresponding image
of the surface texture. Compared to previous surface material classification
solutions, which rely on a careful design of hand-crafted domain-specific
features, our method automatically extracts discriminative features utilizing
the advanced deep learning methodologies. Experiments performed on the TUM
surface material database demonstrate that our method achieves state-of-the-art
classification accuracy robustly and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06660</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06660</id><created>2015-12-21</created><authors><author><keyname>Honold</keyname><forenames>Thomas</forenames></author><author><keyname>Kiermaier</keyname><forenames>Michael</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Constructions and Bounds for Mixed-Dimension Subspace Codes</title><categories>math.CO cs.IT math.IT</categories><comments>28 pages, 2 tables</comments><msc-class>Primary 94B05, 05B25, 51E20, Secondary 51E14, 51E22, 51E23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes in finite projective spaces with the so-called subspace distance as
metric have been proposed for error control in random linear network coding.
The resulting \emph{Main Problem of Subspace Coding}is to determine the maximum
size $A_q(v,d)$ of a code in $\operatorname{PG}(v-1,\mathbb{F}_q)$ with minimum
subspace distance $d$. Here we completely resolve this problem for $d\ge v-1$.
For $d=v-2$ we present some improved bounds and determine $A_2(7,5)=34$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06667</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06667</id><created>2015-12-21</created><authors><author><keyname>Sahraei</keyname><forenames>Saeid</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Polynomially Solvable Instances of the Shortest and Closest Vector
  Problems with Applications to Compute-and-Forward</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A particular instance of the Shortest Vector Problem (SVP) appears in the
context of Compute-and-Forward. Despite the NP-hardness of the SVP, we will
show that this certain instance can be solved in complexity order
$O(n\psi\log(n\psi))$ where $\psi = \sqrt{P\|{\bf h}\|^2+1}$ depends on the
transmission power and the norm of the channel vector. We will then extend our
results to Integer-Forcing and finally, introduce a more general class of
lattices for which the SVP and the and the Closest Vector Problem (CVP) can be
approximated within a constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06678</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06678</id><created>2015-12-21</created><updated>2016-02-18</updated><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>Ooms</keyname><forenames>Aur&#xe9;lien</forenames></author></authors><title>Solving $k$-SUM using few linear queries</title><categories>cs.DS cs.CC cs.CG</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-SUM problem is given $n$ input real numbers to determine whether any
$k$ of them sum to zero. The problem is of tremendous importance in the
emerging field of complexity theory within $P$, and it is in particular open
whether it admits an algorithm of complexity $O(n^c)$ with $c&lt;\lceil
\frac{k}{2} \rceil$. Inspired by an algorithm due to Meiser (1993), we show
that there exist linear decision trees and algebraic computation trees of depth
$O(n^3\log^3 n)$ solving $k$-SUM. Furthermore, we show that there exists a
randomized algorithm that runs in $\tilde{O}(n^{\lceil \frac{k}{2} \rceil+8})$
time, and performs $O(n^3\log^3 n)$ linear queries on the input. Thus, we show
that it is possible to have an algorithm with a runtime almost identical (up to
the $+8$) to the best known algorithm but for the first time also with the
number of queries on the input a polynomial that is independent of $k$. The
$O(n^3\log^3 n)$ bound on the number of linear queries is also a tighter bound
than any known algorithm solving $k$-SUM, even allowing unlimited total time
outside of the queries. By simultaneously achieving few queries to the input
without significantly sacrificing runtime vis-\`{a}-vis known algorithms, we
deepen the understanding of this canonical problem which is a cornerstone of
complexity-within-$P$.
  We also consider a range of tradeoffs between the number of terms involved in
the queries and the depth of the decision tree. In particular, we prove that
there exist $o(n)$-linear decision trees of depth $o(n^4)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06682</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06682</id><created>2015-12-21</created><authors><author><keyname>Sahraei</keyname><forenames>Saeid</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>$K$ Users Caching Two Files: An Improved Achievable Rate</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is an approach to smoothen the variability of traffic over time.
Recently it has been proved that the local memories at the users can be
exploited for reducing the peak traffic in a much more efficient way than
previously believed. In this work we improve upon the existing results and
introduce a novel caching strategy that takes advantage of simultaneous coded
placement and coded delivery in order to decrease the worst case achievable
rate with $2$ files and $K$ users. We will show that for any cache size
$\frac{1}{K}&lt;M&lt;1$ our scheme outperforms the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06690</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06690</id><created>2015-12-21</created><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author></authors><title>Spectral Analysis of Quasi-Cyclic Product Codes</title><categories>cs.IT cs.DM math.CO math.IT math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a linear quasi-cyclic product code of two given
quasi-cyclic codes of relatively prime lengths over finite fields. We give the
spectral analysis of a quasi-cyclic product code in terms of the spectral
analysis of the row- and the column-code. Moreover, we provide a new lower
bound on the minimum Hamming distance of a given quasi-cyclic code and present
a new algebraic decoding algorithm.More specifically, we prove an explicit
(unreduced) basis of an l\_a l\_b-quasi-cyclic product code in terms of the
generator matrix in reduced Gr{\&quot;o}bner basis with respect to the
position-over-term order (RGB/POT) form of the l\_a-quasi-cyclic row- and the
l\_b-quasi-cyclic column-code, respectively. This generalizes the work of
Burton and Weldon for the generator polynomial of a cyclic product code (where
l\_a =l\_b=1). Furthermore, we derive the generator matrix in Pre-RGB/POT form
of an l\_a l\_b-quasi-cyclic product code for two special cases: (i) for l\_a=2
and l\_b=1, and (ii) if the row-code is a 1-level l\_a-quasi-cyclic code (for
arbitrary l\_a) and l\_b=1.For arbitrary l\_a and l\_b, the Pre-RGB/POT form of
the generator matrix of an l\_a l\_b-quasi-cyclic product code is
conjectured.The spectral analysis is applied to the generator matrix of the
product of an l-quasi-cyclic and a cyclic code, and we propose a new lower
bound on the minimum Hamming distance of a given l-quasi-cyclic code. In
addition, we develop an efficient syndrome-based decoding algorithm for
l-phased burst errors with guaranteed decoding radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06697</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06697</id><created>2015-12-21</created><authors><author><keyname>Bilyk</keyname><forenames>Dmitriy</forenames></author><author><keyname>Lacey</keyname><forenames>Michael T.</forenames></author></authors><title>Random Tessellations, Restricted Isometric Embeddings, and One Bit
  Sensing</title><categories>math.CA cs.IT math.IT</categories><comments>22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain mproved bounds for one bit sensing. For instance, let $ K_s$ denote
the set of $ s$-sparse unit vectors in the sphere $ \mathbb S ^{n}$ in
dimension $ n+1$ with sparsity parameter $ 0 &lt; s &lt; n+1$ and assume that $ 0 &lt;
\delta &lt; 1$. We show that for $ m \gtrsim \delta ^{-2} s \log \frac ns$, the
one-bit map $$ x \mapsto \bigl[ {sgn} \langle x,g_j \rangle \bigr] _{j=1} ^{m},
$$ where $ g_j$ are iid gaussian vectors on $ \mathbb R ^{n+1}$, with high
probability has $ \delta $-RIP from $ K_s$ into the $ m$-dimensional Hamming
cube. These bounds match the bounds for the {linear} $ \delta $-RIP given by $
x \mapsto \frac 1m[\langle x,g_j \rangle ] _{j=1} ^{m} $, from the sparse
vectors in $ \mathbb R ^{n}$ into $ \ell ^{1}$. In other words, the one bit and
linear RIPs are equally effective. There are corresponding improvements for
other one-bit properties, such as the sign-product RIP property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06706</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06706</id><created>2015-12-21</created><authors><author><keyname>Bosboom</keyname><forenames>Jeffrey</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Lynch</keyname><forenames>Jayson</forenames></author><author><keyname>Manurangsi</keyname><forenames>Pasin</forenames></author><author><keyname>Rudoy</keyname><forenames>Mikhail</forenames></author><author><keyname>Yodpinyanee</keyname><forenames>Anak</forenames></author></authors><title>Dissection with the Fewest Pieces is Hard, Even to Approximate</title><categories>cs.CG</categories><comments>18 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that it is NP-hard to dissect one simple orthogonal polygon into
another using a given number of pieces, as is approximating the fewest pieces
to within a factor of $1+1/1080-\varepsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06709</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06709</id><created>2015-12-21</created><authors><author><keyname>Sun</keyname><forenames>Xiaoxia</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Sparse Coding with Fast Image Alignment via Large Displacement Optical
  Flow</title><categories>cs.CV</categories><comments>ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation-based classifiers have shown outstanding accuracy and
robustness in image classification tasks even with the presence of intense
noise and occlusion. However, it has been discovered that the performance
degrades significantly either when test image is not aligned with the
dictionary atoms or the dictionary atoms themselves are not aligned with each
other, in which cases the sparse linear representation assumption fails. In
this paper, having both training and test images misaligned, we introduce a
novel sparse coding framework that is able to efficiently adapt the dictionary
atoms to the test image via large displacement optical flow. In the proposed
algorithm, every dictionary atom is automatically aligned with the input image
and the sparse code is then recovered using the adapted dictionary atoms. A
corresponding supervised dictionary learning algorithm is also developed for
the proposed framework. Experimental results on digit datasets recognition
verify the efficacy and robustness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06726</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06726</id><created>2015-12-21</created><authors><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Arjmanidi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Burkovski</keyname><forenames>Andreas</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Reactive Receiver Modeling for Diffusive Molecular Communication Systems
  with Molecule Degradation</title><categories>cs.ET</categories><comments>7 pages, 3 figures. Submitted to the 2016 IEEE International
  Conference on Communications (ICC) on October 31, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the diffusive molecular communication channel
between a transmitter nano-machine and a receiver nano-machine in a fluid
environment. The information molecules released by the transmitter nano-machine
into the environment can degrade in the channel via a first-order degradation
reaction and those that reach the receiver nano-machine can participate in a
reversible bimolecular-reaction with receiver receptor proteins. We derive a
closed-form analytical expression for the expected received signal at the
receiver, i.e., the expected number of activated receptors on the surface of
the receiver. The accuracy of the derived analytical result is verified with a
Brownian motion particle-based simulation of the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06730</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06730</id><created>2015-12-21</created><authors><author><keyname>Kernfeld</keyname><forenames>Eric</forenames></author><author><keyname>Majumder</keyname><forenames>Nathan</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Kilmer</keyname><forenames>Misha</forenames></author></authors><title>Multilinear Subspace Clustering</title><categories>cs.IT cs.CV cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new model and an algorithm for unsupervised
clustering of 2-D data such as images. We assume that the data comes from a
union of multilinear subspaces (UOMS) model, which is a specific structured
case of the much studied union of subspaces (UOS) model. For segmentation under
this model, we develop Multilinear Subspace Clustering (MSC) algorithm and
evaluate its performance on the YaleB and Olivietti image data sets. We show
that MSC is highly competitive with existing algorithms employing the UOS model
in terms of clustering performance while enjoying improvement in computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06735</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06735</id><created>2015-12-21</created><authors><author><keyname>Zhang</keyname><forenames>Ziyu</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Instance-Level Segmentation with Deep Densely Connected MRFs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aim is to provide a pixel-level object instance labeling of a monocular
image. We build on recent work [Zhang et al., ICCV15] that trained a
convolutional neural net to predict instance labeling in local image patches,
extracted exhaustively in a stride from an image. A simple Markov random field
model using several heuristics was then proposed in [Zhang et al., ICCV15] to
derive a globally consistent instance labeling of the image. In this paper, we
formulate the global labeling problem with a novel densely connected Markov
random field and show how to encode various intuitive potentials in a way that
is amenable to efficient mean field inference [Kr\&quot;ahenb\&quot;uhl et al., NIPS11].
Our potentials encode the compatibility between the global labeling and the
patch-level predictions, contrast-sensitive smoothness as well as the fact that
separate regions form different instances. Our experiments on the challenging
KITTI benchmark [Geiger et al., CVPR12] demonstrate that our method achieves a
significant performance boost over the baseline [Zhang et al., ICCV15].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06736</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06736</id><created>2015-12-21</created><authors><author><keyname>Kulik</keyname><forenames>Ariel</forenames></author><author><keyname>Shachnai</keyname><forenames>Hadas</forenames></author><author><keyname>Tamir</keyname><forenames>Gal</forenames></author></authors><title>On Lagrangian Relaxation and Reoptimization Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a general result demonstrating the power of Lagrangian relaxation in
solving constrained maximization problems with arbitrary objective functions.
This yields a unified approach for solving a wide class of {\em subset
selection} problems with linear constraints. Given a problem in this class and
some small $\eps \in (0,1)$, we show that if there exists an $r$-approximation
algorithm for the Lagrangian relaxation of the problem, for some $r \in (0,1)$,
then our technique achieves a ratio of $\frac{r}{r+1} -\! \eps$ to the optimal,
and this ratio is tight.
  The number of calls to the $r$-approximation algorithm, used by our
algorithms, is {\em linear} in the input size and in $\log (1 / \eps)$ for
inputs with cardinality constraint, and polynomial in the input size and in
$\log (1 / \eps)$ for inputs with arbitrary linear constraint. Using the
technique we obtain (re)approximation algorithms for natural (reoptimization)
variants of classic subset selection problems, including real-time scheduling,
the {\em maximum generalized assignment problem (GAP)} and maximum weight
independent set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06747</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06747</id><created>2015-12-21</created><authors><author><keyname>Seto</keyname><forenames>Skyler</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyu</forenames></author><author><keyname>Zhou</keyname><forenames>Yichen</forenames></author></authors><title>Multivariate Time Series Classification Using Dynamic Time Warping
  Template Selection for Human Activity Recognition</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate and computationally efficient means for classifying human activities
have been the subject of extensive research efforts. Most current research
focuses on extracting complex features to achieve high classification accuracy.
We propose a template selection approach based on Dynamic Time Warping, such
that complex feature extraction and domain knowledge is avoided. We demonstrate
the predictive capability of the algorithm on both simulated and real
smartphone data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06751</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06751</id><created>2015-12-21</created><updated>2016-01-31</updated><authors><author><keyname>Zeilberger</keyname><forenames>Noam</forenames></author></authors><title>Linear lambda terms as invariants of rooted trivalent maps</title><categories>cs.LO math.CO math.CT</categories><comments>slightly improved formulation of 4CT + minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main aim of the article is to give a simple and conceptual account for
the correspondence between ($\alpha$-equivalence classes of) closed linear
lambda terms and (isomorphism classes of) rooted trivalent maps on compact
oriented surfaces without boundary, as an instance of a more general
correspondence between linear lambda terms with a context of free variables and
rooted trivalent maps with a boundary of free edges. We begin by recalling a
familiar diagrammatic representation for linear lambda terms, and explain how
these diagrams may be interpreted formally as 1-cells in a symmetric monoidal
closed bicategory equipped with a reflexive object. From there, the &quot;easy&quot;
direction of the correspondence is a simple forgetful operation which erases
annotations on the diagram of a linear lambda term to produce a rooted
trivalent map. The other direction views linear lambda terms as invariants of
their underlying rooted trivalent maps, reconstructing the missing information
through a Tutte-style topological recurrence on maps with free edges. As an
application, we show how to use this analysis to enumerate bridgeless rooted
trivalent maps as linear lambda terms containing no closed subterms, and
conclude with a natural reformulation of the Four Color Theorem as a statement
about lambda calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06757</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06757</id><created>2015-12-21</created><updated>2016-01-26</updated><authors><author><keyname>Huang</keyname><forenames>Jiaji</forenames></author><author><keyname>Qiu</keyname><forenames>Qiang</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>GraphConnect: A Regularization Framework for Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>Theorems need more validation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have proved very successful in domains where large
training sets are available, but when the number of training samples is small,
their performance suffers from overfitting. Prior methods of reducing
overfitting such as weight decay, Dropout and DropConnect are data-independent.
This paper proposes a new method, GraphConnect, that is data-dependent, and is
motivated by the observation that data of interest lie close to a manifold. The
new method encourages the relationships between the learned decisions to
resemble a graph representing the manifold structure. Essentially GraphConnect
is designed to learn attributes that are present in data samples in contrast to
weight decay, Dropout and DropConnect which are simply designed to make it more
difficult to fit to random error or noise. Empirical Rademacher complexity is
used to connect the generalization error of the neural network to spectral
properties of the graph learned from the input data. This framework is used to
show that GraphConnect is superior to weight decay. Experimental results on
several benchmark datasets validate the theoretical analysis, and show that
when the number of training samples is small, GraphConnect is able to
significantly improve performance over weight decay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06785</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06785</id><created>2015-12-21</created><authors><author><keyname>Yang</keyname><forenames>Longqi</forenames></author><author><keyname>Hsieh</keyname><forenames>Cheng-Kang</forenames></author><author><keyname>Estrin</keyname><forenames>Deborah</forenames></author></authors><title>Beyond Classification: Latent User Interests Profiling from Visual
  Contents Analysis</title><categories>cs.IR cs.CV cs.SI</categories><comments>2015 IEEE 15th International Conference on Data Mining Workshops</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User preference profiling is an important task in modern online social
networks (OSN). With the proliferation of image-centric social platforms, such
as Pinterest, visual contents have become one of the most informative data
streams for understanding user preferences. Traditional approaches usually
treat visual content analysis as a general classification problem where one or
more labels are assigned to each image. Although such an approach simplifies
the process of image analysis, it misses the rich context and visual cues that
play an important role in people's perception of images. In this paper, we
explore the possibilities of learning a user's latent visual preferences
directly from image contents. We propose a distance metric learning method
based on Deep Convolutional Neural Networks (CNN) to directly extract
similarity information from visual contents and use the derived distance metric
to mine individual users' fine-grained visual preferences. Through our
preliminary experiments using data from 5,790 Pinterest users, we show that
even for the images within the same category, each user possesses distinct and
individually-identifiable visual preferences that are consistent over their
lifetime. Our results underscore the untapped potential of finer-grained visual
preference profiling in understanding users' preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06788</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06788</id><created>2015-12-21</created><updated>2016-02-20</updated><authors><author><keyname>Yodaiken</keyname><forenames>Victor</forenames></author></authors><title>Practical State Machines for Computer Software and Engineering</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces methods for describing properties of possibly very
large state machines in terms of solutions to recursive functions and applies
those methods to computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06789</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06789</id><created>2015-12-21</created><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author><author><keyname>Dyer</keyname><forenames>Justin</forenames></author><author><keyname>Kim</keyname><forenames>Kee-Eung</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>Information-Theoretic Bounded Rationality</title><categories>stat.ML cs.AI cs.SY math.OC</categories><comments>47 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounded rationality, that is, decision-making and planning under resource
limitations, is widely regarded as an important open problem in artificial
intelligence, reinforcement learning, computational neuroscience and economics.
This paper offers a consolidated presentation of a theory of bounded
rationality based on information-theoretic ideas. We provide a conceptual
justification for using the free energy functional as the objective function
for characterizing bounded-rational decisions. This functional possesses three
crucial properties: it controls the size of the solution space; it has Monte
Carlo planners that are exact, yet bypass the need for exhaustive search; and
it captures model uncertainty arising from lack of evidence or from interacting
with other agents having unknown intentions. We discuss the single-step
decision-making case, and show how to extend it to sequential decisions using
equivalence transformations. This extension yields a very general class of
decision problems that encompass classical decision rules (e.g. EXPECTIMAX and
MINIMAX) as limit cases, as well as trust- and risk-sensitive planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06790</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06790</id><created>2015-12-21</created><authors><author><keyname>Mahendran</keyname><forenames>Siddharth</forenames></author><author><keyname>Vidal</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Car Segmentation and Pose Estimation using 3D Object Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation and 3D pose estimation are two key cogs in any algorithm
for scene understanding. However, state-of-the-art CRF-based models for image
segmentation rely mostly on 2D object models to construct top-down high-order
potentials. In this paper, we propose new top-down potentials for image
segmentation and pose estimation based on the shape and volume of a 3D object
model. We show that these complex top-down potentials can be easily decomposed
into standard forms for efficient inference in both the segmentation and pose
estimation tasks. Experiments on a car dataset show that knowledge of
segmentation helps perform pose estimation better and vice versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06808</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06808</id><created>2015-12-21</created><authors><author><keyname>Bonanno</keyname><forenames>Giacomo</forenames></author></authors><title>Game Theory (Open Access textbook with 165 solved exercises)</title><categories>math.HO cs.GT cs.MA</categories><comments>578 pages, 163 figures</comments><msc-class>91A35, 91A30, 91A80, 62C99</msc-class><doi>10.13140/RG.2.1.3369.7360</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This is an Open Access textbook on non-cooperative Game Theory with 165
solved exercises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06813</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06813</id><created>2015-12-21</created><authors><author><keyname>Boykett</keyname><forenames>Tim</forenames></author></authors><title>Closed Systems of Invertible Maps</title><categories>math.RA cs.LO math.LO</categories><comments>Submitted to the Journal for Multiple Valued Logic and Soft Computing</comments><msc-class>06E75, 08A70, 03G27, 03C05, 94C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalise clones, which are sets of functions $f:A^n \rightarrow A$, to
sets of mappings $f:A^n \rightarrow A^m$. We formalise this and develop
language that we can use to speak about it. We then look at bijective mappings,
which have connections to reversible computation, which is important for
physical (e.g. quantum computation) as well as engineering (e.g. heat
dissipation) reasons. We generalise Toffoli's seminal work on reversible
computation to arbitrary arity logics. In particular, we show that some
restrictions he found for reversible computation on alphabets of order 2 do not
apply for odd order alphabets. For $A$ odd, we can create all invertible
mappings from the Toffoli 1- and 2-gates, demonstrating that we can realise all
reversible mappings from four generators. We discuss various forms of closure,
corresponding to various systems of permitted manipulations. These correspond,
amongst other things, to discussions about ancilla bits in quantum computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06840</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06840</id><created>2015-12-21</created><authors><author><keyname>Li</keyname><forenames>Zhepeng</forenames></author><author><keyname>Fang</keyname><forenames>Xiao</forenames></author><author><keyname>Bai</keyname><forenames>Xue</forenames></author><author><keyname>Sheng</keyname><forenames>Olivia</forenames></author></authors><title>Utility-based Link Recommendation for Online Social Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link recommendation, which suggests links to connect currently unlinked
users, is a key functionality offered by major online social networks. Salient
examples of link recommendation include &quot;People You May Know&quot; on Facebook and
LinkedIn as well as &quot;You May Know&quot; on Google+. The main stakeholders of an
online social network include users (e.g., Facebook users) who use the network
to socialize with other users and an operator (e.g., Facebook Inc.) that
establishes and operates the network for its own benefit (e.g., revenue).
Existing link recommendation methods recommend links that are likely to be
established by users but overlook the benefit a recommended link could bring to
an operator. To address this gap, we define the utility of recommending a link
and formulate a new research problem - the utility-based link recommendation
problem. We then propose a novel utility-based link recommendation method that
recommends links based on the value, cost, and linkage likelihood of a link, in
contrast to existing link recommendation methods which focus solely on linkage
likelihood. Specifically, our method models the dependency relationship between
value, cost, linkage likelihood and utility-based link recommendation decision
using a Bayesian network, predicts the probability of recommending a link with
the Bayesian network, and recommends links with the highest probabilities.
Using data obtained from a major U.S. online social network, we demonstrate
significant performance improvement achieved by our method compared to
prevalent link recommendation methods from representative prior research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06863</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06863</id><created>2015-12-21</created><authors><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Yang</keyname><forenames>Alex</forenames></author></authors><title>Addressing Complex and Subjective Product-Related Queries with Customer
  Reviews</title><categories>cs.IR cs.AI cs.SI</categories><comments>WWW 2016; 14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online reviews are often our first port of call when considering products and
purchases online. When evaluating a potential purchase, we may have a specific
query in mind, e.g. `will this baby seat fit in the overhead compartment of a
747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer
such questions we must either wade through huge volumes of consumer reviews
hoping to find one that is relevant, or otherwise pose our question directly to
the community via a Q/A system.
  In this paper we hope to fuse these two paradigms: given a large volume of
previously answered queries about products, we hope to automatically learn
whether a review of a product is relevant to a given query. We formulate this
as a machine learning problem using a mixture-of-experts-type framework---here
each review is an `expert' that gets to vote on the response to a particular
query; simultaneously we learn a relevance function such that `relevant'
reviews are those that vote correctly. At test time this learned relevance
function allows us to surface reviews that are relevant to new queries
on-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million
questions (and answers) and 13 million reviews. We show quantitatively that it
is effective at addressing both binary and open-ended queries, and
qualitatively that it surfaces reviews that human evaluators consider to be
relevant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06868</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06868</id><created>2015-12-17</created><authors><author><keyname>Martinez-Bernal</keyname><forenames>Jose</forenames></author><author><keyname>Pitones</keyname><forenames>Yuriko</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Minimum distance functions of graded ideals and Reed-Muller-type codes</title><categories>math.AC cs.IT math.CO math.IT math.NT</categories><msc-class>13P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study the minimum distance function of a graded ideal in a
polynomial ring with coefficients in a field, and show that it generalizes the
minimum distance of projective Reed-Muller-type codes over finite fields. This
gives an algebraic formulation of the minimum distance of a projective
Reed-Muller-type code in terms of the algebraic invariants and structure of the
underlying vanishing ideal. Then we give a method, based on Groebner bases and
Hilbert functions, to find lower bounds for the minimum distance of certain
Reed-Muller-type codes. Finally we show explicit upper bounds for the number of
zeros of polynomials in a projective nested cartesian set and give some support
to a conjecture of Carvalho, Lopez-Neumann and Lopez.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06880</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06880</id><created>2015-12-21</created><authors><author><keyname>Soliman</keyname><forenames>Aiman</forenames></author><author><keyname>Yin</keyname><forenames>Junjun</forenames></author><author><keyname>Soltani</keyname><forenames>Kiumars</forenames></author><author><keyname>Padmanabhan</keyname><forenames>Anand</forenames></author><author><keyname>Wang</keyname><forenames>Shaowen</forenames></author></authors><title>Where Chicagoans tweet the most: Semantic analysis of preferential
  return locations of Twitter users</title><categories>cs.SI</categories><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies on human mobility show that human movements are not random and
tend to be clustered. In this connection, the movements of Twitter users
captured by geo-located tweets were found to follow similar patterns, where a
few geographic locations dominate the tweeting activity of individual users.
However, little is known about the semantics (landuse types) and temporal
tweeting behavior at those frequently-visited locations. Furthermore, it is
generally assumed that the top two visited locations for most of the users are
home and work locales (Hypothesis A) and people tend to tweet at their top
locations during a particular time of the day (Hypothesis B). In this paper, we
tested these two frequently cited hypotheses by examining the tweeting patterns
of more than 164,000 unique Twitter users whom were residents of the city of
Chicago during 2014. We extracted landuse attributes for each geo-located tweet
from the detailed inventory of the Chicago Metropolitan Agency for Planning.
Top-visited locations were identified by clustering semantic enriched tweets
using a DBSCAN algorithm. Our results showed that although the top two
locations are likely to be residential and occupational/educational, a portion
of the users deviated from this case, suggesting that the first hypothesis
oversimplify real-world situations. However, our observations indicated that
people tweet at specific times and these temporal signatures are dependent on
landuse types. We further discuss the implication of confounding variables,
such as clustering algorithm parameters and relative accuracy of tweet
coordinates, which are critical factors in any experimental design involving
Twitter data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06888</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06888</id><created>2015-12-21</created><authors><author><keyname>Landgren</keyname><forenames>Peter</forenames></author><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>On Distributed Cooperative Decision-Making in Multiarmed Bandits</title><categories>cs.SY cs.MA math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the explore-exploit tradeoff in distributed cooperative
decision-making using the context of the multiarmed bandit (MAB) problem. For
the distributed cooperative MAB problem, we design the cooperative UCB
algorithm that comprises two interleaved distributed processes: (i) running
consensus algorithms for estimation of rewards, and (ii)
upper-confidence-bound-based heuristics for selection of arms. We rigorously
analyze the performance of the cooperative UCB algorithm and characterize the
influence of communication graph structure on the decision-making performance
of individual agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06890</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06890</id><created>2015-12-21</created><updated>2016-01-28</updated><authors><author><keyname>Gower</keyname><forenames>Robert Mansel</forenames></author><author><keyname>Richtarik</keyname><forenames>Peter</forenames></author></authors><title>Stochastic Dual Ascent for Solving Linear Systems</title><categories>math.NA cs.NA math.OC math.PR</categories><comments>This is a slightly refreshed version of the paper originally
  submitted on Dec 21, 2015. We have added a numerical experiment involving
  randomized Kaczmarz for rank-deficient systems, added a few relevant
  references, and corrected a few typos. Stats: 29 pages, 2 algorithms, 1
  figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new randomized iterative algorithm---stochastic dual ascent
(SDA)---for finding the projection of a given vector onto the solution space of
a linear system. The method is dual in nature: with the dual being a
non-strongly concave quadratic maximization problem without constraints. In
each iteration of SDA, a dual variable is updated by a carefully chosen point
in a subspace spanned by the columns of a random matrix drawn independently
from a fixed distribution. The distribution plays the role of a parameter of
the method. Our complexity results hold for a wide family of distributions of
random matrices, which opens the possibility to fine-tune the stochasticity of
the method to particular applications. We prove that primal iterates associated
with the dual process converge to the projection exponentially fast in
expectation, and give a formula and an insightful lower bound for the
convergence rate. We also prove that the same rate applies to dual function
values, primal function values and the duality gap. Unlike traditional
iterative methods, SDA converges under no additional assumptions on the system
(e.g., rank, diagonal dominance) beyond consistency. In fact, our lower bound
improves as the rank of the system matrix drops. Many existing randomized
methods for linear systems arise as special cases of SDA, including randomized
Kaczmarz, randomized Newton, randomized coordinate descent, Gaussian descent,
and their variants. In special cases where our method specializes to a known
algorithm, we either recover the best known rates, or improve upon them.
Finally, we show that the framework can be applied to the distributed average
consensus problem to obtain an array of new algorithms. The randomized gossip
algorithm arises as a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06893</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06893</id><created>2015-12-21</created><authors><author><keyname>Boyac&#x131;</keyname><forenames>Arman</forenames></author><author><keyname>Ekim</keyname><forenames>Tinaz</forenames></author><author><keyname>Shalom</keyname><forenames>Mordechai</forenames></author></authors><title>The Maximum Cardinality Cut Problem is Polynomial in Proper Interval
  Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the maximum cardinality cut problem is NP-hard even in
chordal graphs. In this paper, we consider the time complexity of the problem
in proper interval graphs, a subclass of chordal graphs, and propose a dynamic
programming algorithm which runs in polynomial-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06900</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06900</id><created>2015-12-21</created><authors><author><keyname>Esteban</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Tresp</keyname><forenames>Volker</forenames></author><author><keyname>Yang</keyname><forenames>Yinchong</forenames></author><author><keyname>Baier</keyname><forenames>Stephan</forenames></author><author><keyname>Krompa&#xdf;</keyname><forenames>Denis</forenames></author></authors><title>Predicting the Co-Evolution of Event and Knowledge Graphs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedding learning, a.k.a. representation learning, has been shown to be able
to model large-scale semantic knowledge graphs. A key concept is a mapping of
the knowledge graph to a tensor representation whose entries are predicted by
models using latent representations of generalized entities. Knowledge graphs
are typically treated as static: A knowledge graph grows more links when more
facts become available but the ground truth values associated with links is
considered time invariant. In this paper we address the issue of knowledge
graphs where triple states depend on time. We assume that changes in the
knowledge graph always arrive in form of events, in the sense that the events
are the gateway to the knowledge graph. We train an event prediction model
which uses both knowledge graph background information and information on
recent events. By predicting future events, we also predict likely changes in
the knowledge graph and thus obtain a model for the evolution of the knowledge
graph as well. Our experiments demonstrate that our approach performs well in a
clinical application, a recommendation engine and a sensor network application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06903</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06903</id><created>2015-12-21</created><authors><author><keyname>Dhople</keyname><forenames>Sairaj V.</forenames></author><author><keyname>Guggilam</keyname><forenames>Swaroop S.</forenames></author><author><keyname>Chen</keyname><forenames>Yu Christine</forenames></author></authors><title>Linear Approximations to AC Power Flow in Rectangular Coordinates</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores solutions to linearized powerflow equations with
bus-voltage phasors represented in rectangular coordinates. The key idea is to
solve for complex-valued perturbations around a nominal voltage profile from a
set of linear equations that are obtained by neglecting quadratic terms in the
original nonlinear power-flow equations. We prove that for lossless networks,
the voltage profile where the real part of the perturbation is suppressed
satisfies active-power balance in the original nonlinear system of equations.
This result motivates the development of approximate solutions that improve
over conventional DC power-flow approximations, since the model includes ZIP
loads. For distribution networks that only contain ZIP loads in addition to a
slack bus, we recover a linear relationship between the approximate voltage
profile and the constant-current component of the loads and the nodal active
and reactive-power injections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06906</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06906</id><created>2015-12-21</created><authors><author><keyname>Eftekhari</keyname><forenames>Armin</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>What Happens to a Manifold Under a Bi-Lipschitz Map?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study geometric and topological properties of the image of a smooth
submanifold of $\mathbb{R}^{n}$ under a bi-Lipschitz map to $\mathbb{R}^{m}$.
In particular, we characterize how the dimension, diameter, volume, and reach
of the embedded manifold relate to the original. Our main result establishes a
lower bound on the reach of the embedded manifold in the case where $m \le n$
and the bi-Lipschitz map is linear. We discuss implications of this work in
signal processing and machine learning, where bi-Lipschitz maps on
low-dimensional manifolds have been constructed using randomized linear
operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06908</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06908</id><created>2015-12-21</created><authors><author><keyname>Cui</keyname><forenames>Yan</forenames></author></authors><title>Research on Scalability of Operating Systems on Multicore Processors</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large number of cores and hardware resource sharing are two characteristics
on multicore processors, which bring new challenges for the design of operating
systems. How to locate and analyze the speedup restrictive factors in operating
systems, how to simulate and avoid the phenomenon that speedup decreases with
the number of cores because of lock contention (i.e., lock thrashing) and how
to avoid the contention of shared resources such as the last level cache are
key challenges for the operating system scalability research on multicore
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06912</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06912</id><created>2015-12-21</created><authors><author><keyname>Savkovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Marengo</keyname><forenames>Elisa</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author></authors><title>Query Stability in Monotonic Data-Aware Business Processes [Extended
  Version]</title><categories>cs.DB</categories><comments>This report is the extended version of a paper accepted at the 19th
  International Conference on Database Theory (ICDT 2016), March 15-18, 2016 -
  Bordeaux, France</comments><acm-class>H.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Organizations continuously accumulate data, often according to some business
processes. If one poses a query over such data for decision support, it is
important to know whether the query is stable, that is, whether the answers
will stay the same or may change in the future because business processes may
add further data. We investigate query stability for conjunctive queries. To
this end, we define a formalism that combines an explicit representation of the
control flow of a process with a specification of how data is read and inserted
into the database. We consider different restrictions of the process model and
the state of the system, such as negation in conditions, cyclic executions,
read access to written data, presence of pending process instances, and the
possibility to start fresh process instances. We identify for which facet
combinations stability of conjunctive queries is decidable and provide
encodings into variants of Datalog that are optimal with respect to the
worst-case complexity of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06915</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06915</id><created>2015-12-21</created><updated>2015-12-24</updated><authors><author><keyname>Cui</keyname><forenames>Yan</forenames></author></authors><title>An Evaluation of Yelp Dataset</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Yelp is one of the largest online searching and reviewing systems for kinds
of businesses, including restaurants, shopping, home services et al. Analyzing
the real world data from Yelp is valuable in acquiring the interests of users,
which helps to improve the design of the next generation system. This paper
targets the evaluation of Yelp dataset, which is provided in the Yelp data
challenge. A bunch of interesting results are found. For instance, to reach any
one in the Yelp social network, one only needs 4.5 hops on average, which
verifies the classical six degree separation theory; Elite user mechanism is
especially effective in maintaining the healthy of the whole network; Users who
write less than 100 business reviews dominate. Those insights are expected to
be considered by Yelp to make intelligent business decisions in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06922</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06922</id><created>2015-12-21</created><authors><author><keyname>Li</keyname><forenames>Yinxiao</forenames></author><author><keyname>Yue</keyname><forenames>Yonghao</forenames></author><author><keyname>Xu</keyname><forenames>Danfei</forenames></author><author><keyname>Grinspun</keyname><forenames>Eitan</forenames></author><author><keyname>Allen</keyname><forenames>Peter</forenames></author></authors><title>Folding Deformable Objects using Predictive Simulation and Trajectory
  Optimization</title><categories>cs.RO</categories><comments>8 pages, 9 figures, Proceedings of IROS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic manipulation of deformable objects remains a challenging task. One
such task is folding a garment autonomously. Given start and end folding
positions, what is an optimal trajectory to move the robotic arm to fold a
garment? Certain trajectories will cause the garment to move, creating
wrinkles, and gaps, other trajectories will fail altogether. We present a novel
solution to find an optimal trajectory that avoids such problematic scenarios.
The trajectory is optimized by minimizing a quadratic objective function in an
off-line simulator, which includes material properties of the garment and
frictional force on the table. The function measures the dissimilarity between
a user folded shape and the folded garment in simulation, which is then used as
an error measurement to create an optimal trajectory. We demonstrate that our
two-arm robot can follow the optimized trajectories, achieving accurate and
efficient manipulations of deformable objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06925</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06925</id><created>2015-12-21</created><authors><author><keyname>Yuan</keyname><forenames>Jiangbo</forenames></author><author><keyname>Liu</keyname><forenames>Xiuwen</forenames></author></authors><title>Transformed Residual Quantization for Approximate Nearest Neighbor
  Search</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of product quantization (PQ) for fast nearest neighbor search
depends on the exponentially reduced complexities of both storage and
computation with respect to the codebook size. Recent efforts have been focused
on employing sophisticated optimization strategies, or seeking more effective
models. Residual quantization (RQ) is such an alternative that holds the same
property as PQ in terms of the aforementioned complexities. In addition to
being a direct replacement of PQ, hybrids of PQ and RQ can yield more gains for
approximate nearest neighbor search. This motivated us to propose a novel
approach to optimizing RQ and the related hybrid models. With an observation of
the general randomness increase in a residual space, we propose a new strategy
that jointly learns a local transformation per residual cluster with an
ultimate goal to reduce overall quantization errors. We have shown that our
approach can achieve significantly better accuracy on nearest neighbor search
than both the original and the optimized PQ on several very large scale
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06927</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06927</id><created>2015-12-21</created><updated>2015-12-29</updated><authors><author><keyname>Jin</keyname><forenames>Jian</forenames></author></authors><title>A C++ library for Multimodal Deep Learning</title><categories>cs.LG</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MDL, Multimodal Deep Learning Library, is a deep learning framework that
supports multiple models, and this document explains its philosophy and
functionality. MDL runs on Linux, Mac, and Unix platforms. It depends on
OpenCV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06938</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06938</id><created>2015-12-21</created><authors><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Chen</keyname><forenames>Erkai</forenames></author><author><keyname>Zhou</keyname><forenames>Hao</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Content-Centric Sparse Multicast Beamforming for Cache-Enabled Cloud RAN</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on Wireless Communications. Conference
  version appears in IEEE GLOBECOM 2015 and IEEE/CIC ICCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a content-centric transmission design in a cloud radio
access network (cloud RAN) by incorporating multicasting and caching. Users
requesting a same content form a multicast group and are served by a same
cluster of base stations (BSs) cooperatively. Each BS has a local cache and it
acquires the requested contents either from its local cache or from the central
processor (CP) via backhaul links. We investigate the dynamic content-centric
BS clustering and multicast beamforming with respect to both channel condition
and caching status. We first formulate a mixed-integer nonlinear programming
problem of minimizing the weighted sum of backhaul cost and transmit power
under the quality-of-service constraint for each multicast group. Theoretical
analysis reveals that all the BSs caching a requested content can be included
in the BS cluster of this content, regardless of the channel conditions. Then
we reformulate an equivalent sparse multicast beamforming (SBF) problem. By
adopting smoothed $\ell_0$-norm approximation and other techniques, the SBF
problem is transformed into the difference of convex (DC) programs and
effectively solved using the convex-concave procedure algorithms. Simulation
results demonstrate significant advantage of the proposed content-centric
transmission. The effects of three heuristic caching strategies are also
evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06941</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06941</id><created>2015-12-21</created><authors><author><keyname>Alpuente</keyname><forenames>Mar&#xed;a</forenames><affiliation>DSIC-UPV</affiliation></author><author><keyname>Pardo</keyname><forenames>Daniel</forenames><affiliation>DSIC-UPV</affiliation></author><author><keyname>Villanueva</keyname><forenames>Alicia</forenames><affiliation>DSIC-UPV</affiliation></author></authors><title>Automatic Inference of Specifications in the K Framework</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PROLE 2015, arXiv:1512.06178</comments><proxy>EPTCS</proxy><acm-class>D.2.1; D.2.4; D.2.5; D.3.1; F.3.1</acm-class><journal-ref>EPTCS 200, 2015, pp. 1-17</journal-ref><doi>10.4204/EPTCS.200.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its many unquestionable benefits, formal specifications are not
widely used in industrial software development. In order to reduce the time and
effort required to write formal specifications, in this paper we propose a
technique for automatically discovering specifications from real code. The
proposed methodology relies on the symbolic execution capabilities recently
provided by the K framework that we exploit to automatically infer formal
specifications from programs that are written in a non-trivial fragment of C,
called KernelC. Roughly speaking, our symbolic analysis of KernelC programs
explains the execution of a (modifier) function by using other (observer)
routines in the program. We implemented our technique in the automated tool
Kindspec 2.0, which generates axioms that describe the precise input/output
behavior of C routines that handle pointer-based structures (i.e., result
values and state change). We describe the implementation of our system and
discuss the differences w.r.t. our previous work on inferring specifications
from C code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06942</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06942</id><created>2015-12-21</created><authors><author><keyname>Lucas</keyname><forenames>Salvador</forenames><affiliation>DSIC, Universitat Polit&#xe8;cnica de Val&#xe8;ncia, Spain</affiliation></author></authors><title>Termination of canonical context-sensitive rewriting and productivity of
  rewrite systems</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PROLE 2015, arXiv:1512.06178</comments><proxy>EPTCS</proxy><acm-class>F.3.2; I.1.3; I.2.2; I.2.5</acm-class><journal-ref>EPTCS 200, 2015, pp. 18-31</journal-ref><doi>10.4204/EPTCS.200.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Termination of programs, i.e., the absence of infinite computations, ensures
the existence of normal forms for all initial expressions, thus providing an
essential ingredient for the definition of a normalization semantics for
functional programs. In lazy functional languages, though, infinite data
structures are often delivered as the outcome of computations. For instance,
the list of all prime numbers can be returned as a neverending stream of
numerical expressions or data structures. If such streams are allowed,
requiring termination is hopeless. In this setting, the notion of productivity
can be used to provide an account of computations with infinite data
structures, as it &quot;captures the idea of computability, of progress of
infinite-list programs&quot; (B.A. Sijtsma, On the Productivity of Recursive List
Definitions, ACM Transactions on Programming Languages and Systems
11(4):633-649, 1989). However, in the realm of Term Rewriting Systems, which
can be seen as (first-order, untyped, unconditional) functional programs,
termination of Context-Sensitive Rewriting (CSR) has been showed equivalent to
productivity of rewrite systems through appropriate transformations. In this
way, tools for proving termination of CSR can be used to prove productivity. In
term rewriting, CSR is the restriction of rewriting that arises when reductions
are allowed on selected arguments of function symbols only. In this paper we
show that well-known results about the computational power of CSR are useful to
better understand the existing connections between productivity of rewrite
systems and termination of CSR, and also to obtain more powerful techniques to
prove productivity of rewrite systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06943</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06943</id><created>2015-12-21</created><authors><author><keyname>Lucas</keyname><forenames>Salvador</forenames><affiliation>DSIC, Universitat Polit&#xe8;cnica de Val&#xe8;ncia, Spain</affiliation></author></authors><title>Synthesis of models for order-sorted first-order theories using linear
  algebra and constraint solving</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PROLE 2015, arXiv:1512.06178</comments><proxy>EPTCS</proxy><acm-class>F.3.1; F.4.1; I.2.2; I.2.3; I.2.4; I.2.5</acm-class><journal-ref>EPTCS 200, 2015, pp. 32-47</journal-ref><doi>10.4204/EPTCS.200.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in termination analysis for declarative programs
emphasize the use of appropriate models for the logical theory representing the
program at stake as a generic approach to prove termination of declarative
programs. In this setting, Order-Sorted First-Order Logic provides a powerful
framework to represent declarative programs. It also provides a target logic to
obtain models for other logics via transformations. We investigate the
automatic generation of numerical models for order-sorted first-order logics
and its use in program analysis, in particular in termination analysis of
declarative programs. We use convex domains to give domains to the different
sorts of an order-sorted signature; we interpret the ranked symbols of sorted
signatures by means of appropriately adapted convex matrix interpretations.
Such numerical interpretations permit the use of existing algorithms and tools
from linear algebra and arithmetic constraint solving to synthesize the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06944</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06944</id><created>2015-12-21</created><authors><author><keyname>Romero-Hern&#xe1;ndez</keyname><forenames>David</forenames><affiliation>Universidad Complutense de Madrid, Spain</affiliation></author><author><keyname>de Frutos-Escrig</keyname><forenames>David</forenames><affiliation>Universidad Complutense de Madrid, Spain</affiliation></author><author><keyname>Della Monica</keyname><forenames>Dario</forenames><affiliation>Reykjavik University, Iceland</affiliation></author></authors><title>Proving Continuity of Coinductive Global Bisimulation Distances: A Never
  Ending Story</title><categories>cs.LO</categories><comments>In Proceedings PROLE 2015, arXiv:1512.06178</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 200, 2015, pp. 48-63</journal-ref><doi>10.4204/EPTCS.200.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed a notion of global bisimulation distance between processes
which goes somehow beyond the notions of bisimulation distance already existing
in the literature, mainly based on bisimulation games. Our proposal is based on
the cost of transformations: how much we need to modify one of the compared
processes to obtain the other. Our original definition only covered finite
processes, but a coinductive approach allows us to extend it to cover infinite
but finitary trees. After having shown many interesting properties of our
distance, it was our intention to prove continuity with respect to projections,
but unfortunately the issue remains open. Nonetheless, we have obtained several
partial results that are presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06945</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06945</id><created>2015-12-21</created><authors><author><keyname>S&#xe1;enz-P&#xe9;rez</keyname><forenames>Fernando</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author></authors><title>Restricted Predicates for Hypothetical Datalog</title><categories>cs.DB cs.AI cs.LO</categories><comments>In Proceedings PROLE 2015, arXiv:1512.06178</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 200, 2015, pp. 64-79</journal-ref><doi>10.4204/EPTCS.200.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypothetical Datalog is based on an intuitionistic semantics rather than on a
classical logic semantics, and embedded implications are allowed in rule
bodies. While the usual implication (i.e., the neck of a Horn clause) stands
for inferring facts, an embedded implication plays the role of assuming its
premise for deriving its consequence. A former work introduced both a formal
framework and a goal-oriented tabled implementation, allowing negation in rule
bodies. While in that work positive assumptions for both facts and rules can
occur in the premise, negative assumptions are not allowed. In this work, we
cover this subject by introducing a new concept: a restricted predicate, which
allows negative assumptions by pruning the usual semantics of a predicate. This
new setting has been implemented in the deductive system DES.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06947</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06947</id><created>2015-12-21</created><authors><author><keyname>Proen&#xe7;a</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Tivoli</keyname><forenames>Massimo</forenames></author></authors><title>Proceedings 14th International Workshop on Foundations of Coordination
  Languages and Self-Adaptive Systems</title><categories>cs.DC cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 201, 2015</journal-ref><doi>10.4204/EPTCS.201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of FOCLASA 2015, the 14th International
Workshop on the Foundations of Coordination Languages and Self-Adaptive
Systems. FOCLASA 2015 was held in Madrid, Spain, on September 5, 2015 as a
satellite event of CONCUR 2015, the 26th International Conference on
Concurrency Theory.
  Modern software systems are distributed, concurrent, mobile, and often
involve composition of heterogeneous components and stand-alone services.
Service coordination and self-adaptation constitute the core characteristics of
distributed and service-oriented systems. Coordination languages and formal
approaches to modelling and reasoning about self-adaptive behaviour help to
simplify the development of complex distributed service-based systems, enable
functional correctness proofs, automated synthesis of correct-by-construction
systems, and improve reusability and maintainability of such systems. The goal
of the FOCLASA workshop is to put together researchers and practitioners of the
aforementioned fields, to share and identify common problems, and to devise
general solutions in the context of coordination languages and self-adaptive
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06955</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06955</id><created>2015-12-21</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author></authors><title>Global Dynamical Solvers for Nonlinear Programming Problems</title><categories>math.OC cs.SY</categories><comments>30 pages, 3 figures. Submitted to SIAM Journal on Control and
  Optimization for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a family of globally defined dynamical systems for a nonlinear
programming problem, such that: (a) the equilibrium points are the unknown (and
sought) critical points of the problem, (b) for every initial condition, the
solution of the corresponding initial value problem converges to the set of
critical points, (c) every strict local minimum is locally asymptotically
stable, (d) the feasible set is a positively invariant set, and (e) the
dynamical system is given explicitly and does not involve the unknown critical
points of the problem. No convexity assumption is employed. The construction of
the family of dynamical systems is based on an extension of the Control
Lyapunov Function methodology, which employs extensions of LaSalle's theorem
and are of independent interest. Examples illustrate the obtained results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06961</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06961</id><created>2015-12-22</created><authors><author><keyname>Ryabko</keyname><forenames>Boris</forenames></author></authors><title>Two-faced processes and random number generators</title><categories>cs.IT cs.CR math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe random processes (with binary alphabet) whose entropy is less
than 1 (per letter), but they mimic true random process, i.e., by definition,
generated sequence can be interpreted as the result of the flips of a fair coin
with sides that are labeled 0 and 1. It gives a possibility to construct Random
Number Generators which possess theoretical guarantees. This, in turn, is
important for applications such as those in cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06963</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06963</id><created>2015-12-22</created><authors><author><keyname>Ren</keyname><forenames>Zhou</forenames></author><author><keyname>Jin</keyname><forenames>Hailin</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Fang</keyname><forenames>Chen</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Multi-Instance Visual-Semantic Embedding</title><categories>cs.CV</categories><comments>9 pages, CVPR 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual-semantic embedding models have been recently proposed and shown to be
effective for image classification and zero-shot learning, by mapping images
into a continuous semantic label space. Although several approaches have been
proposed for single-label embedding tasks, handling images with multiple labels
(which is a more general setting) still remains an open problem, mainly due to
the complex underlying corresponding relationship between image and its labels.
In this work, we present Multi-Instance visual-semantic Embedding model (MIE)
for embedding images associated with either single or multiple labels. Our
model discovers and maps semantically-meaningful image subregions to their
corresponding labels. And we demonstrate the superiority of our method over the
state-of-the-art on two tasks, including multi-label image annotation and
zero-shot learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06966</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06966</id><created>2015-12-22</created><authors><author><keyname>Akhtar</keyname><forenames>Yasmeen</forenames></author><author><keyname>Maity</keyname><forenames>Soumen</forenames></author></authors><title>Covering Arrays on Product Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two vectors $x,y$ in $\mathbb{Z}_g^n$ are $ qualitatively$ $ independent$ if
for all pairs $(a,b)\in \mathbb{Z}_g\times \mathbb{Z}_g$, there exists $i\in
\{1,2,\ldots,n\}$ such that $(x_i,y_i)=(a,b)$. A covering array on a graph $G$,
denoted by $CA(n,G,g)$, is a $|V(G)|\times n$ array on $\mathbb{Z}_g$ with the
property that any two rows which correspond to adjacent vertices in $G$ are
qualitatively independent. The number of columns in such array is called its
$size$. Given a graph $G$, a covering array on $G$ with minimum size is called
$optimal$. Our primary concern in this paper is with constructions that make
optimal covering arrays on large graphs those are obtained from product of
smaller graphs. We consider four most extensively studied graph products in
literature and give upper and lower bounds on the the size of covering arrays
on graph products. We find families of graphs for which the size of covering
array on the Cartesian product achieves the lower bound. Finally, we present a
polynomial time approximation algorithm with approximation ratio
$\log(\frac{V}{2^{k-1}})$ for constructing covering array on graph $G=(V,E)$
with $k&gt;1$ prime factors with respect to the Cartesian product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06970</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06970</id><created>2015-12-22</created><updated>2015-12-29</updated><authors><author><keyname>Tlegenov</keyname><forenames>Yedige</forenames></author><author><keyname>San</keyname><forenames>Wong Yoke</forenames></author><author><keyname>Soon</keyname><forenames>Hong Geok</forenames></author></authors><title>Adaptive Feed Rate Policies for Spiral Drilling Using Markov Decision
  Process</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, the feed rate optimization model based on a Markov Decision
Process (MDP) was introduced for spiral drilling process. Firstly, the
experimental data on spiral drilling was taken from literature for different
axial force parameters and with various feed rate decisions made, having the
length of a hole being drilled as a reward. Proposed optimization model was
computed using value iteration method. Secondly, the results of computations
were displayed for optimal decision to be made on each state. Proposed
decisions for an optimal feed rate could be utilized in order to improve the
efficiency of spiral drilling process in terms of cost and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06974</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06974</id><created>2015-12-22</created><authors><author><keyname>Misra</keyname><forenames>Ishan</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author></authors><title>Learning Visual Classifiers using Human-centric Annotations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many datasets contain human-centric annotations that are the result of humans
applying their own subjective judgements on what to describe and what to
ignore. Examples include image tags and keywords found on photo sharing sites,
or in datasets containing image captions. In this paper, we explore the use of
human-centric annotations for learning image classifiers. Due to human
reporting bias, these annotations miss a significant amount of the information
present in an image. We propose an algorithm to decouple the human reporting
bias from the correct visually grounded labels. Our algorithm provides results
that are highly interpretable for reporting &quot;what's in the image&quot; versus
&quot;what's worth saying.&quot; We show improvements over traditional learning
algorithms for both image classification and image captioning, and evaluate the
algorithm's efficacy along a variety of metrics and datasets, including MS COCO
and Yahoo Flickr 100M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06989</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06989</id><created>2015-12-22</created><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames><affiliation>LIAFA, GANG</affiliation></author><author><keyname>Halld&#xf3;rsson</keyname><forenames>Magn&#xfa;s</forenames><affiliation>LIAFA, GANG</affiliation></author><author><keyname>Korman</keyname><forenames>Amos</forenames><affiliation>LIAFA, GANG</affiliation></author></authors><title>On the Impact of Identifiers on Local Decision</title><categories>cs.DC</categories><comments>Principles of Distributed Systems, 16th International Conference, Dec
  2012, Rome, Italy</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-35476-2_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of identifiers is crucial in distributed computing. Informally,
identities are used for tackling two of the fundamental difficulties that
areinherent to deterministic distributed computing, namely: (1) symmetry
breaking, and (2) topological information gathering. In the context of local
computation, i.e., when nodes can gather information only from nodes at bounded
distances, some insight regarding the role of identities has been established.
For instance, it was shown that, for large classes of construction problems,
the role of the identities can be rather small. However, for theidentities to
play no role, some other kinds of mechanisms for breaking symmetry must be
employed, such as edge-labeling or sense of direction. When it comes to local
distributed decision problems, the specification of the decision task does not
seem to involve symmetry breaking. Therefore, it is expected that, assuming
nodes can gather sufficient information about their neighborhood, one could get
rid of the identities, without employing extra mechanisms for breaking
symmetry. We tackle this question in the framework of the $\local$ model. Let
$\LD$ be the class of all problems that can be decided in a constant number of
rounds in the $\local$ model. Similarly, let $\LD^*$ be the class of all
problems that can be decided at constant cost in the anonymous variant of the
$\local$ model, in which nodes have no identities, but each node can get access
to the (anonymous) ball of radius $t$ around it, for any $t$, at a cost of $t$.
It is clear that $\LD^*\subseteq \LD$. We conjecture that $\LD^*=\LD$. In this
paper, we give several evidences supporting this conjecture. In particular, we
show that it holds for hereditary problems, as well as when the nodes know an
arbitrary upper bound on the total number of nodes. Moreover, we prove that the
conjecture holds in the context of non-deterministic local decision, where
nodes are given certificates (independent of the identities, if they exist),
and the decision consists in verifying these certificates. In short, we prove
that $\NLD^*=\NLD$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06992</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06992</id><created>2015-12-22</created><authors><author><keyname>Zhang</keyname><forenames>Zuhe</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin</forenames></author><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>On the Differential Privacy of Bayesian Inference</title><categories>cs.AI cs.CR cs.LG math.ST stat.ML stat.TH</categories><comments>AAAI 2016, Feb 2016, Phoenix, Arizona, United States</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to communicate findings of Bayesian inference to third parties,
while preserving the strong guarantee of differential privacy. Our main
contributions are four different algorithms for private Bayesian inference on
proba-bilistic graphical models. These include two mechanisms for adding noise
to the Bayesian updates, either directly to the posterior parameters, or to
their Fourier transform so as to preserve update consistency. We also utilise a
recently introduced posterior sampling mechanism, for which we prove bounds for
the specific but general case of discrete Bayesian networks; and we introduce a
maximum-a-posteriori private mechanism. Our analysis includes utility and
privacy bounds, with a novel focus on the influence of graph structure on
privacy. Worked examples and experiments with Bayesian na{\&quot;i}ve Bayes and
Bayesian linear regression illustrate the application of our mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06997</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06997</id><created>2015-12-22</created><authors><author><keyname>Segall</keyname><forenames>Adrian</forenames></author></authors><title>HDR - A Hysteresis-Driven Routing Algorithm for Energy Harvesting Tag
  Networks</title><categories>cs.NI</categories><report-no>CE Tech Report # 005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work contains a first attempt to treat the problem of routing in networks
with energy harvesting units. We propose HDR - a Hysteresis Based Routing
Algorithm and analyse it in a simple diamond network. We also consider a
network with three forwarding nodes. The results are used to give insight into
its application in general topology networks and to general harvesting
patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.06999</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.06999</id><created>2015-12-22</created><authors><author><keyname>Varoquaux</keyname><forenames>Ga&#xeb;l</forenames><affiliation>PARIETAL</affiliation></author><author><keyname>Eickenberg</keyname><forenames>Michael</forenames><affiliation>PARIETAL</affiliation></author><author><keyname>Dohmatob</keyname><forenames>Elvis</forenames><affiliation>PARIETAL</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertand</forenames><affiliation>PARIETAL</affiliation></author></authors><title>FAASTA: A fast solver for total-variation regularization of
  ill-conditioned problems with application to brain imaging</title><categories>q-bio.NC cs.LG stat.CO stat.ML</categories><proxy>ccsd</proxy><journal-ref>Colloque GRETSI, Sep 2015, Lyon, France. Gretsi, 2015,
  http://www.gretsi.fr/colloque2015/myGretsi/programme.php</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total variation (TV) penalty, as many other analysis-sparsity problems,
does not lead to separable factors or a proximal operatorwith a closed-form
expression, such as soft thresholding for the $\ell\_1$ penalty. As a result,
in a variational formulation of an inverse problem or statisticallearning
estimation, it leads to challenging non-smooth optimization problemsthat are
often solved with elaborate single-step first-order methods. When thedata-fit
term arises from empirical measurements, as in brain imaging, it isoften very
ill-conditioned and without simple structure. In this situation, in proximal
splitting methods, the computation cost of thegradient step can easily dominate
each iteration. Thus it is beneficialto minimize the number of gradient
steps.We present fAASTA, a variant of FISTA, that relies on an internal solver
forthe TV proximal operator, and refines its tolerance to balance
computationalcost of the gradient and the proximal steps. We give benchmarks
andillustrations on &quot;brain decoding&quot;: recovering brain maps from
noisymeasurements to predict observed behavior. The algorithm as well as
theempirical study of convergence speed are valuable for any non-exact
proximaloperator, in particular analysis-sparsity problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07004</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07004</id><created>2015-12-22</created><authors><author><keyname>Altaher</keyname><forenames>Ahmed</forenames><affiliation>GIPSA-lab</affiliation></author><author><keyname>Mocanu</keyname><forenames>St&#xe9;phane</forenames><affiliation>GIPSA-lab</affiliation></author><author><keyname>Thiriet</keyname><forenames>Jean-Marc</forenames><affiliation>GIPSA-lab</affiliation></author></authors><title>Evaluation of Time-Critical Communications for IEC 61850-Substation
  Network Architecture</title><categories>cs.NI cs.PF</categories><proxy>ccsd</proxy><journal-ref>Surveillance 8 International Conference , Oct 2015, Roanne,
  France. Proceeding of Surveillance 8 2015,
  \&amp;lt;http://surveillance8.sciencesconf.org/\&amp;gt;</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Present-day developments, in electrical power transmission and distribution,
require considerations of the status quo. In other meaning, international
regulations enforce increasing of reliability and reducing of environment
impact, correspondingly they motivate developing of dependable systems. Power
grids especially intelligent (smart grids) ones become industrial solutions
that follow standardized development. The International standardization, in the
field of power transmission and distribution, improve technology influences.
The rise of dedicated standards for SAS (Substation Automation Systems)
communications, such as the leading International Electro-technical Commission
standard IEC 61850, enforces modern technological trends in this field. Within
this standard, a constraint of low ETE (End-to-End) latency should be
respected, and time-critical status transmission must be achieved. This
experimental study emphasis on IEC 61850 SAS communication standard, e.g. IEC
61850 GOOSE (Generic Object Oriented Substation Events), to implement an
investigational method to determine the protection communication delay. This
method observes GOOSE behaviour by adopting monitoring and analysis
capabilities. It is observed by using network test equipment, i.e. SPAN (Switch
Port Analyser) and TAP (Test Access Point) devices, with on-the-shelf available
hardware and software solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07009</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07009</id><created>2015-12-22</created><authors><author><keyname>Barto</keyname><forenames>Libor</forenames></author><author><keyname>Kazda</keyname><forenames>Alexandr</forenames></author></authors><title>Deciding absorption</title><categories>math.RA cs.CC</categories><msc-class>08A70, 08B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize absorption in finite idempotent algebras by means of
J\'onsson absorption and cube term blockers. As an application we show that it
is decidable whether a given subset is an absorbing subuniverse of an algebra
given by the tables of its basic operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07010</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07010</id><created>2015-12-22</created><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author></authors><title>The risk of divergence</title><categories>cs.GT</categories><comments>3rd International Workshop on Strategic Reasoning, Dec 2015, Oxford,
  United Kingdom. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present infinite extensive strategy profiles with perfect information and
we show that replacing finite by infinite changes the notions and the reasoning
tools. The presentation uses a formalism recently developed by logicians and
computer science theoreticians, called coinduction. This builds a bridge
between economic game theory and the most recent advance in theoretical
computer science and logic. The key result is that rational agents may have
strategy leading to divergence .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07012</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07012</id><created>2015-12-22</created><authors><author><keyname>Vebarin</keyname><forenames>Hamoinba</forenames></author><author><keyname>Difrawi</keyname><forenames>Samourqi</forenames></author></authors><title>SRPS: Secure Routing Protocol for Static Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In sensor networks, nodes cooperatively work to collect data and forward it
to the final destination. Many protocols have been proposed in the literature
to provide routing and secure routing for ad hoc and sensor networks, but these
protocols either very expensive to be used in very resource-limited
environments such as sensor networks, or suffer from the lack of one or more
security guarantees and vulnerable to attacks such as wormhole, Sinkhole,
Sybil, blackhole, selective forwarding, rushing, and fabricating attacks. In
this paper we propose a secure lightweight routing protocol called SRPS. SRPS
uses symmetric cryptographic entities within the capabilities of the sensors,
supports intermediate node authentication of the routing information in
addition to end-to-end authentication, provides secure multiple disjoint paths,
and thwarts all the known attacks against routing infrastructure against
Byzantine cooperative attack model. We analyze the security guarantees of SRPS
and use Ns-2 simulations to show the effectiveness of SRPS in counter-measuring
known attacks against the routing infrastructure. Overhead cost analysis is
conducted to prove the lightweight-ness of SRPS
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07016</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07016</id><created>2015-12-22</created><authors><author><keyname>Jencova</keyname><forenames>Anna</forenames></author></authors><title>Comparison of quantum channels</title><categories>quant-ph cs.IT math.IT</categories><comments>24 pages, an offspring of arXiv:1404.3900</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a pair of quantum channels with the same input space, we show that the
possibility of approximation of one channel by post-processings of the other
can be characterized by comparing the success probabilities for ensembles
obtained as outputs for any ensemble on the input space coupled with an
ancilla. In particular, this yields the randomization criterion for quantum
statistical experiments, with a clear operational interpretation. The results
are obtained using some properties of the diamond norm and its dual, which are
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07019</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07019</id><created>2015-12-22</created><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author><author><keyname>Watrigant</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>The Bi-Objective Workflow Satisfiability Problem and Workflow Resiliency</title><categories>cs.CR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A workflow is a collection of steps that must be executed in some specific
order to achieve an objective. A computerized workflow management system may
enforce a security policy, specified in terms of authorized actions and
constraints, thereby restricting which users can perform particular steps in a
workflow. The existence of a security policy may mean that a workflow is
unsatisfiable, in the sense that it is impossible to find an authorized user
for each step in the workflow and satisfy all constraints. Work in the
literature focuses on the workflow satisfiability problem, which is a decision
problem.
  In this paper, we introduce the Bi-Objective Workflow Satisfiability Problem
(BO-WSP), which enables us to answer optimization problems related to workflows
and security policies. In particular, we are able to compute a &quot;least bad&quot; plan
when some components of the security policy may be violated. In general, BO-WSP
is intractable from both classical and parameterized complexity point of view
(the parameter is the number of steps). We prove that computing a Pareto front
of BO-WSP is fixed-parameter tractable when restricted to user-independent (UI)
constraints, a wide class of practical constraints.
  We then study the important question of workflow resiliency and prove new
results establishing that the classical decision problems are fixed-parameter
tractable when restricted to UI constraints. We then argue that existing models
of workflow resiliency are unlikely to be suitable in practice and propose a
richer and more realistic model. Finally, we demonstrate that many questions
related to resiliency in the context of our model may be reduced to instances
of BO-WSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07021</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07021</id><created>2015-12-22</created><updated>2016-01-27</updated><authors><author><keyname>Sch&#xe4;tzle</keyname><forenames>Alexander</forenames></author><author><keyname>Przyjaciel-Zablocki</keyname><forenames>Martin</forenames></author><author><keyname>Skilevic</keyname><forenames>Simon</forenames></author><author><keyname>Lausen</keyname><forenames>Georg</forenames></author></authors><title>S2RDF: RDF Querying with SPARQL on Spark</title><categories>cs.DB cs.DC</categories><acm-class>C.2.4; D.1.3; E.1; H.2.3; H.2.4; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDF has become very popular for semantic data publishing due to its flexible
and universal graph-like data model. Yet, the ever-increasing size of RDF data
collections makes it more and more infeasible to store and process them on a
single machine, raising the need for distributed approaches. Instead of
building a standalone but closed distributed RDF store, we endorse the usage of
existing infrastructures for Big Data processing, e.g. Hadoop. However, SPARQL
query performance is a major challenge as these platforms are not designed for
RDF processing from ground. Thus, existing Hadoop-based approaches often favor
certain query pattern shape while performance drops significantly for other
shapes. In this paper, we describe a novel relational partitioning schema for
RDF data called ExtVP that uses a semi-join based preprocessing, akin to the
concept of Join Indices in relational databases, to efficiently minimize query
input size regardless of its pattern shape and diameter. Our prototype system
S2RDF is built on top of Spark and uses its relational interface to execute
SPARQL queries over ExtVP. We demonstrate its superior performance in
comparison to state of the art SPARQL-on-Hadoop approaches using the recent
WatDiv test suite. S2RDF achieves sub-second runtimes for majority of queries
on a billion triples RDF graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07030</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07030</id><created>2015-12-22</created><authors><author><keyname>Jin</keyname><forenames>Xiaojie</forenames></author><author><keyname>Xu</keyname><forenames>Chunyan</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Xiong</keyname><forenames>Junjun</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Deep Learning with S-shaped Rectified Linear Activation Units</title><categories>cs.CV</categories><comments>Accepted by AAAI-16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rectified linear activation units are important components for
state-of-the-art deep convolutional networks. In this paper, we propose a novel
S-shaped rectified linear activation unit (SReLU) to learn both convex and
non-convex functions, imitating the multiple function forms given by the two
fundamental laws, namely the Webner-Fechner law and the Stevens law, in
psychophysics and neural sciences. Specifically, SReLU consists of three
piecewise linear functions, which are formulated by four learnable parameters.
The SReLU is learned jointly with the training of the whole deep network
through back propagation. During the training phase, to initialize SReLU in
different layers, we propose a &quot;freezing&quot; method to degenerate SReLU into a
predefined leaky rectified linear unit in the initial several training epochs
and then adaptively learn the good initial values. SReLU can be universally
used in the existing deep networks with negligible additional parameters and
computation cost. Experiments with two popular CNN architectures, Network in
Network and GoogLeNet on scale-various benchmarks including CIFAR10, CIFAR100,
MNIST and ImageNet demonstrate that SReLU achieves remarkable improvement
compared to other activation functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07038</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07038</id><created>2015-12-22</created><authors><author><keyname>Hrab&#xe1;k</keyname><forenames>Pavel</forenames></author><author><keyname>Buk&#xe1;&#x10d;ek</keyname><forenames>Marek</forenames></author></authors><title>Conflict Solution According to &quot;Aggressiveness&quot; of Agents in
  Floor-Field-Based Model</title><categories>cs.MA</categories><comments>Submitted to LNCS (Springer), In proceedings of 11th international
  conference on Parallel Processing and Applied Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution introduces an element of &quot;aggressiveness&quot; into the
Floor-Field based model with adaptive time-span. The aggressiveness is
understood as an ability to win conflicts and push through the crowd. From
experiments it is observed that this ability is not directly correlated with
the desired velocity in the free flow regime. The influence of the
aggressiveness is studied by means of the dependence of the travel time on the
occupancy of a room. A simulation study shows that the conflict solution based
on the aggressiveness parameter can mimic the observations from the experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07041</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07041</id><created>2015-12-22</created><authors><author><keyname>Makarenko</keyname><forenames>A. V.</forenames></author><author><keyname>Volovik</keyname><forenames>M. G.</forenames></author></authors><title>Implementation of deep learning algorithm for automatic detection of
  brain tumors using intraoperative IR-thermal mapping data</title><categories>cs.CV cs.LG q-bio.QM stat.ML</categories><comments>7 pages, 5 figures, 2 tables</comments><msc-class>92C55, 68T45, 68T10, 62M45</msc-class><acm-class>I.5.1; I.4.8; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of deep machine learning for automatic delineation of tumor
areas has been demonstrated for intraoperative neuronavigation using active
IR-mapping with the use of the cold test. The proposed approach employs a
matrix IR-imager to remotely register the space-time distribution of surface
temperature pattern, which is determined by the dynamics of local cerebral
blood flow. The advantages of this technique are non-invasiveness, zero risks
for the health of patients and medical staff, low implementation and
operational costs, ease and speed of use. Traditional IR-diagnostic technique
has a crucial limitation - it involves a diagnostician who determines the
boundaries of tumor areas, which gives rise to considerable uncertainty, which
can lead to diagnosis errors that are difficult to control. The current study
demonstrates that implementing deep learning algorithms allows to eliminate the
explained drawback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07043</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07043</id><created>2015-12-22</created><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Khammash</keyname><forenames>Mustafa</forenames></author></authors><title>Sign properties of Metzler matrices with applications</title><categories>math.DS cs.SY math.OC q-bio.MN</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several results on the sign properties of Metzler matrices are obtained. It
is first established that checking the sign-stability of a Metzler sign-matrix
can be either characterized in terms of the Hurwitz stability of the unit
sign-matrix in the corresponding qualitative class, or in terms of the
acyclicity of the graph associated with the sign-pattern. Similar results are
obtained for the case of block-matrices and mixed-matrices, the latter
containing both sign patterns and fixed real entries. The problem of assessing
the sign-stability of the convex full of a finite family of Metzler matrices is
also solved, and a necessary and sufficient condition for the existence of a
common Lyapunov function for all the matrices in the convex hull is obtained.
The notion of relative sign-stability is also introduced and a sufficient
condition for the relative sign-stability of Metzler matrices is proposed.
Several applications of the results are discussed in the last section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07046</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07046</id><created>2015-12-22</created><authors><author><keyname>Rupnik</keyname><forenames>Jan</forenames></author><author><keyname>Muhic</keyname><forenames>Andrej</forenames></author><author><keyname>Leban</keyname><forenames>Gregor</forenames></author><author><keyname>Skraba</keyname><forenames>Primoz</forenames></author><author><keyname>Fortuna</keyname><forenames>Blaz</forenames></author><author><keyname>Grobelnik</keyname><forenames>Marko</forenames></author></authors><title>News Across Languages - Cross-Lingual Document Similarity and Event
  Tracking</title><categories>cs.IR cs.CL</categories><comments>Accepted for publication in Journal of Artificial Intelligence
  Research, Special Track on Cross-language Algorithms and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world, we follow news which is distributed globally. Significant
events are reported by different sources and in different languages. In this
work, we address the problem of tracking of events in a large multilingual
stream. Within a recently developed system Event Registry we examine two
aspects of this problem: how to compare articles in different languages and how
to link collections of articles in different languages which refer to the same
event. Taking a multilingual stream and clusters of articles from each
language, we compare different cross-lingual document similarity measures based
on Wikipedia. This allows us to compute the similarity of any two articles
regardless of language. Building on previous work, we show there are methods
which scale well and can compute a meaningful similarity between articles from
languages with little or no direct overlap in the training data. Using this
capability, we then propose an approach to link clusters of articles across
languages which represent the same event. We provide an extensive evaluation of
the system as a whole, as well as an evaluation of the quality and robustness
of the similarity measure and the linking algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07048</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07048</id><created>2015-12-22</created><updated>2016-02-10</updated><authors><author><keyname>Bertens</keyname><forenames>Roel</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author><author><keyname>Siebes</keyname><forenames>Arno</forenames></author></authors><title>Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our world is filled with both beautiful and brainy people, but how often does
a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who
is both very beautiful and very smart is more rare than what we would expect
from the combination of the number of beautiful and brainy people. Of course
there will still always be some individuals that defy this stereotype; these
beautiful brainy people are exactly the class of anomaly we focus on in this
paper. They do not posses intrinsically rare qualities, it is the unexpected
combination of factors that makes them stand out.
  In this paper we define the above described class of anomaly and propose a
method to quickly identify them in transaction data. Further, as we take a
pattern set based approach, our method readily explains why a transaction is
anomalous. The effectiveness of our method is thoroughly verified with a wide
range of experiments on both real world and synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07051</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07051</id><created>2015-12-22</created><authors><author><keyname>Kiseleva</keyname><forenames>Julia</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>Alejandro Montes</forenames></author><author><keyname>Kamps</keyname><forenames>Jaap</forenames></author><author><keyname>Spirin</keyname><forenames>Nikita</forenames></author></authors><title>The Impact of Technical Domain Expertise on Search Behavior and Task
  Outcome</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain expertise is regarded as one of the key factors impacting search
success: experts are known to write more effective queries, to select the right
results on the result page, and to find answers satisfying their information
needs. Search transaction logs play the crucial role in the result ranking. Yet
despite the variety in expertise levels of users, all prior interactions are
treated alike, suggesting that weighting in expertise can improve the ranking
for informational tasks. The main aim of this paper is to investigate the
impact of high levels of technical domain expertise on both search behavior and
task outcome. We conduct an online user study with searchers proficient in
programming languages. We focus on Java and Javascript, yet we believe that our
study and results are applicable for other expertise-sensitive search tasks.
The main findings are three-fold: First, we constructed expertise tests that
effectively measure technical domain expertise and correlate well with the
self-reported expertise. Second, we showed that there is a clear position bias,
but technical domain experts were less affected by position bias. Third, we
found that general expertise helped finding the correct answers, but the domain
experts were more successful as they managed to detect better answers. Our work
is using explicit tests to determine user expertise levels, which is an
important step toward fully automatic detection of expertise levels based on
interaction behavior. A deeper understanding of the impact of expertise on
search behavior and task outcome can enable more effective use of expert
behavior in search logs - essentially make everyone search as an expert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07056</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07056</id><created>2015-12-22</created><updated>2016-02-10</updated><authors><author><keyname>Bertens</keyname><forenames>Roel</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author><author><keyname>Siebes</keyname><forenames>Arno</forenames></author></authors><title>Keeping it Short and Simple: Summarising Complex Event Sequences with
  Multivariate Patterns</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to obtain concise descriptions of discrete multivariate
sequential data. In particular, how to do so in terms of rich multivariate
sequential patterns that can capture potentially highly interesting
(cor)relations between sequences. To this end we allow our pattern language to
span over the domains (alphabets) of all sequences, allow patterns to overlap
temporally, as well as allow for gaps in their occurrences.
  We formalise our goal by the Minimum Description Length principle, by which
our objective is to discover the set of patterns that provides the most
succinct description of the data. To discover high-quality pattern sets
directly from data, we introduce DITTO, a highly efficient algorithm that
approximates the ideal result very well.
  Experiments show that DITTO correctly discovers the patterns planted in
synthetic data. Moreover, it scales favourably with the length of the data, the
number of attributes, the alphabet sizes. On real data, ranging from sensor
networks to annotated text, DITTO discovers easily interpretable summaries that
provide clear insight in both the univariate and multivariate structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07067</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07067</id><created>2015-12-22</created><authors><author><keyname>Brodu</keyname><forenames>Etienne</forenames><affiliation>DICE</affiliation></author><author><keyname>Fr&#xe9;not</keyname><forenames>St&#xe9;phane</forenames><affiliation>DICE</affiliation></author><author><keyname>Obl&#xe9;</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Transforming Javascript Event-Loop Into a Pipeline</title><categories>cs.PL cs.DC</categories><proxy>ccsd</proxy><journal-ref>Symposium on Applied Computing, Apr 2016, Pisa, Italy. 2016,
  http://www.acm.org/conferences/sac/sac2016/</journal-ref><doi>10.1145/2851613.2851745</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of a real-time web application often starts with a
feature-driven approach allowing to quickly react to users feedbacks. However,
this approach poorly scales in performance. Yet, the user-base can increase by
an order of magnitude in a matter of hours. This first approach is unable to
deal with the highest connections spikes. It leads the development team to
shift to a scalable approach often linked to new development paradigm such as
dataflow programming. This shift of technology is disruptive and
continuity-threatening. To avoid it, we propose to abstract the feature-driven
development into a more scalable high-level language. Indeed, reasoning on this
high-level language allows to dynamically cope with user-base size evolutions.
We propose a compilation approach that transforms a Javascript, single-threaded
real-time web application into a network of small independent parts
communicating by message streams. We named these parts fluxions, by contraction
between a flow (flux in french) and a function. The independence of these parts
allows their execution to be parallel, and to organize an application on
several processors to cope with its load, in a similar way network routers do
with IP traffic. We test this approach by applying the compiler to a real web
application. We transform this application to parallelize the execution of an
independent part and present the result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07069</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07069</id><created>2015-12-22</created><authors><author><keyname>Barreiro</keyname><forenames>Enrique Wulff</forenames></author></authors><title>Using HistCite software to identify significant articles in subject
  searches of the Web of Science</title><categories>cs.DL</categories><proxy>ccsd</proxy><journal-ref>Documentaci{\'o}n de las Ciencias de la Informaci{\'o}n,
  Universidad Complutense de Madrid, 2007, 30, pp.45-64.
  http://revistas.ucm.es/index.php/DCIN/article/view/DCIN0707110045A</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HistCite TM is a large-scale computer tool for mapping science. Its power of
visualization combines the production of historiographs on the basis of the
analysis of co-citations of documents, with the use of bibliometrics specific
indicators. The objective of this article is, to present the advantages of the
new bibliometrics configuration of HistCite TM (2004) when identifying
articles. The analysis of the histograms that produces HistCite TM , in terms
of cumulative advantage and aging of the citations. And the comparative study
of the results of HistCite TM , in its indicators of amplitude and recognition.
Also is examined its treatment of the sampling problems, by formalizing the
Kendall method of estimating the robust standard deviation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07071</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07071</id><created>2015-12-22</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Haunschild</keyname><forenames>Robin</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author></authors><title>Policy documents as sources for measuring societal impact: How is
  climate change research perceived in policy documents?</title><categories>physics.soc-ph cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current UK Research Excellence Framework (REF) and the Excellence in
Research for Australia (ERA) societal impact measurements are inherent parts of
the national evaluation systems. In this study, we deal with a relatively new
form of societal impact measurements. Recently, Altmetric - a start-up
providing publication level metrics - started to make data for publications
available which have been mentioned in policy documents. We regard this data
source as an interesting possibility to specifically measure the (societal)
impact of research. Using a comprehensive dataset with publications on climate
change as an example, we study the usefulness of the new data source for impact
measurement. Only 1.2 percent (2341) out of 191276 publications on climate
change in the dataset have at least one policy mention. We further reveal that
papers published in Nature and Science as well as from the areas &quot;Earth and
related environmental sciences&quot; and &quot;Social and economic geography&quot; are
especially relevant in the policy context. Given the low coverage of the
climate change literature in policy documents, this study can be only a first
attempt to study this new source of altmetric data. Further empirical studies
are necessary in upcoming years, because mentions in policy documents are of
special interest in the use of altmetric data for measuring target-oriented the
broader impact of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07074</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07074</id><created>2015-12-22</created><authors><author><keyname>Xiao</keyname><forenames>Chunyang</forenames></author></authors><title>Move from Perturbed scheme to exponential weighting average</title><categories>cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an online decision problem, one makes decisions often with a pool of
decision sequence called experts but without knowledge of the future. After
each step, one pays a cost based on the decision and observed rate. One
reasonal goal would be to perform as well as the best expert in the pool. The
modern and well-known way to attain this goal is the algorithm of exponential
weighting. However, recently, another algorithm called follow the perturbed
leader is developed and achieved about the same performance. In our work, we
first show the properties shared in common by the two algorithms which explain
the similarities on the performance. Next we will show that for a specific
perturbation, the two algorithms are identical. Finally, we show with some
examples that follow-the-leader style algorithms extend naturally to a large
class of structured online problems for which the exponential algorithms are
inefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07080</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07080</id><created>2015-12-22</created><authors><author><keyname>Perrett</keyname><forenames>Toby</forenames></author><author><keyname>Mirmehdi</keyname><forenames>Majid</forenames></author><author><keyname>Dias</keyname><forenames>Eduardo</forenames></author></authors><title>Cost-based Feature Transfer for Vehicle Occupant Classification</title><categories>cs.CV</categories><comments>9 pages, 4 figures, 5 tables</comments><acm-class>I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge of human presence and interaction in a vehicle is of growing
interest to vehicle manufacturers for design and safety purposes. We present a
framework to perform the tasks of occupant detection and occupant
classification for automatic child locks and airbag suppression. It operates
for all passenger seats, using a single overhead camera. A transfer learning
technique is introduced to make full use of training data from all seats whilst
still maintaining some control over the bias, necessary for a system designed
to penalize certain misclassifications more than others. An evaluation is
performed on a challenging dataset with both weighted and unweighted
classifiers, demonstrating the effectiveness of the transfer process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07081</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07081</id><created>2015-12-22</created><authors><author><keyname>Audoux</keyname><forenames>Benjamin</forenames></author><author><keyname>Couvreur</keyname><forenames>Alain</forenames></author></authors><title>On tensor products of CSS Codes</title><categories>cs.IT math.CO math.IT quant-ph</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CSS codes are in one-to-one correspondance with length 3 chain complexes. The
latter are naturally endowed with a tensor product $\otimes$ which induces a
similar operation on the former. We investigate this operation, and in
particular its behavior with regard to minimum distances. Given a CSS code
$\mathcal{C}$, we give a criterion which provides a lower bound on the minimum
distance of $\mathcal{C} \otimes \mathcal{D}$ for every CSS code $\mathcal D$.
We apply this result to study the behaviour of iterated tensor powers of codes.
Such sequences of codes are logarithmically LDPC and we prove in particular
that their minimum distances tend generically to infinity. Different known
results are reinterpretated in terms of tensor products. Three new families of
CSS codes are defined, and their iterated tensor powers produce LDPC sequences
of codes with length $n$, row weight in $O(\log n)$ and minimum distances
larger than $n^{\frac{\alpha}{2}}$ for any $\alpha&lt;1$. One family produces
sequences with dimensions larger than $n^\beta$ for any $\beta&lt;1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07098</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07098</id><created>2015-12-22</created><authors><author><keyname>Bravetti</keyname><forenames>Mario</forenames></author></authors><title>Reduction Semantics in Markovian Process Algebra</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Stochastic (Markovian) process algebra extend classical process algebra with
probabilistic exponentially distributed time durations denoted by rates (the
parameter of the exponential distribution). Defining a semantics for such an
algebra, so to derive Continuous Time Markov Chains from system specifications,
requires dealing with transitions labeled by rates. With respect to standard
process algebra semantics this poses a problem: we have to take into account
the multiplicity of several identical transitions (with the same rate). Several
techniques addressing this problem have been introduced in the literature, but
they can only be used for semantic definitions that do not exploit a structural
congruence relation on terms while inferring transitions. On the other hand,
using a structural congruence relation is a useful mechanism that is commonly
adopted, for instance, in order to define semantics in reduction style for
non-basic process algebras such as the pi-calculus or richer ones. In this
paper we show how to define semantics for Markovian process algebra when
structural congruence is used while inferring transitions and, as an
application example, we define the reduction semantics for a stochastic version
of the pi-calculus. Moreover we show such semantics to be correct with respect
to the standard one (defined in labeled operational semantics style).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07103</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07103</id><created>2015-12-22</created><updated>2016-01-24</updated><authors><author><keyname>Xiang</keyname><forenames>Can</forenames></author><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author></authors><title>A Class of Linear Codes with a Few Weights</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1503.06512 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes have been an interesting subject of study for many years, as
linear codes with few weights have applications in secrete sharing,
authentication codes, association schemes, and strongly regular graphs. In this
paper, a class of linear codes with a few weights over the finite field
$\gf(p)$ are presented and their weight distributions are also determined,
where $p$ is an odd prime. Some of the linear codes obtained are optimal in the
sense that they meet certain bounds on linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07108</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07108</id><created>2015-12-22</created><authors><author><keyname>Gu</keyname><forenames>Jiuxiang</forenames></author><author><keyname>Wang</keyname><forenames>Zhenhua</forenames></author><author><keyname>Kuen</keyname><forenames>Jason</forenames></author><author><keyname>Ma</keyname><forenames>Lianyang</forenames></author><author><keyname>Shahroudy</keyname><forenames>Amir</forenames></author><author><keyname>Shuai</keyname><forenames>Bing</forenames></author><author><keyname>Liu</keyname><forenames>Ting</forenames></author><author><keyname>Wang</keyname><forenames>Xingxing</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author></authors><title>Recent Advances in Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>review, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years, deep learning has lead to very good performance on a
variety of problems, such as object recognition, speech recognition and natural
language processing. Among different types of deep neural networks,
convolutional neural networks have been most extensively studied. Due to the
lack of training data and computing power in early days, it is hard to train a
large high-capacity convolutional neural network without overfitting. Recently,
with the rapid growth of data size and the increasing power of graphics
processor unit, many researchers have improved the convolutional neural
networks and achieved state-of-the-art results on various tasks. In this paper,
we provide a broad survey of the recent advances in convolutional neural
networks. Besides, we also introduce some applications of convolutional neural
networks in computer vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07143</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07143</id><created>2015-12-22</created><authors><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames></author><author><keyname>Bola&#xf1;os</keyname><forenames>Marc</forenames></author><author><keyname>Talavera</keyname><forenames>Estefania</forenames></author><author><keyname>Aghaei</keyname><forenames>Maedeh</forenames></author><author><keyname>Nikolov</keyname><forenames>Stavri G.</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author></authors><title>SR-Clustering: Semantic Regularized Clustering for Egocentric Photo
  Streams Segmentation</title><categories>cs.AI cs.CV</categories><comments>23 pages, 10 figures, 2 tables. Submitted to Pattern Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While wearable cameras are becoming increasingly popular, locating relevant
information in large unstructured collections of egocentric images is still a
tedious and time consuming processes. This paper addresses the problem of
organizing egocentric photo streams acquired by a wearable camera into
semantically meaningful segments. First, contextual and semantic information is
extracted for each image by employing a Convolutional Neural Networks approach.
Later, by integrating language processing, a vocabulary of concepts is defined
in a semantic space. Finally, by exploiting the temporal coherence in photo
streams, images which share contextual and semantic attributes are grouped
together. The resulting temporal segmentation is particularly suited for
further analysis, ranging from activity and event recognition to semantic
indexing and summarization. Experiments over egocentric sets of nearly 17,000
images, show that the proposed approach outperforms state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07146</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07146</id><created>2015-12-22</created><authors><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author></authors><title>Refined Error Bounds for Several Learning Algorithms</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the achievable guarantees on the error rates of certain
learning algorithms, with particular focus on refining logarithmic factors.
Many of the results are based on a general technique for obtaining bounds on
the error rates of sample-consistent classifiers with monotonic error regions,
in the realizable case. We prove bounds of this type expressed in terms of
either the VC dimension or the sample compression size. This general technique
also enables us to derive several new bounds on the error rates of general
sample-consistent learning algorithms, as well as refined bounds on the label
complexity of the CAL active learning algorithm. Additionally, we establish a
simple necessary and sufficient condition for the existence of a
distribution-free bound on the error rates of all sample-consistent learning
rules, converging at a rate inversely proportional to the sample size. We also
study learning in the presence of classification noise, deriving a new excess
error rate guarantee for general VC classes under Tsybakov's noise condition,
and establishing a simple and general necessary and sufficient condition for
the minimax excess risk under bounded noise to converge at a rate inversely
proportional to the sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07153</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07153</id><created>2015-12-22</created><authors><author><keyname>Dimri</keyname><forenames>Vivek</forenames></author><author><keyname>Biswas</keyname><forenames>Prof. Ranjit</forenames></author></authors><title>A Novel Approach to Compress Centralized Text Data using Indexed
  Dictionary</title><categories>cs.OH</categories><comments>Paper Accepted in journal: IJASCSE Volume 4, Issue 12 (December 2015)
  http://www.ijascse.org</comments><journal-ref>IJASCSE Volume 4, Issue 12 (December 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data compression is very important feature in terms of saving the memory
space. In this proposal, an indexed dictionary based compression is used for
text data, where the word's reference in dictionary is used for compression.
This approach is not file based, a common dictionary is used for compression.
Which contains the words, the position of the word in dictionary is one of the
key parts of encoded frame which is compressed form of the text word. This is
loss-less compression. This compression approach is also take cares of small
words like one or two characters words which usually decrease the efficiency of
compression algorithms. This approach is also deals with file having special
characters as a word. Special character words, alpha numeric words, normal
texted words and small words all deals differently which makes this approach
more efficient. Since a centralized dictionary is used for data compression,
therefore, this approach is not preferred for transfer compressed file, while
it is suitable to store text data in compressed form in hard disk drive and
centralized storage or cloud drive for memory utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07155</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07155</id><created>2015-12-22</created><authors><author><keyname>Ma</keyname><forenames>Shugao</forenames></author><author><keyname>Bargal</keyname><forenames>Sarah Adel</forenames></author><author><keyname>Zhang</keyname><forenames>Jianming</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author><author><keyname>Sclaroff</keyname><forenames>Stan</forenames></author></authors><title>Do Less and Achieve More: Training CNNs for Action Recognition Utilizing
  Action Images from the Web</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, attempts have been made to collect millions of videos to train CNN
models for action recognition in videos. However, curating such large-scale
video datasets requires immense human labor, and training CNNs on millions of
videos demands huge computational resources. In contrast, collecting action
images from the Web is much easier and training on images requires much less
computation. In addition, labeled web images tend to contain discriminative
action poses, which highlight discriminative portions of a video's temporal
progression. We explore the question of whether we can utilize web action
images to train better CNN models for action recognition in videos. We collect
23.8K manually filtered images from the Web that depict the 101 actions in the
UCF101 action video dataset. We show that by utilizing web action images along
with videos in training, significant performance boosts of CNN models can be
achieved. We then investigate the scalability of the process by leveraging
crawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M
video frames by 393K unfiltered images and get comparable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07158</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07158</id><created>2015-12-22</created><updated>2016-02-18</updated><authors><author><keyname>Zhang</keyname><forenames>Baichuan</forenames></author><author><keyname>Dave</keyname><forenames>Vachik</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author></authors><title>Feature Selection for Classification under Anonymity Constraint</title><categories>cs.LG cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, proliferation of various online platforms and their
increasing adoption by billions of users have heightened the privacy risk of a
user enormously. In fact, security researchers have shown that sparse microdata
containing information about online activities of a user although anonymous,
can still be used to disclose the identity of the user by cross-referencing the
data with other data sources. To preserve the privacy of a user, in existing
works several methods (k-anonymity, l-diversity, differential privacy) are
proposed that ensure a dataset which is meant to share or publish bears small
identity disclosure risk. However, the majority of these methods modify the
data in isolation, without considering their utility in subsequent knowledge
discovery tasks, which makes these datasets less informative. In this work, we
consider labeled data that are generally used for classification, and propose
two methods for feature selection considering two goals: first, on the reduced
feature set the data has small disclosure risk, and second, the utility of the
data is preserved for performing a classification task. Experimental results on
various real-world datasets show that the method is effective and useful in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07160</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07160</id><created>2015-12-22</created><authors><author><keyname>Bae</keyname><forenames>Sang Won</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Computing the $L_1$ Geodesic Diameter and Center of a Polygonal Domain</title><categories>cs.CG cs.DS</categories><comments>24 pages, 12 figures; a preliminary version to appear in STACS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a polygonal domain with $h$ holes and a total of $n$ vertices, we present
algorithms that compute the $L_1$ geodesic diameter in $O(n^2+h^4)$ time and
the $L_1$ geodesic center in $O((n^4+n^2 h^4)\alpha(n))$ time, where
$\alpha(\cdot)$ denotes the inverse Ackermann function. No algorithms were
known for these problems before. For the Euclidean counterpart, the best
algorithms compute the geodesic diameter in $O(n^{7.73})$ or $O(n^7(h+\log n))$
time, and compute the geodesic center in $O(n^{12+\epsilon})$ time. Therefore,
our algorithms are much faster than the algorithms for the Euclidean problems.
Our algorithms are based on several interesting observations on $L_1$ shortest
paths in polygonal domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07162</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07162</id><created>2015-12-22</created><authors><author><keyname>Ma</keyname><forenames>Xi'ao</forenames></author><author><keyname>Wang</keyname><forenames>Guoyin</forenames></author><author><keyname>Yu</keyname><forenames>Hong</forenames></author></authors><title>Heuristic algorithms for finding distribution reducts in probabilistic
  rough set model</title><categories>cs.AI</categories><comments>44 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute reduction is one of the most important topics in rough set theory.
Heuristic attribute reduction algorithms have been presented to solve the
attribute reduction problem. It is generally known that fitness functions play
a key role in developing heuristic attribute reduction algorithms. The
monotonicity of fitness functions can guarantee the validity of heuristic
attribute reduction algorithms. In probabilistic rough set model, distribution
reducts can ensure the decision rules derived from the reducts are compatible
with those derived from the original decision table. However, there are few
studies on developing heuristic attribute reduction algorithms for finding
distribution reducts. This is partly due to the fact that there are no
monotonic fitness functions that are used to design heuristic attribute
reduction algorithms in probabilistic rough set model. The main objective of
this paper is to develop heuristic attribute reduction algorithms for finding
distribution reducts in probabilistic rough set model. For one thing, two
monotonic fitness functions are constructed, from which equivalence definitions
of distribution reducts can be obtained. For another, two modified monotonic
fitness functions are proposed to evaluate the significance of attributes more
effectively. On this basis, two heuristic attribute reduction algorithms for
finding distribution reducts are developed based on addition-deletion method
and deletion method. In particular, the monotonicity of fitness functions
guarantees the rationality of the proposed heuristic attribute reduction
algorithms. Results of experimental analysis are included to quantify the
effectiveness of the proposed fitness functions and distribution reducts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07173</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07173</id><created>2015-12-22</created><authors><author><keyname>Farshidian</keyname><forenames>Farbod</forenames></author><author><keyname>Buchli</keyname><forenames>Jonas</forenames></author></authors><title>Risk Sensitive, Nonlinear Optimal Control: Iterative Linear
  Exponential-Quadratic Optimal Control with Gaussian Noise</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we derive ILEG, an iterative algorithm to find risk
sensitive solutions to nonlinear, stochastic optimal control problems. The
algorithm is based on a linear quadratic approximation of an exponential risk
sensitive nonlinear control problem. ILEG allows to find risk sensitive
policies and thus generalizes previous algorithms to solve nonlinear optimal
control based on iterative linear-quadratic methods. Depending on the setting
of the parameter controlling the risk sensitivity, two different strategies on
how to cope with the risk emerge. For positive-value parameters, the control
policy uses high feedback gains whereas for negative-value parameters, it uses
a robust feedforward control strategy (a robust plan) with low gains. These
results are illustrated with a simple example. This note should be considered
as a preliminary report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07193</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07193</id><created>2015-12-22</created><authors><author><keyname>Waugh</keyname><forenames>Laura</forenames></author><author><keyname>Tarver</keyname><forenames>Hannah</forenames></author><author><keyname>Phillips</keyname><forenames>Mark</forenames></author><author><keyname>Alemneh</keyname><forenames>Daniel</forenames></author></authors><title>Comparison of full-text versus metadata searching in an institutional
  repository: Case study of the UNT Scholarly Works</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Authors in the library science field disagree about the importance of using
costly resources to create local metadata records, particularly for scholarly
materials that have full-text search alternatives. At the University of North
Texas (UNT) Libraries, we decided to test this concept by answering the
question: What percentage of search terms retrieved results based on full-text
versus metadata values for items in the UNT Scholarly Works institutional
repository? The analysis matched search query logs to indexes of the metadata
records and full text of the items in the collection. Results show the
distribution of item discoveries that were based on metadata exclusively, on
full text exclusively, and on the combination of both. This paper describes in
detail the methods and findings of this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07199</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07199</id><created>2015-12-21</created><authors><author><keyname>Chang</keyname><forenames>Chii</forenames></author><author><keyname>Srirama</keyname><forenames>Satish Narayana</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Mobile Cloud Business Process Management System for the Internet of
  Things: Review, Challenges and Blueprint</title><categories>cs.CY</categories><comments>12 pages, 3 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Internet of Things (IoT) represents a comprehensive environment that
consists of large number of sensors and mediators interconnecting heterogeneous
physical objects to the Internet. IoT applications can be found in many areas
such as smart city, smart workplace, smart plants, smart agriculture and
various ubiquitous computing areas. Meanwhile, Business Process Management
Systems (BPMS) has become a successful and efficient solution for coordinated
management and optimised utilisation of resources/entities. However, managing
large scale connected heterogeneous IoT entities in BPMS is facing many issues
that were not considered in the past BPMS. Without fully understanding the
behaviour, capability and state of the thing, the BPMS can fail to manage the
IoT integrated information systems. In this paper, we analyse existing BPMS for
IoT and identify the limitations and their drawbacks. Then, we try to improve
the existing works with the elastic mobile cloud resources. The aim of this
paper is to provide a guidance to Mobile Cloud Computing (MCC) disciplines what
are the issues in BPMS for IoT and how MCC can be the promising solution to
address issues and overcome the challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07207</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07207</id><created>2015-12-20</created><authors><author><keyname>Priyanka</keyname><forenames>B H</forenames></author><author><keyname>Prakash</keyname><forenames>Ravi</forenames></author></authors><title>A Critical Survey Of Privacy Infrastructures</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last two decades, the scale and complexity of the Internet and its
associated technologies built on the World Wide Web has grown exponentially
with access to Internet as a facility occupying a prime place with other
amenities of modern lives. In years to come, usage of Internet may unravel more
pleasant surprises for us as far as novelty in its usage is concerned. As a
democratic function of Internet, and relying on the open model on which it has
been built, there has been concerted efforts in the direction of privacy
protection and use of privacy enhancing tools which have gained tangible
traction. Innovation in use of VPN, TLS/SSL and cryptographic tools are a
testimony to it. Another popular tool is Tor, which has gained widespread
popularity as it is being increasingly used by anonymity seeking users to
effectively maintain their discretion while surfing the web.
  However, there is a darker side to increased proliferation of Internet in our
everyday routine. We are certainly not living in a utopian age and there are
potentials of misuse of Internet as well. Across every nook and cranny of
Internet's sprawling virtual world, there are cyber criminals lurking n
dangerous alleys to use the very same Internet as malevolent tool to abuse it
and cause financial, physical and social harm to ordinary people. Failing to
manage the widespread spawning of World Wide Web has rendered it weak against
misuse. In last few decades especially, Internet has been inundated with
malware, ransomware, viruses, Trojans, illegal spy tools and what not created
with malignant sentiments. In this paper, we will analyze few of the subverting
privacy infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07221</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07221</id><created>2015-12-22</created><authors><author><keyname>Dai</keyname><forenames>Mingbo</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>A Rate Splitting Strategy for Massive MIMO with Imperfect CSIT</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multiuser MIMO broadcast channel, the rate performance is affected by
the multiuser interference when the Channel State Information at the
Transmitter (CSIT) is imperfect. To tackle the detrimental effect of the
multiuser interference, a Rate-Splitting (RS) approach has been proposed
recently, which splits one selected user's message into a common and a private
part, and superimposes the common message on top of the private messages. The
common message is drawn from a public codebook and should be decoded by all
users. In this paper, we generalize the idea of RS into the large-scale array
regime with imperfect CSIT. By further exploiting the channel second-order
statistics, we propose a novel and general framework
Hierarchical-Rate-Splitting (HRS) that is particularly suited to massive MIMO
systems. HRS simultaneously transmits private messages intended to each user
and two kinds of common messages that can be decoded by all users and by a
subset of users, respectively. We analyse the asymptotic sum rate of RS and HRS
and optimize the precoders of the common messages. A closed-form power
allocation is derived which provides insights into the effects of system
parameters. Finally, simulation results validate the significant sum rate gain
of RS and HRS over various baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07248</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07248</id><created>2015-12-22</created><updated>2016-02-06</updated><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author><author><keyname>Mo</keyname><forenames>Qun</forenames></author></authors><title>A Sharp Condition for Exact Support Recovery of Sparse Signals with
  Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><comments>Part of the results have been submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support recovery of sparse signals from noisy measurements with orthogonal
matching pursuit (OMP) have been extensively studied in the literature. In this
paper, we show that for any $K$-sparse signal $\x$, if the sensing matrix $\A$
satisfies the restricted isometry property (RIP) with restricted isometry
constant $\delta_{K+1} &lt; 1/\sqrt {K+1}$, then under some constraints on the
minimum magnitude of the nonzero elements of $\x$, OMP exactly recovers the
support of $\x$ from the measurements $\y=\A\x+\v$ in $K$ iterations, where
$\v$ is a noise vector that is $l_2$ or $l_{\infty}$ bounded. The RIP condition
is sharp in terms of $\delta_{K+1}$ since for any given positive integer $K\geq
2$ and any $1/\sqrt{K+1}\leq t&lt;1$, there always exist a $K$-sparse $\x$ and a
matrix $\A$ satisfying $\delta_{K+1}=t$, for which OMP may fail to recover the
signal $\x$ in $K$ iterations. Also, our constraints on the minimum magnitude
of nonzero elements of $\x$ are much weaker than existing ones. Moreover, we
propose necessary conditions for exact support recovery of $\x$ on the minimum
magnitude of the nonzero elements of $\x$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07250</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07250</id><created>2015-12-22</created><updated>2016-01-04</updated><authors><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>A Triple Helix Model of Medical Innovation: Supply, Demand, and
  Technological Capabilities in terms of Medical Subject Headings</title><categories>cs.DL</categories><comments>Accepted for publication in Research Policy (in press)</comments><doi>10.1016/j.respol.2015.12.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model of innovation that enables us to trace the interplay among
three key dimensions of the innovation process: (i) demand of and (ii) supply
for innovation, and (iii) technological capabilities available to generate
innovation in the forms of products, processes, and services. Building on
triple helix research, we use entropy statistics to elaborate an indicator of
mutual information among these dimensions that can provide indication of
reduction of uncertainty. To do so, we focus on the medical context, where
uncertainty poses significant challenges to the governance of innovation. We
use the Medical Subject Headings (MeSH) of MEDLINE/PubMed to identify
publications classified within the categories &quot;Diseases&quot; (C), &quot;Drugs and
Chemicals&quot; (D), &quot;Analytic, Diagnostic, and Therapeutic Techniques and
Equipment&quot; (E) and use these as knowledge representations of demand, supply,
and technological capabilities, respectively. Three case-studies of medical
research areas are used as representative 'entry perspectives' of the medical
innovation process. These are: (i) human papilloma virus, (ii) RNA
interference, and (iii) magnetic resonance imaging. We find statistically
significant periods of synergy among demand, supply, and technological
capabilities (C-D-E) that point to three-dimensional interactions as a
fundamental perspective for the understanding and governance of the uncertainty
associated with medical innovation. Among the pairwise configurations in these
contexts, the demand-technological capabilities (C-E) provided the strongest
link, followed by the supply-demand (D-C) and the supply-technological
capabilities (D-E) channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07251</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07251</id><created>2015-12-22</created><updated>2015-12-23</updated><authors><author><keyname>Maldonado</keyname><forenames>Felipe</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author><author><keyname>Berbeglia</keyname><forenames>Franco</forenames></author></authors><title>Popularity Signals in Trial-Offer Markets</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers trial-offer and freemium markets where consumer
preferences are modeled by a multinomial logic model with social influence and
position bias. The social signal for a product i is its market share raised to
some power r. The paper shows that, when 0 &lt; r &lt; 1 and a static position
assignment (e.g., a quality ranking) is used, the market converges to a unique
equilibrium where the market shares depend only on product quality, not their
initial appeals or the early dynamics. When r &gt; 1, the market become
unpredictable and goes most likely to a monopoly for some product. Which
product becomes a monopoly depends on the initial conditions of the market.
These theoretical results are complemented by an agent-based simulation which
indicates that convergence is fast when 0 &lt; r &lt; 1 and that the quality ranking
dominates the well-known popularity ranking in terms of market efficiency.
These results shed a new light on the role of social influence which has been
believed to produce unpredictability, inequalities, and inefficiencies in
markets. In contrast, this paper shows that, with a proper social signal and a
proper position assignment for the products, the market becomes predictable and
inequalities and inefficiencies can be controlled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07258</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07258</id><created>2015-12-22</created><authors><author><keyname>Paranjape</keyname><forenames>Ashwin</forenames></author><author><keyname>West</keyname><forenames>Robert</forenames></author><author><keyname>Zia</keyname><forenames>Leila</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Improving Website Hyperlink Structure Using Server Logs</title><categories>cs.SI</categories><comments>in the Proceedings of the 9th International ACM Conference on Web
  Search and Data Mining 2016</comments><acm-class>H.5.4</acm-class><doi>10.1145/2835776.2835832</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good websites should be easy to navigate via hyperlinks, yet maintaining a
high-quality link structure is difficult. Identifying pairs of pages that
should be linked may be hard for human editors, especially if the site is large
and changes frequently. Further, given a set of useful link candidates, the
task of incorporating them into the site can be expensive, since it typically
involves humans editing pages. In the light of these challenges, it is
desirable to develop data-driven methods for automating the link placement
task. Here we develop an approach for automatically finding useful hyperlinks
to add to a website. We show that passively collected server logs, beyond
telling us which existing links are useful, also contain implicit signals
indicating which nonexistent links would be useful if they were to be
introduced. We leverage these signals to model the future usefulness of yet
nonexistent links. Based on our model, we define the problem of link placement
under budget constraints and propose an efficient algorithm for solving it. We
demonstrate the effectiveness of our approach by evaluating it on Wikipedia, a
large website for which we have access to both server logs (used for finding
useful new links) and the complete revision history (containing a ground truth
of new links). As our method is based exclusively on standard server logs, it
may also be applied to any other website, as we show with the example of the
biomedical research site Simtk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07263</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07263</id><created>2015-12-22</created><authors><author><keyname>Portegys</keyname><forenames>Tom</forenames></author></authors><title>General Graph Identification By Hashing</title><categories>cs.DS</categories><comments>First made public: February 2008 Code available at:
  https://sourceforge.net/projects/graph-hashing/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for identifying graphs using MD5 hashing is presented. This allows
fast graph equality comparisons and can also be used to facilitate graph
isomorphism testing. The graphs can be labeled or unlabeled. The method
identifies vertices by hashing the graph configuration in their neighborhoods.
With each vertex hashed, the entire graph can be identified by hashing the
vertex hashes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07271</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07271</id><created>2015-12-22</created><authors><author><keyname>Curti</keyname><forenames>Matteo</forenames></author><author><keyname>Iacus</keyname><forenames>Stefano</forenames></author><author><keyname>Porro</keyname><forenames>Giuseppe</forenames></author><author><keyname>Siletti</keyname><forenames>Elena</forenames></author></authors><title>Measuring Social Well Being in The Big Data Era: Asking or Listening?</title><categories>cs.CY</categories><comments>40 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1512.01569</comments><msc-class>91B15, 91C05, 97K80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The literature on well being measurement seems to suggest that &quot;asking&quot; for a
self-evaluation is the only way to estimate a complete and reliable measure of
well being. At the same time &quot;not asking&quot; is the only way to avoid biased
evaluations due to self-reporting. Here we propose a method for estimating the
welfare perception of a community simply &quot;listening&quot; to the conversations on
Social Network Sites. The Social Well Being Index (SWBI) and its components are
proposed through to an innovative technique of supervised sentiment analysis
called iSA which scales to any language and big data. As main methodological
advantages, this approach can estimate several aspects of social well being
directly from self-declared perceptions, instead of approximating it through
objective (but partial) quantitative variables like GDP; moreover
self-perceptions of welfare are spontaneous and not obtained as answers to
explicit questions that are proved to bias the result. As an application we
evaluate the SWBI in Italy through the period 2012-2015 through the analysis of
more than 143 millions of tweets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07281</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07281</id><created>2015-12-22</created><authors><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author></authors><title>Topical differences between Chinese language Twitter and Sina Weibo</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>5 pages, 2 figures, 2 tables, 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sina Weibo, China's most popular microblogging platform, is currently used by
over $500M$ users and is considered to be a proxy of Chinese social life. In
this study, we contrast the discussions occurring on Sina Weibo and on Chinese
language Twitter in order to observe two different strands of Chinese culture:
people within China who use Sina Weibo with its government imposed restrictions
and those outside that are free to speak completely anonymously. We first
propose a simple ad-hoc algorithm to identify topics of Tweets and Weibo.
Different from previous works on micro-message topic detection, our algorithm
considers topics of the same contents but with different \#tags. Our algorithm
can also detect topics for Tweets and Weibos without any \#tags. Using a large
corpus of Weibo and Chinese language tweets, covering the period from January
$1$ to December $31$, $2012$, we obtain a list of topics using clustered \#tags
that we can then use to compare the two platforms. Surprisingly, we find that
there are no common entries among the Top $100$ most popular topics.
Furthermore, only $9.2\%$ of tweets correspond to the Top $1000$ topics on Sina
Weibo platform, and conversely only $4.4\%$ of weibos were found to discuss the
most popular Twitter topics. Our results reveal significant differences in
social attention on the two platforms, with most popular topics on Sina Weibo
relating to entertainment while most tweets corresponded to cultural or
political contents that is practically non existent in Sina Weibo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07290</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07290</id><created>2015-12-22</created><authors><author><keyname>Nafea</keyname><forenames>Mohamed</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Secure Degrees of Freedom for the MIMO Wire-tap Channel with a
  Multi-antenna Cooperative Jammer</title><categories>cs.IT math.IT</categories><comments>48 pages, 2 figures. Submitted to IEEE Transactions on Information
  Theory, November 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a multiple antenna wire-tap channel in the presence of a
multi-antenna cooperative jammer is studied. In particular, the secure degrees
of freedom (s.d.o.f.) of this channel is established, with $N_t$ antennas at
the transmitter, $N_r$ antennas at the legitimate receiver, and $N_e$ antennas
at the eavesdropper, for all possible values of the number of antennas, $N_c$,
at the cooperative jammer. In establishing the result, several different ranges
of $N_c$ need to be considered separately. The lower and upper bounds for these
ranges of $N_c$ are derived, and are shown to be tight. The achievability
techniques developed rely on a variety of signaling, beamforming, and alignment
techniques which vary according to the (relative) number of antennas at each
terminal and whether the s.d.o.f. is integer valued. Specifically, it is shown
that, whenever the s.d.o.f. is integer valued, Gaussian signaling for both
transmission and cooperative jamming, linear precoding at the transmitter and
the cooperative jammer, and linear processing at the legitimate receiver, are
sufficient for achieving the s.d.o.f. of the channel. By contrast, when the
s.d.o.f. is not an integer, the achievable schemes need to rely on structured
signaling at the transmitter and the cooperative jammer, and joint signal space
and signal scale alignment. The converse is established by combining an upper
bound which allows for full cooperation between the transmitter and the
cooperative jammer, with another upper bound which exploits the secrecy and
reliability constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07293</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07293</id><created>2015-12-22</created><authors><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Optimal Sample Complexity for Blind Gain and Phase Calibration</title><categories>cs.IT math.IT</categories><comments>17 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1501.06120</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind gain and phase calibration (BGPC) is a structured bilinear inverse
problem, which arises in many applications, including inverse rendering in
computational relighting (albedo estimation with unknown lighting), blind phase
and gain calibration in sensor array processing, and multichannel blind
deconvolution. The fundamental question of the uniqueness of the solutions to
such problems has been addressed only recently. In a previous paper, we
proposed studying the identifiability in bilinear inverse problems up to
transformation groups. In particular, we studied several special cases of blind
gain and phase calibration, including the cases of subspace and joint sparsity
models on the signals, and gave sufficient and necessary conditions for
identifiability up to certain transformation groups. However, there were gaps
between the sample complexities in the sufficient conditions and the necessary
conditions. In this paper, under a mild assumption that the signals and models
are generic, we bridge the gaps by deriving tight sufficient conditions with
optimal sample complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07304</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07304</id><created>2015-12-22</created><authors><author><keyname>Bourke</keyname><forenames>Timothy</forenames><affiliation>INRIA</affiliation></author><author><keyname>van Glabbeek</keyname><forenames>Robert J.</forenames><affiliation>NICTA</affiliation></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames><affiliation>NICTA</affiliation></author></authors><title>Mechanizing a Process Algebra for Network Protocols</title><categories>cs.LO</categories><comments>This paper is an extended version of arXiv:1407.3519. The
  Isabelle/HOL source files, and a full proof document, are available in the
  Archive of Formal Proofs, at http://afp.sourceforge.net/entries/AWN.shtml</comments><acm-class>F.3.1; C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the mechanization of a process algebra for Mobile Ad hoc
Networks and Wireless Mesh Networks, and the development of a compositional
framework for proving invariant properties. Mechanizing the core process
algebra in Isabelle/HOL is relatively standard, but its layered structure
necessitates special treatment. The control states of reactive processes, such
as nodes in a network, are modelled by terms of the process algebra. We propose
a technique based on these terms to streamline proofs of inductive invariance.
This is not sufficient, however, to state and prove invariants that relate
states across multiple processes (entire networks). To this end, we propose a
novel compositional technique for lifting global invariants stated at the level
of individual nodes to networks of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07305</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07305</id><created>2015-12-22</created><authors><author><keyname>Paul</keyname><forenames>Sri Raj</forenames></author><author><keyname>Murthy</keyname><forenames>Karthik</forenames></author><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Mellor-Crummey</keyname><forenames>John</forenames></author></authors><title>Distributed Phasers</title><categories>cs.DC</categories><comments>2 page extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A phaser is an expressive synchronization construct that unifies collective
and point-to-point coordination with dynamic task parallelism. Each task can
participate in a phaser as a signaler, a waiter, or both. The participants in a
phaser may change over time as dynamic tasks are added and deleted. In this
poster, we present a highly concurrent and scalable design of phasers for a
distributed memory environment that is suitable for use with asynchronous
partitioned global address space programming models. Our design for a
distributed phaser employs a pair of skip lists augmented with the ability to
collect and propagate synchronization signals. To enable a high degree of
concurrency, addition and deletion of participant tasks are performed in two
phases: a &quot;fast single-link-modify&quot; step followed by multiple hand-overhand
&quot;lazy multi-link-modify&quot; steps. We show that the cost of synchronization and
structural operations on a distributed phaser scales logarithmically, even in
the presence of concurrent structural modifications. To verify the correctness
of our design for distributed phasers, we employ the SPIN model checker. To
address this issue of state space explosion, we describe how we decompose the
state space to separately verify correct handling for different kinds of
messages, which enables complete model checking of our phaser design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07310</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07310</id><created>2015-12-22</created><authors><author><keyname>Lu</keyname><forenames>X.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Opportunistic Cooperation Techniques using Jamming and Relays
  for Physical-Layer Security in Buffer-aided Relay Networks</title><categories>cs.IT math.IT</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate opportunistic relay and jammer cooperation
schemes in multiple-input multiple-output (MIMO) buffer-aided relay networks.
The network consists of one source, an arbitrary number of relay nodes,
legitimate users and eavesdroppers, with the constraints of physical layer
security. We propose an algorithm to select a set of relay nodes to enhance the
legitimate users' transmission and another set of relay nodes to perform
jamming of the eavesdroppers. With Inter-Relay interference (IRI) taken into
account, interference cancellation can be implemented to assist the
transmission of the legitimate users. Secondly, IRI can also be used to further
increase the level of harm of the jamming signal to the eavesdroppers. By
exploiting the fact that the jamming signal can be stored at the relay nodes,
we also propose a hybrid algorithm to set a signal-to-interference and noise
ratio (SINR) threshold at the node to determine the type of signal stored at
the relay node. With this separation, the signals with high SINR are delivered
to the users as conventional relay systems and the low SINR performance signals
are stored as potential jamming signals. Simulation results show that the
proposed techniques obtain a significant improvement in secrecy rate over
previously reported algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07311</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07311</id><created>2015-12-22</created><authors><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Wood</keyname><forenames>Christopher A.</forenames></author></authors><title>BEAD: Best Effort Autonomous Deletion in Content-Centric Networking</title><categories>cs.NI</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A core feature of Content-Centric Networking (CCN) is opportunistic content
caching in routers. It enables routers to satisfy content requests with
in-network cached copies, thereby reducing bandwidth utilization, decreasing
congestion, and improving overall content retrieval latency.
  One major drawback of in-network caching is that content producers have no
knowledge about where their content is stored. This is problematic if a
producer wishes to delete its content. In this paper, we show how to address
this problem with a protocol called BEAD (Best-Effort Autonomous Deletion).
BEAD achieves content deletion via small and secure packets that resemble
current CCN messages. We discuss several methods of routing BEAD messages from
producers to caching routers with varying levels of network overhead and
efficacy. We assess BEAD performance via simulations and provide a detailed
analysis of its properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07312</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07312</id><created>2015-12-22</created><authors><author><keyname>Fehnker</keyname><forenames>Ansgar</forenames></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author></authors><title>Modelling and Analysis of AODV in UPPAAL</title><categories>cs.NI cs.LO</categories><comments>in Proc. 1st International Workshop on Rigorous Protocol Engineering,
  WRiPE 2011</comments><acm-class>C.2.2; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes work in progress towards an automated formal and
rigorous analysis of the Ad hoc On-Demand Distance Vector (AODV) routing
protocol, a popular protocol used in ad hoc wireless networks. We give a brief
overview of a model of AODV implemented in the UPPAAL model checker, and
describe experiments carried out to explore AODV's behaviour in two network
topologies. We were able to locate automatically and confirm some known
problematic and undesirable behaviours. We believe this use of model checking
as a diagnostic tool complements other formal methods based protocol modelling
and verification techniques, such as process algebras. Model checking is in
particular useful for the discovery of protocol limitations and in the
development of improved variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07314</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07314</id><created>2015-12-22</created><authors><author><keyname>Nabi</keyname><forenames>Moin</forenames></author></authors><title>Mid-level Representation for Visual Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual Recognition is one of the fundamental challenges in AI, where the goal
is to understand the semantics of visual data. Employing mid-level
representation, in particular, shifted the paradigm in visual recognition. The
mid-level image/video representation involves discovering and training a set of
mid-level visual patterns (e.g., parts and attributes) and represent a given
image/video utilizing them. The mid-level patterns can be extracted from images
and videos using the motion and appearance information of visual phenomenas.
This thesis targets employing mid-level representations for different
high-level visual recognition tasks, namely (i)image understanding and
(ii)video understanding.
  In the case of image understanding, we focus on object detection/recognition
task. We investigate on discovering and learning a set of mid-level patches to
be used for representing the images of an object category. We specifically
employ the discriminative patches in a subcategory-aware webly-supervised
fashion. We, additionally, study the outcomes provided by employing the
subcategory-based models for undoing dataset bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07319</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07319</id><created>2015-12-22</created><authors><author><keyname>Fehnker</keyname><forenames>Ansgar</forenames></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author></authors><title>A Process Algebra for Wireless Mesh Networks</title><categories>cs.LO cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.7645</comments><acm-class>F.3.2; F.3.1; C.2.2</acm-class><journal-ref>Proc. 21st European Symposium on Programming, ESOP'12, (Helmut
  Seidl, ed.), LNCS 7211, Springer, 2012, pp. 295-315</journal-ref><doi>10.1007/978-3-642-28869-2_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a process algebra for wireless mesh networks that combines novel
treatments of local broadcast, conditional unicast and data structures. In this
framework, we model the Ad-hoc On-Demand Distance Vector (AODV) routing
protocol and (dis)prove crucial properties such as loop freedom and packet
delivery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07330</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07330</id><created>2015-12-22</created><authors><author><keyname>Gupta</keyname><forenames>Srishti</forenames></author><author><keyname>Gupta</keyname><forenames>Payas</forenames></author><author><keyname>Ahamad</keyname><forenames>Mustaque</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Abusing Phone Numbers and Cross-Application Features for Crafting
  Targeted Attacks</title><categories>cs.SI cs.CR</categories><comments>Submitted to AsiaCCS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the convergence of Internet and telephony, new applications (e.g.,
WhatsApp) have emerged as an important means of communication for billions of
users. These applications are becoming an attractive medium for attackers to
deliver spam and carry out more targeted attacks. Since such applications rely
on phone numbers, we explore the feasibility, automation, and scalability of
phishing attacks that can be carried out by abusing a phone number. We
demonstrate a novel system that takes a potential victim's phone number as an
input, leverages information from applications like Truecaller and Facebook
about the victim and his / her social network, checks the presence of phone
number's owner (victim) on the attack channels (over-the-top or OTT messaging
applications, voice, e-mail, or SMS), and finally targets the victim on the
chosen channel. As a proof of concept, we enumerate through a random pool of
1.16 million phone numbers. By using information provided by popular
applications, we show that social and spear phishing attacks can be launched
against 51,409 and 180,000 users respectively. Furthermore, voice phishing or
vishing attacks can be launched against 722,696 users. We also found 91,487
highly attractive targets who can be attacked by crafting whaling attacks. We
show the effectiveness of one of these attacks, phishing, by conducting an
online roleplay user study. We found that social (69.2%) and spear (54.3%)
phishing attacks are more successful than non-targeted phishing attacks (35.5%)
on OTT messaging applications. Although similar results were found for other
mediums like e-mail, we demonstrate that due to the significantly increased
user engagement via new communication applications and the ease with which
phone numbers allow collection of information necessary for these attacks,
there is a clear need for better protection of OTT messaging applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07331</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07331</id><created>2015-12-22</created><authors><author><keyname>Sreehari</keyname><forenames>Suhas</forenames></author><author><keyname>Venkatakrishnan</keyname><forenames>S. V.</forenames></author><author><keyname>Wohlberg</keyname><forenames>Brendt</forenames></author><author><keyname>Drummy</keyname><forenames>Lawrence F.</forenames></author><author><keyname>Simmons</keyname><forenames>Jeffrey P.</forenames></author><author><keyname>Bouman</keyname><forenames>Charles A.</forenames></author></authors><title>Plug-and-Play Priors for Bright Field Electron Tomography and Sparse
  Interpolation</title><categories>cs.CV</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many material and biological samples in scientific imaging are characterized
by non-local repeating structures. These are studied using scanning electron
microscopy and electron tomography. Sparse sampling of individual pixels in a
2D image acquisition geometry, or sparse sampling of projection images with
large tilt increments in a tomography experiment, can enable high speed data
acquisition and minimize sample damage caused by the electron beam.
  In this paper, we present an algorithm for electron tomographic
reconstruction and sparse image interpolation that exploits the non-local
redundancy in images. We adapt a framework, termed plug-and-play (P&amp;P) priors,
to solve these imaging problems in a regularized inversion setting. The power
of the P&amp;P approach is that it allows a wide array of modern denoising
algorithms to be used as a &quot;prior model&quot; for tomography and image
interpolation. We also present sufficient mathematical conditions that ensure
convergence of the P&amp;P approach, and we use these insights to design a new
non-local means denoising algorithm. Finally, we demonstrate that the algorithm
produces higher quality reconstructions on both simulated and real electron
microscope data, along with improved convergence properties compared to other
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07332</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07332</id><created>2015-12-22</created><authors><author><keyname>Sadik</keyname><forenames>Md. Muntakim</forenames></author><author><keyname>Malek</keyname><forenames>Sakib Md. Bin</forenames></author><author><keyname>Rahman</keyname><forenames>Ashikur</forenames></author></authors><title>On Balanced k-coverage in Visual Sensor Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of directional visual sensors, the $k$-coverage problem
determines the orientation of minimal directional sensors so that each target
is covered at least $k$ times. As the problem is NP-complete, a number of
heuristics have been devised to tackle the issue. However, the existing
heuristics provide imbalance coverage of the targets--some targets are covered
$k$ times while others are left totally uncovered or singly covered. The
coverage imbalance is more serious in under-provisioned networks where there do
not exist enough sensors to cover all the targets $k$ times. Therefore, we
address the problem of covering each target at least $k$ times in a balanced
way using minimum number of sensors. We study the existing Integer Linear
Programming (ILP) formulation for single coverage and extend the idea for
$k$-coverage. However, the extension does not balance the coverage of the
targets. We further propose Integer Quadratic Programming (IQP) and Integer
Non-Linear Programming (INLP) formulations that are capable of addressing the
coverage balancing. As the proposed formulations are computationally expensive,
we devise a faster Centralized Greedy $k$-Coverage Algorithm (CGkCA) to
approximate the formulations. Finally, through rigorous simulation experiments
we show the efficacy of the proposed formulations and the CGkCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07336</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07336</id><created>2015-12-22</created><authors><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Deng</keyname><forenames>Yuntian</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Latent Variable Modeling with Diversity-Inducing Mutual Angular
  Regularization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Variable Models (LVMs) are a large family of machine learning models
providing a principled and effective way to extract underlying patterns,
structure and knowledge from observed data. Due to the dramatic growth of
volume and complexity of data, several new challenges have emerged and cannot
be effectively addressed by existing LVMs: (1) How to capture long-tail
patterns that carry crucial information when the popularity of patterns is
distributed in a power-law fashion? (2) How to reduce model complexity and
computational cost without compromising the modeling power of LVMs? (3) How to
improve the interpretability and reduce the redundancy of discovered patterns?
To addresses the three challenges discussed above, we develop a novel
regularization technique for LVMs, which controls the geometry of the latent
space during learning to enable the learned latent components of LVMs to be
diverse in the sense that they are favored to be mutually different from each
other, to accomplish long-tail coverage, low redundancy, and better
interpretability. We propose a mutual angular regularizer (MAR) to encourage
the components in LVMs to have larger mutual angles. The MAR is non-convex and
non-smooth, entailing great challenges for optimization. To cope with this
issue, we derive a smooth lower bound of the MAR and optimize the lower bound
instead. We show that the monotonicity of the lower bound is closely aligned
with the MAR to qualify the lower bound as a desirable surrogate of the MAR.
Using neural network (NN) as an instance, we analyze how the MAR affects the
generalization performance of NN. On two popular latent variable models ---
restricted Boltzmann machine and distance metric learning, we demonstrate that
MAR can effectively capture long-tail patterns, reduce model complexity without
sacrificing expressivity and improve interpretability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07341</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07341</id><created>2015-12-22</created><authors><author><keyname>Wang</keyname><forenames>Qiuyan</forenames></author><author><keyname>Li</keyname><forenames>Fei</forenames></author><author><keyname>Ding</keyname><forenames>Kelan</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>Complete weight enumerators of two classes of linear codes</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><msc-class>94A05, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, linear codes with few weights have been constructed and extensively
studied. In this paper, for an odd prime p, we determined the complete weight
enumerator of two classes of p-ary linear codes constructed from defining set.
Results show that the codes are at almost seven-weight linear codes and they
may have applications in secret sharing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07344</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07344</id><created>2015-12-22</created><authors><author><keyname>Pu</keyname><forenames>Yunchen</forenames></author><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Stevens</keyname><forenames>Andrew</forenames></author><author><keyname>Li</keyname><forenames>Chunyuan</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>A Deep Generative Deconvolutional Image Model</title><categories>cs.CV cs.LG stat.ML</categories><comments>10 pages, 7 figures. Appearing in Proceedings of the 19th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
  2016, Cadiz, Spain. JMLR: W&amp;CP volume 41</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deep generative model is developed for representation and analysis of
images, based on a hierarchical convolutional dictionary-learning framework.
Stochastic {\em unpooling} is employed to link consecutive layers in the model,
yielding top-down image generation. A Bayesian support vector machine is linked
to the top-layer features, yielding max-margin discrimination. Deep
deconvolutional inference is employed when testing, to infer the latent
features, and the top-layer features are connected with the max-margin
classifier for discrimination tasks. The model is efficiently trained using a
Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on
graphical processor units (GPUs) for efficient large-scale learning, and fast
testing. Excellent results are obtained on several benchmark datasets,
including ImageNet, demonstrating that the proposed model achieves results that
are highly competitive with similarly sized convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07347</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07347</id><created>2015-12-22</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Zgang</keyname><forenames>Liang</forenames></author></authors><title>Galois Self-Dual Constacyclic Codes</title><categories>cs.IT math.IT math.RA</categories><comments>Key words: Constacyclic code, Galois inner product, $q$-coset
  function, isometry, Galois self-dual code</comments><msc-class>12E20, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalizing Euclidean inner product and Hermitian inner product, we
introduce Galois inner products, and study the Galois self-dual constacyclic
codes in a very general setting by a uniform method. The conditions for
existence of Galois self-dual and isometrically Galois self-dual constacyclic
codes are obtained. As consequences, the results on self-dual, iso-dual and
Hermitian self-dual constacyclic codes are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07349</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07349</id><created>2015-12-22</created><updated>2016-02-12</updated><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Zhang</keyname><forenames>Baichuan</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>Incremental Method for Spectral Clustering of Increasing Orders</title><categories>cs.SI cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)
of a graph Laplacian matrix have been widely used for spectral clustering and
community detection. However, in real-life applications the number of clusters
or communities (say, K) is generally unknown a-priori. Consequently, the
majority of the existing methods either choose K heuristically or they repeat
the clustering method with different choices of K and accept the best
clustering result. The first option, more often, yield suboptimal result, while
the second option is computationally expensive. In this work, we propose an
incremental method for constructing the eigenspectrum of the graph Laplacian
matrix. This method leverages the eigenstructure of graph Laplacian matrix to
obtain the K-th eigenpairs of the Laplacian matrix given a collection of all
the K-1 smallest eigenpairs. Our proposed method adapts the Laplacian matrix
such that the batch eigenvalue decomposition problem transforms into an
efficient sequential leading eigenpair computation problem. As a practical
application, we consider user-guided spectral clustering. Specifically, we
demonstrate that users can utilize the proposed incremental method for
effective eigenpair computation and determining the desired number of clusters
based on multiple clustering metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07351</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07351</id><created>2015-12-22</created><authors><author><keyname>Guo</keyname><forenames>Yao</forenames></author><author><keyname>Lu</keyname><forenames>Junyang</forenames></author></authors><title>Energy-aware Fixed-Priority Multi-core Scheduling for Real-time Systems</title><categories>cs.OS</categories><comments>conference extension</comments><acm-class>C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-core processors are becoming more and more popular in embedded and
real-time systems. While fixed-priority scheduling with task-splitting in
real-time systems are widely applied, current approaches have not taken into
consideration energy-aware aspects such as dynamic voltage/frequency scheduling
(DVS). In this paper, we propose two strategies to apply dynamic voltage
scaling (DVS) to fixed-priority scheduling algorithms with task-splitting for
periodic real-time tasks on multi-core processors. The first strategy
determines voltage scales for each processor after scheduling (Static DVS),
which ensures all tasks meet the timing requirements on synchronization. The
second strategy adaptively determines the frequency of each task before
scheduling (Adaptive DVS) according to the total utilization of task-set and
number of cores available. The combination of frequency pre-allocation and
task-splitting makes it possible to maximize energy savings with DVS.
Simulation results show that it is possible to achieve significant energy
savings with DVS while preserving the schedulability requirements of real-time
schedulers for multi-core processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07352</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07352</id><created>2015-12-22</created><authors><author><keyname>Fehnker</keyname><forenames>Ansgar</forenames></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author></authors><title>Automated Analysis of AODV using UPPAAL</title><categories>cs.NI cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1512.07312</comments><acm-class>C.2.2; D.2.4</acm-class><journal-ref>Proc. Tools and Algorithms for the Construction and Analysis of
  Systems, TACAS'12 (C. Flanagan &amp; B. K\&quot;onig, eds.), LNCS 7214, Springer,
  2012, pp. 173-187</journal-ref><doi>10.1007/978-3-642-28756-5_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an automated, formal and rigorous analysis of the Ad hoc
On-Demand Distance Vector (AODV) routing protocol, a popular protocol used in
wireless mesh networks.
  We give a brief overview of a model of AODV implemented in the UPPAAL model
checker. It is derived from a process-algebraic model which reflects precisely
the intention of AODV and accurately captures the protocol specification.
Furthermore, we describe experiments carried out to explore AODV's behaviour in
all network topologies up to 5 nodes. We were able to automatically locate
problematic and undesirable behaviours. This is in particular useful to
discover protocol limitations and to develop improved variants. This use of
model checking as a diagnostic tool complements other formal-methods-based
protocol modelling and verification techniques, such as process algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07362</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07362</id><created>2015-12-23</created><updated>2016-02-29</updated><authors><author><keyname>Jain</keyname><forenames>Anoop</forenames></author><author><keyname>Ghose</keyname><forenames>Debasish</forenames></author></authors><title>Synchronization of Multi-Agent Systems With Heterogeneous Controllers</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the synchronization of a multi-agent system where the
agents are coupled through heterogeneous controller gains. Synchronization
refers to the situation where all the agents in a group have a common velocity
direction. We generalize existing results and show that by using heterogeneous
controller gains, the final velocity direction at which the system of agents
synchronize can be controlled. The effect of heterogeneous gains on the
reachable set of this final velocity direction is further analyzed. We also
show that for realistic systems, a limited control force to stabilize the
agents to the synchronized condition can be achieved by confining these
heterogeneous controller gains to an upper bound. Simulations are given to
support the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07370</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07370</id><created>2015-12-23</created><authors><author><keyname>Park</keyname><forenames>Taejin</forenames></author><author><keyname>Lee</keyname><forenames>Taejin</forenames></author></authors><title>Musical instrument sound classification with deep convolutional neural
  network using feature fusion approach</title><categories>cs.SD cs.IR</categories><comments>14 pages, 5 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A new musical instrument classification method using convolutional neural
networks (CNNs) is presented in this paper. Unlike the traditional methods, we
investigated a scheme for classifying musical instruments using the learned
features from CNNs. To create the learned features from CNNs, we not only used
a conventional spectrogram image, but also proposed multiresolution recurrence
plots (MRPs) that contain the phase information of a raw input signal.
Consequently, we fed the characteristic timbre of the particular instrument
into a neural network, which cannot be extracted using a phase-blinded
representations such as a spectrogram. By combining our proposed MRPs and
spectrogram images with a multi-column network, the performance of our proposed
classifier system improves over a system that uses only a spectrogram.
Furthermore, the proposed classifier also outperforms the baseline result from
traditional handcrafted features and classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07372</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07372</id><created>2015-12-23</created><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Choudhury</keyname><forenames>Sutanay</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>Multi-centrality Graph Spectral Decompositions and their Application to
  Cyber Intrusion Detection</title><categories>cs.SI cs.CR stat.ML</categories><comments>To appear in ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern datasets can be represented as graphs and hence spectral
decompositions such as graph principal component analysis (PCA) can be useful.
Distinct from previous graph decomposition approaches based on subspace
projection of a single topological feature, e.g., the Fiedler vector of
centered graph adjacency matrix (graph Laplacian), we propose spectral
decomposition approaches to graph PCA and graph dictionary learning that
integrate multiple features, including graph walk statistics, centrality
measures and graph distances to reference nodes. In this paper we propose a new
PCA method for single graph analysis, called multi-centrality graph PCA
(MC-GPCA), and a new dictionary learning method for ensembles of graphs, called
multi-centrality graph dictionary learning (MC-GDL), both based on spectral
decomposition of multi-centrality matrices. As an application to cyber
intrusion detection, MC-GPCA can be an effective indicator of anomalous
connectivity pattern and MC-GDL can provide discriminative basis for attack
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07402</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07402</id><created>2015-12-23</created><authors><author><keyname>Dousti</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Shafaei</keyname><forenames>Alireza</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Squash 2: A Hierarchical Scalable Quantum Mapper Considering Ancilla
  Sharing</title><categories>quant-ph cs.ET</categories><comments>To appear in Quantum Information Computation Vol.16 No. 3&amp;4,
  pp0332-0356. arXiv admin note: substantial text overlap with arXiv:1412.8004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a multi-core reconfigurable quantum processor architecture, called
Requp, which supports a hierarchical approach to mapping a quantum algorithm
while sharing physical and logical ancilla qubits. Each core is capable of
performing any quantum instruction. Moreover, we introduce a scalable quantum
mapper, called Squash 2, which divides a given quantum circuit into a number of
quantum modules---each module is divided into k parts such that each part will
run on one of k available cores. Experimental results demonstrate that Squash~2
can handle large-scale quantum algorithms while providing an effective
mechanism for sharing ancilla qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07422</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07422</id><created>2015-12-23</created><authors><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames></author><author><keyname>Huang</keyname><forenames>Jim</forenames></author><author><keyname>Archambeau</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Adaptive Algorithms for Online Convex Optimization with Long-term
  Constraints</title><categories>stat.ML cs.LG math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an adaptive online gradient descent algorithm to solve online
convex optimization problems with long-term constraints , which are constraints
that need to be satisfied when accumulated over a finite number of rounds T ,
but can be violated in intermediate rounds. For some user-defined trade-off
parameter $\beta$ $\in$ (0, 1), the proposed algorithm achieves cumulative
regret bounds of O(T^max{$\beta$,1--$\beta$}) and O(T^(1--$\beta$/2)) for the
loss and the constraint violations respectively. Our results hold for convex
losses and can handle arbitrary convex constraints without requiring knowledge
of the number of rounds in advance. Our contributions improve over the best
known cumulative regret bounds by Mahdavi, et al. (2012) that are respectively
O(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and
O(T^2/3) when further restricting to polyhedral domains. We supplement the
analysis with experiments validating the performance of our algorithm in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07423</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07423</id><created>2015-12-23</created><authors><author><keyname>Cornu</keyname><forenames>Benoit</forenames></author><author><keyname>Durieux</keyname><forenames>Thomas</forenames></author><author><keyname>Seinturier</keyname><forenames>Lionel</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>NPEFix: Automatic Runtime Repair of Null Pointer Exceptions in Java</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Null pointer exceptions, also known as null dereferences are the number one
exceptions in the field. In this paper, we propose 9 alternative execution
semantics when a null pointer exception is about to happen. We implement those
alternative execution strategies using code transformation in a tool called
NPEfix. We evaluate our prototype implementation on 11 field null dereference
bugs and 519 seeded failures and show that NPEfix is able to repair at runtime
10/11 and 318/519 failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07430</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07430</id><created>2015-12-23</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>The ERA of FOLE: Foundation</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the representation of ontologies in the first-order
logical environment FOLE (Kent 2013). An ontology defines the primitives with
which to model the knowledge resources for a community of discourse (Gruber
2009). These primitives, consisting of classes, relationships and properties,
are represented by the entity-relationship-attribute ERA data model (Chen
1976). An ontology uses formal axioms to constrain the interpretation of these
primitives. In short, an ontology specifies a logical theory. This paper is the
first in a series of three papers that provide a rigorous mathematical
representation for the ERA data model in particular, and ontologies in general,
within the first-order logical environment FOLE. The first two papers show how
FOLE represents the formalism and semantics of (many-sorted) first-order logic
in a classification form corresponding to ideas discussed in the Information
Flow Framework (IFF). In particular, this first paper provides a foundation
that connects elements of the ERA data model with components of the first-order
logical environment FOLE, and the second paper provides a superstructure that
extends FOLE to the formalisms of first-order logic. The third paper defines an
interpretation of FOLE in terms of the transformational passage, first
described in (Kent 2013), from the classification form of first-order logic to
an equivalent interpretation form, thereby defining the formalism and semantics
of first-order logical/relational database systems (Kent 2011). The FOLE
representation follows a conceptual structures approach, that is completely
compatible with formal concept analysis (Ganter and Wille 1999) and information
flow (Barwise and Seligman 1997).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07435</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07435</id><created>2015-12-23</created><authors><author><keyname>Musco</keyname><forenames>Vincenzo</forenames></author><author><keyname>Carette</keyname><forenames>Antonin</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author><author><keyname>Preux</keyname><forenames>Philippe</forenames></author></authors><title>A Learning Algorithm for Change Impact Prediction: Experimentation on 7
  Java Applications</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change impact analysis consists in predicting the impact of a code change in
a software application. In this paper, we take a learning perspective on change
impact analysis and consider the problem formulated as follows. The artifacts
that are considered are methods of object-oriented software; the change under
study is a change in the code of the method, the impact is the test methods
that fail because of the change that has been performed. We propose an
algorithm, called LCIP that learns from past impacts to predict future impacts.
To evaluate our system, we consider 7 Java software applications totaling
214,000+ lines of code. We simulate 17574 changes and their actual impact
through code mutations, as done in mutation testing. We find that LCIP can
predict the impact with a precision of 69%, a recall of 79%, corresponding to a
F-Score of 55%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07437</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07437</id><created>2015-12-23</created><authors><author><keyname>Gudys</keyname><forenames>Adam</forenames></author><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author></authors><title>QuickProbs 2: towards rapid construction of high-quality alignments of
  large protein families</title><categories>q-bio.QM cs.CE cs.DC cs.DS</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the face of increasing size of sequence databases caused by the
development of high throughput sequencing, multiple alignment algorithms face
one of the greatest challenges yet. In this paper we show that well-established
techniques, refinement and consistency, are ineffective when large protein
families are of interest. We present QuickProbs 2, an algorithm for multiple
sequence alignment. Based on probabilistic models, equipped with novel
column-oriented refinement and selective consistency, it offers outstanding
accuracy. When analysing hundreds of sequences, QuickProbs 2 properly aligns up
to 20% more columns than ClustalOmega, the previous leader for processing
numerous protein families. In the case of smaller sets, for which
consistency-based methods are the best performing, QuickProbs 2 is also
superior to the competitors. Due to computational scalability of selective
consistency and utilisation of massively parallel architectures, presented
algorithm is comparable to ClustalOmega in terms of execution time, and orders
of magnitude faster than full consistency approaches, like MSAProbs or PixAA.
All these make QuickProbs 2 a useful tool for aligning families ranging from
few, to hundreds of proteins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07438</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07438</id><created>2015-12-23</created><authors><author><keyname>Wendzel</keyname><forenames>Steffen</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Zander</keyname><forenames>Sebastian</forenames></author></authors><title>Unified Description for Network Information Hiding Methods</title><categories>cs.CR</categories><comments>24 pages, 7 figures, 1 table; currently under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until now hiding methods in network steganography have been described in
arbitrary ways, making them difficult to compare. For instance, some
publications describe classical channel characteristics, such as robustness and
bandwidth, while others describe the embedding of hidden information. We
introduce the first unified description of hiding methods in network
steganography. Our description method is based on a comprehensive analysis of
the existing publications in the domain. When our description method is applied
by the research community, future publications will be easier to categorize,
compare and extend. Our method can also serve as a basis to evaluate the
novelty of hiding methods proposed in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07444</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07444</id><created>2015-12-23</created><authors><author><keyname>Kouneli</keyname><forenames>Marianna</forenames></author></authors><title>Exploiting Hierarchy for Ranking-based Recommendation</title><categories>cs.IR cs.SI</categories><comments>81 pages, M.Sc. Thesis (in Greek), Department of Computer Engineering
  and Informatics, University of Patras</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this master's thesis is to study and develop a new algorithmic
framework for collaborative filtering (CF) to generate recommendations. The
method we propose is based on the exploitation of the hierarchical structure of
the item space and intuitively &quot;stands&quot; on the property of Near Complete
Decomposability (NCD) which is inherent in the structure of the majority of
hierarchical systems. Building on the intuition behind the NCDawareRank
algorithm and its related concept of NCD proximity, we model our system in a
way that illuminates its endemic characteristics and we propose a new
algorithmic framework for recommendations, called HIR. We focus on combining
the direct with the NCD &quot;neighborhoods&quot; of items to achieve better
characterization of the inter-item relations, in order to improve the quality
of recommendations and alleviate sparsity related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07446</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07446</id><created>2015-12-23</created><authors><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Jinsung Yoon. Mihaela</forenames></author></authors><title>Adaptive Ensemble Learning with Confidence Bounds</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting actionable intelligence from distributed, heterogeneous,
correlated and high-dimensional data sources requires run-time processing and
learning both locally and globally. In the last decade, a large number of
meta-learning techniques have been proposed in which local learners make online
predictions based on their locally-collected data instances, and feed these
predictions to an ensemble learner, which fuses them and issues a global
prediction. However, most of these works do not provide performance guarantees
or, when they do, these guarantees are asymptotic. None of these existing works
provide confidence estimates about the issued predictions or rate of learning
guarantees for the ensemble learner. In this paper, we provide a systematic
ensemble learning method called Hedged Bandits, which comes with both long run
(asymptotic) and short run (rate of learning) performance guarantees. Moreover,
we show that our proposed method outperforms all existing ensemble learning
techniques, even in the presence of concept drift. We illustrate the
performance of Hedged Bandits in the context of medical informatics. However,
the proposed methods have numerous other applications, including network
monitoring and security, online recommendation systems, social networks, smart
cities, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07449</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07449</id><created>2015-12-23</created><authors><author><keyname>Romauch</keyname><forenames>Martin</forenames></author><author><keyname>Vidal</keyname><forenames>Thibaut</forenames></author><author><keyname>Hartl</keyname><forenames>Richard F.</forenames></author></authors><title>The lateral transhipment problem with a-priori routes, and a lot sizing
  application</title><categories>cs.DS</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose exact solution approaches for a lateral transhipment problem
which, given a pre-specified sequence of customers, seeks an optimal inventory
redistribution plan considering travel costs and profits dependent on inventory
levels. Trip-duration and vehicle-capacity constraints are also imposed. The
same problem arises in some lot sizing applications, in the presence of setup
costs and equipment re-qualifications.
  We introduce a pure dynamic programming approach and a branch-and-bound
framework that combines dynamic programming with Lagrangian relaxation.
Computational experiments are conducted to determine the most suitable solution
approach for different instances, depending on their size, vehicle capacities
and duration constraints. The branch-and-bound approach, in particular, solves
problems with up to 50 delivery locations in less than ten seconds on a modern
computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07450</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07450</id><created>2015-12-23</created><updated>2016-01-04</updated><authors><author><keyname>Adams</keyname><forenames>Alyssa</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Reyes</keyname><forenames>Eduardo Hermo</forenames></author><author><keyname>Joosten</keyname><forenames>Joost</forenames></author></authors><title>Interacting Behavior and Emerging Complexity</title><categories>cs.NE cs.CC nlin.CG q-bio.PE</categories><comments>11 pages, 5 figures (in this version a minor typo corrected).
  Presented at AUTOMATA 2015 forthcoming in journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we quantify the change of complexity throughout evolutionary processes?
We attempt to address this question through an empirical approach. In very
general terms, we simulate two simple organisms on a computer that compete over
limited available resources. We implement Global Rules that determine the
interaction between two Elementary Cellular Automata on the same grid. Global
Rules change the complexity of the state evolution output which suggests that
some complexity is intrinsic to the interaction rules themselves. The largest
increases in complexity occurred when the interacting elementary rules had very
little complexity, suggesting that they are able to accept complexity through
interaction only. We also found that some Class 3 or 4 CA rules are more
fragile than others to Global Rules, while others are more robust, hence
suggesting some intrinsic properties of the rules independent of the Global
Rule choice. We provide statistical mappings of Elementary Cellular Automata
exposed to Global Rules and different initial conditions onto different
complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07454</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07454</id><created>2015-12-23</created><authors><author><keyname>Hanbury</keyname><forenames>Allan</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Henning</forenames></author><author><keyname>Balog</keyname><forenames>Krisztian</forenames></author><author><keyname>Brodt</keyname><forenames>Torben</forenames></author><author><keyname>Cormack</keyname><forenames>Gordon V.</forenames></author><author><keyname>Eggel</keyname><forenames>Ivan</forenames></author><author><keyname>Gollub</keyname><forenames>Tim</forenames></author><author><keyname>Hopfgartner</keyname><forenames>Frank</forenames></author><author><keyname>Kalpathy-Cramer</keyname><forenames>Jayashree</forenames></author><author><keyname>Kando</keyname><forenames>Noriko</forenames></author><author><keyname>Krithara</keyname><forenames>Anastasia</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author><author><keyname>Mercer</keyname><forenames>Simon</forenames></author><author><keyname>Potthast</keyname><forenames>Martin</forenames></author></authors><title>Evaluation-as-a-Service: Overview and Outlook</title><categories>cs.CY cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation in empirical computer science is essential to show progress and
assess technologies developed. Several research domains such as information
retrieval have long relied on systematic evaluation to measure progress: here,
the Cranfield paradigm of creating shared test collections, defining search
tasks, and collecting ground truth for these tasks has persisted up until now.
In recent years, however, several new challenges have emerged that do not fit
this paradigm very well: extremely large data sets, confidential data sets as
found in the medical domain, and rapidly changing data sets as often
encountered in industry. Also, crowdsourcing has changed the way that industry
approaches problem-solving with companies now organizing challenges and handing
out monetary awards to incentivize people to work on their challenges,
particularly in the field of machine learning.
  This white paper is based on discussions at a workshop on
Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets
to participants and have them work on the data locally, but keeping the data
central and allowing access via Application Programming Interfaces (API),
Virtual Machines (VM) or other possibilities to ship executables. The objective
of this white paper are to summarize and compare the current approaches and
consolidate the experiences of these approaches to outline the next steps of
EaaS, particularly towards sustainable research infrastructures.
  This white paper summarizes several existing approaches to EaaS and analyzes
their usage scenarios and also the advantages and disadvantages. The many
factors influencing EaaS are overviewed, and the environment in terms of
motivations for the various stakeholders, from funding agencies to challenge
organizers, researchers and participants, to industry interested in supplying
real-world problems for which they require solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07459</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07459</id><created>2015-12-23</created><authors><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Fischer</keyname><forenames>Mareike</forenames></author><author><keyname>Moulton</keyname><forenames>Vincent</forenames></author><author><keyname>Wu</keyname><forenames>Taoyang</forenames></author></authors><title>Reduction rules for the maximum parsimony distance on phylogenetic trees</title><categories>q-bio.PE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In phylogenetics, distances are often used to measure the incongruence
between a pair of phylogenetic trees that are reconstructed by different
methods or using different regions of genome. Motivated by the maximum
parsimony principle in tree inference, we recently introduced the maximum
parsimony (MP) distance, which enjoys various attractive properties due to its
connection with several other well-known tree distances, such as TBR and SPR.
Here we show that computing the MP distance between two trees, a NP-hard
problem in general, is fixed parameter tractable in terms of the TBR distance
between the tree pair. Our approach is based on two reduction rules--the chain
reduction and the subtree reduction--that are widely used in computing TBR and
SPR distances. More precisely, we show that reducing chains to length 4 (but
not shorter) preserves the MP distance. In addition, we describe a
generalization of the subtree reduction which allows the pendant subtrees to be
rooted in different places, and show that this still preserves the MP distance.
We conclude with an extended discussion in which we focus on similarities and
differences between MP distance and TBR distance, and present a number of open
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07469</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07469</id><created>2015-12-23</created><authors><author><keyname>Che</keyname><forenames>Yueling</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Dynamic Base Station Operation in Large-Scale Green Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication. 29 pages, 6 figures, and
  1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, to minimize the on-grid energy cost in a large-scale green
cellular network, we jointly design the optimal base station (BS) on/off
operation policy and the on-grid energy purchase policy from a network-level
perspective. Due to the fluctuations of the on-grid energy prices, the
harvested renewable energy, and the network traffic loads over time, as well as
the BS coordination to hand over the traffic offloaded from the inactive BSs to
the active BSs, it is generally NP-hard to find a network-level optimal
adaptation policy that can minimize the on-grid energy cost over a long-term
and yet assures the downlink transmission quality at the same time. Aiming at
the network-level dynamic system design, we jointly apply stochastic geometry
(Geo) for large-scale green cellular network analysis and dynamic programming
(DP) for adaptive BS on/off operation design and on-grid energy purchase
design, and thus propose a new Geo-DP design approach. By this approach, we
obtain the optimal BS on/off policy, which shows that the optimal BSs' active
operation probability in each horizon is just sufficient to assure the required
downlink transmission quality with time-varying load in the large-scale
cellular network. We also propose a suboptimal on-grid energy purchase policy
with low-complexity, where the low-price on-grid energy is over-purchased in
the current horizon only when the current storage level and the future
renewable energy level are both low. We compare the proposed policy with the
existing schemes and show that our proposed policy can more efficiently save
the on-grid energy cost over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07487</identifier>
 <datestamp>2015-12-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07487</id><created>2015-12-23</created><authors><author><keyname>Nordio</keyname><forenames>Alessandro</forenames></author><author><keyname>Tarable</keyname><forenames>Alberto</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author><author><keyname>Marsan</keyname><forenames>Marco Ajmone</forenames></author></authors><title>Selecting the top-quality item through crowd scoring</title><categories>cs.HC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate crowdsourcing algorithms for finding the top-quality item
within a large collection of objects with unknown intrinsic quality values.
This is an important problem with many relevant applications, for example in
networked recommendation systems. The core of the algorithms is that objects
are distributed to crowd workers, who return a noisy evaluation. All received
evaluations are then combined, to identify the top-quality object. We first
present a simple probabilistic model for the system under investigation. Then,
we devise and study a class of efficient adaptive algorithms to assign in an
effective way objects to workers. We compare the performance of several
algorithms, which correspond to different choices of the design
parameters/metrics. We finally compare our approach based on scoring object
qualities against traditional proposals based on comparisons and tournaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07491</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07491</id><created>2015-12-23</created><authors><author><keyname>Li</keyname><forenames>Hui</forenames><affiliation>INL</affiliation></author><author><keyname>Fourmigue</keyname><forenames>Alain</forenames><affiliation>INL</affiliation></author><author><keyname>Beux</keyname><forenames>S&#xe9;bastien Le</forenames><affiliation>INL</affiliation></author><author><keyname>Letartre</keyname><forenames>Xavier</forenames><affiliation>INL</affiliation></author><author><keyname>'Connor</keyname><forenames>Ian O</forenames><affiliation>INL</affiliation></author><author><keyname>Nicolescu</keyname><forenames>Gabriela</forenames></author></authors><title>Thermal Aware Design Method for VCSEL-Based On-Chip Optical Interconnect</title><categories>cs.ET</categories><comments>IEEE International Conference on Design Automation and Test in Europe
  (DATE 2015), Mar 2015, Grenoble, France. 2015</comments><proxy>ccsd</proxy><doi>10.7873/DATE.2015.0479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Network-on-Chip (ONoC) is an emerging technology considered as one of
the key solutions for future generation on-chip interconnects. However, silicon
photonic devices in ONoC are highly sensitive to temperature variation, which
leads to a lower efficiency of Vertical-Cavity Surface-Emitting Lasers
(VCSELs), a resonant wavelength shift of Microring Resonators (MR), and results
in a lower Signal to Noise Ratio (SNR). In this paper, we propose a methodology
enabling thermal-aware design for optical interconnects relying on
CMOS-compatible VCSEL. Thermal simulations allow designing ONoC interfaces with
low gradient temperature and analytical models allow evaluating the SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07492</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07492</id><created>2015-12-23</created><authors><author><keyname>Li</keyname><forenames>Hui</forenames><affiliation>INL</affiliation></author><author><keyname>Beux</keyname><forenames>S&#xe9;bastien Le</forenames><affiliation>INL</affiliation></author><author><keyname>Nicolescu</keyname><forenames>Gabriela</forenames><affiliation>ECE</affiliation></author><author><keyname>Trajkovic</keyname><forenames>Jelena</forenames><affiliation>ECE</affiliation></author><author><keyname>'Connor</keyname><forenames>Ian O</forenames><affiliation>INL</affiliation></author></authors><title>Optical Crossbars on Chip: a comparative study based on worst-case
  losses</title><categories>cs.ET</categories><comments>Exploiting Silicon Photonics for energy-efficient heterogeneous
  parallel architectures (SiPhotonics'14), Jan 2014, Vienna, Austria</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The many cores design research community have shown high interest in optical
crossbars on chip for more than a decade. Key properties of optical crossbars,
namely a) contention free data routing b) low latency communication and c)
potential for high bandwidth through the use of WDM, motivate several
implementations of this type of interconnect. These implementations demonstrate
very different scalability and power efficiency ability depending on three key
design factors: a) the network topology, b) the considered layout and the c)
the injection losses induced by the fabrication process. In this paper, the
worst-case optical losses of crossbar implementations are compared according to
the factors mentioned above. The comparison results has the potential to help
many cores system designer to select the most appropriate crossbar
implementation according, for instance, to the number of IP cores and the chip
die size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07493</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07493</id><created>2015-12-23</created><authors><author><keyname>Li</keyname><forenames>Hui</forenames><affiliation>INL</affiliation></author><author><keyname>Beux</keyname><forenames>S&#xe9;bastien Le</forenames><affiliation>INL</affiliation></author><author><keyname>Nicolescu</keyname><forenames>Gabriela</forenames><affiliation>INL</affiliation></author><author><keyname>'Connor</keyname><forenames>Ian O</forenames><affiliation>INL</affiliation></author></authors><title>Energy-efficient optical crossbars on chip with multi-layer deposited
  silicon</title><categories>cs.ET</categories><comments>20th Asia and South Pacific Design Automation Conference (ASP-DAC
  2015), Jan 2015, Chiba/Tokyo, Japan</comments><proxy>ccsd</proxy><doi>10.1109/ASPDAC.2015.7058996</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The many cores design research community have shown high interest in optical
crossbars on chip for more than a decade. Key properties of optical crossbars,
namely a) contention-free data routing b) low-latency communication and c)
potential for high bandwidth through the use of WDM, motivate several
implementations. These implementations demonstrate very different scalability
and power efficiency ability depending on three key design factors: a) the
network topology, b) the considered layout and c) the insertion losses induced
by the fabrication process. The emerging design technique relying on
multi-layer deposited silicon allows reducing optical losses, which may lead to
significant reduction of the power consumption. In this paper, multi-layer
deposited silicon based crossbars are proposed and compared. The results
indicate that the proposed ring-based network exhibits, on average, 22% and
51.4% improvement for worst-case and average losses respectively compared to
the most power-efficient related crossbars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07494</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07494</id><created>2015-12-23</created><authors><author><keyname>Bougleux</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LITIS</affiliation></author><author><keyname>Brun</keyname><forenames>Luc</forenames><affiliation>LITIS</affiliation></author><author><keyname>Carletti</keyname><forenames>Vincenzo</forenames><affiliation>LITIS</affiliation></author><author><keyname>Foggia</keyname><forenames>Pasquale</forenames><affiliation>LITIS</affiliation></author><author><keyname>Ga&#xfc;z&#xe8;re</keyname><forenames>Benoit</forenames><affiliation>LITIS</affiliation></author><author><keyname>Vento</keyname><forenames>Mario</forenames></author></authors><title>A Quadratic Assignment Formulation of the Graph Edit Distance</title><categories>cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing efficiently a robust measure of similarity or dissimilarity between
graphs is a major challenge in Pattern Recognition. The Graph Edit Distance
(GED) is a flexible measure of dissimilarity between graphs which arises in
error-tolerant graph matching. It is defined from an optimal sequence of edit
operations (edit path) transforming one graph into an other. Unfortunately, the
exact computation of this measure is NP-hard. In the last decade, several
approaches have been proposed to approximate the GED in polynomial time, mainly
by solving linear programming problems. Among them, the bipartite GED has
received much attention. It is deduced from a linear sum assignment of the
nodes of the two graphs, which can be efficiently computed by Hungarian-type
algorithms. However, edit operations on nodes and edges are not handled
simultaneously, which limits the accuracy of the approximation. To overcome
this limitation, we propose to extend the linear assignment model to a
quadratic one, for directed or undirected graphs having labelized nodes and
edges. This is realized through the definition of a family of edit paths
induced by assignments between nodes. We formally show that the GED, restricted
to the paths in this family, is equivalent to a quadratic assignment problem.
Since this problem is NP-hard, we propose to compute an approximate solution by
an adaptation of the Integer Projected Fixed Point method. Experiments show
that the proposed approach is generally able to reach a more accurate
approximation of the optimal GED than the bipartite GED, with a computational
cost that is still affordable for graphs of non trivial sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07502</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07502</id><created>2015-12-23</created><authors><author><keyname>Turner</keyname><forenames>J. T.</forenames></author><author><keyname>Aha</keyname><forenames>David</forenames></author><author><keyname>Smith</keyname><forenames>Leslie</forenames></author><author><keyname>Gupta</keyname><forenames>Kalyan Moy</forenames></author></authors><title>Convolutional Architecture Exploration for Action Recognition and Image
  Classification</title><categories>cs.CV</categories><comments>12 pages. 11 tables. 0 Images. Written Summer 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a
software package for the training, classifying, and feature extraction of
images. The UCF Sports Action dataset is a widely used machine learning dataset
that has 200 videos taken in 720x480 resolution of 9 different sporting
activities: diving, golf, swinging, kicking, lifting, horseback riding,
running, skateboarding, swinging (various gymnastics), and walking. In this
report we report on a caffe feature extraction pipeline of images taken from
the videos of the UCF Sports Action dataset. A similar test was performed on
overfeat, and results were inferior to caffe. This study is intended to explore
the architecture and hyper parameters needed for effective static analysis of
action in videos and classification over a variety of image datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07505</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07505</id><created>2015-12-23</created><authors><author><keyname>Rok</keyname><forenames>Alexandre</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Shakhar</forenames></author></authors><title>A short proof of the first selection lemma and weak $\frac{1}{r}$-nets
  for moving points</title><categories>cs.DM</categories><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (i) We provide a short and simple proof of the first selection lemma. (ii) We
also prove a selection lemma of a new type in $\Re^d$. For example, when $d=2$
assuming $n$ is large enough we prove that for any set $P$ of $n$ points in
general position there are $\Omega(n^4)$ pairs of segments spanned by $P$ all
of which intersect in some fixed triangle spanned by $P$. (iii) Finally, we
extend the weak $\frac{1}{r}$-net theorem to a kinetic setting where the
underlying set of points is moving polynomially with bounded description
complexity. We establish that one can find a kinetic analog $N$ of a weak
$\frac{1}{r}$-net of cardinality $O(r^{\frac{d(d+1)}{2}}\log^{d}r)$ whose
points are moving with coordinates that are rational functions with bounded
description complexity. Moreover, each member of $N$ has one polynomial
coordinate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07506</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07506</id><created>2015-12-23</created><authors><author><keyname>Doumanoglou</keyname><forenames>Andreas</forenames></author><author><keyname>Kouskouridas</keyname><forenames>Rigas</forenames></author><author><keyname>Malassiotis</keyname><forenames>Sotiris</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Kyun</forenames></author></authors><title>6D Object Detection and Next-Best-View Prediction in the Crowd</title><categories>cs.CV</categories><comments>10 pages, 8 figures, project page:
  http://www.iis.ee.ic.ac.uk/rkouskou/6D_NBV.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  6D object detection and pose estimation in the crowd (scenes with multiple
object instances, severe foreground occlusions and background distractors), has
become an important problem in many rapidly evolving technological areas such
as robotics and augmented reality. Single shot-based 6D pose estimators with
manually designed features are still unable to tackle the above challenges,
motivating the research towards unsupervised feature learning and
next-best-view estimation. In this work, we present a complete framework for
both single shot-based 6D object pose estimation and next-best-view prediction
based on Hough Forests, the state of the art object pose estimator that
performs classification and regression jointly. Rather than using manually
designed features we a) propose an unsupervised feature learnt from
depth-invariant patches using a Sparse Autoencoder and b) offer an extensive
evaluation of various state of the art features. Furthermore, taking advantage
of the clustering performed in the leaf nodes of Hough Forests, we learn to
estimate the reduction of uncertainty in other views, formulating the problem
of selecting the next-best-view. To further improve 6D object pose estimation,
we propose an improved joint registration and hypotheses verification module as
a final refinement step to reject false detections. We provide two additional
challenging datasets inspired from realistic scenarios to extensively evaluate
the state of the art and our framework. One is related to domestic environments
and the other depicts a bin-picking scenario mostly found in industrial
settings. We show that our framework significantly outperforms state of the art
both on public and on our datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07529</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07529</id><created>2015-12-23</created><authors><author><keyname>Zribi</keyname><forenames>Ali</forenames></author><author><keyname>Chtourou</keyname><forenames>Mohamed</forenames></author><author><keyname>Djemel</keyname><forenames>Mohamed</forenames></author></authors><title>A New PID Neural Network Controller Design for Nonlinear Processes</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel adaptive tuning method of PID neural network (PIDNN)
controller for nonlinear process is proposed. The method utilizes an improved
gradient descent method to adjust PIDNN parameters where the margin stability
will be employed to get high tracking performance and robustness with regard to
external load disturbance and parameter variation. Simulation results show the
effectiveness of the proposed algorithm compared with other well-known learning
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07533</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07533</id><created>2015-12-23</created><authors><author><keyname>Bhattacharya</keyname><forenames>Binay</forenames></author><author><keyname>Das</keyname><forenames>Sandip</forenames></author><author><keyname>Higashikawa</keyname><forenames>Yuya</forenames></author><author><keyname>Kameda</keyname><forenames>Tsunehiko</forenames></author><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author></authors><title>Geometric k-Center Problems with Centers Constrained to Two Lines</title><categories>cs.CG</categories><comments>12 pages, 5 figures, JCDCGG2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the $k$-center problem in which the centers are constrained to
lie on two lines. Given a set of $n$ weighted points in the plane, we want to
locate up to $k$ centers on two parallel lines. We present an $O(n\log^2 n)$
time algorithm, which minimizes the weighted distance from any point to a
center. We then consider the unweighted case, where the centers are constrained
to be on two perpendicular lines. Our algorithms run in $O(n\log^2 n)$ time
also in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07537</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07537</id><created>2015-12-23</created><authors><author><keyname>Bhattacharya</keyname><forenames>Binay</forenames></author><author><keyname>Das</keyname><forenames>Sandip</forenames></author><author><keyname>Kameda</keyname><forenames>Tsunehiko</forenames></author></authors><title>Linear-Time Fitting of a $k$-Step Function</title><categories>cs.CG</categories><comments>12 pages, 4 figures, Caldam 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of $n$ weighted points on the $x$-$y$ plane, we want to find a
step function consisting of $k$ horizontal steps such that the maximum vertical
weighted distance from any point to a step is minimized. We solve this problem
in $O(n)$ time when $k$ is a constant. Our approach relies on the
prune-and-search technique, and can be adapted to design similar linear time
algorithms to solve the line-constrained k-center problem and the size-$k$
histogram construction problem as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07550</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07550</id><created>2015-12-23</created><authors><author><keyname>Arunachalam</keyname><forenames>Srinivasan</forenames><affiliation>CWI</affiliation></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Optimizing the Number of Gates in Quantum Search</title><categories>quant-ph cs.DS</categories><comments>11 pages LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $ $In its usual form, Grover's quantum search algorithm uses $O(\sqrt{N})$
queries and $O(\sqrt{N} \log N)$ other elementary gates to find a solution in
an $N$-bit database. Grover in 2002 showed how to reduce the number of other
gates to $O(\sqrt{N}\log\log N)$ for the special case where the database has a
unique solution, without significantly increasing the number of queries. We
show how to reduce this further to $O(\sqrt{N}\log^{(r)} N)$ gates for any
constant $r$, and sufficiently large $N$. This means that, on average, the
gates between two queries barely touch more than a constant number of the $\log
N$ qubits on which the algorithm acts. For a very large $N$ that is a power of
2, we can choose $r$ such that the algorithm uses essentially the minimal
number $\frac{\pi}{4}\sqrt{N}$ of queries, and only
$O(\sqrt{N}\log(\log^{\star} N))$ other gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07563</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07563</id><created>2015-12-23</created><authors><author><keyname>Fass</keyname><forenames>Didier</forenames><affiliation>MOSEL</affiliation></author></authors><title>Affordances and Safe Design of Assistance Wearable Virtual Environment
  of Gesture</title><categories>cs.HC q-bio.NC</categories><proxy>ccsd</proxy><journal-ref>Tared Ahram, Waldemar Karwowski, Dylan Schmorrow. Human Factors
  and Egonomics (AHFE 2015), Jul 2015, Las Vegas, United States. Elsivier,
  Procedia Manufacturing, pp.8, 2015, 6th International Conference on Applied
  Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE
  2015</journal-ref><doi>10.1016/j.promfg.2015.07.343</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safety and reliability are the main issues for designing assistance wearable
virtual environment of technical gesture in aerospace, or health application
domains. That needs the integration in the same isomorphic engineering
framework of human requirements, systems requirements and the rationale of
their relation to the natural and artifactual environment.To explore coupling
integration and design functional organization of support technical gesture
systems, firstly ecological psychologyprovides usa heuristicconcept: the
affordance. On the other hand mathematical theory of integrative physiology
provides us scientific concepts: the stabilizing auto-association principle and
functional interaction.After demonstrating the epistemological consistence of
these concepts, we define an isomorphic framework to describe and model human
systems integration dedicated to human in-the-loop system engineering.We
present an experimental approach of safe design of assistance wearable virtual
environment of gesture based in laboratory and parabolic flights. On the
results, we discuss the relevance of our conceptual approach and the
applications to future assistance of gesture wearable systems engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07564</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07564</id><created>2015-12-23</created><authors><author><keyname>Laghouaouta</keyname><forenames>Youness</forenames></author><author><keyname>Anwar</keyname><forenames>Adil</forenames></author><author><keyname>Nassar</keyname><forenames>Mahmoud</forenames></author></authors><title>A Formal Definition of Model Composition Traceability</title><categories>cs.SE</categories><comments>9 pages, 5 figures</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), 2015,
  vol. 12(6), pp. 46-54</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-modeling based approach, the system under development is described
by several models that represent various perspectives and concerns. Obviously,
these partial representations are less complex than the global model, but they
need to be composed to address validation and synchronization tasks. The model
composition is a crucial model driven development operation, but it remains a
tedious and error prone activity. In this perspective, a traceability mechanism
offers a way to master this complexity by providing support to comprehend the
composition effects. In previous work, we presented a traceability approach
dedicated to this operation. The current takes advantages of these experiments,
and proposes a formalization of the model composition traceability. Also, an
overview of a generic traceability approach is provided. The latter relies on
the formal definition we introduce for the model composition operation and the
related traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07574</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07574</id><created>2015-12-23</created><authors><author><keyname>Camarero</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Carmen</forenames></author><author><keyname>Vallejo</keyname><forenames>Enrique</forenames></author><author><keyname>Beivide</keyname><forenames>Ram&#xf3;n</forenames></author></authors><title>Projective Networks: Topologies for Large Parallel Computer Systems</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interconnection network comprises a significant portion of the cost of
large parallel computers, both in economic terms and power consumption. Several
previous proposals exploit large-radix routers to build scalable low-distance
topologies with the aim of minimizing these costs. However, they fail to
consider potential unbalance in the network utilization, which in some cases
results in suboptimal designs. Based on an appropriate cost model, this paper
advocates the use of networks based on incidence graphs of projective planes,
broadly denoted as Projective Networks. Projective Networks rely on highly
symmetric generalized Moore graphs and encompass several proposed direct (PN
and demi-PN) and indirect (OFT) topologies under a common mathematical
framework. Compared to other proposals with average distance between 2 and 3
hops, these networks provide very high scalability while preserving a balanced
network utilization, resulting in low network costs. Overall, Projective
Networks constitute a competitive alternative for exascale-level
interconnection network design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07587</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07587</id><created>2015-12-23</created><updated>2016-03-05</updated><authors><author><keyname>Masatran</keyname><forenames>Rajasekaran</forenames></author></authors><title>A Latent-Variable Grid Model</title><categories>cs.LG cs.CV stat.ML</categories><comments>8 pages, with 7 figures, 8 algorithms, and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major problem with the conditional random field (CRF) is that its learning
algorithms are computationally expensive. Grid associative CRFs occur
frequently in sequence and image learning. We design a non-markov high-bias
low-variance model as an alternative to this subclass of CRF. Our learning
algorithm uses vector quantization and, at time complexity O(T^d log T^d), is
significantly faster than that of CRF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07590</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07590</id><created>2015-12-23</created><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Postl</keyname><forenames>John</forenames></author></authors><title>Randomized Social Choice Functions Under Metric Preferences</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the quality of randomized social choice mechanisms in a setting
in which the agents have metric preferences: every agent has a cost for each
alternative, and these costs form a metric. We assume that these costs are
unknown to the mechanisms (and possibly even to the agents themselves), which
means we cannot simply select the optimal alternative, i.e. the alternative
that minimizes the total agent cost (or median agent cost). However, we do
assume that the agents know their ordinal preferences that are induced by the
metric space. We examine randomized social choice functions that require only
this ordinal information and select an alternative that is good in expectation
with respect to the costs from the metric. To quantify how good a randomized
social choice function is, we bound the distortion, which is the worst-case
ratio between expected cost of the alternative selected and the cost of the
optimal alternative. We provide new distortion bounds for a variety of
randomized mechanisms, for both general metrics and for important special
cases. Our results show a sizable improvement in distortion over deterministic
mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07592</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07592</id><created>2015-12-23</created><authors><author><keyname>Lore</keyname><forenames>Kin Gwn</forenames></author><author><keyname>Sweet</keyname><forenames>Nicholas</forenames></author><author><keyname>Kumar</keyname><forenames>Kundan</forenames></author><author><keyname>Ahmed</keyname><forenames>Nisar</forenames></author><author><keyname>Sarkar</keyname><forenames>Soumik</forenames></author></authors><title>Deep Value of Information Estimators for Collaborative Human-Machine
  Information Gathering</title><categories>cs.HC</categories><comments>10 pages, to appear in ICCPS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective human-machine collaboration can significantly improve many learning
and planning strategies for information gathering via fusion of 'hard' and
'soft' data originating from machine and human sensors, respectively. However,
gathering the most informative data from human sensors without task overloading
remains a critical technical challenge. In this context, Value of Information
(VOI) is a crucial decision-theoretic metric for scheduling interaction with
human sensors. We present a new Deep Learning based VOI estimation framework
that can be used to schedule collaborative human-machine sensing with
computationally efficient online inference and minimal policy hand-tuning.
Supervised learning is used to train deep convolutional neural networks (CNNs)
to extract hierarchical features from 'images' of belief spaces obtained via
data fusion. These features can be associated with soft data query choices to
reliably compute VOI for human interaction. The CNN framework is described in
detail, and a performance comparison to a feature-based POMDP scheduling policy
is provided. The practical feasibility of our method is also demonstrated on a
mobile robotic search problem with language-based semantic human sensor inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07613</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07613</id><created>2015-12-23</created><updated>2015-12-26</updated><authors><author><keyname>Rucco</keyname><forenames>Matteo</forenames></author><author><keyname>Gonzalez-Diaz</keyname><forenames>Rocio</forenames></author><author><keyname>Jimenez</keyname><forenames>Maria-Jose</forenames></author><author><keyname>Atienza</keyname><forenames>Nieves</forenames></author><author><keyname>Cristalli</keyname><forenames>Cristina</forenames></author><author><keyname>Concettoni</keyname><forenames>Enrico</forenames></author><author><keyname>Ferrante</keyname><forenames>Andrea</forenames></author><author><keyname>Merelli</keyname><forenames>Emanuela</forenames></author></authors><title>A new topological entropy-based approach for measuring similarities
  among piecewise linear functions</title><categories>cs.DM</categories><comments>15 pages, 5 figures</comments><msc-class>55U10, 05E45, 62H30, 28D20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel methodology based on a topological entropy,
the so-called persistent entropy, for addressing the comparison between
discrete piecewise linear functions. The comparison is certified by the
stability theorem for persistent entropy. The theorem is used in the
implementation of a new algorithm. The algorithm transforms a discrete
piecewise linear function into a ?ltered simplicial complex that is analyzed
with persistent homology and persistent entropy. Persistent entropy is used as
discriminant feature for solving the supervised classi?cation problem of real
long length noisy signals of DC electrical motors. The quality of classi?cation
is stated in terms of the area under receiver operating characteristic curve
(AUC=94.52%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07634</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07634</id><created>2015-12-23</created><authors><author><keyname>Heinlein</keyname><forenames>Daniel</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Coset Construction for Subspace Codes</title><categories>math.CO cs.IT math.IT</categories><comments>17 pages, 1 table</comments><msc-class>Primary 05B25, 51E20, Secondary 51E22, 51E23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main problems of the young research area of network coding is to
compute good lower and upper bounds of the achievable so-called subspace codes
in $\operatorname{PG}(n,q)$ for a given minimal distance. Here we generalize a
construction of Etzion and Silberstein to a wide range of parameters. This
construction, named coset construction, improves several of the previously best
known subspace codes and attains the MRD bound for an infinite family of
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07636</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07636</id><created>2015-12-23</created><authors><author><keyname>Boufounos</keyname><forenames>Petros T</forenames></author><author><keyname>Rane</keyname><forenames>Shantanu</forenames></author><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author></authors><title>Representation and Coding of Signal Geometry</title><categories>cs.IT cs.AI cs.IR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approaches to signal representation and coding theory have traditionally
focused on how to best represent signals using parsimonious representations
that incur the lowest possible distortion. Classical examples include linear
and non-linear approximations, sparse representations, and rate-distortion
theory. Very often, however, the goal of processing is to extract specific
information from the signal, and the distortion should be measured on the
extracted information. The corresponding representation should, therefore,
represent that information as parsimoniously as possible, without necessarily
accurately representing the signal itself.
  In this paper, we examine the problem of encoding signals such that
sufficient information is preserved about their pairwise distances and their
inner products. For that goal, we consider randomized embeddings as an encoding
mechanism and provide a framework to analyze their performance. We also
demonstrate that it is possible to design the embedding such that it represents
different ranges of distances with different precision. These embeddings also
allow the computation of kernel inner products with control on their inner
product-preserving properties. Our results provide a broad framework to design
and analyze embeddins, and generalize existing results in this area, such as
random Fourier kernels and universal embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07638</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07638</id><created>2015-12-23</created><authors><author><keyname>Reverdy</keyname><forenames>Paul</forenames></author><author><keyname>Srivastava</keyname><forenames>Vaibhav</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>Satisficing in multi-armed bandit problems</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satisficing is a relaxation of maximizing and allows for less risky
decision-making in the face of uncertainty. We propose two sets of satisficing
objectives for the multi-armed bandit problem, where the objective is to
achieve reward-based decision-making performance above a given threshold. We
show that these new problems are equivalent to various standard multi-armed
bandit problems with maximizing objectives and use the equivalence to find
bounds on performance. The different objectives can result in qualitatively
different behavior; for example, agents explore their options continually in
one case and only a finite number of times in another. For the case of Gaussian
rewards we show an additional equivalence between the two sets of satisficing
objectives that allows algorithms developed for one set to be applied to the
other. We then develop variants of the Upper Credible Limit (UCL) algorithm
that solve the problems with satisficing objectives and show that these
modified UCL algorithms achieve efficient satisficing performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07650</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07650</id><created>2015-12-23</created><authors><author><keyname>David</keyname><forenames>Yahel</forenames></author><author><keyname>Shimkin</keyname><forenames>Nahum</forenames></author></authors><title>The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms</title><categories>stat.ML cs.AI cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1508.05608</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced
with several stochastic arms, each a source of i.i.d. rewards of unknown
distribution. At each time step the agent chooses an arm, and observes the
reward of the obtained sample. Each sample is considered here as a separate
item with the reward designating its value, and the goal is to find an item
with the highest possible value. Our basic assumption is a known lower bound on
the {\em tail function} of the reward distributions. Under the PAC framework,
we provide a lower bound on the sample complexity of any
$(\epsilon,\delta)$-correct algorithm, and propose an algorithm that attains
this bound up to logarithmic factors. We analyze the robustness of the proposed
algorithm and in addition, we compare the performance of this algorithm to the
variant in which the arms are not distinguishable by the agent and are chosen
randomly at each stage. Interestingly, when the maximal rewards of the arms
happen to be similar, the latter approach may provide better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07654</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07654</id><created>2015-12-23</created><updated>2016-03-02</updated><authors><author><keyname>Missimer</keyname><forenames>Eric</forenames></author><author><keyname>Zhao</keyname><forenames>Katherine</forenames></author><author><keyname>West</keyname><forenames>Richard</forenames></author></authors><title>Mixed-Criticality Scheduling with I/O</title><categories>cs.OS</categories><comments>Second version has replaced simulation experiments with real machine
  experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of scheduling tasks with different
criticality levels in the presence of I/O requests. In mixed-criticality
scheduling, higher criticality tasks are given precedence over those of lower
criticality when it is impossible to guarantee the schedulability of all tasks.
While mixed-criticality scheduling has gained attention in recent years, most
approaches typically assume a periodic task model. This assumption does not
always hold in practice, especially for real-time and embedded systems that
perform I/O. For example, many tasks block on I/O requests until devices signal
their completion via interrupts; both the arrival of interrupts and the waking
of blocked tasks can be aperiodic. In our prior work, we developed a scheduling
technique in the Quest real-time operating system, which integrates the
time-budgeted management of I/O operations with Sporadic Server scheduling of
tasks. This paper extends our previous scheduling approach with support for
mixed-criticality tasks and I/O requests on the same processing core. Results
show the effective schedulability of different task sets in the presence of I/O
requests is superior in our approach compared to traditional methods that
manage I/O using techniques such as Sporadic Servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07675</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07675</id><created>2015-12-23</created><authors><author><keyname>Lou</keyname><forenames>Taishan</forenames></author></authors><title>Desensitized Cubature Kalman Filter with Uncertain Parameter</title><categories>cs.SY</categories><comments>10 pages, 11figures</comments><report-no>4023</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust desensitized cubature Kalman filtering (DCKF) for nonlinear systems
with uncertain parameter is proposed. Sensitivity matrices are defined as the
integral form, and desensitized cost function is designed by penalizing the
posterior covariance trace by a sensitivity-weighting sum of the posteriori
sensitivities. The DCKF gain is obtained by minimizing the desensitized cost
function to amend the state estimation. Then, the sensitivity propagation of
the state estimate errors is described, and the sensitivity of the root square
matrix is obtained by solving a special equation. The effectiveness of the
proposed DCKF was demonstrated by two numerical simulations with uncertain
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07679</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07679</id><created>2015-12-23</created><authors><author><keyname>Dulac-Arnold</keyname><forenames>Gabriel</forenames></author><author><keyname>Evans</keyname><forenames>Richard</forenames></author><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author><author><keyname>Coppin</keyname><forenames>Ben</forenames></author></authors><title>Reinforcement Learning in Large Discrete Action Spaces</title><categories>cs.AI cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to reason in an environment with a large number of discrete
actions is essential to bringing reinforcement learning to a larger class of
problems. Recommender systems, industrial plants and language models are only
some of the many real-world tasks involving large numbers of discrete actions
for which current methods can be difficult or even impossible to apply.
  An ability to generalize over the set of actions as well as sub-linear
complexity relative to the size of the set are both necessary to handle such
tasks. Current approaches are not able to provide both of these, which
motivates the work in this paper. Our proposed approach leverages prior
information about the actions to embed them in a continuous space upon which it
can generalize. Additionally, approximate nearest-neighbor methods allow for
logarithmic-time lookup complexity relative to the number of actions, which is
necessary for time-wise tractable training. This combined approach allows
reinforcement learning methods to be applied to large-scale learning problems
previously intractable with current methods. We demonstrate our algorithm's
abilities on a series of tasks having up to one million actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07680</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07680</id><created>2015-12-23</created><authors><author><keyname>Bravetti</keyname><forenames>Mario</forenames><affiliation>University of Bologna, Italy / INRIA, France</affiliation></author></authors><title>Towards Dynamic Updates in Service Composition</title><categories>cs.LO</categories><comments>In Proceedings FOCLASA 2015, arXiv:1512.06947. arXiv admin note:
  substantial text overlap with arXiv:1411.3791</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 201, 2015, pp. 1-17</journal-ref><doi>10.4204/EPTCS.201.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey our results about verification of adaptable processes. We present
adaptable processes as a way of overcoming the limitations that process calculi
have for describing patterns of dynamic process evolution. Such patterns rely
on direct ways of controlling the behavior and location of running processes,
and so they are at the heart of the adaptation capabilities present in many
modern concurrent systems. Adaptable processes have named scopes and are
sensible to actions of dynamic update at runtime; this allows to express
dynamic and static topologies of adaptable processes as well as different
evolvability patterns for concurrent processes. We introduce a core calculus of
adaptable processes and consider verification problems for them: first based on
specific properties related to error occurrence, that we call bounded and
eventual adaptation, and then by considering a simple yet expressive temporal
logic over adaptable processes. We provide (un)decidability results of such
verification problems over adaptable processes considering the spectrum of
topologies/evolvability patterns introduced. We then consider distributed
adaptability, where a process can update part of a protocol by performing
dynamic distributed updates over a set of protocol participants. Dynamic
updates in this context are presented as an extension of our work on
choreographies and behavioural contracts in multiparty interactions. We show
how update mechanisms considered for adaptable processes can be used to extend
the theory of choreography and orchestration/contracts, allowing them to be
modified at run-time by internal (self-adaptation) or external intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07681</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07681</id><created>2015-12-23</created><authors><author><keyname>Canciani</keyname><forenames>Andrea</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa, Pisa, Italy</affiliation></author><author><keyname>Degano</keyname><forenames>Pierpaolo</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa, Pisa, Italy</affiliation></author><author><keyname>Ferrari</keyname><forenames>Gian-Luigi</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa, Pisa, Italy</affiliation></author><author><keyname>Galletta</keyname><forenames>Letterio</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa, Pisa, Italy</affiliation></author></authors><title>A Context-Oriented Extension of F#</title><categories>cs.PL</categories><comments>In Proceedings FOCLASA 2015, arXiv:1512.06947</comments><proxy>EPTCS</proxy><acm-class>D.1.1; D.1.6; D.3.4</acm-class><journal-ref>EPTCS 201, 2015, pp. 18-32</journal-ref><doi>10.4204/EPTCS.201.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-Oriented programming languages provide us with primitive constructs
to adapt program behaviour depending on the evolution of their operational
environment, namely the context. In previous work we proposed ML_CoDa, a
context-oriented language with two-components: a declarative constituent for
programming the context and a functional one for computing. This paper
describes the implementation of ML_CoDa as an extension of F#.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07682</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07682</id><created>2015-12-23</created><authors><author><keyname>Autili</keyname><forenames>Marco</forenames><affiliation>University of L'Aquila</affiliation></author><author><keyname>Di Salle</keyname><forenames>Amleto</forenames><affiliation>University of L'Aquila</affiliation></author><author><keyname>Perucci</keyname><forenames>Alexander</forenames><affiliation>University of L'Aquila</affiliation></author><author><keyname>Tivoli</keyname><forenames>Massimo</forenames><affiliation>University of L'Aquila</affiliation></author></authors><title>On the Automated Synthesis of Enterprise Integration Patterns to Adapt
  Choreography-based Distributed Systems</title><categories>cs.SE</categories><comments>In Proceedings FOCLASA 2015, arXiv:1512.06947</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 201, 2015, pp. 33-47</journal-ref><doi>10.4204/EPTCS.201.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Future Internet is becoming a reality, providing a large-scale computing
environments where a virtually infinite number of available services can be
composed so to fit users' needs. Modern service-oriented applications will be
more and more often built by reusing and assembling distributed services. A key
enabler for this vision is then the ability to automatically compose and
dynamically coordinate software services. Service choreographies are an
emergent Service Engineering (SE) approach to compose together and coordinate
services in a distributed way. When mismatching third-party services are to be
composed, obtaining the distributed coordination and adaptation logic required
to suitably realize a choreography is a non-trivial and error prone task.
Automatic support is then needed. In this direction, this paper leverages
previous work on the automatic synthesis of choreography-based systems, and
describes our preliminary steps towards exploiting Enterprise Integration
Patterns to deal with a form of choreography adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07684</identifier>
 <datestamp>2016-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07684</id><created>2015-12-23</created><authors><author><keyname>Cherif</keyname><forenames>Asma</forenames><affiliation>Umm al-Qura University, Saudi Arabia</affiliation></author><author><keyname>Imine</keyname><forenames>Abdessamad</forenames><affiliation>Lorraine University and Inria Nancy Grand-Est, France</affiliation></author></authors><title>A Constraint-based Approach for Generating Transformation Patterns</title><categories>cs.DC cs.LO</categories><comments>In Proceedings FOCLASA 2015, arXiv:1512.06947</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 201, 2015, pp. 48-62</journal-ref><doi>10.4204/EPTCS.201.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Undoing operations is an indispensable feature for many collaborative
applications, mainly collaborative editors. It provides the ability to restore
a correct state of shared data after erroneous operations. In particular,
selective undo allows to undo any operation and is based on rearranging
operations in the history thanks to the Operational Transformation (OT)
approach. OT is an optimistic replication technique allowing for updating the
shared data concurrently while maintaining convergence. It is a challenging
task how to meaningfully combine OT and undo approaches. Indeed, undoing
operations that are received and executed out-of-order at different sites leads
to divergence cases. Even though various undo solutions have been proposed over
the recent years, they are either limited or erroneous.
  In this paper, we propose a constraint-based approach to address the undo
problem. We use Constraint Satisfaction Problem (CSP) theory to devise correct
and undoable transformation patterns (w.r.t OT and undo properties) which
considerably simplifies the design of collaborative objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07685</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07685</id><created>2015-12-23</created><authors><author><keyname>Manaf</keyname><forenames>Nurulhuda A.</forenames><affiliation>Computer Science, University of Surrey</affiliation></author><author><keyname>Moschoyiannis</keyname><forenames>Sotiris</forenames><affiliation>Computer Science, University of Surrey</affiliation></author><author><keyname>Krause</keyname><forenames>Paul</forenames><affiliation>Computer Science, University of Surrey</affiliation></author></authors><title>Service Choreography, SBVR, and Time</title><categories>cs.SE cs.CL</categories><comments>In Proceedings FOCLASA 2015, arXiv:1512.06947</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 201, 2015, pp. 63-77</journal-ref><doi>10.4204/EPTCS.201.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the use of structured natural language (English) in specifying
service choreographies, focusing on the what rather than the how of the
required coordination of participant services in realising a business
application scenario. The declarative approach we propose uses the OMG standard
Semantics of Business Vocabulary and Rules (SBVR) as a modelling language. The
service choreography approach has been proposed for describing the global
orderings of the invocations on interfaces of participant services. We
therefore extend SBVR with a notion of time which can capture the coordination
of the participant services, in terms of the observable message exchanges
between them. The extension is done using existing modelling constructs in
SBVR, and hence respects the standard specification. The idea is that users -
domain specialists rather than implementation specialists - can verify the
requested service composition by directly reading the structured English used
by SBVR. At the same time, the SBVR model can be represented in formal logic so
it can be parsed and executed by a machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07700</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07700</id><created>2015-12-23</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Chai</keyname><forenames>Bo</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Huang</keyname><forenames>Shisheng</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Yang</keyname><forenames>Zaiyue</forenames></author></authors><title>Energy Storage Sharing in Smart Grid: A Modified Auction Based Approach</title><categories>cs.SY</categories><comments>Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the solution of joint energy storage (ES) ownership
sharing between multiple shared facility controllers (SFCs) and those dwelling
in a residential community. The main objective is to enable the residential
units (RUs) to decide on the fraction of their ES capacity that they want to
share with the SFCs of the community in order to assist them storing
electricity, e.g., for fulfilling the demand of various shared facilities. To
this end, a modified auction-based mechanism is designed that captures the
interaction between the SFCs and the RUs so as to determine the auction price
and the allocation of ES shared by the RUs that governs the proposed joint ES
ownership. The fraction of the capacity of the storage that each RU decides to
put into the market to share with the SFCs and the auction price are determined
by a noncooperative Stackelberg game formulated between the RUs and the
auctioneer. It is shown that the proposed auction possesses the incentive
compatibility and the individual rationality properties, which are leveraged
via the unique Stackelberg equilibrium (SE) solution of the game. Numerical
experiments are provided to confirm the effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07709</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07709</id><created>2015-12-23</created><authors><author><keyname>Gupta</keyname><forenames>Kavya</forenames></author><author><keyname>Raj</keyname><forenames>Ankita</forenames></author><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author></authors><title>Analysis and Synthesis Prior Greedy Algorithms for Non-linear Sparse
  Recovery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of recovering sparse solutions to non
linear inverse problems. We look at two variants of the basic problem, the
synthesis prior problem when the solution is sparse and the analysis prior
problem where the solution is cosparse in some linear basis. For the first
problem, we propose non linear variants of the Orthogonal Matching Pursuit
(OMP) and CoSamp algorithms; for the second problem we propose a non linear
variant of the Greedy Analysis Pursuit (GAP) algorithm. We empirically test the
success rates of our algorithms on exponential and logarithmic functions. We
model speckle denoising as a non linear sparse recovery problem and apply our
technique to solve it. Results show that our method outperforms state of the
art methods in ultrasound speckle denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07711</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07711</id><created>2015-12-23</created><authors><author><keyname>Lu</keyname><forenames>Yongxi</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author><author><keyname>Lazebnik</keyname><forenames>Svetlana</forenames></author></authors><title>Adaptive Object Detection Using Adjacency and Zoom Prediction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art object detection systems rely on an accurate set of region
proposals. Recent methods such as MultiBox, YOLO and RPN use a single neural
net evaluation to hypothesize promising object locations. While these
approaches are computationally efficient, they rely on fixed image regions as
anchors for predictions. In this paper we propose to use a search strategy that
adaptively directs computational resources to sub-regions likely to contain
objects. Compared to methods based on fixed anchor locations, our approach
naturally adapts to cases where object instances are sparse and small. Our
approach is comparable in terms of accuracy to the state-of-the-art Faster
R-CNN approach while using two orders of magnitude fewer anchors on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07712</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07712</id><created>2015-12-23</created><authors><author><keyname>Gogna</keyname><forenames>Anupriya</forenames></author><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author></authors><title>Fast Acquisition for Quantitative MRI Maps: Sparse Recovery from
  Non-linear Measurements</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the problem of estimating proton density and T1 maps from
two partially sampled K-space scans such that the total acquisition time
remains approximately the same as a single scan. Existing multi parametric non
linear curve fitting techniques require a large number (8 or more) of echoes to
estimate the maps resulting in prolonged (clinically infeasible) acquisition
times. Our simulation results show that our method yields very accurate and
robust results from only two partially sampled scans (total scan time being the
same as a single echo MRI). We model PD and T1 maps to be sparse in some
transform domain. The PD map is recovered via standard Compressed Sensing based
recovery technique. Estimating the T1 map requires solving an analysis prior
sparse recovery problem from non linear measurements, since the relationship
between T1 values and intensity values or K space samples is not linear. For
the first time in this work, we propose an algorithm for analysis prior sparse
recovery for non linear measurements. We have compared our approach with the
only existing technique based on matrix factorization from non linear
measurements; our method yields considerably superior results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07716</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07716</id><created>2015-12-23</created><authors><author><keyname>Perkins</keyname><forenames>Hugh</forenames></author><author><keyname>Xu</keyname><forenames>Minjie</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Fast Parallel SVM using Data Augmentation</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of the most popular classifiers, linear SVMs still have challenges in
dealing with very large-scale problems, even though linear or sub-linear
algorithms have been developed recently on single machines. Parallel computing
methods have been developed for learning large-scale SVMs. However, existing
methods rely on solving local sub-optimization problems. In this paper, we
develop a novel parallel algorithm for learning large-scale linear SVM. Our
approach is based on a data augmentation equivalent formulation, which casts
the problem of learning SVM as a Bayesian inference problem, for which we can
develop very efficient parallel sampling methods. We provide empirical results
for this parallel sampling SVM, and provide extensions for SVR, non-linear
kernels, and provide a parallel implementation of the Crammer and Singer model.
This approach is very promising in its own right, and further is a very useful
technique to parallelize a broader family of general maximum-margin models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07719</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07719</id><created>2015-12-24</created><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Zhang</keyname><forenames>Xiande</forenames></author></authors><title>Linear Size Constant-Composition Codes</title><categories>math.CO cs.IT math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Johnson-type upper bound on the maximum size of a code of length $n$,
distance $d=2w-1$ and constant composition ${\overline{w}}$ is
$\lfloor\dfrac{n}{w_1}\rfloor$, where $w$ is the total weight and $w_1$ is the
largest component of ${\overline{w}}$. Recently, Chee {\em et al.} proved that
this upper bound can be achieved for all constant-composition codes of
sufficiently large lengths. Let $N_{ccc}({\overline{w}})$ be the smallest such
length. The determination of $N_{ccc}({\overline{w}})$ is trivial for binary
codes. This paper provides a lower bound on $N_{ccc}({\overline{w}})$, which is
shown to be tight for all ternary and quaternary codes by giving several new
combinatorial constructions. Consequently, by refining method, we determine the
values of $N_{ccc}({\overline{w}})$ for all $q$-ary constant-composition codes
such that $3w_1\geq w$ with a few possible exceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07720</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07720</id><created>2015-12-24</created><authors><author><keyname>Subramanian</keyname><forenames>Ganesh</forenames></author></authors><title>Efficient and Secure Routing Protocol for Wireless Sensor Networks
  through Optimal Power Control and Optimal Handoff-Based Recovery Mechanism</title><categories>cs.NI</categories><comments>8 pages</comments><journal-ref>http://dx.doi.org/10.1155/2012/971685</journal-ref><doi>10.1155/2012/971685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in wireless sensor network (WSN) technology have provided the
availability of small and low-cost sensor with capability of sensing various
types of physical and environmental conditions, data processing, and wireless
communication. In WSN, the sensor nodes have a limited transmission range, and
their processing and storage capabilities as well as their energy resources are
also limited. Modified triple umpiring system (MTUS) has already proved its
better performance in Wireless Sensor Networks. In this paper, we extended the
MTUS by incorporating optimal signal to noise ratio (SNR)-based power control
mechanism and optimal handoff-based self-recovery features to form an efficient
and secure routing for WSN. Extensive investigation studies using Glomosim-2.03
Simulator show that efficient and secure routing protocol (ESRP) with optimal
power control mechanism, and handoff-based self-recovery can significantly
reduce the power usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07721</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07721</id><created>2015-12-24</created><authors><author><keyname>Fletcher</keyname><forenames>Sam</forenames></author><author><keyname>Islam</keyname><forenames>Md Zahidul</forenames></author></authors><title>Measuring pattern retention in anonymized data -- where one measure is
  not enough</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore how modifying data to preserve privacy affects the
quality of the patterns discoverable in the data. For any analysis of modified
data to be worth doing, the data must be as close to the original as possible.
Therein lies a problem -- how does one make sure that modified data still
contains the information it had before modification? This question is not the
same as asking if an accurate classifier can be built from the modified data.
Often in the literature, the prediction accuracy of a classifier made from
modified (anonymized) data is used as evidence that the data is similar to the
original. We demonstrate that this is not the case, and we propose a new
methodology for measuring the retention of the patterns that existed in the
original data. We then use our methodology to design three measures that can be
easily implemented, each measuring aspects of the data that no pre-existing
techniques can measure. These measures do not negate the usefulness of
prediction accuracy or other measures -- they are complementary to them, and
support our argument that one measure is almost never enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07729</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07729</id><created>2015-12-24</created><authors><author><keyname>Najibi</keyname><forenames>Mahyar</forenames></author><author><keyname>Rastegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>G-CNN: an Iterative Grid Based Object Detector</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce G-CNN, an object detection technique based on CNNs which works
without proposal algorithms. G-CNN starts with a multi-scale grid of fixed
bounding boxes. We train a regressor to move and scale elements of the grid
towards objects iteratively. G-CNN models the problem of object detection as
finding a path from a fixed grid to boxes tightly surrounding the objects.
G-CNN with around 180 boxes in a multi-scale grid performs comparably to Fast
R-CNN which uses around 2K bounding boxes generated with a proposal technique.
This strategy makes detection faster by removing the object proposal stage as
well as reducing the number of boxes to be processed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07730</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07730</id><created>2015-12-24</created><updated>2015-12-25</updated><authors><author><keyname>Ling</keyname><forenames>Shuyang</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author></authors><title>Blind Deconvolution Meets Blind Demixing: Algorithms and Performance
  Bounds</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $r$ sensors, each one intends to send a function $x_i$ (e.g. a
signal or image) to a receiver common to all $r$ sensors. Before transmission,
each $x_i$ is multiplied by an &quot;encoding matrix&quot; $A_i$. During transmission
each $A_ix_i$ gets convolved with a function $h_i$. The receiver records the
function $y$, given by the sum of all these convolved signals. Assume that the
receiver knowns all the $A_i$, but does neither know the $x_i$ nor the $h_i$.
When and under which conditions is it possible to recover the individual
signals $x_i$ and the channels $h_i$ from just one received signal $y$? This
challenging problem, which intertwines blind deconvolution with blind demixing,
appears in a variety of applications, such as audio processing, image
processing, neuroscience, spectroscopy, and astronomy. It is also expected to
play a central role in connection with the future Internet-of-Things. We will
prove that under reasonable and practical assumptions, it is possible to solve
this otherwise highly ill-posed problem and recover the $r$ transmitted
functions $x_i$ and the impulse responses $h_i$ in a robust, reliable, and
efficient manner from just one single received function $y$ by solving a
semidefinite program. We derive explicit bounds on the number of measurements
needed for successful recovery and prove that our method is robust in presence
of noise. Our theory is actually a bit pessimistic, since numerical experiments
demonstrate that, quite remarkably, recovery is still possible if the number of
measurements is close to the number of degrees of freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07734</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07734</id><created>2015-12-24</created><authors><author><keyname>Wang</keyname><forenames>Zhichun</forenames></author><author><keyname>Li</keyname><forenames>Juanzi</forenames></author></authors><title>RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent
  Predicate Cycles</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several large-scale RDF knowledge bases have been built and applied
in many knowledge-based applications. To further increase the number of facts
in RDF knowledge bases, logic rules can be used to predict new facts based on
the existing ones. Therefore, how to automatically learn reliable rules from
large-scale knowledge bases becomes increasingly important. In this paper, we
propose a novel rule learning approach named RDF2Rules for RDF knowledge bases.
RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting
frequent patterns in knowledge bases, and then generates rules from the mined
FPCs. Because each FPC can produce multiple rules, and effective pruning
strategy is used in the process of mining FPCs, RDF2Rules works very
efficiently. Another advantage of RDF2Rules is that it uses the entity type
information when generates and evaluates rules, which makes the learned rules
more accurate. Experiments show that our approach outperforms the compared
approach in terms of both efficiency and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07735</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07735</id><created>2015-12-24</created><authors><author><keyname>Data</keyname><forenames>Deepesh</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Manoj M.</forenames></author></authors><title>Communication and Randomness Lower Bounds for Secure Computation</title><categories>cs.CR cs.IT math.IT</categories><comments>30 pages, Submitted to IEEE Transaction of Information Theory. arXiv
  admin note: substantial text overlap with arXiv:1311.7584</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In secure multiparty computation (MPC), mutually distrusting users
collaborate to compute a function of their private data without revealing any
additional information about their data to other users. While it is known that
information theoretically secure MPC is possible among $n$ users (connected by
secure and noiseless links and have access to private randomness) against the
collusion of less than $n/2$ users in the honest-but-curious model, relatively
less is known about the communication and randomness complexity of secure
computation.
  In this work, we employ information theoretic techniques to obtain lower
bounds on the amount of communication and randomness required for secure MPC.
We restrict ourselves to a concrete interactive setting involving 3 users under
which all functions are securely computable against corruption of a single user
in the honest-but-curious model. We derive lower bounds for both the perfect
security case (i.e., zero-error and no leakage of information) and asymptotic
security (where the probability of error and information leakage vanish as
block-length goes to $\infty$).
  Our techniques include the use of a data processing inequality for residual
information (i.e., the gap between mutual information and G\'acs-K\&quot;orner
common information), a new information inequality for 3-user protocols, and the
idea of distribution switching. Our lower bounds are shown to be tight for
various functions of interest. In particular, we show concrete functions which
have &quot;communication-ideal&quot; protocols, i.e., which achieve the minimum
communication simultaneously on all links in the network, and also use minimum
amount of randomness. Also, we obtain the first explicit example of a function
that incurs a higher communication cost than the input length in the secure
computation model of &quot;Feige, Kilian, and Naor [STOC, 1994]&quot;, who had shown that
such functions exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07743</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07743</id><created>2015-12-24</created><authors><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Maeder</keyname><forenames>Andreas</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Sahin</keyname><forenames>Onur</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Cloud Radio Access Network: Virtualizing Wireless Access for Dense
  Heterogeneous Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear on JCN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Radio Access Network (C-RAN) refers to the virtualization of base
station functionalities by means of cloud computing. This results in a novel
cellular architecture in which low-cost wireless access points, known as radio
units (RUs) or remote radio heads (RRHs), are centrally managed by a
reconfigurable centralized &quot;cloud&quot;, or central, unit (CU). C-RAN allows
operators to reduce the capital and operating expenses needed to deploy and
maintain dense heterogeneous networks. This critical advantage, along with
spectral efficiency, statistical multiplexing and load balancing gains, make
C-RAN well positioned to be one of the key technologies in the development of
5G systems. In this paper, a succinct overview is presented regarding the state
of the art on the research on C-RAN with emphasis on fronthaul compression,
baseband processing, medium access control, resource allocation, system-level
considerations and standardization efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07748</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07748</id><created>2015-12-24</created><authors><author><keyname>Nakamura</keyname><forenames>Tomohiko</forenames></author><author><keyname>Nakamura</keyname><forenames>Eita</forenames></author><author><keyname>Sagayama</keyname><forenames>Shigeki</forenames></author></authors><title>Real-Time Audio-to-Score Alignment of Music Performances Containing
  Errors and Arbitrary Repeats and Skips</title><categories>cs.SD cs.LG cs.MM</categories><comments>12 pages, 8 figures, version accepted in IEEE/ACM Transactions on
  Audio, Speech, and Language Processing</comments><doi>10.1109/TASLP.2015.2507862</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses real-time alignment of audio signals of music
performance to the corresponding score (a.k.a. score following) which can
handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)
in performances. This type of score following is particularly useful in
automatic accompaniment for practices and rehearsals, where errors and
repeats/skips are often made. Simple extensions of the algorithms previously
proposed in the literature are not applicable in these situations for scores of
practical length due to the problem of large computational complexity. To cope
with this problem, we present two hidden Markov models of monophonic
performance with errors and arbitrary repeats/skips, and derive efficient
score-following algorithms with an assumption that the prior probability
distributions of score positions before and after repeats/skips are independent
from each other. We confirmed real-time operation of the algorithms with music
scores of practical length (around 10000 notes) on a modern laptop and their
tracking ability to the input performance within 0.7 s on average after
repeats/skips in clarinet performance data. Further improvements and extension
for polyphonic signals are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07755</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07755</id><created>2015-12-24</created><authors><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Uzun</keyname><forenames>Ersin</forenames></author><author><keyname>Wood</keyname><forenames>Christopher A.</forenames></author></authors><title>Living in a PIT-less World: A Case Against Stateful Forwarding in
  Content-Centric Networking</title><categories>cs.NI</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-Centric Networking (ICN) is a recent paradigm that claims to
mitigate some limitations of the current IP-based Internet architecture. The
centerpiece of ICN is named and addressable content, rather than hosts or
interfaces. Content-Centric Networking (CCN) is a prominent ICN instance that
shares the fundamental architectural design with its equally popular academic
sibling Named-Data Networking (NDN). CCN eschews source addresses and creates
one-time virtual circuits for every content request (called an interest). As an
interest is forwarded it creates state in intervening routers and the requested
content back is delivered over the reverse path using that state.
  Although a stateful forwarding plane might be beneficial in terms of
efficiency, and resilience to certain types of attacks, this has not been
decisively proven via realistic experiments. Since keeping per-interest state
complicates router operations and makes the infrastructure susceptible to
router state exhaustion attacks (e.g., there is currently no effective defense
against interest flooding attacks), the value of the stateful forwarding plane
in CCN should be re-examined.
  In this paper, we explore supposed benefits and various problems of the
stateful forwarding plane. We then argue that its benefits are uncertain at
best and it should not be a mandatory CCN feature. To this end, we propose a
new stateless architecture for CCN that provides nearly all functionality of
the stateful design without its headaches. We analyze performance and resource
requirements of the proposed architecture, via experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07766</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07766</id><created>2015-12-24</created><authors><author><keyname>Koseleff</keyname><forenames>P. -V</forenames><affiliation>OURAGAN, IMJ-PRG, UPMC</affiliation></author><author><keyname>Pecker</keyname><forenames>D</forenames><affiliation>IMJ-PRG, UPMC</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>OURAGAN, IMJ-PRG, UPMC</affiliation></author><author><keyname>Tran</keyname><forenames>C</forenames><affiliation>UPMC, IMJ-PRG</affiliation></author></authors><title>Computing Chebyshev knot diagrams</title><categories>cs.SC</categories><proxy>ccsd</proxy><report-no>NOT1001.5192</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Chebyshev curve $\cC(a,b,c,\phi)$ has a parametrization of the form$
x(t)=T\_a(t)$; \ $y(t)=T\_b(t)$; $z(t)= T\_c(t + \phi)$, where $a,b,c$are
integers, $T\_n(t)$ is the Chebyshev polynomialof degree $n$ and $\phi \in
\RR$. When $\cC(a,b,c,\phi)$ is nonsingular,it defines a polynomial knot. We
determine all possible knot diagrams when $\phi$ varies. Let $a,b,c$ be
integers, $a$ is odd, $(a,b)=1$, we show that one can list all possible knots
$\cC(a,b,c,\phi)$ in$\tcO(n^2)$ bit operations, with $n=abc$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07767</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07767</id><created>2015-12-24</created><authors><author><keyname>Miotk</keyname><forenames>Mateusz</forenames></author><author><keyname>Topp</keyname><forenames>Jerzy</forenames></author></authors><title>Hangable Graphs</title><categories>cs.DM math.CO</categories><msc-class>05C05, 05C12, 05C76</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V_G,E_G)$ be a connected graph. The distance $d_G(u,v)$ between
vertices $u$ and $v$ in $G$ is the length of a shortest $u-v$ path in $G$. The
eccentricity of a vertex $v$ in $G$ is the integer $e_G(v)= \max\{ d_G(v,u)
\colon u\in V_G\}$. The diameter of $G$ is the integer $d(G)=
\max\{e_G(v)\colon v\in V_G\}$. The periphery of a~vertex $v$ of $G$ is the set
$P_G(v)= \{u\in V_G\colon d_G(v,u)= e_G(v)\}$, while the periphery of $G$ is
the set $P(G)= \{v\in V_G\colon e_G(v)=d(G)\}$. We say that graph $G$ is
hangable if $P_G(v)\subequal P(G)$ for every vertex $v$ of $G$. In this paper
we prove that every block graph is hangable and discuss the hangability of
products of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07771</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07771</id><created>2015-12-24</created><updated>2016-01-06</updated><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>Kamphorst</keyname><forenames>Bart</forenames></author><author><keyname>Zwart</keyname><forenames>Bert</forenames></author></authors><title>Achievable Performance of Blind Policies in Heavy Traffic</title><categories>math.PR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a GI/GI/1 queue, we show that the average sojourn time under the (blind)
Randomized Multilevel Feedback algorithm is no worse than that under the
Shortest Remaining Processing Time algorithm times a logarithmic function of
the system load. Moreover, it is verified that this bound is tight in heavy
traffic, up to a constant multiplicative factor. We obtain this result by
combining techniques from two disparate areas: competitive analysis and applied
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07780</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07780</id><created>2015-12-24</created><authors><author><keyname>Verborgh</keyname><forenames>Ruben</forenames></author><author><keyname>Arndt</keyname><forenames>D&#xf6;rthe</forenames></author><author><keyname>Van Hoecke</keyname><forenames>Sofie</forenames></author><author><keyname>De Roo</keyname><forenames>Jos</forenames></author><author><keyname>Mels</keyname><forenames>Giovanni</forenames></author><author><keyname>Steiner</keyname><forenames>Thomas</forenames></author><author><keyname>Gabarro</keyname><forenames>Joaquim</forenames></author></authors><title>The Pragmatic Proof: Hypermedia API Composition and Execution</title><categories>cs.LO</categories><comments>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine clients are increasingly making use of the Web to perform tasks.
While Web services traditionally mimic remote procedure calling interfaces, a
new generation of so-called hypermedia APIs works through hyperlinks and forms,
in a way similar to how people browse the Web. This means that existing
composition techniques, which determine a procedural plan upfront, are not
sufficient to consume hypermedia APIs, which need to be navigated at runtime.
Clients instead need a more dynamic plan that allows them to follow hyperlinks
and use forms with a preset goal. Therefore, in this article, we show how
compositions of hypermedia APIs can be created by generic Semantic Web
reasoners. This is achieved through the generation of a proof based on semantic
descriptions of the APIs' functionality. To pragmatically verify the
applicability of compositions, we introduce the notion of pre-execution and
post-execution proofs. The runtime interaction between a client and a server is
guided by proofs but driven by hypermedia, allowing the client to react to the
application's actual state indicated by the server's response. We describe how
to generate compositions from descriptions, discuss a computer-assisted process
to generate descriptions, and verify reasoner performance on various
composition tasks using a benchmark suite. The experimental results lead to the
conclusion that proof-based consumption of hypermedia APIs is a feasible
strategy at Web scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07782</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07782</id><created>2015-12-24</created><authors><author><keyname>&#xc7;akmak</keyname><forenames>Burak</forenames></author><author><keyname>Urup</keyname><forenames>Daniel N.</forenames></author><author><keyname>Meyer</keyname><forenames>Florian</forenames></author><author><keyname>Pedersen</keyname><forenames>Troels</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard H.</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>Cooperative Localization for Mobile Networks: A Distributed Belief
  Propagation - Mean Field Message Passing Algorithm</title><categories>cs.SY cs.MA</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a hybrid message passing method for distributed cooperative
localization and tracking of mobile agents. Belief propagation (BP) and mean
field (MF) message passing are employed for, respectively, the motion-related
and measurement-related parts of the underlying factor graph. Using a Gaussian
belief approximation, closed-form expressions of all messages are obtained, and
only three real values per message passing iteration have to be broadcast to
neighboring agents. Despite these very low communication requirements, the
estimation accuracy can be comparable to that of particle-based BP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07783</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07783</id><created>2015-12-24</created><authors><author><keyname>Patil</keyname><forenames>Aakash</forenames></author><author><keyname>Shen</keyname><forenames>Shanlan</forenames></author><author><keyname>Yao</keyname><forenames>Enyi</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>Hardware Architecture for Large Parallel Array of Random Feature
  Extractors applied to Image Recognition</title><categories>cs.ET cs.NE</categories><comments>Submitted for ELM special issue in Neurocomputing, 18 pages, 7
  figures, 3 tables. ACM class: &quot;Hardware/Emerging Technologies&quot;</comments><acm-class>C.3; C.5.4; I.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a low-power and compact hardware implementation of Random
Feature Extractor (RFE) core. With complex tasks like Image Recognition
requiring a large set of features, we show how weight reuse technique can allow
to virtually expand the random features available from RFE core. Further, we
show how to avoid computation cost wasted for propagating &quot;incognizant&quot; or
redundant random features. For proof of concept, we validated our approach by
using our RFE core as the first stage of Extreme Learning Machine (ELM)--a two
layer neural network--and were able to achieve $&gt;97\%$ accuracy on MNIST
database of handwritten digits. ELM's first stage of RFE is done on an analog
ASIC occupying $5$mm$\times5$mm area in $0.35\mu$m CMOS and consuming $5.95$
$\mu$J/classify while using $\approx 5000$ effective hidden neurons. The ELM
second stage consisting of just adders can be implemented as digital circuit
with estimated power consumption of $20.9$ nJ/classify. With a total energy
consumption of only $5.97$ $\mu$J/classify, this low-power mixed signal ASIC
can act as a co-processor in portable electronic gadgets with cameras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07797</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07797</id><created>2015-12-24</created><authors><author><keyname>Yu</keyname><forenames>Jiaqian</forenames><affiliation>CVC, GALEN</affiliation></author><author><keyname>Blaschko</keyname><forenames>Matthew</forenames></author></authors><title>The Lov\'asz Hinge: A Convex Surrogate for Submodular Losses</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning with non-modular losses is an important problem when sets of
predictions are made simultaneously. The main tools for constructing convex
surrogate loss functions for set prediction are margin rescaling and slack
rescaling. In this work, we show that these strategies lead to tight convex
surrogates iff the underlying loss function is increasing in the number of
incorrect predictions. However, gradient or cutting-plane computation for these
functions is NP-hard for non-supermodular loss functions. We propose instead a
novel surrogate loss function for submodular losses, the Lov{\'a}sz hinge,
which leads to O(p log p) complexity with O(p) oracle accesses to the loss
function to compute a gradient or cutting-plane. We prove that the Lov{\'a}sz
hinge is convex and yields an extension. As a result, we have developed the
first tractable convex surrogates in the literature for submodular losses. We
demonstrate the utility of this novel convex surrogate through several set
prediction tasks, including on the PASCAL VOC and Microsoft COCO datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07800</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07800</id><created>2015-12-24</created><authors><author><keyname>Korman</keyname><forenames>Amos</forenames><affiliation>LIAFA, GANG</affiliation></author><author><keyname>Kutten</keyname><forenames>Shay</forenames><affiliation>Department of Information and Computer sciences Osaka University</affiliation></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames><affiliation>Department of Information and Computer sciences Osaka University</affiliation></author></authors><title>Fast and compact self-stabilizing verification, computation, and fault
  detection of an MST</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>Distributed Computing, Springer Verlag, 2015, 28 (4)</journal-ref><doi>10.1007/s00446-015-0242-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates the usefulness of distributed local verification of
proofs, as a tool for the design of self-stabilizing algorithms.In particular,
it introduces a somewhat generalized notion of distributed local proofs, and
utilizes it for improving the time complexity significantly, while maintaining
space optimality. As a result, we show that optimizing the memory size carries
at most a small cost in terms of time, in the context of Minimum Spanning Tree
(MST). That is, we present algorithms that are both time and space efficient
for both constructing an MST and for verifying it.This involves several parts
that may be considered contributions in themselves.First, we generalize the
notion of local proofs, trading off the time complexity for memory efficiency.
This adds a dimension to the study of distributed local proofs, which has been
gaining attention recently. Specifically, we design a (self-stabilizing) proof
labeling scheme which is memory optimal (i.e., $O(\log n)$ bits per node), and
whose time complexity is $O(\log ^2 n)$ in synchronous networks, or $O(\Delta
\log ^3 n)$ time in asynchronous ones, where $\Delta$ is the maximum degree of
nodes. This answers an open problem posed by Awerbuch and Varghese (FOCS 1991).
We also show that $\Omega(\log n)$ time is necessary, even in synchronous
networks. Another property is that if $f$ faults occurred, then, within the
requireddetection time above, they are detected by some node in the $O(f\log
n)$ locality of each of the faults.Second, we show how to enhance a known
transformer that makes input/output algorithms self-stabilizing. It now takes
as input an efficient construction algorithm and an efficient self-stabilizing
proof labeling scheme, and produces an efficient self-stabilizing algorithm.
When used for MST, the transformer produces a memory optimal self-stabilizing
algorithm, whose time complexity, namely, $O(n)$, is significantly better even
than that of previous algorithms. (The time complexity of previous MST
algorithms that used $\Omega(\log^2 n)$ memory bits per node was $O(n^2)$, and
the time for optimal space algorithms was $O(n|E|)$.) Inherited from our proof
labelling scheme, our self-stabilising MST construction algorithm also has the
following two properties: (1) if faults occur after the construction ended,
then they are detected by some nodes within $O(\log ^2 n)$ time in synchronous
networks, or within $O(\Delta \log ^3 n)$ time in asynchronous ones, and (2) if
$f$ faults occurred, then, within the required detection time above, they are
detected within the $O(f\log n)$ locality of each of the faults. We also show
how to improve the above two properties, at the expense of some increase in the
memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07805</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07805</id><created>2015-12-24</created><authors><author><keyname>Su</keyname><forenames>Maomeng</forenames></author><author><keyname>Zhang</keyname><forenames>Mingxing</forenames></author><author><keyname>Chen</keyname><forenames>Kang</forenames></author><author><keyname>Wu</keyname><forenames>Yongwei</forenames></author><author><keyname>Li</keyname><forenames>Guoliang</forenames></author></authors><title>RFP: A Remote Fetching Paradigm for RDMA-Accelerated Systems</title><categories>cs.DC</categories><comments>11 pages, 10 figures; Key Words: RDMA and InfiniBand, Remote Fetching
  Paradigm, IOPS, and Small Data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote Direct Memory Access (RDMA) is an efficient way to improve the
performance of traditional client-server systems. Currently, there are two main
design paradigms for RDMA-accelerated systems. The first allows the clients to
directly operate the server's memory and totally bypasses the CPUs at server
side. The second follows the traditional server-reply paradigm, which asks the
server to write results back to the clients. However, the first method has to
expose server's memory and needs tremendous re-design of upper-layer software,
which is complex, unsafe, error-prone, and inefficient. The second cannot
achieve high input/output operations per second (IOPS), because it employs
out-bound RDMA-write at server side which is not efficient.
  We find that the performance of out-bound RDMA-write and in-bound RDMA-read
is asymmetric and the latter is 5 times faster than the former. Based on this
observation, we propose a novel design paradigm named Remote Fetching Paradigm
(RFP). In RFP, the server is still responsible for processing requests from the
clients. However, counter-intuitively, instead of sending results back to the
clients through out-bound RDMA-write, the server only writes the results in
local memory buffers, and the clients use in-bound RDMA-read to remotely fetch
these results. Since in-bound RDMA-read achieves much higher IOPS than
out-bound RDMA-write, our model is able to bring higher performance than the
traditional models.
  In order to prove the effectiveness of RFP, we design and implement an
RDMA-accelerated in-memory key-value store following the RFP model. To further
improve the IOPS, we propose an optimization mechanism that combines status
checking and result fetching. Experiment results show that RFP can improve the
IOPS by 160%~310% against state-of-the-art models for in-memory key-value
stores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07806</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07806</id><created>2015-12-24</created><authors><author><keyname>Deng</keyname><forenames>Zhi-Hong</forenames></author></authors><title>Mining Top-K Co-Occurrence Items</title><categories>cs.DB cs.DS</categories><comments>12 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequent itemset mining has emerged as a fundamental problem in data mining
and plays an important role in many data mining tasks, such as association
analysis, classification, etc. In the framework of frequent itemset mining, the
results are itemsets that are frequent in the whole database. However, in some
applications, such recommendation systems and social networks, people are more
interested in finding out the items that occur with some user-specified
itemsets (query itemsets) most frequently in a database. In this paper, we
address the problem by proposing a new mining task named top-k co-occurrence
item mining, where k is the desired number of items to be found. Four baseline
algorithms are presented first. Then, we introduce a special data structure
named Pi-Tree (Prefix itemset Tree) to maintain the information of itemsets.
Based on Pi-Tree, we propose two algorithms, namely PT (Pi-Tree-based
algorithm) and PT-TA (Pi-Tree-based algorithm with TA pruning), for mining
top-k co-occurrence items by incorporating several novel strategies for pruning
the search space to achieve high efficiency. The performance of PT and PT-TA
was evaluated against the four proposed baseline algorithms on both synthetic
and real databases. Extensive experiments show that PT not only outperforms
other algorithms substantially in terms execution time but also has excellent
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07807</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07807</id><created>2015-12-24</created><updated>2016-01-25</updated><authors><author><keyname>Virtanen</keyname><forenames>Seppo</forenames></author><author><keyname>Afrabandpey</keyname><forenames>Homayun</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Visualizations Relevant to The User By Multi-View Latent Variable
  Factorization</title><categories>cs.LG cs.IR</categories><comments>IEEE International Conference on Acoustic, Speech and Signal
  Processing 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A main goal of data visualization is to find, from among all the available
alternatives, mappings to the 2D/3D display which are relevant to the user.
Assuming user interaction data, or other auxiliary data about the items or
their relationships, the goal is to identify which aspects in the primary data
support the user\'s input and, equally importantly, which aspects of the
user\'s potentially noisy input have support in the primary data. For solving
the problem, we introduce a multi-view embedding in which a latent
factorization identifies which aspects in the two data views (primary data and
user data) are related and which are specific to only one of them. The
factorization is a generative model in which the display is parameterized as a
part of the factorization and the other factors explain away the aspects not
expressible in a two-dimensional display. Functioning of the model is
demonstrated on several data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07815</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07815</id><created>2015-12-24</created><authors><author><keyname>Pansari</keyname><forenames>Pankaj</forenames></author><author><keyname>Kumar</keyname><forenames>M. Pawan</forenames></author></authors><title>Truncated Max-of-Convex Models</title><categories>cs.CV</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Truncated convex models (TCM) are special cases of pairwise random fields
that have been widely used in computer vision. However, by restricting the
order of the potentials to be at most two, they fail to capture useful image
statistics. We propose a natural generalization of TCM to high-order random
fields, which we call truncated max-of-convex models (TMCM). The energy
function of TMCM consists of two types of potentials: (i) unary potentials,
which have no restriction on their form; and (ii) high-order potentials, which
are the sum of the truncation of the m largest convex distances over disjoint
pairs of random variables in an arbitrary size clique. The use of a convex
distance function encourages smoothness, while truncation allows for
discontinuities in the labeling. By using m &gt; 1, TMCM provides robustness
towards errors in the clique definition. In order to minimize the energy
function of a TMCM over all possible labelings, we design an efficient
st-mincut based range expansion algorithm. We prove the accuracy of our
algorithm by establishing strong multiplicative bounds for several special
cases of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07818</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07818</id><created>2015-12-24</created><authors><author><keyname>Aljarbouh</keyname><forenames>Ayman</forenames><affiliation>HYCOMES</affiliation></author><author><keyname>Caillaud</keyname><forenames>Benoit</forenames><affiliation>HYCOMES</affiliation></author></authors><title>Robust Simulation for Hybrid Systems: Chattering Path Avoidance</title><categories>cs.CE cs.SY math.DS</categories><comments>The 56th Conference on Simulation and Modelling (SIMS 56), Oct 2015,
  Link\&quot;oping, Sweden. 2015, Link\&quot;oping University Press</comments><proxy>ccsd</proxy><doi>10.3384/ecp15119175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sliding mode approach is recognized as an efficient tool for treating the
chattering behavior in hybrid systems. However, the amplitude of chattering, by
its nature, is proportional to magnitude of discontinuous control. A possible
scenario is that the solution trajectories may successively enter and exit as
well as slide on switching mani-folds of different dimensions. Naturally, this
arises in dynamical systems and control applications whenever there are
multiple discontinuous control variables. The main contribution of this paper
is to provide a robust computational framework for the most general way to
extend a flow map on the intersection of p intersected (n--1)-dimensional
switching manifolds in at least p dimensions. We explore a new formulation to
which we can define unique solutions for such particular behavior in hybrid
systems and investigate its efficient computation/simulation. We illustrate the
concepts with examples throughout the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07827</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07827</id><created>2015-12-24</created><updated>2016-01-31</updated><authors><author><keyname>You</keyname><forenames>Tao</forenames></author><author><keyname>Shia</keyname><forenames>Ben-Chang</forenames></author><author><keyname>Zhang</keyname><forenames>Zhong-Yuan</forenames></author></authors><title>Community Detection in Complex Networks Using Density-based Clustering
  Algorithm</title><categories>cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Like clustering analysis, community detection aims at assigning nodes in a
network into different communities. Fdp is a recently proposed density-based
clustering algorithm which does not need the number of clusters as prior input
and the result is insensitive to its parameter. However, Fdp cannot be directly
applied to community detection due to its inability to recognize the community
centers in the network. To solve the problem, a new community detection method
(named IsoFdp) is proposed in this paper. First, we use Isomap technique to map
the network data into a low dimensional manifold which can reveal diverse
pair-wised similarity. Then Fdp is applied to detect the communities in
networks. An improved partition density function is proposed to select the
proper number of communities automatically. We test our method on both
synthetic and real-world networks, and the results demonstrate the
effectiveness of our algorithm over the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07831</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07831</id><created>2015-12-24</created><updated>2016-02-20</updated><authors><author><keyname>Qiu</keyname><forenames>Jiezhong</forenames></author><author><keyname>Li</keyname><forenames>Yixuan</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Lu</keyname><forenames>Zheng</forenames></author><author><keyname>Ye</keyname><forenames>Hao</forenames></author><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author><author><keyname>Hopcroft</keyname><forenames>John</forenames></author></authors><title>The Lifecycle and Cascade of WeChat Social Messaging Groups</title><categories>cs.SI</categories><comments>10 pages, 8 figures, to appear in proceedings of the 25th
  International World Wide Web Conference (WWW 2016)</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social instant messaging services are emerging as a transformative form with
which people connect, communicate with friends in their daily life - they
catalyze the formation of social groups, and they bring people stronger sense
of community and connection. However, research community still knows little
about the formation and evolution of groups in the context of social messaging
- their lifecycles, the change in their underlying structures over time, and
the diffusion processes by which they develop new members. In this paper, we
analyze the daily usage logs from WeChat group messaging platform - the largest
standalone messaging communication service in China - with the goal of
understanding the processes by which social messaging groups come together,
grow new members, and evolve over time. Specifically, we discover a strong
dichotomy among groups in terms of their lifecycle, and develop a separability
model by taking into account a broad range of group-level features, showing
that long-term and short-term groups are inherently distinct. We also found
that the lifecycle of messaging groups is largely dependent on their social
roles and functions in users' daily social experiences and specific purposes.
Given the strong separability between the long-term and short-term groups, we
further address the problem concerning the early prediction of successful
communities. In addition to modeling the growth and evolution from group-level
perspective, we investigate the individual-level attributes of group members
and study the diffusion process by which groups gain new members. By
considering members' historical engagement behavior as well as the local social
network structure that they embedded in, we develop a membership cascade model
and demonstrate the effectiveness by achieving AUC of 95.31% in predicting
inviter, and an AUC of 98.66% in predicting invitee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07839</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07839</id><created>2015-12-22</created><authors><author><keyname>Sokoloski</keyname><forenames>Sacha</forenames></author></authors><title>Nonlinear Filtering with Population Codes</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a method for the nonlinear filtering of dynamic
population codes with unknown latent dynamics. This method is based on a
parameterized function which updates beliefs while keeping the dimension of the
belief space constant. In order to optimize this function, we construct a
graphical model representation of the belief-latent state dynamics, and show
that a certain marginalization of the graphical model has the form of a
conditional exponential family harmonium. This marginalization allows us to
maximize the likelihood of observations, and thereby optimize the parameterized
function, by the method of contrastive divergence minimization. By defining the
parameterized function as a multilayer perceptron, we show how the stochastic
contrastive divergence gradient may be computed with backpropagation. We
conclude by demonstrating the effectiveness of this approach on a filtering
problem based on a stochastic pendulum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07849</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07849</id><created>2015-12-24</created><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Dross</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Narrowing the gap in the clique-width dichotomy for $(H_1,H_2)$-free
  graphs</title><categories>cs.DM math.CO</categories><comments>31 pages, 3 figures</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue a recent systematic study into the clique-width of
$(H_1,H_2)$-free graphs and present five new classes of $(H_1,H_2)$-free graphs
of bounded clique-width. As a consequence, we have reduced the number of open
cases from 13 to 8. Four of the five new graph classes have in common that one
of their two forbidden induced subgraphs is the triangle. As such, we
generalize corresponding results for $H$-free bipartite graphs. Our new results
are based on the exploitation of a common technique, which aims to reduce the
graph under consideration to a $k$-partite graph that can be decomposed in a
certain way which, as we prove, ensures boundedness of their clique-width. We
also discuss the (immediate) consequences of our work for the study into
determining the complexity of the Vertex Colouring problem for $(H_1,H_2)$-free
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07851</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07851</id><created>2015-12-24</created><updated>2016-01-25</updated><authors><author><keyname>Keshet</keyname><forenames>Joseph</forenames></author><author><keyname>Kariv</keyname><forenames>Adam</forenames></author><author><keyname>Dagan</keyname><forenames>Arnon</forenames></author><author><keyname>Volk</keyname><forenames>Dvir</forenames></author><author><keyname>Simhon</keyname><forenames>Joey</forenames></author></authors><title>Context-Based Prediction of App Usage</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are around a hundred installed apps on an average smartphone. The high
number of apps and the limited number of app icons that can be displayed on the
device's screen requires a new paradigm to address their visibility to the
user. In this paper we propose a new online algorithm for dynamically
predicting a set of apps that the user is likely to use. The algorithm runs on
the user's device and constantly learns the user's habits at a given time,
location, and device state. It is designed to actively help the user to
navigate to the desired app as well as to provide a personalized feeling, and
hence is aimed at maximizing the AUC. We show both theoretically and
empirically that the algorithm maximizes the AUC, and yields good results on a
set of 1,000 devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07856</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07856</id><created>2015-12-24</created><authors><author><keyname>Sengupta</keyname><forenames>Avik</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Cache Aided Wireless Networks: Tradeoffs between Storage and Latency</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the fundamental information theoretic limits of cache-aided
wireless networks, in which edge nodes (or transmitters) are endowed with
caches that can store popular content, such as multimedia files. This
architecture aims to localize popular multimedia content by proactively pushing
it closer to the edge of the wireless network, thereby alleviating backhaul
load. An information theoretic model of such networks is presented, that
includes the introduction of a new metric, namely normalized delivery time
(NDT), which captures the worst case time to deliver any requested content to
the users. We present new results on the trade-off between latency, measured
via the NDT, and the cache storage capacity of the edge nodes. In particular, a
novel information theoretic lower bound on NDT is presented for cache aided
networks. The optimality of this bound is shown for several system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07872</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07872</id><created>2015-12-24</created><authors><author><keyname>Del Pia</keyname><forenames>Alberto</forenames></author><author><keyname>Michini</keyname><forenames>Carla</forenames></author></authors><title>On the diameter of lattice polytopes</title><categories>cs.CG math.CO</categories><msc-class>52B05, 52B20, 90C05</msc-class><acm-class>G.1.6; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the diameter of a d-dimensional lattice polytope
in [0,k]^n is at most (k - 1/2) d. This result implies that the diameter of a
d-dimensional half-integral polytope is at most 3/2 d. We also show that for
half-integral polytopes the latter bound is tight for any d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07876</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07876</id><created>2015-12-24</created><authors><author><keyname>Liu</keyname><forenames>Chao</forenames></author><author><keyname>Ghosal</keyname><forenames>Sambuddha</forenames></author><author><keyname>Jiang</keyname><forenames>Zhanhong</forenames></author><author><keyname>Sarkar</keyname><forenames>Soumik</forenames></author></authors><title>An unsupervised spatiotemporal graphical modeling approach to anomaly
  detection in distributed CPS</title><categories>cs.LG</categories><comments>10 pages, to appear in ICCPS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern distributed cyber-physical systems (CPSs) encounter a large variety of
physical faults and cyber anomalies and in many cases, they are vulnerable to
catastrophic fault propagation scenarios due to strong connectivity among the
sub-systems. This paper presents a new data-driven framework for system-wide
anomaly detection for addressing such issues. The framework is based on a
spatiotemporal feature extraction scheme built on the concept of symbolic
dynamics for discovering and representing causal interactions among the
subsystems of a CPS. The extracted spatiotemporal features are then used to
learn system-wide patterns via a Restricted Boltzmann Machine (RBM). The
results show that: (1) the RBM free energy in the off-nominal conditions are
different from that in the nominal conditions and can be used for anomaly
detection; (2) the framework can capture multiple nominal modes with one
graphical model; (3) the case studies with simulated data and an integrated
building system validates the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07892</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07892</id><created>2015-12-24</created><authors><author><keyname>Koh</keyname><forenames>Dax Enshan</forenames></author></authors><title>Further extensions of Clifford circuits and their classical simulation
  complexities</title><categories>quant-ph cs.CC</categories><comments>18 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extended Clifford circuits straddle the boundary between classical and
quantum computational power. Whether such circuits are efficiently classically
simulable seems to depend delicately on the ingredients of the circuits. While
some combinations of ingredients lead to efficiently classically simulable
circuits, other combinations that might just be slightly different, lead to
circuits which are likely not. We extend the results of Jozsa and Van den Nest
(arXiv:1305.6190) by studying two further extensions of Clifford circuits.
Firstly, we consider how the classical simulation complexity changes when we
allow for more general measurements. Secondly, we investigate different notions
of what it means to &quot;classically simulate&quot; a quantum circuit. These further
extensions give us 24 new combinations of ingredients compared to Jozsa and Van
den Nest, and we give a complete classification of their classical simulation
complexities. Our results provide more examples where seemingly modest changes
to the ingredients of Clifford circuits lead to large changes in the classical
simulation complexity under plausible complexity assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07893</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07893</id><created>2015-12-08</created><authors><author><keyname>Amendola</keyname><forenames>Luca</forenames><affiliation>U. of Heidelberg, Germany</affiliation></author><author><keyname>Marra</keyname><forenames>Valerio</forenames><affiliation>UFES, Vit&#xf3;ria, Brazil</affiliation></author><author><keyname>Quartin</keyname><forenames>Miguel</forenames><affiliation>UFRJ, Rio de Janeiro, Brazil</affiliation></author></authors><title>The evolving perception of controversial movies</title><categories>physics.soc-ph cs.SI</categories><comments>Published in Palgrave Communications; 9 pages. Supplementary
  information on the journal website</comments><doi>10.1057/palcomms.2015.38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polarization of opinion is an important feature of public debate on
political, social and cultural topics. The availability of large internet
databases of users' ratings has permitted quantitative analysis of polarization
trends-for instance, previous studies have included analyses of controversial
topics on Wikipedia, as well as the relationship between online reviews and a
product's perceived quality. Here, we study the dynamics of polarization in the
movie ratings collected by the Internet Movie database (IMDb) website in
relation to films produced over the period 1915-2015. We define two statistical
indexes, dubbed hard and soft controversiality, which quantify polarized and
uniform rating distributions, respectively. We find that controversy decreases
with popularity and that hard controversy is relatively rare. Our findings also
suggest that more recent movies are more controversial than older ones and we
detect a trend of &quot;convergence to the mainstream&quot; with a time scale of roughly
40-50 years. This phenomenon appears qualitatively different from trends
observed in both online reviews of commercial products and in political debate,
and we speculate that it may be connected with the absence of long-lived &quot;echo
chambers&quot; in the cultural domain. This hypothesis can and should be tested by
extending our analysis to other forms of cultural expression and/or to
databases with different demographic user bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07901</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07901</id><created>2015-12-24</created><updated>2016-01-18</updated><authors><author><keyname>Bressan</keyname><forenames>Marco</forenames></author><author><keyname>Peserico</keyname><forenames>Enoch</forenames></author><author><keyname>Pretto</keyname><forenames>Luca</forenames></author></authors><title>Simple set cardinality estimation through random sampling</title><categories>cs.DM</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple algorithm for estimating the cardinality of a set $I$,
based on a $RandomSample(I)$ primitive that returns an element of $I$ uniformly
at random. Our algorithm with probability $(1-p_{err})$ returns an estimate of
$|I|$ accurate within a factor $(1\pm\delta_{err})$ invoking $RandomSample(I)$
at most
$O\big(\frac{1}{\delta_{err}}\sqrt{\log(\frac{1}{p_{err}})}\sqrt{|I|}\big)$
times (for
$\frac{1}{\delta_{err}}\sqrt{\log(\frac{1}{p_{err}})}=O(\sqrt{|I|})$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07919</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07919</id><created>2015-12-24</created><authors><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>DuPrie</keyname><forenames>Kimberly</forenames></author><author><keyname>Mink</keyname><forenames>Jessica</forenames></author><author><keyname>Nemiroff</keyname><forenames>Robert</forenames></author><author><keyname>Robitaille</keyname><forenames>Thomas</forenames></author><author><keyname>Shamir</keyname><forenames>Lior</forenames></author><author><keyname>Shortridge</keyname><forenames>Keith</forenames></author><author><keyname>Taylor</keyname><forenames>Mark</forenames></author><author><keyname>Teuben</keyname><forenames>Peter</forenames></author><author><keyname>Wallin</keyname><forenames>John</forenames></author></authors><title>Improving Software Citation and Credit</title><categories>cs.DL astro-ph.IM</categories><comments>Birds of a Feather session organized by the Astrophysics Source Code
  Library (ASCL, http://ascl.net/ ); to be published in Proceedings of ADASS
  XXV (Sydney, Australia; October, 2015). 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past year has seen movement on several fronts for improving software
citation, including the Center for Open Science's Transparency and Openness
Promotion (TOP) Guidelines, the Software Publishing Special Interest Group that
was started at January's AAS meeting in Seattle at the request of that
organization's Working Group on Astronomical Software, a Sloan-sponsored
meeting at GitHub in San Francisco to begin work on a cohesive research
software citation-enabling platform, the work of Force11 to &quot;transform and
improve&quot; research communication, and WSSSPE's ongoing efforts that include
software publication, citation, credit, and sustainability.
  Brief reports on these efforts were shared at the BoF, after which
participants discussed ideas for improving software citation, generating a list
of recommendations to the community of software authors, journal publishers,
ADS, and research authors. The discussion, recommendations, and feedback will
help form recommendations for software citation to those publishers represented
in the Software Publishing Special Interest Group and the broader community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07928</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07928</id><created>2015-12-24</created><authors><author><keyname>Hong</keyname><forenames>Seunghoon</forenames></author><author><keyname>Oh</keyname><forenames>Junhyuk</forenames></author><author><keyname>Han</keyname><forenames>Bohyung</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author></authors><title>Learning Transferrable Knowledge for Semantic Segmentation with Deep
  Convolutional Neural Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel weakly-supervised semantic segmentation algorithm based on
Deep Convolutional Neural Network (DCNN). Contrary to existing
weakly-supervised approaches, our algorithm exploits auxiliary segmentation
annotations available for different categories to guide segmentations on images
with only image-level class labels. To make the segmentation knowledge
transferrable across categories, we design a decoupled encoder-decoder
architecture with attention model. In this architecture, the model generates
spatial highlights of each category presented in an image using an attention
model, and subsequently generates foreground segmentation for each highlighted
region using decoder. Combining attention model, we show that the decoder
trained with segmentation annotations in different categories can boost the
performance of weakly-supervised semantic segmentation. The proposed algorithm
demonstrates substantially improved performance compared to the
state-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012
dataset when our model is trained with the annotations in 60 exclusive
categories in Microsoft COCO dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07931</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07931</id><created>2015-12-24</created><authors><author><keyname>Chen</keyname><forenames>Hugh</forenames></author><author><keyname>Erol</keyname><forenames>Yusuf</forenames></author><author><keyname>Shen</keyname><forenames>Eric</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Probabilistic Model-Based Approach for Heart Beat Detection</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, hospitals are ubiquitous and integral to modern society. Patients
flow in and out of a veritable whirlwind of paperwork, consultations, and
potential inpatient admissions, through an abstracted system that is not
without flaws. One of the biggest flaws in the medical system is perhaps an
unexpected one: the patient alarm system. One longitudinal study reported an
88.8% rate of false alarms, with other studies reporting numbers of similar
magnitudes. These false alarm rates lead to a number of deleterious effects
that manifest in a significantly lower standard of care across clinics.
  This paper discusses a model-based probabilistic inference approach to
identifying variables at a detection level. We design a generative model that
complies with an overview of human physiology and perform approximate Bayesian
inference. One primary goal of this paper is to justify a Bayesian modeling
approach to increasing robustness in a physiological domain.
  We use three data sets provided by Physionet, a research resource for complex
physiological signals, in the form of the Physionet 2014 Challenge set-p1 and
set-p2, as well as the MGH/MF Waveform Database. On the extended data set our
algorithm is on par with the other top six submissions to the Physionet 2014
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07937</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07937</id><created>2015-12-24</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Arnold</keyname><forenames>Curtis</forenames></author></authors><title>Towards Approaches to Continuous Assessment of Cyber Risk in Security of
  Computer Networks</title><categories>cs.CR</categories><comments>A version of this paper appeared in IEEE Security and Privacy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the current status and research challenges in the area of cyber
security often called continuous monitoring and risk scoring (CMRS). We focus
on two most salient aspects of CMRS. First, continuous collection of data
through automated feeds; hence the term continuous monitoring. Typical data
collected for continuous monitoring purposes include network traffic
information as well as host information from host-based agents. Second,
analysis of the collected data in order to assess the risks - the risk scoring.
This assessment may include flagging especially egregious vulnerabilities and
exposures, or computing metrics that provide an overall characterization of the
network's risk level. Currently used risk metrics are often simple sums or
counts of vulnerabilities and missing patches.
  The research challenges pertaining to CMRS fall mainly into two categories.
The first centers on the problem of integrating and fusing highly heterogeneous
information. The second group of challenges is the lack of rigorous approaches
to computing risk. Existing risk scoring algorithms remain limited to ad hoc
heuristics such as simple sums of vulnerability scores or counts of things like
missing patches or open ports, etc. Weaknesses and potentially misleading
nature of such metrics are well recognized. For example, the individual
vulnerability scores are dangerously reliant on subjective, human, qualitative
input, potentially inaccurate and expensive to obtain. Further, the total
number of vulnerabilities may matters far less than how vulnerabilities are
distributed over hosts, or over time. Similarly, neither topology of the
network nor the roles and dynamics of inter-host interactions are considered by
simple sums of vulnerabilities or missing patches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07941</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07941</id><created>2015-12-24</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Hansberger</keyname><forenames>Jeff</forenames></author><author><keyname>Waltz</keyname><forenames>Edward</forenames></author><author><keyname>Corpac</keyname><forenames>Peter</forenames></author></authors><title>An Experimental Evaluation of Computational Techniques for Planning and
  Assessment of International Interventions</title><categories>cs.CY</categories><comments>A version of this paper appeared in the International Journal of
  Command and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the experimental methodology developed and employed in a series
of experiments within the Defense Advanced Research Projects Agency (DARPA)
Conflict Modeling, Planning, and Outcomes Exploration (COMPOEX) Program. The
primary purpose of the effort was development of tools and methods for
analysis, planning and predictive assessment of plans for complex operations
where integrated political-military-economic-social-infrastructure and
information (PMESII) considerations play decisive roles. As part of the
program, our team executed several broad-based experiments, involving dozens of
experts from several agencies simultaneously. The methodology evolved from one
experiment to another because of the lessons learned. The paper presents the
motivation, objectives, and structure of this interagency experiment series;
the methods we explored in the experiments; and the results, lessons learned
and recommendations for future efforts of such nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07942</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07942</id><created>2015-12-24</created><authors><author><keyname>Chalupka</keyname><forenames>Krzysztof</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author><author><keyname>Eberhardt</keyname><forenames>Frederick</forenames></author></authors><title>Multi-Level Cause-Effect Systems</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a domain-general account of causation that applies to settings in
which macro-level causal relations between two systems are of interest, but the
relevant causal features are poorly understood and have to be aggregated from
vast arrays of micro-measurements. Our approach generalizes that of Chalupka et
al. (2015) to the setting in which the macro-level effect is not specified. We
formalize the connection between micro- and macro-variables in such situations
and provide a coherent framework describing causal relations at multiple levels
of analysis. We present an algorithm that discovers macro-variable causes and
effects from micro-level measurements obtained from an experiment. We further
show how to design experiments to discover macro-variables from observational
micro-variable data. Finally, we show that under specific conditions, one can
identify multiple levels of causal structure. Throughout the article, we use a
simulated neuroscience multi-unit recording experiment to illustrate the ideas
and the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07943</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07943</id><created>2015-12-24</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Ownby</keyname><forenames>Michael</forenames></author></authors><title>Toward a Research Agenda in Adversarial Reasoning: Computational
  Approaches to Anticipating the Opponent's Intent and Actions</title><categories>cs.AI</categories><comments>A version of this paper was presented at the SPIE Symposium on
  Enabling Technologies for Simulation Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines adversarial reasoning as computational approaches to
inferring and anticipating an enemy's perceptions, intents and actions. It
argues that adversarial reasoning transcends the boundaries of game theory and
must also leverage such disciplines as cognitive modeling, control theory, AI
planning and others. To illustrate the challenges of applying adversarial
reasoning to real-world problems, the paper explores the lessons learned in the
CADET - a battle planning system that focuses on brigade-level ground
operations and involves adversarial reasoning. From this example of current
capabilities, the paper proceeds to describe RAID - a DARPA program that aims
to build capabilities in adversarial reasoning, and how such capabilities would
address practical requirements in Defense and other application areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07947</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07947</id><created>2015-12-24</created><authors><author><keyname>Li</keyname><forenames>Edward</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Haider</keyname><forenames>Masoom A.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain
  Stochastically Fully Connected Conditional Random Fields</title><categories>cs.CV physics.med-ph stat.ME</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology for
the screening and diagnosis of frequently occurring cancers. However image
quality may suffer by long acquisition times for MRIs due to patient motion, as
well as result in great patient discomfort. Reducing MRI acquisition time can
reduce patient discomfort and as a result reduces motion artifacts from the
acquisition process. Compressive sensing strategies, when applied to MRI, have
been demonstrated to be effective at decreasing acquisition times significantly
by sparsely sampling the \emph{k}-space during the acquisition process.
However, such a strategy requires advanced reconstruction algorithms to produce
high quality and reliable images from compressive sensing MRI. This paper
proposes a new reconstruction approach based on cross-domain stochastically
fully connected conditional random fields (CD-SFCRF) for compressive sensing
MRI. The CD-SFCRF introduces constraints in both \emph{k}-space and spatial
domains within a stochastically fully connected graphical model to produce
improved MRI reconstruction. Experimental results using T2-weighted (T2w)
imaging and diffusion-weighted imaging (DWI) of the prostate show strong
performance in preserving fine details and tissue structures in the
reconstructed images when compared to other tested methods even at low sampling
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07950</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07950</id><created>2015-12-24</created><authors><author><keyname>Kang</keyname><forenames>Yu</forenames></author><author><keyname>Zhou</keyname><forenames>Yangfan</forenames></author><author><keyname>Xu</keyname><forenames>Hui</forenames></author><author><keyname>Lyu</keyname><forenames>Michael R.</forenames></author></authors><title>PersisDroid: Android Performance Diagnosis via Anatomizing Asynchronous
  Executions</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android applications (apps) grow dramatically in recent years. Apps are user
interface (UI) centric typically. Rapid UI responsiveness is key consideration
to app developers. However, we still lack a handy tool for profiling app
performance so as to diagnose performance problems. This paper presents
PersisDroid, a tool specifically designed for this task. The key notion of
PersisDroid is that the UI-triggered asynchronous executions also contribute to
the UI performance, and hence its performance should be properly captured to
facilitate performance diagnosis. However, Android allows tremendous ways to
start the asynchronous executions, posing a great challenge to profiling such
execution. This paper finds that they can be grouped into six categories. As a
result, they can be tracked and profiled according to the specifics of each
category with a dynamic instrumentation approach carefully tailored for
Android. PersisDroid can then properly profile the asynchronous executions in
task granularity, which equips it with low-overhead and high compatibility
merits. Most importantly, the profiling data can greatly help the developers in
detecting and locating performance anomalies. We code and open-source release
PersisDroid. The tool is applied in diagnosing 20 open-source apps, and we find
11 of them contain potential performance problems, which shows its
effectiveness in performance diagnosis for Android apps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07951</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07951</id><created>2015-12-24</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Kheradvar</keyname><forenames>A.</forenames></author><author><keyname>Jafarkhani</keyname><forenames>H.</forenames></author></authors><title>A Combined Deep-Learning and Deformable-Model Approach to Fully
  Automatic Segmentation of the Left Ventricle in Cardiac MRI</title><categories>cs.CV</categories><comments>to appear in Medical Image Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation of the left ventricle (LV) from cardiac magnetic resonance
imaging (MRI) datasets is an essential step for calculation of clinical indices
such as ventricular volume and ejection fraction. In this work, we employ deep
learning algorithms combined with deformable models to develop and evaluate a
fully automatic segmentation tool for the LV from short-axis cardiac MRI
datasets. The method employs deep learning algorithms to learn the segmentation
task from the ground true data. Convolutional networks are employed to
automatically detect the LV chamber in MRI dataset. Stacked autoencoders are
utilized to infer the shape of the LV. The inferred shape is incorporated into
deformable models to improve the accuracy and robustness of the segmentation.
We validated our method using 45 cardiac MR datasets taken from the MICCAI 2009
LV segmentation challenge and showed that it outperforms the state-of-the art
methods. Excellent agreement with the ground truth was achieved. Validation
metrics, percentage of good contours, Dice metric, average perpendicular
distance and conformity, were computed as 96.69%, 0.94, 1.81mm and 0.86, versus
those of 79.2%-95.62%, 0.87-0.9, 1.76-2.97mm and 0.67-0.78, obtained by other
methods, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07956</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07956</id><created>2015-12-25</created><updated>2016-02-29</updated><authors><author><keyname>Hoxha</keyname><forenames>Bardh</forenames></author><author><keyname>Dokhanchi</keyname><forenames>Adel</forenames></author><author><keyname>Fainekos</keyname><forenames>Georgios</forenames></author></authors><title>Mining Parametric Temporal Logic Properties in Model Based Design for
  Cyber-Physical Systems</title><categories>cs.LO cs.FL</categories><comments>18 Pages, 15 figures, 2 tables, 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the advantages of adopting a Model Based Development (MBD) process is
that it enables testing and verification at early stages of development.
However, it is often desirable to not only verify/falsify certain formal system
specifications, but also to automatically explore the properties that the
system satisfies. In this work, we present a framework that enables property
exploration for Cyber-Physical Systems. Namely, given a parametric
specification with multiple parameters, our solution can automatically infer
the ranges of parameters for which the property does not hold on the system. In
this paper, we consider parametric specifications in Metric or Signal Temporal
Logic (MTL or STL). Using robust semantics for MTL, the parameter mining
problem can be converted into a Pareto optimization problem for which we can
provide an approximate solution by utilizing stochastic optimization methods.
We include solutions and algorithms for the exploration and visualization of
multi-parametric specifications. The framework is demonstrated on an industrial
size, high-fidelity engine model and examples from the related literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07962</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07962</id><created>2015-12-25</created><updated>2016-02-27</updated><authors><author><keyname>Chen</keyname><forenames>Changyou</forenames></author><author><keyname>Carlson</keyname><forenames>David</forenames></author><author><keyname>Gan</keyname><forenames>Zhe</forenames></author><author><keyname>Li</keyname><forenames>Chunyuan</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Bridging the Gap between Stochastic Gradient MCMC and Stochastic
  Optimization</title><categories>stat.ML cs.LG</categories><comments>Merry Christmas from the Santa (algorithm). AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian
analogs to popular stochastic optimization methods; however, this connection is
not well studied. We explore this relationship by applying simulated annealing
to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two
key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)
adaptive element-wise momentum weights. The zero-temperature limit gives a
novel stochastic optimization method with adaptive element-wise momentum
weights, while conventional optimization methods only have a shared, static
momentum weight. Under certain assumptions, our theoretical analysis suggests
the proposed simulated annealing approach converges close to the global optima.
Experiments on several deep neural network models show state-of-the-art results
compared to related stochastic optimization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07966</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07966</id><created>2015-12-25</created><authors><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Campaigning in Heterogeneous Social Networks: Optimal Control of SI
  Information Epidemics</title><categories>cs.SI cs.SY math.OC physics.soc-ph</categories><comments>14 pages, 10 figures. Published version can be accessed at
  http://dx.doi.org/10.1109/TNET.2014.2361801</comments><journal-ref>IEEE/ACM Transactions on Networking, vol. 24, Issue 1, pp.
  383-396, Feb. 2016</journal-ref><doi>10.1109/TNET.2014.2361801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal control problem of maximizing the spread of an
information epidemic on a social network. Information propagation is modeled as
a Susceptible-Infected (SI) process and the campaign budget is fixed. Direct
recruitment and word-of-mouth incentives are the two strategies to accelerate
information spreading (controls). We allow for multiple controls depending on
the degree of the nodes/individuals. The solution optimally allocates the
scarce resource over the campaign duration and the degree class groups. We
study the impact of the degree distribution of the network on the controls and
present results for Erdos-Renyi and scale free networks. Results show that more
resource is allocated to high degree nodes in the case of scale free networks
but medium degree nodes in the case of Erdos-Renyi networks. We study the
effects of various model parameters on the optimal strategy and quantify the
improvement offered by the optimal strategy over the static and bang-bang
control strategies. The effect of the time varying spreading rate on the
controls is explored as the interest level of the population in the subject of
the campaign may change over time. We show the existence of a solution to the
formulated optimal control problem, which has non-linear isoperimetric
constraints, using novel techniques that is general and can be used in other
similar optimal control problems. This work may be of interest to political,
social awareness, or crowdfunding campaigners and product marketing managers,
and with some modifications may be used for mitigating biological epidemics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07972</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07972</id><created>2015-12-25</created><authors><author><keyname>Yan</keyname><forenames>Lin</forenames></author><author><keyname>Guo</keyname><forenames>Yao</forenames></author><author><keyname>Chen</keyname><forenames>Xiangqun</forenames></author><author><keyname>Mei</keyname><forenames>Hong</forenames></author></authors><title>A Study on Power Side Channels on Mobile Devices</title><categories>cs.CR</categories><comments>Published as a conference paper at Internetware 2015. Please cite
  this paper as: &quot;Lin Yan, Yao Guo, Xiangqun Chen, Hong Mei. A Study on Power
  Side Channels on Mobile Devices. The Seventh Asia-Pacific Symposium on
  Internetware (Internetware 2015), Wuhan, China.&quot; Bibtex Link:
  http://sei.pku.edu.cn/~yaoguo/papers/Yan-Internetware-15.txt</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power side channel is a very important category of side channels, which can
be exploited to steal confidential information from a computing system by
analyzing its power consumption. In this paper, we demonstrate the existence of
various power side channels on popular mobile devices such as smartphones.
Based on unprivileged power consumption traces, we present a list of real-world
attacks that can be initiated to identify running apps, infer sensitive UIs,
guess password lengths, and estimate geo-locations. These attack examples
demonstrate that power consumption traces can be used as a practical side
channel to gain various confidential information of mobile apps running on
smartphones. Based on these power side channels, we discuss possible
exploitations and present a general approach to exploit a power side channel on
an Android smartphone, which demonstrates that power side channels pose
imminent threats to the security and privacy of mobile users. We also discuss
possible countermeasures to mitigate the threats of power side channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07980</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07980</id><created>2015-12-25</created><authors><author><keyname>Salehinejad</keyname><forenames>Hojjat</forenames></author><author><keyname>Rahnamayan</keyname><forenames>Shahryar</forenames></author><author><keyname>Tizhoosh</keyname><forenames>Hamid R.</forenames></author></authors><title>Micro-Differential Evolution: Diversity Enhancement and Comparative
  Study</title><categories>cs.NE</categories><comments>More completed version is submitted for review to Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The differential evolution (DE) algorithm suffers from high computational
time due to slow nature of evaluation. In contrast, micro-DE (MDE) algorithms
employ a very small population size, which can converge faster to a reasonable
solution. However, these algorithms are vulnerable to a premature convergence
as well as to high risk of stagnation. In this paper, MDE algorithm with
vectorized random mutation factor (MDEVM) is proposed, which utilizes the small
size population benefit while empowers the exploration ability of mutation
factor through randomizing it in the decision variable level. The idea is
supported by analyzing mutation factor using Monte-Carlo based simulations. To
facilitate the usage of MDE algorithms with very-small population sizes, new
mutation schemes for population sizes less than four are also proposed.
Furthermore, comprehensive comparative simulations and analysis on performance
of the MDE algorithms over various mutation schemes, population sizes, problem
types (i.e. uni-modal, multi-modal, and composite), problem dimensionalities,
and mutation factor ranges are conducted by considering population diversity
analysis for stagnation and trapping in local optimum situations. The studies
are conducted on 28 benchmark functions provided for the IEEE CEC-2013
competition. Experimental results demonstrate high performance and convergence
speed of the proposed MDEVM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07982</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07982</id><created>2015-12-25</created><authors><author><keyname>Tzima</keyname><forenames>Fani A.</forenames></author><author><keyname>Allamanis</keyname><forenames>Miltiadis</forenames></author><author><keyname>Filotheou</keyname><forenames>Alexandros</forenames></author><author><keyname>Mitkas</keyname><forenames>Pericles A.</forenames></author></authors><title>Inducing Generalized Multi-Label Rules with Learning Classifier Systems</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, multi-label classification has attracted a significant body
of research, motivated by real-life applications, such as text classification
and medical diagnoses. Although sparsely studied in this context, Learning
Classifier Systems are naturally well-suited to multi-label classification
problems, whose search space typically involves multiple highly specific
niches. This is the motivation behind our current work that introduces a
generalized multi-label rule format -- allowing for flexible label-dependency
modeling, with no need for explicit knowledge of which correlations to search
for -- and uses it as a guide for further adapting the general Michigan-style
supervised Learning Classifier System framework. The integration of the
aforementioned rule format and framework adaptations results in a novel
algorithm for multi-label classification whose behavior is studied through a
set of properly defined artificial problems. The proposed algorithm is also
thoroughly evaluated on a set of multi-label datasets and found competitive to
other state-of-the-art multi-label classification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07988</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07988</id><created>2015-12-25</created><updated>2016-02-13</updated><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A weakly universal cellular automaton on the pentagrid with two states</title><categories>cs.DM nlin.CG</categories><comments>38 pages 24 figures. arXiv admin note: text overlap with
  arXiv:1510.09129</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that there is a weakly universal cellular automaton
on the pentagrid with two states. This paper improves in some sense a previous
result with three states. Both results make use of \textit{\`a la Moore}
neighbourhood. However, the result with three states is rotation invariant
while that with two states is not. In both cases, at each step of the
computation, the set of non quiescent states has always infinitely many cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.07989</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.07989</id><created>2015-12-25</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Classification of graphs using contractible transformations. Homotopy
  equivalence of graphs. Basic representatives and complexity of homotopy
  equivalence classes</title><categories>cs.DM math.CO</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main concept of this classification is that graphs belonging to the same
equivalence class have similar topological properties. Graphs are called
homotopy equivalent if one of them can be converted to the other one by
contractible transformations. A basic representative and complexity of a
homotopy equivalence class are defined and investigated. Finding a basic
representative within a homotopy equivalence class simplifies many topological
problems in digital imaging. A method is designed for the classification of
graphs by complexity and basic representatives. This method relies on computer
experiments which show that contractible transformations are a graph analogue
of homotopy in algebraic topology. Diagrams are given of basic representatives
with the complexity N less than 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08008</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08008</id><created>2015-12-25</created><authors><author><keyname>Beykikhoshk</keyname><forenames>Adham</forenames></author><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Discovering topic structures of a temporally evolving document corpus</title><categories>cs.IR cs.LG</categories><comments>2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a novel framework for the discovery of the topical
content of a data corpus, and the tracking of its complex structural changes
across the temporal dimension. In contrast to previous work our model does not
impose a prior on the rate at which documents are added to the corpus nor does
it adopt the Markovian assumption which overly restricts the type of changes
that the model can capture. Our key technical contribution is a framework based
on (i) discretization of time into epochs, (ii) epoch-wise topic discovery
using a hierarchical Dirichlet process-based model, and (iii) a temporal
similarity graph which allows for the modelling of complex topic changes:
emergence and disappearance, evolution, splitting, and merging. The power of
the proposed framework is demonstrated on two medical literature corpora
concerned with the autism spectrum disorder (ASD) and the metabolic syndrome
(MetS) -- both increasingly important research subjects with significant social
and healthcare consequences. In addition to the collected ASD and metabolic
syndrome literature corpora which we made freely available, our contribution
also includes an extensive empirical analysis of the proposed framework. We
describe a detailed and careful examination of the effects that our
algorithms's free parameters have on its output, and discuss the significance
of the findings both in the context of the practical application of our
algorithm as well as in the context of the existing body of work on temporal
topic analysis. Our quantitative analysis is followed by several qualitative
case studies highly relevant to the current research on ASD and MetS, on which
our algorithm is shown to capture well the actual developments in these fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08009</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08009</id><created>2015-12-25</created><authors><author><keyname>V&#xe1;k&#xe1;r</keyname><forenames>Matthijs</forenames></author></authors><title>A Framework for Dependent Types and Effects</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalise Levy's call-by-push-value (CBPV) to dependent type theory, to
gain a better understanding of how to combine dependent types with effects. We
define a dependently typed extension of CBPV, dCBPV-, and show that it has a
very natural small-step operational semantics, which satisfies subject
reduction and (depending on the effects present) determinism and strong
normalization, and an elegant categorical semantics, which - surprisingly - is
no more complicated than the simply typed semantics.
  We have full and faithful translations from a dependently typed version of
Moggi's monadic metalanguage and of a call-by-name (CBN) dependent type theory
into dCBPV- which give rise to the expected operational behaviour. However, it
turns out that dCBPV- does not suffice to encode call-by-value (CBV) dependent
type theory or the strong (dependent) elimination rules for positive
connectives in CBN-dependent type theory.
  To mend this problem, we discuss a second, more expressive system dCBPV+,
which additionally has a principle of Kleisli extension for dependent
functions. We obtain the desired CBV- and CBN-translations of dependent type
theory into dCBPV+. It too has a natural categorical semantics and operational
semantics. However, depending on the effects we consider, we may lose
uniqueness of typing, as the type of a computation may become more specified as
certain effects are executed. This idea can be neatly formalized using a notion
of subtyping.
  We hope that the theoretical framework of this paper on the one hand provides
at least a partial answer to the fundamental type theoretic question of how one
can understand the relationship between computational effects and dependent
types. On the other hand, we hope it can contribute a small-step towards the
ultimate goal of an elegant fully fledged language for certified effectful
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08017</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08017</id><created>2015-12-25</created><authors><author><keyname>Dasgupta</keyname><forenames>Poorna Banerjee</forenames></author></authors><title>An Analytical Evaluation of Matricizing Least-Square-Errors Curve
  Fitting to Support High Performance Computation on Large Datasets</title><categories>cs.DC</categories><comments>3 pages, Published with International Journal of Computer Trends and
  Technology (IJCTT), Volume-30 Number-2, December-2015</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V30(2):113-115, December 2015</journal-ref><doi>10.14445/22312803/IJCTT-V30P120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The procedure of Least Square-Errors curve fitting is extensively used in
many computer applications for fitting a polynomial curve of a given degree to
approximate a set of data. Although various methodologies exist to carry out
curve fitting on data, most of them have shortcomings with respect to
efficiency especially where huge datasets are involved. This paper proposes and
analyzes a matricized approach to the Least Square-Errors curve fitting with
the primary objective of parallelizing the whole algorithm so that high
performance efficiency can be achieved when algorithmic execution takes place
on colossal datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08019</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08019</id><created>2015-12-25</created><authors><author><keyname>Afsar</keyname><forenames>Mehdi</forenames></author></authors><title>Energy-Efficient Coalition Formation in Sensor Networks: a
  Game-Theoretic Approach</title><categories>cs.NI</categories><comments>Submitted to CCECE'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important challenge in Wireless Sensor Networks (WSNs) is the energy
constraint. Numerous solutions have been proposed to alleviate this problem,
the most efficient of which is to cluster the sensor nodes. Although clustering
in the realm of WSNs has widely been explored by researchers, a few effective
mechanisms in grouping the nodes, including coalitional games, need more
attention and research. This motivated us to employ cooperative games and to
propose a Coalitional Game-Theoretic Clustering (CGTC) algorithm for WSNs.
Basically two kinds of coalitions are formed regarding the location of sensor
nodes, where local parameters play an important role in forming coalitions.
Moreover, the Shapley value is adopted as the solution concept. The result of
simulation confirms the effectiveness of CGTC in terms of energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08030</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08030</id><created>2015-12-25</created><authors><author><keyname>Eryilmaz</keyname><forenames>Sukru Burc</forenames></author><author><keyname>Kuzum</keyname><forenames>Duygu</forenames></author><author><keyname>Yu</keyname><forenames>Shimeng</forenames></author><author><keyname>Wong</keyname><forenames>H. -S. Philip</forenames></author></authors><title>Device and System Level Design Considerations for
  Analog-Non-Volatile-Memory Based Neuromorphic Architectures</title><categories>cs.NE cs.AI</categories><comments>4 pages, In Electron Devices Meeting (IEDM), 2015 IEEE International
  (pp. 4.1). IEEE</comments><journal-ref>Electron Devices Meeting (IEDM), IEEE International
  ,pp.4.1.1-4.1.4, 2015</journal-ref><doi>10.1109/IEDM.2015.7409622</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an overview of recent progress in the brain inspired
computing field with a focus on implementation using emerging memories as
electronic synapses. Design considerations and challenges such as requirements
and design targets on multilevel states, device variability, programming
energy, array-level connectivity, fan-in/fanout, wire energy, and IR drop are
presented. Wires are increasingly important in design decisions, especially for
large systems, and cycle-to-cycle variations have large impact on learning
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08041</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08041</id><created>2015-12-25</created><authors><author><keyname>Sun</keyname><forenames>Yuan</forenames></author><author><keyname>Ye</keyname><forenames>Shiwei</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Kameda</keyname><forenames>Tsunehiko</forenames></author></authors><title>Improved Algorithms for Exact and Approximate Boolean Matrix
  Decomposition</title><categories>cs.DM</categories><comments>DSAA2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An arbitrary $m\times n$ Boolean matrix $M$ can be decomposed {\em exactly}
as $M =U\circ V$, where $U$ (resp. $V$) is an $m\times k$ (resp. $k\times n$)
Boolean matrix and $\circ$ denotes the Boolean matrix multiplication operator.
We first prove an exact formula for the Boolean matrix $J$ such that $M =M\circ
J^T$ holds, where $J$ is maximal in the sense that if any 0 element in $J$ is
changed to a 1 then this equality no longer holds. Since minimizing $k$ is
NP-hard, we propose two heuristic algorithms for finding suboptimal but good
decomposition. We measure the performance (in minimizing $k$) of our algorithms
on several real datasets in comparison with other representative heuristic
algorithms for Boolean matrix decomposition (BMD). The results on some popular
benchmark datasets demonstrate that one of our proposed algorithms performs as
well or better on most of them. Our algorithms have a number of other
advantages: They are based on exact mathematical formula, which can be
interpreted intuitively. They can be used for approximation as well with
competitive &quot;coverage.&quot; Last but not least, they also run very fast. Due to
interpretability issues in data mining, we impose the condition, called the
&quot;column use condition,&quot; that the columns of the factor matrix $U$ must form a
subset of the columns of $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08047</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08047</id><created>2015-12-25</created><authors><author><keyname>Al-Kadi</keyname><forenames>Omar Sultan</forenames></author></authors><title>Assessment of texture measures susceptibility to noise in conventional
  and contrast enhanced computed tomography lung tumour images</title><categories>cs.CV</categories><comments>10 pages, 9 figures</comments><journal-ref>Computerized Medical Imaging and Graphics, vol.34, pp.494-503,
  2010</journal-ref><doi>10.1016/j.compmedimag.2009.12.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noise is one of the major problems that hinder an effective texture analysis
of disease in medical images, which may cause variability in the reported
diagnosis. In this paper seven texture measurement methods (two wavelet, two
model and three statistical based) were applied to investigate their
susceptibility to subtle noise caused by acquisition and reconstruction
deficiencies in computed tomography (CT) images. Features of lung tumours were
extracted from two different conventional and contrast enhanced CT image
data-sets under filtered and noisy conditions. When measuring the noise in the
background open-air region of the analysed CT images, noise of Gaussian and
Rayleigh distributions with varying mean and variance was encountered, and
Fisher distance was used to differentiate between an original extracted lung
tumour region of interest (ROI) with the filtered and noisy reconstructed
versions. It was determined that the wavelet packet (WP) and fractal dimension
measures were the least affected, while the Gaussian Markov random field,
run-length and co-occurrence matrices were the most affected by noise.
Depending on the selected ROI size, it was concluded that texture measures with
fewer extracted features can decrease susceptibility to noise, with the WP and
the Gabor filter having a stable performance in both filtered and noisy CT
versions and for both data-sets. Knowing how robust each texture measure under
noise presence is can assist physicians using an automated lung texture
classification system in choosing the appropriate feature extraction algorithm
for a more accurate diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08048</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08048</id><created>2015-12-25</created><authors><author><keyname>Narayanan</keyname><forenames>Sandeep Nair</forenames></author><author><keyname>Mittal</keyname><forenames>Sudip</forenames></author><author><keyname>Joshi</keyname><forenames>Anupam</forenames></author></authors><title>Using Data Analytics to Detect Anomalous States in Vehicles</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicles are becoming more and more connected, this opens up a larger attack
surface which not only affects the passengers inside vehicles, but also people
around them. These vulnerabilities exist because modern systems are built on
the comparatively less secure and old CAN bus framework which lacks even basic
authentication. Since a new protocol can only help future vehicles and not
older vehicles, our approach tries to solve the issue as a data analytics
problem and use machine learning techniques to secure cars. We develop a Hidden
Markov Model to detect anomalous states from real data collected from vehicles.
Using this model, while a vehicle is in operation, we are able to detect and
issue alerts. Our model could be integrated as a plug-n-play device in all new
and old cars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08049</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08049</id><created>2015-12-25</created><authors><author><keyname>Al-Kadi</keyname><forenames>Omar S.</forenames></author></authors><title>Texture measures combination for improved meningioma classification of
  histopathological images</title><categories>cs.CV</categories><journal-ref>Pattern Recognition, vol.43, pp.2043-2053, 2010</journal-ref><doi>10.1016/j.patcog.2010.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing an improved technique which can assist pathologists in correctly
classifying meningioma tumours with a significant accuracy is our main
objective. The proposed technique, which is based on optimum texture measure
combination, inspects the separability of the RGB colour channels and selects
the channel which best segments the cell nuclei of the histopathological
images. The morphological gradient was applied to extract the region of
interest for each subtype and for elimination of possible noise (e.g. cracks)
which might occur during biopsy preparation. Meningioma texture features are
extracted by four different texture measures (two model-based and two
statistical-based) and then corresponding features are fused together in
different combinations after excluding highly correlated features, and a
Bayesian classifier was used for meningioma subtype discrimination. The
combined Gaussian Markov random field and run-length matrix texture measures
outperformed all other combinations in terms of quantitatively characterising
the meningioma tissue, achieving an overall classification accuracy of 92.50%,
improving from 83.75% which is the best accuracy achieved if the texture
measures are used individually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08050</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08050</id><created>2015-12-25</created><updated>2015-12-30</updated><authors><author><keyname>Doku-Amponsah</keyname><forenames>Kwabena</forenames></author></authors><title>Large deviation, Basic Information Theory for Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><msc-class>94A15, 94A24, 60F10, 05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we prove Shannon-MacMillan-Breiman Theorem for Wireless
Sensor Networks modelled as coloured geometric random graphs. For large $n,$ we
show that a Wireless Sensor Network consisting of $n$ sensors in $[0,1]^d$
connected by an average number of links of order $n\log n $ can be coded by
about $[n(\log n )^2\pi^{d/2}/(d/2)!]\,\mathcal{H}$ bits, where $\mathcal{H}$
is an explicitly defined entropy. In the process, we derive a joint large
deviation principle (LDP) for the \emph{empirical sensor measure} and \emph{the
empirical link measure} of coloured random geometric graph models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08051</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08051</id><created>2015-12-25</created><authors><author><keyname>Al-Kadi</keyname><forenames>Omar S.</forenames></author></authors><title>A Multiresolution Clinical Decision Support System Based on Fractal
  Model Design for Classification of Histological Brain Tumours</title><categories>cs.CV</categories><journal-ref>Computerized Medical Imaging and Graphics, vol. 41, pp. 67-79,
  2015</journal-ref><doi>10.1016/j.compmedimag.2014.05.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tissue texture is known to exhibit a heterogeneous or non-stationary nature,
therefore using a single resolution approach for optimum classification might
not suffice. A clinical decision support system that exploits the subband
textural fractal characteristics for best bases selection of meningioma brain
histopathological image classification is proposed. Each subband is analysed
using its fractal dimension instead of energy, which has the advantage of being
less sensitive to image intensity and abrupt changes in tissue texture. The
most significant subband that best identifies texture discontinuities will be
chosen for further decomposition, and its fractal characteristics would
represent the optimal feature vector for classification. The performance was
tested using the support vector machine (SVM), Bayesian and k-nearest neighbour
(kNN) classifiers and a leave-one-patient-out method was employed for
validation. Our method outperformed the classical energy based selection
approaches, achieving for SVM, Bayesian and kNN classifiers an overall
classification accuracy of 94.12%, 92.50% and 79.70%, as compared to 86.31%,
83.19% and 51.63% for the co-occurrence matrix, and 76.01%, 73.50% and 50.69%
for the energy texture signatures, respectively. These results indicate the
potential usefulness as a decision support system that could complement
radiologists diagnostic capability to discriminate higher order statistical
textural information, for which it would be otherwise difficult via ordinary
human vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08055</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08055</id><created>2015-12-25</created><authors><author><keyname>Censi</keyname><forenames>Andrea</forenames></author></authors><title>A Mathematical Theory of Co-Design</title><categories>cs.LO cs.RO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a mathematical theory of &quot;co-design&quot;, in which the
objects of investigation are &quot;design problems&quot;, defined as tuples of
&quot;functionality space&quot;, &quot;implementation space&quot;, and &quot;resources space&quot;, together
with a feasibility relation. &quot;Monotone&quot; design problems are those for which
functionality and resources are partially ordered and related by an
order-preserving map. This condition is essentially the core of engineering:
increasing the functionality does not decrease the resources needed. Specific
examples are given for the domain of Robotics. Everything generalizes to
everything else in engineering.
  Monotone Co-Design Problems (MCDPs) are defined as the composition of
monotone design problems by three operations, equivalent to the concepts of
series, parallel, and feedback. Monotonicity is preserved by these operations.
Furthermore, the invariance group for these properties are all order
isomorphisms, thus this is a completely intrinsic theory.
  As a class of optimization problems, MCDPs are multi-objective, nonconvex,
nondifferentiable, noncontinuous, and not even defined on continuous spaces.
Yet there exists a complete solution: if there exists a procedure to solve the
primitive design problems, then there exists a systematic procedure to solve
the larger MCDP. The solution of an MCDP can be cast as the problem of finding
the least fixed point of a certain map on the set of resources antichains, a
concept similar to Pareto fronts. The solution can be obtained by the iterative
application of a map. In particular, no differential operators are needed. The
iteration always converges: if it converges to finite values, that is
guaranteed to be the set of minimal resources. If it converges to infinity (in
a sense to be specified), then the sequence is a certificate of infeasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08061</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08061</id><created>2015-12-25</created><authors><author><keyname>Nasim</keyname><forenames>Mehwish</forenames></author><author><keyname>Rextin</keyname><forenames>Aimal</forenames></author><author><keyname>Khan</keyname><forenames>Numair</forenames></author><author><keyname>Malik</keyname><forenames>Muhammad Muddassir</forenames></author></authors><title>On Temporal Regularity in Social Interactions: Predicting Mobile Phone
  Calls</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we predict outgoing mobile phone calls using a machine learning
approach. We analyze to which extent the activity of mobile phone users is
predictable. The premise is that mobile phone users exhibit temporal regularity
in their interactions with majority of their contacts. In the sociological
context, most social interactions have fairly reliable temporal regularity. If
we quantify the extension of this behavior to interactions on mobile phones we
expect that caller-callee interaction is not merely a result of randomness,
rather it exhibits a temporal pattern. To this end, we tested our approach on
an anonymized mobile phone usage dataset collected specifically for analyzing
temporal patterns in mobile phone communication. The data consists of 783 users
and more than 12,000 caller-callee pairs. The results show that users' historic
calling patterns can predict future calls with reasonable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08062</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08062</id><created>2015-12-25</created><authors><author><keyname>Zeng</keyname><forenames>William</forenames></author></authors><title>The Abstract Structure of Quantum Algorithms</title><categories>quant-ph cs.LO</categories><comments>174 pages, many figures. University of Oxford doctoral thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum information brings together theories of physics and computer science.
This synthesis challenges the basic intuitions of both fields. In this thesis,
we show that adopting a unified and general language for process theories
advances foundations and practical applications of quantum information. Our
first set of results analyze quantum algorithms with a process theoretic
structure. We contribute new constructions of the Fourier transform and
Pontryagin duality in dagger symmetric monoidal categories. We then use this
setting to study generalized unitary oracles and give a new quantum blackbox
algorithm for the identification of group homomorphisms, solving the GROUPHOMID
problem. In the remaining section, we construct a novel model of quantum
blackbox algorithms in non-deterministic classical computation. Our second set
of results concerns quantum foundations. We complete work begun by Coecke et
al., definitively connecting the Mermin non-locality of a process theory with a
simple algebraic condition on that theory's phase groups. This result allows us
to offer new experimental tests for Mermin non-locality and new protocols for
quantum secret sharing. In our final chapter, we exploit the shared process
theoretic structure of quantum information and distributional compositional
linguistics. We propose a quantum algorithm adapted from Weibe et al. to
classify sentences by meaning. The clarity of the process theoretic setting
allows us to recover a speedup that is lost in the naive application of the
algorithm. The main mathematical tools used in this thesis are group theory
(esp. Fourier theory on finite groups), monoidal category theory, and
categorical algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08064</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08064</id><created>2015-12-25</created><authors><author><keyname>Hanneke</keyname><forenames>Steve</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author><author><keyname>Yang</keyname><forenames>Liu</forenames></author></authors><title>Statistical Learning under Nonstationary Mixing Processes</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a special case of the problem of statistical learning without the
i.i.d. assumption. Specifically, we suppose a learning method is presented with
a sequence of data points, and required to make a prediction (e.g., a
classification) for each one, and can then observe the loss incurred by this
prediction. We go beyond traditional analyses, which have focused on stationary
mixing processes or nonstationary product processes, by combining these two
relaxations to allow nonstationary mixing processes. We are particularly
interested in the case of $\beta$-mixing processes, with the sum of changes in
marginal distributions growing sublinearly in the number of samples. Under
these conditions, we propose a learning method, and establish that for bounded
VC subgraph classes, the cumulative excess risk grows sublinearly in the number
of predictions, at a quantified rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08065</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08065</id><created>2015-12-25</created><authors><author><keyname>Jin</keyname><forenames>Ming</forenames></author><author><keyname>Spanos</keyname><forenames>Costas</forenames></author></authors><title>Inverse Reinforcement Learning via Deep Gaussian Process</title><categories>cs.LG cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The report proposes a new approach for inverse reinforcement learning based
on deep Gaussian process (GP), which is capable of learning complicated reward
structures with few demonstrations. The model stacks multiple latent GP layers
to learn abstract representations of the state feature space, which is linked
to the demonstrations through the Maximum Entropy learning framework. As
analytic derivation of the model evidence is prohibitive due to the
nonlinearity of latent variables, variational inference is employed for
approximate inference, based on a special choice of variational distributions.
This guards the model from over training, achieving the Automatic Occam's
Razor. Experiments on the benchmark test, i.e., object world, as well as a new
setup, i.e., binary world, are performed, where the proposed method outperforms
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08066</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08066</id><created>2015-12-25</created><authors><author><keyname>Jang</keyname><forenames>Chung-Hyok</forenames><affiliation>Foreign Language Faculty, Kim Il Sung University</affiliation></author><author><keyname>Kim</keyname><forenames>Kwang-Hyok</forenames><affiliation>Computer Science College, Kim Il Sung University</affiliation></author></authors><title>The Improvement of Negative Sentences Translation in English-to-Korean
  Machine Translation</title><categories>cs.CL</categories><comments>9 pages, 1 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the algorithm for translating English negative sentences
into Korean in English-Korean Machine Translation (EKMT). The proposed
algorithm is based on the comparative study of English and Korean negative
sentences. The earlier translation software cannot translate English negative
sentences into accurate Korean equivalents. We established a new algorithm for
the negative sentence translation and evaluated it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08070</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08070</id><created>2015-12-25</created><updated>2016-02-01</updated><authors><author><keyname>Boyd</keyname><forenames>Sylvia</forenames></author><author><keyname>Legault</keyname><forenames>Philippe</forenames></author></authors><title>Toward a 6/5 Bound for the Minimum Cost 2-Edge Connected Spanning
  Subgraph Problem</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a complete graph $K_{n}=(V, E)$ with non-negative edge costs $c\in
{\mathbb R}^{E}$, the problem $2EC$ is that of finding a 2-edge connected
spanning multi-subgraph of $K_{n}$ of minimum cost. The integrality gap
$\alpha\text{2EC}$ of the linear programming relaxation
$\text{2EC}^{\text{LP}}$ for $2EC$ has been conjectured to be $\frac{6}{5}$,
although currently we only know that
$\frac{6}{5}\leq\alpha\text{2EC}\leq\frac{3}{2}$. In this paper, we explore the
idea of using the structure of solutions for $\text{2EC}^{\text{LP}}$ and the
concept of convex combination to obtain improved bounds for $\alpha\text{2EC}$.
We focus our efforts on a family $J$ of half-integer solutions that appear to
give the largest integrality gap for $\text{2EC}^{\text{LP}}$. We successfully
show that the conjecture $\alpha\text{2EC} = \frac{6}{5}$ is true for any cost
functions optimized by some $x^{*}\in J$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08075</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08075</id><created>2015-12-26</created><authors><author><keyname>Park</keyname><forenames>Taejin</forenames></author><author><keyname>Lee</keyname><forenames>Taejin</forenames></author></authors><title>Multichannel audio signal source separation based on an Interchannel
  Loudness Vector Sum</title><categories>cs.SD</categories><comments>5 pages, 4 figures and 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, a Blind Source Separation (BSS) algorithm for multichannel
audio contents is proposed. Unlike common BSS algorithms targeting stereo audio
contents or microphone array signals, our technique is targeted at multichannel
audio such as 5.1 and 7.1ch audio. Since most multichannel audio object sources
are panned using the Inter-channel Loudness Difference (ILD), we employ the
ILVS (Inter-channel Loudness Vector Sum) concept to cluster common signals
(such as background music) from each channel. After separating the common
signals from each channel, we employ an Expectation Maximization (EM) algorithm
with a von-Mises distribution to successfully classify the clustering of sound
source objects and separate the audio signals from the original mixture. Our
proposed method can therefore separate common audio signals and object source
signals from multiple channels with reasonable quality. Our multichannel audio
content separation technique can be applied to an upmix system or a cinema
audio system requiring multichannel audio source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08083</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08083</id><created>2015-12-26</created><authors><author><keyname>Abbas</keyname><forenames>Houssam</forenames></author><author><keyname>Jang</keyname><forenames>Kuk Jin</forenames></author><author><keyname>Jiang</keyname><forenames>Zhihao</forenames></author><author><keyname>Mangharam</keyname><forenames>Rahul</forenames></author></authors><title>Model Checking Implantable Cardioverter Defibrillators</title><categories>cs.SY</categories><comments>Hybrid Systems: Computation and Control 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ventricular Fibrillation is a disorganized electrical excitation of the heart
that results in inadequate blood flow to the body. It usually ends in death
within seconds. The most common way to treat the symptoms of fibrillation is to
implant a medical device, known as an Implantable Cardioverter Defibrillator
(ICD), in the patient's body. Model-based verification can supply rigorous
proofs of safety and efficacy. In this paper, we build a hybrid system model of
the human heart+ICD closed loop, and show it to be a STORMED system, a class of
o-minimal hybrid systems that admit finite bisimulations. In general, it may
not be possible to compute the bisimulation. We show that approximate
reachability can yield a finite simulation for STORMED systems, which improves
on the existing verification procedure. In the process, we show that certain
compositions respect the STORMED property. Thus it is possible to model check
important formal properties of ICDs in a closed loop with the heart, such as
delayed therapy, missed therapy, or inappropriately administered therapy. The
results of this paper are theoretical and motivate the creation of concrete
model checking procedures for STORMED systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08084</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08084</id><created>2015-12-26</created><authors><author><keyname>Park</keyname><forenames>TaeJin</forenames></author><author><keyname>Kang</keyname><forenames>Kyeong Ok</forenames></author></authors><title>Time delay estimator for predetermined repeated signal robust to
  narrowband interference</title><categories>cs.IT math.IT</categories><comments>5 pages and 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, time delay estimation techniques robust to narrowband
interference (NBI) are proposed. Owing to the deluge of wireless signal
interference these days, narrowband interference is a common problem for
communication and positioning systems. To mitigate the effect of this narrow
band interference, we propose a robust time delay estimator for a predetermined
repeated synchronization signal in an NBI environment. We exploit an ensemble
of average and sample covariance matrices to estimate the noise profile. In
addition, to increase the detection probability, we suppress the variance of
likelihood value by employing a von-Mises distribution in the time-delay
estimator. Our proposed time delay estimator shows a better performance in an
NBI environment compared to a typical time delay estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08086</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08086</id><created>2015-12-26</created><authors><author><keyname>Huang</keyname><forenames>Shaoli</forenames></author><author><keyname>Xu</keyname><forenames>Zhe</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Zhang</keyname><forenames>Ya</forenames></author></authors><title>Part-Stacked CNN for Fine-Grained Visual Categorization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of fine-grained visual categorization, the ability to
interpret models as human-understandable visual manuals is sometimes as
important as achieving high classification accuracy. In this paper, we propose
a novel Part-Stacked CNN architecture that explicitly explains the fine-grained
recognition process by modeling subtle differences from object parts. Based on
manually-labeled strong part annotations, the proposed architecture consists of
a fully convolutional network to locate multiple object parts and a two-stream
classification network that en- codes object-level and part-level cues
simultaneously. By adopting a set of sharing strategies between the computation
of multiple object parts, the proposed architecture is very efficient running
at 20 frames/sec during inference. Experimental results on the CUB-200-2011
dataset reveal the effectiveness of the proposed architecture, from both the
perspective of classification accuracy and model interpretability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08103</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08103</id><created>2015-12-26</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Gu</keyname><forenames>Yun</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Chen</keyname><forenames>Xiaogang</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author></authors><title>Data Driven Robust Image Guided Depth Map Restoration</title><categories>cs.CV</categories><comments>9 pages, 9 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth maps captured by modern depth cameras such as Kinect and Time-of-Flight
(ToF) are usually contaminated by missing data, noises and suffer from being of
low resolution. In this paper, we present a robust method for high-quality
restoration of a degraded depth map with the guidance of the corresponding
color image. We solve the problem in an energy optimization framework that
consists of a novel robust data term and smoothness term. To accommodate not
only the noise but also the inconsistency between depth discontinuities and the
color edges, we model both the data term and smoothness term with a robust
exponential error norm function. We propose to use Iteratively Re-weighted
Least Squares (IRLS) methods for efficiently solving the resulting highly
non-convex optimization problem. More importantly, we further develop a
data-driven adaptive parameter selection scheme to properly determine the
parameter in the model. We show that the proposed approach can preserve fine
details and sharp depth discontinuities even for a large upsampling factor
($8\times$ for example). Experimental results on both simulated and real
datasets demonstrate that the proposed method outperforms recent
state-of-the-art methods in coping with the heavy noise, preserving sharp depth
discontinuities and suppressing the texture copy artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08106</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08106</id><created>2015-12-26</created><updated>2016-01-28</updated><authors><author><keyname>Bouyer</keyname><forenames>Patricia</forenames></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames></author><author><keyname>Randour</keyname><forenames>Mickael</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Laursen</keyname><forenames>Simon</forenames></author></authors><title>Average-energy games (full version)</title><categories>cs.LO cs.FL cs.GT</categories><comments>Full version of GandALF 2015 paper (arXiv:1509.07205)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-player quantitative zero-sum games provide a natural framework to
synthesize controllers with performance guarantees for reactive systems within
an uncontrollable environment. Classical settings include mean-payoff games,
where the objective is to optimize the long-run average gain per action, and
energy games, where the system has to avoid running out of energy.
  We study average-energy games, where the goal is to optimize the long-run
average of the accumulated energy. We show that this objective arises naturally
in several applications, and that it yields interesting connections with
previous concepts in the literature. We prove that deciding the winner in such
games is in NP $\cap$ coNP and at least as hard as solving mean-payoff games,
and we establish that memoryless strategies suffice to win. We also consider
the case where the system has to minimize the average-energy while maintaining
the accumulated energy within predefined bounds at all times: this corresponds
to operating with a finite-capacity storage for energy. We give results for
one-player and two-player games, and establish complexity bounds and memory
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08112</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08112</id><created>2015-12-26</created><authors><author><keyname>Deng</keyname><forenames>Shi-Wen</forenames></author><author><keyname>Han</keyname><forenames>Ji-Qing</forenames></author></authors><title>Ramanujan subspace pursuit for signal periodic decomposition</title><categories>cs.IT math.IT</categories><comments>13 pages, 8 figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The period estimation and periodic decomposition of a signal are the
long-standing problems in the field of signal processing and biomolecular
sequence analysis. To address such problems, we introduce the Ramanujan
subspace pursuit (RSP) based on the Ramanujan subspace. As a greedy iterative
algorithm, the RSP can uniquely decompose any signal into a sum of exactly
periodic components, by selecting and removing the most dominant periodic
component from the residual signal in each iteration. In the RSP, a novel
periodicity metric is derived based on the energy of the exactly periodic
component obtained by orthogonally projecting the residual signal into the
Ramanujan subspace, and is then used to select the most dominant periodic
component in each iteration. To reduce the computational cost of the RSP, we
also propose the fast RSP (FRSP) based on the relationship between the periodic
subspace and the Ramanujan subspace, and based on the maximum likelihood
estimation of the energy of the periodic component in the periodic subspace.
The fast RSP has a lower computational cost and can decompose a signal of
length $N$ into the sum of $K$ exactly periodic components in $ \mathcal{O}(K
N\log N)$. In addition, our results show that the RSP outperforms the current
algorithms for period estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08120</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08120</id><created>2015-12-26</created><updated>2016-01-16</updated><authors><author><keyname>Shang</keyname><forenames>Fanhua</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author></authors><title>Regularized Orthogonal Tensor Decompositions for Multi-Relational
  Learning</title><categories>cs.LG cs.AI</categories><comments>18 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-relational learning has received lots of attention from researchers in
various research communities. Most existing methods either suffer from
superlinear per-iteration cost, or are sensitive to the given ranks. To address
both issues, we propose a scalable core tensor trace norm Regularized
Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor
analytics, which can be generalized as a graph Laplacian regularized version by
using auxiliary information or a sparse higher-order orthogonal iteration
(SHOOI) version. We first induce the equivalence relation of the Schatten
p-norm (0&lt;p&lt;\infty) of a low multi-linear rank tensor and its core tensor. Then
we achieve a much smaller matrix trace norm minimization problem. Finally, we
develop two efficient augmented Lagrange multiplier algorithms to solve our
problems with convergence guarantees. Extensive experiments using both real and
synthetic datasets, even though with only a few observations, verified both the
efficiency and effectiveness of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08133</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08133</id><created>2015-12-26</created><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author></authors><title>The Utility of Abstaining in Binary Classification</title><categories>cs.LG</categories><comments>Short survey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the problem of binary classification in machine learning, with a
twist - the classifier is allowed to abstain on any datum, professing ignorance
about the true class label without committing to any prediction. This is
directly motivated by applications like medical diagnosis and fraud risk
assessment, in which incorrect predictions have potentially calamitous
consequences. We focus on a recent spate of theoretically driven work in this
area that characterizes how allowing abstentions can lead to fewer errors in
very general settings. Two areas are highlighted: the surprising possibility of
zero-error learning, and the fundamental tradeoff between predicting
sufficiently often and avoiding incorrect predictions. We review efficient
algorithms with provable guarantees for each of these areas. We also discuss
connections to other scenarios, notably active learning, as they suggest
promising directions of further inquiry in this emerging field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08144</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08144</id><created>2015-12-26</created><authors><author><keyname>Mart&#xed;nez-Pe&#xf1;as</keyname><forenames>Umberto</forenames></author><author><keyname>Pellikaan</keyname><forenames>Ruud</forenames></author></authors><title>Rank error-correcting pairs</title><categories>cs.IT math.IT</categories><msc-class>15B33, 94B35, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error-correcting pairs were introduced independently by Pellikaan and
K\&quot;otter as a general method of decoding linear codes with respect to the
Hamming metric using coordinatewise products of vectors, and are used for many
well-known families of codes. In this paper, we define new types of vector
products, extending the coordinatewise product, some of which preserve symbolic
products of linearized polynomials after evaluation and some of which coincide
with usual products of matrices. Then we define rank error-correcting pairs for
codes that are linear over the extension field and for codes that are linear
over the base field, and relate both types. Bounds on the minimum rank distance
of codes and MRD conditions are given. Finally we show that some well-known
families of rank-metric codes admit rank error-correcting pairs, and show that
the given algorithm generalizes the classical algorithm using error-correcting
pairs for the Hamming metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08147</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08147</id><created>2015-12-26</created><authors><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Sublinear-Time Maintenance of Breadth-First Spanning Trees in Partially
  Dynamic Networks</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in the International
  Colloquium on Automata, Languages and Programming (ICALP) 2013</comments><doi>10.1007/978-3-642-39212-2_53</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of maintaining a breadth-first spanning tree (BFS tree)
in partially dynamic distributed networks modeling a sequence of either
failures or additions of communication links (but not both). We present
deterministic $(1+\epsilon)$-approximation algorithms whose amortized time
(over some number of link changes) is sublinear in $D$, the maximum diameter of
the network.
  Our technique also leads to a deterministic $(1+\epsilon)$-approximate
incremental algorithm for single-source shortest paths (SSSP) in the sequential
(usual RAM) model. Prior to our work, the state of the art was the classic
exact algorithm of Even and Shiloach [JACM 1981] that is optimal under some
assumptions [Roditty and Zwick ESA 2004, Henzinger et al. STOC 2015]. Our
result is the first to show that, in the incremental setting, this bound can be
beaten in certain cases if some approximation is allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08148</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08148</id><created>2015-12-26</created><authors><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Krinninger</keyname><forenames>Sebastian</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Decremental Single-Source Shortest Paths on Undirected Graphs in
  Near-Linear Total Update Time</title><categories>cs.DS</categories><comments>A preliminary version of this paper was presented at the 55th IEEE
  Symposium on Foundations of Computer Science (FOCS 2014)</comments><doi>10.1109/FOCS.2014.24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the decremental single-source shortest paths (SSSP) problem we want to
maintain the distances between a given source node $s$ and every other node in
an $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static
counterpart can be easily solved in near-linear time, this decremental problem
is much more challenging even in the undirected unweighted case. In this case,
the classic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been
the fastest known algorithm for three decades. At the cost of a
$(1+\epsilon)$-approximation factor, the running time was recently improved to
$O(n^{2+o(1)})$ by Bernstein and Roditty [SODA 2011]. In this paper, we bring
the running time down to near-linear: We give a $(1+\epsilon)$-approximation
algorithm with $O(m^{1+o(1)})$ total update time, thus obtaining near-linear
time. Moreover, we obtain $O(m^{1+o(1)}\log W)$ time for the weighted case,
where the edge weights are integers from $1$ to $W$.
  In contrast to the previous results which rely on maintaining a sparse
emulator, our algorithm relies on maintaining a so-called sparse $(h,
\epsilon)$-hop set introduced by Cohen [JACM 2000] in the PRAM literature. An
$(h, \epsilon)$-hop set of a graph $G=(V, E)$ is a set $F$ of weighted edges
such that the distance between any pair of nodes in $G$ can be
$(1+\epsilon)$-approximated by their $h$-hop distance (given by a path
containing at most $h$ edges) on $G'=(V, E\cup F)$. Our algorithm can maintain
an $(n^{o(1)}, \epsilon)$-hop set of near-linear size in near-linear time under
edge deletions. It is the first of its kind to the best of our knowledge. To
maintain approximate distances using this hop set, we extend the monotone
Even-Shiloach tree of Henzinger et al. [FOCS 2013] and combine it with the
bounded-hop SSSP technique of Bernstein [FOCS 2009, STOC 2013] and M\k{a}dry
[STOC 2010].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08150</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08150</id><created>2015-12-26</created><authors><author><keyname>Orakzai</keyname><forenames>Faisal</forenames></author><author><keyname>Devogele</keyname><forenames>Thomas</forenames></author><author><keyname>Calders</keyname><forenames>Toon</forenames></author></authors><title>Towards Distributed Convoy Pattern Mining</title><categories>cs.DC</categories><comments>SIGSPATIAL'15 November 03-06, 2015, Bellevue, WA, USA</comments><doi>10.1145/2820783.2820840</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining movement data to reveal interesting behavioral patterns has gained
attention in recent years. One such pattern is the convoy pattern which
consists of at least m objects moving together for at least k consecutive time
instants where m and k are user-defined parameters. Existing algorithms for
detecting convoy patterns, however do not scale to real-life dataset sizes.
Therefore a distributed algorithm for convoy mining is inevitable. In this
paper, we discuss the problem of convoy mining and analyze different data
partitioning strategies to pave the way for a generic distributed convoy
pattern mining algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08168</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08168</id><created>2015-12-26</created><updated>2016-02-10</updated><authors><author><keyname>Inaba</keyname><forenames>Kazuhiro</forenames></author></authors><title>Quick Brown Fox in Formal Languages</title><categories>cs.FL</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Given a finite alphabet $\Sigma$ and a deterministic finite automaton on
$\Sigma$, the problem of determining whether the language recognized by the
automaton contains any pangram is \NP-complete. Various other language classes
and problems around pangrams are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08169</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08169</id><created>2015-12-26</created><authors><author><keyname>Radecki</keyname><forenames>Peter</forenames></author><author><keyname>Hencey</keyname><forenames>Brandon</forenames></author></authors><title>Self-Excitation: An Enabler for Online Thermal Estimation and Model
  Predictive Control of Buildings</title><categories>cs.SY cs.LG</categories><comments>11 pages, 10 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a method to improve buildings' thermal predictive
control performance via online identification and excitation (active learning
process) that minimally disrupts normal operations. In previous studies we have
demonstrated scalable methods to acquire multi-zone thermal models of passive
buildings using a gray-box approach that leverages building topology and
measurement data. Here we extend the method to multi-zone actively controlled
buildings and examine how to improve the thermal model estimation by using the
controller to excite unknown portions of the building's dynamics. Comparing
against a baseline thermostat controller, we demonstrate the utility of both
the initially acquired and improved thermal models within a Model Predictive
Control (MPC) framework, which anticipates weather uncertainty and time-varying
temperature set-points. A simulation study demonstrates self-excitation
improves model estimation, which corresponds to improved MPC energy savings and
occupant comfort. By coupling building topology, estimation, and control
routines into a single online framework, we have demonstrated the potential for
low-cost scalable methods to actively learn and control buildings to ensure
occupant comfort and minimize energy usage, all while using the existing
building's HVAC sensors and hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08178</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08178</id><created>2015-12-27</created><authors><author><keyname>Fiot</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Dinuzzo</keyname><forenames>Francesco</forenames></author></authors><title>Electricity Demand Forecasting by Multi-Task Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the application of kernel-based multi-task learning techniques to
forecast the demand of electricity in multiple nodes of a distribution network.
We show that recently developed output kernel learning techniques are
particularly well suited to solve this problem, as they allow to flexibly model
the complex seasonal effects that characterize electricity demand data, while
learning and exploiting correlations between multiple demand profiles. We also
demonstrate that kernels with a multiplicative structure yield superior
predictive performance with respect to the widely adopted (generalized)
additive models. Our study is based on residential and industrial smart meter
data provided by the Irish Commission for Energy Regulation (CER).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08183</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08183</id><created>2015-12-27</created><updated>2016-02-08</updated><authors><author><keyname>Li</keyname><forenames>Bofang</forenames></author><author><keyname>Liu</keyname><forenames>Tao</forenames></author><author><keyname>Du</keyname><forenames>Xiaoyong</forenames></author><author><keyname>Zhang</keyname><forenames>Deyuan</forenames></author><author><keyname>Zhao</keyname><forenames>Zhe</forenames></author></authors><title>Learning Document Embeddings by Predicting N-grams for Sentiment
  Classification of Long Movie Reviews</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bag-of-ngram based methods still achieve state-of-the-art results for tasks
such as sentiment classification of long movie reviews, despite semantic
information is partially lost for these methods. Many document embeddings
methods have been proposed to capture semantics, but they still can't
outperform bag-of-ngram based methods on this task. In this paper, we modify
the architecture of the recently proposed Paragraph Vector, allowing it to
learn document vectors by predicting not only words, but n-gram features as
well. Our model is able to capture both semantics and word order in documents
while keeping the expressive power of learned vectors. Experimental results on
IMDB movie review dataset shows that our model outperforms previous deep
learning models and bag-of-ngram based models due to the above advantages. More
robust results are also obtained when our model is combined with other models.
The source code of our model will be also published together with this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08187</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08187</id><created>2015-12-27</created><authors><author><keyname>Somani</keyname><forenames>Gaurav</forenames></author><author><keyname>Gaur</keyname><forenames>Manoj Singh</forenames></author><author><keyname>Sanghi</keyname><forenames>Dheeraj</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>DDoS Attacks in Cloud Computing: Issues, Taxonomy, and Future Directions</title><categories>cs.CR</categories><comments>Submitted to ACM Computing Surveys</comments><acm-class>C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud security related issues are relevant for various stakeholders in
decision-making for cloud adoption. Apart from data breaches, the attack space
is also being re-looked for cloud-specific solutions as it also affects
resource management and delivery of service quality. Distributed Denial of
Service (DDoS) attack is one such fatal attack in the cloud space. This survey
paper aims to present developments related to DDoS attack solutions in the
cloud. We also expand our discussion to a novel subclass of Denial of Service
(DoS) attack, i.e., the Economic Denial of Sustainability (EDoS) attack that
has recently reported in the literature. This comprehensive survey provides
detailed insight into the characterization, prevention, detection and
mitigation mechanisms of these attacks. Additionally, a comprehensive solution
taxonomy has been prepared to classify methods used by various contributions.
This survey concludes that there is a high requirement of solutions, which are
designed keeping utility computing models in mind. Accurate auto scaling
decisions, multi-layer mitigation, and defense using profound resources, are
some of the key requirements of desired solutions. At the end, we provide a
definite guideline on effective solution building and its detailed requirements
to help the community towards designing defense mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08189</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08189</id><created>2015-12-27</created><updated>2015-12-28</updated><authors><author><keyname>Ma</keyname><forenames>Lisheng</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Wu</keyname><forenames>Bin</forenames></author><author><keyname>Taleb</keyname><forenames>Tarik</forenames></author></authors><title>Cost-Efficient Data Backup for Data Center Networks against
  {\epsilon}-Time Early Warning Disaster</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data backup in data center networks (DCNs) is critical to minimize the data
loss under disaster. This paper considers the cost-efficient data backup for
DCNs against a disaster with {\epsilon} early warning time. Given a
geo-distributed DCN and such a {\epsilon}-time early warning disaster, we
investigate the issue of how to back up the data in DCN nodes under risk to
other safe DCN nodes within the {\epsilon} early warning time constraint.
Specifically, an Integer Linear Program (ILP)-based theoretical framework is
proposed to identify the optimal selections of backup DCN nodes and data
transmission paths, such that the overall data backup cost is minimized.
Extensive numerical results are also provided to illustrate the proposed
framework for DCN data backup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08194</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08194</id><created>2015-12-27</created><authors><author><keyname>Merzky</keyname><forenames>Andre</forenames></author><author><keyname>Santcroos</keyname><forenames>Mark</forenames></author><author><keyname>Turilli</keyname><forenames>Matteo</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>RADICAL-Pilot: Scalable Execution of Heterogeneous and Dynamic Workloads
  on Supercomputers</title><categories>cs.DC</categories><comments>18 pages, 15 figures, submitted to ICS'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally high-performance computing (HPC) systems have been optimized to
support mostly monolithic workloads. The workload of many important scientific
applications however, is comprised of spatially and temporally heterogeneous
tasks that are often dynamically inter-related. These workloads can benefit
from being executed at scale on HPC resources but a tension exists between
their resource utilization requirements and the capabilities of HPC system
software and HPC usage policies. Pilot systems have successfully been used to
address this tension. In this paper we introduce RADICAL-Pilot (RP), a scalable
and interoperable pilot system that faithfully implements the Pilot
abstraction. We describe its design and characterize the performance of its
components, as well as its performance on multiple heterogeneous HPC systems.
Specifically, we characterize RP's task execution component (the RP Agent),
which is engineered for optimal resource utilization while maintaining the full
generality of the Pilot abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08201</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08201</id><created>2015-12-27</created><authors><author><keyname>Rzepka</keyname><forenames>Kamil</forenames></author><author><keyname>Skurowski</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Adamczyk</keyname><forenames>B&#x142;a&#x17c;ej</forenames></author><author><keyname>Pil&#x15b;niak</keyname><forenames>Adam</forenames></author></authors><title>Design of portable power consumption measuring system for green
  computing needs</title><categories>cs.OH</categories><comments>submitted to Studia Informatica ( http://studiainformatica.polsl.pl/
  )</comments><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents the design of a digital power measurement device
intended for the green IT. Article comprises: use case analysis, accuracy and
precision measurements and real life test of apache web server as exemplary
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08204</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08204</id><created>2015-12-27</created><authors><author><keyname>McDonald</keyname><forenames>Andrew M.</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Stamos</keyname><forenames>Dimitris</forenames></author></authors><title>New Perspectives on $k$-Support and Cluster Norms</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a regularizer which is defined as a parameterized infimum of
quadratics, and which we call the box-norm. We show that the k-support norm, a
regularizer proposed by [Argyriou et al, 2012] for sparse vector prediction
problems, belongs to this family, and the box-norm can be generated as a
perturbation of the former. We derive an improved algorithm to compute the
proximity operator of the squared box-norm, and we provide a method to compute
the norm. We extend the norms to matrices, introducing the spectral k-support
norm and spectral box-norm. We note that the spectral box-norm is essentially
equivalent to the cluster norm, a multitask learning regularizer introduced by
[Jacob et al. 2009a], and which in turn can be interpreted as a perturbation of
the spectral k-support norm. Centering the norm is important for multitask
learning and we also provide a method to use centered versions of the norms as
regularizers. Numerical experiments indicate that the spectral k-support and
box-norms and their centered variants provide state of the art performance in
matrix completion and multitask learning problems respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08212</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08212</id><created>2015-12-27</created><authors><author><keyname>Rim</keyname><forenames>David</forenames></author><author><keyname>Honari</keyname><forenames>Sina</forenames></author><author><keyname>Hasan</keyname><forenames>Md Kamrul</forenames></author><author><keyname>Pal</keyname><forenames>Chris</forenames></author></authors><title>Improving Facial Analysis and Performance Driven Animation through
  Disentangling Identity and Expression</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present techniques for improving performance driven facial animation,
emotion recognition, and facial key-point or landmark prediction techniques
using learned identity invariant representations. Established approaches to
these problems can work well if sufficient examples and labels for a particular
identity are available and factors of variation are highly controlled. However,
labeled examples of facial expressions, emotions and key-points for new
individuals are difficult and costly to obtain. In this paper we improve the
ability of techniques to generalize to new and unseen individuals by explicitly
modeling previously seen variations related to identity and expression. We use
a weakly-supervised approach in which identity labels are used to learn the
different factors of variation linked to identity separately from factors
related to expression. We show how probabilistic modeling of these sources of
variation allows one to learn identity-invariant representations for
expressions which can then used to identity-normalize various procedures for
facial expression analysis and animation control. We also show how to extend
the widely used techniques of active appearance models and constrained local
models through replacing the underlying point distribution models which are
typically constructed using principal component analysis with
identity-expression factorized representations. We present a wide variety of
experiments in which we consistently improve performance on emotion
recognition, markerless performance-driven facial animation and facial
key-point tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08220</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08220</id><created>2015-12-27</created><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Wei</keyname><forenames>Hengjia</forenames></author><author><keyname>Shangguan</keyname><forenames>Chong</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>New bounds and constructions for multiply constant-weight codes</title><categories>cs.IT math.IT</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiply constant-weight codes (MCWCs) were introduced recently to improve
the reliability of certain physically unclonable function response. In this
paper, the bounds of MCWCs and the constructions of optimal MCWCs are studied.
Firstly, we derive three different types of upper bounds which improve the
Johnson-type bounds given by Chee {\sl et al.} in some parameters. The
asymptotic lower bound of MCWCs is also examined. Then we obtain the asymptotic
existence of two classes of optimal MCWCs, which shows that the Johnson-type
bounds for MCWCs with distances $2\sum_{i=1}^mw_i-2$ or $2mw-w$ are
asymptotically exact. Finally, we construct a class of optimal MCWCs with total
weight four and distance six by establishing the connection between such MCWCs
and a new kind of combinatorial structures. As a consequence, the maximum sizes
of MCWCs with total weight less than or equal to four are determined almost
completely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08230</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08230</id><created>2015-12-27</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Thirty eight things to do with live slime mould</title><categories>cs.ET</categories><comments>this is a draft of the chapter to appear in 'Advances in
  Unconventional Computing' (Sprineer, 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slime mould \emph{Physarum polycephalum} is a large single cell capable for
distributed sensing, concurrent information processing, parallel computation
and decentralised actuation. The ease of culturing and experimenting with
Physarum makes this slime mould an ideal substrate for real-world
implementations of unconventional sensing and computing devices. In the last
decade the Physarum became a swiss knife of the unconventional computing: give
the slime mould a problem it will solve it. We provide a concise summary of
what exact computing and sensing operations are implemented with live slime
mould. The Physarum devices range from morphological processors for
computational geometry to experimental archeology tools, from self-routing
wires to memristors, from devices approximating a shortest path to analog
physical models of space exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08240</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08240</id><created>2015-12-27</created><authors><author><keyname>Krijthe</keyname><forenames>Jesse H.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Robust Semi-supervised Least Squares Classification by Implicit
  Constraints</title><categories>stat.ML cs.LG</categories><comments>26 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the implicitly constrained least squares (ICLS) classifier, a
novel semi-supervised version of the least squares classifier. This classifier
minimizes the squared loss on the labeled data among the set of parameters
implied by all possible labelings of the unlabeled data. Unlike other
discriminative semi-supervised methods, this approach does not introduce
explicit additional assumptions into the objective function, but leverages
implicit assumptions already present in the choice of the supervised least
squares classifier. This method can be formulated as a quadratic programming
problem and its solution can be found using a simple gradient descent
procedure. We prove that, in a limited 1-dimensional setting, this approach
never leads to performance worse than the supervised classifier. Experimental
results show that also in the general multidimensional case performance
improvements can be expected, both in terms of the squared loss that is
intrinsic to the classifier, as well as in terms of the expected classification
error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08250</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08250</id><created>2015-12-27</created><authors><author><keyname>Monshizadeh</keyname><forenames>Nima</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan J.</forenames></author><author><keyname>Scherpen</keyname><forenames>Jacquelien M. A.</forenames></author></authors><title>A Networked Reduced Model for Electrical Networks with Constant Power
  Loads</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider structure preserving power networks with proper algebraic
constraints resulting from constant power loads. Both for the linear and the
nonlinear model of the network, we propose explicit reduced order models which
are expressed in terms of ordinary differential equations. The relative
frequencies among all the buses in the original power grid are readily
tractable in the proposed reduced models. For deriving these reduced models, we
introduce the ``projected pseudo incidence&quot; matrix which yields a novel
decomposition of the reduced Laplacian matrix. With the help of this new
matrix, we are able to eliminate the proper algebraic constraints while
preserving the crucial frequency information of the loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08258</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08258</id><created>2015-12-27</created><authors><author><keyname>Che</keyname><forenames>Tong</forenames></author></authors><title>Eventual Wait-Free Synchronization</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Eventually linearizable objects are novel shared memory programming
constructs introduced as an analogy to eventual consistency in message-passing
systems. However, their behaviors in shared memory systems are so mysterious
that very little general theoretical properties of them is known.
  In this paper, we lay the theoretical foundation of the study of eventually
linearizable objects. We prove that the n-process eventually linearizable
fetch-and-cons (n-FAC) object is universal and can be used to classify the
eventually linearizable objects. In particular, we define the concept of
eventual consensus number of an abstract data type and prove that the eventual
consensus number can be used as a good characterization of the synchronization
power of eventual objects. Thus we got a complete hierarchy of eventually
linearizable objects, as a perfect analogy of the consensus hierarchy. In this
way, the synchronization power of eventual linearizability become much more
well understood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08269</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08269</id><created>2015-12-27</created><authors><author><keyname>Yang</keyname><forenames>Fanny</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Statistical and Computational Guarantees for the Baum-Welch Algorithm</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling
of discrete time series, with applications including speech recognition,
computational biology, computer vision and econometrics. Estimating an HMM from
its observation process is often addressed via the Baum-Welch algorithm, which
is known to be susceptible to local optima. In this paper, we first give a
general characterization of the basin of attraction associated with any global
optimum of the population likelihood. By exploiting this characterization, we
provide non-asymptotic finite sample guarantees on the Baum-Welch updates,
guaranteeing geometric convergence to a small ball of radius on the order of
the minimax rate around a global optimum. As a concrete example, we prove a
linear rate of convergence for a hidden Markov mixture of two isotropic
Gaussians given a suitable mean separation and an initialization within a ball
of large radius around (one of) the true parameters. To our knowledge, these
are the first rigorous local convergence guarantees to global optima for the
Baum-Welch algorithm in a setting where the likelihood function is nonconvex.
We complement our theoretical results with thorough numerical simulations
studying the convergence of the Baum-Welch algorithm and illustrating the
accuracy of our predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08279</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08279</id><created>2015-12-27</created><authors><author><keyname>Ebert-Uphoff</keyname><forenames>Imme</forenames></author><author><keyname>Deng</keyname><forenames>Yi</forenames></author></authors><title>Using Causal Discovery to Track Information Flow in Spatio-Temporal Data
  - A Testbed and Experimental Results Using Advection-Diffusion Simulations</title><categories>cs.LG</categories><comments>40 pages, 19 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal discovery algorithms based on probabilistic graphical models have
emerged in geoscience applications for the identification and visualization of
dynamical processes. The key idea is to learn the structure of a graphical
model from observed spatio-temporal data, which indicates information flow,
thus pathways of interactions, in the observed physical system. Studying those
pathways allows geoscientists to learn subtle details about the underlying
dynamical mechanisms governing our planet. Initial studies using this approach
on real-world atmospheric data have shown great potential for scientific
discovery. However, in these initial studies no ground truth was available, so
that the resulting graphs have been evaluated only by whether a domain expert
thinks they seemed physically plausible. This paper seeks to fill this gap. We
develop a testbed that emulates two dynamical processes dominant in many
geoscience applications, namely advection and diffusion, in a 2D grid. Then we
apply the causal discovery based information tracking algorithms to the
simulation data to study how well the algorithms work for different scenarios
and to gain a better understanding of the physical meaning of the graph
results, in particular of instantaneous connections. We make all data sets used
in this study available to the community as a benchmark.
  Keywords: Information flow, graphical model, structure learning, causal
discovery, geoscience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08291</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08291</id><created>2015-12-27</created><updated>2016-02-28</updated><authors><author><keyname>Lu</keyname><forenames>Jingwei</forenames></author><author><keyname>Zhuang</keyname><forenames>Hao</forenames></author><author><keyname>Kang</keyname><forenames>Ilgweon</forenames></author><author><keyname>Chen</keyname><forenames>Pengwen</forenames></author><author><keyname>Cheng</keyname><forenames>Chung-Kuan</forenames></author></authors><title>ePlace-3D: Electrostatics based Placement for 3D-ICs</title><categories>cs.OH</categories><comments>8 pages, 7 figures, ISPD 2016</comments><doi>10.1145/2872334.2872361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a flat, analytic, mixed-size placement algorithm ePlace-3D for
three-dimension integrated circuits (3D-ICs) using nonlinear optimization. Our
contributions are (1) electrostatics based 3D density function with globally
uniform smoothness (2) 3D numerical solution with improved spectral formulation
(3) 3D nonlinear pre-conditioner for convergence acceleration (4) interleaved
2D-3D placement for efficiency enhancement. Our placer outperforms the leading
work mPL6-3D and NTUplace3-3D with 6.44% and 37.15% shorter wirelength, 9.11%
and 10.27% fewer 3D vertical interconnects (VI) on average of IBM-PLACE
circuits. Validation on the large-scale modern mixed-size (MMS) 3D circuits
shows high performance and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08292</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08292</id><created>2015-12-27</created><authors><author><keyname>Mehrabi</keyname><forenames>Saeed</forenames></author></authors><title>Guarding the Vertices of an Orthogonal Terrain using Vertex Guards</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A terrain T is an x-monotone polygonal chain in the plane; T is orthogonal if
each edge of T is either horizontal or vertical. In this paper, we give an
exact algorithm for the problem of guarding the convex vertices of an
orthogonal terrain with the minimum number of reflex vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08299</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08299</id><created>2015-12-27</created><updated>2016-01-25</updated><authors><author><keyname>Liang</keyname><forenames>Qingkai</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Survivability in Time-varying Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-varying graphs are a useful model for networks with dynamic connectivity
such as vehicular networks, yet, despite their great modeling power, many
important features of time-varying graphs are still poorly understood. In this
paper, we study the survivability properties of time-varying networks against
unpredictable interruptions. We first show that the traditional definition of
survivability is not effective in time-varying networks, and propose a new
survivability framework. To evaluate the survivability of time-varying networks
under the new framework, we propose two metrics that are analogous to MaxFlow
and MinCut in static networks. We show that some fundamental
survivability-related results such as Menger's Theorem only conditionally hold
in time-varying networks. Then we analyze the complexity of computing the
proposed metrics and develop several approximation algorithms. Finally, we
conduct trace-driven simulations to demonstrate the application of our
survivability framework to the robust design of a real-world bus communication
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08301</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08301</id><created>2015-12-27</created><updated>2016-01-04</updated><authors><author><keyname>Zhang</keyname><forenames>Shiliang</forenames></author><author><keyname>Liu</keyname><forenames>Cong</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author><author><keyname>Wei</keyname><forenames>Si</forenames></author><author><keyname>Dai</keyname><forenames>Lirong</forenames></author><author><keyname>Hu</keyname><forenames>Yu</forenames></author></authors><title>Feedforward Sequential Memory Networks: A New Structure to Learn
  Long-term Dependency</title><categories>cs.NE</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel neural network structure, namely
\emph{feedforward sequential memory networks (FSMN)}, to model long-term
dependency in time series without using recurrent feedback. The proposed FSMN
is a standard fully-connected feedforward neural network equipped with some
learnable memory blocks in its hidden layers. The memory blocks use a
tapped-delay line structure to encode the long context information into a
fixed-size representation as short-term memory mechanism. We have evaluated the
proposed FSMNs in several standard benchmark tasks, including speech
recognition and language modelling. Experimental results have shown FSMNs
significantly outperform the conventional recurrent neural networks (RNN),
including LSTMs, in modeling sequential signals like speech or language.
Moreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs
due to the inherent non-recurrent model structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08309</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08309</id><created>2015-12-27</created><authors><author><keyname>Malladi</keyname><forenames>Rakesh</forenames></author><author><keyname>Kalamangalam</keyname><forenames>Giridhar</forenames></author><author><keyname>Tandon</keyname><forenames>Nitin</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Identifying Seizure Onset Zone from the Causal Connectivity Inferred
  Using Directed Information</title><categories>q-bio.NC cs.IT math.IT stat.ME</categories><comments>Submitted to IEEE Journal of Selected Topics in Signal Processing,
  special issue on Advanced Signal Processing in Brain Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we developed a model-based and a data-driven estimator for
directed information (DI) to infer the causal connectivity graph between
electrocorticographic (ECoG) signals recorded from brain and to identify the
seizure onset zone (SOZ) in epileptic patients. Directed information, an
information theoretic quantity, is a general metric to infer causal
connectivity between time-series and is not restricted to a particular class of
models unlike the popular metrics based on Granger causality or transfer
entropy. The proposed estimators are shown to be almost surely convergent.
Causal connectivity between ECoG electrodes in five epileptic patients is
inferred using the proposed DI estimators, after validating their performance
on simulated data. We then proposed a model-based and a data-driven SOZ
identification algorithm to identify SOZ from the causal connectivity inferred
using model-based and data-driven DI estimators respectively. The data-driven
SOZ identification outperforms the model-based SOZ identification algorithm
when benchmarked against visual analysis by neurologist, the current clinical
gold standard. The causal connectivity analysis presented here is the first
step towards developing novel non-surgical treatments for epilepsy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08311</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08311</id><created>2015-12-27</created><authors><author><keyname>Hu</keyname><forenames>Zhongwei</forenames></author><author><keyname>Yuan</keyname><forenames>Chaowei</forenames></author><author><keyname>Zhu</keyname><forenames>Fengchao</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author></authors><title>Weighted Sum Transmit Power Minimization for Full-Duplex System with
  SWIPT and Self-Energy Recycling</title><categories>cs.NI cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This correspondence considers a full-duplex (FD) point-to-point system
consisting of one multi-antenna full-duplex access point (FD-AP) and one
two-antenna full-duplex mobile station (FD-MS). We adopt simultaneous wireless
information and power transfer (SWIPT) scheme and apply the self-energy
recycling at FD-MS. In order to minimize the weighted sum transmit power, we
jointly design the transmit beamforming vector of FD-AP, the receive power
splitting (PS) ratio of FD-MS, as well as the transmit power value of FD-MS.
Since the original problem is non-convex, we apply semidefinite relaxation
(SDR) and obtain a new convex problem. We further prove that both problems have
exactly the same solutions. Finally, simulations are provided to verify our
analysis, and the comparison with a half-duplex (HD) system demonstrates the
significant performance gain from self-energy recycling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08314</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08314</id><created>2015-12-27</created><authors><author><keyname>Brun</keyname><forenames>Olivier</forenames></author><author><keyname>Wang</keyname><forenames>Lan</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Data Driven SMART Intercontinental Overlay Networks</title><categories>cs.NI cs.DC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the use of Big Data and machine learning based analytics
to the real-time management of Internet scale Quality-of-Service Route
Optimisation with the help of an overlay network. Based on the collection of
large amounts of data sampled each $2$ minutes over a large number of
source-destinations pairs, we show that intercontinental Internet Protocol (IP)
paths are far from optimal with respect to Quality of Service (QoS) metrics
such as end-to-end round-trip delay. We therefore develop a machine learning
based scheme that exploits large scale data collected from communicating node
pairs in a multi-hop overlay network that uses IP between the overlay nodes
themselves, to select paths that provide substantially better QoS than IP. The
approach inspired from Cognitive Packet Network protocol, uses Random Neural
Networks with Reinforcement Learning based on the massive data that is
collected, to select intermediate overlay hops resulting in significantly
better QoS than IP itself. The routing scheme is illustrated on a $20$-node
intercontinental overlay network that collects close to $2\times 10^6$
measurements per week, and makes scalable distributed routing decisions.
Experimental results show that this approach improves QoS significantly and
efficiently in a scalable manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08321</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08321</id><created>2015-12-28</created><authors><author><keyname>Kim</keyname><forenames>Jooyeon</forenames></author><author><keyname>Keegan</keyname><forenames>Brian C.</forenames></author><author><keyname>Park</keyname><forenames>Sungjoon</forenames></author><author><keyname>Oh</keyname><forenames>Alice</forenames></author></authors><title>The Proficiency-Congruency Dilemma: Virtual Team Design and Performance
  in Multiplayer Online Games</title><categories>cs.HC cs.CY</categories><comments>To appear In Proceedings of the 34th Annual ACM Conference on Human
  Factors in Computing Systems (CHI 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplayer online battle arena games provide an excellent opportunity to
study team performance. When designing a team, players must negotiate a
\textit{proficiency-congruency dilemma} between selecting roles that best match
their experience and roles that best complement the existing roles on the team.
We adopt a mixed-methods approach to explore how users negotiate this dilemma.
Using data from \textit{League of Legends}, we define a similarity space to
operationalize team design constructs about role proficiency, generality, and
congruency. We collect publicly available data from 3.36 million users to test
the influence of these constructs on team performance. We also conduct focus
groups with novice and elite players to understand how players' team design
practices vary with expertise. We find that player proficiency increases team
performance more than team congruency. These findings have implications for
players, designers, and theorists about how to recommend team designs that
jointly prioritize individuals' expertise and teams' compatibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08325</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08325</id><created>2015-12-28</created><authors><author><keyname>Zhao</keyname><forenames>Yao-Dong</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Shang</keyname><forenames>Ming-Sheng</forenames></author></authors><title>A Fast Recommendation Algorithm for Social Tagging Systems : A Delicious
  Case</title><categories>cs.IR cs.SI</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tripartite graph is one of the commonest topological structures in social
tagging systems such as Delicious, which has three types of nodes (i.e., users,
URLs and tags). Traditional recommender systems developed based on
collaborative filtering for the social tagging systems bring very high demands
on CPU time cost. In this paper, to overcome this drawback, we propose a novel
approach that extracts non-overlapping user clusters and corresponding
overlapping item clusters simultaneously through coarse clustering to
accelerate the user-based collaborative filtering and develop a fast
recommendation algorithm for the social tagging systems. The experimental
results show that the proposed approach is able to dramatically reduce the
processing time cost greater than $90\%$ and relatively enhance the accuracy in
comparison with the ordinary user-based collaborative filtering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08347</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08347</id><created>2015-12-28</created><updated>2016-01-03</updated><authors><author><keyname>Lou</keyname><forenames>Yang</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author><author><keyname>Hu</keyname><forenames>Jianwei</forenames></author></authors><title>Communicating with sentences: A multi-word naming game model</title><categories>cs.CL physics.soc-ph</categories><comments>Paper(13pp,10figures) + SI(7pp,8figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Naming game simulates the process of naming a single object by a single word,
in which a population of communicating agents can reach global consensus
asymptotically through iteratively pair-wise conversations. In this paper, we
propose an extension of the single-word naming game, to a multi-word naming
game (MWNG), which simulates the naming game process when agents name an object
by a sentence (i.e., a series of multiple words) for describing a complex
object such as an opinion or an event. We first define several categories of
words, and then organize sentences by combining words from different word
categories. We refer to a formatted combination of several words as a pattern.
In such an MWNG, through a pair-wise conversation, it requires the hearer to
achieve consensus with the speaker with respect to both every single word in
the sentence as well as the sentence pattern, so as to guarantee the correct
meaning of the saying; otherwise, they fail reaching consensus in the
interaction. We employ three typical topologies used for the underlying
communication network, namely random-graph, small-world and scale-free
networks. We validate the model by using both conventional English language
patterns and man-made test sentence patterns in performing the MWNG. Our
simulation results show that: 1) the new sentence sharing model is an extension
of the classical lexicon sharing model; 2) the propagating, learning and
converging processes are more complicated than that in the conventional naming
game; 3) the convergence time is non-decreasing as the network becomes better
connected; 4) the agents are prone to accept short sentence patterns. These new
findings may help deepen our understanding of the human language development
from a network science perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08354</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08354</id><created>2015-12-28</created><authors><author><keyname>Fidler</keyname><forenames>Markus</forenames></author><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author></authors><title>Non-Asymptotic Delay Bounds for (k,l) Fork-Join Systems and Multi-Stage
  Fork-Join Networks</title><categories>cs.PF cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel systems have received increasing attention with numerous recent
applications such as fork-join systems, load-balancing, and l-out-of-k
redundancy. Common to these systems is a join or resequencing stage, where
tasks that have finished service may have to wait for the completion of other
tasks so that they leave the system in a predefined order. These
synchronization constraints make the analysis of parallel systems challenging
and few explicit results are known. In this work, we model parallel systems
using a max-plus approach that enables us to derive statistical bounds of
waiting and sojourn times. Taking advantage of max-plus system theory, we also
show end-to-end delay bounds for multi-stage fork-join networks. We contribute
solutions for basic G|G|1 fork-join systems, parallel systems with
load-balancing, as well as general (k,l) fork-join systems with redundancy. Our
results provide insights into the respective advantages of l-out-of-k
redundancy vs. load-balancing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08365</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08365</id><created>2015-12-28</created><authors><author><keyname>Cioc&#x103;nea-Teodorescu</keyname><forenames>Iuliana</forenames></author></authors><title>The Module Isomorphism Problem for Finite Rings and Related Results</title><categories>math.RA cs.SC</categories><comments>7 pages, submitted, an abstract of this paper appeared in ACM
  Communications in Computer Algebra, Volume 49, Issue 1, page 14, March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R$ be a finite ring and let $M, N$ be two finite left $R$-modules. We
present two distinct deterministic algorithms that decide in polynomial time
whether or not $M$ and $N$ are isomorphic, and if they are, exhibit an
isomorphism. As by-products, we are able to determine the largest isomorphic
common direct summand between two modules and the minimum number of generators
of a module. By not requiring $R$ to contain a field, avoiding computation of
the Jacobson radical and not distinguishing between large and small
characteristic, both algorithms constitute improvements to known results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08366</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08366</id><created>2015-12-28</created><authors><author><keyname>Raut</keyname><forenames>Manoj K.</forenames></author></authors><title>Computing Theory Prime Implicates in Modal Logic</title><categories>cs.LO</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algorithm to compute theory prime implicates, a generalization of prime
implicates, in propositional logic has been suggested in \cite{Marquis}. In
this paper we have extended that algorithm to compute theory prime implicates
of a knowledge base $X$ with respect to another knowledge base $\Box Y$ in
modal logic $\mathcal{K}$. We have also extended the query answering algorithm
in modal logic $\mathcal{K}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08383</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08383</id><created>2015-12-28</created><authors><author><keyname>Hababeh</keyname><forenames>Ismail</forenames></author></authors><title>Data Migration among Different Clouds</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing services are becoming more and more popular. However, the
high concentration of data and services on the clouds make them attractive
targets for various security attacks, including DoS, data theft, and privacy
attacks. Additionally, cloud providers may fail to comply with service level
agreement in terms of performance, availability, and security guarantees.
Moreover, users may choose to utilize public cloud services from multiple
vendors for various reasons including fault tolerance and availability.
Therefore, it is of paramount importance to have secure and efficient
mechanisms that enable users to transparently copy and move their data from one
provider to another. In this paper, we explore the state of the art inter cloud
migration techniques and identify the potential security threats in the scope
of Hadoop Distributed File System HDFS. We propose an inter cloud data
migration mechanism that offers better security guarantees and faster response
time for migrating large scale data files in cloud database management systems.
The proposed approach enhances the data security processes used to achieve
secure data migration between cloud nodes thus improves applications response
time and throughput. The performance of the proposed approach is validated by
measuring its impact on response time and throughput, and comparing the
performance to that of other techniques in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08395</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08395</id><created>2015-12-28</created><authors><author><keyname>Andro</keyname><forenames>Mathieu</forenames></author><author><keyname>Saleh</keyname><forenames>Imad</forenames></author></authors><title>Biblioth\`eques num\'eriques et gamification : panorama et \'etat de
  l'art</title><categories>cs.DL</categories><comments>in French.
  http://www.adbs.fr/i2d-n-4-decembre-2015-dossier-les-services-d-information-au-prisme-de-la-valeur-153432.htm</comments><proxy>ccsd</proxy><journal-ref>I2D -- Information, donn\'ees &amp; documents, A.D.B.S., 2015, 52 (4),
  pp.70-79</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an overview of the main gamification projects for
digital libraries, either for tagging or OCR correction. This overview is
followed by a state of the art with functionalities, motivations, sociology of
contributors and the scope of gamification compared to the serious games and
explicit crowdsourcing. Finally a comparison of results between explicit
crowdsourcing and gamification is proposed.
  [English Title: Digital libraries and gamification: overview and state of the
art]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08409</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08409</id><created>2015-12-28</created><authors><author><keyname>Feitelson</keyname><forenames>Dror G.</forenames></author></authors><title>Using Students as Experimental Subjects in Software Engineering Research
  -- A Review and Discussion of the Evidence</title><categories>cs.SE</categories><comments>53 pages including 5 pages of tables and 191 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Should students be used as experimental subjects in software engineering?
Given that students are in many cases readily available and cheap it is no
surprise that the vast majority of controlled experiments in software
engineering use them. But they can be argued to constitute a convenience sample
that may not represent the target population (typically &quot;real&quot; developers),
especially in terms of experience and proficiency. This causes many researchers
(and reviewers) to have reservations about the external validity of
student-based experiments, and claim that students should not be used. Based on
an extensive review of published works that have compared students to
professionals, we find that picking on &quot;students&quot; is counterproductive for two
main reasons. First, classifying experimental subjects by their status is
merely a proxy for more important and meaningful classifications, such as
classifying them according to their abilities, and effort should be invested in
defining and using these more meaningful classifications. Second, in many cases
using students is perfectly reasonable, and student subjects can be used to
obtain reliable results and further the research goals. In particular, this
appears to be the case when the study involves basic programming and
comprehension skills, when tools or methodologies that do not require an
extensive learning curve are being compared, and in the initial formative
stages of large industrial research initiatives -- in other words, in many of
the cases that are suitable for controlled experiments of limited scope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08413</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08413</id><created>2015-12-28</created><authors><author><keyname>Lam</keyname><forenames>Philip</forenames></author><author><keyname>Wang</keyname><forenames>Lili</forenames></author><author><keyname>Ngan</keyname><forenames>Henry Y. T.</forenames></author><author><keyname>Yung</keyname><forenames>Nelson H. C.</forenames></author><author><keyname>Yeh</keyname><forenames>Anthony G. O.</forenames></author></authors><title>Outlier Detection In Large-scale Traffic Data By Na\&quot;ive Bayes Method
  and Gaussian Mixture Model Method</title><categories>cs.CV</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is meaningful to detect outliers in traffic data for traffic management.
However, this is a massive task for people from large-scale database to
distinguish outliers. In this paper, we present two methods: Kernel Smoothing
Na\&quot;ive Bayes (NB) method and Gaussian Mixture Model (GMM) method to
automatically detect any hardware errors as well as abnormal traffic events in
traffic data collected at a four-arm junction in Hong Kong. Traffic data was
recorded in a video format, and converted to spatial-temporal (ST) traffic
signals by statistics. The ST signals are then projected to a two-dimensional
(2D) (x,y)-coordinate plane by Principal Component Analysis (PCA) for dimension
reduction. We assume that inlier data are normal distributed. As such, the NB
and GMM methods are successfully applied in outlier detection (OD) for traffic
data. The kernel smooth NB method assumes the existence of kernel distributions
in traffic data and uses Bayes' Theorem to perform OD. In contrast, the GMM
method believes the traffic data is formed by the mixture of Gaussian
distributions and exploits confidence region for OD. This paper would address
the modeling of each method and evaluate their respective performances.
Experimental results show that the NB algorithm with Triangle kernel and GMM
method achieve up to 93.78% and 94.50% accuracies, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08417</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08417</id><created>2015-12-28</created><updated>2016-01-13</updated><authors><author><keyname>Ivanov</keyname><forenames>Todor</forenames></author><author><keyname>Beer</keyname><forenames>Max-Georg</forenames></author></authors><title>Evaluating Hive and Spark SQL with BigBench</title><categories>cs.DB cs.DC</categories><comments>50 pages, 20 Tables</comments><report-no>Technical Report No. 2015-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this work was to utilize BigBench [1] as a Big Data
benchmark and evaluate and compare two processing engines: MapReduce [2] and
Spark [3]. MapReduce is the established engine for processing data on Hadoop.
Spark is a popular alternative engine that promises faster processing times
than the established MapReduce engine. BigBench was chosen for this comparison
because it is the first end-to-end analytics Big Data benchmark and it is
currently under public review as TPCx-BB [4]. One of our goals was to evaluate
the benchmark by performing various scalability tests and validate that it is
able to stress test the processing engines. First, we analyzed the steps
necessary to execute the available MapReduce implementation of BigBench [1] on
Spark. Then, all the 30 BigBench queries were executed on MapReduce/Hive with
different scale factors in order to see how the performance changes with the
increase of the data size. Next, the group of HiveQL queries were executed on
Spark SQL and compared with their respective Hive runtimes. This report gives a
detailed overview on how to setup an experimental Hadoop cluster and execute
BigBench on both Hive and Spark SQL. It provides the absolute times for all
experiments preformed for different scale factors as well as query results
which can be used to validate correct benchmark execution. Additionally,
multiple issues and workarounds were encountered and solved during our work. An
evaluation of the resource utilization (CPU, memory, disk and network usage) of
a subset of representative BigBench queries is presented to illustrate the
behavior of the different query groups on both processing engines. Last but not
least it is important to mention that larger parts of this report are taken
from the master thesis of Max-Georg Beer, entitled &quot;Evaluation of BigBench on
Apache Spark Compared to MapReduce&quot; [5].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08419</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08419</id><created>2015-12-28</created><authors><author><keyname>Yu</keyname><forenames>Hao</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Dynamic Power Allocation in MIMO Fading Systems Without Channel
  Distribution Information</title><categories>cs.IT math.IT</categories><comments>A short version of this paper will be presented in INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers dynamic power allocation in MIMO fading systems with
unknown channel state distributions. First, the ideal case of perfect
instantaneous channel state information at the transmitter (CSIT) is treated.
Using the drift-plus-penalty method, a dynamic power allocation policy is
developed and shown to approach optimality, regardless of the channel state
distribution and without requiring knowledge of this distribution. Next, the
case of delayed and quantized channel state information is considered. Optimal
utility is fundamentally different in this case, and a different online
algorithm is developed that is based on convex projections. The proposed
algorithm for this delayed-CSIT case is shown to have an $O(\delta)$ optimality
gap, where $\delta$ is the quantization error of CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08422</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08422</id><created>2015-12-28</created><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Rui</keyname><forenames>Men</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Xu</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Yan</keyname><forenames>Rui</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Recognizing Entailment and Contradiction by Tree-based Convolution</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing entailment and contradiction between two sentences has wide
applications in NLP. Traditional methods include feature-rich classifiers or
formal reasoning. However, they are usually limited in terms of accuracy and
scope. Recently, the renewed prosperity neural networks has made many
improvements in a variety of NLP tasks. In particular, the tree-based
convolutional neural network (TBCNN) proposed in our previous work has achieved
high performance in several sentence-level classification tasks. But whether
TBCNN is applicable to recognize entailment and contradiction between two
sentences remains unknown. In this paper, we propose TBCNN-pair model for
entailment/contradiction recognition. Experimental results in a large dataset
verifies the rationale of using TBCNN as the sentence-level model; leveraging
additional heuristics like element-wise product/difference further improves the
result, and outperforms published results by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08424</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08424</id><created>2015-12-28</created><authors><author><keyname>Welk</keyname><forenames>Martin</forenames></author></authors><title>Graph entropies in texture segmentation of images</title><categories>cs.CV</categories><comments>28 pages, 6 figures</comments><acm-class>I.4.7; I.4.6; G.2.2; I.2.10; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the applicability of a set of texture descriptors introduced in
recent work by the author to texture-based segmentation of images. The texture
descriptors under investigation result from applying graph indices from
quantitative graph theory to graphs encoding the local structure of images. The
underlying graphs arise from the computation of morphological amoebas as
structuring elements for adaptive morphology, either as weighted or unweighted
Dijkstra search trees or as edge-weighted pixel graphs within structuring
elements. In the present paper we focus on texture descriptors in which the
graph indices are entropy-based, and use them in a geodesic active contour
framework for image segmentation. Experiments on several synthetic and one
real-world image are shown to demonstrate texture segmentation by this
approach. Forthermore, we undertake an attempt to analyse selected
entropy-based texture descriptors with regard to what information about texture
they actually encode. Whereas this analysis uses some heuristic assumptions, it
indicates that the graph-based texture descriptors are related to fractal
dimension measures that have been proven useful in texture analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08425</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08425</id><created>2015-12-28</created><updated>2016-01-19</updated><authors><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Convexified Modularity Maximization for Degree-corrected Stochastic
  Block Models</title><categories>math.ST cs.LG cs.SI stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model (SBM) is a popular framework for studying
community detection in networks. This model is limited by the assumption that
all nodes in the same community are statistically equivalent and have equal
expected degrees. The degree-corrected stochastic block model (DCSBM) is a
natural extension of SBM that allows for degree heterogeneity within
communities. This paper proposes a convexified modularity maximization approach
for estimating the hidden communities under DCSBM. Our approach is based on a
convex programming relaxation of the classical (generalized) modularity
maximization formulation, followed by a novel doubly-weighted $ \ell_1 $-norm $
k $-median procedure. We establish non-asymptotic theoretical guarantees for
both approximate clustering and perfect clustering. Our approximate clustering
results are insensitive to the minimum degree, and hold even in sparse regime
with bounded average degrees. In the special case of SBM, these theoretical
results match the best-known performance guarantees of computationally feasible
algorithms. Numerically, we provide an efficient implementation of our
algorithm, which is applied to both synthetic and real-world networks.
Experiment results show that our method enjoys competitive performance compared
to the state of the art in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08427</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08427</id><created>2015-12-28</created><authors><author><keyname>Han</keyname><forenames>Li</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author><author><keyname>Qiang</keyname><forenames>Ruixin</forenames></author></authors><title>Incentivizing Exploration with Heterogeneous Value of Money</title><categories>cs.GT</categories><comments>WINE 2015</comments><journal-ref>LNCS 9470 (2015) 370-383</journal-ref><doi>10.1007/978-3-662-48995-6_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Frazier et al. proposed a natural model for crowdsourced
exploration of different a priori unknown options: a principal is interested in
the long-term welfare of a population of agents who arrive one by one in a
multi-armed bandit setting. However, each agent is myopic, so in order to
incentivize him to explore options with better long-term prospects, the
principal must offer the agent money. Frazier et al. showed that a simple class
of policies called time-expanded are optimal in the worst case, and
characterized their budget-reward tradeoff.
  The previous work assumed that all agents are equally and uniformly
susceptible to financial incentives. In reality, agents may have different
utility for money. We therefore extend the model of Frazier et al. to allow
agents that have heterogeneous and non-linear utilities for money. The
principal is informed of the agent's tradeoff via a signal that could be more
or less informative.
  Our main result is to show that a convex program can be used to derive a
signal-dependent time-expanded policy which achieves the best possible
Lagrangian reward in the worst case. The worst-case guarantee is matched by
so-called &quot;Diamonds in the Rough&quot; instances; the proof that the guarantees
match is based on showing that two different convex programs have the same
optimal solution for these specific instances. These results also extend to the
budgeted case as in Frazier et al. We also show that the optimal policy is
monotone with respect to information, i.e., the approximation ratio of the
optimal policy improves as the signals become more informative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08451</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08451</id><created>2015-12-28</created><updated>2016-01-08</updated><authors><author><keyname>AlJadda</keyname><forenames>Khalifeh</forenames></author><author><keyname>Ranzinger</keyname><forenames>Rene</forenames></author><author><keyname>Porterfield</keyname><forenames>Melody</forenames></author><author><keyname>Weatherly</keyname><forenames>Brent</forenames></author><author><keyname>Korayem</keyname><forenames>Mohammed</forenames></author><author><keyname>Miller</keyname><forenames>John A.</forenames></author><author><keyname>Rasheed</keyname><forenames>Khaled</forenames></author><author><keyname>Kochut</keyname><forenames>Krys J.</forenames></author><author><keyname>York</keyname><forenames>William S.</forenames></author></authors><title>GELATO and SAGE: An Integrated Framework for MS Annotation</title><categories>cs.AI cs.CE q-bio.QM</categories><comments>To be submitted to Bioinformatics journal, Oxford press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several algorithms and tools have been developed to (semi) automate the
process of glycan identification by interpreting Mass Spectrometric data.
However, each has limitations when annotating MSn data with thousands of MS
spectra using uncurated public databases. Moreover, the existing tools are not
designed to manage MSn data where n &gt; 2. We propose a novel software package to
automate the annotation of tandem MS data. This software consists of two major
components. The first, is a free, semi-automated MSn data interpreter called
the Glycomic Elucidation and Annotation Tool (GELATO). This tool extends and
automates the functionality of existing open source projects, namely,
GlycoWorkbench (GWB) and GlycomeDB. The second is a machine learning model
called Smart Anotation Enhancement Graph (SAGE), which learns the behavior of
glycoanalysts to select annotations generated by GELATO that emulate human
interpretation of the spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08454</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08454</id><created>2015-12-28</created><authors><author><keyname>Wang</keyname><forenames>Yongge</forenames></author></authors><title>Quantum Resistant Random Linear Code Based Public Key Encryption Scheme
  RLCE</title><categories>cs.CR</categories><msc-class>94B05, 94A60, 11T71, 68P25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice based encryption schemes and linear code based encryption schemes
have received extensive attention in recent years since they have been
considered as post-quantum candidate encryption schemes. Though LLL reduction
algorithm has been one of the major cryptanalysis techniques for lattice based
cryptographic systems, key recovery cryptanalysis techniques for linear code
based cryptographic systems are generally scheme specific. In recent years,
several important techniques such as Sidelnikov-Shestakov attack, filtration
attacks, and algebraic attacks have been developed to crypt-analyze linear code
based encryption schemes. Though most of these cryptanalysis techniques are
relatively new, they prove to be very powerful and many systems have been
broken using them. Thus it is important to design linear code based
cryptographic systems that are immune against these attacks. This paper
proposes linear code based encryption scheme RLCE which shares many
characteristics with random linear codes. Our analysis shows that the scheme
RLCE is secure against existing attacks and we hope that the security of the
RLCE scheme is equivalent to the hardness of decoding random linear codes.
Example parameters for different security levels are recommended for the scheme
RLCE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08455</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08455</id><created>2015-12-28</created><authors><author><keyname>Wu</keyname><forenames>Tao</forenames></author><author><keyname>Chen</keyname><forenames>Leiting</forenames></author><author><keyname>Xian</keyname><forenames>Xingping</forenames></author><author><keyname>Guo</keyname><forenames>Yuxiao</forenames></author></authors><title>Full-scale Cascade Dynamics Prediction with a Local-First Approach</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information cascades are ubiquitous in various social networking web sites.
What mechanisms drive information diffuse in the networks? How does the
structure and size of the cascades evolve in time? When and which users will
adopt a certain message? Approaching these questions can considerably deepen
our understanding about information cascades and facilitate various vital
applications, including viral marketing, rumor prevention and even link
prediction. Most previous works focus only on the final cascade size
prediction. Meanwhile, they are always cascade graph dependent methods, which
make them towards large cascades prediction and lead to the criticism that
cascades may only be predictable after they have already grown large. In this
paper, we study a fundamental problem: full-scale cascade dynamics prediction.
That is, how to predict when and which users are activated at any time point of
a cascading process. Here we propose a unified framework, FScaleCP, to solve
the problem. Given history cascades, we first model the local spreading
behaviors as a classification problem. Through data-driven learning, we
recognize the common patterns by measuring the driving mechanisms of cascade
dynamics. After that we present an intuitive asynchronous propagation method
for full-scale cascade dynamics prediction by effectively aggregating the local
spreading behaviors. Extensive experiments on social network data set suggest
that the proposed method performs noticeably better than other state-of-the-art
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08456</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08456</id><created>2015-12-28</created><authors><author><keyname>Perez-Parras</keyname><forenames>Jessica</forenames></author><author><keyname>Gomez-Galan</keyname><forenames>Jose</forenames></author></authors><title>Knowledge and Influence of MOOC Courses on Initial Teacher Training</title><categories>cs.CY</categories><comments>19 pages, 4 figures</comments><journal-ref>International Journal of Educational Excellence, 2015, Vol. 1, No.
  2, 81-99, ISSN 2373-5929</journal-ref><doi>10.18562/ijee.2015.0008</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The impact of MOOC courses in the processes of distance learning has been
extremely important from the very beginning. They offer an innovative model of
massive teaching, which exploits in a paradigmatic manner the potential and
relevance that ICT's currently have in modern society. The present article has
as its primary objective the analysis of the presence of these courses and the
role that they represent in teacher training, and their knowledge and influence
on the future teachers that are currently being formed at university level. A
case study has been carried out with descriptive not experimental methodology,
from a quantitative base. The sample study has been undertaken in Spain
(n=200). Its main result being the determination of the minimal impact that the
MOOC phenomenon has had on the students polled. Equally, a significant lack of
knowledge has been revealed in all its dimensions (professional, pedagogical,
structural, etc.), with only a minority of those in the sample group having
indicated that they have studied any of the courses, or know to some extent the
main platforms of the world in which they are offered. A large number of those
surveyed therefore are unaware of the existence of these courses. As a result,
it has been established that, regardless of the quality of the learning and the
didactic and methodological characteristics that the MOOC courses offer, their
study and analysis is considered necessary for future educational
professionals. It is imperative that at the level of Higher Education, and
especially in the faculties of teacher training, that the most recent advances
in the field of ICT's are introduced in the study plan and in the academic
programs, for they constitute the base of modern society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08457</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08457</id><created>2015-12-28</created><authors><author><keyname>Leibo</keyname><forenames>Joel Z.</forenames></author><author><keyname>Cornebise</keyname><forenames>Julien</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Hassabis</keyname><forenames>Demis</forenames></author></authors><title>Approximate Hubel-Wiesel Modules and the Data Structures of Neural
  Computation</title><categories>cs.NE q-bio.NC</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a framework for modeling the interface between
perception and memory on the algorithmic level of analysis. It is consistent
with phenomena associated with many different brain regions. These include
view-dependence (and invariance) effects in visual psychophysics and
inferotemporal cortex physiology, as well as episodic memory recall
interference effects associated with the medial temporal lobe. The perspective
developed here relies on a novel interpretation of Hubel and Wiesel's
conjecture for how receptive fields tuned to complex objects, and invariant to
details, could be achieved. It complements existing accounts of two-speed
learning systems in neocortex and hippocampus (e.g., McClelland et al. 1995)
while significantly expanding their scope to encompass a unified view of the
entire pathway from V1 to hippocampus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08469</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08469</id><created>2015-12-28</created><authors><author><keyname>Caarls</keyname><forenames>Wouter</forenames></author><author><keyname>Hargreaves</keyname><forenames>Eduardo</forenames></author><author><keyname>Menasch&#xe9;</keyname><forenames>Daniel S.</forenames></author></authors><title>Q-caching: an integrated reinforcement-learning approach for caching and
  routing in information-centric networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content delivery, such as video streaming, is one of the most prevalent
Internet applications. Although very popular, the continuous growth of such
applications poses novel performance and scalability challenges.
Information-centric networks put content at the center, and propose novel
solutions to such challenges but also pose new questions on the interface
between caching and routing. In this paper, building on top of Q-routing we
propose a caching strategy, namely Q-caching, which leverages information that
is already collected by the routing algorithm. Q-caching promotes content
diversity in the network, reducing the load at custodians and average download
times for clients. In stylized topologies, we show that the gains of Q-caching
against state-of-the-art algorithms are significant. We then consider the RNP
topology, and show that Q-caching performance is more flexible while
competitive when compared against existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08475</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08475</id><created>2015-12-28</created><authors><author><keyname>Khosravi</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Mansouri</keyname><forenames>Suleiman</forenames></author><author><keyname>Keshavarz</keyname><forenames>Ahmad</forenames></author><author><keyname>Rostami</keyname><forenames>Habib</forenames></author></authors><title>MRF-based multispectral image fusion using an adaptive approach based on
  edge-guided interpolation</title><categories>cs.CV</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In interpretation of remote sensing images, it is possible that the images
which are supplied by different sensors wouldn't be understandable or we could
not get vital information from them. For better visual perception of images, it
is essential to operate series of pre-processing and elementary corrections and
then operate a series of main processing for more precise analysis on the
image. There are several approaches for processing which depend on type of
remote sensing image. The discussed approach in this article, i.e. image
fusion, is using natural colors of an optical image for adding color to
gray-scale satellite image which gives us the ability to better observe the HR
image of the OLI sensor of Landsat. This process previously with emphasis on
details of fusion technique was performed, but we are going to relieve concept
of interpolation process that did not have suitable attentions in past. In fact
we see many important software tools such as ENVI and ERDAS as most famous
remote sensing image processing software tools have only classical
interpolation techniques (such as BL and CC). Therefore ENVI-based and
ERDAS-based researches in image fusion area and even other fusion researches
often do not use new and better interpolations and only are concentrating on
fusion details for achievement of better quality, so we only focus on
interpolation impact in fusion quality in a specific application, i.e. Landsat
multi-spectral images. The important feature of this approach is using a
statistical, adaptive, edge-guided and MRF-based interpolation method for
improving color quality in MRF-based images with maintenance of high resolution
in practice. Numerical Simulations show selection of suitable interpolation
technique in MRF-based images creates better quality rather than classical
interpolations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08493</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08493</id><created>2015-12-24</created><updated>2015-12-29</updated><authors><author><keyname>Yao</keyname><forenames>Lina</forenames></author><author><keyname>Sheng</keyname><forenames>Quan Z.</forenames></author><author><keyname>Ngu</keyname><forenames>Anne H. H.</forenames></author><author><keyname>Gao</keyname><forenames>Byron J.</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author><author><keyname>Li</keyname><forenames>Xue</forenames></author></authors><title>Unveiling Contextual Similarity of Things via Mining Human-Thing
  Interactions in the Internet of Things</title><categories>cs.CY cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent advances in radio-frequency identification (RFID), wireless
sensor networks, and Web services, physical things are becoming an integral
part of the emerging ubiquitous Web. Finding correlations of ubiquitous things
is a crucial prerequisite for many important applications such as things
search, discovery, classification, recommendation, and composition. This
article presents DisCor-T, a novel graph-based method for discovering
underlying connections of things via mining the rich content embodied in
human-thing interactions in terms of user, temporal and spatial information. We
model these various information using two graphs, namely spatio-temporal graph
and social graph. Then, random walk with restart (RWR) is applied to find
proximities among things, and a relational graph of things (RGT) indicating
implicit correlations of things is learned. The correlation analysis lays a
solid foundation contributing to improved effectiveness in things management.
To demonstrate the utility, we develop a flexible feature-based classification
framework on top of RGT and perform a systematic case study. Our evaluation
exhibits the strength and feasibility of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08512</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08512</id><created>2015-12-28</created><authors><author><keyname>Owens</keyname><forenames>Andrew</forenames></author><author><keyname>Isola</keyname><forenames>Phillip</forenames></author><author><keyname>McDermott</keyname><forenames>Josh</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author><author><keyname>Adelson</keyname><forenames>Edward H.</forenames></author><author><keyname>Freeman</keyname><forenames>William T.</forenames></author></authors><title>Visually Indicated Sounds</title><categories>cs.CV cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Materials make distinctive sounds when they are hit or scratched -- dirt
makes a thud; ceramic makes a clink. These sounds reveal aspects of an object's
material properties, as well as the force and motion of the physical
interaction. In this paper, we introduce an algorithm that learns to synthesize
sound from videos of people hitting objects with a drumstick. The algorithm
uses a recurrent neural network to predict sound features from videos and then
produces a waveform from these features with an example-based synthesis
procedure. We demonstrate that the sounds generated by our model are realistic
enough to fool participants in a &quot;real or fake&quot; psychophysical experiment, and
that they convey significant information about the material properties in a
scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08515</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08515</id><created>2015-12-25</created><authors><author><keyname>Collier</keyname><forenames>Zachary A.</forenames><affiliation>US Army Engineer Research &amp; Development Center, Concord, MA, USA</affiliation></author><author><keyname>Panwar</keyname><forenames>Mahesh</forenames><affiliation>Contractor to US Army Engineer Research &amp; Development Center, Concord, MA, USA</affiliation></author><author><keyname>Ganin</keyname><forenames>Alexander A.</forenames><affiliation>University of Virginia, Charlottesville, VA, USA</affiliation></author><author><keyname>Kott</keyname><forenames>Alex</forenames><affiliation>US Army Research Laboratory, Adelphi, MD, USA</affiliation></author><author><keyname>Linkov</keyname><forenames>Igor</forenames><affiliation>US Army Engineer Research &amp; Development Center, Concord, MA, USA</affiliation></author></authors><title>Security Metrics in Industrial Control Systems</title><categories>cs.CR cs.CY cs.SE physics.data-an</categories><comments>Chapter in In: Colbert, E. and Kott, A. (eds.), &quot;Cyber Security of
  Industrial Control Systems, Including SCADA Systems,&quot; Springer, NY, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk is the best known and perhaps the best studied example within a much
broader class of cyber security metrics. However, risk is not the only possible
cyber security metric. Other metrics such as resilience can exist and could be
potentially very valuable to defenders of ICS systems. Often, metrics are
defined as measurable properties of a system that quantify the degree to which
objectives of the system are achieved. Metrics can provide cyber defenders of
an ICS with critical insights regarding the system. Metrics are generally
acquired by analyzing relevant attributes of that system. In terms of cyber
security metrics, ICSs tend to have unique features: in many cases, these
systems are older technologies that were designed for functionality rather than
security. They are also extremely diverse systems that have different
requirements and objectives. Therefore, metrics for ICSs must be tailored to a
diverse group of systems with many features and perform many different
functions. In this chapter, we first outline the general theory of performance
metrics, and highlight examples from the cyber security domain and ICS in
particular. We then focus on a particular example of a class of metrics that is
different from the one we have considered in earlier chapters. Instead of risk,
here we consider metrics of resilience. Resilience is defined by the National
Academy of Sciences (2012) as the ability to prepare and plan for, absorb,
recover from, or more successfully adapt to actual or potential adverse events.
This chapter presents two approaches for the generation of metrics based on the
concept of resilience using a matrix-based approach and a network-based
approach. Finally, a discussion of the benefits and drawbacks of different
methods is presented along with a process and tips intended to aid in devising
effective metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08518</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08518</id><created>2015-12-27</created><updated>2016-01-01</updated><authors><author><keyname>Cheng</keyname><forenames>Peng</forenames></author><author><keyname>Lian</keyname><forenames>Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Shahabi</keyname><forenames>Cyrus</forenames></author></authors><title>Prediction-Based Task Assignment on Spatial Crowdsourcing</title><categories>cs.DB</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of mobile devices, the spatial crowdsourcing has
recently attracted much attention from the database community. Specifically,
the spatial crowdsourcing refers to a system that automatically assigns a
number of location-based workers with spatial tasks nearby . Previous works on
the spatial crowdsourcing usually designed task assignment strategies that
maximize some assignment scores. However, their assignment strategies only
considered the existing workers and tasks in the spatial crowdsourcing system,
which might achieve local optimality, due to the unavailability of future
workers/tasks that may join the system. Thus, in this paper, our goal is to
achieve globally optimal task assignments at the current timestamp, by taking
into account not only the existing, but also those future workers/tasks. We
formalize an important problem, namely prediction-based spatial crowdsourcing
(PB-SC), which finds a global optimal strategy for worker-and-task assignments
for multiple rounds, based on both existing and predicted task/worker
locations, such that the total assignment quality score of multiple rounds is
maximized, under the constraint of the travel budget. The PB-SC problem is very
challenging, in terms of the prediction accuracy and efficiency. In this paper,
we design an effective prediction method to estimate spatial distributions of
workers and tasks in the future, and then utilize the predicted ones in our
procedure of worker-and-task assignments. We prove that, the PB-SC problem is
NP-hard, and thus intractable. Therefore, we propose efficient approximation
algorithms, greedy and divide-and-conquer, to deal with the PB-SC problem, by
considering both current and future task/worker distributions. Through
extensive experiments, we demonstrate the efficiency and effectiveness of our
PB-SC processing approaches on real/synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08525</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08525</id><created>2015-12-28</created><authors><author><keyname>AlJadda</keyname><forenames>Khalifeh</forenames></author><author><keyname>Korayem</keyname><forenames>Mohammed</forenames></author><author><keyname>Ortiz</keyname><forenames>Camilo</forenames></author><author><keyname>Grainger</keyname><forenames>Trey</forenames></author><author><keyname>Miller</keyname><forenames>John A.</forenames></author><author><keyname>Rasheed</keyname><forenames>Khaled</forenames></author><author><keyname>Kochut</keyname><forenames>Krys J.</forenames></author><author><keyname>York</keyname><forenames>William S.</forenames></author><author><keyname>Ranzinger</keyname><forenames>Rene</forenames></author><author><keyname>Porterfield</keyname><forenames>Melody</forenames></author></authors><title>Mining Massive Hierarchical Data Using a Scalable Probabilistic
  Graphical Model</title><categories>cs.AI</categories><comments>To be submitted to Big Data Journal. arXiv admin note: substantial
  text overlap with arXiv:1407.5656</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Graphical Models (PGM) are very useful in the fields of machine
learning and data mining. The crucial limitation of those models,however, is
the scalability. The Bayesian Network, which is one of the most common PGMs
used in machine learning and data mining, demonstrates this limitation when the
training data consists of random variables, each of them has a large set of
possible values. In the big data era, one would expect new extensions to the
existing PGMs to handle the massive amount of data produced these days by
computers, sensors and other electronic devices. With hierarchical data - data
that is arranged in a treelike structure with several levels - one would expect
to see hundreds of thousands or millions of values distributed over even just a
small number of levels. When modeling this kind of hierarchical data across
large data sets, Bayesian Networks become infeasible for representing the
probability distributions. In this paper we introduce an extension to Bayesian
Networks to handle massive sets of hierarchical data in a reasonable amount of
time and space. The proposed model achieves perfect precision of 1.0 and high
recall of 0.93 when it is used as multi-label classifier for the annotation of
mass spectrometry data. On another data set of 1.5 billion search logs provided
by CareerBuilder.com the model was able to predict latent semantic
relationships between search keywords with accuracy up to 0.80.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08540</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08540</id><created>2015-12-28</created><authors><author><keyname>Bross</keyname><forenames>Shraga I.</forenames></author><author><keyname>Laufer</keyname><forenames>Yaron</forenames></author></authors><title>Sending a Bivariate Gaussian Source Over a Gaussian MAC with
  Unidirectional Conferencing Encoders</title><categories>cs.IT math.IT</categories><comments>The paper has been accepted for publication in IT Transactions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the transmission of a memoryless bivariate Gaussian source over a
two-user additive Gaussian multiple-access channel with unidirectional
conferencing encoders. Here, prior to each transmission block, Encoder 1, which
observes the first source component, is allowed to communicate with Encoder 2,
which observes the second source component, via a unidirectional noise-free
bit-pipe of given capacity. The main results of this work are sufficient
conditions and a necessary condition for the achievability of a distortion pair
expressed as a function of the channel SNR and of the source correlation. The
main sufficient condition is obtained by an extension of the vector-quantizer
scheme suggested by Lapidoth-Tinguely, for the case without conferencing, to
the case with unidirectional conference. In the high-SNR regime, and when the
capacity of the conference channel is unlimited, these necessary and sufficient
conditions are shown to agree. We evaluate the precise high-SNR asymptotics for
a subset of distortion pairs when the capacity of the conference channel is
unlimited in which case we show that a separation based scheme attains these
optimal distortion pairs. However, with symmetric average-power constraints and
fixed conferencing capacity, at high-SNR the latter separation based scheme is
shown to be suboptimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08545</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08545</id><created>2015-12-28</created><authors><author><keyname>Dasari</keyname><forenames>Venkat R.</forenames></author><author><keyname>Humble</keyname><forenames>Travis S.</forenames></author></authors><title>OpenFlow Arbitrated Programmable Network Channels for Managing Quantum
  Metadata</title><categories>quant-ph cs.ET cs.NI</categories><comments>16 pages, comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-defined networking (SDN) offers robust and flexible strategies for
managing diverse network devices and uses. We adapt the principles of SDN to
the deployment of quantum networks, which are composed from unique devices that
operate according to the laws of quantum mechanics. Quantum networks must
classically exchange complex metadata between devices in order to carry out
information for protocols such as teleportation, super-dense coding, and
quantum key distribution. We show how quantum metadata can be managed within a
software-defined network using the OpenFlow protocol. We begin by defining a
quantum physical layer QPHY within the classical network stack to express the
quantum communications channel and we then describe how OpenFlow management of
classical optical channels is compatible with emerging quantum communication
protocols. We next give an example specification of the metadata needed to
manage and control QPHY behavior and we extend the OpenFlow interface to
accommodate this quantum metadata. We conclude by discussing near-term
experimental efforts that can realize SDN's principles for quantum
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08546</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08546</id><created>2015-12-28</created><updated>2016-03-01</updated><authors><author><keyname>Caliskan-Islam</keyname><forenames>Aylin</forenames></author><author><keyname>Yamaguchi</keyname><forenames>Fabian</forenames></author><author><keyname>Dauber</keyname><forenames>Edwin</forenames></author><author><keyname>Harang</keyname><forenames>Richard</forenames></author><author><keyname>Rieck</keyname><forenames>Konrad</forenames></author><author><keyname>Greenstadt</keyname><forenames>Rachel</forenames></author><author><keyname>Narayanan</keyname><forenames>Arvind</forenames></author></authors><title>When Coding Style Survives Compilation: De-anonymizing Programmers from
  Executable Binaries</title><categories>cs.CR</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to identify authors of computer programs based on their coding
style is a direct threat to the privacy and anonymity of programmers. Previous
work has examined attribution of authors from both source code and compiled
binaries, and found that while source code can be attributed with very high
accuracy, the attribution of executable binary appears to be much more
difficult. Many potentially distinguishing features present in source code,
e.g. variable names, are removed in the compilation process, and compiler
optimization may alter the structure of a program, further obscuring features
that are known to be useful in determining authorship.
  We examine executable binary authorship attribution from the standpoint of
machine learning, using a novel set of features that include ones obtained by
decompiling the executable binary to source code. We show that many syntactical
features present in source code do in fact survive compilation and can be
recovered from decompiled executable binary. This allows us to add a powerful
set of techniques from the domain of source code authorship attribution to the
existing ones used for binaries, resulting in significant improvements to
accuracy and scalability. We demonstrate this improvement on data from the
Google Code Jam, obtaining attribution accuracy of up to 92% with 100 candidate
programmers. We also demonstrate that our approach is robust to basic
obfuscations, a range of compiler optimization settings, and binaries that have
been stripped of their symbol tables. Finally, for the first time we are aware
of, we demonstrate that authorship attribution can be performed on both
obfuscated binaries, and real world code found &quot;in the wild&quot; by performing
attribution on single-author GitHub repositories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08553</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08553</id><created>2015-12-28</created><authors><author><keyname>Garn</keyname><forenames>Wolfgang</forenames></author><author><keyname>Louvieris</keyname><forenames>Panos</forenames></author></authors><title>Conditional probability generation methods for high reliability
  effects-based decision making</title><categories>cs.AI</categories><comments>18 pages, 3 figures</comments><msc-class>68T37, 62C12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision making is often based on Bayesian networks. The building blocks for
Bayesian networks are its conditional probability tables (CPTs). These tables
are obtained by parameter estimation methods, or they are elicited from subject
matter experts (SME). Some of these knowledge representations are insufficient
approximations. Using knowledge fusion of cause and effect observations lead to
better predictive decisions. We propose three new methods to generate CPTs,
which even work when only soft evidence is provided. The first two are novel
ways of mapping conditional expectations to the probability space. The third is
a column extraction method, which obtains CPTs from nonlinear functions such as
the multinomial logistic regression. Case studies on military effects and burnt
forest desertification have demonstrated that so derived CPTs have highly
reliable predictive power, including superiority over the CPTs obtained from
SMEs. In this context, new quality measures for determining the goodness of a
CPT and for comparing CPTs with each other have been introduced. The predictive
power and enhanced reliability of decision making based on the novel CPT
generation methods presented in this paper have been confirmed and validated
within the context of the case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08554</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08554</id><created>2015-12-28</created><authors><author><keyname>Schawe</keyname><forenames>Hendrik</forenames></author><author><keyname>Hartmann</keyname><forenames>Alexander K.</forenames></author></authors><title>Phase Transitions of Traveling Salesperson Problems solved with Linear
  Programming and Cutting Planes</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Traveling Salesperson problem asks for the shortest cyclic tour visiting
a set of cities given their pairwise distances and belongs to the NP-hard
complexity class, which means that with all known algorithms in the worst case
instances are not solveable in polynomial time, i.e., the problem is hard.
Though that does not mean, that there are not subsets of the problem which are
easy to solve. To examine numerically transitions from an easy to a hard phase,
a random ensemble of cities in the Euclidean plane given a parameter {\sigma},
which governs the hardness, is introduced. Here, a linear programming approach
together with suitable cutting planes is applied. Such algorithms operate
outside the space of feasible solutions and are often used in practical
application but rarely studied in physics so far. We observe several
transitions. To characterize these transitions, scaling assumptions from
continuous phase transitions are applied
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08555</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08555</id><created>2015-12-28</created><authors><author><keyname>Turner</keyname><forenames>Jonathan</forenames></author></authors><title>Maximium Priority Matchings</title><categories>cs.DS</categories><report-no>WUCSE-2015-06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be an undirected graph with $n$ vertices and $m$ edges, in
which each vertex $u$ is assigned an integer priority in $[1,n]$, with 1 being
the &quot;highest&quot; priority. Let $M$ be a matching of $G$. We define the priority
score of $M$ to be an $n$-ary integer in which the $i$-th most-significant
digit is the number of vertices with priority $i$ that are incident to an edge
in $M$. We describe a variation of the augmenting path method (Edmonds'
algorithm) that finds a matching with maximum priority score in $O(mn)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08562</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08562</id><created>2015-12-28</created><authors><author><keyname>Fox</keyname><forenames>Roy</forenames></author><author><keyname>Pakman</keyname><forenames>Ari</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>G-Learning: Taming the Noise in Reinforcement Learning via Soft Updates</title><categories>cs.LG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-free reinforcement learning algorithms such as Q-learning perform
poorly in the early stages of learning in noisy environments, because much
effort is spent on unlearning biased estimates of the state-action function.
The bias comes from selecting, among several noisy estimates, the apparent
optimum, which may actually be suboptimal. We propose G-learning, a new
off-policy learning algorithm that regularizes the noise in the space of
optimal actions by penalizing deterministic policies at the beginning of the
learning. Moreover, it enables naturally incorporating prior distributions over
optimal actions when available. The stochastic nature of G-learning also makes
it more cost-effective than Q-learning in noiseless but exploration-risky
domains. We illustrate these ideas in several examples where G-learning results
in significant improvements of the learning rate and the learning cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08569</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08569</id><created>2015-12-28</created><authors><author><keyname>Bilisoly</keyname><forenames>Roger</forenames></author></authors><title>Analyzing Walter Skeat's Forty-Five Parallel Extracts of William
  Langland's Piers Plowman</title><categories>stat.AP cs.CL</categories><comments>Presented at the Joint Statistical Meetings 2015 in Seattle,
  Washington. 9 pages long</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Walter Skeat published his critical edition of William Langland's 14th
century alliterative poem, Piers Plowman, in 1886. In preparation for this he
located forty-five manuscripts, and to compare dialects, he published excerpts
from each of these. This paper does three statistical analyses using these
excerpts, each of which mimics a task he did in writing his critical edition.
First, he combined multiple versions of a poetic line to create a best line,
which is compared to the mean string that is computed by a generalization of
the arithmetic mean that uses edit distance. Second, he claims that a certain
subset of manuscripts varies little. This is quantified by computing a string
variance, which is closely related to the above generalization of the mean.
Third, he claims that the manuscripts fall into three groups, which is a
clustering problem that is addressed by using edit distance. The overall goal
is to develop methodology that would be of use to a literary critic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08571</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08571</id><created>2015-12-28</created><authors><author><keyname>Anwar</keyname><forenames>Sajid</forenames></author><author><keyname>Hwang</keyname><forenames>Kyuyeon</forenames></author><author><keyname>Sung</keyname><forenames>Wonyong</forenames></author></authors><title>Structured Pruning of Deep Convolutional Neural Networks</title><categories>cs.NE cs.LG stat.ML</categories><comments>11 pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real time application of deep learning algorithms is often hindered by high
computational complexity and frequent memory accesses. Network pruning is a
promising technique to solve this problem. However, pruning usually results in
irregular network connections that not only demand extra representation efforts
but also do not fit well on parallel computation. We introduce structured
sparsity at various scales for convolutional neural networks, which are channel
wise, kernel wise and intra kernel strided sparsity. This structured sparsity
is very advantageous for direct computational resource savings on embedded
computers, parallel computing environments and hardware based systems. To
decide the importance of network connections and paths, the proposed method
uses a particle filtering approach. The importance weight of each particle is
assigned by computing the misclassification rate with corresponding
connectivity pattern. The pruned network is re-trained to compensate for the
losses due to pruning. While implementing convolutions as matrix products, we
particularly show that intra kernel strided sparsity with a simple constraint
can significantly reduce the size of kernel and feature map matrices. The
pruned network is finally fixed point optimized with reduced word length
precision. This results in significant reduction in the total storage size
providing advantages for on-chip memory based implementations of deep neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08575</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08575</id><created>2015-12-28</created><authors><author><keyname>Fox</keyname><forenames>Roy</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>Optimal Selective Attention in Reactive Agents</title><categories>cs.LG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In POMDPs, information about the hidden state, delivered through
observations, is both valuable to the agent, allowing it to base its actions on
better informed internal states, and a &quot;curse&quot;, exploding the size and
diversity of the internal state space. One attempt to deal with this is to
focus on reactive policies, that only base their actions on the most recent
observation. However, even reactive policies can be demanding on resources, and
agents need to pay selective attention to only some of the information
available to them in observations. In this report we present the
minimum-information principle for selective attention in reactive agents. We
further motivate this approach by reducing the general problem of optimal
control in POMDPs, to reactive control with complex observations. Lastly, we
explore a newly discovered phenomenon of this optimization process - period
doubling bifurcations. This necessitates periodic policies, and raises many
more questions regarding stability, periodicity and chaos in optimal control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08580</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08580</id><created>2015-12-28</created><authors><author><keyname>Wang</keyname><forenames>Hongjian</forenames></author><author><keyname>Li</keyname><forenames>Zhenhui</forenames></author><author><keyname>Kuo</keyname><forenames>Yu-Hsuan</forenames></author><author><keyname>Kifer</keyname><forenames>Dan</forenames></author></authors><title>A Simple Baseline for Travel Time Estimation using Large-Scale Trip Data</title><categories>cs.LG cs.CY</categories><comments>12 pages</comments><acm-class>H.2.8; I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The increased availability of large-scale trajectory data around the world
provides rich information for the study of urban dynamics. For example, New
York City Taxi Limousine Commission regularly releases source-destination
information about trips in the taxis they regulate. Taxi data provide
information about traffic patterns, and thus enable the study of urban flow --
what will traffic between two locations look like at a certain date and time in
the future? Existing big data methods try to outdo each other in terms of
complexity and algorithmic sophistication. In the spirit of &quot;big data beats
algorithms&quot;, we present a very simple baseline which outperforms
state-of-the-art approaches, including Bing Maps and Baidu Maps (whose APIs
permit large scale experimentation). Such a travel time estimation baseline has
several important uses, such as navigation (fast travel time estimates can
serve as approximate heuristics for A search variants for path finding) and
trip planning (which uses operating hours for popular destinations along with
travel time estimates to create an itinerary).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08602</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08602</id><created>2015-12-29</created><authors><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author><author><keyname>Vladu</keyname><forenames>Adrian</forenames></author><author><keyname>Wong</keyname><forenames>Sam Chiu-wai</forenames></author></authors><title>Tight Bounds for Approximate Carath\'eodory and Beyond</title><categories>cs.DS cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a deterministic nearly-linear time algorithm for approximating any
point inside a convex polytope with a sparse convex combination of the
polytope's vertices. Our result provides a constructive proof for the
Approximate Carath\'{e}odory Problem, which states that any point inside a
polytope contained in the $\ell_p$ ball of radius $D$ can be approximated to
within $\epsilon$ in $\ell_p$ norm by a convex combination of only $O\left(D^2
p/\epsilon^2\right)$ vertices of the polytope for $p \geq 2$. We also show that
this bound is tight, using an argument based on anti-concentration for the
binomial distribution.
  Along the way of establishing the upper bound, we develop a technique for
minimizing norms over convex sets with complicated geometry; this is achieved
by running Mirror Descent on a dual convex function obtained via Sion's
Theorem.
  As simple extensions of our method, we then provide new algorithms for
submodular function minimization and SVM training. For submodular function
minimization we obtain a simplification and (provable) speed-up over Wolfe's
algorithm, the method commonly found to be the fastest in practice. For SVM
training, we obtain $O(1/\epsilon^2)$ convergence for arbitrary kernels; each
iteration only requires matrix-vector operations involving the kernel matrix,
so we overcome the obstacle of having to explicitly store the kernel or compute
its Cholesky factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08609</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08609</id><created>2015-12-29</created><authors><author><keyname>Kumar</keyname><forenames>Muthukrishnan Senthil</forenames></author><author><keyname>Dadlani</keyname><forenames>Aresh</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Performance Analysis of an Unreliable $M/G/1$ Retrial Queue with Coupled
  Switching</title><categories>cs.NI cs.PF</categories><comments>Submitted to Operations Research Letters on September 1, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the stationary characteristics of an $M/G/1$ retrial queue
where the server, subject to active failures, primarily attends incoming calls
and directs outgoing calls only when idle. On finding the server unavailable
(busy or failed), inbound calls join the orbit and reattempt for service at
exponentially-distributed time intervals. The system stability condition and
probability generating functions of the number of calls in orbit and system are
derived and evaluated numerically in the context of mean system size, server
availability, failure frequency, and orbit waiting time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08622</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08622</id><created>2015-12-29</created><authors><author><keyname>Steila</keyname><forenames>Silvia</forenames></author><author><keyname>Yokoyama</keyname><forenames>Keita</forenames></author></authors><title>Reverse Mathematical Bounds for the Termination Theorem</title><categories>math.LO cs.LO</categories><comments>30 pages</comments><msc-class>03B30, 03F35, 03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2004 Podelski and Rybalchenko expressed the termination of
transition-based programs as a property of well-founded relations. The
classical proof by Podelski and Rybalchenko requires Ramsey's Theorem for pairs
which is a purely classical result, therefore extracting bounds from the
original proof is non-trivial task. Our goal is to investigate the termination
analysis from the point of view of Reverse Mathematics. By studying the
strength of Podelski and Rybalchenko's Termination Theorem we can extract some
information about termination bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08626</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08626</id><created>2015-12-29</created><authors><author><keyname>Nemirovsky</keyname><forenames>Danil</forenames></author></authors><title>Block Advertisement Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin, a decentralized cryptocurrency, has attracted a lot of attention
from academia, financial service industry and enthusiasts. The trade-off
between transaction confirmation throughput and centralization of hash power do
not allow Bitcoin to perform at the same level as modern payment systems. Block
Advertisement Protocol is proposed as a step to resolve this issue. The
protocol allows block mining and block relaying to happen in parallel. The
protocol dictates a miner to advertise the block it is going to mine allowing
other miners to collect all the transactions in advance. When a block in mined,
only header is relayed since most of data about the block is already known to
all miners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08634</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08634</id><created>2015-12-29</created><authors><author><keyname>Wang</keyname><forenames>Hongyu</forenames></author><author><keyname>Xu</keyname><forenames>Jin</forenames></author><author><keyname>Yao</keyname><forenames>Bing</forenames></author></authors><title>Improper Graceful and Odd-graceful Labellings of Graph Theory</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define some new labellings for trees, called the in-improper
and out-improper odd-graceful labellings such that some trees labelled with the
new labellings can induce graceful graphs having at least a cycle. We, next,
apply the new labellings to construct large scale of graphs having improper
graceful/odd-graceful labellings or having graceful/odd-graceful labellings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08646</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08646</id><created>2015-12-29</created><authors><author><keyname>Kathiravelu</keyname><forenames>Pradeeban</forenames></author><author><keyname>Veiga</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>Not Every Flow is Equal: SMART Discrimination in Redundancy</title><categories>cs.NI cs.DC</categories><comments>INESC-ID Tec. Rep. 17/2015, October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-Defined Data Centers (SDDC) extend virtualization, software-defined
networking and systems, and middleboxes to provide a better quality of service
(QoS). While many network flow routing algorithms exist, most of them fail to
adapt to the dynamic nature of the data center and cloud networks and their
users' and enterprise requirements. This paper presents SMART, a
Software-Defined Networking (SDN) middlebox architecture for reliable
transfers. As an architectural enhancement for network flows allocation,
routing, and control, SMART ensures timely delivery of flows by diverting them
to a less congested path dynamically in the software-defined data center
networks. SMART also clones packets of higher priority flows to route in an
alternative path, along with the original flow. Hence SMART offers a
differentiated QoS through varying levels of redundancy in the flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08648</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08648</id><created>2015-12-29</created><authors><author><keyname>Kurzejamski</keyname><forenames>Grzegorz</forenames></author><author><keyname>Zawistowski</keyname><forenames>Jacek</forenames></author><author><keyname>Sarwas</keyname><forenames>Grzegorz</forenames></author></authors><title>A framework for robust object multi-detection with a vote aggregation
  and a cascade filtering</title><categories>cs.CV</categories><comments>23rd International Conference in Central Europe on Computer Graphics,
  Visualization and Computer Vision (WSCG) 2015 Short Paper. Computer Science
  Research Notes CSRN 2502, ISSN 2464-4617, ISBN 978-80-86943-66-4, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework designed for the multi-object detection
purposes and adjusted for the application of product search on the market
shelves. The framework uses a single feedback loop and a pattern resizing
mechanism to demonstrate the top effectiveness of the state-of-the-art local
features. A high detection rate with a low false detection chance can be
achieved with use of only one pattern per object and no manual parameters
adjustments. The method incorporates well known local features and a basic
matching process to create a reliable voting space. Further steps comprise of
metric transformations, graphical vote space representation, two-phase vote
aggregation process and a cascade of verifying filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08650</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08650</id><created>2015-12-29</created><authors><author><keyname>Molkaraie</keyname><forenames>Mehdi</forenames></author></authors><title>An Importance Sampling Scheme for Models in a Strong External Field</title><categories>physics.comp-ph cs.IT math.IT stat.CO</categories><comments>Proc. IEEE Int. Symp. on Information Theory (ISIT), Hong Kong, June
  14-19, 2015. arXiv admin note: text overlap with arXiv:1401.4912</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Monte Carlo methods to estimate the partition function of the
two-dimensional Ising model in the presence of an external magnetic field. The
estimation is done in the dual of the Forney factor graph representing the
model. The proposed methods can efficiently compute an estimate of the
partition function in a wide range of model parameters. As an example, we
consider models that are in a strong external field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08652</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08652</id><created>2015-12-29</created><authors><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panos</forenames></author></authors><title>Pairwise Secret Key Agreement based on Location-derived Common
  Randomness</title><categories>cs.IT math.IT</categories><comments>A shorter version of this paper is accepted to ISZ 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A source model of key sharing between three users is considered in which each
pair of them wishes to agree on a secret key hidden from the remaining user.
There are rate-limited public channels for communications between the users. We
give an inner bound on the secret key capacity region in this framework.
Moreover, we investigate a practical setup in which localization information of
the users as the correlated observations are exploited to share pairwise keys
between the users. The inner and outer bounds of the key capacity region are
analyzed in this setup for the case of i.i.d. Gaussian observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08657</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08657</id><created>2015-12-29</created><authors><author><keyname>Wang</keyname><forenames>Xiaomin</forenames></author><author><keyname>Yao</keyname><forenames>Bing</forenames></author><author><keyname>Xu</keyname><forenames>Jin</forenames></author></authors><title>The Join of Scale-free Network Models</title><categories>cs.SI math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on constructing the domi-join model by doing the join operation
based on two smallest dominating sets of two network models and analysis the
properties of domi-join model, such as power law distribution, small world.
Besides, we will import two class of edge-bound growing network models to
explain the process of domi-join model. Then we compute the average degree,
clustering coefficient, power law distribution of the domi-join model. Finally,
we discuss an impressive method for cutting down redundant operation of
domi-join model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08669</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08669</id><created>2015-12-29</created><authors><author><keyname>Wang</keyname><forenames>Da-Han</forenames></author><author><keyname>Wang</keyname><forenames>Hanzi</forenames></author><author><keyname>Zhang</keyname><forenames>Dong</forenames></author><author><keyname>Li</keyname><forenames>Jonathan</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Robust Scene Text Recognition Using Sparse Coding based Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an effective scene text recognition method using
sparse coding based features, called Histograms of Sparse Codes (HSC) features.
For character detection, we use the HSC features instead of using the
Histograms of Oriented Gradients (HOG) features. The HSC features are extracted
by computing sparse codes with dictionaries that are learned from data using
K-SVD, and aggregating per-pixel sparse codes to form local histograms. For
word recognition, we integrate multiple cues including character detection
scores and geometric contexts in an objective function. The final recognition
results are obtained by searching for the words which correspond to the maximum
value of the objective function. The parameters in the objective function are
learned using the Minimum Classification Error (MCE) training method.
Experiments on several challenging datasets demonstrate that the proposed
HSC-based scene text recognition method outperforms HOG-based methods
significantly and outperforms most state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08679</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08679</id><created>2015-12-29</created><authors><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panos</forenames></author></authors><title>Key Agreement over an Interference Channel with Noiseless Feedback:
  Achievable Region &amp; Distributed Allocation</title><categories>cs.IT math.IT</categories><comments>A shorter version of this paper was presented at CNS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secret key establishment leveraging the physical layer as a source of common
randomness has been investigated in a range of settings. We investigate the
problem of establishing, in an information-theoretic sense, a secret key
between a user and a base-station (BS) (more generally, part of a wireless
infrastructure), but for two such user-BS pairs attempting the key
establishment simultaneously. The challenge in this novel setting lies in that
a user can eavesdrop another BS-user communications. It is thus paramount to
ensure the two keys are established with no leakage to the other user, in spite
the interference across neighboring cells. We model the system with BS-user
communication through an interference channel and user-BS communication through
a public channel. We find the region including achievable secret key rates for
the general case that the interference channel (IC) is discrete and memoryless.
Our results are examined for a Gaussian IC. In this setup, we investigate the
performance of different transmission schemes for power allocation. The chosen
transmission scheme by each BS essentially affects the secret key rate of the
other BS-user. Assuming base stations are trustworthy but that they seek to
maximize the corresponding secret key rate, a game-theoretic setting arises to
analyze the interaction between the base stations.We model our key agreement
scenario in normal form for different power allocation schemes to understand
performance without cooperation. Numerical simulations illustrate the
inefficiency of the Nash equilibrium outcome and motivate further research on
cooperative or coordinated schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08689</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08689</id><created>2015-12-29</created><updated>2016-01-06</updated><authors><author><keyname>Brockschmidt</keyname><forenames>Marc</forenames></author><author><keyname>Cook</keyname><forenames>Byron</forenames></author><author><keyname>Ishtiaq</keyname><forenames>Samin</forenames></author><author><keyname>Khlaaf</keyname><forenames>Heidy</forenames></author><author><keyname>Piterman</keyname><forenames>Nir</forenames></author></authors><title>T2: Temporal Property Verification</title><categories>cs.LO</categories><comments>Full version of TACAS'16 tool paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the open-source tool T2, the first public release from the
TERMINATOR project. T2 has been extended over the past decade to support
automatic temporal-logic proving techniques and to handle a general class of
user-provided liveness and safety properties. Input can be provided in a native
format and in C, via the support of the LLVM compiler framework. We briefly
discuss T2's architecture, its underlying techniques, and conclude with an
experimental illustration of its competitiveness and directions for future
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08710</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08710</id><created>2015-12-29</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>de Bianchi</keyname><forenames>Massimiliano Sassoli</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>On the Foundations of the Brussels Operational-Realistic Approach to
  Cognition</title><categories>cs.AI quant-ph</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scientific community is becoming more and more interested in the research
that applies the mathematical formalism of quantum theory to model human
decision-making. In this paper, we provide the theoretical foundations of the
quantum approach to cognition that we developed in Brussels. These foundations
rest on the results of two decade studies on the axiomatic and
operational-realistic approaches to the foundations of quantum physics. The
deep analogies between the foundations of physics and cognition lead us to
investigate the validity of quantum theory as a general and unitary framework
for cognitive processes, and the empirical success of the Hilbert space models
derived by such investigation provides a strong theoretical confirmation of
this validity. However, two situations in the cognitive realm, 'question order
effects' and 'response replicability', indicate that even the Hilbert space
framework could be insufficient to reproduce the collected data. This does not
mean that the mentioned operational-realistic approach would be incorrect, but
simply that a larger class of measurements would be in force in human
cognition, so that an extended quantum formalism may be needed to deal with all
of them. As we will explain, the recently derived 'extended Bloch
representation' of quantum theory (and the associated 'general
tension-reduction' model) precisely provides such extended formalism, while
remaining within the same unitary interpretative framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08716</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08716</id><created>2015-12-26</created><updated>2016-01-05</updated><authors><author><keyname>Asor</keyname><forenames>Ohad</forenames></author><author><keyname>Carmi</keyname><forenames>Avishy</forenames></author></authors><title>On Approximating Univariate NP-Hard Integrals</title><categories>cs.NA cs.CC</categories><comments>Need to show more evidence to the claims</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximating a definite integral of product of cosines to within an accuracy
of n binary digits where the integrand depends on input integers x[k] given in
binary radix, is equivalent to counting the number of equal-sum partitions of
the integers and is thus a #P problem. Similarly, integrating this function
from zero to infinity and deciding whether the result is either zero or
infinity is an NP-Complete problem. Efficient numerical integration methods
such as the double exponential formula and the sinc approximation have been
around since the mid 70's. Noting the hardness of approximating the integral we
argue that the proven rates of convergence of such methods cannot possibly be
correct since they give rise to an anomalous result as P=#P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08724</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08724</id><created>2015-12-29</created><authors><author><keyname>Pinto</keyname><forenames>Sebasti&#xe1;n</forenames></author><author><keyname>Balenzuela</keyname><forenames>Pablo</forenames></author><author><keyname>Dorso</keyname><forenames>Claudio O.</forenames></author></authors><title>Setting the Agenda: Different strategies of a Mass Media in a model of
  cultural dissemination</title><categories>physics.soc-ph cs.SI</categories><comments>23 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Day by day, people exchange opinions about a given new with relatives,
friends, and coworkers. In most cases, they get informed about a given issue by
reading newspapers, listening to the radio, or watching TV, i.e., through a
Mass Media (MM). However, the importance of a given new can be stimulated by
the Media by assigning newspaper's pages or time in TV programs. In this sense,
we say that the Media has the power to &quot;set the agenda&quot;, i.e., it decides which
new is important and which is not. On the other hand, the Media can know
people's concerns through, for instance, websites or blogs where they express
their opinions, and then it can use this information in order to be more
appealing to an increasing number of people. In this work, we study different
scenarios in an agent-based model of cultural dissemination, in which a given
Mass Media has a specific purpose: To set a particular topic of discussion and
impose its point of view to as many social agents as it can. We model this by
making the Media has a fixed feature, representing its point of view in the
topic of discussion, while it tries to attract new consumers, by taking
advantage of feedback mechanisms, represented by adaptive features. We explore
different strategies that the Media can adopt in order to increase the affinity
with potential consumers and then the probability to be successful in imposing
this particular topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08751</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08751</id><created>2015-12-29</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>Evolution of quasi-characteristic functions in quantum stochastic
  systems with Weyl quantization of energy operators</title><categories>math-ph cs.SY math.MP math.PR quant-ph</categories><comments>26 pages, 1 figure, submitted to Russian Journal of Mathematical
  Physics</comments><msc-class>81S22, 81S25, 81S30, 81P16, 81S05, 81Q15, 35Q40, 37M25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers open quantum systems whose dynamic variables satisfy
canonical commutation relations and are governed by Markovian
Hudson-Parthasarathy quantum stochastic differential equations driven by
external bosonic fields. The dependence of the Hamiltonian and the system-field
coupling operators on the system variables is represented using the Weyl
functional calculus. This leads to an integro-differential equation (IDE) for
the evolution of the quasi-characteristic function (QCF) which encodes the
dynamics of mixed moments of the system variables. Unlike quantum master
equations for reduced density operators, this IDE involves only complex-valued
functions on finite-dimensional Euclidean spaces and extends the Wigner-Moyal
phase-space approach for quantum stochastic systems. The dynamics of the QCF
and the related Wigner quasi-probability density function (QPDF) are discussed
in more detail for the case when the coupling operators depend linearly on the
system variables and the Hamiltonian has a nonquadratic part represented in the
Weyl quantization form. For this class of quantum stochastic systems, we also
consider an approximate computation of invariant states and discuss the
deviation from Gaussian quantum states in terms of the $\chi^2$-divergence (or
the second-order Renyi relative entropy) applied to the QPDF. The results of
the paper may find applications to investigating different aspects of the
moment stability, relaxation dynamics and invariant states in open quantum
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08754</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08754</id><created>2015-12-29</created><authors><author><keyname>Smolinsky</keyname><forenames>Lawrence</forenames></author></authors><title>Discrete power law with exponential cutoff and Lotka's Law</title><categories>cs.DL physics.data-an</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first bibliometric law appeared in Alfred J. Lotka's 1926 examination of
author productivity in chemistry and physics. The result is that the
productivity distribution is thought to be described by a power law. In this
paper, Lotka's original data on author productivity in chemistry is
reconsidered by comparing the fit of the data to both a discrete power law and
a discrete power law with exponential cutoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08756</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08756</id><created>2015-12-29</created><updated>2016-01-07</updated><authors><author><keyname>Raffel</keyname><forenames>Colin</forenames></author><author><keyname>Ellis</keyname><forenames>Daniel P. W.</forenames></author></authors><title>Feed-Forward Networks with Attention Can Solve Some Long-Term Memory
  Problems</title><categories>cs.LG cs.NE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recurrent neural networks (RNNs) have proven to be powerful models in
problems involving sequential data. Recently, RNNs have been augmented with
&quot;attention&quot; mechanisms which allow the network to focus on different parts of
an input sequence when computing their output. We propose a simplified model of
attention which is applicable to feed-forward neural networks and demonstrate
that it can solve some long-term memory problems (specifically, those where
temporal order doesn't matter). In fact, we show empirically that our model can
solve these problems for sequence lengths which are both longer and more widely
varying than the best results attained with RNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08757</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08757</id><created>2015-12-29</created><authors><author><keyname>Corr&#xea;a</keyname><forenames>Ricardo C.</forenames></author><author><keyname>Donne</keyname><forenames>Diego Delle</forenames></author><author><keyname>Koch</keyname><forenames>Ivo</forenames></author><author><keyname>Marenco</keyname><forenames>Javier</forenames></author></authors><title>General Cut-Generating Procedures for the Stable Set Polytope</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose general separation procedures for generating cuts for the stable
set polytope, inspired by a procedure by Rossi and Smriglio and applying a
lifting method by Xavier and Camp\^{e}lo. In contrast to existing
cut-generating procedures, ours generate both rank and non-rank valid
inequalities, hence they are of a more general nature than existing methods.
This is accomplished by iteratively solving a lifting problem, which consists
of a maximum weighted stable set problem on a smaller graph. Computational
experience on DIMACS benchmark instances shows that the proposed approach may
be a useful tool for generating cuts for the stable set polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08787</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08787</id><created>2015-12-29</created><authors><author><keyname>Ganti</keyname><forenames>Ravi</forenames></author><author><keyname>Balzano</keyname><forenames>Laura</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Matrix Completion Under Monotonic Single Index Models</title><categories>stat.ML cs.LG</categories><comments>21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most recent results in matrix completion assume that the matrix under
consideration is low-rank or that the columns are in a union of low-rank
subspaces. In real-world settings, however, the linear structure underlying
these models is distorted by a (typically unknown) nonlinear transformation.
This paper addresses the challenge of matrix completion in the face of such
nonlinearities. Given a few observations of a matrix that are obtained by
applying a Lipschitz, monotonic function to a low rank matrix, our task is to
estimate the remaining unobserved entries. We propose a novel matrix completion
method that alternates between low-rank matrix estimation and monotonic
function estimation to estimate the missing matrix elements. Mean squared error
bounds provide insight into how well the matrix can be estimated based on the
size, rank of the matrix and properties of the nonlinear transformation.
Empirical results on synthetic and real-world datasets demonstrate the
competitiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08790</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08790</id><created>2015-12-26</created><authors><author><keyname>Anidu</keyname><forenames>A. O.</forenames></author><author><keyname>Arekete</keyname><forenames>S. A.</forenames></author><author><keyname>Adedayo</keyname><forenames>A. O.</forenames></author><author><keyname>Adekoya</keyname><forenames>A. O.</forenames></author></authors><title>Dynamic Computation of Runge Kutta Fourth Order Algorithm for First and
  Second Order Ordinary Differential Equation Using Java</title><categories>cs.MS</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential equations arise in mathematics, physics,medicine, pharmacology,
communications, image processing and animation, etc. An Ordinary Differential
Equation (ODE) is a differential equation if it involves derivatives with
respect to only one independent variable which can be studied from different
perspectives, such as: analytical methods, graphical methods and numerical
methods. This research paper therefore revises the standard Runge - Kutta
fourth order algorithm by using compiler techniques to dynamically evaluate the
inputs and implement the algorithm for both first and second order derivatives
of the ODE. We have been able to develop and implement the software that can be
used to evaluate inputs and compute solutions (approximately and analytically)
for the ODE function at a more efficient rate than the traditional method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08799</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08799</id><created>2015-12-29</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Sun</keyname><forenames>Maoyuan</forenames></author><author><keyname>Mi</keyname><forenames>Peng</forenames></author><author><keyname>Tatti</keyname><forenames>Nikolaj</forenames></author><author><keyname>North</keyname><forenames>Chris</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Interactive Discovery of Coordinated Relationship Chains with Maximum
  Entropy Models</title><categories>cs.DB cs.HC</categories><comments>The journal version of paper is submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern visual analytic tools promote human-in-the-loop analysis but are
limited in their ability to direct the user toward interesting and promising
directions of study. This problem is especially acute when the analysis task is
exploratory in nature, e.g., the discovery of potentially coordinated
relationships in massive text datasets. Such tasks are very common in domains
like intelligence analysis and security forensics where the goal is to uncover
surprising coalitions bridging multiple types of relations. We introduce new
maximum entropy models to discover surprising chains of relationships
leveraging count data about entity occurrences in documents. These models are
embedded in a visual analytic system called MERCER that treats relationship
bundles as first class objects and directs the user toward promising lines of
inquiry. We demonstrate how user input can judiciously direct analysis toward
valid conclusions whereas a purely algorithmic approach could be led astray.
Experimental results on both synthetic and real datasets from the intelligence
community are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08806</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08806</id><created>2015-12-29</created><authors><author><keyname>Shaham</keyname><forenames>Uri</forenames></author><author><keyname>Lederman</keyname><forenames>Roy</forenames></author></authors><title>Common Variable Learning and Invariant Representation Learning using
  Siamese Neural Networks</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an Artificial Neural Network (ANN) approach for discovering common
hidden variables and for learning of invariant representations. This approach
uses synchronicity and concurrence to recognize desired structures in data and
discard superfluous structures, in the absence of labels and a data generation
model. In the common variable discovery problem, the ANN uses measurements from
two distinct sensors to construct a representation of the common hidden
variable that is manifested in both sensors, and discards sensor-specific
variables. In the invariant representation learning problem, the network uses
multiple observations of objects under transformations to construct a
representation which is invariant to the transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08808</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08808</id><created>2015-12-29</created><authors><author><keyname>Bunte</keyname><forenames>Kerstin</forenames></author><author><keyname>Lepp&#xe4;aho</keyname><forenames>Eemeli</forenames></author><author><keyname>Saarinen</keyname><forenames>Inka</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Sparse group factor analysis for biclustering of multiple data sources</title><categories>cs.LG cs.IR stat.ML</categories><comments>8 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Modelling methods that find structure in data are necessary with
the current high volumes of genomic data, and there have been various efforts
to find subsets of genes exhibiting consistent patterns over subsets of
treatments. These biclustering techniques have usually focused on gene
expression data only. We present a Bayesian approach for joint biclustering of
multiple data sources, enabling data-driven detection of linear structure
present in parts of the data sources. Results: Our simulation studies show that
the proposed method reliably infers biclusters from heterogeneous data sources.
We then tested the method on data from the NCI-DREAM drug sensitivity
prediction challenge, resulting in an excellent prediction accuracy. Moreover,
the predictions are based on several biclusters which provide insight into the
data sources, in this case on gene expression, DNA methylation, protein
abundance, exome sequence, functional connectivity fingerprints and drug
sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08811</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08811</id><created>2015-12-29</created><authors><author><keyname>Szwed</keyname><forenames>Piotr</forenames></author></authors><title>Combining Fuzzy Cognitive Maps and Discrete Random Variables</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an extension to the Fuzzy Cognitive Maps (FCMs) that
aims at aggregating a number of reasoning tasks into a one parallel run. The
described approach consists in replacing real-valued activation levels of
concepts (and further influence weights) by random variables. Such extension,
followed by the implemented software tool, allows for determining ranges
reached by concept activation levels, sensitivity analysis as well as
statistical analysis of multiple reasoning results. We replace multiplication
and addition operators appearing in the FCM state equation by appropriate
convolutions applicable for discrete random variables. To make the model
computationally feasible, it is further augmented with aggregation operations
for discrete random variables. We discuss four implemented aggregators, as well
as we report results of preliminary tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08814</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08814</id><created>2015-12-29</created><authors><author><keyname>Al-Kadi</keyname><forenames>Omar</forenames></author></authors><title>Combined statistical and model based texture features for improved image
  classification</title><categories>cs.CV</categories><comments>4th International Conference on Advances in Medical, Signal and
  Information Processing, pp. 175-178, Italy, 2008</comments><doi>10.1049/cp:20080455</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to improve the accuracy of texture classification based on
extracting texture features using five different texture methods and
classifying the patterns using a naive Bayesian classifier. Three
statistical-based and two model-based methods are used to extract texture
features from eight different texture images, then their accuracy is ranked
after using each method individually and in pairs. The accuracy improved up to
97.01% when model based -Gaussian Markov random field (GMRF) and fractional
Brownian motion (fBm) - were used together for classification as compared to
the highest achieved using each of the five different methods alone; and proved
to be better in classifying as compared to statistical methods. Also, using
GMRF with statistical based methods, such as Gray level co-occurrence (GLCM)
and run-length (RLM) matrices, improved the overall accuracy to 96.94% and
96.55%; respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08822</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08822</id><created>2015-12-29</created><authors><author><keyname>Lou</keyname><forenames>Youcheng</forenames></author><author><keyname>Yu</keyname><forenames>Lean</forenames></author><author><keyname>Wang</keyname><forenames>Shouyang</forenames></author></authors><title>Privacy Preservation in Distributed Subgradient Optimization Algorithms</title><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy preservation is becoming an increasingly important issue in data
mining and machine learning. In this paper, we consider the privacy preserving
features of distributed subgradient optimization algorithms. We first show that
a well-known distributed subgradient synchronous optimization algorithm, in
which all agents make their optimization updates simultaneously at all times,
is not privacy preserving in the sense that the malicious agent can learn other
agents' subgradients asymptotically. Then we propose a distributed subgradient
projection asynchronous optimization algorithm without relying on any existing
privacy preservation technique, where agents can exchange data between
neighbors directly. In contrast to synchronous algorithms, in the new
asynchronous algorithm agents make their optimization updates asynchronously.
The introduced projection operation and asynchronous optimization mechanism can
guarantee that the proposed asynchronous optimization algorithm is privacy
preserving. Moreover, we also establish the optimal convergence of the newly
proposed algorithm. The proposed privacy preservation techniques shed light on
developing other privacy preserving distributed optimization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08823</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08823</id><created>2015-12-29</created><updated>2016-01-05</updated><authors><author><keyname>Almeida</keyname><forenames>Ricardo</forenames></author><author><keyname>Hol&#xed;k</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author></authors><title>Reduction of Nondeterministic Tree Automata</title><categories>cs.FL</categories><comments>Extended version (including proofs) of material presented at TACAS
  2016</comments><report-no>EDI-INF-RR1421</report-no><msc-class>68Q45</msc-class><acm-class>F.1.1; D.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present an efficient algorithm to reduce the size of nondeterministic tree
automata, while retaining their language. It is based on new transition pruning
techniques, and quotienting of the state space w.r.t. suitable equivalences. It
uses criteria based on combinations of downward and upward simulation preorder
on trees, and the more general downward and upward language inclusions. Since
tree-language inclusion is EXPTIME-complete, we describe methods to compute
good approximations in polynomial time. We implemented our algorithm as a
module of the well-known libvata tree automata library, and tested its
performance on a given collection of tree automata from various applications of
libvata in regular model checking and shape analysis, as well as on various
classes of randomly generated tree automata. Our algorithm yields substantially
smaller and sparser automata than all previously known reduction techniques,
and it is still fast enough to handle large instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08824</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08824</id><created>2015-12-29</created><updated>2016-01-13</updated><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames></author><author><keyname>Ciobanu</keyname><forenames>Radu</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Sangnier</keyname><forenames>Arnaud</forenames></author><author><keyname>Sproston</keyname><forenames>Jeremy</forenames></author></authors><title>Qualitative Analysis of VASS-Induced MDPs</title><categories>cs.LO</categories><comments>Extended version (including all proofs) of material presented at
  FOSSACS 2016</comments><report-no>EDI-INF-RR1422</report-no><msc-class>90C40</msc-class><acm-class>D.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider infinite-state Markov decision processes (MDPs) that are induced
by extensions of vector addition systems with states (VASS). Verification
conditions for these MDPs are described by reachability and Buchi objectives
w.r.t. given sets of control-states. We study the decidability of some
qualitative versions of these objectives, i.e., the decidability of whether
such objectives can be achieved surely, almost-surely, or limit-surely. While
most such problems are undecidable in general, some are decidable for large
subclasses in which either only the controller or only the random environment
can change the counter values (while the other side can only change
control-states).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08826</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08826</id><created>2015-12-29</created><authors><author><keyname>Dev</keyname><forenames>Kapil</forenames></author><author><keyname>Lau</keyname><forenames>Manfred</forenames></author></authors><title>Improving Style Similarity Metrics of 3D Shapes</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of style similarity metrics has been recently developed for various
media types such as 2D clip art and 3D shapes. We explore this style metric
problem and improve existing style similarity metrics of 3D shapes in four
novel ways. First, we consider the color and texture of 3D shapes which are
important properties that have not been previously considered. Second, we
explore the effect of clustering a dataset of 3D models by comparing between
style metrics for a single object type and style metrics that combine clusters
of object types. Third, we explore the idea of user-guided learning for this
problem. Fourth, we introduce an iterative approach that can learn a metric
from a general set of 3D models. We demonstrate these contributions with
various classes of 3D shapes and with applications such as style-based
similarity search and scene composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08829</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08829</id><created>2015-12-29</created><authors><author><keyname>Tan</keyname><forenames>Feng</forenames></author><author><keyname>Lohmiller</keyname><forenames>Winfried</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>Simultaneous Localization And Mapping Without Linearization</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply a combination of linear time varying (LTV) Kalman filtering and
nonlinear contraction tools to the problem of simultaneous mapping and local-
ization (SLAM), in a fashion which avoids linearized approximations altogether.
By exploiting virtual synthetic measurements, the LTV Kalman observer avoids
errors and approximations brought by the linearization process in the EKF SLAM.
Fur- thermore, conditioned on the robot position, the covariances between
landmarks are fully decoupled, making the algorithm easily scalable.
Contraction analysis is used to establish stability of the algorithm and
quantify its convergence rate. We pro- pose four versions based on different
combinations of sensor information, ranging from traditional bearing
measurements and radial measurements to optical flows and time-to-contact
measurements. As shown in simulations, the proposed algorithm is simple and
fast, and it can solve SLAM problems in both 2D and 3D scenarios with
guaranteed convergence rates in a full nonlinear context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08831</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08831</id><created>2015-12-29</created><authors><author><keyname>Polyakovskiy</keyname><forenames>Sergey</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>The Packing While Traveling Problem</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1411.5768</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Packing While Traveling problem as a new non-linear
knapsack problem. Given are a set of cities that have a set of items of
distinct profits and weights and a vehicle that may collect the items when
visiting all the cities in fixed order. Each selected item contributes its
profit, but produces a transportation cost relative to its weight. The problem
asks to find a subset of the items such that the total gain is maximized. We
investigate constrained and unconstrained versions of the problem and show that
both are NP-hard. We propose a pre-processing scheme that decreases the size of
instances making them easier for computation. We provide lower and upper bounds
based on mixed-integer programming (MIP) adopting the ideas of piecewise linear
approximation. Furthermore, we introduce two exact approaches: one is based on
MIP employing linearization technique, and another is a branch-infer-and-bound
(BIB) hybrid approach that compounds the upper bound procedure with a
constraint programming model strengthened with customized constraints. Our
experimental results show the effectiveness of our exact and approximate
solutions in terms of solution quality and computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08836</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08836</id><created>2015-12-29</created><authors><author><keyname>Sun</keyname><forenames>Wen</forenames></author><author><keyname>Venkatraman</keyname><forenames>Arun</forenames></author><author><keyname>Boots</keyname><forenames>Byron</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Learning to Filter with Predictive State Inference Machines</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent state space models are one of the most fundamental and widely used
tools for modeling dynamical systems. Traditional Maximum Likelihood Estimation
(MLE) based approaches aim to maximize the likelihood objective, which is
non-convex due to latent states. While non-convex optimization methods like EM
can learn models that locally optimize the likelihood objective, using the
locally optimal model for an inference task such as Bayesian filtering usually
does not have performance guarantees. In this work, we propose a method that
considers the inference procedure on the dynamical system as a composition of
predictors. Instead of optimizing a given parametrization of latent states, we
learn predictors for inference in predictive belief space, where we can use
sufficient features of observations for supervision of our learning algorithm.
We further show that our algorithm, the Predictive State Inference Machine, has
theoretical performance guarantees on the inference task. Empirical
verification across several of dynamical system benchmarks ranging from a
simulated helicopter to recorded telemetry traces from a robot showcase the
abilities of training Inference Machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08846</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08846</id><created>2015-12-29</created><authors><author><keyname>Scaringe</keyname><forenames>Raymond P.</forenames></author></authors><title>Apollonius Solutions in Rd</title><categories>cs.CG</categories><comments>30 pages, 9 figures</comments><msc-class>I.3.5</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voronoi and related diagrams have technological applications, for example, in
motion planning and surface reconstruction, and also find significant use in
materials science, molecular biology, and crystallography. Apollonius diagrams
arguably provide the most natural division of space for many materials and
technology problems, but compared to Voronoi and power diagrams, their use has
been limited, presumably by the complexity of their calculation. In this work,
we report explicit equations for the vertices of the Apollonius diagram in a
d-dimensional Euclidean space. We show that there are special lines that
contain vertices of more than one type of diagram and this property can be
exploited to develop simple vertex expressions for the Apollonius diagram.
Finding the Apollonius vertices is not significantly more difficult or
expensive than computing those of the power diagram and have application beyond
their use in calculating the diagram. The expressions reported here lend
themselves to the use of standard vector and matrix libraries and the stability
and precision their use implies. They can also be used in algorithms with
multi-precision numeric types and those adhering to the exact algorithms
paradigm. The results have been coded in C++ for the 2-d and 3-d cases and an
example of their use in characterizing the shape of a void in a molecular
crystal is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08849</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08849</id><created>2015-12-30</created><authors><author><keyname>Wang</keyname><forenames>Shuohang</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author></authors><title>Learning Natural Language Inference with LSTM</title><categories>cs.CL cs.AI cs.NE</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language inference (NLI) is a fundamentally important task in natural
language processing that has many applications. The recently released Stanford
Natural Language Inference (SNLI) corpus has made it possible to develop and
evaluate learning-centered methods such as deep neural networks for the NLI
task. In this paper, we propose a special long short-term memory (LSTM)
architecture for NLI. Our model builds on top of a recently proposed neutral
attention model for NLI but is based on a significantly different idea. Instead
of deriving sentence embeddings for the premise and the hypothesis to be used
for classification, our solution uses a matching-LSTM that performs
word-by-word matching of the hypothesis with the premise. This LSTM is able to
place more emphasis on important word-level matching results. In particular, we
observe that this LSTM remembers important mismatches that are critical for
predicting the contradiction or the neutral relationship label. Our experiments
on the SNLI corpus show that our model outperforms the state of the art,
achieving an accuracy of 86.1% on the test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08852</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08852</id><created>2015-12-30</created><authors><author><keyname>Khrennikov</keyname><forenames>Andrei</forenames></author></authors><title>Randomness: quantum versus classical</title><categories>quant-ph cs.IT math.IT math.PR physics.data-an physics.hist-ph</categories><comments>arXiv admin note: text overlap with arXiv:1410.5773</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent tremendous development of quantum information theory led to a number
of quantum technological projects, e.g., quantum random generators. This
development stimulates a new wave of interest in quantum foundations. One of
the most intriguing problems of quantum foundations is elaboration of a
consistent and commonly accepted interpretation of quantum state. Closely
related problem is clarification of the notion of quantum randomness and its
interrelation with classical randomness. In this short review we shall discuss
basics of classical theory of randomness (which by itself is very complex and
characterized by diversity of approaches) and compare it with irreducible
quantum randomness. The second part of this review is devoted to the
information interpretation of quantum mechanics (QM) in the spirit of Zeilinger
and Brukner (and QBism of Fuchs et al.) and physics in general (e.g., Wheeler's
&quot;it from bit&quot;) as well as digital philosophy of Chaitin (with historical
coupling to ideas of Leibnitz). Finally, we continue discussion on
interrelation of quantum and classical randomness and information
interpretation of QM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08854</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08854</id><created>2015-12-30</created><authors><author><keyname>Wang</keyname><forenames>Qifei</forenames></author></authors><title>An Overview of Emerging Technologies for High Efficiency 3D Video Coding</title><categories>cs.MM</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D video coding is one of the most popular research area in multimedia. This
paper reviews the recent progress of the coding technologies for multiview
video (MVV) and free view-point video (FVV) which is represented by MVV and
depth maps. We first discuss the traditional multiview video coding (MVC)
framework with different prediction structures. The rate-distortion performance
and the view switching delay of the three main coding prediction structures are
analyzed. We further introduce the joint coding technologies for MVV and depth
maps and evaluate the rate-distortion performance of them. The scalable 3D
video coding technologies are reviewed by the quality and view scalability,
respectively. Finally, we summarize the bit allocation work of 3D video coding.
This paper also points out some future research problems in high efficiency 3D
video coding such as the view switching latency optimization in coding
structure and bit allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08863</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08863</id><created>2015-12-30</created><authors><author><keyname>Zhao</keyname><forenames>Shengjia</forenames></author><author><keyname>Chaturapruek</keyname><forenames>Sorathan</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author></authors><title>Closing the Gap Between Short and Long XORs for Model Counting</title><categories>cs.CC</categories><comments>The 30th Association for the Advancement of Artificial Intelligence
  (AAAI-16) Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many recent algorithms for approximate model counting are based on a
reduction to combinatorial searches over random subsets of the space defined by
parity or XOR constraints. Long parity constraints (involving many variables)
provide strong theoretical guarantees but are computationally difficult. Short
parity constraints are easier to solve but have weaker statistical properties.
It is currently not known how long these parity constraints need to be. We
close the gap by providing matching necessary and sufficient conditions on the
required asymptotic length of the parity constraints. Further, we provide a new
family of lower bounds and the first non-trivial upper bounds on the model
count that are valid for arbitrarily short XORs. We empirically demonstrate the
effectiveness of these bounds on model counting benchmarks and in a
Satisfiability Modulo Theory (SMT) application motivated by the analysis of
contingency tables in statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08867</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08867</id><created>2015-12-30</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author></authors><title>Modelling and Verifying the AODV Routing Protocol</title><categories>cs.NI cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.7645</comments><acm-class>C.2.2; F.3.1; F.3.2</acm-class><doi>10.1007/s00446-015-0262-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formal specification of the Ad hoc On-Demand Distance
Vector (AODV) routing protocol using AWN (Algebra for Wireless Networks), a
recent process algebra which has been tailored for the modelling of Mobile Ad
Hoc Networks and Wireless Mesh Network protocols. Our formalisation models the
exact details of the core functionality of AODV, such as route discovery, route
maintenance and error handling. We demonstrate how AWN can be used to reason
about critical protocol properties by providing detailed proofs of loop freedom
and route correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08873</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08873</id><created>2015-12-30</created><authors><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Fehnker</keyname><forenames>Ansgar</forenames></author></authors><title>A Rigorous Analysis of AODV and its Variants</title><categories>cs.NI cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.7645</comments><acm-class>C.2.2; F.3.1; F.3.2</acm-class><journal-ref>Proc. Modeling, Analysis and Simulation of Wireless and Mobile
  Systems, MSWiM'12, ACM, 2012, pp. 203-212</journal-ref><doi>10.1145/2387238.2387274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a rigorous analysis of the Ad hoc On-Demand Distance
Vector (AODV) routing protocol using a formal specification in AWN (Algebra for
Wireless Networks), a process algebra which has been specifically tailored for
the modelling of Mobile Ad Hoc Networks and Wireless Mesh Network protocols.
Our formalisation models the exact details of the core functionality of AODV,
such as route discovery, route maintenance and error handling. We demonstrate
how AWN can be used to reason about critical protocol correctness properties by
providing a detailed proof of loop freedom. In contrast to evaluations using
simulation or other formal methods such as model checking, our proof is generic
and holds for any possible network scenario in terms of network topology, node
mobility, traffic pattern, etc. A key contribution of this paper is the
demonstration of how the reasoning and proofs can relatively easily be adapted
to protocol variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08887</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08887</id><created>2015-12-30</created><authors><author><keyname>Pourkamali-Anaraki</keyname><forenames>Farhad</forenames></author></authors><title>Estimation of the sample covariance matrix from compressive measurements</title><categories>stat.ML cs.LG</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the estimation of the sample covariance matrix from
low-dimensional random projections of data known as compressive measurements.
In particular, we present an unbiased estimator to extract the covariance
structure from compressive measurements obtained by a general class of random
projection matrices consisting of i.i.d. zero-mean entries and finite first
four moments. In contrast to previous works, we make no structural assumptions
about the underlying covariance matrix such as being low-rank. In fact, our
analysis is based on a non-Bayesian data setting which requires no
distributional assumptions on the set of data samples. Furthermore, inspired by
the generality of the projection matrices, we propose an approach to covariance
estimation that utilizes very sparse random projections with Bernoulli entries.
Therefore, our algorithm can be used to estimate the covariance matrix in
applications with limited memory and computation power at the acquisition
devices. Experimental results demonstrate that our approach allows for accurate
estimation of the sample covariance matrix on several real-world data sets,
including video data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08891</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08891</id><created>2015-12-30</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author><author><keyname>Tan</keyname><forenames>Wee Lum</forenames></author><author><keyname>Portmann</keyname><forenames>Marius</forenames></author></authors><title>Sequence Numbers Do Not Guarantee Loop Freedom; AODV Can Yield Routing
  Loops</title><categories>cs.NI cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1312.7645</comments><acm-class>C.2.2; F.3.1</acm-class><journal-ref>Proc. Modeling, Analysis and Simulation of Wireless and Mobile
  Systems, MSWiM'13, ACM, 2013, pp. 91-100</journal-ref><doi>10.1145/2507924.2507943</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of mobile ad-hoc networks and wireless mesh networks, sequence
numbers are often used in routing protocols to avoid routing loops. It is
commonly stated in protocol specifications that sequence numbers are sufficient
to guarantee loop freedom if they are monotonically increased over time. A
classical example for the use of sequence numbers is the popular Ad hoc
On-Demand Distance Vector (AODV) routing protocol. The loop freedom of AODV is
not only a common belief, it has been claimed in the abstract of its RFC and at
least two proofs have been proposed. AODV-based protocols such as AODVv2 (DYMO)
and HWMP also claim loop freedom due to the same use of sequence numbers.
  In this paper we show that AODV is not a priori loop free; by this we counter
the proposed proofs in the literature. In fact, loop freedom hinges on
non-evident assumptions to be made when resolving ambiguities occurring in the
RFC. Thus, monotonically increasing sequence numbers, by themselves, do not
guarantee loop freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08899</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08899</id><created>2015-12-30</created><authors><author><keyname>Sch&#xfc;ller</keyname><forenames>Peter</forenames></author></authors><title>Modeling Variations of First-Order Horn Abduction in Answer Set
  Programming using On-Demand Constraints and Flexible Value Invention</title><categories>cs.AI</categories><comments>Technical Report</comments><msc-class>68N17</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study abduction in First Order Horn logic theories where all atoms can be
abduced and we are looking for prefered solutions with respect to objective
functions cardinality minimality, Coherence, or Weighted Abduction. We
represent this reasoning problem in Answer Set Programming (ASP), in order to
obtain a flexible framework for experimenting with global constraints and
objective functions, and to test the boundaries of what is possible with ASP,
because realizing this problem in ASP is challenging as it requires value
invention and equivalence between certain constants as the Unique Names
Assumption does not hold in general. For permitting reasoning in cyclic
theories, we formally describe fine-grained variations of limiting
Skolemization. We evaluate our encodings and extensions experimentally on the
ACCEL benchmark for plan recognition in Natural Language Understanding. Our
encodings are publicly available, modular, and our approach is more efficient
than state-of-the-art solvers on the ACCEL benchmark. We identify term
equivalence as a main instantiation bottleneck, and experiment with on-demand
constraints that were used to eliminate the same bottleneck in state-of-the-art
solvers and make them applicable for larger datasets. Surprisingly, experiments
show that this method is beneficial only for cardinality minimality with our
ASP encodings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08903</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08903</id><created>2015-12-30</created><authors><author><keyname>Hwang</keyname><forenames>Kyuyeon</forenames></author><author><keyname>Lee</keyname><forenames>Minjae</forenames></author><author><keyname>Sung</keyname><forenames>Wonyong</forenames></author></authors><title>Online Keyword Spotting with a Character-Level Recurrent Neural Network</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a context-aware keyword spotting model employing a
character-level recurrent neural network (RNN) for spoken term detection in
continuous speech. The RNN is end-to-end trained with connectionist temporal
classification (CTC) to generate the probabilities of character and
word-boundary labels. There is no need for the phonetic transcription, senone
modeling, or system dictionary in training and testing. Also, keywords can
easily be added and modified by editing the text based keyword list without
retraining the RNN. Moreover, the unidirectional RNN processes an infinitely
long input audio streams without pre-segmentation and keywords are detected
with low-latency before the utterance is finished. Experimental results show
that the proposed keyword spotter significantly outperforms the deep neural
network (DNN) and hidden Markov model (HMM) based keyword-filler model even
with less computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08943</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08943</id><created>2015-12-30</created><authors><author><keyname>Bortnikov</keyname><forenames>Vita</forenames></author><author><keyname>Chockler</keyname><forenames>Gregory</forenames></author><author><keyname>Perelman</keyname><forenames>Dmitri</forenames></author><author><keyname>Roytman</keyname><forenames>Alexey</forenames></author><author><keyname>Shachor</keyname><forenames>Shlomit</forenames></author><author><keyname>Shnayderman</keyname><forenames>Ilya</forenames></author></authors><title>Reconfigurable State Machine Replication from Non-Reconfigurable
  Building Blocks</title><categories>cs.DC</categories><acm-class>C.2.4; C.4; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconfigurable state machine replication is an important enabler of
elasticity for replicated cloud services, which must be able to dynamically
adjust their size as a function of changing load and resource availability. We
introduce a new generic framework to allow the reconfigurable state machine
implementation to be derived from a collection of arbitrary non-reconfigurable
state machines. Our reduction framework follows the black box approach, and
does not make any assumptions with respect to its execution environment apart
from reliable channels. It allows higher-level services to leverage speculative
command execution to ensure uninterrupted progress during the reconfiguration
periods as well as in situations where failures prevent the reconfiguration
agreement from being reached in a timely fashion. We apply our framework to
obtain a reconfigurable speculative state machine from the non-reconfigurable
Paxos implementation, and analyze its performance on a realistic distributed
testbed. Our results show that our framework incurs negligible overheads in the
absence of reconfiguration, and allows steady throughput to be maintained
throughout the reconfiguration periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08949</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08949</id><created>2015-12-30</created><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Simple, Robust and Optimal Ranking from Pairwise Comparisons</title><categories>cs.LG cs.AI cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider data in the form of pairwise comparisons of n items, with the
goal of precisely identifying the top k items for some value of k &lt; n, or
alternatively, recovering a ranking of all the items. We consider a simple
counting algorithm that ranks the items in order of the number of pairwise
comparisons won, and show it has three important and useful features: (a)
Computational efficiency: the simplicity of the method leads to speed-ups of
several orders of magnitude in computation time as compared to prior work; (b)
Robustness: our theoretical guarantees make no assumptions on the
pairwise-comparison probabilities, while prior work is restricted to the
specific BTL model and performs poorly if the data is not true to it; and (c)
Optimality: we show that up to constant factors, our algorithm achieves the
information-theoretic limits for recovering the top-k subset. Finally, we
extend our results to obtain sharp guarantees for approximate recovery under
the Hamming distortion metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08951</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08951</id><created>2015-12-30</created><authors><author><keyname>Sarma</keyname><forenames>Siddhartha</forenames></author><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Robust Power Allocation and Outage Analysis for Secrecy in Independent
  Parallel Gaussian Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>4 pages, 2 figures. Author version of the paper published in IEEE
  Wireless Communications Letters. Published version is accessible at
  http://dx.doi.org/10.1109/LWC.2015.2497347</comments><journal-ref>IEEE Wireless Communications Letters, vol. 5, Issue 1, pp. 68-71,
  Feb. 2016</journal-ref><doi>10.1109/LWC.2015.2497347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies parallel independent Gaussian channels with uncertain
eavesdropper channel state information (CSI). Firstly, we evaluate the
probability of zero secrecy rate in this system for (i) given instantaneous
channel conditions and (ii) a Rayleigh fading scenario. Secondly, when non-zero
secrecy is achievable in the low SNR regime, we aim to solve a robust power
allocation problem which minimizes the outage probability at a target secrecy
rate. We bound the outage probability and obtain a linear fractional program
that takes into account the uncertainty in eavesdropper CSI while allocating
power on the parallel channels. Problem structure is exploited to solve this
optimization problem efficiently. We find the proposed scheme effective for
uncertain eavesdropper CSI in comparison with conventional power allocation
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08969</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08969</id><created>2015-12-30</created><authors><author><keyname>Moud&#x159;&#xed;k</keyname><forenames>Josef</forenames></author><author><keyname>Baudi&#x161;</keyname><forenames>Petr</forenames></author><author><keyname>Neruda</keyname><forenames>Roman</forenames></author></authors><title>Evaluating Go Game Records for Prediction of Player Attributes</title><categories>cs.AI</categories><journal-ref>Computational Intelligence and Games (CIG), 2015 IEEE Conference
  on , vol., no., pp.162-168, Aug. 31 2015-Sept. 2 2015</journal-ref><doi>10.1109/CIG.2015.7317909</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a way of extracting and aggregating per-move evaluations from sets
of Go game records. The evaluations capture different aspects of the games such
as played patterns or statistic of sente/gote sequences. Using machine learning
algorithms, the evaluations can be utilized to predict different relevant
target variables. We apply this methodology to predict the strength and playing
style of the player (e.g. territoriality or aggressivity) with good accuracy.
We propose a number of possible applications including aiding in Go study,
seeding real-work ranks of internet players or tuning of Go-playing programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08982</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08982</id><created>2015-12-30</created><authors><author><keyname>Ghosh</keyname><forenames>Sucheta</forenames></author></authors><title>Technical Report: a tool for measuring Prosodic Accommodation</title><categories>cs.SD cs.CL</categories><comments>This work is done during my one year postdoctoral period in Trinity
  College Dublin. My contribution to the evaluation of the framework is
  submitted as a solo patent application with number IDF ref no: SG02-454-01</comments><report-no>IDF ref no: SG02-454-01</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Social interaction is a dynamic and joint activity where all participants are
engaged and coordinate their behaviour in the co-construction of meaning. It is
observed that conversational partners adapt their pitch, intensity and timing
behaviour to their in- terlocutors. The majority of research has focused on its
linear manifestation over the course of an interaction. De Looze et al
hypothesised that it evolves dynamically with functional social aspects. In the
work of De Looze et al, they proposed through their praat based feature
extraction and matlab based visualisation that one can visualise prosodic
accommodation at the positive correlation threshold values. and the capture of
its dynamic manifestation. Here we seek to build a complete system for
measuring prosodic accommodation with matlab. This work uses data collected in
a pilot training scenario where senior pilot and co-pilot are engaged in
conversation during the flight. Additionally, we use the data from a ship wreck
task by two pilots. We also attempt to evaluate this measures of accommodation
with ground truth labels given by a trainer of Crew (or sometimes Crisis)
Resource Management (CRM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08986</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08986</id><created>2015-12-30</created><authors><author><keyname>Diesburg</keyname><forenames>Sarah</forenames></author><author><keyname>Feldhaus</keyname><forenames>C. Adam</forenames></author><author><keyname>Fardan</keyname><forenames>Mojtaba Al</forenames></author><author><keyname>Schlicht</keyname><forenames>Jonathan</forenames></author><author><keyname>Ploof</keyname><forenames>Nigel</forenames></author></authors><title>Is Your Data Gone? Comparing Perceived Effectiveness of Thumb Drive
  Deletion Methods to Actual Effectiveness</title><categories>cs.CR</categories><comments>12 pages, 15 figures, 4 tables. Author pre-print</comments><acm-class>D.4.2; D.4.6; H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies have shown that many users do not use effective data
deletion techniques upon sale or surrender of storage devices. A logical
assumption is that many users are still confused concerning proper sanitization
techniques of devices upon surrender. This paper strives to measure this
assumption through a buyback study with a survey component. We recorded
participants' thoughts and beliefs concerning deletion, as well as general
demographic information, in relation to actual deletion effectiveness on USB
thumb drives. Thumb drives were chosen for this study due to their relative low
cost, ease of use, and ubiquity. In addition, we also bought used thumb drives
from eBay and Amazon Marketplace to use as a comparison to the wider world.
  We found that there is no statistically significant difference between
buyback and market drives in terms of deletion methods nor presence of
sensitive data, and thus our study may be predictive of the perceptions of the
market sellers. In our combined data sets, we found over 60% of the drives
tested still had recoverable sensitive data, and in the buyback group, we found
no correlation between users' perceived versus actual effectiveness of deletion
methods. Our results suggest the security community may need to take a
different approach to increase the usability, availability, and/or necessity of
strong deletion methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08990</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08990</id><created>2015-12-30</created><authors><author><keyname>Borgstr&#xf6;m</keyname><forenames>Johannes</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Gordon</keyname><forenames>Andrew D.</forenames></author><author><keyname>Szymczak</keyname><forenames>Marcin</forenames></author></authors><title>A Lambda-Calculus Foundation for Universal Probabilistic Programming</title><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We develop the operational semantics of a probabilistic lambda-calculus with
continuous distributions, as a foundation for universal probabilistic
programming languages such as Church, Anglican, and Venture. Our first
contribution is to adapt the classic operational semantics of lambda-calculus
to the continuous case, via creating a measure space on terms and defining
step-indexed approximations. We prove equivalence of big-step and small-step
formulations of this distributional semantics. Our second contribution is to
formalize the implementation technique of trace MCMC for our calculus and to
show correctness. A key step is defining a sampling semantics of a term as a
function from a trace of random samples to a value, and showing that the
distribution induced by integrating over all traces equals the distributional
semantics. Another step is defining sufficient conditions for the distribution
induced by trace MCMC to converge to the distributional semantics. To the best
of our knowledge, this is the first rigorous correctness proof for trace MCMC
for a higher-order functional language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08991</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08991</id><created>2015-12-30</created><authors><author><keyname>Montaser</keyname><forenames>Rasha</forenames></author><author><keyname>Younes</keyname><forenames>Ahmed</forenames></author><author><keyname>Abdel-Aty</keyname><forenames>Mahmoud</forenames></author></authors><title>New Designs of Universal Reversible Gate Library</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new algorithms to synthesize exact universal reversible gate
library for various types of gates and costs. We use the powerful algebraic
software GAP for implementation and examination of our algorithms and the
reversible logic synthesis problems have been reduced to group theory problems.
It is shown that minimization of arbitrary cost functions of gates and orders
of magnitude are faster than its previously counterparts for reversible logic
synthesis. Experimental results show that a significant improvement over the
previously proposed synthesis algorithm is obtained compared with the existing
approaches to reversible logic synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.08995</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.08995</id><created>2015-12-30</created><authors><author><keyname>Turner</keyname><forenames>Jonathan</forenames></author></authors><title>The Edge Group Coloring Problem with Applications to Multicast Switching</title><categories>cs.DS</categories><report-no>WUCSE-2015-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a natural generalization of the classical edge coloring
problem in graphs that provides a useful abstraction for two well-known
problems in multicast switching. We show that the problem is NP-hard and
evaluate the performance of several approximation algorithms, both analytically
and experimentally. We find that for random $\chi$-colorable graphs, the number
of colors used by the best algorithms falls within a small constant factor of
$\chi$, where the constant factor is mainly a function of the ratio of the
number of outputs to inputs. When this ratio is less than 10, the best
algorithms produces solutions that use fewer than $2\chi$ colors. In addition,
one of the algorithms studied finds high quality approximate solutions for any
graph with high probability, where the probability of a low quality solution is
a function only of the random choices made by the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09002</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09002</id><created>2015-12-30</created><authors><author><keyname>Turner</keyname><forenames>Jonathan</forenames></author></authors><title>The Bounded Edge Coloring Problem and Offline Crossbar Scheduling</title><categories>cs.DS</categories><report-no>WUCSE-2015-07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a variant of the classical edge coloring problem in
graphs that can be applied to an offline scheduling problem for crossbar
switches. We show that the problem is NP-complete, develop three lower bounds
bounds on the optimal solution value and evaluate the performance of several
approximation algorithms, both analytically and experimentally. We show how to
approximate an optimal solution with a worst-case performance ratio of $3/2$
and our experimental results demonstrate that the best algorithms produce
results that very closely track a lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09017</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09017</id><created>2015-12-30</created><authors><author><keyname>Habib</keyname><forenames>Abdulelah H</forenames></author><author><keyname>Pecena</keyname><forenames>Zachary K</forenames></author><author><keyname>Disfani</keyname><forenames>Vahid R</forenames></author><author><keyname>Kleissl</keyname><forenames>Jan</forenames></author><author><keyname>de Callafon</keyname><forenames>Raymond A.</forenames></author></authors><title>Reliability of Dynamic Load Scheduling with Solar Forecast Scenarios</title><categories>cs.SY</categories><comments>6 pager, 4 figures, Syscon 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents and evaluates the performance of an optimal scheduling
algorithm that selects the on/off combinations and timing of a finite set of
dynamic electric loads on the basis of short term predictions of the power
delivery from a photovoltaic source. In the algorithm for optimal scheduling,
each load is modeled with a dynamic power profile that may be different for on
and off switching. Optimal scheduling is achieved by the evaluation of a
user-specified criterion function with possible power constraints. The
scheduling algorithm exploits the use of a moving finite time horizon and the
resulting finite number of scheduling combinations to achieve real-time
computation of the optimal timing and switching of loads. The moving time
horizon in the proposed optimal scheduling algorithm provides an opportunity to
use short term (time moving) predictions of solar power based on advection of
clouds detected in sky images. Advection, persistence, and perfect forecast
scenarios are used as input to the load scheduling algorithm to elucidate the
effect of forecast errors on mis-scheduling. The advection forecast creates
less events where the load demand is greater than the available solar energy,
as compared to persistence. Increasing the decision horizon leads to increasing
error and decreased efficiency of the system, measured as the amount of power
consumed by the aggregate loads normalized by total solar power. For a
standalone system with a real forecast, energy reserves are necessary to
provide the excess energy required by mis-scheduled loads. A method for battery
sizing is proposed for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09022</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09022</id><created>2015-12-30</created><authors><author><keyname>Tarhini</keyname><forenames>Ali</forenames></author><author><keyname>Scott</keyname><forenames>Michael James</forenames></author></authors><title>Cross-Cultural Differences in Students' Intention to Use RSS Feeds
  between Lebanon and the United Kingdom: A Multi-Group Invariance Analysis
  Based on the Technology Acceptance Model</title><categories>cs.CY</categories><comments>15 pages, 2 figures, 10 tables</comments><acm-class>K.3.1; J.4</acm-class><journal-ref>Electronic Journal of e-Learning, 13(1): 14-29, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Really Simple Syndication (RSS) offers a means for university students to
receive timely updates from virtual learning environments. However, despite its
utility, only 21% of students surveyed at a Lebanese university claim to have
ever used the technology. To investigate whether a cultural influence is
affecting intention to use RSS, the survey was extended to the British context
to conduct a cross-cultural comparison. Using the Technology Adoption Model
(TAM) as a research framework, 437 students responded to a questionnaire
containing four constructs: intention to use (INT); attitude towards benefit
(ATT); perceived usefulness (PU); and perceived ease of use (PEOU). Principle
components analysis (PCA) and structural equation modelling (SEM) were used to
explore the psychometric qualities of the scale. The results show that adoption
was significantly higher, but also modest, in the British context at 36%.
Configural and metric invariance were fully supported, while scalar and
factorial invariance were partially supported. Analysis reveals that, as a
potential consequence of culture, there are significant differences between PU
and PEOU across the two contexts studied, potentially as a consequence of
culture. It is recommended that faculty demonstrate to students how RSS can be
used effectively in order to increase awareness and emphasise usefulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09023</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09023</id><created>2015-12-30</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>Clustering scientific publications based on citation relations: A
  systematic comparison of different methods</title><categories>cs.DL cs.SI physics.data-an</categories><comments>24 pages, 7 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering methods are applied regularly in the bibliometric literature to
identify research areas or scientific fields. These methods are for instance
used to group publications into clusters based on their relations in a citation
network. In the network science literature, many clustering methods, often
referred to as graph partitioning or community detection techniques, have been
developed. Focusing on the problem of clustering the publications in a citation
network, we present a systematic comparison of the performance of a large
number of these clustering methods. Using a number of different citation
networks, some of them relatively small and others very large, we extensively
study the statistical properties of the results provided by different methods.
In addition, we also carry out an expert-based assessment of the results
produced by different methods. The expert-based assessment focuses on
publications in the field of scientometrics. Our findings seem to indicate that
there is a trade-off between different properties that may be considered
desirable for a good clustering of publications. Overall, map equation methods
appear to perform best in our analysis, suggesting that these methods deserve
more attention from the bibliometric community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09032</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09032</id><created>2015-12-30</created><authors><author><keyname>Madnani</keyname><forenames>Khushraj</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Pandya</keyname><forenames>Paritosh</forenames></author></authors><title>Metric Temporal Logic with Counting</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ability to count number of occurrences of events within a specified time
interval is very useful in specification of resource bounded real time
computation. In this paper, we study an extension of Metric Temporal Logic
($\mathsf{MTL}$) with two different counting modalities called $\mathsf{C}$ and
$\mathsf{UT}$ (until with threshold), which enhance the expressive power of
$\mathsf{MTL}$ in orthogonal fashion. We confine ourselves only to the future
fragment of $\mathsf{MTL}$ interpreted in a pointwise manner over finite timed
words. We provide a comprehensive study of the expressive power of logic
$\mathsf{CTMTL}$ and its fragments using the technique of EF games extended
with suitable counting moves. Finally, as our main result, we establish the
decidability of $\mathsf{CTMTL}$ by giving an equisatisfiable reduction from
$\mathsf{CTMTL}$ to $\mathsf{MTL}$. The reduction provides one more example of
the use of temporal projections with oversampling introduced earlier for
proving decidability. Our reduction also implies that $\mathsf{MITL}$ extended
with $\mathsf{C}$ and $\mathsf{UT}$ modalities is elementarily decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09041</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09041</id><created>2015-12-30</created><authors><author><keyname>Xu</keyname><forenames>Chenliang</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>Actor-Action Semantic Segmentation with Grouping Process Models</title><categories>cs.CV</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actor-action semantic segmentation made an important step toward advanced
video understanding problems: what action is happening; who is performing the
action; and where is the action in space-time. Current models for this problem
are local, based on layered CRFs, and are unable to capture long-ranging
interaction of video parts. We propose a new model that combines these local
labeling CRFs with a hierarchical supervoxel decomposition. The supervoxels
provide cues for possible groupings of nodes, at various scales, in the CRFs to
encourage adaptive, high-order groups for more effective labeling. Our model is
dynamic and continuously exchanges information during inference: the local CRFs
influence what supervoxels in the hierarchy are active, and these active nodes
influence the connectivity in the CRF; we hence call it a grouping process
model. The experimental results on a recent large-scale video dataset show a
large margin of 60% relative improvement over the state of the art, which
demonstrates the effectiveness of the dynamic, bidirectional flow between
labeling and grouping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09047</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09047</id><created>2015-12-30</created><authors><author><keyname>Shirokov</keyname><forenames>M. E.</forenames></author></authors><title>Tight continuity bounds for the quantum mutual information and for the
  Holevo quantity</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>17 pages, any comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantitative analysis of continuity of the quantum mutual information and
of the Holevo quantity is presented.
  First we obtain Fannes type and Winter type tight continuity bounds for the
quantum mutual information and their specifications for qc-states.
  Then we show that the Holevo quantity is continuous on the set of all
ensembles of m states if and only if either m or the dimension of underlying
Hilbert space is finite and obtain Fannes type tight continuity bounds for the
Holevo quantity in this case.
  In general case we prove conditions for local continuity of the Holevo
quantity and consider their corollaries (preserving of local continuity under
quantum channels, stability with respect to perturbation of states).
  Winter's type tight continuity bounds for the Holevo quantity under the
energy constraint on the average state of ensembles is obtained and applied to
the system of finite number of quantum oscillators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09049</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09049</id><created>2015-12-30</created><authors><author><keyname>Xu</keyname><forenames>Chenliang</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing</title><categories>cs.CV</categories><comments>In Review at International Journal of Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervoxel segmentation has strong potential to be incorporated into early
video analysis as superpixel segmentation has in image analysis. However, there
are many plausible supervoxel methods and little understanding as to when and
where each is most appropriate. Indeed, we are not aware of a single
comparative study on supervoxel segmentation. To that end, we study seven
supervoxel algorithms, including both off-line and streaming methods, in the
context of what we consider to be a good supervoxel: namely, spatiotemporal
uniformity, object/region boundary detection, region compression and parsimony.
For the evaluation we propose a comprehensive suite of seven quality metrics to
measure these desirable supervoxel characteristics. In addition, we evaluate
the methods in a supervoxel classification task as a proxy for subsequent
high-level uses of the supervoxels in video analysis. We use six existing
benchmark video datasets with a variety of content-types and dense human
annotations. Our findings have led us to conclusive evidence that the
hierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) and
temporal superpixels (TSP) methods are the top-performers among the seven
methods. They all perform well in terms of segmentation accuracy, but vary in
regard to the other desiderata: GBH captures object boundaries best; SWA has
the best potential for region compression; and TSP achieves the best
undersegmentation error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09070</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09070</id><created>2015-12-30</created><authors><author><keyname>Allen</keyname><forenames>Robert Burnell</forenames></author></authors><title>Repositories with Direct Representation</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new generation of digital repositories could be based on direct
representation of the contents with rich semantics and models rather than be
collections of documents. The contents of such repositories would be highly
structured which should help users to focus on meaningful relationships of the
contents. These repositories would implement earlier proposals for
model-oriented information organization by extending current work on ontologies
to cover state changes, instances, and scenarios. They could also apply other
approaches such as object-oriented design and frame semantics. In addition to
semantics, the representation needs to allow for discourse and repository
knowledge-support services and policies. For instance, the knowledge base would
need to be systematically updated as new findings and theories reshape it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09074</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09074</id><created>2015-12-30</created><authors><author><keyname>Ngampruetikorn</keyname><forenames>V.</forenames></author><author><keyname>Stephens</keyname><forenames>Greg J</forenames></author></authors><title>Bias, Belief and Consensus: Collective opinion formation on fluctuating
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of online networks, societies are substantially more
connected with individual members able to easily modify and maintain their own
social links. Here, we show that active network maintenance exposes agents to
confirmation bias, the tendency to confirm one's beliefs, and we explore how
this affects collective opinion formation. We introduce a model of binary
opinion dynamics on a complex network with fast, stochastic rewiring and show
that confirmation bias induces a segregation of individuals with different
opinions. We use the dynamics of global opinion to generally categorize opinion
update rules and find that confirmation bias always stabilizes the consensus
state. Finally, we show that the time to reach consensus has a non-monotonic
dependence on the magnitude of the bias, suggesting a novel avenue for
large-scale opinion engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09075</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09075</id><created>2015-12-30</created><authors><author><keyname>Thomas</keyname><forenames>Philip S.</forenames></author></authors><title>A Notation for Markov Decision Processes</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper specifies a notation for Markov decision processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09080</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09080</id><created>2015-12-30</created><updated>2016-02-15</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Sandon</keyname><forenames>Colin</forenames></author></authors><title>Detection in the stochastic block model with multiple clusters: proof of
  the achievability conjectures, acyclic BP, and the information-computation
  gap</title><categories>math.PR cs.CC cs.IT cs.LG cs.SI math.IT</categories><comments>Extended version with further details on the algorithms and methods</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a paper that initiated the modern study of the stochastic block model,
Decelle et al., backed up Mossel et al., made a fascinating conjecture: Denote
by $k$ the number of balanced communities, $a/n$ the probability of connecting
inside communities and $b/n$ across, and set
$\mathrm{SNR}=|a-b|/\sqrt{k(a+(k-1)b)}$; for any $k \geq 2$, it is possible to
detect communities efficiently whenever $\mathrm{SNR}&gt;1$ (the KS threshold),
whereas for $k\geq 5$, it is possible to detect communities
information-theoretically for some $\mathrm{SNR}&lt;1$. Massouli\'e, Mossel et
al.\ and Bordenave et al.\ succeeded in proving that the KS threshold is
efficiently achievable for $k=2$, while Mossel et al.\ proved that it cannot be
crossed information-theoretically for $k=2$. The above conjecture remained open
for $k \geq 3$.
  This paper proves this conjecture. For the efficient part, an acyclic belief
propagation (ABP) algorithm is developed and proved to detect communities for
any $k$ down the KS threshold in time $O(n \log n)$. Achieving this requires
showing optimality of BP in the presence of cycles and random initialization, a
challenge in the realm of graphical models. The paper also connects ABP to a
power iteration method on a $r$-nonbacktracking operator, formalizing the
message passing and spectral interplay. Further, it shows that the model can be
learned efficiently down the KS threshold, implying that ABP improves upon the
state-of-the-art both in terms of complexity and universality. For the
information-theoretic (IT) part, a non-efficient algorithm sampling a typical
clustering is shown to break down the KS threshold at $k=5$. The emerging gap
is shown to be large in some cases; if $a=0$, the KS threshold reads $b \gtrsim
k^2$ whereas the IT bound reads $b \gtrsim k \ln(k)$. This thus makes the SBM a
good study case for information-computation gaps. The results extend to general
SBMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09090</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09090</id><created>2015-12-30</created><authors><author><keyname>Baum</keyname><forenames>Moritz</forenames></author><author><keyname>Buchhold</keyname><forenames>Valentin</forenames></author><author><keyname>Dibbelt</keyname><forenames>Julian</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Fast Computation of Isochrones in Road Networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing isochrones in road networks, where the
objective is to identify the region that is reachable from a given source
within a certain amount of time. While there is a wide range of practical
applications for this problem (e.g., reachability analyses, geomarketing,
visualizing the cruising range of a vehicle), there has been little research on
fast computation of isochrones on large, realistic inputs. In this work, we
formalize the notion of isochrones in road networks and present a basic
approach for the resulting problem based on Dijkstra's algorithm. Moreover, we
consider several speedup techniques that are based on previous approaches for
one-to-many shortest path computation (or similar scenarios). In contrast to
such related problems, the set of targets is not part of the input when
computing isochrones. We extend known Multilevel Dijkstra techniques (such as
CRP) to the isochrone scenario, adapting a previous technique called isoGRASP
to our problem setting (thereby, enabling faster queries). Moreover, we
introduce a family of algorithms based on (single-level) graph partitions,
following different strategies to exploit the efficient access patterns of
PHAST, a well-known approach towards one-to-all queries. Our experimental study
reveals that all speedup techniques allow fast isochrone computation on input
graphs at continental scale, while providing different tradeoffs between
preprocessing effort, space consumption, and query performance. Finally, we
demonstrate that all techniques scale well when run in parallel, decreasing
query times to a few milliseconds (orders of magnitude faster than the basic
approach) and enabling even interactive applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09103</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09103</id><created>2015-12-30</created><updated>2016-01-13</updated><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author></authors><title>Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling</title><categories>math.OC cs.DS math.NA stat.ML</categories><comments>new authors added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accelerated coordinate descent methods are widely used in optimization and
machine learning. By taking cheap-to-compute coordinate gradients in each
iteration, they are usually faster than accelerated full gradient descent, and
thus suitable for large-scale optimization problems.
  In this paper, we improve the running time of accelerated coordinate descent,
using a clean and novel non-uniform sampling method. In particular, if a
function $f$ is coordinate-wise smooth with parameters $L_1,L_2,\dots,L_n$, we
obtain an algorithm that converges in $O(\sum_i \sqrt{L_i} /
\sqrt{\varepsilon})$ iterations for convex $f$ and in $\tilde{O}(\sum_i
\sqrt{L_i} / \sqrt{\sigma})$ iterations for $\sigma$-strongly convex $f$. The
best known result was $\tilde{O}(\sqrt{n \sum_i L_i} / \sqrt{\varepsilon})$ for
convex $f$ and $\tilde{O}(\sqrt{n \sum_i L_i} / \sqrt{\sigma})$ for
$\sigma$-strongly convex $f$, due to Lee and Sidford. Thus, our algorithm
improves the running time by a factor up to $\sqrt{n}$.
  Our speed-up naturally applies to important problems such as empirical risk
minimizations and solving linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09114</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09114</id><created>2015-12-30</created><authors><author><keyname>David-Barrett</keyname><forenames>Tamas</forenames></author><author><keyname>Kertesz</keyname><forenames>Janos</forenames></author><author><keyname>Rotkirch</keyname><forenames>Anna</forenames></author><author><keyname>Ghosh</keyname><forenames>Asim</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Kunal</forenames></author><author><keyname>Monsivais</keyname><forenames>Daniel</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Communication with family and friends across the life course</title><categories>physics.soc-ph cs.SI</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Each stage of the human life course is characterized by a distinctive pattern
of social relations. We study how the intensity and importance of the closest
social contacts vary across the life course, using a large database of mobile
communication from a European country. We first determine the most likely
social relationship type from these mobile phone records by relating the age
and gender of the caller and recipient to the frequency, length, and direction
of calls. We then show how communication patterns between parents and children,
romantic partner, and friends vary across the six main stages of the adult
family life course. Young adulthood is dominated by a gradual shift of call
activity from parents to close friends, and then to a romantic partner,
culminating in the period of early family formation during which the focus is
on the romantic partner. During middle adulthood call patterns suggest a high
dependence on the parents of the ego, who, presumably often provide
alloparental care, while at this stage female same-gender friendship also
peaks. During post-reproductive adulthood, individuals and especially women
balance close social contacts among three generations. The age of
grandparenthood brings the children entering adulthood and family formation
into the focus, and is associated with a realignment of close social contacts
especially among women, while the old age is dominated by dependence on their
children.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09132</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09132</id><created>2015-12-30</created><authors><author><keyname>Baum</keyname><forenames>Moritz</forenames></author><author><keyname>Dibbelt</keyname><forenames>Julian</forenames></author><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Dynamic Time-Dependent Route Planning in Road Networks with User
  Preferences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been tremendous progress in algorithmic methods for computing
driving directions on road networks. Most of that work focuses on
time-independent route planning, where it is assumed that the cost on each arc
is constant per query. In practice, the current traffic situation significantly
influences the travel time on large parts of the road network, and it changes
over the day. One can distinguish between traffic congestion that can be
predicted using historical traffic data, and congestion due to unpredictable
events, e.g., accidents. In this work, we study the \emph{dynamic and
time-dependent} route planning problem, which takes both prediction (based on
historical data) and live traffic into account. To this end, we propose a
practical algorithm that, while robust to user preferences, is able to
integrate global changes of the time-dependent metric~(e.g., due to traffic
updates or user restrictions) faster than previous approaches, while allowing
subsequent queries that enable interactive applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09156</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09156</id><created>2015-12-30</created><authors><author><keyname>Ubaru</keyname><forenames>Shashanka</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Saad</keyname><forenames>Yousef</forenames></author></authors><title>Low rank approximation and decomposition of large matrices using error
  correcting codes</title><categories>cs.IT cs.LG cs.NA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank approximation is an important tool used in many applications of
signal processing and machine learning. Recently, randomized sketching
algorithms were proposed to effectively construct low rank approximations and
obtain approximate singular value decompositions of large matrices. Similar
ideas were used to solve least squares regression problems. In this paper, we
show how matrices from error correcting codes can be used to find such low rank
approximations and matrix decompositions, and extend the framework to linear
least squares regression problems. The benefits of using these code matrices
are the following: (i) They are easy to generate and they reduce randomness
significantly. (ii) Code matrices with mild properties satisfy the subspace
embedding property, and have a better chance of preserving the geometry of an
entire subspace of vectors. (iii) For parallel and distributed applications,
code matrices have significant advantages over structured random matrices and
Gaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,
which require sampling $O(k\log k)$ columns for a rank-$k$ approximation, the
log factor is not necessary for certain types of code matrices. That is,
$(1+\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$
approximation with $O(k/\epsilon)$ samples. (v) Fast multiplication is possible
with structured code matrices, so fast approximations can be achieved for
general dense input matrices. (vi) For least squares regression problem
$\min\|Ax-b\|_2$ where $A\in \mathbb{R}^{n\times d}$, the $(1+\epsilon)$
relative error approximation can be achieved with $O(d/\epsilon)$ samples, with
high probability, when certain code matrices are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09163</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09163</id><created>2015-12-30</created><authors><author><keyname>Johnson</keyname><forenames>Paul V.</forenames></author><author><keyname>Parnell</keyname><forenames>Jared A. Q.</forenames></author><author><keyname>Kim</keyname><forenames>Joowan</forenames></author><author><keyname>Saunter</keyname><forenames>Christopher D.</forenames></author><author><keyname>Love</keyname><forenames>Gordon D.</forenames></author><author><keyname>Banks</keyname><forenames>Martin S.</forenames></author></authors><title>Dynamic lens and monovision 3D displays to improve viewer comfort</title><categories>cs.HC physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stereoscopic 3D (S3D) displays provide an additional sense of depth compared
to non-stereoscopic displays by sending slightly different images to the two
eyes. But conventional S3D displays do not reproduce all natural depth cues. In
particular, focus cues are incorrect causing mismatches between accommodation
and vergence: The eyes must accommodate to the display screen to create sharp
retinal images even when binocular disparity drives the eyes to converge to
other distances. This mismatch causes visual discomfort and reduces visual
performance. We propose and assess two new techniques that are designed to
reduce the vergence-accommodation conflict and thereby decrease discomfort and
increase visual performance. These techniques are much simpler to implement
than previous conflict-reducing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09170</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09170</id><created>2015-12-30</created><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Guzman</keyname><forenames>Cristobal</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Statistical Query Algorithms for Stochastic Convex Optimization</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic convex optimization, where the objective is the expectation of a
random convex function, is an important and widely used method with numerous
applications in machine learning, statistics, operations research and other
areas. We study the complexity of stochastic convex optimization given only
statistical query (SQ) access to the objective function. We show that
well-known and popular methods, including first-order iterative methods and
polynomial-time methods, can be implemented using only statistical queries. For
many cases of interest we derive nearly matching upper and lower bounds on the
estimation (sample) complexity including linear optimization in the most
general setting. We then present several consequences for machine learning,
differential privacy and proving concrete lower bounds on the power of convex
optimization based methods.
  A new technical ingredient of our work is SQ algorithms for estimating the
mean vector of a distribution over vectors in $\mathbb{R}^d$ with optimal
estimation complexity. This is a natural problem and we show that our solutions
can be used to get substantially improved SQ versions of Perceptron and other
online algorithms for learning halfspaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09176</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09176</id><created>2015-12-30</created><updated>2016-01-11</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Xing</keyname><forenames>Tianwei</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Personalized Course Sequence Recommendations</title><categories>cs.CY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the variability in student learning it is becoming increasingly
important to tailor courses as well as course sequences to student needs. This
paper presents a systematic methodology for offering personalized course
sequence recommendations to students. First, a forward-search
backward-induction algorithm is developed that can optimally select course
sequences to decrease the time required for a student to graduate. The
algorithm accounts for prerequisite requirements (typically present in higher
level education) and course availability. Second, using the tools of
multi-armed bandits, an algorithm is developed that can optimally recommend a
course sequence that both reduces the time to graduate while also increasing
the overall GPA of the student. The algorithm dynamically learns how students
with different contextual backgrounds perform for given course sequences and
then recommends an optimal course sequence for new students. Using real-world
student data from the UCLA Mechanical and Aerospace Engineering department, we
illustrate how the proposed algorithms outperform other methods that do not
include student contextual information when making course sequence
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09180</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09180</id><created>2015-12-30</created><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author></authors><title>Deterministic and Ensemble-Based Spatially-Coupled Product Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several authors have proposed spatially-coupled (or convolutional-like)
variants of Elias' product codes (PCs). In this paper, we consider a code
construction that recovers some of these codes (e.g., staircase and block-wise
braided codes) as special cases and study the iterative decoding performance
over the binary erasure channel. Even though our code construction is
deterministic (and not based on a randomized ensemble), we show that it is
still possible to rigorously derive the density evolution (DE) equations that
govern the asymptotic performance. The obtained DE equations are then compared
to those for a related spatially-coupled PC ensemble. In particular, we show
that there exists a family of (deterministic) braided codes that follows the
same DE equation as the ensemble, for any spatial length and coupling width. We
also prove that for spatially-coupled PCs, employing component codes with
different erasure-correcting capabilities entails a threshold penalty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09184</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09184</id><created>2015-12-30</created><authors><author><keyname>Shi</keyname><forenames>Hao-Jun Michael</forenames></author><author><keyname>Case</keyname><forenames>Mindy</forenames></author><author><keyname>Gu</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Tu</keyname><forenames>Shenyinying</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author></authors><title>Methods for Quantized Compressed Sensing</title><categories>cs.IT math.IT math.NA</categories><msc-class>94A12, 60D05, 90C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compare and catalog the performance of various greedy
quantized compressed sensing algorithms that reconstruct sparse signals from
quantized compressed measurements. We also introduce two new greedy approaches
for reconstruction: Quantized Compressed Sampling Matching Pursuit (QCoSaMP)
and Adaptive Outlier Pursuit for Quantized Iterative Hard Thresholding
(AOP-QIHT). We compare the performance of greedy quantized compressed sensing
algorithms for a given bit-depth, sparsity, and noise level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09186</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09186</id><created>2015-12-30</created><authors><author><keyname>Friggens</keyname><forenames>David</forenames></author><author><keyname>Groves</keyname><forenames>Lindsay</forenames></author></authors><title>Collapsing Threads Safely with Soft Invariants</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Canonical abstraction is a static analysis technique that represents states
as 3-valued logical structures, and produces finite abstract systems. Despite
providing a finite bound, these abstractions may still suffer from the state
explosion problem. Notably, for concurrent programs with arbitrary
interleaving, if threads in a state are abstracted based on their location,
then the number of locations will be a combinatorial factor in the size of the
statespace. We present an approach using canonical abstraction that avoids this
state explosion by &quot;collapsing&quot; all of the threads in a state into a single
abstract representative. Properties of threads that would be lost by the
abstraction, but are needed for verification, are retained by defining
conditional &quot;soft invariant&quot; instrumentation predicates. This technique is used
to adapt previous models for verifying linearizability of nonblocking
concurrent data structure algorithms, resulting in exponentially smaller
statespaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09194</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09194</id><created>2015-12-30</created><updated>2016-02-03</updated><authors><author><keyname>Zhou</keyname><forenames>Shuchang</forenames></author><author><keyname>Wu</keyname><forenames>Jia-Nan</forenames></author><author><keyname>Wu</keyname><forenames>Yuxin</forenames></author><author><keyname>Zhou</keyname><forenames>Xinyu</forenames></author></authors><title>Exploiting Local Structures with the Kronecker Layer in Convolutional
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and study a technique to reduce the number of
parameters and computation time in convolutional neural networks. We use
Kronecker product to exploit the local structures within convolution and
fully-connected layers, by replacing the large weight matrices by combinations
of multiple Kronecker products of smaller matrices. Just as the Kronecker
product is a generalization of the outer product from vectors to matrices, our
method is a generalization of the low rank approximation method for convolution
neural networks. We also introduce combinations of different shapes of
Kronecker product to increase modeling capacity. Experiments on SVHN, scene
text recognition and ImageNet dataset demonstrate that we can achieve $3.3
\times$ speedup or $3.6 \times$ parameter reduction with less than 1\% drop in
accuracy, showing the effectiveness and efficiency of our method. Moreover, the
computation efficiency of Kronecker layer makes using larger feature map
possible, which in turn enables us to outperform the previous state-of-the-art
on both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character
recognition) datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09204</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09204</id><created>2015-12-30</created><authors><author><keyname>Hu</keyname><forenames>Weici</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author></authors><title>Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index
  Policies</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider effort allocation in crowdsourcing, where we wish to assign
labeling tasks to imperfect homogeneous crowd workers to maximize overall
accuracy in a continuous-time Bayesian setting, subject to budget and time
constraints. The Bayes-optimal policy for this problem is the solution to a
partially observable Markov decision process, but the curse of dimensionality
renders the computation infeasible. Based on the Lagrangian Relaxation
technique in Adelman &amp; Mersereau (2008), we provide a computationally tractable
instance-specific upper bound on the value of this Bayes-optimal policy, which
can in turn be used to bound the optimality gap of any other sub-optimal
policy. In an approach similar in spirit to the Whittle index for restless
multiarmed bandits, we provide an index policy for effort allocation in
crowdsourcing and demonstrate numerically that it outperforms other stateof-
arts and performs close to optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09207</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09207</id><created>2015-12-30</created><authors><author><keyname>Cojocaru</keyname><forenames>Liliana</forenames></author></authors><title>Around Context-Free Grammars - a Normal Form, a Representation Theorem,
  and a Regular Approximation</title><categories>cs.FL</categories><comments>31 pages, 5 figures</comments><msc-class>68Q45, 68Q42</msc-class><acm-class>F.4.2, F.4.3, G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a normal form for context-free grammars, called Dyck normal
form. This is a syntactical restriction of the Chomsky normal form, in which
the two nonterminals occurring on the right-hand side of a rule are paired
nonterminals. This pairwise property allows to define a homomorphism from Dyck
words to words generated by a grammar in Dyck normal form. We prove that for
each context-free language L, there exist an integer K and a homomorphism h
such that L=h(D'_K), where D'_K is a subset of the one-sided Dyck language over
K letters. Through a transition-like diagram for a context-free grammar in Dyck
normal form, we effectively build a regular language R that satisfies the
Chomsky-Schutzenberger theorem. Using graphical approaches we refine R such
that the Chomsky-Schutzenberger theorem still holds. Based on this readjustment
we sketch a transition diagram for a regular grammar that generates a regular
superset approximation for the initial context-free language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09210</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09210</id><created>2015-12-30</created><authors><author><keyname>Escalante</keyname><forenames>Jose A. Morales</forenames></author><author><keyname>Gamba</keyname><forenames>Irene M.</forenames></author></authors><title>Reflective Boundary conditions in Discontinuous Galerkin Methods for
  Boltzmann-Poisson Models of Electron Transport in Semiconductors and Zero
  Flux Condition for General Mixed Reflection</title><categories>math.NA cond-mat.mes-hall cond-mat.stat-mech cs.CE</categories><comments>Pre-print Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We shall discuss the use of Discontinuous Galerkin (DG) Finite Element
Methods to solve Boltzmann - Poisson (BP) models of electron transport in
semiconductor devices at nano scales. We consider the mathematical and
numerical modeling of Reflective Boundary Conditions in 2D devices and their
implementation in DG-BP schemes. We study the specular, diffusive and mixed
reflection BC on physical boundaries of the device for the modeling of surface
roughness, comparing the influence of these different reflection cases in the
computational prediction of moments close to the boundaries and their
associated scale. We observe in our preliminary numerical simulations an effect
due to diffusivity at the reflection boundaries over the kinetic moments of the
probability density function, whose influence is not restricted to the
boundaries but rather over the position space domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09227</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09227</id><created>2015-12-31</created><authors><author><keyname>Zhang</keyname><forenames>Zemin</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author></authors><title>Denoising and Completion of 3D Data via Multidimensional Dictionary
  Learning</title><categories>cs.LG cs.CV cs.DS</categories><comments>9 pages, submitted to Conference on Computer Vision and Pattern
  Recognition (CVPR) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new dictionary learning algorithm for multidimensional data
is proposed. Unlike most conventional dictionary learning methods which are
derived for dealing with vectors or matrices, our algorithm, named KTSVD,
learns a multidimensional dictionary directly via a novel algebraic approach
for tensor factorization as proposed in [3, 12, 13]. Using this approach one
can define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D
data to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based
on the idea of sparse coding (using group-sparsity over multidimensional
coefficient vectors), alternates between estimating a compact representation
and dictionary learning. We analyze our KTSVD algorithm and demonstrate its
result on video completion and multispectral image denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09228</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09228</id><created>2015-12-31</created><authors><author><keyname>Jung</keyname><forenames>Minyoung</forenames></author><author><keyname>Burgstaller</keyname><forenames>Bernd</forenames></author><author><keyname>Blieberger</keyname><forenames>Johann</forenames></author></authors><title>Efficient Construction of Simultaneous Deterministic Finite Automata on
  Multicores Using Rabin Fingerprints</title><categories>cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose several optimizations for the SFA construction
algorithm, which greatly reduce the in-memory footprint and the processing
steps required to construct an SFA. We introduce fingerprints as a space- and
time-efficient way to represent SFA states. To compute fingerprints, we apply
the Barrett reduction algorithm and accelerate it using recent additions to the
x86 instruction set architecture. We exploit fingerprints to introduce hashing
for further optimizations. Our parallel SFA construction algorithm is
nonblocking and utilizes instruction-level, data-level, and task-level
parallelism of coarse-, medium- and fine-grained granularity. We adapt static
workload distributions and align the SFA data-structures with the constraints
of multicore memory hierarchies, to increase the locality of memory accesses
and facilitate HW prefetching. We conduct experiments on the PROSITE protein
database for FAs of up to 702 FA states to evaluate performance and
effectiveness of our proposed optimizations. Evaluations have been conducted on
a 4 CPU (64 cores) AMD Opteron 6378 system and a 2 CPU (28 cores, 2
hyperthreads per core) Intel Xeon E5-2697 v3 system. The observed speedups over
the sequential baseline algorithm are up to 118541x on the AMD system and
2113968x on the Intel system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09236</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09236</id><created>2015-12-31</created><authors><author><keyname>Dudycz</keyname><forenames>Szymon</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jan</forenames></author><author><keyname>Paluch</keyname><forenames>Katarzyna</forenames></author><author><keyname>Rybicki</keyname><forenames>Bartosz</forenames></author></authors><title>A $4/5$ - Approximation Algorithm for the Maximum Traveling Salesman
  Problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the maximum traveling salesman problem (Max TSP) we are given a complete
undirected graph with nonnegative weights on the edges and we wish to compute a
traveling salesman tour of maximum weight. We present a fast combinatorial
$\frac 45$ - approximation algorithm for Max TSP. The previous best
approximation for this problem was $\frac 79$. The new algorithm is based on a
novel technique of eliminating difficult subgraphs via half-edges, a new method
of edge coloring and a technique of exchanging edges. A half-edge of edge
$e=(u,v)$ is informally speaking &quot;a half of $e$ containing either $u$ or $v$&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09239</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09239</id><created>2015-12-31</created><authors><author><keyname>Altman</keyname><forenames>Eitan</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Touati</keyname><forenames>Corinne</forenames></author></authors><title>Load Balancing Congestion Games and their Asymptotic Behavior</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central question in routing games has been to establish conditions for the
uniqueness of the equilibrium, either in terms of network topology or in terms
of costs. This question is well understood in two classes of routing games. The
first is the non-atomic routing introduced by Wardrop on 1952 in the context of
road traffic in which each player (car) is infinitesimally small; a single car
has a negligible impact on the congestion. Each car wishes to minimize its
expected delay. Under arbitrary topology, such games are known to have a convex
potential and thus a unique equilibrium. The second framework is splitable
atomic games: there are finitely many players, each controlling the route of a
population of individuals (let them be cars in road traffic or packets in the
communication networks). In this paper, we study two other frameworks of
routing games in which each of several players has an integer number of
connections (which are population of packets) to route and where there is a
constraint that a connection cannot be split. Through a particular game with a
simple three link topology, we identify various novel and surprising properties
of games within these frameworks. We show in particular that equilibria are non
unique even in the potential game setting of Rosenthal with strictly convex
link costs. We further show that non-symmetric equilibria arise in symmetric
networks. I. INTRODUCTION A central question in routing games has been to
establish conditions for the uniqueness of the equilibria, either in terms of
the network topology or in terms of the costs. A survey on these issues is
given in [1]. The question of uniqueness of equilibria has been studied in two
different frameworks. The first, which we call F1, is the non-atomic routing
introduced by Wardrop on 1952 in the context of road traffic in which each
player (car) is infinitesimally small; a single car has a negligible impact on
the congestion. Each car wishes to minimize its expected delay. Under arbitrary
topology, such games are known to have a convex potential and thus have a
unique equilibrium [2]. The second framework, denoted by F2, is splitable
atomic games. There are finitely many players, each controlling the route of a
population of individuals. This type of games have already been studied in the
context of road traffic by Haurie and Marcotte [3] but have become central in
the telecom community to model routing decisions of Internet Service Providers
that can decide how to split the traffic of their subscribers among various
routes so as to minimize network congestion [4]. In this paper we study
properties of equilibria in two other frameworks of routing games which exhibit
surprising
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09243</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09243</id><created>2015-12-31</created><authors><author><keyname>Mehraban</keyname><forenames>Saeed</forenames></author></authors><title>Computational Complexity of Some Quantum Theories in $1+1$ Dimensions</title><categories>quant-ph cs.CC</categories><comments>The material presented here is based on the author's Master's thesis,
  advised by Scott Aaronson, submitted to the department of electrical
  engineering and computer science at MIT on August 28, 2015. Editions and
  modifications has been made to the original thesis, also a new chapter,
  chapter 4 is added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of certain integrable quantum theories
in 1+1 dimensions. We formalize a model of quantum computation based on these
theories. In this model, distinguishable particles start out with known momenta
and initial superposition of different configurations. Then the label of these
particles are measured at the end. We prove that additive approximation to
single amplitudes of these models can be obtained by the one-clean-qubit model,
if no initial superpositions are allowed. However, if arbitrary initial states
and non-adaptive intermediate measurements are allowed, we show that
conditioned on infinite polynomial hierarchy assumption it is hard to sample
from the output distribution of these models on a classical randomized
computer. A classical analogue of this model is also formalized and its
computational power is pinned down within the complexity classes below BPP and
NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09251</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09251</id><created>2015-12-31</created><authors><author><keyname>Bagheri</keyname><forenames>Samineh</forenames></author><author><keyname>Konen</keyname><forenames>Wolfgang</forenames></author><author><keyname>Emmerich</keyname><forenames>Michael</forenames></author><author><keyname>B&#xe4;ck</keyname><forenames>Thomas</forenames></author></authors><title>Solving the G-problems in less than 500 iterations: Improved efficient
  constrained optimization by surrogate modeling and adaptive parameter control</title><categories>math.OC cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constrained optimization of high-dimensional numerical problems plays an
important role in many scientific and industrial applications. Function
evaluations in many industrial applications are severely limited and no
analytical information about objective function and constraint functions is
available. For such expensive black-box optimization tasks, the constraint
optimization algorithm COBRA was proposed, making use of RBF surrogate modeling
for both the objective and the constraint functions. COBRA has shown remarkable
success in solving reliably complex benchmark problems in less than 500
function evaluations. Unfortunately, COBRA requires careful adjustment of
parameters in order to do so.
  In this work we present a new self-adjusting algorithm SACOBRA, which is
based on COBRA and capable to achieve high-quality results with very few
function evaluations and no parameter tuning. It is shown with the help of
performance profiles on a set of benchmark problems (G-problems, MOPTA08) that
SACOBRA consistently outperforms any COBRA algorithm with fixed parameter
setting. We analyze the importance of the several new elements in SACOBRA and
find that each element of SACOBRA plays a role to boost up the overall
optimization performance. We discuss the reasons behind and get in this way a
better understanding of high-quality RBF surrogate modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09254</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09254</id><created>2015-12-31</created><authors><author><keyname>Moud&#x159;&#xed;k</keyname><forenames>Josef</forenames></author><author><keyname>Neruda</keyname><forenames>Roman</forenames></author></authors><title>Evolving Non-linear Stacking Ensembles for Prediction of Go Player
  Attributes</title><categories>cs.AI</categories><comments>Published in 2015 IEEE Symposium Series on Computational Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an application of non-linear stacking ensembles for
prediction of Go player attributes. An evolutionary algorithm is used to form a
diverse ensemble of base learners, which are then aggregated by a stacking
ensemble. This methodology allows for an efficient prediction of different
attributes of Go players from sets of their games. These attributes can be
fairly general, in this work, we used the strength and style of the players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09263</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09263</id><created>2015-12-31</created><authors><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author><author><keyname>Pareschi</keyname><forenames>Fabio</forenames></author><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Rovatti</keyname><forenames>Riccardo</forenames></author><author><keyname>Setti</keyname><forenames>Gianluca</forenames></author></authors><title>On the security of a class of diffusion mechanisms for image encryption</title><categories>cs.CR</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for fast and strong image cryptosystems motivates researchers to
develop new techniques to apply traditional cryptographic primitives in order
to exploit the intrinsic features of digital images. One of the most popular
and mature technique is the use of complex ynamic phenomena, including chaotic
orbits and quantum walks, to generate the required key stream. In this paper,
under the assumption of plaintext attacks we investigate the security of a
classic diffusion mechanism (and of its variants) used as the core
cryptographic rimitive in some image cryptosystems based on the aforementioned
complex dynamic phenomena. We have theoretically found that regardless of the
key schedule process, the data complexity for recovering each element of the
equivalent secret key from these diffusion mechanisms is only O(1). The
proposed analysis is validated by means of numerical examples. Some additional
cryptographic applications of our work are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09272</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09272</id><created>2015-12-31</created><authors><author><keyname>Kumar</keyname><forenames>B G Vijay</forenames></author><author><keyname>Carneiro</keyname><forenames>Gustavo</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author></authors><title>Learning Local Image Descriptors with Deep Siamese and Triplet
  Convolutional Networks by Minimising Global Loss Functions</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent innovations in training deep convolutional neural network (ConvNet)
models have motivated the design of new methods to automatically learn local
image descriptors. The latest deep ConvNets proposed for this task consist of a
siamese network that is trained by penalising misclassification of pairs of
local image patches. Current results from machine learning show that replacing
this siamese by a triplet network can improve the classification accuracy in
several problems, but this has yet to be demonstrated for local image
descriptor learning. Moreover, current siamese and triplet networks have been
trained with stochastic gradient descent that computes the gradient from
individual pairs or triplets of local image patches, which can make them prone
to overfitting. In this paper, we first propose the use of triplet networks for
the problem of local image descriptor learning. Furthermore, we also propose
the use of a global loss that minimises the overall classification error in the
training set, which can improve the generalisation capability of the model.
Using the UBC benchmark dataset for comparing local image descriptors, we show
that the triplet network produces a more accurate embedding than the siamese
network in terms of the UBC dataset errors. Moreover, we also demonstrate that
a combination of the triplet and global losses produces the best embedding in
the field, using this triplet network. Finally, we also show that the use of
the central-surround siamese network trained with the global loss produces the
best result of the field on the UBC dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09287</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09287</id><created>2015-12-31</created><authors><author><keyname>Potapov</keyname><forenames>Vladimir N.</forenames></author></authors><title>Linear codes for an effective quantization of data</title><categories>cs.IT math.CO math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a set $S$ of vertices of the Boolean
$n$-cube having cardinality $2^{n-k}$ and intersecting with maximum number of
$k$-dimensional faces. We prove an upper bound on the number of such faces. The
solution of the problem in the class of linear codes is found. Connections
between this problem, cryptography and an efficiency of quantization are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09293</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09293</id><created>2015-12-31</created><authors><author><keyname>Buzna</keyname><forenames>Lubos</forenames></author><author><keyname>Carvalho</keyname><forenames>Rui</forenames></author></authors><title>Controlling Congestion on Complex Networks</title><categories>physics.soc-ph cs.NI math.OC</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the Internet to road networks and the power grid, modern life depends on
controlling flows on critical infrastructure networks that often operate in a
congested state. Yet, we have a limited understanding of the relative
performance of the control mechanisms available to manage congestion and of the
interplay between network topology, path layout and congestion control
algorithms. Here, we consider two flow algorithms (max-flow and uniform-flow),
and two more realistic congestion control schemes (max-min fairness and
proportional fairness). We analyse how the algorithms and network topology
affect throughput, fairness and the location of bottleneck edges. Our results
show that on large random networks a network operator can implement the
trade-off (proportional fairness) instead of the fair allocation (max-min
fairness) with little sacrifice in throughput. We illustrate how the previously
studied uniform-flow approach leaves networks severely underutilised in
comparison with congestion control algorithms, and how uniform-flow can
(erroneously) lead to the conclusion that ER networks are more efficient than
SF at the onset of congestion. For proportional fairness and max-min fairness,
we found that SF networks are more unequal than ER. Finally, we found that
links with high edge betweenness centrality are good candidates for bottleneck
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09295</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09295</id><created>2015-12-31</created><authors><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author></authors><title>Strategies and Principles of Distributed Machine Learning on Big Data</title><categories>stat.ML cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of Big Data has led to new demands for Machine Learning (ML) systems
to learn complex models with millions to billions of parameters, that promise
adequate capacity to digest massive datasets and offer powerful predictive
analytics thereupon. In order to run ML algorithms at such scales, on a
distributed cluster with 10s to 1000s of machines, it is often the case that
significant engineering efforts are required --- and one might fairly ask if
such engineering truly falls within the domain of ML research or not. Taking
the view that Big ML systems can benefit greatly from ML-rooted statistical and
algorithmic insights --- and that ML researchers should therefore not shy away
from such systems design --- we discuss a series of principles and strategies
distilled from our recent efforts on industrial-scale ML solutions. These
principles and strategies span a continuum from application, to engineering,
and to theoretical research and development of Big ML systems and
architectures, with the goal of understanding how to make them efficient,
generally-applicable, and supported with convergence and scaling guarantees.
They concern four key questions which traditionally receive little attention in
ML research: How to distribute an ML program over a cluster? How to bridge ML
computation with inter-machine communication? How to perform such
communication? What should be communicated between machines? By exposing
underlying statistical and algorithmic characteristics unique to ML programs
but not typically seen in traditional computer programs, and by dissecting
successful cases to reveal how we have harnessed these principles to design and
develop both high-performance distributed ML software as well as
general-purpose ML frameworks, we present opportunities for ML researchers and
practitioners to further shape and grow the area that lies between ML and
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09300</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09300</id><created>2015-12-31</created><updated>2016-02-10</updated><authors><author><keyname>Larsen</keyname><forenames>Anders Boesen Lindbo</forenames></author><author><keyname>S&#xf8;nderby</keyname><forenames>S&#xf8;ren Kaae</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Autoencoding beyond pixels using a learned similarity metric</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an autoencoder that leverages learned representations to better
measure similarities in data space. By combining a variational autoencoder with
a generative adversarial network we can use learned feature representations in
the GAN discriminator as basis for the VAE reconstruction objective. Thereby,
we replace element-wise errors with feature-wise errors to better capture the
data distribution while offering invariance towards e.g. translation. We apply
our method to images of faces and show that it outperforms VAEs with
element-wise similarity measures in terms of visual fidelity. Moreover, we show
that the method learns an embedding in which high-level abstract visual
features (e.g. wearing glasses) can be modified using simple arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09301</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09301</id><created>2015-12-31</created><updated>2016-03-08</updated><authors><author><keyname>Hackl</keyname><forenames>C. M.</forenames></author><author><keyname>Kamper</keyname><forenames>M. J.</forenames></author><author><keyname>Kullick</keyname><forenames>J.</forenames></author><author><keyname>Mitchell</keyname><forenames>J.</forenames></author></authors><title>Nonlinear PI current control of reluctance synchronous machines</title><categories>cs.SY</categories><comments>9 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses nonlinear proportional-integral (PI) current control
with anti-windup of reluctance synchronous machines (RSMs) for which the flux
linkage maps are known. The nonlinear controller design is based on the tuning
rule &quot;Magnitude Optimum criterion&quot; [1]. Due to the nonlinear flux linkage, the
current dynamics of RSMs are highly nonlinear and, so, the parameters of the PI
controllers and the disturbance compensation feedforward control must be
adjusted online (e.g., for each sampling instant). The theoretical results are
illustrated and validated by simulation and measurement results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09307</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09307</id><created>2015-12-31</created><updated>2016-03-03</updated><authors><author><keyname>Sakuldee</keyname><forenames>Fattah</forenames></author><author><keyname>Suwanna</keyname><forenames>Sujin</forenames></author></authors><title>Unitary-Scaling Decomposition and Dissipative Behaviour in
  Finite-Dimensional Linblad Dynamics</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a decomposition of a dissipative dynamical map, which is used
in quantum dynamics of a finite open quantum system, into two distinct types of
mapping on a Hilbert-Schmidt space of quantum states. One type of the maps
corresponds to reversible behaviours, while the other to irreversible
characteristics. For a finite dimensional system, in which one can equip the
state-observable system by a complex matrix space, we employ real vectors or
Bloch representations and express a dynamical map on the state space as a real
matrix acting on the representation. It is found that rotation and scaling
transformations on the real vector space, obtained from the
orthogonal-symmetric or the real-polar decomposition, behave as building blocks
for a dynamical map. Together with the time parameter, we introduce an
additional parameter, which is related to a scaling parameter in the scaling
part of the dynamical matrix. Specifically for the Lindblad-type dynamical
maps, which form a one-parameter semigroup, we interpret the conditions on the
Lindblad map in our framework, where the scaling parameter can be expressed as
a function of time, inducing a one-parameter map. As results, we find that the
change of the linear entropy or purity, which indicates dissipative behaviours,
increases in time and possesses an asymptote expected in thermodynamics. The
rate of change of the linear entropy depends on the structure of the scaling
part of the dynamical matrix. In addition, the initial state plays an important
role in this rate of change. The dissipative behaviours and the partition of
eigensubspaces for bit-flipping, phase-flipping, and depolarisation matrices
are discussed and illustrated in qubit systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09311</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09311</id><created>2015-12-31</created><authors><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Finite-time Analysis of the Distributed Detection Problem</title><categories>cs.SY math.OC</categories><comments>6 pages, To Appear in Allerton Conference on Communication, Control,
  and Computing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of distributed detection in fixed and
switching networks. A network of agents observe partially informative signals
about the unknown state of the world. Hence, they collaborate with each other
to identify the true state. We propose an update rule building on distributed,
stochastic optimization methods. Our main focus is on the finite-time analysis
of the problem. For fixed networks, we bring forward the notion of
Kullback-Leibler cost to measure the efficiency of the algorithm versus its
centralized analog. We bound the cost in terms of the network size, spectral
gap and relative entropy of agents' signal structures. We further consider the
problem in random networks where the structure is realized according to a
stationary distribution. We then prove that the convergence is exponentially
fast (with high probability), and the non-asymptotic rate scales inversely in
the spectral gap of the expected network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09314</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09314</id><created>2015-12-31</created><authors><author><keyname>Chlebus</keyname><forenames>Bogdan S.</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author></authors><title>Asynchronous Exclusive Selection</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An asynchronous system of $n$ processes prone to crashes, along with a number
of shared read-write registers, is the distributed setting. We consider
assigning integer numbers to processes in an exclusive manner, in the sense
that no integer is assigned to two distinct processes. In the problem called
Renaming, any $k\le n$ processes, which hold original names from a range
$[N]=\{1,\ldots,N\}$, contend to acquire unique integers in a smaller range
$[M]$ as new names using some $r$ auxiliary shared registers. We develop a
wait-free $(k,N)$-renaming solution operating in $O(\log k (\log N + \log
k\log\log N))$ local steps, for $M=O(k)$, and with $r=O(k\log(N/k))$ auxiliary
registers. We develop a wait-free $k$-renaming algorithm operating in $O(k)$
time, with $M=2k-1$ and with $r=O(k^2)$ registers. We develop an almost
adaptive wait-free $N$-renaming algorithm, with $N$ known, operating in
$O(\log^2 k (\log N + \log k\log\log N))$ time, with $M=O(k)$ and with
$r=O(n\log(N/n))$ registers. We give a fully adaptive solution of Renaming,
with neither $k$ nor $N$ known, having $M=8k-\lg k-1$ as the bound on new
names, operating in $O(k)$ steps and using $O(n^2)$ registers. As regards lower
bounds, we show that a wait-free solution of Renaming requires
$1+\min\{k-2,\log_{2r} \frac{N}{2M}\}$ steps in the worst case. We apply
renaming algorithms to obtain solutions to a problem called Store-and-Collect,
which is about operations of storing and collecting under additional
requirements. We consider the problem called Unbounded-Naming in which
processes may repeatedly request new names, while no name can be reused once
assigned, so that infinitely many integers need to be exclusively assigned as
names in an execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09327</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09327</id><created>2015-12-31</created><authors><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author><author><keyname>Hasenclever</keyname><forenames>Leonard</forenames></author><author><keyname>Lienart</keyname><forenames>Thibaut</forenames></author><author><keyname>Vollmer</keyname><forenames>Sebastian</forenames></author><author><keyname>Webb</keyname><forenames>Stefan</forenames></author><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author><author><keyname>Blundell</keyname><forenames>Charles</forenames></author></authors><title>Distributed Bayesian Learning with Stochastic Natural-gradient
  Expectation Propagation and the Posterior Server</title><categories>cs.LG stat.ML</categories><comments>30 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper makes two contributions to Bayesian machine learning algorithms.
Firstly, we propose stochastic natural gradient expectation propagation (SNEP),
a novel alternative to expectation propagation (EP), a popular variational
inference algorithm. SNEP is a black box variational algorithm, in that it does
not require any simplifying assumptions on the distribution of interest, beyond
the existence of some Monte Carlo sampler for estimating the moments of the EP
tilted distributions. Further, as opposed to EP which has no guarantee of
convergence, SNEP can be shown to be convergent, even when using Monte Carlo
moment estimates. Secondly, we propose a novel architecture for distributed
Bayesian learning which we call the posterior server. The posterior server
allows scalable and robust Bayesian learning in cases where a dataset is stored
in a distributed manner across a cluster, with each compute node containing a
disjoint subset of data. An independent Markov chain Monte Carlo (MCMC) sampler
is run on each compute node, with direct access only to the local data subset,
but which targets an approximation to the global posterior distribution given
all data across the whole cluster. This is achieved by using a distributed
asynchronous implementation of SNEP to pass messages across the cluster. We
demonstrate SNEP and the posterior server on distributed Bayesian learning of
logistic regression and neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09328</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09328</id><created>2015-12-23</created><authors><author><keyname>Dridi</keyname><forenames>Raouf</forenames></author><author><keyname>Alghassi</keyname><forenames>Hedayat</forenames></author></authors><title>Homology Computation of Large Point Clouds using Quantum Annealing</title><categories>quant-ph cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a quantum annealing algorithm for computation of
homology of large point clouds. It is designed to work on resizable cloud
computing platforms. The algorithm takes as input a witness complex K
approximating the given point cloud. It uses quantum annealing to compute a
clique cover of the 1-skeleton of K and then uses this cover to blow up K into
a Mayer-Vietoris complex. Finally, it computes the homology of the
Mayer-Vietoris complex in parallel. The novelty here is the use of clique
covering which, compared to graph partitioning, substantially reduces the
computational load assigned to each slave machine in the computing cluster. We
describe two different clique covers and their quantum annealing formulation in
the form of quadratic unconstrained binary optimization (QUBO).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09333</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09333</id><created>2015-12-31</created><updated>2016-01-25</updated><authors><author><keyname>Elkayam</keyname><forenames>Nir</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>On the calculation of the minimax-converse of the channel coding problem</title><categories>cs.IT math.IT</categories><comments>Extended version of a submission to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A minimax-converse has been suggested for the general channel coding problem
by Polyanskiy etal. This converse comes in two flavors. The first flavor is
generally used for the analysis of the coding problem with non-vanishing error
probability and provides an upper bound on the rate given the error
probability. The second flavor fixes the rate and provides a lower bound on the
error probability. Both converses are given as a min-max optimization problem
of an appropriate binary hypothesis testing problem. The properties of the
first converse were studies by Polyanskiy and a saddle point was proved. In
this paper we study the properties of the second form and prove that it also
admits a saddle point. Moreover, an algorithm for the computation of the saddle
point, and hence the bound, is developed. In the DMC case, the algorithm runs
in a polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09335</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09335</id><created>2015-12-31</created><updated>2016-01-26</updated><authors><author><keyname>Dahan</keyname><forenames>Mathieu</forenames></author><author><keyname>Amin</keyname><forenames>Saurabh</forenames></author></authors><title>Network Flow Routing under Strategic Link Disruptions</title><categories>cs.GT cs.SY</categories><comments>8 pages, 6 figures, Proceedings of the 53rd Annual Allerton
  Conference on Communication, Control, and Computing, Urbana, IL, October 2015</comments><msc-class>91A43 (Primary), 05C21 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a 2-player strategic game for network routing under link
disruptions. Player 1 (defender) routes flow through a network to maximize her
value of effective flow while facing transportation costs. Player 2 (attacker)
simultaneously disrupts one or more links to maximize her value of lost flow
but also faces cost of disrupting links. This game is strategically equivalent
to a zero-sum game. Linear programming duality and the max-flow min-cut theorem
are applied to obtain properties that are satisfied in any mixed Nash
equilibrium. In any equilibrium, both players achieve identical payoffs. While
the defender's expected transportation cost decreases in attacker's marginal
value of lost flow, the attacker's expected cost of attack increases in
defender's marginal value of effective flow. Interestingly, the expected amount
of effective flow decreases in both these parameters. These results can be
viewed as a generalization of the classical max-flow with minimum
transportation cost problem to adversarial environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09349</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09349</id><created>2015-12-31</created><authors><author><keyname>Turner</keyname><forenames>Jonathan</forenames></author></authors><title>Faster Maximium Priority Matchings in Bipartite Graphs</title><categories>cs.DS</categories><report-no>WUCSE-2015-08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A maximum priority matching is a matching in an undirected graph that
maximizes a priority score defined with respect to given vertex priorities. An
earlier paper showed how to find maximum priority matchings in unweighted
graphs. This paper describes an algorithm for bipartite graphs that is faster
when the number of distinct priority classes is limited. For graphs with $k$
distinct priority classes it runs in $O(kmn^{1/2})$ time, where $n$ is the
number of vertices in the graph and $m$ is the number of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09354</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09354</id><created>2015-12-31</created><updated>2016-02-04</updated><authors><author><keyname>D'Andreagiovanni</keyname><forenames>Fabio</forenames></author><author><keyname>Mett</keyname><forenames>Fabian</forenames></author><author><keyname>Pulaj</keyname><forenames>Jonad</forenames></author></authors><title>An (MI)LP-based Primal Heuristic for 3-Architecture Connected Facility
  Location in Urban Access Network Design</title><categories>math.OC cs.AI cs.NI</categories><comments>Accepted for publication in Proc. of EvoStar - EvoApplications 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the 3-architecture Connected Facility Location Problem arising
in the design of urban telecommunication access networks. We propose an
original optimization model for the problem that includes additional variables
and constraints to take into account wireless signal coverage. Since the
problem can prove challenging even for modern state-of-the art optimization
solvers, we propose to solve it by an original primal heuristic which combines
a probabilistic fixing procedure, guided by peculiar Linear Programming
relaxations, with an exact MIP heuristic, based on a very large neighborhood
search. Computational experiments on a set of realistic instances show that our
heuristic can find solutions associated with much lower optimality gaps than a
state-of-the-art solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09358</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09358</id><created>2015-12-31</created><updated>2016-02-17</updated><authors><author><keyname>Kuijper</keyname><forenames>Wouter</forenames></author></authors><title>Geometric Memory Management</title><categories>cs.DS</categories><comments>This is a draft version of the report, future updates are planned</comments><msc-class>60K30</msc-class><acm-class>D.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we discuss the concepts of geometric memory align- ment,
geometric memory allocation and geometric memory mapping. We introduce block
trees as an efficient data structure for representing geo- metrically aligned
block allocation states. We introduce niche maps as an efficient means to find
the right place to allocate a chunk of a given size whilst maintaining good
packing and avoiding fragmentation. We intro- duce ledging as a process to deal
with chunk sizes that are not a power of two. We discuss using block trees for
memory mapping in the context of virtual memory management. We show how ledging
can also be applied at the level of virtual memory in order to create fixed
size virtual mem- ory spaces. Finally we discuss implementation of both
geometric memory allocation as well as geometric memory mapping in hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09363</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09363</id><created>2015-12-31</created><authors><author><keyname>Oliveira</keyname><forenames>Fabiano de S.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author></authors><title>Counting independent terms in big-oh notation</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of computational complexity is concerned both with the intrinsic
hardness of computational problems and with the efficiency of algorithms to
solve them. Given such a problem, normally one designs an algorithm to solve it
and sets about establishing bounds on its performance as functions of the
algorithm's variables, particularly upper bounds expressed via the big-oh
notation. But if we were given some inscrutable code and were asked to figure
out its big-oh profile from performance data on a given set of inputs, how hard
would we have to grapple with the various possibilities before zooming in on a
reasonably small set of candidates? Here we show that, even if we restricted
our search to upper bounds given by polynomials, the number of possibilities
could be arbitrarily large for two or more variables. This is unexpected, given
the available body of examples on algorithmic efficiency, and serves to
illustrate the many facets of the big-oh notation, as well as its
counter-intuitive twists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.09369</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.09369</id><created>2015-12-31</created><authors><author><keyname>Lopez-Garcia</keyname><forenames>Pedro</forenames></author><author><keyname>Haemmerle</keyname><forenames>Remy</forenames></author><author><keyname>Klemen</keyname><forenames>Maximiliano</forenames></author><author><keyname>Liqat</keyname><forenames>Umer</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author></authors><title>Towards Energy Consumption Verification via Static Analysis</title><categories>cs.PL cs.DC cs.LO</categories><comments>Presented at HIP3ES, 2015 (arXiv: 1501.03064)</comments><report-no>HIP3ES/2015/04</report-no><acm-class>F.3.2; D.3.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we leverage an existing general framework for resource usage
verification and specialize it for verifying energy consumption specifications
of embedded programs. Such specifications can include both lower and upper
bounds on energy usage, and they can express intervals within which energy
usage is to be certified to be within such bounds. The bounds of the intervals
can be given in general as functions on input data sizes. Our verification
system can prove whether such energy usage specifications are met or not. It
can also infer the particular conditions under which the specifications hold.
To this end, these conditions are also expressed as intervals of functions of
input data sizes, such that a given specification can be proved for some
intervals but disproved for others. The specifications themselves can also
include preconditions expressing intervals for input data sizes. We report on a
prototype implementation of our approach within the CiaoPP system for the XC
language and XS1-L architecture, and illustrate with an example how embedded
software developers can use this tool, and in particular for determining values
for program parameters that ensure meeting a given energy budget while
minimizing the loss in quality of service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00013</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00013</id><created>2015-12-31</created><authors><author><keyname>Guliyev</keyname><forenames>Namig J.</forenames></author><author><keyname>Ismailov</keyname><forenames>Vugar E.</forenames></author></authors><title>A single hidden layer feedforward network with only one neuron in the
  hidden layer can approximate any univariate function</title><categories>cs.NE cs.IT math.IT math.NA</categories><comments>11 pages, 1 figure; for associated SageMath worksheet, see
  http://sites.google.com/site/njguliyev/papers/sigmoidal</comments><msc-class>41A30, 92B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility of approximating a continuous function on a compact subset of
the real line by a feedforward single hidden layer neural network with a
sigmoidal activation function has been studied in many papers. Such networks
can approximate an arbitrary continuous function provided that an unlimited
number of neurons in a hidden layer is permitted. In this paper, we consider
constructive approximation on any finite interval of $\mathbb{R}$ by neural
networks with only one neuron in the hidden layer. We construct algorithmically
a smooth, sigmoidal, almost monotone activation function $\sigma$ providing
approximation to an arbitrary continuous function within any degree of
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00019</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00019</id><created>2015-12-31</created><authors><author><keyname>Ji</keyname><forenames>Hyoungju</forenames></author><author><keyname>Kim</keyname><forenames>Younsun</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author><author><keyname>Onggosanusi</keyname><forenames>Eko</forenames></author><author><keyname>Nam</keyname><forenames>Younghan</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong</forenames></author><author><keyname>Lee</keyname><forenames>Byungju</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Overview of Full-Dimension MIMO in LTE-Advanced Pro</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) systems with large number of
basestation antennas, often called massive MIMO systems, have received much
attention in academia and industry as a means to improve the spectral
efficiency, energy efficiency, and also processing complexity. Mobile
communication industry has initiated a feasibility study to meet the increasing
demand of future wireless systems. Field trials of the proof-of-concept systems
have demonstrated the potential gain of the FD-MIMO and 3rd generation
partnership project (3GPP) standard body has initiated the standardization
activity for the seamless integration of this technology into current 4G
cellular carrier frequency. A study item, process done before a formal
standardization process, has been completed in June 2015 and the follow up
(work item) process will be initiated shortly for the formal standardization of
Release 13. In this article, we provides an overview of the FD-MIMO system,
with emphasis on the discussion and debate conducted on standardization process
of Release 13. We present key features for FD-MIMO systems, summary of the
major issues for the standardization and practical system design, and
performance evaluation for typical FD-MIMO scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00022</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00022</id><created>2015-12-31</created><updated>2016-01-04</updated><authors><author><keyname>Li</keyname><forenames>Hongzhi</forenames></author><author><keyname>Ellis</keyname><forenames>Joseph G.</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Event Specific Multimodal Pattern Mining with Image-Caption Pairs</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a novel framework and algorithms for discovering
image patch patterns from a large corpus of weakly supervised image-caption
pairs generated from news events. Current pattern mining techniques attempt to
find patterns that are representative and discriminative, we stipulate that our
discovered patterns must also be recognizable by humans and preferably with
meaningful names. We propose a new multimodal pattern mining approach that
leverages the descriptive captions often accompanying news images to learn
semantically meaningful image patch patterns. The mutltimodal patterns are then
named using words mined from the associated image captions for each pattern. A
novel evaluation framework is provided that demonstrates our patterns are 26.2%
more semantically meaningful than those discovered by the state of the art
vision only pipeline, and that we can provide tags for the discovered images
patches with 54.5% accuracy with no direct supervision. Our methods also
discover named patterns beyond those covered by the existing image datasets
like ImageNet. To the best of our knowledge this is the first algorithm
developed to automatically mine image patch patterns that have strong semantic
meaning specific to high-level news events, and then evaluate these patterns
based on that criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00024</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00024</id><created>2015-12-31</created><authors><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Samulowitz</keyname><forenames>Horst</forenames></author><author><keyname>Tesauro</keyname><forenames>Gerald</forenames></author></authors><title>Selecting Near-Optimal Learners via Incremental Data Allocation</title><categories>cs.LG stat.ML</categories><comments>AAAI-2016: The Thirtieth AAAI Conference on Artificial Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a novel machine learning (ML) problem setting of sequentially
allocating small subsets of training data amongst a large set of classifiers.
The goal is to select a classifier that will give near-optimal accuracy when
trained on all data, while also minimizing the cost of misallocated samples.
This is motivated by large modern datasets and ML toolkits with many
combinations of learning algorithms and hyper-parameters. Inspired by the
principle of &quot;optimism under uncertainty,&quot; we propose an innovative strategy,
Data Allocation using Upper Bounds (DAUB), which robustly achieves these
objectives across a variety of real-world datasets.
  We further develop substantial theoretical support for DAUB in an idealized
setting where the expected accuracy of a classifier trained on $n$ samples can
be known exactly. Under these conditions we establish a rigorous sub-linear
bound on the regret of the approach (in terms of misallocated data), as well as
a rigorous bound on suboptimality of the selected classifier. Our accuracy
estimates using real-world datasets only entail mild violations of the
theoretical scenario, suggesting that the practical behavior of DAUB is likely
to approach the idealized behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00025</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00025</id><created>2015-12-31</created><authors><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Saleh</keyname><forenames>Babak</forenames></author></authors><title>Write a Classifier: Predicting Visual Classifiers from Unstructured Text
  Descriptions</title><categories>cs.CV cs.CL cs.LG</categories><comments>Journal submission. arXiv admin note: text overlap with
  arXiv:1506.08529</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People typically learn through exposure to visual concepts associated with
linguistic descriptions. For instance, teaching visual object categories to
children is often accompanied by descriptions in text or speech. In a machine
learning context, these observations motivates us to ask whether this learning
process could be computationally modeled to learn visual classifiers. More
specifically, the main question of this work is how to utilize purely textual
description of visual classes with no training images, to learn explicit visual
classifiers for them. We propose and investigate two baseline formulations,
based on regression and domain transfer that predict a linear classifier. Then,
we propose a new constrained optimization formulation that combines a
regression function and a knowledge transfer function with additional
constraints to predict the linear classifier parameters for new classes. We
also propose a generic kernelized models where a kernel classifier, in the form
defined by the representer theorem, is predicted. The kernelized models allow
defining any two RKHS kernel functions in the visual space and text space,
respectively, and could be useful for other applications. We finally propose a
kernel function between unstructured text descriptions that builds on
distributional semantics, which shows an advantage in our setting and could be
useful for other applications. We applied all the studied models to predict
visual classifiers for two fine-grained categorization datasets, and the
results indicate successful predictions of our final model against several
baselines that we designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00027</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00027</id><created>2015-12-31</created><authors><author><keyname>Fuchs</keyname><forenames>Thomas J.</forenames></author><author><keyname>Buhmann</keyname><forenames>Joachim M.</forenames></author></authors><title>Computational Pathology: Challenges and Promises for Tissue Analysis</title><categories>cs.CV cs.AI</categories><journal-ref>Computerized Medical Imaging and Graphics, vol. 35, 7-8, p.
  515-530, 2011</journal-ref><doi>10.1016/j.compmedimag.2011.02.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The histological assessment of human tissue has emerged as the key challenge
for detection and treatment of cancer. A plethora of different data sources
ranging from tissue microarray data to gene expression, proteomics or
metabolomics data provide a detailed overview of the health status of a
patient. Medical doctors need to assess these information sources and they rely
on data driven automatic analysis tools. Methods for classification, grouping
and segmentation of heterogeneous data sources as well as regression of noisy
dependencies and estimation of survival probabilities enter the processing
workflow of a pathology diagnosis system at various stages. This paper reports
on state-of-the-art of the design and effectiveness of computational pathology
workflows and it discusses future research directions in this emergent field of
medical informatics and diagnostic machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00028</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00028</id><created>2015-12-31</created><authors><author><keyname>Funai</keyname><forenames>Colin</forenames></author><author><keyname>Tapparello</keyname><forenames>Cristiano</forenames></author><author><keyname>Heinzelman</keyname><forenames>Wendi</forenames></author></authors><title>Supporting Multi-hop Device-to-Device Networks Through WiFi Direct
  Multi-group Networking</title><categories>cs.NI</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing availability of mobile devices that natively support
Device-to-Device (D2D) communication protocols, we are presented with a unique
opportunity to realize large scale ad hoc wireless networks. Recently, a novel
D2D protocol named WiFi Direct has been proposed and standardized by the WiFi
Alliance with the objective of facilitating the interconnection of nearby
devices. However, WiFi Direct has been designed following a client-server
hierarchical architecture, where a single device manages all the communications
within a group of devices. In this paper, we propose and analyze different
solutions for supporting the communications between multiple WiFi Direct groups
using Android OS devices. By describing the WiFi Direct standard and the
limitations of the current implementation of the Android WiFi Direct framework,
we present possible solutions to interconnect different groups to create
multi-hop ad hoc networks. Experimental results show that our proposed
approaches are feasible with different overhead in terms of energy consumption
and delay at the gateway node. Additionally, our experimental results
demonstrate the superiority of techniques that exploit the device ability to
maintain simultaneous physical connections to multiple groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00032</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00032</id><created>2015-12-31</created><authors><author><keyname>Dur&#xe1;n</keyname><forenames>Guillermo</forenames></author><author><keyname>Safe</keyname><forenames>Mart&#xed;n D.</forenames></author><author><keyname>Warnes</keyname><forenames>Xavier S.</forenames></author></authors><title>Neighborhood covering and independence on two superclasses of cographs</title><categories>cs.DM math.CO</categories><msc-class>05C15, 05C69, 05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simple graph $G$, a set $C \subseteq V(G)$ is a neighborhood cover
set if every edge and vertex of $G$ belongs to some $G[v]$ with $v \in C$,
where $G[v]$ denotes the subgraph of $G$ induced by the closed neighborhood of
the vertex $v$. Two elements of $E(G) \cup V(G)$ are neighborhood-independent
if there is no vertex $v\in V(G)$ such that both elements are in $G[v]$. A set
$S\subseteq V(G)\cup E(G)$ is neighborhood-independent if every pair of
elements of $S$ is neighborhood-independent. Let $\rho_{\mathrm n}(G)$ be the
size of a minimum neighborhood cover set and $\alpha_{\mathrm n}(G)$ of a
maximum neighborhood-independent set. Lehel and Tuza defined
neighborhood-perfect graphs $G$ as those where the equality $\rho_{\mathrm
n}(G') = \alpha_{\mathrm n}(G')$ holds for every induced subgraph $G'$ of $G$.
  In this work we prove forbidden induced subgraph characterizations of the
class of neighborhood-perfect graphs, restricted to two superclasses of
cographs: $P_4$-tidy graphs and tree-cographs. We give as well linear-time
algorithms for solving the recognition problem of neighborhood-perfect graphs
and the problem of finding a minimum neighborhood cover set and a maximum
neighborhood-independent set in these same classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00034</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00034</id><created>2015-12-31</created><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Stochastic Neural Networks with Monotonic Activation Functions</title><categories>stat.ML cs.LG cs.NE</categories><comments>AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Laplace approximation that closely links any smooth monotonic
activation function to its stochastic counterpart using Gaussian noise. We
investigate the application of this approximation in training a family of
Restricted Boltzmann Machines (RBM) that are closely linked to Bregman
divergences. This family -- exponential family RBM (Exp-RBM) -- is a subset of
the exponential family Harmoniums that expresses family members through a
choice of smooth monotonic non-linearities for neurons. Using contrastive
divergence along with our Gaussian approximation, we show that Exp-RBMs can
learn useful representations using novel stochastic units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00042</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00042</id><created>2015-12-31</created><authors><author><keyname>Starek</keyname><forenames>Joseph A.</forenames></author><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Maher</keyname><forenames>Gabriel D.</forenames></author><author><keyname>Barbee</keyname><forenames>Brent W.</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Fast, Safe, and Propellant-Efficient Spacecraft Planning under
  Clohessy-Wiltshire-Hill Dynamics</title><categories>cs.SY</categories><comments>Submitted to the AIAA Journal of Guidance, Control, and Dynamics
  (JGCD) special issue entitled &quot;Computational Guidance and Control&quot;. This
  submission is the journal version corresponding to the conference manuscript
  &quot;Real-Time, Propellant-Efficient Spacecraft Planning under
  Clohessy-Wiltshire-Hill Dynamics&quot; accepted to the 2016 IEEE Aerospace
  Conference in Big Sky, MT, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a sampling-based motion planning algorithm for real-time
and propellant-optimized autonomous spacecraft trajectory generation in
near-circular orbits. Specifically, this paper leverages recent algorithmic
advances in the field of robot motion planning to the problem of
impulsively-actuated, propellant-optimized rendezvous and proximity operations
under the Clohessy-Wiltshire-Hill (CWH) dynamics model. The approach calls upon
a modified version of the Fast Marching Tree (FMT*) algorithm to grow a set of
feasible trajectories over a deterministic, low-dispersion set of sample points
covering the free state space. To enforce safety, the tree is only grown over
the subset of actively-safe samples, from which there exists a feasible
one-burn collision avoidance maneuver that can safely circularize the
spacecraft orbit along its coasting arc under a given set of potential thruster
failures. Key features of the proposed algorithm include: (i) theoretical
guarantees in terms of trajectory safety and performance, (ii) amenability to
real-time implementation, and (iii) generality, in the sense that a large class
of constraints can be handled directly. As a result, the proposed algorithm
offers the potential for widespread application, ranging from on-orbit
satellite servicing to orbital debris removal and autonomous inspection
missions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00062</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00062</id><created>2016-01-01</created><authors><author><keyname>Luo</keyname><forenames>Jerry</forenames></author><author><keyname>Shapiro</keyname><forenames>Kayla</forenames></author><author><keyname>Shi</keyname><forenames>Hao-Jun Michael</forenames></author><author><keyname>Yang</keyname><forenames>Qi</forenames></author><author><keyname>Zhu</keyname><forenames>Kan</forenames></author></authors><title>Practical Algorithms for Learning Near-Isometric Linear Embeddings</title><categories>stat.ML cs.LG math.OC</categories><msc-class>90C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two practical non-convex approaches for learning near-isometric,
linear embeddings of finite sets of data points. Following Hegde, et. al. [6],
given a set of training points $\mathcal{X}$, we consider the secant set
$S(\mathcal{X})$ that consists of all pairwise difference vectors of
$\mathcal{X}$, normalized to lie on the unit sphere. The problem can be
formulated as finding a symmetric and positive semi-definite matrix $\psi$ that
preserves the norms of all the vectors in $S(\mathcal{X})$ up to a distortion
parameter $\delta$. Motivated by non-negative matrix factorization, we
reformulate our problem into a Frobenius norm minimization problem, which is
solved by the Alternating Direction Method of Multipliers (ADMM) and develop an
algorithm, FroMax. Another method solves for a projection matrix $\psi$ by
minimizing the restricted isometry property (RIP) directly over the set of
symmetric, positive semi-definite matrices. Applying ADMM and a Moreau
decomposition on a proximal mapping, we develop another algorithm, NILE-Pro,
for dimensionality reduction. Both non-convex approaches are then empirically
demonstrated to be more computationally efficient than prior convex approaches
for a number of applications in machine learning and signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00072</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00072</id><created>2016-01-01</created><updated>2016-01-05</updated><authors><author><keyname>Almazrooie</keyname><forenames>Mishal</forenames></author><author><keyname>Vadiveloo</keyname><forenames>Mogana</forenames></author><author><keyname>Abdullah</keyname><forenames>Rosni</forenames></author></authors><title>GPU-Based Fuzzy C-Means Clustering Algorithm for Image Segmentation</title><categories>cs.DC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fast and practical GPU-based implementation of Fuzzy C-Means
(FCM) clustering algorithm for image segmentation is proposed. First, an
extensive analysis is conducted to study the dependency among the image pixels
in the algorithm for parallelization. The proposed GPU-based FCM has been
tested on digital brain simulated dataset to segment white matter(WM), gray
matter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The execution
time of the sequential FCM is 2798 seconds for an image dataset with the size
of 1MB. While the proposed GPU-based FCM requires only 4.2seconds for the
similar size of image dataset. An estimated 674-fold superlinear speedup is
measured for the data size of 700 KB on a CUDA device that has 448 processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00073</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00073</id><created>2016-01-01</created><authors><author><keyname>Nandi</keyname><forenames>Arindam</forenames></author><author><keyname>Yang</keyname><forenames>Ying</forenames></author><author><keyname>Kennedy</keyname><forenames>Oliver</forenames></author><author><keyname>Glavic</keyname><forenames>Boris</forenames></author><author><keyname>Fehling</keyname><forenames>Ronny</forenames></author><author><keyname>Liu</keyname><forenames>Zhen Hua</forenames></author><author><keyname>Gawlick</keyname><forenames>Dieter</forenames></author></authors><title>Mimir: Bringing CTables into Practice</title><categories>cs.DB cs.PL</categories><comments>Under submission; The first two authors should be considered a joint
  first-author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present state of the art in analytics requires high upfront investment of
human effort and computational resources to curate datasets, even before the
first query is posed. So-called pay-as-you-go data curation techniques allow
these high costs to be spread out, first by enabling queries over uncertain and
incomplete data, and then by assessing the quality of the query results. We
describe the design of a system, called Mimir, around a recently introduced
class of probabilistic pay-as-you-go data cleaning operators called Lenses.
Mimir wraps around any deterministic database engine using JDBC, extending it
with support for probabilistic query processing. Queries processed through
Mimir produce uncertainty-annotated result cursors that allow client
applications to quickly assess result quality and provenance. We also present a
GUI that provides analysts with an interactive tool for exploring the
uncertainty exposed by the system. Finally, we present optimizations that make
Lenses scalable, and validate this claim through experimental evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00082</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00082</id><created>2016-01-01</created><authors><author><keyname>Barbosa</keyname><forenames>Geraldo A.</forenames></author></authors><title>A wireless physically secure key distribution system</title><categories>cs.CR</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A secure key distribution protocol protected by light's noise was introduced
in 2003 [Phys. Rev. A 68, 052307 (2003)]. That protocol utilized the shot noise
of light present in the optical channel (eg., an optical fiber) to restrict
information leaks to an adversary. An initial shared information between the
legitimate users allowed them to extract more information from the channel than
the one obtained by the adversary. That original paper recognized the need for
a privacy amplification step but no specific protocol was presented. More
recently that original idea was improved with a specific privacy amplification
protocol [arXiv:1406.1543v2 [cs.CR] 8 Jul 2015] while keeping the use of an
optical communication channel. This work merges main ideas of the protection
given by the light's noise in a protocol applied to wireless channels. The use
of a wireless channels together with recorded physical noise was introduced
from 2005 to 2007 (see eg, arXiv:quant-ph/0510011 v2 16 Nov 2005 and
arXiv:0705.2243v2 [quant-ph] 17 May 2007). This work improves those embrionary
ideas of wireless channels secured by recorded optical noise. The need for
specific optical channels is eliminated with the wireless variation and opens
up the possibility to apply the technique to mobile devices. This work
introduces this new scheme and calculates the associated security level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00087</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00087</id><created>2016-01-01</created><updated>2016-01-04</updated><authors><author><keyname>Korayem</keyname><forenames>Mohammed</forenames></author></authors><title>Sentiment/Subjectivity Analysis Survey for Languages other than English</title><categories>cs.CL</categories><comments>This is a little bit old survey from my Ph.D qualifying exams,
  possibly of interest to someone somewhere</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subjective and sentiment analysis has gained considerable attention recently.
Most of the resources and systems built so far are done for English. The need
for designing systems for other languages is increasing. This paper surveys
different ways used for building systems for subjective and sentiment analysis
for languages other than English. There are three different types of systems
used for building these systems. The first (and the best) one is the language
specific systems. The second type of systems involves reusing or transferring
sentiment resources from English to the target language. The third type of
methods is based on using language independent methods. The paper presents a
separate section devoted to Arabic sentiment analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00088</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00088</id><created>2016-01-01</created><authors><author><keyname>Chan</keyname><forenames>Stanley H.</forenames></author><author><keyname>Zickler</keyname><forenames>Todd</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author></authors><title>Demystifying Symmetric Smoothing Filters</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many patch-based image denoising algorithms can be formulated as applying a
smoothing filter to the noisy image. Expressed as matrices, the smoothing
filters must be row normalized so that each row sums to unity. Surprisingly, if
we apply a column normalization before the row normalization, the performance
of the smoothing filter can often be significantly improved. Prior works showed
that such performance gain is related to the Sinkhorn-Knopp balancing
algorithm, an iterative procedure that symmetrizes a row-stochastic matrix to a
doubly-stochastic matrix. However, a complete understanding of the performance
gain phenomenon is still lacking.
  In this paper, we study the performance gain phenomenon from a statistical
learning perspective. We show that Sinkhorn-Knopp is equivalent to an
Expectation-Maximization (EM) algorithm of learning a Product of Gaussians
(PoG) prior of the image patches. By establishing the correspondence between
the steps of Sinkhorn-Knopp and the EM algorithm, we provide a geometrical
interpretation of the symmetrization process. The new PoG model also allows us
to develop a new denoising algorithm called Product of Gaussian Non-Local-Means
(PoG-NLM). PoG-NLM is an extension of the Sinkhorn-Knopp and is a
generalization of the classical non-local means. Despite its simple
formulation, PoG-NLM outperforms many existing smoothing filters and has a
similar performance compared to BM3D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00098</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00098</id><created>2016-01-01</created><authors><author><keyname>Ponomarev</keyname><forenames>Anton</forenames></author></authors><title>Nonlinear Predictor Feedback for Input-Affine Systems with Distributed
  Input Delays</title><categories>math.OC cs.SY</categories><doi>10.1109/TAC.2015.2496191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction-based transformation is applied to control-affine systems with
distributed input delays. Transformed system state is calculated as a
prediction of the system's future response to the past input with future input
set to zero. Stabilization of the new system leads to Lyapunov-Krasovskii
proven stabilization of the original one. Conditions on the original system
are: smooth linearly bounded open-loop vector field and smooth uniformly
bounded input vectors. About the transformed system which turns out to be
affine in the undelayed input but with input vectors dependent on the input
history and system state, we assume existence of a linearly bounded stabilizing
feedback and quadratically bounded control-Lyapunov function. If all
assumptions hold globally, then achieved exponential stability is global,
otherwise local. Analytical and numerical control design examples are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00112</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00112</id><created>2016-01-01</created><authors><author><keyname>Troshchiev</keyname><forenames>Yu. V.</forenames></author></authors><title>Stability and bifurcation properties of the algorithms for keeping of
  differential equations solutions on the required level</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms of control of differential equations solutions are under
investigation in the article. Idealized and real modifications of the
algorithms are distinguished. An equation, which can be the base equation for
investigation of the idealized algorithms properties, is constructed. The
difference appearing for real systems and real algorithms is for separate
investigation. This difference tends to zero under tending to zero of the time
step of control. If the systems of equations satisfy or almost satisfy some
properties for which the algorithms are intended, then the results are similar
numerically as well. One of the algorithms demonstrates high reliability.
Another one is of more complex properties. Bifurcations, periodic solutions and
strange attractors are possible in both algorithms in addition to stable steady
states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00119</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00119</id><created>2016-01-01</created><authors><author><keyname>McKay</keyname><forenames>John</forenames></author><author><keyname>Raj</keyname><forenames>Raghu</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Isaacs</keyname><forenames>Jason</forenames></author></authors><title>Discriminative Sparsity for Sonar ATR</title><categories>cs.CV</categories><comments>Conference Paper for Oceans 2015 Washington DC (IEEE and MTS
  organizers)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancements in Sonar image capture have enabled researchers to apply
sophisticated object identification algorithms in order to locate targets of
interest in images such as mines. Despite progress in this field, modern sonar
automatic target recognition (ATR) approaches lack robustness to the amount of
noise one would expect in real-world scenarios, the capability to handle
blurring incurred from the physics of image capture, and the ability to excel
with relatively few training samples. We address these challenges by adapting
modern sparsity-based techniques with dictionaries comprising of training from
each class. We develop new discriminative (as opposed to generative) sparse
representations which can help automatically classify targets in Sonar imaging.
Using a simulated SAS data set from the Naval Surface Warfare Center (NSWC), we
obtained compelling classification rates for multi-class problems even in cases
with considerable noise and sparsity in training samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00122</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00122</id><created>2016-01-01</created><authors><author><keyname>Alkadeki</keyname><forenames>Hatm</forenames></author><author><keyname>Wang</keyname><forenames>Xingang</forenames></author><author><keyname>Odetayo</keyname><forenames>Michael</forenames></author></authors><title>Improving Performance of IEEE 802.11 by a Dynamic Control Backoff
  Algorithm Under Unsaturated Traffic Loads</title><categories>cs.NI</categories><comments>9pages,6figures,2015, International Journal of Wireless &amp; Mobile
  Networks (IJWMN) Vol. 7, No. 6, December 2015</comments><doi>10.5121/ijwmn.2015.7605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 802.11 backoff algorithm is very important for controlling system
throughput over contentionbased wireless networks. For this reason, there are
many studies on wireless network performance focus on developing backoff
algorithms. However, most existing models are based on saturated traffic loads,
which are not a real representation of actual network conditions. In this
paper, a dynamic control backoff time algorithm is proposed to enhance both
delay and throughput performance of the IEEE 802.11 distributed coordination
function. This algorithm considers the distinction between high and low traffic
loads in order to deal with unsaturated traffic load conditions. In particular,
the equilibrium point analysis model is used to represent the algorithm under
various traffic load conditions. Results of extensive simulation experiments
illustrate that the proposed algorithm yields better performance throughput and
a better average transmission packet delay than related algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00126</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00126</id><created>2016-01-01</created><authors><author><keyname>Ballet</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Pieltant</keyname><forenames>Julia</forenames></author><author><keyname>Rambaud</keyname><forenames>Matthieu</forenames></author></authors><title>On some bounds for symmetric tensor rank of multiplication in finite
  fields</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1107.1184</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish new upper bounds about symmetric bilinear complexity in any
extension of finite fields. Note that these bounds are not asymptotical but
uniform. Moreover, we discuss the validity of certain published bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00129</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00129</id><created>2016-01-01</created><authors><author><keyname>Attia</keyname><forenames>Ahmed</forenames></author><author><keyname>Stefanescu</keyname><forenames>Razvan</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>The Reduced-Order Hybrid Monte Carlo Sampling Smoother</title><categories>cs.NA stat.AP stat.CO</categories><comments>32 pages, 2 figures</comments><report-no>CSTR-1/2016</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid Monte-Carlo (HMC) sampling smoother is a fully non-Gaussian
four-dimensional data assimilation algorithm that works by directly sampling
the posterior distribution formulated in the Bayesian framework. The smoother
in its original formulation is computationally expensive due to the intrinsic
requirement of running the forward and adjoint models repeatedly. Here we
present computationally efficient versions of the HMC sampling smoother based
on reduced-order approximations of the underlying model dynamics. The schemes
developed herein are tested numerically using the shallow-water equations model
on Cartesian coordinates. The results reveal that the reduced-order versions of
the smoother are capable of accurately capturing the posterior probability
density, while being significantly faster than the original full order
formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00139</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00139</id><created>2016-01-02</created><authors><author><keyname>Garrido</keyname><forenames>Camilo</forenames></author><author><keyname>Mora</keyname><forenames>Ricardo</forenames></author><author><keyname>Gutierrez</keyname><forenames>Claudio</forenames></author></authors><title>Group Centrality for Semantic Networks: a SWOT analysis featuring Random
  Walks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group centrality is an extension of the classical notion of centrality for
individuals, to make it applicable to sets of them. We perform a SWOT
(strengths, weaknesses, opportunities and threats) analysis of the use of group
centrality in semantic networks, for different centrality notions: degree,
closeness, betweenness, giving prominence to random walks. Among our main
results stand out the relevance and NP-hardness of the problem of finding the
most central set in a semantic network for an specific centrality measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00141</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00141</id><created>2016-01-02</created><updated>2016-02-01</updated><authors><author><keyname>Elango</keyname><forenames>Bakthavachalam</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Kannan</keyname><forenames>Govindaraju</forenames></author></authors><title>Detecting the historical roots of tribology research: a bibliometric
  analysis</title><categories>cs.DL</categories><comments>Accepted for publicaion in the Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, the historical roots of tribology are investigated using a
newly developed scientometric method called Referenced Publication Years
Spectroscopy. The study is based on cited references in tribology research
publications. The Science Citation Index Expanded is used as data source. The
results show that RPYS has the potential to identify the important publications
: Most of the publications which have been identified in this study as highly
cited (referenced) publications are landmark publications in the field of
tribology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00144</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00144</id><created>2016-01-02</created><authors><author><keyname>Sangwisut</keyname><forenames>Ekkasit</forenames></author><author><keyname>Jitman</keyname><forenames>Somphong</forenames></author><author><keyname>Udomkavanich</keyname><forenames>Patanee</forenames></author></authors><title>Constacyclic and Quasi-Twisted Hermitian Self-Dual Codes over Finite
  Fields</title><categories>math.RA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constacyclic and quasi-twisted Hermitian self-dual codes over finite fields
are studied. An algorithm for factorizing $x^n-\lambda$ over $\mathbb{F}_{q^2}$
is given, where $\lambda$ is a unit in $\mathbb{F}_{q^2}$. Based on this
factorization, the dimensions of the Hermitian hulls of $\lambda$-constacyclic
codes of length $n$ over $\mathbb{F}_{q^2}$ are determined. The
characterization and enumeration of constacyclic Hermitian self-dual (resp.,
complementary dual) codes of length $n$ over $\mathbb{F}_{q^2}$ are given
through their Hermitian hulls. Subsequently, a new family of MDS constacyclic
Hermitian self-dual codes over $\mathbb{F}_{q^2}$ is introduced.
  As a generalization of constacyclic codes, quasi-twisted Hermitian self-dual
codes are studied. Using the factorization of $x^n-\lambda$ and the Chinese
Remainder Theorem, quasi-twisted codes can be viewed as a product of linear
codes of shorter length some over extension fields of $\mathbb{F}_{q^2}$.
Necessary and sufficient conditions for quasi-twisted codes to be Hermitian
self-dual are given. The enumeration of such self-dual codes is determined as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00149</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00149</id><created>2016-01-02</created><updated>2016-01-07</updated><authors><author><keyname>Piao</keyname><forenames>Xinglin</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>A Submodule Clustering Method for Multi-way Data by Sparse and Low-Rank
  Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new submodule clustering method via sparse and low-rank representation for
multi-way data is proposed in this paper. Instead of reshaping multi-way data
into vectors, this method maintains their natural orders to preserve data
intrinsic structures, e.g., image data kept as matrices. To implement
clustering, the multi-way data, viewed as tensors, are represented by the
proposed tensor sparse and low-rank model to obtain its submodule
representation, called a free module, which is finally used for spectral
clustering. The proposed method extends the conventional subspace clustering
method based on sparse and low-rank representation to multi-way data submodule
clustering by combining t-product operator. The new method is tested on several
public datasets, including synthetical data, video sequences and toy images.
The experiments show that the new method outperforms the state-of-the-art
methods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation
(LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation
(RobustLatLRR) and Sparse Submodule Clustering method (SSmC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00152</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00152</id><created>2016-01-02</created><authors><author><keyname>Mekikis</keyname><forenames>Prodromos-Vasileios</forenames></author><author><keyname>Antonopoulos</keyname><forenames>Angelos</forenames></author><author><keyname>Kartsakli</keyname><forenames>Elli</forenames></author><author><keyname>Lalos</keyname><forenames>Aris S.</forenames></author><author><keyname>Alonso</keyname><forenames>Luis</forenames></author><author><keyname>Verikoukis</keyname><forenames>Christos</forenames></author></authors><title>Information Exchange in Randomly Deployed Dense WSNs with Wireless
  Energy Harvesting Capabilities</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communications. Full
  version - Contains all appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As large-scale dense and often randomly deployed wireless sensor networks
(WSNs) become widespread, local information exchange between co-located sets of
nodes may play a significant role in handling the excessive traffic volume.
Moreover, to account for the limited life-span of the wireless devices,
harvesting the energy of the network transmissions provides significant
benefits to the lifetime of such networks. In this paper, we study the
performance of communication in dense networks with wireless energy harvesting
(WEH)-enabled sensor nodes. In particular, we examine two different
communication scenarios (direct and cooperative) for data exchange and we
provide theoretical expressions for the probability of successful
communication. Then, considering the importance of lifetime in WSNs, we employ
state-of-the-art WEH techniques and realistic energy converters, quantifying
the potential energy gains that can be achieved in the network. Our analytical
derivations, which are validated by extensive Monte-Carlo simulations,
highlight the importance of WEH in dense networks and identify the trade-offs
between the direct and cooperative communication scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00159</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00159</id><created>2016-01-02</created><authors><author><keyname>Xie</keyname><forenames>Zhongle</forenames></author><author><keyname>Cai</keyname><forenames>Qingchao</forenames></author><author><keyname>Jagadish</keyname><forenames>H. V.</forenames></author><author><keyname>Ooi</keyname><forenames>Beng Chin</forenames></author><author><keyname>Wong</keyname><forenames>Weng-Fai</forenames></author></authors><title>PI : a Parallel in-memory skip list based Index</title><categories>cs.DB cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Due to the coarse granularity of data accesses and the heavy use of latches,
indices in the B-tree family are not efficient for in-memory databases,
especially in the context of today's multi-core architecture.
  In this paper, we present PI, a Parallel in-memory skip list based Index that
lends itself naturally to the parallel and concurrent environment, particularly
with non-uniform memory access. In PI, incoming queries are collected, and
disjointly distributed among multiple threads for processing to avoid the use
of latches. For each query, PI traverses the index in a Breadth-First-Search
(BFS) manner to find the list node with the matching key, exploiting SIMD
processing to speed up the search process. In order for query processing to be
latch-free, PI employs a light-weight communication protocol that enables
threads to re-distribute the query workload among themselves such that each
list node that will be modified as a result of query processing will be
accessed by exactly one thread. We conducted extensive experiments, and the
results show that PI can be up to three times as fast as the Masstree, a
state-of-the-art B-tree based index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00163</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00163</id><created>2016-01-02</created><authors><author><keyname>Xiao</keyname><forenames>Mingyu</forenames></author></authors><title>A Parameterized algorithm for Bounded-Degree Vertex Deletion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $d$-bounded-degree vertex deletion problem, to delete at most $k$
vertices in a given graph to make the maximum degree of the remaining graph at
most $d$, finds applications in computational biology, social network analysis
and some others. It can be regarded as a special case of the $(d+2)$-hitting
set problem and generates the famous vertex cover problem. The
$d$-bounded-degree vertex deletion problem is NP-hard for each fixed $d\geq 0$.
In terms of parameterized complexity, the problem parameterized by $k$ is
W[2]-hard for unbounded $d$ and fixed-parameter tractable for each fixed $d\geq
0$. Previously, (randomized) parameterized algorithms for this problem with
running time bound $O^*((d+1)^k)$ are only known for $d\leq2$. In this paper,
we give a uniform parameterized algorithm deterministically solving this
problem in $O^*((d+1)^k)$ time for each $d\geq 3$. Note that it is an open
problem whether the $d'$-hitting set problem can be solved in $O^*((d'-1)^k)$
time for $d'\geq 3$. Our result answers this challenging open problem
affirmatively for a special case. Furthermore, our algorithm also gets a
running time bound of $O^*(3.0645^k)$ for the case that $d=2$, improving the
previous deterministic bound of $O^*(3.24^k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00164</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00164</id><created>2016-01-02</created><authors><author><keyname>Xiao</keyname><forenames>Mingyu</forenames></author></authors><title>On a generalization of Nemhauser and Trotter's local optimization
  theorem</title><categories>cs.DS</categories><journal-ref>ISAAC 2015, LNCS 9472, 442-452</journal-ref><doi>10.1007/978-3-662-48971-0_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fellows, Guo, Moser and Niedermeier~[JCSS2011] proved a generalization of
Nemhauser and Trotter's theorem, which applies to \textsc{Bounded-Degree Vertex
Deletion} (for a fixed integer $d\geq 0$, to delete $k$ vertices of the input
graph to make the maximum degree of it $\leq d$) and gets a linear-vertex
kernel for $d=0$ and $1$, and a superlinear-vertex kernel for each $d\geq 2$.
It is still left as an open problem whether \textsc{Bounded-Degree Vertex
Deletion} admits a linear-vertex kernel for each $d\geq 3$. In this paper, we
refine the generalized Nemhauser and Trotter's theorem and get a linear-vertex
kernel for each $d\geq 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00167</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00167</id><created>2016-01-02</created><authors><author><keyname>Panaousis</keyname><forenames>Emmanouil</forenames></author><author><keyname>Laszka</keyname><forenames>Aron</forenames></author><author><keyname>Pohl</keyname><forenames>Johannes</forenames></author><author><keyname>Noack</keyname><forenames>Andreas</forenames></author><author><keyname>Alpcan</keyname><forenames>Tansu</forenames></author></authors><title>Game-Theoretic Model of Incentivizing Privacy-Aware Users to Consent to
  Location Tracking</title><categories>cs.GT</categories><comments>8 pages, 7 figures, In Proceedings of 2015 IEEE
  Trustcom/BigDataSE/ISPA</comments><doi>10.1109/Trustcom.2015.476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, mobile users have a vast number of applications and services at
their disposal. Each of these might impose some privacy threats on users'
&quot;Personally Identifiable Information&quot; (PII). Location privacy is a crucial part
of PII, and as such, privacy-aware users wish to maximize it. This privacy can
be, for instance, threatened by a company, which collects users' traces and
shares them with third parties. To maximize their location privacy, users can
decide to get offline so that the company cannot localize their devices. The
longer a user stays connected to a network, the more services he might receive,
but his location privacy decreases. In this paper, we analyze the trade-off
between location privacy, the level of services that a user experiences, and
the profit of the company. To this end, we formulate a Stackelberg Bayesian
game between the User (follower) and the Company (leader). We present
theoretical results characterizing the equilibria of the game. To the best of
our knowledge, our work is the first to model the economically rational
decision-making of the service provider (i.e., the Company) in conjunction with
the rational decision-making of users who wish to protect their location
privacy. To evaluate the performance of our approach, we have used real-data
from a testbed, and we have also shown that the game-theoretic strategy of the
Company outperforms non-strategic methods. Finally, we have considered
different User privacy types, and have determined the service level that
incentivizes the User to stay connected as long as possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00172</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00172</id><created>2016-01-02</created><authors><author><keyname>Cai</keyname><forenames>Ning</forenames></author></authors><title>On Quantitatively Measuring Controllability of Complex Networks</title><categories>cs.SY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter deals with the controllability issue of complex networks. An
index is chosen to quantitatively measure the extent of controllability of
given network. The effect of this index is analyzed based on empirical studies
on various classes of network topologies, such as random network, small-world
network, and scale-free network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00176</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00176</id><created>2016-01-02</created><authors><author><keyname>Li</keyname><forenames>Jiawei</forenames></author></authors><title>Interdependent Relationships in Game Theory: A Generalized Model</title><categories>cs.GT</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized model of games is proposed, with which both cooperative games
and non-cooperative games can be expressed and analyzed, as well as those games
that are neither cooperative nor non-cooperative. The model is based on
relationships between players and supposed relationships. A relationship is a
numerical value that denotes how one player cares for the payoffs of another
player. A supposed relationship is a players belief about the relationship
between two players. The players choose their strategies by taking into
consideration not only the material payoffs but also relationships and their
change. Two games, a prisoners dilemma and a repeated ultimatum game, are
analyzed as examples of application of this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00181</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00181</id><created>2016-01-02</created><authors><author><keyname>Cohen</keyname><forenames>Nathann</forenames></author><author><keyname>Pasechnik</keyname><forenames>Dmitrii V.</forenames></author></authors><title>Implementing Brouwer's database of strongly regular graphs</title><categories>math.CO cs.DM</categories><comments>15 pages, LaTeX</comments><msc-class>05E30, 68-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Andries Brouwer maintains a public database of existence results for strongly
regular graphs on $n\leq 1300$ vertices. We implemented most of the infinite
families of graphs listed there in the open-source software Sagemath, as well
as provided constructions of the &quot;sporadic&quot; cases, to obtain a graph for each
set of parameters with known examples. Besides providing a convenient way to
verify these existence results from the actual graphs, it also extends the
database to higher values of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00182</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00182</id><created>2016-01-02</created><updated>2016-01-04</updated><authors><author><keyname>Jiang</keyname><forenames>Dawei</forenames></author><author><keyname>Cai</keyname><forenames>Qingchao</forenames></author><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Ooi</keyname><forenames>Beng Chin</forenames></author><author><keyname>Tan</keyname><forenames>Kian-Lee</forenames></author><author><keyname>Tung</keyname><forenames>Anthony K. H.</forenames></author></authors><title>Cohort Query Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Internet applications such as websites and mobile games produce a
large amount of activity data representing information associated with user
actions such as login or online purchases. Cohort analysis, originated from
Social Science, is a powerful data exploration technique for ?finding unusual
user behavior trends in large activity datasets using the concept of cohort.
This paper presents the design and implementation of database support for
cohort analysis. We introduce an extended relational data model for
representing a collection of activity data as an activity relation, and define
a set of cohort operators on the activity relations, for composing cohort
queries. To evaluate a cohort query, we present three schemes: a SQL based
approach which translates a cohort query into a set of SQL statements for
execution, a materialized view approach which materializes birth activity
tuples to speed up SQL execution, and a new cohort query evaluation scheme
specially designed for cohort query processing. We implement the first two
schemes on MySQL and MonetDB respectively and develop a prototype of our own
cohort query engine, COHANA, for the third scheme. An extensive experimental
evaluation shows that the performance of the proposed cohort query evaluation
scheme is up to three orders of magnitude faster than the performance of the
two SQL based schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00184</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00184</id><created>2016-01-02</created><authors><author><keyname>Feher</keyname><forenames>Ben</forenames></author><author><keyname>Sidi</keyname><forenames>Lior</forenames></author><author><keyname>Shabtai</keyname><forenames>Asaf</forenames></author><author><keyname>Puzis</keyname><forenames>Rami</forenames></author></authors><title>The Security of WebRTC</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WebRTC is an API that allows users to share streaming information, whether it
is text, sound, video or files. It is supported by all major browsers and has a
flexible underlying infrastructure. In this study we review current WebRTC
structure and security in the contexts of communication disruption,
modification and eavesdropping. In addition, we examine WebRTC security in a
few representative scenarios, setting up and simulating real WebRTC
environments and attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00191</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00191</id><created>2016-01-02</created><authors><author><keyname>Osegi</keyname><forenames>N. E.</forenames></author></authors><title>An Improved Intelligent Agent for Mining Real-Time Databases Using
  Modified Cortical Learning Algorithms</title><categories>cs.NE</categories><comments>16 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTM
have been developed by Numenta Incorporation from which variations and
modifications are currently being investigated upon. HTM offers better promises
as a future computational model of the neocortex the seat of intelligence in
the brain. Currently, intelligent agents are embedded in almost every modern
day electronic system found in homes, offices and industries worldwide. In this
paper, we present a first step in realising useful HTM like applications
specifically for mining a synthetic and real time dataset based on a novel
intelligent agent framework, and demonstrate how a modified version of this
very important computational technique will lead to improved recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00199</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00199</id><created>2016-01-02</created><authors><author><keyname>Alabort-i-Medina</keyname><forenames>Joan</forenames></author><author><keyname>Zafeiriou</keyname><forenames>Stefanos</forenames></author></authors><title>A Unified Framework for Compositional Fitting of Active Appearance
  Models</title><categories>cs.CV</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Appearance Models (AAMs) are one of the most popular and
well-established techniques for modeling deformable objects in computer vision.
In this paper, we study the problem of fitting AAMs using Compositional
Gradient Descent (CGD) algorithms. We present a unified and complete view of
these algorithms and classify them with respect to three main characteristics:
i) cost function; ii) type of composition; and iii) optimization method.
Furthermore, we extend the previous view by: a) proposing a novel Bayesian cost
function that can be interpreted as a general probabilistic formulation of the
well-known project-out loss; b) introducing two new types of composition,
asymmetric and bidirectional, that combine the gradients of both image and
appearance model to derive better conver- gent and more robust CGD algorithms;
and c) providing new valuable insights into existent CGD algorithms by
reinterpreting them as direct applications of the Schur complement and the
Wiberg method. Finally, in order to encourage open research and facilitate
future comparisons with our work, we make the implementa- tion of the
algorithms studied in this paper publicly available as part of the Menpo
Project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00210</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00210</id><created>2016-01-02</created><authors><author><keyname>Al-Kadi</keyname><forenames>O. S.</forenames></author><author><keyname>Watson</keyname><forenames>D.</forenames></author></authors><title>Susceptibility of texture measures to noise: an application to lung
  tumor CT images</title><categories>cs.CV</categories><comments>8th International Conference on BioInformatics and BioEngineering,
  Greece, 2008</comments><doi>10.1109/BIBE.2008.4696789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Five different texture methods are used to investigate their susceptibility
to subtle noise occurring in lung tumor Computed Tomography (CT) images caused
by acquisition and reconstruction deficiencies. Noise of Gaussian and Rayleigh
distributions with varying mean and variance was encountered in the analyzed CT
images. Fisher and Bhattacharyya distance measures were used to differentiate
between an original extracted lung tumor region of interest (ROI) with a
filtered and noisy reconstructed versions. Through examining the texture
characteristics of the lung tumor areas by five different texture measures, it
was determined that the autocovariance measure was least affected and the gray
level co-occurrence matrix was the most affected by noise. Depending on the
selected ROI size, it was concluded that the number of extracted features from
each texture measure increases susceptibility to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00211</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00211</id><created>2016-01-02</created><authors><author><keyname>Al-Kadi</keyname><forenames>Omar S.</forenames></author></authors><title>A fractal dimension based optimal wavelet packet analysis technique for
  classification of meningioma brain tumours</title><categories>cs.CV</categories><comments>IEEE International Conference on Image Processing, Egypt, 2009</comments><doi>10.1109/ICIP.2009.5414534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the heterogeneous nature of tissue texture, using a single resolution
approach for optimum classification might not suffice. In contrast, a
multiresolution wavelet packet analysis can decompose the input signal into a
set of frequency subbands giving the opportunity to characterise the texture at
the appropriate frequency channel. An adaptive best bases algorithm for optimal
bases selection for meningioma histopathological images is proposed, via
applying the fractal dimension (FD) as the bases selection criterion in a
tree-structured manner. Thereby, the most significant subband that better
identifies texture discontinuities will only be chosen for further
decomposition, and its fractal signature would represent the extracted feature
vector for classification. The best basis selection using the FD outperformed
the energy based selection approaches, achieving an overall classification
accuracy of 91.25% as compared to 83.44% and 73.75% for the co-occurrence
matrix and energy texture signatures; respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00212</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00212</id><created>2016-01-02</created><authors><author><keyname>Al-Kadi</keyname><forenames>Omar S.</forenames></author></authors><title>Supervised Texture Segmentation: A Comparative Study</title><categories>cs.CV</categories><comments>IEEE Jordan Conf. on Applied Electrical Engineering and Computing
  Technologies, Jordan, 2011</comments><doi>10.1109/AEECT.2011.6132529</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to compare between four different types of feature extraction
approaches in terms of texture segmentation. The feature extraction methods
that were used for segmentation are Gabor filters (GF), Gaussian Markov random
fields (GMRF), run-length matrix (RLM) and co-occurrence matrix (GLCM). It was
shown that the GF performed best in terms of quality of segmentation while the
GLCM localises the texture boundaries better as compared to the other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00221</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00221</id><created>2016-01-02</created><authors><author><keyname>Chitty</keyname><forenames>Darren M.</forenames></author></authors><title>Faster GPU Based Genetic Programming Using A Two Dimensional Stack</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic Programming (GP) is a computationally intensive technique which also
has a high degree of natural parallelism. Parallel computing architectures have
become commonplace especially with regards Graphics Processing Units (GPU).
Hence, versions of GP have been implemented that utilise these highly parallel
computing platforms enabling significant gains in the computational speed of GP
to be achieved. However, recently a two dimensional stack approach to GP using
a multi-core CPU also demonstrated considerable performance gains. Indeed,
performances equivalent to or exceeding that achieved by a GPU were
demonstrated. This paper will demonstrate that a similar two dimensional stack
approach can also be applied to a GPU based approach to GP to better exploit
the underlying technology. Performance gains are achieved over a standard
single dimensional stack approach when utilising a GPU. Overall, a peak
computational speed of over 55 billion Genetic Programming Operations per
Second are observed, a two fold improvement over the best GPU based single
dimensional stack approach from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00236</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00236</id><created>2016-01-02</created><authors><author><keyname>Vepakomma</keyname><forenames>Praneeth</forenames></author><author><keyname>Tonde</keyname><forenames>Chetan</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Supervised Dimensionality Reduction via Distance Correlation
  Maximization</title><categories>cs.LG stat.ML</categories><comments>23 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work, we propose a novel formulation for supervised dimensionality
reduction based on a nonlinear dependency criterion called Statistical Distance
Correlation, Szekely et. al. (2007). We propose an objective which is free of
distributional assumptions on regression variables and regression model
assumptions. Our proposed formulation is based on learning a low-dimensional
feature representation $\mathbf{z}$, which maximizes the squared sum of
Distance Correlations between low dimensional features $\mathbf{z}$ and
response $y$, and also between features $\mathbf{z}$ and covariates
$\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective
using the Generalized Minimization Maximizaiton method of \Parizi et. al.
(2015). We show superior empirical results on multiple datasets proving the
effectiveness of our proposed approach over several relevant state-of-the-art
supervised dimensionality reduction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00238</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00238</id><created>2016-01-02</created><authors><author><keyname>Liu</keyname><forenames>Tongliang</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Xu</keyname><forenames>Dong</forenames></author></authors><title>Dimensionality-Dependent Generalization Bounds for $k$-Dimensional
  Coding Schemes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-dimensional coding schemes refer to a collection of methods that
attempt to represent data using a set of representative $k$-dimensional
vectors, and include non-negative matrix factorization, dictionary learning,
sparse coding, $k$-means clustering and vector quantization as special cases.
Previous generalization bounds for the reconstruction error of the
$k$-dimensional coding schemes are mainly dimensionality independent. A major
advantage of these bounds is that they can be used to analyze the
generalization error when data is mapped into an infinite- or high-dimensional
feature space. However, many applications use finite-dimensional data features.
Can we obtain dimensionality-dependent generalization bounds for
$k$-dimensional coding schemes that are tighter than dimensionality-independent
bounds when data is in a finite-dimensional feature space? The answer is
positive. In this paper, we address this problem and derive a
dimensionality-dependent generalization bound for $k$-dimensional coding
schemes by bounding the covering number of the loss function class induced by
the reconstruction error. The bound is of order
$\mathcal{O}\left(\left(mk\ln(mkn)/n\right)^{\lambda_n}\right)$, where $m$ is
the dimension of features, $k$ is the number of the columns in the linear
implementation of coding schemes, $n$ is the size of sample, $\lambda_n&gt;0.5$
when $n$ is finite and $\lambda_n=0.5$ when $n$ is infinite. We show that our
bound can be tighter than previous results, because it avoids inducing the
worst-case upper bound on $k$ of the loss function and converges faster. The
proposed generalization bound is also applied to some specific coding schemes
to demonstrate that the dimensionality-dependent bound is an indispensable
complement to these dimensionality-independent generalization bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00239</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00239</id><created>2016-01-02</created><authors><author><keyname>Lai</keyname><forenames>Hongliang</forenames></author><author><keyname>Shen</keyname><forenames>Lili</forenames></author></authors><title>Fixed points of adjoint functors enriched in a quantaloid</title><categories>cs.LO math.CT</categories><comments>27 pages</comments><msc-class>18A40, 18D20, 03B70, 06B23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation theorems are established for fixed points of adjoint functors
between categories enriched in a small quantaloid. In a very general setting
these results set up a common framework for representation theorems of concept
lattices in formal concept analysis (FCA) and rough set theory (RST), which not
only extend the realm of formal contexts to multi-typed and multi-valued ones,
but also provide a general approach to construct various kinds of
representation theorems. Besides incorporating several well-known
representation theorems in FCA and RST as well as formulating new ones, it is
shown that concept lattices in RST can always be represented as those in FCA
through relative pseudo-complements of the given contexts, especially if the
contexts are valued in a non-Girard quantaloid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00242</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00242</id><created>2016-01-02</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author></authors><title>Non-Concept Software Subsystems: Tangible and Intangible</title><categories>cs.SE</categories><comments>17 pages, 19 figures, Extended from KEOD'2012 Conference</comments><acm-class>D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concepts modified by a Non- prefix apparently denote a negation, an opposite
of the concept without this prefix. But, generally the situation is rather
subtle: non- implies only partial negation and the concept suggests preserved
identity with some reduced quality or absent attribute. In this work tangible
and intangible software subsystems based upon Non- concepts are defined and
pluggable ontologies are proposed for their representation. Pluggable
ontologies are a kind of nano-ontologies, which by their minimal size
facilitate fast composition of new software subsystems. These ontologies are
made pluggable by Design Sockets, a novel kind of class. These are abstract
connectors for removed/added parts, functionalities or identities, and for
subdued qualities. Design Sockets are the basis of a Design Pattern for
dynamically modifiable software systems. Pragmatic implications of Non-
concepts include manageable design of product lines with multiple models. Non-
concepts are also relevant to the controversy whether composition is or is not
identity. The resolution is not sharp. Identity is entangled with composition,
and is preserved to a certain extent, until further removal causes identity
breakdown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00245</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00245</id><created>2016-01-03</created><authors><author><keyname>Tang</keyname><forenames>Li</forenames></author><author><keyname>Hu</keyname><forenames>Guangyuan</forenames></author><author><keyname>Liu</keyname><forenames>Weishu</forenames></author></authors><title>Funding acknowledgment analysis:Queries and Caveats</title><categories>cs.DL</categories><comments>8 pages 2 tables, accepted by JASIST</comments><msc-class>68-02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thomson Reuters' Web of Science (WoS) began systematically collecting
acknowledgment information in August 2008. Since then, bibliometric analysis of
funding acknowledgment (FA) has been growing and has aroused intense interest
and attention from both academia and policy makers. Examining the distribution
of FA by citation index database, by language, and by acknowledgment type, we
noted coverage limitations and potential biases in each analysis. We argue that
in spite of its great value, bibliometric analysis of FA should be used with
caution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00246</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00246</id><created>2016-01-03</created><authors><author><keyname>Tallapragada</keyname><forenames>Pavankumar</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>Jorge</forenames></author></authors><title>Hierarchical-distributed optimized coordination of intersection traffic</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of coordinating the vehicular traffic at an
intersection and on the branches leading to it for minimizing a combination of
total travel time and energy consumption. We propose a provably safe
hierarchical-distributed solution to balance computational complexity and
optimality of the system operation. In our design, a central intersection
manager communicates with vehicles heading towards the intersection, groups
them into clusters (termed bubbles) as they appear, and determines an optimal
schedule of passage through the intersection for each bubble. The vehicles in
each bubble receive their schedule and implement local distributed control to
ensure system-wide inter-vehicular safety while respecting speed and
acceleration limits, conforming to the assigned schedule, and seeking to
optimize their individual trajectories. Our analysis rigorously establishes
that the different aspects of the hierarchical design operate in concert and
that the safety guarantees provided by the proposed design are satisfied. We
illustrate its execution in a suite of simulations and compare its performance
to traditional signal-based coordination over a wide range of system
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00248</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00248</id><created>2016-01-03</created><authors><author><keyname>Arora</keyname><forenames>Kushal</forenames></author></authors><title>Contrastive Perplexity: A new evaluation metric for sentence level
  language models</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perplexity(per word) is the most widely used metric for evaluating language
models. This is mostly due to a its ease of computation, lack of dependence on
external tools like speech recognition pipeline and a good theoretical
justification for why it should work. Despite this, there has been no dearth of
criticism for this metric. Most of this criticism center around lack of
correlation with extrinsic metrics like word error rate(WER), dependence upon
shared vocabulary for model comparison and unsuitability for un-normalized
language model evaluation. In this paper we address the last problem of
inability to evaluate un-normalized models by introducing a new discriminative
evaluation metric that predicts model's performance based on its ability to
discriminate between test sentences and their deformed version. Due to its
discriminative formulation, this approach can work with un-normalized
probabilities while retaining perplexity's ease of computation. We show a
strong correlation between our new metric and perplexity across a range of
models on WSJ datasets. We also hypothesize a stronger correlation between WER
and our new metric vis-a-vis perplexity due to similar discriminative
objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00252</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00252</id><created>2016-01-03</created><authors><author><keyname>Kordecki</keyname><forenames>Wojciech</forenames></author><author><keyname>&#x141;yczkowska-Han&#x107;kowiak</keyname><forenames>Anna</forenames></author></authors><title>Greedy online colouring with buffering</title><categories>cs.DM</categories><msc-class>05C15: 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online graph colouring. Whenever a node is
requested, a colour must be assigned to the node, and this colour must be
different from the colours of any of its neighbours. According to the greedy
algorithm the node is coloured by the colour with the smallest possible $k$.
  The goal is to use as few colours as possible. We propose an algorithm, where
the node is coloured not immediately, but only after the collection of next
requests stored in the buffer of size $j$. In other words, the first node in
the buffer is coloured definitively taking into account all possible
colourisations of the remaining nodes in the buffer. If there are $r$ possible
corrected colourings, then the one with the probability $1/r$ is chosen. The
first coloured node is removed from the buffer to enable the entrance of the
next request. A number of colours in a two examples of graphs: crown graphs and
Kneser graphs have been analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00256</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00256</id><created>2016-01-03</created><authors><author><keyname>Gaafar</keyname><forenames>Mohamed</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Abediseid</keyname><forenames>Walid</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Underlay Spectrum Sharing Techniques with In-band Full-Duplex Systems
  using Improper Gaussian Signaling</title><categories>cs.IT math.IT</categories><comments>Submitted to TWC. arXiv admin note: text overlap with
  arXiv:1510.02902</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing the spectrum with in-band full-duplex (FD) primary users (PU) is a
challenging and interesting problem in the underlay cognitive radio (CR)
systems. The self-interference introduced at the primary network may
dramatically impede the secondary user (SU) opportunity to access the spectrum.
To tackle this problem, we use the so-called improper Gaussian signaling.
Particularly, we assume a system with a SU pair working in a half-duplex mode
that uses improper Gaussian signaling while the FD PU pair implements the
regular proper Gaussian signaling. First, we derive a closed form expression
and an upper bound for the SU and PU outage probabilities, respectively.
Second, we optimize the SU signal parameters to minimize its outage probability
while maintaining the required PU quality-of-service based on the average
channel state information (CSI). Moreover, we provide the conditions to reap
merits from employing improper Gaussian signaling at the SU. Third, we design
the SU signal parameters based on perfect knowledge of its direct link
instantaneous CSI and investigate all benefits that can be achieved at both the
SU and PU. Finally, we provide some numerical results that demonstrate the
advantages of using improper Gaussian signaling to access the spectrum of the
FD PU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00260</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00260</id><created>2016-01-03</created><authors><author><keyname>Rasti</keyname><forenames>Pejman</forenames></author><author><keyname>Demirel</keyname><forenames>Hasan</forenames></author><author><keyname>Anbarjafari</keyname><forenames>Gholamreza</forenames></author></authors><title>Image Resolution Enhancement by Using Interpolation Followed by
  Iterative Back Projection</title><categories>cs.CV</categories><comments>4 pages, Signal Processing and Communications Applications Conference
  (SIU), 2013</comments><doi>10.1109/SIU.2013.6531593</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new super resolution technique based on the
interpolation followed by registering them using iterative back projection
(IBP). Low resolution images are being interpolated and then the interpolated
images are being registered in order to generate a sharper high resolution
image. The proposed technique has been tested on Lena, Elaine, Pepper, and
Baboon. The quantitative peak signal-to-noise ratio (PSNR) and structural
similarity index (SSIM) results as well as the visual results show the
superiority of the proposed technique over the conventional and state-of-art
image super resolution techniques. For Lena's image, the PSNR is 6.52 dB higher
than the bicubic interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00270</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00270</id><created>2016-01-03</created><authors><author><keyname>Huang</keyname><forenames>Shan</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Zhang</keyname><forenames>Haijian</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author></authors><title>Frequency Estimation of Multiple Sinusoids with Sub-Nyquist Sampling
  Sequences</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1508.05723</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some applications of frequency estimation, the frequencies of multiple
sinusoids are required to be estimated from sub-Nyquist sampling sequences. In
this paper, we propose a novel method based on subspace techniques to estimate
the frequencies by using under-sampled samples. We analyze the impact of
under-sampling and demonstrate that three sub-Nyquist sequences are general
enough to estimate the frequencies under some condition. The frequencies
estimated from one sequence are unfolded in frequency domain, and then the
other two sequences are used to pick the correct frequencies from all possible
frequencies. Simulations illustrate the validity of the theory. Numerical
results show that this method is feasible and accurate at quite low sampling
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00271</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00271</id><created>2016-01-03</created><authors><author><keyname>Adjiashvili</keyname><forenames>David</forenames></author><author><keyname>Baggio</keyname><forenames>Andrea</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>Firefighting on Trees Beyond Integrality Gaps</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Firefighter problem and a variant of it, known as Resource Minimization
for Fire Containment (RMFC), are natural models for optimal inhibition of
harmful spreading processes. Despite considerable progress on several fronts,
the approximability of these problems is still badly understood. This is the
case even when the underlying graph is a tree, which is one of the most-studied
graph structures in this context and the focus of this paper. In their simplest
version, a fire spreads from one fixed vertex step by step from burning to
adjacent non-burning vertices, and at each time step $B$-many non-burning
vertices can be protected from catching fire. The Firefighter problem asks, for
a given $B$, to maximize the number of vertices that will not catch fire,
whereas RMFC (on a tree) asks to find the smallest $B$ which allows for saving
all leaves of the tree. Prior to this work, the best known approximation ratios
were an $O(1)$-approximation for the Firefighter problem and an $O(\log^*
n)$-approximation for RMFC, both being LP-based and matching the integrality
gaps of two natural LP relaxations.
  We improve on both approximations by presenting a PTAS for the Firefighter
problem and an $O(1)$-approximation for RMFC, both qualitatively matching the
known hardness results. Our results are obtained through a combination of the
LP with several new techniques, which allow for efficiently enumerating subsets
of super-constant size of a good solution to reduce the integrality gap of the
LPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00275</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00275</id><created>2016-01-03</created><updated>2016-01-11</updated><authors><author><keyname>Chepurnoy</keyname><forenames>Alexander</forenames></author></authors><title>Interactive Proof-of-stake</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper examines decentralized cryptocurrency protocols that are based on
the use of internal tokens as identity tools. An analysis of security problems
with popular Proof-of-stake consensus protocols is provided. A new protocol,
Interactive Proof-of-stake, is proposed. The main ideas of the protocol are to
reduce a number of variables a miner can iterate over to a minimum and also to
bring a communication into block generation. The protocol is checked against
known attacks. It is shown that Interactive Proof-of-stake is more secure than
current pure Proof-of-stake protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00286</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00286</id><created>2016-01-03</created><authors><author><keyname>Hamann</keyname><forenames>Michael</forenames></author><author><keyname>Lindner</keyname><forenames>Gerd</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Structure-Preserving Sparsification Methods for Social Networks</title><categories>cs.SI cs.DC physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparsification reduces the size of networks while preserving structural and
statistical properties of interest. Various sparsifying algorithms have been
proposed in different contexts. We contribute the first systematic conceptual
and experimental comparison of \textit{edge sparsification} methods on a
diverse set of network properties. It is shown that they can be understood as
methods for rating edges by importance and then filtering globally or locally
by these scores. We show that applying a local filtering technique improves the
preservation of all kinds of properties. In addition, we propose a new
sparsification method (\textit{Local Degree}) which preserves edges leading to
local hub nodes. All methods are evaluated on a set of social networks from
Facebook, Google+, Twitter and LiveJournal with respect to network properties
including diameter, connected components, community structure, multiple node
centrality measures and the behavior of epidemic simulations. In order to
assess the preservation of the community structure, we also include experiments
on synthetically generated networks with ground truth communities. Experiments
with our implementations of the sparsification methods (included in the
open-source network analysis tool suite NetworKit) show that many network
properties can be preserved down to about 20\% of the original set of edges for
sparse graphs with a reasonable density. The experimental results allow us to
differentiate the behavior of different methods and show which method is
suitable with respect to which property. While our Local Degree method is best
for preserving connectivity and short distances, other newly introduced local
variants are best for preserving the community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00287</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00287</id><created>2016-01-03</created><authors><author><keyname>Lostanlen</keyname><forenames>Vincent</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Wavelet Scattering on the Pitch Spiral</title><categories>cs.SD cs.AI</categories><comments>Proceedings of the 18th International Conference on Digital Audio
  Effects (DAFx-15), Trondheim, Norway, Nov 30 - Dec 3, 2015, pp. 429--432. 4
  pages, 3 figures</comments><msc-class>65T60</msc-class><journal-ref>Proceedings of the 18th International Conference on Digital Audio
  Effects (DAFx-15), Trondheim, Norway, Nov 30 - Dec 3, 2015, pp. 429--432</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new representation of harmonic sounds that linearizes the
dynamics of pitch and spectral envelope, while remaining stable to deformations
in the time-frequency plane. It is an instance of the scattering transform, a
generic operator which cascades wavelet convolutions and modulus
nonlinearities. It is derived from the pitch spiral, in that convolutions are
successively performed in time, log-frequency, and octave index. We give a
closed-form approximation of spiral scattering coefficients for a nonstationary
generalization of the harmonic source-filter model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00288</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00288</id><created>2016-01-03</created><authors><author><keyname>Comins</keyname><forenames>Jordan A.</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Identification of long-term concept-symbols among citations: Can
  documents be clustered in terms of common intellectual histories?</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Citation classics&quot; are not only highly cited, but also cited during several
decades. We test whether the peaks in the spectrograms generated by Reference
Publication Years Spectroscopy (RPYS) indicate such long-term impact by
comparing across RPYS for subsequent time intervals. Multi-RPYS enables us to
distinguish between short-term citation peaks at the research front that decay
within ten years versus historically constitutive (long-term) citations that
function as concept symbols (Small, 1978). Using these constitutive citations,
one is able to cluster document sets (e.g., journals) in terms of
intellectually shared histories. We test this premise by clustering 40 journals
in the Web of Science Category of Information and Library Science using
multi-RPYS. It follows that RPYS can not only be used for retrieving roots of
sets under study (cited), but also for algorithmic historiography of the citing
sets. Significant references are historically rooted symbols among other
citations that function as currency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00289</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00289</id><created>2016-01-03</created><authors><author><keyname>Koch</keyname><forenames>Jannis</forenames></author><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author><author><keyname>Vogel</keyname><forenames>Maximilian</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>An Empirical Comparison of Big Graph Frameworks in the Context of
  Network Analysis</title><categories>cs.DC cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Complex networks are relational data sets commonly represented as graphs. The
analysis of their intricate structure is relevant to many areas of science and
commerce, and data sets may reach sizes that require distributed storage and
processing. We describe and compare programming models for distributed
computing with a focus on graph algorithms for large-scale complex network
analysis. Four frameworks - GraphLab, Apache Giraph, Giraph++ and Apache Flink
- are used to implement algorithms for the representative problems Connected
Components, Community Detection, PageRank and Clustering Coefficients. The
implementations are executed on a computer cluster to evaluate the frameworks'
suitability in practice and to compare their performance to that of the
single-machine, shared-memory parallel network analysis package NetworKit. Out
of the distributed frameworks, GraphLab and Apache Giraph generally show the
best performance. In our experiments a cluster of eight computers running
Apache Giraph enables the analysis of a network with about 2 billion edges,
which is too large for a single machine of the same type. However, for networks
that fit into memory of one machine, the performance of the shared-memory
parallel implementation is far better than the distributed ones. The study
provides experimental evidence for selecting the appropriate framework
depending on the task and data volume.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00299</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00299</id><created>2016-01-03</created><authors><author><keyname>Safarpour</keyname><forenames>Mehdi</forenames></author><author><keyname>Charmi</keyname><forenames>Mostafa</forenames></author></authors><title>Capacity Enlargement Of The PVD Steganography Method Using The GLM
  Technique</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most steganographic methods, increasing in the capacity leads to decrease
in the quality of the stego-image, so in this paper, we propose to combine two
existing techniques, Pixel value differencing and Gray Level Modification, to
come up with a hybrid steganography scheme which can hide more information
without having to compromise much on the quality of the stego-image.
Experimental results demonstrate that the proposed approach has larger capacity
while its results are imperceptible. In comparison with original PVD method
criterion of the quality is declined by 2% dB averagely while the capacity is
increased around 25%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00306</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00306</id><created>2016-01-03</created><authors><author><keyname>Yilmaz</keyname><forenames>Yasin</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Multimodal Event Detection in Twitter Hashtag Networks</title><categories>stat.AP cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event detection in a multimodal Twitter dataset is considered. We treat the
hashtags in the dataset as instances with two modes: text and geolocation
features. The text feature consists of a bag-of-words representation. The
geolocation feature consists of geotags (i.e., geographical coordinates) of the
tweets. Fusing the multimodal data we aim to detect, in terms of topic and
geolocation, the interesting events and the associated hashtags. To this end, a
generative latent variable model is assumed, and a generalized
expectation-maximization (EM) algorithm is derived to learn the model
parameters. The proposed method is computationally efficient, and lends itself
to big datasets. Experimental results on a Twitter dataset from August 2014
show the efficacy of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00311</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00311</id><created>2016-01-03</created><updated>2016-01-17</updated><authors><author><keyname>Yaroslavsky</keyname><forenames>Leonid</forenames></author></authors><title>Back to the sampling theory: a practical and less redundant alternative
  to Compressed sensing</title><categories>cs.CV physics.optics</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of image acquisition and reconstruction with lesser number of
samples than it is required for image display is addressed. In last several
years, the &quot;Compressed sensing&quot; approach to solving this problem was advanced
and gained considerable popularity. In the paper, a method is suggested that
represents a practical and substantially more economical alternative to the
&quot;Compressed sensing&quot;, results of experimental verification of the method are
presented and some its limitations are discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00313</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00313</id><created>2016-01-03</created><updated>2016-02-09</updated><authors><author><keyname>Fontanari</keyname><forenames>Jos&#xe9; F.</forenames></author></authors><title>When more of the same is better</title><categories>cs.MA physics.soc-ph</categories><journal-ref>Europhysics Letters 113 (2016) 28009</journal-ref><doi>10.1209/0295-5075/113/28009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problem solving (e.g., drug design, traffic engineering, software
development) by task forces represents a substantial portion of the economy of
developed countries. Here we use an agent-based model of cooperative problem
solving systems to study the influence of diversity on the performance of a
task force. We assume that agents cooperate by exchanging information on their
partial success and use that information to imitate the more successful agent
in the system -- the model. The agents differ only in their propensities to
copy the model. We find that, for easy tasks, the optimal organization is a
homogeneous system composed of agents with the highest possible copy
propensities. For difficult tasks, we find that diversity can prevent the
system from being trapped in sub-optimal solutions. However, when the system
size is adjusted to maximize performance the homogeneous systems outperform the
heterogeneous systems, i.e., for optimal performance, sameness should be
preferred to diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00318</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00318</id><created>2016-01-03</created><updated>2016-01-30</updated><authors><author><keyname>Zhao</keyname><forenames>Han</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author></authors><title>A Unified Approach for Learning the Parameters of Sum-Product Networks</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unified approach for learning the parameters of Sum-Product
networks (SPNs). We prove that any complete and decomposable SPN is equivalent
to a mixture of trees where each tree corresponds to a product of univariate
distributions. Based on the mixture model perspective, we characterize the
objective function when learning SPNs based on the maximum likelihood
estimation (MLE) principle and show that the optimization problem can be
formulated as a signomial program. Both the projected gradient descent (PGD)
and the exponentiated gradient (EG) in this setting can be viewed as first
order approximations of the signomial program after proper transformation of
the objective function. Based on the signomial program formulation, we
construct two parameter learning algorithms for SPNs by using sequential
monomial approximations (SMA) and the concave-convex procedure (CCCP),
respectively. The two proposed methods naturally admit multiplicative updates,
hence effectively avoiding the projection operation. With the help of the
unified framework, we also show that, in the case of SPNs, CCCP leads to the
same algorithm as Expectation Maximization (EM) despite the fact that they are
different in general. Extensive experiments on 20 data sets demonstrate the
effectiveness and efficiency of the two proposed approaches for learning SPNs.
We also show that the proposed methods can improve the performance of structure
learning and yield state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00321</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00321</id><created>2016-01-03</created><authors><author><keyname>Chen</keyname><forenames>Zheng</forenames></author><author><keyname>Lee</keyname><forenames>Jemin</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Cooperative Caching and Transmission Design in Cluster-Centric Small
  Cell Networks</title><categories>cs.NI</categories><comments>13 pages, 10 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless content caching in small cell networks (SCNs) has recently been
considered as an efficient way to reduce the traffic and the energy consumption
of the backhaul in emerging heterogeneous cellular networks (HetNets). In this
paper, we consider a cluster-centric SCN with combined design of cooperative
caching and transmission policy. Small base stations (SBSs) are grouped into
disjoint clusters, in which in-cluster cache space is utilized as an entity. We
propose a combined caching scheme where part of the available cache space is
reserved for caching the most popular content in every SBS, while the remaining
is used for cooperatively caching different partitions of the less popular
content in different SBSs, as a means to increase local content diversity.
Depending on the availability and placement of the requested content,
coordinated multipoint (CoMP) technique with either joint transmission (JT) or
parallel transmission (PT) is used to deliver content to the served user. Using
Poisson point process (PPP) for the SBS location distribution and a hexagonal
grid model for the clusters, we provide analytical results on the successful
content delivery probability of both transmission schemes for a user located at
the cluster center. Our analysis shows an inherent tradeoff between
transmission diversity and content diversity in our combined
caching-transmission design. We also study optimal cache space assignment for
two objective functions: maximization of the cache service performance and the
energy efficiency. Simulation results show that the proposed scheme achieves
performance gain by leveraging cache-level and signal-level cooperation and
adapting to the network environment and user QoS requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00323</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00323</id><created>2016-01-03</created><authors><author><keyname>Grossman</keyname><forenames>Robert L.</forenames></author><author><keyname>Greenway</keyname><forenames>Matthew</forenames></author><author><keyname>Heath</keyname><forenames>Allison P.</forenames></author><author><keyname>Powell</keyname><forenames>Ray</forenames></author><author><keyname>Suarez</keyname><forenames>Rafael D.</forenames></author><author><keyname>Wells</keyname><forenames>Walt</forenames></author><author><keyname>White</keyname><forenames>Kevin</forenames></author><author><keyname>Atkinson</keyname><forenames>Malcolm</forenames></author><author><keyname>Klampanos</keyname><forenames>Iraklis</forenames></author><author><keyname>Alvarez</keyname><forenames>Heidi L.</forenames></author><author><keyname>Harvey</keyname><forenames>Christine</forenames></author><author><keyname>Mambretti</keyname><forenames>Joe J.</forenames></author></authors><title>The Design of a Community Science Cloud: The Open Science Data Cloud
  Perspective</title><categories>cs.CE</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the design, and implementation of the Open Science
Data Cloud, or OSDC. The goal of the OSDC is to provide petabyte-scale data
cloud infrastructure and related services for scientists working with large
quantities of data. Currently, the OSDC consists of more than 2000 cores and 2
PB of storage distributed across four data centers connected by 10G networks.
We discuss some of the lessons learned during the past three years of operation
and describe the software stacks used in the OSDC. We also describe some of the
research projects in biology, the earth sciences, and social sciences enabled
by the OSDC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00335</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00335</id><created>2016-01-03</created><updated>2016-01-13</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Riedel</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Asymptotic Intrinsic Universality and Reprogrammability by Behavioural
  Emulation</title><categories>cs.CC cs.FL nlin.CG</categories><comments>16 pages, 7 images. Invited contribution in Advances in
  Unconventional Computation. A. Adamatzky (ed), Springer Verlag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We advance a Bayesian concept of 'intrinsic asymptotic universality' taking
to its final conclusions previous conceptual and numerical work based upon a
concept of a reprogrammability test and an investigation of the complex
qualitative behaviour of computer programs. Our method may quantify the trust
and confidence of the computing capabilities of natural and classical systems,
and quantify computers by their degree of reprogrammability. We test the method
to provide evidence in favour of a conjecture concerning the computing
capabilities of Busy Beaver Turing machines as candidates for Turing
universality. The method has recently been used to quantify the number of
'intrinsically universal' cellular automata, with results that point towards
the pervasiveness of universality due to a widespread capacity for emulation.
Our method represents an unconventional approach to the classical and seminal
concept of Turing universality, and it may be extended and applied in a broader
context to natural computation, by (in something like the spirit of the Turing
test) observing the behaviour of a system under circumstances where formal
proofs of universality are difficult, if not impossible to come by.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00346</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00346</id><created>2016-01-03</created><authors><author><keyname>Joglar-Alcubilla</keyname><forenames>Javier</forenames></author></authors><title>Generic Tracking Specifications Translation from Time Domain to
  Frequency Domain</title><categories>cs.SY</categories><comments>Matlab Programs:
  https://www.dropbox.com/sh/x7ywzymrkkr2qks/AABC2dvUDAfVPWBubWzo-qWQa?dl=0</comments><msc-class>93C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In certain types of robust control techniques, it is common having to deal
with control problems where the specifications, described in the time domain,
need to be translated to the frequency domain. This usually happens in
techniques, such as Quantitative Feedback Theory, where the control problem is
developed in the frequency domain. Therefore, not only process plants and
disturbances should be specified in this domain, but also the limits and
restrictions initially imposed in time. The question is important if we
consider that any deviation in the parameters transfer from one domain to
another will decisively influence in the development of the problem and, above
all, in the finally result expressed in temporal terms. The technique presented
allows the translation of the upper frequency limit in generic tracking
specifications from time domain to frequency domain accurately. It will use
approaches based on 2nd order systems or an envelope approach based on higher
order systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00350</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00350</id><created>2016-01-03</created><authors><author><keyname>Zayyani</keyname><forenames>Hadi</forenames></author><author><keyname>Korki</keyname><forenames>Mehdi</forenames></author><author><keyname>Marvasti</keyname><forenames>Farrokh</forenames></author></authors><title>Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in
  Wireless Sensor Networks</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a sparse diffusion steepest-descent algorithm for one
bit compressed sensing in wireless sensor networks. The approach exploits the
diffusion strategy from distributed learning in the one bit compressed sensing
framework. To estimate a common sparse vector cooperatively from only the sign
of measurements, steepest-descent is used to minimize the suitable global and
local convex cost functions. A diffusion strategy is suggested for distributive
learning of the sparse vector. Simulation results show the effectiveness of the
proposed distributed algorithm compared to the state-of-the-art non
distributive algorithms in the one bit compressed sensing framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00360</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00360</id><created>2015-12-29</created><authors><author><keyname>Salehinejad</keyname><forenames>Hojjat</forenames></author></authors><title>Internet of Things for Residential Areas: Toward Personalized Energy
  Management Using Big Data</title><categories>cs.NI</categories><comments>Draft of technical report. Limited version under preparation for
  submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent management of machines, particularly in a residence area, has
been of interest for many years. However, such system design has always been
limited to simple control of machines from a local area or remotely from the
Internet. In this report, for the first time, an intelligent system is
proposed, where not only provides intelligent control ability of machines to
user, but also utilizes big data and optimization techniques to provide
promotional offers to the user to optimize energy consumption of machines.
Since a high traffic communication is involved among the machines and the
optimization-big data core of system, the communication core of the proposed
system is designed based on cloud, where many challenging issues such as
spectrum assignment and resource management are involved. To deal with that,
the communication network in the home area network (HAN) is designed based on
the cognitive radio system, where a new spectrum assignment method based on the
ant colony optimization (ACO) algorithm is proposed to perform spectrum
assignment to the machines in the HAN. Performance evaluation of the proposed
spectrum assignment method shows its performance in fair spectrum assignment
among machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00363</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00363</id><created>2016-01-03</created><authors><author><keyname>Shao</keyname><forenames>Jianxiong</forenames></author><author><keyname>Qin</keyname><forenames>Yu</forenames></author><author><keyname>Feng</keyname><forenames>Dengguo</forenames></author></authors><title>Computational Soundness Results for Stateful Applied pi Calculus</title><categories>cs.CR cs.LO</categories><comments>to appear in POST 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, many researches have been done to establish symbolic models
of stateful protocols. Two works among them, the SAPIC tool and StatVerif tool,
provide a high-level specification language and an automated analysis. Their
language, the stateful applied \pi-calculus, is extended from the applied
\pi-calculus by defining explicit state constructs. Symbolic abstractions of
cryptography used in it make the analysis amenable to automation. However, this
might overlook the attacks based on the algebraic properties of the
cryptographic algorithms. In our paper, we establish the computational
soundness results for stateful applied \pi-calculus used in SAPIC tool and
StatVerif tool.
  In our approach, we build our results on the CoSP framework. For SAPIC, we
embed the non-monotonic protocol states into the CoSP protocols, and prove that
the resulting CoSP protocols are efficient. Through the embedding, we provide
the computational soundness result for SAPIC (by Theorem 1). For StatVerif, we
encode the StatVerif process into a subset of SAPIC process, and obtain the
computational soundness result for StatVerif (by Theorem 2). Our encoding shows
the differences between the semantics of the two languages. Our work inherits
the modularity of CoSP, which allows for easily extending the proofs to
specific cryptographic primitives. Thus we establish a computationally sound
automated verification result for the input languages of SAPIC and StatVerif
that use public-key encryption and signatures (by Theorem 3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00367</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00367</id><created>2015-12-30</created><authors><author><keyname>Maheo</keyname><forenames>Arthur</forenames></author><author><keyname>Kilby</keyname><forenames>Philip</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Benders Decomposition for the Design of a Hub and Shuttle Public Transit
  System</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The BusPlus project aims at improving the off-peak hours public transit
service in Canberra, Australia. To address the difficulty of covering a large
geographic area, BusPlus proposes a hub and shuttle model consisting of a
combination of a few high-frequency bus routes between key hubs and a large
number of shuttles that bring passengers from their origin to the closest hub
and take them from their last bus stop to their destination. This paper focuses
on the design of bus network and proposes an efficient solving method to this
multimodal network design problem based on the Benders decomposition method.
Starting from a MIP formulation of the problem, the paper presents a Benders
decomposition approach using dedicated solution techniques for solving
independent sub-problems, Pareto optimal cuts, cut bundling, and core point
update. Computational results on real-world data from Canberra's public transit
system justify the design choices and show that the approach outperforms the
MIP formulation by two orders of magnitude. Moreover, the results show that the
hub and shuttle model may decrease transit time by a factor of 2, while staying
within the costs of the existing transit system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00372</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00372</id><created>2016-01-03</created><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>Mutual Information and Diverse Decoding Improve Neural Machine
  Translation</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-to-sequence neural translation models learn semantic and syntactic
relations between sentence pairs by optimizing the likelihood of the target
given the source, i.e., $p(y|x)$, an objective that ignores other potentially
useful sources of information. We introduce an alternative objective function
for neural MT that maximizes the mutual information between the source and
target sentences, modeling the bi-directional dependency of sources and
targets. We implement the model with a simple re-ranking method, and also
introduce a decoding algorithm that increases diversity in the N-best list
produced by the first pass. Applied to the WMT German/English and
French/English tasks, both mechanisms offer a consistent performance boost on
both standard LSTM and attention-based neural MT architectures. The result is
the best published performance for a single (non-ensemble) neural MT system, as
well as the potential application of our diverse decoding algorithm to other
NLP re-ranking tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00374</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00374</id><created>2016-01-03</created><authors><author><keyname>Zhou</keyname><forenames>Zheng</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Zhao</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author></authors><title>Wireless-Powered Cooperative Communications: Power-Splitting Relaying
  with Energy Accumulation</title><categories>cs.IT math.IT</categories><comments>15 pages, 7 figures. Manuscript received Apr. 15, 2015 by IEEE
  Journal on Selected Areas in Communications, revised Sep. 5, 2015, accepted
  Dec. 11, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A harvest-use-store power splitting (PS) relaying strategy with distributed
beamforming is proposed for wirelesspowered multi-relay cooperative networks in
this paper. Different from the conventional battery-free PS relaying strategy,
harvested energy is prioritized to power information relaying while the
remainder is accumulated and stored for future usage with the help of a battery
in the proposed strategy, which supports an efficient utilization of harvested
energy. However, PS affects throughput at subsequent time slots due to the
battery operations including the charging and discharging. To this end, PS and
battery operations are coupled with distributed beamforming. A throughput
optimization problem to incorporate these coupled operations is formulated
though it is intractable. To address the intractability of the optimization,a
layered optimization method is proposed to achieve the optimal joint PS and
battery operation design with non-causal channel state information (CSI), in
which the PS and the battery operation can be analyzed in a decomposed manner.
Then, a general case with causal CSI is considered, where the proposed layered
optimization method is extended by utilizing the statistical properties of CSI.
To reach a better tradeoff between performance and complexity, a greedy method
that requires no information about subsequent time slots is proposed.
Simulation results reveal the upper and lower bound on performance of the
proposed strategy, which are reached by the layered optimization method with
non-causal CSI and the greedy method, respectively. Moreover, the proposed
strategy outperforms the conventional PS-based relaying without energy
accumulation and time switching-based relaying strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00392</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00392</id><created>2016-01-04</created><authors><author><keyname>Xiang</keyname><forenames>Ju</forenames></author><author><keyname>Hu</keyname><forenames>Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Bao</keyname><forenames>Mei-Hua</forenames></author><author><keyname>Tang</keyname><forenames>Liang</forenames></author><author><keyname>Tang</keyname><forenames>Yan-Ni</forenames></author><author><keyname>Gao</keyname><forenames>Yuan-Yuan</forenames></author><author><keyname>Li</keyname><forenames>Jian-Ming</forenames></author><author><keyname>Chen</keyname><forenames>Benyan</forenames></author><author><keyname>Hu</keyname><forenames>Jing-Bo</forenames></author></authors><title>Enhancing community detection by local structural information</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>18 pages, 11figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world networks such as the gene networks, protein-protein
interaction networks and metabolic networks exhibit community structures,
meaning the existence of groups of densely connected vertices in the networks.
Many local similarity measures in the networks are closely related to the
concept of the community structures, and may have positive effect on community
detection in the networks. Here, various local similarity measures are used to
extract the local structural information and then are applied to community
detection in the networks by using the edge-reweighting strategy. The effect of
the local similarity measures on community detection is carefully investigated
and compared in various networks. The experimental results show that the local
similarity measures are crucial to the improvement for the community detection
methods, while the positive effect of the local similarity measures is closely
related to the networks under study and the applied community detection
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00393</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00393</id><created>2016-01-04</created><authors><author><keyname>Mei</keyname><forenames>Jincheng</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Lu</keyname><forenames>Bao-Liang</forenames></author></authors><title>On the Reducibility of Submodular Functions</title><categories>cs.LG stat.ML</categories><comments>To appear in AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scalability of submodular optimization methods is critical for their
usability in practice. In this paper, we study the reducibility of submodular
functions, a property that enables us to reduce the solution space of
submodular optimization problems without performance loss. We introduce the
concept of reducibility using marginal gains. Then we show that by adding
perturbation, we can endow irreducible functions with reducibility, based on
which we propose the perturbation-reduction optimization framework. Our
theoretical analysis proves that given the perturbation scales, the
reducibility gain could be computed, and the performance loss has additive
upper bounds. We further conduct empirical studies and the results demonstrate
that our proposed framework significantly accelerates existing optimization
methods for irreducible submodular functions with a cost of only small
performance losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00396</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00396</id><created>2016-01-04</created><authors><author><keyname>Wijenayake</keyname><forenames>Udaya</forenames></author><author><keyname>Choi</keyname><forenames>Sung-In</forenames></author><author><keyname>Park</keyname><forenames>Soon-Yong</forenames></author></authors><title>Automatic Detection and Decoding of Photogrammetric Coded Targets</title><categories>cs.CV</categories><comments>3 pages, 4 figures, Electronics, Information and Communications
  (ICEIC), 2014 International Conference on</comments><doi>10.1109/ELINFOCOM.2014.6914413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Close-range Photogrammetry is widely used in many industries because of the
cost effectiveness and efficiency of the technique. In this research, we
introduce an automated coded target detection method which can be used to
enhance the efficiency of the Photogrammetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00397</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00397</id><created>2016-01-04</created><authors><author><keyname>Pedersen</keyname><forenames>Jesper</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Andriyanova</keyname><forenames>Iryna</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author></authors><title>Distributed Storage in Mobile Wireless Networks with Device-to-Device
  Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the use of distributed storage (DS) to reduce the communication
cost of content delivery in a wireless network. Content is stored (cached) in a
number of mobile devices using an erasure correcting code. A user retrieves
content from other mobile devices using device-to-device communication or from
the base station (BS), at the expense of a higher communication cost. We
address the repair problem when a device that stores data leaves the network.
We introduce a repair scheduling where repair is performed periodically. We
derive analytical expressions for the overall communication cost of content
download and data repair as a function of the repair interval. The derived
expressions are then used to evaluate the communication cost entailed by DS
using maximum distance separable (MDS) codes, regenerating codes, and locally
repairable codes. Our results show that DS can reduce the communication cost
with respect to the case where content is downloaded only from the BS, provided
that repairs are performed frequently enough. The required repair frequency
depends on the code used for storage and network parameters. Interestingly, we
show that MDS codes, which do not perform well for classical DS, can yield a
low overall communication cost in wireless DS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00400</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00400</id><created>2016-01-04</created><authors><author><keyname>Abdulnabi</keyname><forenames>Abrar H.</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Lu</keyname><forenames>Jiwen</forenames></author><author><keyname>Jia</keyname><forenames>Kui</forenames></author></authors><title>Multi-task CNN Model for Attribute Prediction</title><categories>cs.CV</categories><comments>11 pages, 3 figures, ieee transaction paper</comments><journal-ref>IEEE Transactions on Multimedia, Nov 2015, pp. 1949-1959</journal-ref><doi>10.1109/TMM.2015.2477680</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a joint multi-task learning algorithm to better predict
attributes in images using deep convolutional neural networks (CNN). We
consider learning binary semantic attributes through a multi-task CNN model,
where each CNN will predict one binary attribute. The multi-task learning
allows CNN models to simultaneously share visual knowledge among different
attribute categories. Each CNN will generate attribute-specific feature
representations, and then we apply multi-task learning on the features to
predict their attributes. In our multi-task framework, we propose a method to
decompose the overall model's parameters into a latent task matrix and
combination matrix. Furthermore, under-sampled classifiers can leverage shared
statistics from other classifiers to improve their performance. Natural
grouping of attributes is applied such that attributes in the same group are
encouraged to share more knowledge. Meanwhile, attributes in different groups
will generally compete with each other, and consequently share less knowledge.
We show the effectiveness of our method on two popular attribute datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00402</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00402</id><created>2016-01-04</created><authors><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames><affiliation>ULISBOA</affiliation></author><author><keyname>Gouveia</keyname><forenames>Maria Joao</forenames><affiliation>ULISBOA</affiliation></author><author><keyname>Santocanale</keyname><forenames>Luigi</forenames><affiliation>LIF, AMU</affiliation></author></authors><title>Fixed-point elimination in the intuitionistic propositional calculus</title><categories>cs.LO math.CT math.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a consequence of existing literature that least and greatest
fixed-points of monotone polynomials on Heyting algebras-that is, the algebraic
models of the Intuitionistic Propositional Calculus-always exist, even when
these algebras are not complete as lattices. The reason is that these extremal
fixed-points are definable by formulas of the IPC. Consequently, the
$\mu$-calculus based on intuitionistic logic is trivial, every $\mu$-formula
being equivalent to a fixed-point free formula. We give in this paper an
axiomatization of least and greatest fixed-points of formulas, and an algorithm
to compute a fixed-point free formula equivalent to a given $\mu$-formula. The
axiomatization of the greatest fixed-point is simple. The axiomatization of the
least fixed-point is more complex, in particular every monotone formula
converges to its least fixed-point by Kleene's iteration in a finite number of
steps, but there is no uniform upper bound on the number of iterations. We
extract, out of the algorithm, upper bounds for such n, depending on the size
of the formula. For some formulas, we show that these upper bounds are
polynomial and optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00413</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00413</id><created>2016-01-04</created><authors><author><keyname>Qu</keyname><forenames>Daiming</forenames></author><author><keyname>Wang</keyname><forenames>Fang</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Farhang-Boroujeny</keyname><forenames>Behrouz</forenames></author></authors><title>Improving Bandwidth Efficiency of FBMC-OQAM Through Virtual Symbols</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filter bank multicarrier (FBMC) systems that are based on offset quadrature
amplitude modulation (OQAM), namely, FBMC-OQAM, have been criticized for their
inefficiency in the use of spectral resources, because of the long ramp-up and
ramp-down tails at the beginning and the end of each data packet, respectively.
We propose a novel method for shortening these tails. By appending a set of
virtual (i.e., none data carrying) symbols to the beginning and the end of each
packet, and clever selection of these symbols, we show that the ramp-up and
rampdown tails in FMBC-OQAM can be suppressed to an extent that they deem as
negligible and thus may be ignored. This shortens the length of signal burst in
each FBMC-OQAM packet, hence, improves on its bandwidth efficiency, viz., the
same data is transmitted over a shorter period of time. We develop an
optimization method that allows computation of virtual symbols, for each data
packet. Simulation results show that, compared to existing methods, the
proposed tail-shortening approach leads to a superior out-of-band (OOB)
emission performance and a much lower error vector magnitude (EVM) for the
demodulated symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00414</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00414</id><created>2016-01-04</created><authors><author><keyname>Yin</keyname><forenames>Ming</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>He</keyname><forenames>Zhaoshui</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author></authors><title>Kernel Sparse Subspace Clustering on Symmetric Positive Definite
  Manifolds</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse subspace clustering (SSC), as one of the most successful subspace
clustering methods, has achieved notable clustering accuracy in computer vision
tasks. However, SSC applies only to vector data in Euclidean space. As such,
there is still no satisfactory approach to solve subspace clustering by ${\it
self-expressive}$ principle for symmetric positive definite (SPD) matrices
which is very useful in computer vision. In this paper, by embedding the SPD
matrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspace
clustering method is constructed on the SPD manifold through an appropriate
Log-Euclidean kernel, termed as kernel sparse subspace clustering on the SPD
Riemannian manifold (KSSCR). By exploiting the intrinsic Riemannian geometry
within data, KSSCR can effectively characterize the geodesic distance between
SPD matrices to uncover the underlying subspace structure. Experimental results
on two famous database demonstrate that the proposed method achieves better
clustering results than the state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00416</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00416</id><created>2016-01-04</created><authors><author><keyname>Rungger</keyname><forenames>Matthias</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>Computing Robust Controlled Invariant Sets of Linear Systems</title><categories>math.OC cs.SY math.DS</categories><msc-class>Primary 93B51, Secondary 93B52, 93C05</msc-class><acm-class>D.2.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider controllable linear discrete-time systems with perturbations and
present two methods to compute robust controlled invariant sets. The first
method results in an (arbitrarily precise) outer approximation of the maximal
robust controlled invariant set, while the second method provides an inner
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00421</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00421</id><created>2016-01-04</created><authors><author><keyname>Chappelon</keyname><forenames>Jonathan</forenames><affiliation>IMAG</affiliation></author><author><keyname>Mart&#xed;nez-Sandoval</keyname><forenames>Leonardo</forenames><affiliation>IMAG</affiliation></author><author><keyname>Montejano</keyname><forenames>Luis</forenames><affiliation>IMAG</affiliation></author><author><keyname>Montejano</keyname><forenames>Luis Pedro</forenames><affiliation>IMAG</affiliation></author><author><keyname>Alfons&#xed;n</keyname><forenames>Jorge Ram&#xed;rez</forenames><affiliation>IMAG</affiliation></author></authors><title>Codimension two and three Kneser Transversals</title><categories>math.CO cs.DM math.MG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k,d,\lambda\geqslant1$ be integers with $d\geqslant\lambda $. In 2010,
the following function was introduced:$m(k,d,\lambda)\overset{\mathrm{def}}{=}$
the maximum positive integer $n$ such that every set of $n$ points (not
necessarily in general position) in $\mathbb{R}^{d}$ has the property that the
convex hulls of all $k$-sets have a common transversal $(d-\lambda)$-plane.This
is a continuation of a recent work in which it is introduced and studied a
natural discrete version of $m$ by considering the existence of \emph{complete
Kneser transversals} (i.e., $(d-\lambda)$-transversals $L$ to the convex hulls
of all $k$-sets and $L$ containing $(d-\lambda)+1$ points of the given set of
points).In this paper, we introduce and study the notions of \emph{stability}
and \emph{unstability}. We give results when $\lambda =2,3$, among other
results, we present a classification of (complete) Kneser transversals. These
results lead us to new upper and lower bounds for $m$. Finally, by using
oriented matroid machinery, we present computational results concerning
(complete) Kneser transversal in some special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00428</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00428</id><created>2016-01-04</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Talagala</keyname><forenames>Dumidu</forenames></author><author><keyname>Liu</keyname><forenames>Chi Harold</forenames></author><author><keyname>Estrella</keyname><forenames>Julio C.</forenames></author></authors><title>Energy Efficient Location and Activity-aware On-Demand Mobile
  Distributed Sensing Platform for Sensing as a Service in IoT Clouds</title><categories>cs.NI</categories><comments>IEEE Transactions on Computational Social Systems 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) envisions billions of sensors deployed around us
and connected to the Internet, where the mobile crowd sensing technologies are
widely used to collect data in different contexts of the IoT paradigm. Due to
the popularity of Big Data technologies, processing and storing large volumes
of data has become easier than ever. However, large scale data management tasks
still require significant amounts of resources that can be expensive regardless
of whether they are purchased or rented (e.g. pay-as-you-go infrastructure).
Further, not everyone is interested in such large scale data collection and
analysis. More importantly, not everyone has the financial and computational
resources to deal with such large volumes of data. Therefore, a timely need
exists for a cloud-integrated mobile crowd sensing platform that is capable of
capturing sensors data, on-demand, based on conditions enforced by the data
consumers. In this paper, we propose a context-aware, specifically, location
and activity-aware mobile sensing platform called C-MOSDEN ( Context-aware
Mobile Sensor Data ENgine) for the IoT domain. We evaluated the proposed
platform using three real-world scenarios that highlight the importance of
'selective sensing'. The computational effectiveness and efficiency of the
proposed platform are investigated and is used to highlight the advantages of
context-aware selective sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00445</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00445</id><created>2016-01-04</created><updated>2016-03-02</updated><authors><author><keyname>Gaafar</keyname><forenames>Mohamed</forenames></author><author><keyname>Khafagy</keyname><forenames>Mohammad Galal</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Improper Gaussian Signaling in Full-Duplex Relay Channels with Residual
  Self-Interference</title><categories>cs.IT math.IT</categories><comments>revised (v2), Accepted in IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the potential employment of improper Gaussian signaling (IGS) in
full-duplex cooperative settings with residual self-interference (RSI). IGS is
recently shown to outperform traditional proper Gaussian signaling (PGS) in
several interference-limited channel settings. In this work, IGS is employed in
an attempt to alleviate the RSI adverse effect in full-duplex relaying (FDR).
To this end, we derive a tight upper bound expression for the end-to-end outage
probability in terms of the relay signal parameters represented in its power
and circularity coefficient. We further show that the derived upper bound is
either monotonic or unimodal in the relay's circularity coefficient. This
result allows for easily locating the global optimal point using known
numerical methods. Based on the analysis, IGS allows FDR systems to operate
even with high RSI. It is shown that, while the communication totally fails
with PGS as the RSI increases, the IGS outage probability approaches a fixed
value that depends on the channel statistics and target rate. The obtained
results show that IGS can leverage higher relay power budgets than PGS to
improve the performance, meanwhile it relieves its RSI impact via tuning the
signal impropriety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00449</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00449</id><created>2016-01-04</created><authors><author><keyname>McDonald</keyname><forenames>Andrew M.</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Stamos</keyname><forenames>Dimitris</forenames></author></authors><title>Fitting Spectral Decay with the $k$-Support Norm</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectral $k$-support norm enjoys good estimation properties in low rank
matrix learning problems, empirically outperforming the trace norm. Its unit
ball is the convex hull of rank $k$ matrices with unit Frobenius norm. In this
paper we generalize the norm to the spectral $(k,p)$-support norm, whose
additional parameter $p$ can be used to tailor the norm to the decay of the
spectrum of the underlying model. We characterize the unit ball and we
explicitly compute the norm. We further provide a conditional gradient method
to solve regularization problems with the norm, and we derive an efficient
algorithm to compute the Euclidean projection on the unit ball in the case
$p=\infty$. In numerical experiments, we show that allowing $p$ to vary
significantly improves performance over the spectral $k$-support norm on
various matrix completion benchmarks, and better captures the spectral decay of
the underlying model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00453</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00453</id><created>2016-01-04</created><authors><author><keyname>Borkowski</keyname><forenames>J&#xf3;zef</forenames></author><author><keyname>Kania</keyname><forenames>Dariusz</forenames></author></authors><title>Interpolated-DFT-Based Fast and Accurate Amplitude and Phase Estimation
  for the Control of Power</title><categories>cs.SY</categories><comments>in Metrology and Measurement Systems, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of energy produced in renewable energy systems has to be at the
high level specified by respective standards and directives. The estimation
accuracy of grid signal parameters is one of the most important factors
affecting this quality. This paper presents a method for a very fast and
accurate amplitude and phase grid signal estimation using the Fast Fourier
Transform procedure and maximum decay sidelobes windows. The most important
features of the method are the elimination of the impact associated with the
conjugate's component on the results and the straightforward implementation.
Moreover, the measurement time is very short - even far less than one period of
the grid signal. The influence of harmonics on the results is reduced by using
a bandpass prefilter. Even using a 40 dB FIR prefilter for the grid signal with
THD = 38%, SNR = 53 dB and a 20-30% slow decay exponential drift the maximum
error of the amplitude estimation is approximately 1% and approximately 0.085
rad of the phase estimation in a real-time DSP system for 512 samples. The
errors are smaller by several orders of magnitude for more accurate prefilters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00455</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00455</id><created>2016-01-04</created><authors><author><keyname>Alonso-Sanz</keyname><forenames>Ramon</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Actin automata with memory</title><categories>cs.ET nlin.CG</categories><comments>Accepted for publication in &quot;International Journal of Bifurcation and
  Chaos&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actin is a globular protein which forms long polar filaments in eukaryotic.
The actin filaments play roles of cytoskeleton, motility units , information
processing and learning. We model actin filament as a double chain of finite
state machines, nodes, which take states `0' and `1'. The states are
abstractions of absence and presence of a sub-threshold charge on an actin
units corresponding to the nodes. All nodes update their state in parallel in
discrete time. A node updates its current state depending on states of two
closest neighbours in the node chain and two closest neighbours in the
complementary chain. Previous models of actin automata considered momentary
state transitions of nodes. We enrich the actin automata model by assuming that
states of nodes depends not only on the current states of neighbouring node but
also on their past states. Thus, we assess the effect of memory of past states
on the dynamics of acting automata. We demonstrate in computational experiments
that memory slows down propagation of perturbations, decrease entropy of
space-time patterns generated, transforms travelling localisations to
stationary oscillators, and stationary oscillations to still patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00473</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00473</id><created>2016-01-04</created><authors><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>The discretised lognormal and hooked power law distributions for
  complete citation data: Best options for modelling and regression</title><categories>cs.DL</categories><comments>Thelwall, M. (in press). The discretised lognormal and hooked power
  law distributions for complete citation data: Best options for modelling and
  regression. Journal of Informetrics</comments><journal-ref>Journal of Informetrics, 10(2), 336-346.
  doi:10.1016/j.joi.2015.12.007</journal-ref><doi>10.1016/j.joi.2015.12.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the statistical distribution that best fits citation data is
important to allow robust and powerful quantitative analyses. Whilst previous
studies have suggested that both the hooked power law and discretised lognormal
distributions fit better than the power law and negative binomial
distributions, no comparisons so far have covered all articles within a
discipline, including those that are uncited. Based on an analysis of 26
different Scopus subject areas in seven different years, this article reports
comparisons of the discretised lognormal and the hooked power law with citation
data, adding 1 to citation counts in order to include zeros. The hooked power
law fits better in two thirds of the subject/year combinations tested for
journal articles that are at least three years old, including most medical,
life and natural sciences, and for virtually all subject areas for younger
articles. Conversely, the discretised lognormal tends to fit best for arts,
humanities, social science and engineering fields. The difference between the
fits of the distributions is mostly small, however, and so either could
reasonably be used for modelling citation data. For regression analyses,
however, the best option is to use ordinary least squares regression applied to
the natural logarithm of citation counts plus one, especially for sets of
younger articles, because of the increased precision of the parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00479</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00479</id><created>2016-01-04</created><authors><author><keyname>Albers</keyname><forenames>Susanne</forenames></author><author><keyname>Kraft</keyname><forenames>Dennis</forenames></author></authors><title>Motivating Time-Inconsistent Agents: A Computational Approach</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the computational complexity of motivating
time-inconsistent agents to complete long term projects. We resort to an
elegant graph-theoretic model, introduced by Kleinberg and Oren, which consists
of a task graph $G$ with $n$ vertices, including a source $s$ and target $t$,
and an agent that incrementally constructs a path from $s$ to $t$ in order to
collect rewards. The twist is that the agent is present-biased and discounts
future costs and rewards by a factor $\beta\in [0,1]$. Our design objective is
to ensure that the agent reaches $t$ i.e.\ completes the project, for as little
reward as possible. Such graphs are called motivating. We consider two
strategies. First, we place a single reward $r$ at $t$ and try to guide the
agent by removing edges from $G$. We prove that deciding the existence of such
motivating subgraphs is NP-complete if $r$ is fixed. More importantly, we
generalize our reduction to a hardness of approximation result for computing
the minimum $r$ that admits a motivating subgraph. In particular, we show that
no polynomial-time approximation to within a ratio of $\sqrt{n}/4$ or less is
possible, unless ${\rm P}={\rm NP}$. Furthermore, we develop a
$(1+\sqrt{n})$-approximation algorithm and thus settle the approximability of
computing motivating subgraphs. Secondly, we study motivating reward
configurations, where non-negative rewards $r(v)$ may be placed on arbitrary
vertices $v$ of $G$. The agent only receives the rewards of visited vertices.
Again we give an NP-completeness result for deciding the existence of a
motivating reward configuration within a fixed budget $b$. This result even
holds if $b=0$, which in turn implies that no efficient approximation of a
minimum $b$ within a ration grater or equal to $1$ is possible, unless ${\rm
P}={\rm NP}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00481</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00481</id><created>2016-01-04</created><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author><author><keyname>Baeza-Yates</keyname><forenames>Ricardo</forenames></author></authors><title>Data Portraits and Intermediary Topics: Encouraging Exploration of
  Politically Diverse Profiles</title><categories>cs.HC</categories><comments>12 pages, 7 figures. To be presented at ACM Intelligent User
  Interfaces 2016</comments><acm-class>H.4.3; H.5.2</acm-class><doi>10.1145/2856767.2856776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In micro-blogging platforms, people connect and interact with others.
However, due to cognitive biases, they tend to interact with like-minded people
and read agreeable information only. Many efforts to make people connect with
those who think differently have not worked well. In this paper, we
hypothesize, first, that previous approaches have not worked because they have
been direct -- they have tried to explicitly connect people with those having
opposing views on sensitive issues. Second, that neither recommendation or
presentation of information by themselves are enough to encourage behavioral
change. We propose a platform that mixes a recommender algorithm and a
visualization-based user interface to explore recommendations. It recommends
politically diverse profiles in terms of distance of latent topics, and
displays those recommendations in a visual representation of each user's
personal content. We performed an &quot;in the wild&quot; evaluation of this platform,
and found that people explored more recommendations when using a biased
algorithm instead of ours. In line with our hypothesis, we also found that the
mixture of our recommender algorithm and our user interface, allowed
politically interested users to exhibit an unbiased exploration of the
recommended profiles. Finally, our results contribute insights in two aspects:
first, which individual differences are important when designing platforms
aimed at behavioral change; and second, which algorithms and user interfaces
should be mixed to help users avoid cognitive mechanisms that lead to biased
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00501</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00501</id><created>2016-01-04</created><authors><author><keyname>Bova</keyname><forenames>Simone</forenames></author></authors><title>SDDs are Exponentially More Succinct than OBDDs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduced by Darwiche (2011), sentential decision diagrams (SDDs) are
essentially as tractable as ordered binary decision diagrams (OBDDs), but tend
to be more succinct in practice. This makes SDDs a prominent representation
language, with many applications in artificial intelligence and knowledge
compilation. We prove that SDDs are more succinct than OBDDs also in theory, by
constructing a family of boolean functions where each member has polynomial SDD
size but exponential OBDD size. This exponential separation improves a
quasipolynomial separation recently established by Razgon (2013), and settles
an open problem in knowledge compilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00506</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00506</id><created>2016-01-04</created><authors><author><keyname>Dhanalakshmi</keyname><forenames>S.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author><author><keyname>Kumar</keyname><forenames>D. Sunil</forenames></author></authors><title>Tri-connectivity Augmentation in Trees</title><categories>math.CO cs.DM</categories><comments>10 pages, 2 figures, 3 algorithms, Presented in ICGTA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a connected graph, a {\em minimum vertex separator} is a minimum set of
vertices whose removal creates at least two connected components. The vertex
connectivity of the graph refers to the size of the minimum vertex separator
and a graph is $k$-vertex connected if its vertex connectivity is $k$, $k\geq
1$. Given a $k$-vertex connected graph $G$, the combinatorial problem {\em
vertex connectivity augmentation} asks for a minimum number of edges whose
augmentation to $G$ makes the resulting graph $(k+1)$-vertex connected. In this
paper, we initiate the study of $r$-vertex connectivity augmentation whose
objective is to find a $(k+r)$-vertex connected graph by augmenting a minimum
number of edges to a $k$-vertex connected graph, $r \geq 1$. We shall
investigate this question for the special case when $G$ is a tree and $r=2$. In
particular, we present a polynomial-time algorithm to find a minimum set of
edges whose augmentation to a tree makes it 3-vertex connected. Using lower
bound arguments, we show that any tri-vertex connectivity augmentation of trees
requires at least $\lceil \frac {2l_1+l_2}{2} \rceil$ edges, where $l_1$ and
$l_2$ denote the number of degree one vertices and degree two vertices,
respectively. Further, we establish that our algorithm indeed augments this
number, thus yielding an optimum algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00516</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00516</id><created>2016-01-04</created><updated>2016-01-05</updated><authors><author><keyname>Ameri</keyname><forenames>Michael</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author></authors><title>Why Just Boogie? Translating Between Intermediate Verification Languages</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The verification systems Boogie and Why3 use their respective intermediate
languages to generate verification conditions from high-level programs. Since
the two systems support different back-end provers (such as Z3 and Alt-Ergo)
and are used to encode different high-level languages (such as C# and Java),
being able to translate between their intermediate languages would provide a
way to reuse one system's features to verify programs meant for the other. This
paper describes a translation of Boogie into WhyML (Why3's intermediate
language) that preserves semantics, verifiability, and program structure to a
large degree. We implemented the translation as a tool and applied it to 194
Boogie-verified programs of various sources and sizes; Why3 verified 84% of the
translated programs with the same outcome as Boogie. These results indicate
that the translation is often effective and practically applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00517</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00517</id><created>2016-01-04</created><authors><author><keyname>Zehl</keyname><forenames>Sven</forenames></author><author><keyname>Zubow</keyname><forenames>Anatolij</forenames></author><author><keyname>Wolisz</keyname><forenames>Adam</forenames></author><author><keyname>Doering</keyname><forenames>Michael</forenames></author></authors><title>ResFi: A Secure Framework for Self Organized Radio Resource Management
  in Residential WiFi Networks</title><categories>cs.NI</categories><report-no>TKN-15-0005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dense deployments of residential WiFi networks individual users suffer
performance degradation due to both contention and interference. While Radio
Resource Management (RRM) is known to mitigate this effects its application in
residential WiFi networks being by nature unplanned and individually managed
creates a big challenge. We propose ResFi - a framework supporting creation of
RRM functionality in legacy deployments. The radio interfaces are used for
efficient discovery of adjacent APs and as a side-channel to establish a secure
communication among the individual Access Point Management Applications within
a neighborhood over the wired Internet backbone. We have implemented a
prototype of ResFi and studied its performance in our testbed. As a showcase we
have implemented various RRM applications among others a distributed channel
assignment algorithm using ResFi. ResFi is provided to the community as open
source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00524</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00524</id><created>2015-12-30</created><authors><author><keyname>Tropashko</keyname><forenames>Vadim</forenames></author></authors><title>Ideal Databases</title><categories>cs.DB</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From algebraic geometry perspective database relations are succinctly defined
as Finite Varieties. After establishing basic framework, we give analytic proof
of Heath theorem from Database Dependency theory. Next, we leverage
Algebra/Geometry dictionary and focus on algebraic counterparts of finite
varieties, polynomial ideals. It is well known that intersection and sum of
ideals are lattice operations. We generalize this fact to ideals from different
rings, therefore establishing that algebra of ideals is Relational Lattice. The
final stop is casting the framework into Linear Algebra, and traversing to
Quantum Theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00526</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00526</id><created>2016-01-04</created><authors><author><keyname>Lebeau</keyname><forenames>Fabrice</forenames><affiliation>ENS Lyon</affiliation></author><author><keyname>Touati</keyname><forenames>Corinne</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Altman</keyname><forenames>Eitan</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Abuzainab</keyname><forenames>Nof</forenames></author></authors><title>The Social Medium Selection Game</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider in this paper competition of content creators in routing their
content through various media. The routing decisions may correspond to the
selection of a social network (e.g. twitter versus facebook or linkedin) or of
a group within a given social network. The utility for a player to send its
content to some medium is given as the difference between the dissemination
utility at this medium and some transmission cost. We model this game as a
congestion game and compute the pure potential of the game. In contrast to the
continuous case, we show that there may be various equilibria. We show that the
potential is M-concave which allows us to characterize the equilibria and to
propose an algorithm for computing it. We then give a learning mechanism which
allow us to give an efficient algorithm to determine an equilibrium. We finally
determine the asymptotic form of the equilibrium and discuss the implications
on the social medium selection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00529</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00529</id><created>2016-01-04</created><updated>2016-01-05</updated><authors><author><keyname>Kowalski</keyname><forenames>Robert</forenames></author><author><keyname>Sadri</keyname><forenames>Fariba</forenames></author></authors><title>Programming in logic without logic programming</title><categories>cs.AI</categories><comments>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, we proposed a logic-based framework in which computation is
the execution of actions in an attempt to make reactive rules of the form if
antecedent then consequent true in a canonical model of a logic program
determined by an initial state, sequence of events, and the resulting sequence
of subsequent states. In this model-theoretic semantics, reactive rules are the
driving force, and logic programs play only a supporting role.
  In the canonical model, states, actions and other events are represented with
timestamps. But in the operational semantics, for the sake of efficiency,
timestamps are omitted and only the current state is maintained. State
transitions are performed reactively by executing actions to make the
consequents of rules true whenever the antecedents become true. This
operational semantics is sound, but incomplete. It cannot make reactive rules
true by preventing their antecedents from becoming true, or by proactively
making their consequents true before their antecedents become true.
  In this paper, we characterize the notion of reactive model, and prove that
the operational semantics can generate all and only such models. In order to
focus on the main issues, we omit the logic programming component of the
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00530</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00530</id><created>2016-01-04</created><authors><author><keyname>Sun</keyname><forenames>Biao</forenames></author><author><keyname>Feng</keyname><forenames>Hui</forenames></author><author><keyname>Xu</keyname><forenames>Xinxin</forenames></author></authors><title>HISTORY: An Efficient and Robust Algorithm for Noisy 1-bit Compressed
  Sensing</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sparse signal recovery from 1-bit measurements.
Due to the noise present in the acquisition and transmission process, some
quantized bits may be flipped to their opposite states. These sign flips may
result in severe performance degradation. In this study, a novel algorithm,
termed HISTORY, is proposed. It consists of Hamming support detection and
coefficients recovery. The HISTORY algorithm has high recovery accuracy and is
robust to strong measurement noise. Numerical results are provided to
demonstrate the effectiveness and superiority of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00533</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00533</id><created>2015-12-27</created><authors><author><keyname>Blanke</keyname><forenames>Tobias</forenames><affiliation>CMB, ALPAGE</affiliation></author><author><keyname>Kristel</keyname><forenames>Conny</forenames><affiliation>CMB, ALPAGE</affiliation></author><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>CMB, ALPAGE</affiliation></author></authors><title>Crowds for Clouds: Recent Trends in Humanities Research Infrastructures</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Agiati Benardou, Erik Champion, Costis Dallas, Lorna Hughes
  Cultural Heritage Digital Tools and Infrastructures, 2016, 978-1-4724-4712-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanities have convincingly argued that they need transnational research
opportunities and through the digital transformation of their disciplines also
have the means to proceed with it on an up to now unknown scale. The digital
transformation of research and its resources means that many of the artifacts,
documents, materials, etc. that interest humanities research can now be
combined in new and innovative ways. Due to the digital transformations, (big)
data and information have become central to the study of culture and society.
Humanities research infrastructures manage, organise and distribute this kind
of information and many more data objects as they becomes relevant for social
and cultural research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00543</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00543</id><created>2016-01-04</created><authors><author><keyname>Meng</keyname><forenames>Xiangming</forenames><affiliation>David</affiliation></author><author><keyname>Wu</keyname><forenames>Sheng</forenames><affiliation>David</affiliation></author><author><keyname>Kuang</keyname><forenames>Linling</forenames><affiliation>David</affiliation></author><author><keyname>Defeng</keyname><affiliation>David</affiliation></author><author><keyname>Huang</keyname></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>Approximate Message Passing with Nearest Neighbor Sparsity Pattern
  Learning</title><categories>cs.IT cs.LG math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering clustered sparse signals with no prior
knowledge of the sparsity pattern. Beyond simple sparsity, signals of interest
often exhibits an underlying sparsity pattern which, if leveraged, can improve
the reconstruction performance. However, the sparsity pattern is usually
unknown a priori. Inspired by the idea of k-nearest neighbor (k-NN) algorithm,
we propose an efficient algorithm termed approximate message passing with
nearest neighbor sparsity pattern learning (AMP-NNSPL), which learns the
sparsity pattern adaptively. AMP-NNSPL specifies a flexible spike and slab
prior on the unknown signal and, after each AMP iteration, sets the sparse
ratios as the average of the nearest neighbor estimates via expectation
maximization (EM). Experimental results on both synthetic and real data
demonstrate the superiority of our proposed algorithm both in terms of
reconstruction performance and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00549</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00549</id><created>2016-01-04</created><authors><author><keyname>Kari</keyname><forenames>Dariush</forenames></author><author><keyname>Delibalta</keyname><forenames>Ibrahim</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman Serdar</forenames></author></authors><title>Boosted Adaptive Filters</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the boosting notion extensively used in different machine
learning applications to adaptive signal processing literature and implement
several different adaptive filtering algorithms. In this framework, we have
several adaptive filtering algorithms, i.e., the weak learners, that run in
parallel on a common task such as equalization, classification, regression or
filtering. For each newly received input vector and observation pair, each
algorithm adapts itself based on the performance of the other adaptive
algorithms in the mixture on this current data pair. These relative updates
provide the boosting effect such that the algorithms in the mixture learn a
different attribute of the data providing diversity. The outputs of these
parallel running algorithms are then combined using adaptive mixture
approaches. We demonstrate the intrinsic connections of boosting with the
adaptive mixture methods and data reuse algorithms. Specifically, we study
parallel running recursive least squares and least mean squares algorithms and
provide different boosted versions of these well known adaptation methods. We
provide the MSE performance results as well as computational complexity bounds
for the boosted adaptive filters. We demonstrate over widely used real life
data sets in the machine learning and adaptive signal processing literatures
that we can substantially improve the performances of these algorithms due to
the boosting effect with a relatively small computational complexity increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00571</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00571</id><created>2016-01-04</created><authors><author><keyname>Colman</keyname><forenames>Ewan</forenames></author><author><keyname>Charlton</keyname><forenames>Nathaniel</forenames></author></authors><title>Interpreting network communicability with stochastic models and data</title><categories>physics.soc-ph cs.SI</categories><comments>15 Pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced concept of dynamic communicability is a valuable tool
for ranking the importance of nodes in a temporal network. Two metrics,
broadcast score and receive score, were introduced to measure the centrality of
a node with respect to a model of contagion based on time-respecting walks.
This article examines the temporal and structural factors influencing these
metrics by considering a versatile stochastic temporal network model. We
analytically derive formulae to accurately predict the expectation of the
broadcast and receive scores when one or more columns in a temporal edge-list
are shuffled. These methods are then applied to two publicly available
data-sets and we quantify how much the centrality of each individual depends on
structural or temporal influences. From our analysis we highlight two practical
contributions: a way to control for temporal variation when computing dynamic
communicability, and the conclusion that the broadcast and receive scores can,
under a range of circumstances, be replaced by the row and column sums of the
matrix exponential of a weighted adjacency matrix given by the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00574</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00574</id><created>2016-01-04</created><authors><author><keyname>Teich</keyname><forenames>Brendan</forenames></author><author><keyname>Lutz</keyname><forenames>Roman</forenames></author><author><keyname>Kassarnig</keyname><forenames>Valentin</forenames></author></authors><title>NFL Play Prediction</title><categories>cs.LG</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on NFL game data we try to predict the outcome of a play in multiple
different ways. An application of this is the following: by plugging in various
play options one could determine the best play for a given situation in real
time. While the outcome of a play can be described in many ways we had the most
promising results with a newly defined measure that we call &quot;progress&quot;. We see
this work as a first step to include predictive analysis into NFL playcalling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00584</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00584</id><created>2016-01-04</created><updated>2016-01-05</updated><authors><author><keyname>Louren&#xe7;o</keyname><forenames>Cl&#xe1;udio Belo</forenames></author><author><keyname>Frade</keyname><forenames>Maria Jo&#xe3;o</forenames></author><author><keyname>Pinto</keyname><forenames>Jorge Sousa</forenames></author></authors><title>A Single-Assignment Translation for Annotated Programs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a translation of While programs annotated with loop invariants
into a dynamic single-assignment language with a dedicated iterating construct.
We prove that the translation is sound and complete. This is a companion report
to our paper Formalizing Single-assignment Program Verification: an
Adaptation-complete Approach [6].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00595</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00595</id><created>2016-01-04</created><authors><author><keyname>Papageorgiou</keyname><forenames>George</forenames></author><author><keyname>Bouboulis</keyname><forenames>Pantelis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author></authors><title>Robust non-linear regression analysis: A greedy approach employing
  kernels and application to image denoising</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of robust non-linear estimation in the presence of both
bounded noise and outliers. Assuming that the unknown non-linear function
belongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to accurately
estimate the coefficients of the kernel regression matrix. Due to the existence
of outliers, common techniques such as the Kernel Ridge Regression (KRR), or
the Support Vector Regression (SVR) turn out to be inadequate. Instead, we
employ sparse modeling arguments to model and estimate the outliers, adopting a
greedy approach. In particular, the proposed robust scheme, i.e., Kernel Greedy
Algorithm for Robust Denoising (KGARD), is a modification of the classical
Orthogonal Matching Pursuit (OMP) algorithm. In a nutshell, the proposed scheme
alternates between a KRR task and an OMP-like selection step. Convergence
properties as well as theoretical results concerning the identification of the
outliers are provided. Moreover, KGARD is compared against other cutting edge
methods (using toy examples) to demonstrate its performance and verify the
aforementioned theoretical results. Finally, the proposed robust estimation
framework is applied to the task of image denoising, showing that it can
enhance the denoising process significantly, when outliers are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00599</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00599</id><created>2016-01-04</created><authors><author><keyname>Zeppelzauer</keyname><forenames>Matthias</forenames></author><author><keyname>Schopfhauser</keyname><forenames>Daniel</forenames></author></authors><title>Multimodal Classification of Events in Social Media</title><categories>cs.CV cs.IR cs.MM</categories><comments>Preprint of accepted manuscript for the Elsevier Image and Vision
  Computing Journal (IMAVIS). The paper will be published by IMAVIS under DOI
  10.1016/j.imavis.2015.12.004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large amount of social media hosted on platforms like Flickr and Instagram
is related to social events. The task of social event classification refers to
the distinction of event and non-event-related content as well as the
classification of event types (e.g. sports events, concerts, etc.). In this
paper, we provide an extensive study of textual, visual, as well as multimodal
representations for social event classification. We investigate strengths and
weaknesses of the modalities and study synergy effects between the modalities.
Experimental results obtained with our multimodal representation outperform
state-of-the-art methods and provide a new baseline for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00608</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00608</id><created>2016-01-04</created><authors><author><keyname>Ewaisha</keyname><forenames>Ahmed</forenames></author></authors><title>Scheduling and Power Allocation to Optimize Service and Queue-Waiting
  Times in Cognitive Radio Uplinks</title><categories>cs.IT math.IT</categories><comments>Keywords: Cognitive Radios, Delay optimal scheduling, optimal
  stopping rule, Lyapunov optimization, Priority Queuing, Interference
  management, Underlay, Overlay. arXiv admin note: substantial text overlap
  with arXiv:1410.7460</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we study the packet delay as a QoS metric in CR systems. The
packet delay includes the queue waiting time and the service time. In this
work, we study the effect of both the scheduling and the power allocation
algorithms on the delay performance of the SUs.
  To study the delay due to the service time we study a multichannel system
where the channels are sensed sequentially, we study the tradeoff between
throughput and delay. The problem is formulated as an optimal stopping rule
problem where there is a tradeoff between the service time and the throughput.
This tradeoff results from skipping low-quality channels to seek the
possibility of finding high-quality ones in the future at the expense of a
higher probability of being blocked from transmission since these future
channels might be busy.
  On the other hand, the queue waiting time is studied by considering a
multi-user single channel system. Specifically, we study the effect of
scheduling and power allocation on the delay performance of all SUs in the
system. We propose a delay optimal algorithm that protects the PUs from harmful
interference and provides the required delay guarantees to users. Conventional
scheduling algorithms do not provide such guarantees if the interference
channels are heterogeneous. This is because they are developed for conventional
non-CR wireless systems that neglect interference since channels are
orthogonal.
  Finally, we present two potential extensions to these studied problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00617</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00617</id><created>2016-01-04</created><authors><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Coresets and Sketches</title><categories>cs.CG</categories><comments>Initial version of Chapter 49 in Handbook on Discrete and
  Computational Geometry, 3rd edition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric data summarization has become an essential tool in both geometric
approximation algorithms and where geometry intersects with big data problems.
In linear or near-linear time large data sets can be compressed into a summary,
and then more intricate algorithms can be run on the summaries whose results
approximate those of the full data set. Coresets and sketches are the two most
important classes of these summaries. We survey five types of coresets and
sketches: shape-fitting, density estimation, high-dimensional vectors, high-
dimensional point sets / matrices, and clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00620</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00620</id><created>2016-01-04</created><authors><author><keyname>Bing</keyname><forenames>Lidong</forenames></author><author><keyname>Ling</keyname><forenames>Mingyang</forenames></author><author><keyname>Wang</keyname><forenames>Richard C.</forenames></author><author><keyname>Cohen</keyname><forenames>William W.</forenames></author></authors><title>Distant IE by Bootstrapping Using Lists and Document Structure</title><categories>cs.CL</categories><comments>7 pages, to appear at AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distant labeling for information extraction (IE) suffers from noisy training
data. We describe a way of reducing the noise associated with distant IE by
identifying coupling constraints between potential instance labels. As one
example of coupling, items in a list are likely to have the same label. A
second example of coupling comes from analysis of document structure: in some
corpora, sections can be identified such that items in the same section are
likely to have the same label. Such sections do not exist in all corpora, but
we show that augmenting a large corpus with coupling constraints from even a
small, well-structured corpus can improve performance substantially, doubling
F1 on one task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00626</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00626</id><created>2016-01-04</created><authors><author><keyname>Shi</keyname><forenames>Baoxu</forenames></author><author><keyname>Weninger</keyname><forenames>Tim</forenames></author></authors><title>Scalable Models for Computing Hierarchies in Information Networks</title><categories>cs.AI cs.DL cs.LG</categories><comments>Preprint for &quot;Knowledge and Information Systems&quot; paper, in press</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Information hierarchies are organizational structures that often used to
organize and present large and complex information as well as provide a
mechanism for effective human navigation. Fortunately, many statistical and
computational models exist that automatically generate hierarchies; however,
the existing approaches do not consider linkages in information {\em networks}
that are increasingly common in real-world scenarios. Current approaches also
tend to present topics as an abstract probably distribution over words, etc
rather than as tangible nodes from the original network. Furthermore, the
statistical techniques present in many previous works are not yet capable of
processing data at Web-scale. In this paper we present the Hierarchical
Document Topic Model (HDTM), which uses a distributed vertex-programming
process to calculate a nonparametric Bayesian generative model. Experiments on
three medium size data sets and the entire Wikipedia dataset show that HDTM can
infer accurate hierarchies even over large information networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00630</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00630</id><created>2016-01-04</created><authors><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Tang</keyname><forenames>Pingfan</forenames></author></authors><title>Approximate Distribution of L1 Median on Uncertain Data</title><categories>cs.DM stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the L1 median for locationally uncertain points with discrete
distributions. That is, each point in a data set has a discrete probability
distribution describing its location. The L1 median is a robust estimator,
useful when there are outliers in the point set. However given the
probabilistic nature of this data, there is a distribution describing the L1
median, not a single location. We show how to construct and estimate this
median distribution in near-linear or quadratic time in 1 and 2 dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00643</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00643</id><created>2016-01-03</created><authors><author><keyname>Yadav</keyname><forenames>Chandra Shekhar</forenames></author><author><keyname>Sharan</keyname><forenames>Aditi</forenames></author></authors><title>Hybrid Approach for Single Text Document Summarization using Statistical
  and Sentiment Features</title><categories>cs.IR</categories><comments>20 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summarization is a way to represent same information in concise way with
equal sense. This can be categorized in two type Abstractive and Extractive
type. Our work is focused around Extractive summarization. A generic approach
to extractive summarization is to consider sentence as an entity, score each
sentence based on some indicative features to ascertain the quality of sentence
for inclusion in summary. Sort the sentences on the score and consider top n
sentences for summarization. Mostly statistical features have been used for
scoring the sentences. We are proposing a hybrid model for a single text
document summarization. This hybrid model is an extraction based approach,
which is combination of Statistical and semantic technique. The hybrid model
depends on the linear combination of statistical measures : sentence position,
TF-IDF, Aggregate similarity, centroid, and semantic measure. Our idea to
include sentiment analysis for salient sentence extraction is derived from the
concept that emotion plays an important role in communication to effectively
convey any message hence, it can play a vital role in text document
summarization. For comparison we have generated five system summaries Proposed
Work, MEAD system, Microsoft system, OPINOSIS system, and Human generated
summary, and evaluation is done using ROUGE score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00669</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00669</id><created>2016-01-04</created><authors><author><keyname>Augello</keyname><forenames>Agnese</forenames></author><author><keyname>Infantino</keyname><forenames>Ignazio</forenames></author><author><keyname>Lieto</keyname><forenames>Antonio</forenames></author><author><keyname>Pilato</keyname><forenames>Giovanni</forenames></author><author><keyname>Rizzo</keyname><forenames>Riccardo</forenames></author><author><keyname>Vella</keyname><forenames>Filippo</forenames></author></authors><title>Artwork creation by a cognitive architecture integrating computational
  creativity and dual process approaches</title><categories>cs.AI</categories><comments>30 pages, 8 figures, to appear in Biologically Inspired Cognitive
  Architectures 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a novel cognitive architecture (CA) for computational
creativity based on the Psi model and on the mechanisms inspired by dual
process theories of reasoning and rationality. In recent years, many cognitive
models have focused on dual process theories to better describe and implement
complex cognitive skills in artificial agents, but creativity has been
approached only at a descriptive level. In previous works we have described
various modules of the cognitive architecture that allows a robot to execute
creative paintings. By means of dual process theories we refine some relevant
mechanisms to obtain artworks, and in particular we explain details about the
resolution level of the CA dealing with different strategies of access to the
Long Term Memory (LTM) and managing the interaction between S1 and S2 processes
of the dual process theory. The creative process involves both divergent and
convergent processes in either implicit or explicit manner. This leads to four
activities (exploratory, reflective, tacit, and analytic) that, triggered by
urges and motivations, generate creative acts. These creative acts exploit both
the LTM and the WM in order to make novel substitutions to a perceived image by
properly mixing parts of pictures coming from different domains. The paper
highlights the role of the interaction between S1 and S2 processes, modulated
by the resolution level, which focuses the attention of the creative agent by
broadening or narrowing the exploration of novel solutions, or even drawing the
solution from a set of already made associations. An example of artificial
painter is described in some experimentations by using a robotic platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00670</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00670</id><created>2016-01-04</created><updated>2016-03-04</updated><authors><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>Kucukelbir</keyname><forenames>Alp</forenames></author><author><keyname>McAuliffe</keyname><forenames>Jon D.</forenames></author></authors><title>Variational Inference: A Review for Statisticians</title><categories>stat.CO cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the core problems of modern statistics is to approximate
difficult-to-compute probability distributions. This problem is especially
important in Bayesian statistics, which frames all inference about unknown
quantities as a calculation about the posterior. In this paper, we review
variational inference (VI), a method from machine learning that approximates
probability distributions through optimization. VI has been used in myriad
applications and tends to be faster than classical methods, such as Markov
chain Monte Carlo sampling. The idea behind VI is to first posit a family of
distributions and then to find the member of that family which is close to the
target. Closeness is measured by Kullback-Leibler divergence. We review the
ideas behind mean-field variational inference, discuss the special case of VI
applied to exponential family models, present a full example with a Bayesian
mixture of Gaussians, and derive a variant that uses stochastic optimization to
scale up to massive data. We discuss modern research in VI and highlight
important open problems. VI is powerful, but it is not yet well understood. Our
hope in writing this paper is to catalyze statistical research on this
widely-used class of algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00681</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00681</id><created>2016-01-04</created><authors><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author></authors><title>Modeling and Simulation of Molecular Communication Systems with a
  Reversible Adsorption Receiver</title><categories>cs.ET cs.IT math.IT</categories><comments>14 pages, 8 figures, 1 algorithm, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an analytical model for the diffusive molecular
communication (MC) system with a reversible adsorption receiver in a fluid
environment. The widely used concentration shift keying (CSK) is considered for
modulation. The time-varying spatial distribution of the information molecules
under the reversible adsorption and desorption reaction at the surface of a
receiver is analytically characterized. Based on the spatial distribution, we
derive the net number of newly-adsorbed information molecules expected in any
time duration. We further derive the number of newly-adsorbed molecules
expected at the steady state to demonstrate the equilibrium concentration.
Given the number of newly-adsorbed information molecules, the bit error
probability of the proposed MC system is analytically approximated.
Importantly, we present a simulation framework for the proposed model that
accounts for the diffusion and reversible reaction. Simulation results show the
accuracy of our derived expressions, and demonstrate the positive effect of the
adsorption rate and the negative effect of the desorption rate on the error
probability of reversible adsorption receiver with last transmit bit-1.
Moreover, our analytical results simplify to the special cases of a full
adsorption receiver and a partial adsorption receiver, both of which do not
include desorption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00691</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00691</id><created>2016-01-04</created><updated>2016-01-07</updated><authors><author><keyname>Asor</keyname><forenames>Ohad</forenames></author></authors><title>Spectral and Modular Analysis of #P Problems</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present various analytic and number theoretic results concerning the #SAT
problem as reflected when reduced into a #PART problem. As an application we
propose a heuristic to probabilistically estimate the solution of #SAT
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00701</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00701</id><created>2016-01-04</created><authors><author><keyname>Brito</keyname><forenames>Carlos S. N.</forenames></author><author><keyname>Gerstner</keyname><forenames>Wulfram</forenames></author></authors><title>Nonlinear Hebbian learning as a unifying principle in receptive field
  formation</title><categories>q-bio.NC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of sensory receptive fields has been modeled in the past by a
variety of models including normative models such as sparse coding or
independent component analysis and bottom-up models such as spike-timing
dependent plasticity or the Bienenstock-Cooper-Munro model of synaptic
plasticity. Here we show that the above variety of approaches can all be
unified into a single common principle, namely Nonlinear Hebbian Learning. When
Nonlinear Hebbian Learning is applied to natural images, receptive field shapes
were strongly constrained by the input statistics and preprocessing, but
exhibited only modest variation across different choices of nonlinearities in
neuron models or synaptic plasticity rules. Neither overcompleteness nor sparse
network activity are necessary for the development of localized receptive
fields. The analysis of alternative sensory modalities such as auditory models
or V2 development lead to the same conclusions. In all examples, receptive
fields can be predicted a priori by reformulating an abstract model as
nonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural
statistics can account for many aspects of receptive field formation across
models and sensory modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00706</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00706</id><created>2016-01-04</created><authors><author><keyname>Yang</keyname><forenames>Jimei</forenames></author><author><keyname>Reed</keyname><forenames>Scott</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author></authors><title>Weakly-supervised Disentangling with Recurrent Transformations for 3D
  View Synthesis</title><categories>cs.LG cs.AI cs.CV</categories><comments>This was published in NIPS 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem for both graphics and vision is to synthesize novel
views of a 3D object from a single image. This is particularly challenging due
to the partial observability inherent in projecting a 3D object onto the image
space, and the ill-posedness of inferring object shape and pose. However, we
can train a neural network to address the problem if we restrict our attention
to specific object categories (in our case faces and chairs) for which we can
gather ample training data. In this paper, we propose a novel recurrent
convolutional encoder-decoder network that is trained end-to-end on the task of
rendering rotated objects starting from a single image. The recurrent structure
allows our model to capture long-term dependencies along a sequence of
transformations. We demonstrate the quality of its predictions for human faces
on the Multi-PIE dataset and for a dataset of 3D chair models, and also show
its ability to disentangle latent factors of variation (e.g., identity and
pose) without using full supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00707</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00707</id><created>2016-01-04</created><authors><author><keyname>&#xd6;zsu</keyname><forenames>M. Tamer</forenames></author></authors><title>A Survey of RDF Data Management Systems</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDF is increasingly being used to encode data for the semantic web and for
data exchange. There have been a large number of works that address RDF data
management. In this paper we provide an overview of these works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00710</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00710</id><created>2016-01-04</created><authors><author><keyname>Zoph</keyname><forenames>Barret</forenames></author><author><keyname>Knight</keyname><forenames>Kevin</forenames></author></authors><title>Multi-Source Neural Translation</title><categories>cs.CL</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We build a multi-source machine translation model and train it to maximize
the probability of a target English string given French and German sources.
Using the neural encoder-decoder framework, we explore several combination
methods and report up to +4.8 Bleu increases on top of a very strong
attention-based neural translation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00713</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00713</id><created>2016-01-04</created><authors><author><keyname>Bukatin</keyname><forenames>Michael</forenames></author><author><keyname>Matthews</keyname><forenames>Steve</forenames></author></authors><title>Almost Continuous Transformations of Software and Higher-order Dataflow
  Programming</title><categories>cs.PL</categories><comments>12 pages, July 9, 2015 preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two classes of stream-based computations which admit taking
linear combinations of execution runs: probabilistic sampling and generalized
animation. The dataflow architecture is a natural platform for programming with
streams. The presence of linear combinations allows us to introduce the notion
of almost continuous transformation of dataflow graphs. We introduce a new
approach to higher-order dataflow programming: a dynamic dataflow program is a
stream of dataflow graphs evolving by almost continuous transformations. A
dynamic dataflow program would typically run while it evolves. We introduce
Fluid, an experimental open source system for programming with dataflow graphs
and almost continuous transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00720</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00720</id><created>2016-01-04</created><authors><author><keyname>Ahmad</keyname><forenames>Subutai</forenames></author><author><keyname>Hawkins</keyname><forenames>Jeff</forenames></author></authors><title>How do neurons operate on sparse distributed representations? A
  mathematical theory of sparsity, neurons and active dendrites</title><categories>q-bio.NC cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a formal mathematical model for sparse representations in
neocortex based on a neuron model and associated operations. The design of our
model neuron is inspired by recent experimental findings on active dendritic
processing and NMDA spikes in pyramidal neurons. We derive a number of scaling
laws that characterize the accuracy of such neurons in detecting activation
patterns in a neuronal population under adverse conditions. We introduce the
union property which shows that synapses for multiple patterns can be randomly
mixed together within a segment and still lead to highly accurate recognition.
We describe simulation results that provide overall insight into sparse
representations as well as two primary results. First we show that pattern
recognition by a neuron can be extremely accurate and robust with high
dimensional sparse inputs even when using a tiny number of synapses to
recognize large patterns. Second, equations representing recognition accuracy
of a dendrite predict optimal NMDA spiking thresholds under a generous set of
assumptions. The prediction tightly matches NMDA spiking thresholds measured in
the literature. Our model neuron matches many of the known properties of
pyramidal neurons. As such the theory provides a unified and practical
mathematical framework for understanding the benefits and limits of sparse
representations in cortical networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00721</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00721</id><created>2016-01-04</created><authors><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Chen</keyname><forenames>Zhengchuan</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Optimum Transmission Policies for Energy Harvesting Sensor Networks
  Powered By a Mobile Control Center</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures. The short version was presented in part at 2015
  IEEE International Conference on Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless energy transfer, namely RF-based energy harvesting, is a potential
way to prolong the lifetime of energy-constrained devices, especially in
wireless sensor networks. However, due to huge propagation attenuation, its
energy efficiency is regarded as the biggest bottleneck to widely applications.
It is critical to find appropriate transmission policies to improve the global
energy efficiency in this kind of systems. To this end, this paper focuses on
the sensor networks scenario, where a mobile control center powers the sensors
by RF signal and also collects information from them. Two related schemes,
called as harvest-and-use scheme and harvest-store-use scheme, are
investigated, respectively. In harvest-and-use scheme, as a benchmark, both
constant and adaptive transmission modes from sensors are discussed. To
harvest-store-use scheme, we propose a new concept, the best opportunity for
wireless energy transfer, and use it to derive an explicit closed-form
expression of optimal transmission policy. It is shown by simulation that a
considerable improvement in terms of energy efficiency can be obtained with the
help of the transmission policies developed in this paper. Furthermore, the
transmission policies is also discussed under the constraint of fixed
information rate. The minimal required power, the performance loss from the new
constraint as well as the effect of fading are then presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00722</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00722</id><created>2016-01-04</created><authors><author><keyname>Qi</keyname><forenames>Guanglei</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Li</keyname><forenames>Jinghua</forenames></author></authors><title>Matrix Variate RBM and Its Applications</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann Machine (RBM) is an importan- t generative model
modeling vectorial data. While applying an RBM in practice to images, the data
have to be vec- torized. This results in high-dimensional data and valu- able
spatial information has got lost in vectorization. In this paper, a
Matrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed by
generalizing the classic RBM to explicitly model matrix data. In the new RBM
model, both input and hidden variables are in matrix forms which are connected
by bilinear transforms. The MVRBM has much less model parameters, resulting in
a faster train- ing algorithm while retaining comparable performance as the
classic RBM. The advantages of the MVRBM have been demonstrated on two
real-world applications: Image super- resolution and handwritten digit
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00732</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00732</id><created>2016-01-04</created><updated>2016-01-06</updated><authors><author><keyname>Tierney</keyname><forenames>Stephen</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhengwu</forenames></author></authors><title>Low-Rank Representation over the Manifold of Curves</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning it is common to interpret each data point as a vector in
Euclidean space. However the data may actually be functional i.e.\ each data
point is a function of some variable such as time and the function is
discretely sampled. The naive treatment of functional data as traditional
multivariate data can lead to poor performance since the algorithms are
ignoring the correlation in the curvature of each function. In this paper we
propose a method to analyse subspace structure of the functional data by using
the state of the art Low-Rank Representation (LRR). Experimental evaluation on
synthetic and real data reveals that this method massively outperforms
conventional LRR in tasks concerning functional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00738</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00738</id><created>2016-01-05</created><authors><author><keyname>Zeng</keyname><forenames>Jiaan</forenames></author></authors><title>Resource Sharing for Multi-Tenant NoSQL Data Store in Cloud</title><categories>cs.DC cs.AI cs.DB cs.SY</categories><comments>PhD dissertation, December 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tenancy hosting of users in cloud NoSQL data stores is favored by cloud
providers because it enables resource sharing at low operating cost.
Multi-tenancy takes several forms depending on whether the back-end file system
is a local file system (LFS) or a parallel file system (PFS), and on whether
tenants are independent or share data across tenants. In this thesis I focus on
and propose solutions to two cases: independent data-local file system, and
shared data-parallel file system.
  In the independent data-local file system case, resource contention occurs
under certain conditions in Cassandra and HBase, two state-of-the-art NoSQL
stores, causing performance degradation for one tenant by another. We
investigate the interference and propose two approaches. The first provides a
scheduling scheme that can approximate resource consumption, adapt to workload
dynamics and work in a distributed fashion. The second introduces a
workload-aware resource reservation approach to prevent interference. The
approach relies on a performance model obtained offline and plans the
reservation according to different workload resource demands. Results show the
approaches together can prevent interference and adapt to dynamic workloads
under multi-tenancy.
  In the shared data-parallel file system case, it has been shown that running
a distributed NoSQL store over PFS for shared data across tenants is not cost
effective. Overheads are introduced due to the unawareness of the NoSQL store
of PFS. This dissertation targets the key-value store (KVS), a specific form of
NoSQL stores, and proposes a lightweight KVS over a parallel file system to
improve efficiency. The solution is built on an embedded KVS for high
performance but uses novel data structures to support concurrent writes.
Results show the proposed system outperforms Cassandra and Voldemort in several
different workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00740</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00740</id><created>2016-01-05</created><authors><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Koppula</keyname><forenames>Hema S</forenames></author><author><keyname>Soh</keyname><forenames>Shane</forenames></author><author><keyname>Raghavan</keyname><forenames>Bharad</forenames></author><author><keyname>Singh</keyname><forenames>Avi</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep
  Learning Architecture</title><categories>cs.RO cs.CV cs.LG</categories><comments>Journal Version (ICCV and ICRA combination with more system details)
  http://brain4cars.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced Driver Assistance Systems (ADAS) have made driving safer over the
last decade. They prepare vehicles for unsafe road conditions and alert drivers
if they perform a dangerous maneuver. However, many accidents are unavoidable
because by the time drivers are alerted, it is already too late. Anticipating
maneuvers beforehand can alert drivers before they perform the maneuver and
also give ADAS more time to avoid or prepare for the danger.
  In this work we propose a vehicular sensor-rich platform and learning
algorithms for maneuver anticipation. For this purpose we equip a car with
cameras, Global Positioning System (GPS), and a computing device to capture the
driving context from both inside and outside of the car. In order to anticipate
maneuvers, we propose a sensory-fusion deep learning architecture which jointly
learns to anticipate and fuse multiple sensory streams. Our architecture
consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory
(LSTM) units to capture long temporal dependencies. We propose a novel training
procedure which allows the network to predict the future given only a partial
temporal context. We introduce a diverse data set with 1180 miles of natural
freeway and city driving, and show that we can anticipate maneuvers 3.5 seconds
before they occur in real-time with a precision and recall of 90.5\% and 87.4\%
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00741</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00741</id><created>2016-01-05</created><authors><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Sharma</keyname><forenames>Shikhar</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Learning Preferences for Manipulation Tasks from Online Coactive
  Feedback</title><categories>cs.RO cs.AI cs.LG</categories><comments>IJRR accepted (Learning preferences over trajectories from coactive
  feedback)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning preferences over trajectories for mobile
manipulators such as personal robots and assembly line robots. The preferences
we learn are more intricate than simple geometric constraints on trajectories;
they are rather governed by the surrounding context of various objects and
human interactions in the environment. We propose a coactive online learning
framework for teaching preferences in contextually rich environments. The key
novelty of our approach lies in the type of feedback expected from the user:
the human user does not need to demonstrate optimal trajectories as training
data, but merely needs to iteratively provide trajectories that slightly
improve over the trajectory currently proposed by the system. We argue that
this coactive preference feedback can be more easily elicited than
demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds
of our algorithm match the asymptotic rates of optimal trajectory algorithms.
  We implement our algorithm on two high degree-of-freedom robots, PR2 and
Baxter, and present three intuitive mechanisms for providing such incremental
feedback. In our experimental evaluation we consider two context rich settings
-- household chores and grocery store checkout -- and show that users are able
to train the robot with just a few feedbacks (taking only a few
minutes).\footnote{Parts of this work has been published at NIPS and ISRR
conferences~\citep{Jain13,Jain13b}. This journal submission presents a
consistent full paper, and also includes the proof of regret bounds, more
details of the robotic system, and a thorough related work.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00751</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00751</id><created>2016-01-05</created><authors><author><keyname>Ghaznavi</keyname><forenames>Milad</forenames></author><author><keyname>Shahriar</keyname><forenames>Nashid</forenames></author><author><keyname>Ahmed</keyname><forenames>Reaz</forenames></author><author><keyname>Boutaba</keyname><forenames>Raouf</forenames></author></authors><title>Service Function Chaining Simplified</title><categories>cs.NI</categories><comments>9 pages, 7 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Middleboxes have become a vital part of modern networks by providing service
functions such as content filtering, load balancing and optimization of network
traffic. An ordered sequence of middleboxes composing a logical service is
called service chain. Service Function Chaining (SFC) enables us to define
these service chains. Recent optimization models of SFCs assume that the
functionality of a middlebox is provided by a single software appliance,
commonly known as Virtual Network Function (VNF). This assumption limits SFCs
to the throughput of an individual VNF and resources of a physical machine
hosting the VNF instance. Moreover, typical service providers offer VNFs with
heterogeneous throughput and resource configurations. Thus, deploying a service
chain with custom throughput can become a tedious process of stitching
heterogeneous VNF instances. In this paper, we describe how we can overcome
these limitations without worrying about underlying VNF configurations and
resource constraints. This prospect is achieved by distributed deploying
multiple VNF instances providing the functionality of a middlebox and modeling
the optimal deployment of a service chain as a mixed integer programming
problem. The proposed model optimizes host and bandwidth resources allocation,
and determines the optimal placement of VNF instances, while balancing workload
and routing traffic among these VNF instances. We show that this problem is
NP-Hard and propose a heuristic solution called Kariz. Kariz utilizes a tuning
parameter to control the trade-off between speed and accuracy of the solution.
Finally, our solution is evaluated using simulations in data-center networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00763</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00763</id><created>2016-01-05</created><updated>2016-01-12</updated><authors><author><keyname>Wang</keyname><forenames>Pei</forenames></author><author><keyname>Wang</keyname><forenames>Shuai</forenames></author><author><keyname>Ming</keyname><forenames>Jiang</forenames></author><author><keyname>Jiang</keyname><forenames>Yufei</forenames></author><author><keyname>Wu</keyname><forenames>Dinghao</forenames></author></authors><title>Translingual Obfuscation</title><categories>cs.CR cs.SE</categories><comments>The extended version of a paper to appear in the Proceedings of the
  1st IEEE European Symposium on Security and Privacy, 2016, (EuroS&amp;P '16), 20
  pages, updated references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program obfuscation is an important software protection technique that
prevents attackers from revealing the programming logic and design of the
software. We introduce translingual obfuscation, a new software obfuscation
scheme which makes programs obscure by &quot;misusing&quot; the unique features of
certain programming languages. Translingual obfuscation translates part of a
program from its original language to another language which has a different
programming paradigm and execution model, thus increasing program complexity
and impeding reverse engineering. In this paper, we investigate the feasibility
and effectiveness of translingual obfuscation with Prolog, a logic programming
language. We implement translingual obfuscation in a tool called BABEL, which
can selectively translate C functions into Prolog predicates. By leveraging two
important features of the Prolog language, i.e., unification and backtracking,
BABEL obfuscates both the data layout and control flow of C programs, making
them much more difficult to reverse engineer. Our experiments show that BABEL
provides effective and stealthy software obfuscation, while the cost is only
modest compared to one of the most popular commercial obfuscators on the
market. With BABEL, we verified the feasibility of translingual obfuscation,
which we consider to be a promising new direction for software obfuscation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00770</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00770</id><created>2016-01-05</created><authors><author><keyname>Miwa</keyname><forenames>Makoto</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author></authors><title>End-to-end Relation Extraction using LSTMs on Sequences and Tree
  Structures</title><categories>cs.CL cs.LG</categories><comments>10 pages, 1 figure, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel end-to-end neural model to extract entities and relations
between them. Our recurrent neural network based model stacks bidirectional
sequential LSTM-RNNs and bidirectional tree-structured LSTM-RNNs to capture
both word sequence and dependency tree substructure information. This allows
our model to jointly represent both entities and relations with shared
parameters. We further encourage detection of entities during training and use
of entity information in relation extraction via curriculum learning and
scheduled sampling. Our model improves over the state-of-the-art feature-based
model on end-to-end relation extraction, achieving 3.5% and 4.8% relative error
reductions in F-score on ACE2004 and ACE2005, respectively. We also show
improvements over the state-of-the-art convolutional neural network based model
on nominal relation classification (SemEval-2010 Task 8), with 2.5% relative
error reduction in F-score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00781</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00781</id><created>2016-01-05</created><authors><author><keyname>Kurzejamski</keyname><forenames>Grzegorz</forenames></author><author><keyname>Zawistowski</keyname><forenames>Jacek</forenames></author><author><keyname>Sarwas</keyname><forenames>Grzegorz</forenames></author></authors><title>Robust Method of Vote Aggregation and Proposition Verification for
  Invariant Local Features</title><categories>cs.CV</categories><comments>8 pages Short Paper, presented at VISAPP 2015 Conference in Berlin,
  March. Proceedings of the 10th International Conference on Computer Vision
  Theory and Applications, 252-259, 2015, Berlin, Germany, ISBN
  978-989-758-090-1</comments><doi>10.5220/0005267002520259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for analysis of the vote space created from the
local features extraction process in a multi-detection system. The method is
opposed to the classic clustering approach and gives a high level of control
over the clusters composition for further verification steps. Proposed method
comprises of the graphical vote space presentation, the proposition generation,
the two-pass iterative vote aggregation and the cascade filters for
verification of the propositions. Cascade filters contain all of the minor
algorithms needed for effective object detection verification. The new approach
does not have the drawbacks of the classic clustering approaches and gives a
substantial control over process of detection. Method exhibits an exceptionally
high detection rate in conjunction with a low false detection chance in
comparison to alternative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00788</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00788</id><created>2016-01-05</created><authors><author><keyname>Maehara</keyname><forenames>Daiki</forenames></author><author><keyname>Tran</keyname><forenames>Gia Khanh</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Araki</keyname><forenames>Kiyomichi</forenames></author></authors><title>Experimental Study on Battery-less Sensor Network Activated by
  Multi-point Wireless Energy Transmission</title><categories>cs.NI</categories><comments>This paper is submitted to IEICE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper empirically validates battery-less sensor activation via wireless
energy transmission to release sensors from wires and batteries. To seamlessly
extend the coverage and activate sensor nodes distributed in any indoor
environment, we proposed multi-point wireless energy transmission with carrier
shift diversity. In this scheme, multiple transmitters are employed to
compensate path-loss attenuation and orthogonal frequencies are allocated to
the multiple transmitters to avoid the destructive interference that occurs
when the same frequency is used by all transmitters. In our previous works, the
effectiveness of the proposed scheme was validated theoretically and also
empirically by using just a spectrum analyzer to measure the received power. In
this paper, we develop low-energy battery-less sensor nodes whose consumed
power and required received power for activation are respectively 142 uW and
400 uW. In addition, we conduct indoor experiments in which the received power
and activation of battery-less sensor node are simultaneously observed by using
the developed battery-less sensor node and a spectrum analyzer. The results
show that the coverage of single-point and multi-point wireless energy
transmission without carrier shift diversity are, respectively, 84.4% and
83.7%, while the coverage of the proposed scheme is 100%. It can be concluded
that the effectiveness of the proposed scheme can be verified by our
experiments using real battery-less sensor nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00796</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00796</id><created>2016-01-05</created><authors><author><keyname>Khodaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panos</forenames></author></authors><title>The Key to Intelligent Transportation: Identity and Credential
  Management in Vehicular Communication Systems</title><categories>cs.CR</categories><comments>6 pages, 2 figures, IEEE Vehicular Technology Magazine, Volume:10,
  Issue: 4</comments><doi>10.1109/MVT.2015.2479367</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Communication (VC) systems will greatly enhance intelligent
transportation systems. But their security and the protection of their users'
privacy are a prerequisite for deployment. Efforts in industry and academia
brought forth a multitude of diverse proposals. These have now converged to a
common view, notably on the design of a security infrastructure, a Vehicular
Public Key Infrastructure (VPKI) that shall enable secure conditionally
anonymous VC. Standardization efforts and industry readiness to adopt this
approach hint to its maturity. However, there are several open questions
remaining, and it is paramount to have conclusive answers before deployment. In
this article, we distill and critically survey the state of the art for
identity and credential management in VC systems, and we sketch a roadmap for
addressing a set of critical remaining security and privacy challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00816</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00816</id><created>2016-01-05</created><authors><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames><affiliation>Flowers</affiliation></author></authors><title>Open challenges in understanding development and evolution of speech
  forms: The roles of embodied self-organization, motivation and active
  exploration</title><categories>cs.AI cs.CL cs.CY cs.LG</categories><proxy>ccsd</proxy><journal-ref>Journal of Phonetics, Elsevier, 2015, 53, pp.5</journal-ref><doi>10.1016/j.wocn.2015.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses open scientific challenges for understanding
development and evolution of speech forms, as a commentary to Moulin-Frier et
al. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models
of the origins of speech forms, with a focus on their assumptions , we study
the fundamental question of how speech can be formed out of non--speech, at
both developmental and evolutionary scales. In particular, we emphasize the
importance of embodied self-organization , as well as the role of mechanisms of
motivation and active curiosity-driven exploration in speech formation. Finally
, we discuss an evolutionary-developmental perspective of the origins of
speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00825</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00825</id><created>2016-01-05</created><authors><author><keyname>Palazzo</keyname><forenames>Simone</forenames></author><author><keyname>Spampinato</keyname><forenames>Concetto</forenames></author><author><keyname>Giordano</keyname><forenames>Daniela</forenames></author></authors><title>Gamifying Video Object Segmentation</title><categories>cs.CV</categories><comments>Submitted to PAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video object segmentation can be considered as one of the most challenging
computer vision problems. Indeed, so far, no existing solution is able to
effectively deal with the peculiarities of real-world videos, especially in
cases of articulated motion and object occlusions; limitations that appear more
evident when we compare their performance with the human one. However, manually
segmenting objects in videos is largely impractical as it requires a lot of
human time and concentration. To address this problem, in this paper we propose
an interactive video object segmentation method, which exploits, on one hand,
the capability of humans to identify correctly objects in visual scenes, and on
the other hand, the collective human brainpower to solve challenging tasks. In
particular, our method relies on a web game to collect human inputs on object
locations, followed by an accurate segmentation phase achieved by optimizing an
energy function encoding spatial and temporal constraints between object
regions as well as human-provided input. Performance analysis carried out on
challenging video datasets with some users playing the game demonstrated that
our method shows a better trade-off between annotation times and segmentation
accuracy than interactive video annotation and automated video object
segmentation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00833</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00833</id><created>2016-01-05</created><authors><author><keyname>Ghosh</keyname><forenames>Sucheta</forenames></author><author><keyname>Cernak</keyname><forenames>Milos</forenames></author><author><keyname>Palit</keyname><forenames>Sarbani</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>An Analysis of Rhythmic Staccato-Vocalization Based on Frequency
  Demodulation for Laughter Detection in Conversational Meetings</title><categories>cs.SD</categories><comments>5 pages, 1 figure, conference paper</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Human laugh is able to convey various kinds of meanings in human
communications. There exists various kinds of human laugh signal, for example:
vocalized laugh and non vocalized laugh. Following the theories of psychology,
among all the vocalized laugh type, rhythmic staccato-vocalization
significantly evokes the positive responses in the interactions. In this paper
we attempt to exploit this observation to detect human laugh occurrences, i.e.,
the laughter, in multiparty conversations from the AMI meeting corpus. First,
we separate the high energy frames from speech, leaving out the low energy
frames through power spectral density estimation. We borrow the algorithm of
rhythm detection from the area of music analysis to use that on the high energy
frames. Finally, we detect rhythmic laugh frames, analyzing the candidate
rhythmic frames using statistics. This novel approach for detection of
`positive' rhythmic human laughter performs better than the standard laughter
classification baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00834</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00834</id><created>2016-01-05</created><authors><author><keyname>Jordane</keyname><forenames>Lorandel</forenames></author><author><keyname>Pr&#xe9;votet</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>H&#xe9;lard</keyname><forenames>Maryline</forenames></author></authors><title>Fast Power and Energy Efficiency Analysis of FPGA-based Wireless
  Base-band Processing</title><categories>cs.DC cs.NI</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, demands for high performance keep on increasing in the wireless
communication domain. This leads to a consistent rise of the complexity and
designing such systems has become a challenging task. In this context, energy
efficiency is considered as a key topic, especially for embedded systems in
which design space is often very constrained. In this paper, a fast and
accurate power estimation approach for FPGA-based hardware systems is applied
to a typical wireless communication system. It aims at providing power
estimates of complete systems prior to their implementations. This is made
possible by using a dedicated library of high-level models that are
representative of hardware IPs. Based on high-level simulations, design space
exploration is made a lot faster and easier. The definition of a scenario and
the monitoring of IP's time-activities facilitate the comparison of several
domain-specific systems. The proposed approach and its benefits are
demonstrated through a typical use case in the wireless communication domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00835</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00835</id><created>2016-01-05</created><authors><author><keyname>Chiang</keyname><forenames>Mung</forenames></author></authors><title>Fog Networking: An Overview on Research Opportunities</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past 15 years have seen the rise of the Cloud, along with rapid increase
in Internet backbone traffic and more sophisticated cellular core networks.
There are three different types of Clouds: (1) data center, (2) backbone IP
network and (3) cellular core network, responsible for computation, storage,
communication and network management. Now the functions of these three types of
Clouds are descending to be among or near the end users, i.e., to the edge of
networks, as Fog. This article presents an overview on research opportunities
of Fog networking: an architecture that users one or a collaborative multitude
of end-user clients or near-user edge devices to carry out a substantial amount
of storage, communication and management. Architecture allocates
functionalities, while engineering artifacts that may use a Fog architecture
include 5G, home/personal networking, and the Internet of Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00839</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00839</id><created>2016-01-05</created><authors><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Approximate Distance Oracles for Planar Graphs with Improved Query
  Time-Space Tradeoff</title><categories>cs.DS</categories><comments>20 pages, 9 figures of which 2 illustrate pseudo-code. This is the
  SODA 2016 version but with the definition of C_i in Phase I fixed and the
  analysis slightly modified accordingly. The main change is in the subsection
  bounding query time and stretch for Phase I</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider approximate distance oracles for edge-weighted n-vertex
undirected planar graphs. Given fixed epsilon &gt; 0, we present a
(1+epsilon)-approximate distance oracle with O(n(loglog n)^2) space and
O((loglog n)^3) query time. This improves the previous best product of query
time and space of the oracles of Thorup (FOCS 2001, J. ACM 2004) and Klein
(SODA 2002) from O(n log n) to O(n(loglog n)^5).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00846</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00846</id><created>2016-01-05</created><authors><author><keyname>Khodaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Jin</keyname><forenames>Hongyu</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panos</forenames></author></authors><title>Towards Deploying a Scalable &amp; Robust Vehicular Identity and Credential
  Management Infrastructure</title><categories>cs.CR</categories><comments>8 pages, 13 figures, IEEE Vehicular Networking Conference (VNC). IEEE
  VNC, Dec. 2014, pp. 33-40</comments><doi>10.1109/VNC.2014.7013306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several years of academic and industrial research efforts have converged to a
common understanding on fundamental security building blocks for the upcoming
Vehicular Communication (VC) systems. There is a growing consensus towards
deploying a Vehicular Public-Key Infrastructure (VPKI) enables pseudonymous
authentication, with standardization efforts in that direction. However, there
are still significant technical issues that remain unresolved. Existing
proposals for instantiating the VPKI either need additional detailed
specifications or enhanced security and privacy features. Equally important,
there is limited experimental work that establishes the VPKI efficiency and
scalability. In this paper, we are concerned with exactly these issues. We
leverage the common VPKI approach and contribute an enhanced system with
precisely defined, novel features that improve its resilience and the user
privacy protection. In particular, we depart from the common assumption that
the VPKI entities are fully trusted and we improve user privacy in the face of
an honest-but-curious security infrastructure. Moreover, we fully implement our
VPKI, in a standard-compliant manner, and we perform an extensive evaluation.
Along with stronger protection and richer functionality, our system achieves
very significant performance improvement over prior systems - contributing the
most advanced VPKI towards deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00847</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00847</id><created>2016-01-04</created><authors><author><keyname>Breuer</keyname><forenames>David</forenames></author><author><keyname>Nikoloski</keyname><forenames>Zoran</forenames></author></authors><title>DeFiNe: an optimisation-based method for robust disentangling of
  filamentous networks</title><categories>cs.DS</categories><journal-ref>Sci Rep, 2015, 5:18267</journal-ref><doi>10.1038/srep18267</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thread-like structures are pervasive across scales, from polymeric proteins
to root systems to galaxy filaments, and their characteristics can be readily
investigated in the network formalism. Yet, network links usually represent
only parts of filaments, which, when neglected, may lead to erroneous
conclusions from network-based analyses. The existing alternatives to detect
filaments in network representations require tuning of parameters over a large
range of values and treat all filaments equally, thus, precluding automated
analysis of diverse filamentous systems. Here, we propose a fully automated and
robust optimisation-based approach to detect filaments of consistent
intensities and angles in a given network. We test and demonstrate the accuracy
of our solution with contrived, biological, and cosmic filamentous structures.
In particular, we show that the proposed approach provides powerful automated
means to study properties of individual actin filaments in their network
context. Our solution is made publicly available as an open-source tool,
DeFiNe, facilitating decomposition of any given network into individual
filaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00852</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00852</id><created>2016-01-05</created><authors><author><keyname>Ghadirzadeh</keyname><forenames>Ali</forenames></author><author><keyname>B&#xfc;tepage</keyname><forenames>Judith</forenames></author><author><keyname>Kragic</keyname><forenames>Danica</forenames></author><author><keyname>Bj&#xf6;rkman</keyname><forenames>M&#xe5;rten</forenames></author></authors><title>Self-learning and adaptation in a sensorimotor framework</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework to autonomously achieve a task, where autonomy
is acquired by learning sensorimotor patterns of a robot, while it is
interacting with its environment. To accomplish the task, using the learned
sensorimotor contingencies, our approach predicts a sequence of actions that
will lead to the desirable observations. Gaussian processes (GP) with automatic
relevance determination is used to learn the sensorimotor mapping. In this way,
relevant sensory and motor components can be systematically found in
high-dimensional sensory and motor spaces. We propose an incremental GP
learning strategy, which discerns between situations, when an update or an
adaptation must be implemented. RRT* is exploited to enable long-term planning
and generating a sequence of states that lead to a given goal; while a
gradient-based search finds the optimum action to steer to a neighbouring state
in a single time step. Our experimental results prove the successfulness of the
proposed framework to learn a joint space controller with high data dimensions
(10$\times$15). It demonstrates short training phase (less than 12 seconds),
real-time performance and rapid adaptations capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00855</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00855</id><created>2016-01-05</created><authors><author><keyname>Saleiro</keyname><forenames>Pedro</forenames></author><author><keyname>Teixeira</keyname><forenames>Jorge</forenames></author><author><keyname>Soares</keyname><forenames>Carlos</forenames></author><author><keyname>Oliveira</keyname><forenames>Eug&#xe9;nio</forenames></author></authors><title>TimeMachine: Entity-centric Search and Visualization of News Archives</title><categories>cs.IR</categories><comments>Advances in Information Retrieval: 38th European Conference on IR
  Research, ECIR 2016, Padua, Italy, March 20-23, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a dynamic web tool that allows interactive search and
visualization of large news archives using an entity-centric approach. Users
are able to search entities using keyword phrases expressing news stories or
events and the system retrieves the most relevant entities to the user query
based on automatically extracted and indexed entity profiles. From the
computational journalism perspective, TimeMachine allows users to explore media
content through time using automatic identification of entity names, jobs,
quotations and relations between entities from co-occurrences networks
extracted from the news articles. TimeMachine demo is available at
http://maquinadotempo.sapo.pt/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00863</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00863</id><created>2016-01-05</created><updated>2016-03-07</updated><authors><author><keyname>Peng</keyname><forenames>Zhimin</forenames></author><author><keyname>Wu</keyname><forenames>Tianyu</forenames></author><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author><author><keyname>Yan</keyname><forenames>Ming</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>Coordinate Friendly Structures, Algorithms and Applications</title><categories>math.OC cs.CE cs.DC math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on coordinate update methods, which are useful for solving
problems involving large or high-dimensional datasets. They decompose a problem
into simple subproblems, where each updates one, or a small block of, variables
while fixing others. These methods can deal with linear and nonlinear mappings,
smooth and nonsmooth functions, as well as convex and nonconvex problems. In
addition, they are easy to parallelize.
  The great performance of coordinate update methods depends on solving simple
subproblems. To derive simple subproblems for several new classes of
applications, this paper systematically studies coordinate friendly operators
that perform low-cost coordinate updates.
  Based on the discovered coordinate friendly operators, as well as operator
splitting techniques, we obtain new coordinate update algorithms for a variety
of problems in machine learning, image processing, as well as sub-areas of
optimization. Several problems are treated with coordinate update for the first
time in history. The obtained algorithms are scalable to large instances
through parallel and even asynchronous computing. We present numerical examples
to illustrate how effective these algorithms are.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00864</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00864</id><created>2016-01-05</created><updated>2016-01-08</updated><authors><author><keyname>Gandica</keyname><forenames>Yerali</forenames></author><author><keyname>Carvalho</keyname><forenames>Joao</forenames></author><author><keyname>Aidos</keyname><forenames>Fernando Sampaio Dos</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Carletti</keyname><forenames>Timoteo</forenames></author></authors><title>On the origin of burstiness in human behavior: The wikipedia edits case</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of human activities exhibit a bursty pattern, namely periods of very
high activity that are followed by rest periods. Records of this process
generate time series of events whose inter-event times follow a probability
distribution that displays a fat tail. The grounds for such phenomenon are not
yet clearly understood. In the present work we use the freely available
Wikipedia's editing records to tackle this question by measuring the level of
burstiness, as well as the memory effect of the editing tasks performed by
different editors in different pages. Our main finding is that, even though the
editing activity is conditioned by the circadian 24 hour cycle, the conditional
probability of an activity of a given duration at a given time of the day is
independent from the latter. This suggests that the human activity seems to be
related to the high &quot;cost&quot; of starting an action as opposed to the much lower
&quot;cost&quot; of continuing that action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00873</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00873</id><created>2016-01-05</created><authors><author><keyname>Mhiri</keyname><forenames>Mariem</forenames></author><author><keyname>Varma</keyname><forenames>Vineeth S.</forenames></author><author><keyname>Cheikhrouhou</keyname><forenames>Karim</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Samet</keyname><forenames>Abdelaziz</forenames></author></authors><title>Cross-layer distributed power control: A repeated games formulation to
  improve the sum energy-efficiency</title><categories>cs.NI cs.GT cs.IT math.IT</categories><comments>36 pages, single column draft format</comments><journal-ref>EURASIP Journal on Wireless Communications and Networking 2015, 1
  (2015): 1-16</journal-ref><doi>10.1186/s13638-015-0486-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this work is to improve the energy-efficiency (EE) of a
multiple access channel (MAC) system, through power control, in a distributed
manner. In contrast with many existing works on energy-efficient power control,
which ignore the possible presence of a queue at the transmitter, we consider a
new generalized cross-layer EE metric. This approach is relevant when the
transmitters have a non-zero energy cost even when the radiated power is zero
and takes into account the presence of a finite packet buffer and packet
arrival at the transmitter. As the Nash equilibrium (NE) is an
energy-inefficient solution, the present work aims at overcoming this deficit
by improving the global energy-efficiency. Indeed, as the considered system has
multiple agencies each with their own interest, the performance metric
reflecting the individual interest of each decision maker is the global
energy-efficiency defined then as the sum over individual energy-efficiencies.
Repeated games (RG) are investigated through the study of two dynamic games
(finite RG and discounted RG), whose equilibrium is defined when introducing a
new operating point (OP), Pareto-dominating the NE and relying only on
individual channel state information (CSI). Accordingly, closed-form
expressions of the minimum number of stages of the game for finite RG (FRG) and
the maximum discount factor of the discounted RG (DRG) were established. The
cross-layer model in the RG formulation leads to achieving a shorter minimum
number of stages in the FRG even for higher number of users. In addition, the
social welfare (sum of utilities) in the DRG decreases slightly with the
cross-layer model when the number of users increases while it is reduced
considerably with the Goodman model. Finally, we show that in real systems with
random packet arrivals, the cross-layer power control algorithm outperforms the
Goodman algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00876</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00876</id><created>2016-01-05</created><authors><author><keyname>Mabillard</keyname><forenames>Isaac</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>Eliminating Higher-Multiplicity Intersections, II. The Deleted Product
  Criterion in the $r$-Metastable Range</title><categories>math.GT cs.CG</categories><comments>28 pages, 10 figures</comments><msc-class>57Q35, 55S35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by Tverberg-type problems in topological combinatorics and by
classical results about embeddings (maps without double points), we study the
question whether a finite simplicial complex K can be mapped into R^d without
higher-multiplicity intersections. We focus on conditions for the existence of
almost r-embeddings, i.e., maps from K to R^d without r-intersection points
among any set of r pairwise disjoint simplices of K.
  Generalizing the classical Haefliger-Weber embeddability criterion, we show
that a well-known necessary deleted product condition for the existence of
almost r-embeddings is sufficient in a suitable r-metastable range of
dimensions (r d &gt; (r+1) dim K +2). This significantly extends one of the main
results of our previous paper (which treated the special case where d=rk and
dim K=(r-1)k, for some k&gt; 3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00877</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00877</id><created>2016-01-05</created><updated>2016-01-20</updated><authors><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author><author><keyname>Wrede</keyname><forenames>Sebastian</forenames></author></authors><title>Proceedings of the Sixth International Workshop on Domain-Specific
  Languages and Models for Robotic Systems (DSLRob 2015)</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sixth International Workshop on Domain-Specific Languages and Models for
Robotic Systems (DSLRob'15) was held September 28, 2015 in Hamburg (Germany),
as part of the IROS 2015 conference. The main topics of the workshop were
Domain-Specific Languages (DSLs) and Model-driven Software Development (MDSD)
for robotics. A domain-specific language is a programming language dedicated to
a particular problem domain that offers specific notations and abstractions
that increase programmer productivity within that domain. Model-driven software
development offers a high-level way for domain users to specify the
functionality of their system at the right level of abstraction. DSLs and
models have historically been used for programming complex systems. However
recently they have garnered interest as a separate field of study. Robotic
systems blend hardware and software in a holistic way that intrinsically raises
many crosscutting concerns (concurrency, uncertainty, time constraints, ...),
for which reason, traditional general-purpose languages often lead to a poor
fit between the language features and the implementation requirements. DSLs and
models offer a powerful, systematic way to overcome this problem, enabling the
programmer to quickly and precisely implement novel software solutions to
complex problems within the robotics domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00881</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00881</id><created>2015-12-28</created><updated>2016-03-04</updated><authors><author><keyname>Obuchi</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Cross validation in LASSO and its acceleration</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>32 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate leave-one-out cross validation (CV) as a determinator of the
weight of the penalty term in the least absolute shrinkage and selection
operator (LASSO). First, on the basis of the message passing algorithm and a
perturbative discussion assuming that the number of observations is
sufficiently large, we provide simple formulas for approximately assessing two
types of CV errors, which enable us to significantly reduce the necessary cost
of computation. These formulas also provide a simple connection of the CV
errors to the residual sums of squares between the reconstructed and the given
measurements. Second, on the basis of this finding, we analytically evaluate
the CV errors when the design matrix is given as a simple random matrix in the
large size limit by using the replica method. Finally, these results are
compared with those of numerical simulations on finite-size systems and are
confirmed to be correct. We also apply the simple formulas of the first type of
CV error to an actual dataset of the supernovae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00883</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00883</id><created>2015-12-28</created><authors><author><keyname>Yao</keyname><forenames>Bing</forenames></author><author><keyname>Yao</keyname><forenames>Ming</forenames></author><author><keyname>Chen</keyname><forenames>Xiang-en</forenames></author></authors><title>Probing Graph Proper Total Colorings With Additional Constrained
  Conditions</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph colorings are becoming an increasingly useful family of mathematical
models for a broad range of applications, such as time tabling and scheduling,
frequency assignment, register allocation, computer security and so on. Graph
proper total colorings with additional constrained conditions have been
investigated intensively in the last decade year. In this article some new
graph proper total colorings with additional constrained conditions are
defined, and approximations to the chromatic numbers of these colorings are
researched, as well as some graphs having these colorings have been verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00893</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00893</id><created>2016-01-05</created><authors><author><keyname>Melamud</keyname><forenames>Oren</forenames></author><author><keyname>McClosky</keyname><forenames>David</forenames></author><author><keyname>Patwardhan</keyname><forenames>Siddharth</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author></authors><title>The Role of Context Types and Dimensionality in Learning Word Embeddings</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first extensive evaluation of how using different types of
context to learn skip-gram word embeddings affects performance on a wide range
of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic
tasks tend to exhibit a clear preference to particular types of contexts and
higher dimensionality, more careful tuning is required for finding the optimal
settings for most of the extrinsic tasks that we considered. Furthermore, for
these extrinsic tasks, we find that once the benefit from increasing the
embedding dimensionality is mostly exhausted, simple concatenation of word
embeddings, learned with different context types, can yield further performance
gains. As an additional contribution, we propose a new variant of the skip-gram
model that learns word embeddings from weighted contexts of substitute words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00894</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00894</id><created>2016-01-05</created><updated>2016-01-07</updated><authors><author><keyname>Bates</keyname><forenames>Daniel</forenames></author><author><keyname>Chadwick</keyname><forenames>Alex</forenames></author><author><keyname>Mullins</keyname><forenames>Robert</forenames></author></authors><title>Configurable memory systems for embedded many-core processors</title><categories>cs.AR cs.DC</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The memory system of a modern embedded processor consumes a large fraction of
total system energy. We explore a range of different configuration options and
show that a reconfigurable design can make better use of the resources
available to it than any fixed implementation, and provide large improvements
in both performance and energy consumption. Reconfigurability becomes
increasingly useful as resources become more constrained, so is particularly
relevant in the embedded space.
  For an optimised architectural configuration, we show that a configurable
cache system performs an average of 20% (maximum 70%) better than the best
fixed implementation when two programs are competing for the same resources,
and reduces cache miss rate by an average of 70% (maximum 90%). We then present
a case study of AES encryption and decryption, and find that a custom memory
configuration can almost double performance, with further benefits being
achieved by specialising the task of each core when parallelising the program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00899</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00899</id><created>2016-01-05</created><updated>2016-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verd&#xfa;</keyname><forenames>Sergio</forenames></author></authors><title>Common Randomness and Key Generation with Limited Interaction</title><categories>cs.IT math.IT</categories><comments>40 pages, short version submitted to ISIT 2016. Fixed a few small
  mistakes in v1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic two-terminal common randomness (CR) and key generation models are
considered, where the communication between the terminals may be limited, and
in particular may not be enough to achieve the maximum CR/key rate. We
introduce general notions of $XY$-absolutely continuity and $XY$-concave
function, and characterize the first order CR/key-communication tradeoff in
terms of the evaluation of the $XY$-concave envelope of a functional defined on
a set of distributions, which is simpler than the multi-letter
characterization. Two extreme cases are given special attention. First, in the
regime of very small communication rates, the CR bits per interaction bit
(CRBIB) and key bits per interaction bit (KBIB) are expressed with a new
&quot;symmetrical strong data processing constant&quot;, defined as the minimum of a
parameter such that a certain information-theoretic functional touches its
$XY$-concave envelope at a given source distribution. We also provide a
computationally friendly strong converse bound for CRBIB and a similar (but not
necessarily strong) one for KBIB in terms of the supremum of the maximal
correlation coefficient over a set of distributions. The proof uses
hypercontractivity and properties of the R\'enyi divergence. A criterion the
tightness of the bound is given with applications to binary symmetric sources.
Second, a new characterization of the minimum interaction rate needed for
achieving the maximum key rate (MIMK) is given, and we resolve a conjecture by
Tyagi and Narayan \cite{tyagi2013common} regarding the MIMK for binary sources.
We also propose a new conjecture for binary symmetric sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00901</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00901</id><created>2016-01-05</created><authors><author><keyname>Starc</keyname><forenames>Janez</forenames></author><author><keyname>Mladeni&#x107;</keyname><forenames>Dunja</forenames></author></authors><title>Joint learning of ontology and semantic parser from text</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic parsing methods are used for capturing and representing semantic
meaning of text. Meaning representation capturing all the concepts in the text
may not always be available or may not be sufficiently complete. Ontologies
provide a structured and reasoning-capable way to model the content of a
collection of texts. In this work, we present a novel approach to joint
learning of ontology and semantic parser from text. The method is based on
semi-automatic induction of a context-free grammar from semantically annotated
text. The grammar parses the text into semantic trees. Both, the grammar and
the semantic trees are used to learn the ontology on several levels -- classes,
instances, taxonomic and non-taxonomic relations. The approach was evaluated on
the first sentences of Wikipedia pages describing people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00909</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00909</id><created>2016-01-05</created><authors><author><keyname>Petrovici</keyname><forenames>Mihai A.</forenames></author><author><keyname>Bytschok</keyname><forenames>Ilja</forenames></author><author><keyname>Bill</keyname><forenames>Johannes</forenames></author><author><keyname>Schemmel</keyname><forenames>Johannes</forenames></author><author><keyname>Meier</keyname><forenames>Karlheinz</forenames></author></authors><title>The high-conductance state enables neural sampling in networks of LIF
  neurons</title><categories>q-bio.NC cs.NE physics.bio-ph</categories><comments>3 pages, 1 figure</comments><doi>10.1186/1471-2202-16-S1-O2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The apparent stochasticity of in-vivo neural circuits has long been
hypothesized to represent a signature of ongoing stochastic inference in the
brain. More recently, a theoretical framework for neural sampling has been
proposed, which explains how sample-based inference can be performed by
networks of spiking neurons. One particular requirement of this approach is
that the neural response function closely follows a logistic curve.
  Analytical approaches to calculating neural response functions have been the
subject of many theoretical studies. In order to make the problem tractable,
particular assumptions regarding the neural or synaptic parameters are usually
made. However, biologically significant activity regimes exist which are not
covered by these approaches: Under strong synaptic bombardment, as is often the
case in cortex, the neuron is shifted into a high-conductance state (HCS)
characterized by a small membrane time constant. In this regime, synaptic time
constants and refractory periods dominate membrane dynamics.
  The core idea of our approach is to separately consider two different &quot;modes&quot;
of spiking dynamics: burst spiking and transient quiescence, in which the
neuron does not spike for longer periods. We treat the former by propagating
the PDF of the effective membrane potential from spike to spike within a burst,
while using a diffusion approximation for the latter. We find that our
prediction of the neural response function closely matches simulation data.
Moreover, in the HCS scenario, we show that the neural response function
becomes symmetric and can be well approximated by a logistic function, thereby
providing the correct dynamics in order to perform neural sampling. We hereby
provide not only a normative framework for Bayesian inference in cortex, but
also powerful applications of low-power, accelerated neuromorphic systems to
relevant machine learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00912</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00912</id><created>2016-01-05</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Stoianov</keyname><forenames>Nikolai</forenames></author><author><keyname>Baykal</keyname><forenames>Nazife</forenames></author><author><keyname>Moller</keyname><forenames>Alfred</forenames></author><author><keyname>Sawilla</keyname><forenames>Reginald</forenames></author><author><keyname>Jain</keyname><forenames>Pram</forenames></author><author><keyname>Lange</keyname><forenames>Mona</forenames></author><author><keyname>Vidu</keyname><forenames>Cristian</forenames></author></authors><title>Assessing Mission Impact of Cyberattacks: Report of the NATO IST-128
  Workshop</title><categories>cs.CR</categories><report-no>ARL-TR-7566</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents the results of a workshop conducted by the North
Atlantic Treaty Organization (NATO) Information Systems Technology (IST) Panel
in Istanbul, Turkey, in June 2015 to explore science and technology for
characterizing the impact of cyber-attacks on missions. Military mission
success is highly dependent on the communications and information systems
(CISs) that support the mission and their use in the cyber battlespace. The
inexorably growing dependency on computational information processing for
weapons, intelligence, communication, and logistics systems continues to
increase the vulnerability of missions to various cyber threats. Attacks on
CISs or other cyber incidents degrade or disrupt the usage of CISs, and the
resulting mission capability, performance, and completion. These incidents are
expected to increase in frequency and sophistication. The workshop participants
concluded that the key to solving the mission impact assessment problem was in
adopting and developing a new model-driven paradigm that creates and validates
mechanisms of modeling the mission organization, the mission(s), and the
cyber-vulnerable systems that support the mission(s). Such models then simulate
or portray the impacts of the cyber-attacks. In addition, such model-based
analysis could explore multiple alternative mitigation and work-around
strategies - an essential part of coping with mission impact - and select the
optimal course of mitigating actions. Only such a paradigm can be expected to
provide meaningful, actionable information about mission impacts that have not
been seen before or do not match prior experiences and patterns. The papers
presented at this workshop are available in an accompanying volume, Proceedings
of the NATO Workshop IST-128, Assessing Mission Impact of Cyber Attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00917</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00917</id><created>2016-01-05</created><updated>2016-02-05</updated><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Luo</keyname><forenames>Hongyin</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author><author><keyname>Chua</keyname><forenames>Tat-Seng</forenames></author></authors><title>DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing
  Hyperparameters of Deep Neural Networks</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of deep neural networks is well-known to be sensitive to the
setting of their hyperparameters. Recent advances in reverse-mode automatic
differentiation allow for optimizing hyperparameters with gradients. The
standard way of computing these gradients involves a forward and backward pass
of computations. However, the backward pass usually needs to consume
unaffordable memory to store all the intermediate variables to exactly reverse
the forward training procedure. In this work we propose a new method, DrMAD, to
distill the knowledge of the forward pass into a shortcut path, through which
we approximately reverse the training trajectory. Experiments on several image
benchmark datasets show that DrMAD is at least 45 times faster and consumes 100
times less memory compared to state-of-the-art methods for optimizing
hyperparameters with minimal compromise to its effectiveness. To the best of
our knowledge, DrMAD is the first research attempt to make it practical to
automatically tune thousands of hyperparameters of deep neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00925</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00925</id><created>2016-01-05</created><authors><author><keyname>der Br&#xfc;ck</keyname><forenames>Tim vor</forenames></author><author><keyname>Eger</keyname><forenames>Steffen</forenames></author><author><keyname>Mehler</keyname><forenames>Alexander</forenames></author></authors><title>Complex Decomposition of the Negative Distance kernel</title><categories>cs.LG</categories><comments>Proceedings of the IEEE International Conference on Machine Learning
  an Applications, Miami, Florida, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Support Vector Machine (SVM) has become a very popular machine learning
method for text classification. One reason for this relates to the range of
existing kernels which allow for classifying data that is not linearly
separable. The linear, polynomial and RBF (Gaussian Radial Basis Function)
kernel are commonly used and serve as a basis of comparison in our study. We
show how to derive the primal form of the quadratic Power Kernel (PK) -- also
called the Negative Euclidean Distance Kernel (NDK) -- by means of complex
numbers. We exemplify the NDK in the framework of text categorization using the
Dewey Document Classification (DDC) as the target scheme. Our evaluation shows
that the power kernel produces F-scores that are comparable to the reference
kernels, but is -- except for the linear kernel -- faster to compute. Finally,
we show how to extend the NDK-approach by including the Mahalanobis distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00955</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00955</id><created>2016-01-05</created><authors><author><keyname>Nan</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Joseph</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Optimally Pruning Decision Tree Ensembles With Feature Cost</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning decision rules for prediction with
feature budget constraint. In particular, we are interested in pruning an
ensemble of decision trees to reduce expected feature cost while maintaining
high prediction accuracy for any test example. We propose a novel 0-1 integer
program formulation for ensemble pruning. Our pruning formulation is general -
it takes any ensemble of decision trees as input. By explicitly accounting for
feature-sharing across trees together with accuracy/cost trade-off, our method
is able to significantly reduce feature cost by pruning subtrees that introduce
more loss in terms of feature cost than benefit in terms of prediction accuracy
gain. Theoretically, we prove that a linear programming relaxation produces the
exact solution of the original integer program. This allows us to use efficient
convex optimization tools to obtain an optimally pruned ensemble for any given
budget. Empirically, we see that our pruning algorithm significantly improves
the performance of the state of the art ensemble method BudgetRF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00960</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00960</id><created>2016-01-05</created><authors><author><keyname>Zhan</keyname><forenames>Andong</forenames></author><author><keyname>Little</keyname><forenames>Max A.</forenames></author><author><keyname>Harris</keyname><forenames>Denzil A.</forenames></author><author><keyname>Abiola</keyname><forenames>Solomon O.</forenames></author><author><keyname>Dorsey</keyname><forenames>E. Ray</forenames></author><author><keyname>Saria</keyname><forenames>Suchi</forenames></author><author><keyname>Terzis</keyname><forenames>Andreas</forenames></author></authors><title>High Frequency Remote Monitoring of Parkinson's Disease via Smartphone:
  Platform Overview and Medication Response Detection</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: The aim of this study is to develop a smartphone-based
high-frequency remote monitoring platform, assess its feasibility for remote
monitoring of symptoms in Parkinson's disease, and demonstrate the value of
data collected using the platform by detecting dopaminergic medication
response. Methods: We have developed HopkinsPD, a novel smartphone-based
monitoring platform, which measures symptoms actively (i.e. data are collected
when a suite of tests is initiated by the individual at specific times during
the day), and passively (i.e. data are collected continuously in the
background). After data collection, we extract features to assess measures of
five key behaviors related to PD symptoms -- voice, balance, gait, dexterity,
and reaction time. A random forest classifier is used to discriminate
measurements taken after a dose of medication (treatment) versus before the
medication dose (baseline). Results: A worldwide study for remote PD monitoring
was established using HopkinsPD in July, 2014. This study used entirely remote,
online recruitment and installation, demonstrating highly cost-effective
scalability. In six months, 226 individuals (121 PD and 105 controls)
contributed over 46,000 hours of passive monitoring data and approximately
8,000 instances of structured tests of voice, balance, gait, reaction, and
dexterity. To the best of our knowledge, this is the first study to have
collected data at such a scale for remote PD monitoring. Moreover, we
demonstrate the initial ability to discriminate treatment from baseline with
71.0(+-0.4)% accuracy, which suggests medication response can be monitored
remotely via smartphone-based measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00978</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00978</id><created>2016-01-05</created><authors><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author><author><keyname>Lo</keyname><forenames>Henry Z.</forenames></author><author><keyname>Lu</keyname><forenames>Tingting</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author></authors><title>Crater Detection via Convolutional Neural Networks</title><categories>cs.CV</categories><comments>2 Pages. Submitted to 47th Lunar and Planetary Science Conference
  (LPSC 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Craters are among the most studied geomorphic features in the Solar System
because they yield important information about the past and present geological
processes and provide information about the relative ages of observed geologic
formations. We present a method for automatic crater detection using advanced
machine learning to deal with the large amount of satellite imagery collected.
The challenge of automatically detecting craters comes from their is complex
surface because their shape erodes over time to blend into the surface.
Bandeira provided a seminal dataset that embodied this challenge that is still
an unsolved pattern recognition problem to this day. There has been work to
solve this challenge based on extracting shape and contrast features and then
applying classification models on those features. The limiting factor in this
existing work is the use of hand crafted filters on the image such as Gabor or
Sobel filters or Haar features. These hand crafted methods rely on domain
knowledge to construct. We would like to learn the optimal filters and features
based on training examples. In order to dynamically learn filters and features
we look to Convolutional Neural Networks (CNNs) which have shown their
dominance in computer vision. The power of CNNs is that they can learn image
filters which generate features for high accuracy classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00987</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00987</id><created>2016-01-05</created><authors><author><keyname>Muldoon</keyname><forenames>Sarah Feldt</forenames></author><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>Gu</keyname><forenames>Shi</forenames></author><author><keyname>Cieslak</keyname><forenames>Matthew</forenames></author><author><keyname>Grafton</keyname><forenames>Scott T.</forenames></author><author><keyname>Vettel</keyname><forenames>Jean M.</forenames></author><author><keyname>Bassett</keyname><forenames>Danielle S.</forenames></author></authors><title>Stimulation-based control of dynamic brain networks</title><categories>q-bio.NC cs.SY</categories><comments>54 pages, 10 figures, includes Supplementary Information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to modulate brain states using targeted stimulation is
increasingly being employed to treat neurological disorders and to enhance
human performance. Despite the growing interest in brain stimulation as a form
of neuromodulation, much remains unknown about the network-level impact of
these focal perturbations. To study the system wide impact of regional
stimulation, we employ a data-driven computational model of nonlinear brain
dynamics to systematically explore the effects of targeted stimulation.
Validating predictions from network control theory, we uncover the relationship
between regional controllability and the focal versus global impact of
stimulation, and we relate these findings to differences in the underlying
network architecture. Finally, by mapping brain regions to cognitive systems,
we observe that the default mode system imparts large global change despite
being highly constrained by structural connectivity. This work forms an
important step towards the development of personalized stimulation protocols
for medical treatment or performance enhancement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.00998</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.00998</id><created>2016-01-05</created><authors><author><keyname>Robicquet</keyname><forenames>Alexandre</forenames></author><author><keyname>Alahi</keyname><forenames>Alexandre</forenames></author><author><keyname>Sadeghian</keyname><forenames>Amir</forenames></author><author><keyname>Anenberg</keyname><forenames>Bryan</forenames></author><author><keyname>Doherty</keyname><forenames>John</forenames></author><author><keyname>Wu</keyname><forenames>Eli</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>Forecasting Social Navigation in Crowded Complex Scenes</title><categories>cs.CV cs.RO cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When humans navigate a crowed space such as a university campus or the
sidewalks of a busy street, they follow common sense rules based on social
etiquette. In this paper, we argue that in order to enable the design of new
algorithms that can take fully advantage of these rules to better solve tasks
such as target tracking or trajectory forecasting, we need to have access to
better data in the first place. To that end, we contribute the very first large
scale dataset (to the best of our knowledge) that collects images and videos of
various types of targets (not just pedestrians, but also bikers, skateboarders,
cars, buses, golf carts) that navigate in a real-world outdoor environment such
as a university campus. We present an extensive evaluation where different
methods for trajectory forecasting are evaluated and compared. Moreover, we
present a new algorithm for trajectory prediction that exploits the complexity
of our new dataset and allows to: i) incorporate inter-class interactions into
trajectory prediction models (e.g, pedestrian vs bike) as opposed to just
intra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree
to which the social forces are regulating an interaction. We call the latter
&quot;social sensitivity&quot;and it captures the sensitivity to which a target is
responding to a certain interaction. An extensive experimental evaluation
demonstrates the effectiveness of our novel approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01001</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01001</id><created>2016-01-05</created><authors><author><keyname>Kaminski</keyname><forenames>Benjamin Lucien</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author><author><keyname>Matheja</keyname><forenames>Christoph</forenames></author><author><keyname>Olmedo</keyname><forenames>Federico</forenames></author></authors><title>Weakest Precondition Reasoning for Expected Run-Times of Probabilistic
  Programs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a wp-style calculus for obtaining bounds on the expected
run-time of probabilistic programs. Its application includes determining the
(possibly infinite) expected termination time of a probabilistic program and
proving positive almost-sure termination - does a program terminate with
probability one in finite expected time? We provide several proof rules for
bounding the run-time of loops, and prove the soundness of the approach with
respect to a simple operational model. We show that our approach is a
conservative extension of Nielson's approach for reasoning about the run-time
of deterministic programs. We analyze the expected run-time of some example
programs including a one-dimensional random walk and the coupon collector
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01006</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01006</id><created>2016-01-05</created><updated>2016-01-21</updated><authors><author><keyname>Han</keyname><forenames>Fei</forenames></author><author><keyname>Reily</keyname><forenames>Brian</forenames></author><author><keyname>Hoff</keyname><forenames>William</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author></authors><title>Space-Time Representation of People Based on 3D Skeletal Data: A Review</title><categories>cs.CV</categories><comments>If you notice typos or errors in this paper, or if your paper on
  skeleton-based representations is not included in the manuscript, please feel
  free to contact me. The update and errata information can be found at
  https://github.com/hanfeiid/SkeletonReview</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatiotemporal human representation based on 3D visual perception data is a
rapidly growing research area. Based on the information sources, these
representations can be broadly categorized into two groups based on RGB-D
information or 3D skeleton data. Recently, skeleton-based human representations
have been intensively studied and kept attracting an increasing attention, due
to their robustness to variations of viewpoint, human body scale and motion
speed as well as the realtime, online performance. This paper presents a
comprehensive survey of existing space-time representations of people based on
3D skeletal data, and provides an informative categorization and analysis of
these methods from the perspectives, including information modality,
representation encoding, structure and transition, and feature engineering. We
also provide a brief overview of skeleton acquisition devices and construction
methods, enlist a number of public benchmark datasets with skeleton data, and
discuss potential future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01008</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01008</id><created>2016-01-05</created><authors><author><keyname>Derakhshandeh</keyname><forenames>Zahra</forenames></author><author><keyname>Gmyr</keyname><forenames>Robert</forenames></author><author><keyname>Richa</keyname><forenames>Andrea W.</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author><author><keyname>Strothmann</keyname><forenames>Thim</forenames></author></authors><title>Universal Coating for Programmable Matter</title><categories>cs.ET cs.DC</categories><comments>32 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea behind universal coating is to have a thin layer of a specific
substance covering an object of any shape so that one can measure a certain
condition (like temperature or cracks) at any spot on the surface of the object
without requiring direct access to that spot. We study the universal coating
problem in the context of self-organizing programmable matter consisting of
simple computational elements, called particles, that can establish and release
bonds and can actively move in a self-organized way. Based on that matter, we
present a worst-case work-optimal universal coating algorithm that uniformly
coats any object of arbitrary shape and size that allows a uniform coating. Our
particles are anonymous, do not have any global information, have constant-size
memory, and utilize only local interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01019</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01019</id><created>2016-01-05</created><authors><author><keyname>Mohan</keyname><forenames>Shankar</forenames></author><author><keyname>Shia</keyname><forenames>Victor</forenames></author><author><keyname>Vasudevan</keyname><forenames>Ram</forenames></author></authors><title>Convex Computation of the Reachable Set for Hybrid Systems with
  Parametric Uncertainty</title><categories>math.OC cs.SY</categories><comments>25 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To verify the correct operation of systems, engineers need to determine the
set of configurations of a dynamical model that are able to safely reach a
specified configuration under a control law. Unfortunately, constructing models
for systems interacting in highly dynamic environments is difficult. This paper
addresses this challenge by presenting a convex optimization method to
efficiently compute the set of configurations of a polynomial hybrid dynamical
system that are able to safely reach a user defined target set despite
parametric uncertainty in the model. This class of models describes, for
example, legged robots moving over uncertain terrains. The presented approach
utilizes the notion of occupation measures to describe the evolution of
trajectories of a nonlinear hybrid dynamical system with parametric uncertainty
as a linear equation over measures whose supports coincide with the
trajectories under investigation. This linear equation with user defined
support constraints is approximated with vanishing conservatism using a
hierarchy of semidefinite programs that are each proven to compute an
inner/outer approximation to the set of initial conditions that can reach the
user defined target set safely in spite of uncertainty. The efficacy of this
method is illustrated on a collection of six representative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01038</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01038</id><created>2016-01-05</created><authors><author><keyname>van Hoeij</keyname><forenames>Mark</forenames></author><author><keyname>Monagan</keyname><forenames>Michael</forenames></author></authors><title>A Modular Algorithm for Computing Polynomial GCDs over Number Fields
  presented with Multiple Extensions</title><categories>cs.SC</categories><comments>36 pages. An early version of this paper appeared in the Proceedings
  of ISSAC 2002, ACM Press, pp. 207-213. This version represents work done
  between 2003 and 2005 outlined as contributions 3, 4 and 5 in the abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the monic gcd of two polynomials over a
number field L = Q(alpha_1,...,alpha_n). Langemyr and McCallum have already
shown how Brown's modular GCD algorithm for polynomials over Q can be modified
to work for Q(alpha) and subsequently, Langemyr extended the algorithm to L[x].
Encarnacion also showed how to use rational number to make the algorithm for
Q(alpha) output sensitive, that is, the number of primes used depends on the
size of the integers in the gcd and not on bounds based on the input
polynomials.
  Our first contribution is an extension of Encarnacion's modular GCD algorithm
to the case n&gt;1, which, like Encarnacion's algorithm, is is output sensitive.
  Our second contribution is a proof that it is not necessary to test if p
divides the discriminant. This simplifies the algorithm; it is correct without
this test.
  Our third contribution is a modification to the algorithm to treat the case
of reducible extensions. Such cases arise when solving systems of polynomial
equations.
  Our fourth contribution is an implementation of the modular GCD algorithm in
Maple and in Magma. Both implementations use a recursive dense polynomial data
structure for representing polynomials over number fields with multiple field
extensions.
  Our fifth contribution is a primitive fraction-free algorithm. This is the
best non-modular approach. We present timing comparisons of the Maple and Magma
implementations demonstrating various optimizations and comparing them with the
monic Euclidan algorithm and our primitive fraction-free algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01050</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01050</id><created>2016-01-05</created><authors><author><keyname>Bukatin</keyname><forenames>Michael</forenames></author><author><keyname>Matthews</keyname><forenames>Steve</forenames></author></authors><title>Dataflow Graphs as Matrices and Programming with Higher-order Matrix
  Elements</title><categories>cs.PL</categories><comments>8 pages, August 27, 2015 preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dataflow architecture for two classes of computations which admit
taking linear combinations of execution runs: probabilistic sampling and
generalized animation. We improve the earlier technique of almost continuous
program transformations by adopting a discipline of bipartite graphs linking
nodes obtained via general transformations and nodes obtained via linear
transformations which makes it possible to develop and evolve dataflow programs
over these classes of computations by continuous program transformations. The
use of bipartite graphs allows us to represent the dataflow programs from this
class as matrices of real numbers and evolve and modify programs by continuous
change of these numbers.
  We develop a formalism for higher-order dataflow programming for this class
of dataflow graphs based on the higher-order matrix elements. Some of our
software experiments are briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01054</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01054</id><created>2016-01-05</created><authors><author><keyname>Wright</keyname><forenames>Matthew</forenames></author><author><keyname>Gomes</keyname><forenames>Gabriel</forenames></author><author><keyname>Horowitz</keyname><forenames>Roberto</forenames></author><author><keyname>Kurzhanskiy</keyname><forenames>Alex A.</forenames></author></authors><title>On node and route choice models for high-dimensional road networks</title><categories>cs.SY</categories><comments>25 pages, 5 figures (one in color). arXiv admin note: substantial
  text overlap with arXiv:1509.04995</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses first order node models for macroscopic multi-commodity
traffic simulation. There are two parts: (1) input-output flow computation
based on input demand, input link priorities, split ratios that define how
incoming flow is distributed between output links, and output supply; and (2) a
traffic assignment algorithm for the case when split ratios are not known a
priori or are only partially known.
  Input link priorities define how output supply is distributed between
incoming flows. We deal with arbitrary input link priorities. Also, in this
node model we specifically address the issue of the First-In-First-Out (FIFO)
rule: in the case of one of the output links being overly congested the FIFO
rule may be too restrictive, blocking the flow to other outputs and causing
unreasonable spillback. We relax the FIFO rule through a parametrization that
describes how output links influence each other and allows tuning the model to
anything between full FIFO and no FIFO.
  In the existing research the prevailing approach to local traffic assignment
is to use a mode choice (logit or probit) model. In this paper we discuss the
shortcomings of the logit model and introduce a nonparametric route choice
algorithm that produces split ratios based only on link demands and supplies.
  All methods in this paper are presented for multi-commodity traffic flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01058</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01058</id><created>2016-01-05</created><updated>2016-01-08</updated><authors><author><keyname>Katz</keyname><forenames>Gilad</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author></authors><title>Wikiometrics: A Wikipedia Based Ranking System</title><categories>cs.DL cs.AI cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new concept - Wikiometrics - the derivation of metrics and
indicators from Wikipedia. Wikipedia provides an accurate representation of the
real world due to its size, structure, editing policy and popularity. We
demonstrate an innovative mining methodology, where different elements of
Wikipedia - content, structure, editorial actions and reader reviews - are used
to rank items in a manner which is by no means inferior to rankings produced by
experts or other methods. We test our proposed method by applying it to two
real-world ranking problems: top world universities and academic journals. Our
proposed ranking methods were compared to leading and widely accepted
benchmarks, and were found to be extremely correlative but with the advantage
of the data being publically available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01060</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01060</id><created>2016-01-05</created><authors><author><keyname>Cao</keyname><forenames>Xiangyong</forenames></author><author><keyname>Zhao</keyname><forenames>Qian</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Low-rank Matrix Factorization under General Mixture Noise Distributions</title><categories>cs.CV</categories><comments>13 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision problems can be posed as learning a low-dimensional
subspace from high dimensional data. The low rank matrix factorization (LRMF)
represents a commonly utilized subspace learning strategy. Most of the current
LRMF techniques are constructed on the optimization problems using L1-norm and
L2-norm losses, which mainly deal with Laplacian and Gaussian noises,
respectively. To make LRMF capable of adapting more complex noise, this paper
proposes a new LRMF model by assuming noise as Mixture of Exponential Power
(MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combining
the penalized likelihood method with MoEP distributions. Such setting
facilitates the learned LRMF model capable of automatically fitting the real
noise through MoEP distributions. Each component in this mixture is adapted
from a series of preliminary super- or sub-Gaussian candidates. Moreover, by
facilitating the local continuity of noise components, we embed Markov random
field into the PMoEP model and further propose the advanced PMoEP-MRF model. An
Expectation Maximization (EM) algorithm and a variational EM (VEM) algorithm
are also designed to infer the parameters involved in the proposed PMoEP and
the PMoEP-MRF model, respectively. The superseniority of our methods is
demonstrated by extensive experiments on synthetic data, face modeling,
hyperspectral image restoration and background subtraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01067</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01067</id><created>2016-01-05</created><authors><author><keyname>Jing</keyname><forenames>Rui-Juan</forenames></author><author><keyname>Yuan</keyname><forenames>Chun-Ming</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author></authors><title>A Polynomial-time Algorithm to Compute Generalized Hermite Normal Form
  of Matrices over Z[x]</title><categories>cs.SC cs.CC</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a polynomial-time algorithm is given to compute the
generalized Hermite normal form for a matrix F over Z[x], or equivalently, the
reduced Groebner basis of the Z[x]-module generated by the column vectors of F.
The algorithm is also shown to be practically more efficient than existing
algorithms. The algorithm is based on three key ingredients. First, an F4 style
algorithm to compute the Groebner basis is adopted, where a novel prolongation
is designed such that the coefficient matrices under consideration have
polynomial sizes. Second, fast algorithms to compute Hermite normal forms of
matrices over Z are used. Third, the complexity of the algorithm are guaranteed
by a nice estimation for the degree and height bounds of the polynomials in the
generalized Hermite normal form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01069</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01069</id><created>2016-01-05</created><authors><author><keyname>Yang</keyname><forenames>Yue</forenames></author><author><keyname>Yin</keyname><forenames>Yanling</forenames></author><author><keyname>Hu</keyname><forenames>Zixia</forenames></author></authors><title>MAC Protocols Design for Smart Metering Network</title><categories>cs.NI</categories><journal-ref>Automation, Control and Intelligent Systems, 2015; 3(5): 87-94</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new generation of power metering system - i.e. Advanced Metering
Infrastructure (AMI) - is expected to enable remote reading, control, demand
response and other advanced functions, based on the integration of a new
two-way communication network, which will be referred as Smart Metering Network
(SMN). In this paper, we focus on the design principles of multiple access
control (MAC) protocols for SMN. First, we list several AMI applications and
its benefits to the current power grid and user experience. Next, we introduces
several features of SMN relevant to the design choice of the MAC protocols,
including the SMN architecture and candidate communication technologies. After
that, we propose some performance evaluation metrics, such as scalability
issue, traffic types, delay and etc, and give a survey of the associated
research issues for the SMN MAC protocols design. In addition, we also note
progress within the new IEEE standardization task group (IEEE 802.11ah TG)
currently working to create SMN standards, especially in the MAC protocols
aspect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01070</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01070</id><created>2016-01-05</created><updated>2016-01-07</updated><authors><author><keyname>Dai</keyname><forenames>Binbin</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Energy Efficiency of Downlink Transmission Strategies for Cloud Radio
  Access Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures, accepted by JSAC Energy-Efficient Techniques for
  5G Wireless Communication Systems special issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the energy efficiency of the cloud radio access network
(C-RAN), specifically focusing on two fundamental and different downlink
transmission strategies, namely the data-sharing strategy and the compression
strategy. In the data-sharing strategy, the backhaul links connecting the
central processor (CP) and the base-stations (BSs) are used to carry user
messages -- each user's messages are sent to multiple BSs; the BSs locally form
the beamforming vectors then cooperatively transmit the messages to the user.
In the compression strategy, the user messages are precoded centrally at the
CP, which forwards a compressed version of the analog beamformed signals to the
BSs for cooperative transmission. This paper compares the energy efficiencies
of the two strategies by formulating an optimization problem of minimizing the
total network power consumption subject to user target rate constraints, where
the total network power includes the BS transmission power, BS activation
power, and load-dependent backhaul power. To tackle the discrete and nonconvex
nature of the optimization problems, we utilize the techniques of reweighted
$\ell_1$ minimization and successive convex approximation to devise provably
convergent algorithms. Our main finding is that both the optimized data-sharing
and compression strategies in C-RAN achieve much higher energy efficiency as
compared to the non-optimized coordinated multi-point transmission, but their
comparative effectiveness in energy saving depends on the user target rate. At
low user target rate, data-sharing consumes less total power than compression,
however, as the user target rate increases, the backhaul power consumption for
data-sharing increases significantly leading to better energy efficiency of
compression at the high user rate regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01073</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01073</id><created>2016-01-05</created><authors><author><keyname>Firat</keyname><forenames>Orhan</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Multi-Way, Multilingual Neural Machine Translation with a Shared
  Attention Mechanism</title><categories>cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose multi-way, multilingual neural machine translation. The proposed
approach enables a single neural translation model to translate between
multiple languages, with a number of parameters that grows only linearly with
the number of languages. This is made possible by having a single attention
mechanism that is shared across all language pairs. We train the proposed
multi-way, multilingual model on ten language pairs from WMT'15 simultaneously
and observe clear performance improvements over models trained on only one
language pair. In particular, we observe that the proposed model significantly
improves the translation quality of low-resource language pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01074</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01074</id><created>2016-01-05</created><updated>2016-03-04</updated><authors><author><keyname>Obuchi</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Sparse approximation problem: how rapid simulated annealing succeeds and
  fails</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>12 pages, 7 figures, a proceedings of HD^3-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information processing techniques based on sparseness have been actively
studied in several disciplines. Among them, a mathematical framework to
approximately express a given dataset by a combination of a small number of
basis vectors of an overcomplete basis is termed the {\em sparse
approximation}. In this paper, we apply simulated annealing, a metaheuristic
algorithm for general optimization problems, to sparse approximation in the
situation where the given data have a planted sparse representation and noise
is present. The result in the noiseless case shows that our simulated annealing
works well in a reasonable parameter region: the planted solution is found
fairly rapidly. This is true even in the case where a common relaxation of the
sparse approximation problem, the $\ell_1$-relaxation, is ineffective. On the
other hand, when the dimensionality of the data is close to the number of
non-zero components, another metastable state emerges, and our algorithm fails
to find the planted solution. This phenomenon is associated with a first-order
phase transition. In the case of very strong noise, it is no longer meaningful
to search for the planted solution. In this situation, our algorithm determines
a solution with close-to-minimum distortion fairly quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01079</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01079</id><created>2016-01-06</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Liu</keyname><forenames>Lihua</forenames></author></authors><title>A Note on &quot;Confidentiality-Preserving Image Search: A Comparative Study
  Between Homomorphic Encryption and Distance-Preserving Randomization&quot;</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Lu et al. have proposed two image search schemes based on additive
homomorphic encryption [IEEE Access, 2 (2014), 125-141]. We remark that both
two schemes are flawed because: (1) the first scheme does not make use of the
additive homomorphic property at all; (2) the additive homomorphic encryption
in the second scheme is unnecessary and can be replaced by a more efficient
symmetric key encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01083</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01083</id><created>2016-01-06</created><authors><author><keyname>Phien</keyname><forenames>Ho N.</forenames></author><author><keyname>Tuan</keyname><forenames>Hoang D.</forenames></author><author><keyname>Bengua</keyname><forenames>Johann A.</forenames></author><author><keyname>Do</keyname><forenames>Minh N.</forenames></author></authors><title>Efficient tensor completion: Low-rank tensor train</title><categories>cs.NA</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel formulation of the tensor completion problem to
impute missing entries of data represented by tensors. The formulation is
introduced in terms of tensor train (TT) rank which can effectively capture
global information of tensors thanks to its construction by a well-balanced
matricization scheme. Two algorithms are proposed to solve the corresponding
tensor completion problem. The first one called simple low-rank tensor
completion via tensor train (SiLRTC-TT) is intimately related to minimizing the
TT nuclear norm. The second one is based on a multilinear matrix factorization
model to approximate the TT rank of the tensor and called tensor completion by
parallel matrix factorization via tensor train (TMac-TT). These algorithms are
applied to complete both synthetic and real world data tensors. Simulation
results of synthetic data show that the proposed algorithms are efficient in
estimating missing entries for tensors with either low Tucker rank or TT rank
while Tucker-based algorithms are only comparable in the case of low Tucker
rank tensors. When applied to recover color images represented by ninth-order
tensors augmented from third-order ones, the proposed algorithms outperforms
the Tucker-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01085</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01085</id><created>2016-01-06</created><authors><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Hoang</keyname><forenames>Cong Duy Vu</forenames></author><author><keyname>Vymolova</keyname><forenames>Ekaterina</forenames></author><author><keyname>Yao</keyname><forenames>Kaisheng</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Haffari</keyname><forenames>Gholamreza</forenames></author></authors><title>Incorporating Structural Alignment Biases into an Attentional Neural
  Translation Model</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural encoder-decoder models of machine translation have achieved impressive
results, rivalling traditional translation models. However their modelling
formulation is overly simplistic, and omits several key inductive biases built
into traditional models. In this paper we extend the attentional neural
translation model to include structural biases from word based alignment
models, including positional bias, Markov conditioning, fertility and agreement
over translation directions. We show improvements over a baseline attentional
model and standard phrase-based model over several language pairs, evaluating
on difficult languages in a low resource setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01087</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01087</id><created>2016-01-06</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Xiang</keyname><forenames>Hongyu</forenames></author><author><keyname>Cheng</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Yan</keyname><forenames>Shi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Inter-tier Interference Suppression in Heterogeneous Cloud Radio Access
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating cloud computing into heterogeneous networks, the heterogeneous
cloud radio access network (H-CRAN) has been proposed as a promising paradigm
to enhance both spectral and energy efficiencies. Developing interference
suppression strategies is critical for suppressing the inter-tier interference
between remote radio heads (RRHs) and a macro base station (MBS) in H-CRANs. In
this paper, inter-tier interference suppression techniques are considered in
the contexts of collaborative processing and cooperative radio resource
allocation (CRRA). In particular, interference collaboration (IC) and
beamforming (BF) are proposed to suppress the inter-tier interference, and
their corresponding performance is evaluated. Closed-form expressions for the
overall outage probabilities, system capacities, and average bit error rates
under these two schemes are derived. Furthermore, IC and BF based CRRA
optimization models are presented to maximize the RRH-accessed users' sum rates
via power allocation, which is solved with convex optimization. Simulation
results demonstrate that the derived expressions for these performance metrics
for IC and BF are accurate; and the relative performance between IC and BF
schemes depends on system parameters, such as the number of antennas at the
MBS, the number of RRHs, and the target signal-to-interference-plus-noise ratio
threshold. Furthermore, it is seen that the sum rates of IC and BF schemes
increase almost linearly with the transmit power threshold under the proposed
CRRA optimization solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01089</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01089</id><created>2016-01-06</created><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>On the Scaling Exponent of Polar Codes for Binary-Input
  Energy-Harvesting Channels</title><categories>cs.IT math.IT</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the scaling exponent of polar codes for binary-input
energy-harvesting (EH) channels with infinite-capacity batteries. The EH
process is characterized by a sequence of i.i.d. random variables with finite
variances. The scaling exponent $\mu$ of polar codes for a binary-input
memoryless channel (BMC) $q_{Y|X}$ with capacity $I(q_{Y|X})$ characterizes the
closest gap between the capacity and non-asymptotic achievable rates in the
following way: For a fixed average error probability $\varepsilon \in (0, 1)$,
the closest gap between the capacity $I(q_{Y|X})$ and a non-asymptotic
achievable rate $R_n$ for a length-$n$ polar code scales as $n^{-1/\mu}$, i.e.,
$\min\{|I(q_{Y|X})-R_n|\} = \Theta(n^{-1/\mu})$. It has been shown that the
scaling exponent $\mu$ for any binary-input memoryless symmetric channel (BMSC)
with $I(q_{Y|X})\in(0,1)$ lies between 3.579 and 4.714, where the upper bound
$4.714$ was shown by an explicit construction of polar codes. Our main result
shows that $4.714$ remains to be a valid upper bound on the scaling exponent
for any binary-input EH channel, i.e., a BMC subject to additional EH
constraints. Our result thus implies that the EH constraints do not worsen the
rate of convergence to capacity if polar codes are employed. The main result is
proved by leveraging the following three existing results: scaling exponent
analyses for BMSCs, construction of polar codes designed for binary-input
memoryless asymmetric channels, and the save-and-transmit strategy for EH
channels. An auxiliary contribution of this paper is that the upper bound on
$\mu$ holds for binary-input memoryless asymmetric channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01090</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01090</id><created>2016-01-06</created><updated>2016-01-07</updated><authors><author><keyname>Yazdanshenasan</keyname><forenames>Zeinab</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Afshang</keyname><forenames>Mehrnaz</forenames></author><author><keyname>Chong</keyname><forenames>Peter Han Joo</forenames></author></authors><title>Poisson Hole Process: Theory and Applications to Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages. Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference field in wireless networks is often modeled by a homogeneous
Poisson Point Process (PPP). While it is realistic in modeling the inherent
node irregularity and provides meaningful first-order results, it falls short
in modeling the effect of interference management techniques, which typically
introduce some form of spatial interaction among active transmitters. In some
applications, such as cognitive radio and device-to-device networks, this
interaction may result in the formation of holes in an otherwise homogeneous
interference field. The resulting interference field can be accurately modeled
as a Poisson Hole Process (PHP). Despite the importance of PHP in many
applications, the exact characterization of interference experienced by a
typical node in a PHP is not known. In this paper, we derive several tight
upper and lower bounds on the Laplace transform of this interference. Numerical
comparisons reveal that the new bounds outperform all known bounds and
approximations, and are remarkably tight in all operational regimes of
interest. The key in deriving these tight and yet simple bounds is to capture
the local neighborhood around the typical point accurately while simplifying
the far field to attain tractability. Ideas for tightening these bounds further
by incorporating the effect of overlaps in the holes are also discussed. These
results immediately lead to an accurate characterization of the coverage
probability of the typical node in a PHP under Rayleigh fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01092</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01092</id><created>2016-01-06</created><authors><author><keyname>Bose</keyname><forenames>Joy</forenames></author><author><keyname>Singhai</keyname><forenames>Amit</forenames></author><author><keyname>Patankar</keyname><forenames>Anish</forenames></author><author><keyname>Kumar</keyname><forenames>Ankit</forenames></author></authors><title>Attention Sensitive Web Browsing</title><categories>cs.HC</categories><comments>5 pages, 7 figures</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With a number of cheap commercial dry EEG kits available today, it is
possible to look at user attention driven scenarios for interaction with the
web browser. Using EEG to determine the user's attention level is preferable to
using methods such as gaze tracking or time spent on the webpage. In this paper
we use the attention level in three different ways. First, as a control
mechanism, to control user interface elements such as menus or buttons. Second,
to make the web browser responsive to the current attention level. Third, as a
means for the web developer to control the user experience based on the level
of attention paid by the user, thus creating attention sensitive websites. We
present implementation details for each of these, using the NeuroSky MindWave
sensor. We also explore issues in the system, and possibility of an EEG based
web standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01100</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01100</id><created>2016-01-06</created><authors><author><keyname>Qiang</keyname><forenames>Guo</forenames></author><author><keyname>Dan</keyname><forenames>Tu</forenames></author><author><keyname>Guohui</keyname><forenames>Li</forenames></author><author><keyname>Jun</keyname><forenames>Lei</forenames></author></authors><title>Memory Matters: Convolutional Recurrent Neural Network for Scene Text
  Recognition</title><categories>cs.CV</categories><comments>6 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text recognition in natural scene is a challenging problem due to the many
factors affecting text appearance. In this paper, we presents a method that
directly transcribes scene text images to text without needing of sophisticated
character segmentation. We leverage recent advances of deep neural networks to
model the appearance of scene text images with temporal dynamics. Specifically,
we integrates convolutional neural network (CNN) and recurrent neural network
(RNN) which is motivated by observing the complementary modeling capabilities
of the two models. The main contribution of this work is investigating how
temporal memory helps in an segmentation free fashion for this specific
problem. By using long short-term memory (LSTM) blocks as hidden units, our
model can retain long-term memory compared with HMMs which only maintain
short-term state dependences. We conduct experiments on Street View House
Number dataset containing highly variable number images. The results
demonstrate the superiority of the proposed method over traditional HMM based
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01102</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01102</id><created>2016-01-06</created><updated>2016-02-17</updated><authors><author><keyname>Yu</keyname><forenames>Rose</forenames></author><author><keyname>Qiu</keyname><forenames>Huida</forenames></author><author><keyname>Wen</keyname><forenames>Zhen</forenames></author><author><keyname>Lin</keyname><forenames>Ching-Yung</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>A Survey on Social Media Anomaly Detection</title><categories>cs.LG cs.SI</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media anomaly detection is of critical importance to prevent malicious
activities such as bullying, terrorist attack planning, and fraud information
dissemination. With the recent popularity of social media, new types of
anomalous behaviors arise, causing concerns from various parties. While a large
amount of work have been dedicated to traditional anomaly detection problems,
we observe a surge of research interests in the new realm of social media
anomaly detection. In this paper, we present a survey on existing approaches to
address this problem. We focus on the new type of anomalous phenomena in the
social media and review the recent developed techniques to detect those special
types of anomalies. We provide a general overview of the problem domain, common
formulations, existing methodologies and potential directions. With this work,
we hope to call out the attention from the research community on this
challenging problem and open up new directions that we can contribute in the
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01104</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01104</id><created>2016-01-06</created><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames><affiliation>LIAFA, GANG</affiliation></author><author><keyname>Korman</keyname><forenames>Amos</forenames><affiliation>LIAFA, GANG</affiliation></author><author><keyname>Kutten</keyname><forenames>Shay</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author><author><keyname>Yuval</keyname><forenames>Emek</forenames></author></authors><title>Notions of Connectivity in Overlay Networks</title><categories>cs.DC</categories><comments>Structural Information and Communication Complexity - 19th
  International Colloquium, 2012, Jun 2012, Reykjavik, Iceland. 2015</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-31104-8_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot; How well connected is the network? &quot; This is one of the most fundamental
questions one would ask when facing the challenge of designing a communication
network. Three major notions of connectivity have been considered in the
literature, but in the context of traditional (single-layer) networks, they
turn out to be equivalent. This paper introduces a model for studying the three
notions of connectivity in multi-layer networks. Using this model, it is easy
to demonstrate that in multi-layer networks the three notions may differ
dramatically. Unfortunately, in contrast to the single-layer case, where the
values of the three connectivity notions can be computed efficiently, it has
been recently shown in the context of WDM networks (results that can be easily
translated to our model) that the values of two of these notions of
connectivity are hard to compute or even approximate in multi-layer networks.
The current paper shed some positive light into the multi-layer connectivity
topic: we show that the value of the third connectivity notion can be computed
in polynomial time and develop an approximation for the construction of well
connected overlay networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01113</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01113</id><created>2016-01-06</created><authors><author><keyname>Balco</keyname><forenames>Samuel</forenames></author><author><keyname>Frittella</keyname><forenames>Sabine</forenames></author><author><keyname>Greco</keyname><forenames>Giuseppe</forenames></author><author><keyname>Kurz</keyname><forenames>Alexander</forenames></author><author><keyname>Palmigiano</keyname><forenames>Alessandra</forenames></author></authors><title>Tool support for reasoning in display calculi</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a tool for reasoning in and about propositional sequent calculi.
One aim is to support reasoning in calculi that contain a hundred rules or
more, so that even relatively small pen and paper derivations become tedious
and error prone. As an example, we implement the display calculus D.EAK of
dynamic epistemic logic. Second, we provide embeddings of the calculus in the
theorem prover Isabelle for formalising proofs about D.EAK. As a case study we
show that the solution of the muddy children puzzle is derivable for any number
of muddy children. Third, there is a set of meta-tools, that allows us to adapt
the tool for a wide variety of user defined calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01116</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01116</id><created>2016-01-06</created><authors><author><keyname>Csoma</keyname><forenames>Attila</forenames></author><author><keyname>Guly&#xe1;s</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Toka</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>On Measuring the Geographic Diversity of Internet Routes</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Route diversity in networks is elemental for establishing reliable,
high-capacity connections with appropriate security between endpoints. As for
the Internet, route diversity has already been studied at both Autonomous
System- and router-level topologies by means of graph theoretical disjoint
paths. In this paper we complement these approaches by proposing a method for
measuring the diversity of Internet paths in a geographical sense. By
leveraging the recent developments in IP geolocation we show how to map the
paths discovered by traceroute into geographically equivalent classes. This
allows us to identify the geographical footprints of the major transmission
paths between end-hosts, and building on our observations, we propose a
quantitative measure for geographical diversity of Internet routes between any
two hosts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01118</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01118</id><created>2016-01-06</created><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Uniform-Circuit and Logarithmic-Space Approximations of Refined
  Combinatorial Optimization Problems</title><categories>cs.CC cs.DM</categories><comments>(37 pages, A4, 10pt, 1 figure) This is a complete version of a
  preliminary report, which appeared in the Proceedings of the 7th
  International Conference on Combinatorial Optimization and Applications
  (COCOA 2013), Chengdu, China, December 12--14, 2013, Lecture Notes in
  Computer Science, Springer-Verlag, vol.8287, pp.318--329, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant progress has been made in the past three decades over the study
of combinatorial NP optimization problems and their associated optimization and
approximate classes, such as NPO, PO, APX (or APXP), and PTAS. Unfortunately, a
collection of problems that are simply placed inside the P-solvable
optimization class PO never have been studiously analyzed regarding their exact
computational complexity. To improve this situation, the existing framework
based on polynomial-time computability needs to be expanded and further refined
for an insightful analysis of various approximation algorithms targeting
optimization problems within PO. In particular, we deal with those problems
characterized in terms of logarithmic-space computations and uniform-circuit
computations. We are focused on nondeterministic logarithmic-space (NL)
optimization problems or NPO problems. Our study covers a wide range of
optimization and approximation classes, dubbed as, NLO, LO, APXL, and LSAS as
well as new classes NC1O, APXNC1, NC1AS, and AC0O, which are founded on uniform
families of Boolean circuits. Although many NL decision problems can be
naturally converted into NL optimization (NLO) problems, few NLO problems have
been studied vigorously. We thus provide a number of new NLO problems falling
into those low-complexity classes. With the help of NC1 or AC0
approximation-preserving reductions, we also identify the most difficult
problems (known as complete problems) inside those classes. Finally, we
demonstrate a number of collapses and separations among those refined
optimization and approximation classes with or without unproven
complexity-theoretical assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01121</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01121</id><created>2016-01-06</created><authors><author><keyname>Kopinski</keyname><forenames>Thomas</forenames><affiliation>ENSTA ParisTech U2IS/RV</affiliation></author><author><keyname>Magand</keyname><forenames>St&#xe9;phane</forenames><affiliation>ENSTA ParisTech U2IS/RV</affiliation></author><author><keyname>Handmann</keyname><forenames>Uwe</forenames><affiliation>Flowers, ENSTA ParisTech U2IS/RV</affiliation></author><author><keyname>Gepperth</keyname><forenames>Alexander</forenames><affiliation>Flowers, ENSTA ParisTech U2IS/RV</affiliation></author></authors><title>A pragmatic approach to multi-class classification</title><categories>cs.LG</categories><comments>European Symposium on artificial neural networks (ESANN), Apr 2015,
  Bruges, Belgium. 2015</comments><proxy>ccsd</proxy><doi>10.1109/IJCNN.2015.7280768</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel hierarchical approach to multi-class classification which
is generic in that it can be applied to different classification models (e.g.,
support vector machines, perceptrons), and makes no explicit assumptions about
the probabilistic structure of the problem as it is usually done in multi-class
classification. By adding a cascade of additional classifiers, each of which
receives the previous classifier's output in addition to regular input data,
the approach harnesses unused information that manifests itself in the form of,
e.g., correlations between predicted classes. Using multilayer perceptrons as a
classification model, we demonstrate the validity of this approach by testing
it on a complex ten-class 3D gesture recognition task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01142</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01142</id><created>2016-01-06</created><authors><author><keyname>Gao</keyname><forenames>Yang</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Streaming Gibbs Sampling for LDA Model</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming variational Bayes (SVB) is successful in learning LDA models in an
online manner. However previous attempts toward developing online Monte-Carlo
methods for LDA have little success, often by having much worse perplexity than
their batch counterparts. We present a streaming Gibbs sampling (SGS) method,
an online extension of the collapsed Gibbs sampling (CGS). Our empirical study
shows that SGS can reach similar perplexity as CGS, much better than SVB. Our
distributed version of SGS, DSGS, is much more scalable than SVB mainly because
the updates' communication complexity is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01145</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01145</id><created>2016-01-06</created><authors><author><keyname>Zhou</keyname><forenames>Yiren</forenames></author><author><keyname>Cheung</keyname><forenames>Ngai-Man</forenames></author></authors><title>Vehicle Classification using Transferable Deep Neural Network Features</title><categories>cs.CV</categories><comments>4 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address vehicle detection on rear view vehicle images captured from a
distance along multi-lane highways, and vehicle classification using
transferable features from Deep Neural Network. We address the following
problems that are specific to our application: how to utilize dash lane
markings to assist vehicle detection, what features are useful for
classification on vehicle categories, and how to utilize Deep Neural Network
when the size of the labelled data is limited. Experiment results suggest our
approach outperforms other state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01152</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01152</id><created>2016-01-06</created><authors><author><keyname>Katz</keyname><forenames>Gil</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Distributed Binary Detection with Lossy Data Compression</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem where a statistician in a two-node system receives
rate-limited information from a transmitter about marginal observations of a
memoryless process generated from two possible distributions. Using its own
observations, this receiver is required to first identify the legitimacy of its
sender by declaring the joint distribution of the process, and then depending
on such authentication it generates the adequate reconstruction of the
observations satisfying an average per-letter distortion. The performance of
this setup is investigated through the corresponding rate-error-distortion
region describing the trade-off between: the communication rate, the error
exponent induced by the detection and the distortion incurred by the source
reconstruction. In the special case of testing against independence, where the
alternative hypothesis implies that the sources are independent, the optimal
rate-error-distortion region is characterized. An application example to binary
symmetric sources is given subsequently and the explicit expression for the
rate-error-distortion region is provided as well. The case of &quot;general
hypotheses&quot; is also investigated. A new achievable rate-error-distortion region
is derived based on the use of non-asymptotic binning, improving the quality of
communicated descriptions. While it is shown that the error exponent is further
improved through the introduction of a new approach by which testing is
performed on &quot;bins of sequences&quot; rather than decoding single sequences and then
testing. Benefits of the proposed methods were demonstrated through numerical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01157</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01157</id><created>2016-01-06</created><authors><author><keyname>Kopinski</keyname><forenames>Thomas</forenames><affiliation>ENSTA ParisTech U2IS/RV, Flowers</affiliation></author><author><keyname>Gepperth</keyname><forenames>Alexander</forenames><affiliation>ENSTA ParisTech U2IS/RV, Flowers</affiliation></author><author><keyname>Handmann</keyname><forenames>Uwe</forenames></author></authors><title>A simple technique for improving multi-class classification with neural
  networks</title><categories>cs.LG</categories><comments>European Symposium on artificial neural networks (ESANN), Jun 2015,
  Bruges, Belgium</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method to perform multi-class pattern classification with
neural networks and test it on a challenging 3D hand gesture recognition
problem. Our method consists of a standard one-against-all (OAA)
classification, followed by another network layer classifying the resulting
class scores, possibly augmented by the original raw input vector. This allows
the network to disambiguate hard-to-separate classes as the distribution of
class scores carries considerable information as well, and is in fact often
used for assessing the confidence of a decision. We show that by this approach
we are able to significantly boost our results, overall as well as for
particular difficult cases, on the hard 10-class gesture classification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01166</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01166</id><created>2016-01-06</created><authors><author><keyname>Kumar</keyname><forenames>Bhupendra</forenames></author><author><keyname>Prakriya</keyname><forenames>Shankar</forenames></author></authors><title>Rate Performance of Adaptive Link Selection in Buffer-Aided Cognitive
  Relay Networks</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE INFOCOM Workshop for
  possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance of a two-hop cognitive relay network with a
buffered decode and forward (DF) relay. We derive expressions for the rate
performance of an adaptive link selection-based buffered relay (ALSBR) scheme
with peak power and peak interference constraints on the secondary nodes, and
compare its performance with that of conventional unbuffered relay (CUBR) and
conventional buffered relay (CBR) schemes. Use of buffered relays with adaptive
link selection is shown to be particularly advantageous in underlay cognitive
radio networks. The insights developed are of significance to system designers
since cognitive radio frameworks are being explored for use in 5G systems.
Computer simulation results are presented to demonstrate accuracy of the
derived expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01168</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01168</id><created>2016-01-06</created><authors><author><keyname>Brough</keyname><forenames>Tara</forenames></author><author><keyname>Cain</keyname><forenames>Alan J.</forenames></author></authors><title>Automaton semigroups: new construction results and examples of
  non-automaton semigroups</title><categories>math.GR cs.FL</categories><comments>27 pages, 6 figures</comments><msc-class>20M35, 68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the class of automaton semigroups from two perspectives:
closure under constructions, and examples of semigroups that are not automaton
semigroups. We prove that (semigroup) free products of finite semigroups always
arise as automaton semigroups, and that the class of automaton monoids is
closed under forming wreath products with finite monoids. We also consider
closure under certain kinds of Rees matrix constructions, strong semilattices,
and small extensions. Finally, we prove that no subsemigroup of $(\mathbb{N},
+)$ arises as an automaton semigroup. (Previously, $(\mathbb{N},+)$ itself was
the unique example of a finitely generated residually finite semigroup that was
known not to arise as an automaton semigroup.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01183</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01183</id><created>2016-01-06</created><authors><author><keyname>Zheng</keyname><forenames>Tong-Xing</forenames></author><author><keyname>Wang</keyname><forenames>Hui-Ming</forenames></author></authors><title>Optimal Power Allocation for Artificial Noise under Imperfect CSI
  against Spatially Random Eavesdroppers</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures</comments><doi>10.1109/TVT.2015.2513003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence, we study the secure multiantenna transmission with
artificial noise (AN) under imperfect channel state information in the presence
of spatially randomly distributed eavesdroppers. We derive the optimal
solutions of the power allocation between the information signal and the AN for
minimizing the secrecy outage probability (SOP) under a target secrecy rate and
for maximizing the secrecy rate under a SOP constraint, respectively. Moreover,
we provide an interesting insight that channel estimation error affects the
optimal power allocation strategy in opposite ways for the above two
objectives. When the estimation error increases, more power should be allocated
to the information signal if we aim to decrease the rate-constrained SOP,
whereas more power should be allocated to the AN if we aim to increase the
SOP-constrained secrecy rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01191</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01191</id><created>2016-01-06</created><authors><author><keyname>Huynh</keyname><forenames>The Dang</forenames><affiliation>LINCS</affiliation></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames><affiliation>LINCS</affiliation></author><author><keyname>Viennot</keyname><forenames>Laurent</forenames><affiliation>GANG, LINCS</affiliation></author></authors><title>LiveRank: How to Refresh Old Datasets</title><categories>cs.SI</categories><proxy>ccsd</proxy><doi>10.1080/15427951.2015.1098756</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of refreshing a dataset. More precisely ,
given a collection of nodes gathered at some time (Web pages, users from an
online social network) along with some structure (hyperlinks, social
relationships), we want to identify a significant fraction of the nodes that
still exist at present time. The liveness of an old node can be tested through
an online query at present time. We call LiveRank a ranking of the old pages so
that active nodes are more likely to appear first. The quality of a LiveRank is
measured by the number of queries necessary to identify a given fraction of the
active nodes when using the LiveRank order. We study different scenarios from a
static setting where the Liv-eRank is computed before any query is made, to
dynamic settings where the LiveRank can be updated as queries are processed.
Our results show that building on the PageRank can lead to efficient LiveRanks,
for Web graphs as well as for online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01195</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01195</id><created>2016-01-06</created><authors><author><keyname>Sarkar</keyname><forenames>Kamal</forenames></author></authors><title>Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON
  2015</title><categories>cs.CL</categories><comments>NLP Tool Contest on &quot;POS Tagging For Code-mixed Indian Social Media
  Text&quot; held in conjunction with International Conference on Natural Language
  Processing(ICON 2015). arXiv admin note: text overlap with arXiv:1512.03950,
  arXiv:1405.7397</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the experiments carried out by us at Jadavpur University
as part of the participation in ICON 2015 task: POS Tagging for Code-mixed
Indian Social Media Text. The tool that we have developed for the task is based
on Trigram Hidden Markov Model that utilizes information from dictionary as
well as some other word level features to enhance the observation probabilities
of the known tokens as well as unknown tokens. We submitted runs for
Bengali-English, Hindi-English and Tamil-English Language pairs. Our system has
been trained and tested on the datasets released for ICON 2015 shared task: POS
Tagging For Code-mixed Indian Social Media Text. In constrained mode, our
system obtains average overall accuracy (averaged over all three language
pairs) of 75.60% which is very close to other participating two systems (76.79%
for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In
unconstrained mode, our system obtains average overall accuracy of 70.65% which
is also close to the system (72.85% for AMRITA_CEN) which obtains the highest
average overall accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01197</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01197</id><created>2016-01-06</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Three-coloring triangle-free graphs on surfaces VII. A linear-time
  algorithm</title><categories>cs.DM math.CO</categories><comments>22 pages, no figures</comments><msc-class>05C15 (Primary) 05C85 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a linear-time algorithm to decide 3-colorability of a triangle-free
graph embedded in a fixed surface, and a quadratic-time algorithm to output a
3-coloring in the affirmative case. The algorithms also allow to prescribe the
coloring for a bounded number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01199</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01199</id><created>2016-01-06</created><updated>2016-02-16</updated><authors><author><keyname>Thor</keyname><forenames>Andreas</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Introducing CitedReferencesExplorer (CRExplorer): A program for
  Reference Publication Year Spectroscopy with Cited References Standardization</title><categories>cs.DL</categories><comments>Accepted for publication in the Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new tool - the CitedReferencesExplorer (CRExplorer,
www.crexplorer.net) - which can be used to disambiguate and analyze the cited
references (CRs) of a publication set downloaded from the Web of Science (WoS).
The tool is especially suitable to identify those publications which have been
frequently cited by the researchers in a field and thereby to study for example
the historical roots of a research field or topic. CRExplorer simplifies the
identification of key publications by enabling the user to work with both a
graph for identifying most frequently cited reference publication years (RPYs)
and the list of references for the RPYs which have been most frequently cited.
A further focus of the program is on the standardization of CRs. It is a
serious problem in bibliometrics that there are several variants of the same CR
in the WoS. In this study, CRExplorer is used to study the CRs of all papers
published in the Journal of Informetrics. The analyses focus on the most
important papers published between 1980 and 1990.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01203</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01203</id><created>2016-01-06</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Fiala</keyname><forenames>Dalibor</forenames></author></authors><title>Publication boost in Web of Science journals and its effect on citation
  distributions</title><categories>cs.DL cs.SI physics.data-an</categories><comments>14 pages, 5 figures, J. Assoc. Inf. Sci. Tec. (2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the dramatic increase in the number of research
articles indexed in the Web of Science database impacts the commonly observed
distributions of citations within these articles. First, we document that the
growing number of physics articles in recent years is due to existing journals
publishing more and more papers rather than more new journals coming into being
as it happens in computer science. And second, even though the references from
the more recent papers generally cover a longer time span, the newer papers are
cited more frequently than the older ones if the uneven paper growth is not
corrected for. Nevertheless, despite this change in the distribution of
citations, the citation behavior of scientists does not seem to have changed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01216</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01216</id><created>2016-01-06</created><authors><author><keyname>Norousi</keyname><forenames>Ramin</forenames></author><author><keyname>Schmid</keyname><forenames>Volker J.</forenames></author></authors><title>Automatic 3D object detection of Proteins in Fluorescent labeled
  microscope images with spatial statistical analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since manual object detection is very inaccurate and time consuming, some
automatic object detection tools have been developed in recent years. At the
moment, there is no image analysis software available which provides an
automatic, objective assessment of 3D foci which is generally applicable.
Complications arise from discrete foci which are very close or even come in
contact to other foci, moreover they are of variable sizes and show variable
signal-to-noise, and must be analyzed fully in 3D. Therefore we introduce the
3D-OSCOS (3D-Object Segmentation and Colocalization Analysis based on Spatial
statistics) algorithm which is implemented as a user-friendly toolbox for
interactive detection of 3D objects and visualization of labeled images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01218</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01218</id><created>2016-01-06</created><authors><author><keyname>Kari</keyname><forenames>Dariush</forenames></author><author><keyname>Vanli</keyname><forenames>Nuri Denizcan</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman Serdar</forenames></author></authors><title>Adaptive and Efficient Nonlinear Channel Equalization for Underwater
  Acoustic Communication</title><categories>cs.LG cs.IT cs.SD math.IT</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate underwater acoustic (UWA) channel equalization and introduce
hierarchical and adaptive nonlinear channel equalization algorithms that are
highly efficient and provide significantly improved bit error rate (BER)
performance. Due to the high complexity of nonlinear equalizers and poor
performance of linear ones, to equalize highly difficult underwater acoustic
channels, we employ piecewise linear equalizers. However, in order to achieve
the performance of the best piecewise linear model, we use a tree structure to
hierarchically partition the space of the received signal. Furthermore, the
equalization algorithm should be completely adaptive, since due to the highly
non-stationary nature of the underwater medium, the optimal MSE equalizer as
well as the best piecewise linear equalizer changes in time. To this end, we
introduce an adaptive piecewise linear equalization algorithm that not only
adapts the linear equalizer at each region but also learns the complete
hierarchical structure with a computational complexity only polynomial in the
number of nodes of the tree. Furthermore, our algorithm is constructed to
directly minimize the final squared error without introducing any ad-hoc
parameters. We demonstrate the performance of our algorithms through highly
realistic experiments performed on accurately simulated underwater acoustic
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01220</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01220</id><created>2016-01-03</created><authors><author><keyname>Gamage</keyname><forenames>Bashini Jeewanthi</forenames></author></authors><title>The Impact of Project Management in Virtual Environment: A Software
  Industry Perspective</title><categories>cs.CY cs.SE</categories><comments>5, University Of Sri Jayewardenepura, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual team in a project within an Organization could achieve optimize
project performance by acquiring appropriate human resources, coordination,
communication and regular performance evaluation. According to the literature
many ICT tools will collaborate to manage virtual teams, but still most of the
projects lead to failure in the software industry. Aim of this research is to
discover the most affected factors for virtual project human resource
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01222</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01222</id><created>2015-12-29</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Fractal social organization as a foundation to pervasive social
  computing services</title><categories>cs.CY</categories><comments>The paper makes use, in minimal part, of text and pictures from
  papers [18,19,20,21]. It has been submitted for publication in the Elsevier
  journal &quot;Pervasive and Mobile Computing&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pervasive social computing is a promising approach that promises to empower
both the individual and the whole and thus candidates itself as a foundation to
the &quot;smarter&quot; social organizations that our new turbulent and resource-scarce
worlds so urgently requires. In this contribution we first identify those that
we consider as the major requirements to be fulfilled in order to realize an
effective pervasive social computing infrastructure. We then conjecture that
our service-oriented community and fractal social organization fulfill those
requirements and therefore constitute an effective strategy to design pervasive
social computing infrastructures. In order to motivate our conjecture, in this
paper we discuss a model of social translucence and discuss fractal social
organization as a referral service empowering a social system's parts and
whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01228</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01228</id><created>2016-01-06</created><authors><author><keyname>West</keyname><forenames>J.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Some Experimental Issues in Financial Fraud Detection: An Investigation</title><categories>cs.CR cs.AI</categories><comments>J. West and Maumita Bhattacharya. &quot;Some Experimental Issues in
  Financial Fraud Detection: An Investigation&quot;, In the Proceedings of The 5th
  International Symposium on Cloud and Service Computing (SC2 2015), IEEE CS
  Press</comments><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial fraud detection is an important problem with a number of design
aspects to consider. Issues such as algorithm selection and performance
analysis will affect the perceived ability of proposed solutions, so for
auditors and re-searchers to be able to sufficiently detect financial fraud it
is necessary that these issues be thoroughly explored. In this paper we will
revisit the key performance metrics used for financial fraud detection with a
focus on credit card fraud, critiquing the prevailing ideas and offering our
own understandings. There are many different performance metrics that have been
employed in prior financial fraud detection research. We will analyse several
of the popular metrics and compare their effectiveness at measuring the ability
of detection mechanisms. We further investigated the performance of a range of
computational intelligence techniques when applied to this problem domain, and
explored the efficacy of several binary classification methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01229</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01229</id><created>2016-01-06</created><updated>2016-01-07</updated><authors><author><keyname>Fett</keyname><forenames>Daniel</forenames></author><author><keyname>K&#xfc;sters</keyname><forenames>Ralf</forenames></author><author><keyname>Schmitz</keyname><forenames>Guido</forenames></author></authors><title>A Comprehensive Formal Security Analysis of OAuth 2.0</title><categories>cs.CR</categories><comments>Parts of this work extend the web model presented in arXiv:1411.7210,
  arXiv:1403.1866 and arXiv:1508.01719</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The OAuth 2.0 protocol allows users to grant relying parties access to
resources at identity providers. In addition to being used for this kind of
authorization, OAuth is also often employed for authentication in single
sign-on (SSO) systems. OAuth 2.0 is, in fact, one of the most widely used
protocols in the web for these purposes, with companies such as Google,
Facebook, or PayPal acting as identity providers and millions of websites
connecting to these services as relying parties. OAuth 2.0 is at the heart of
Facebook Login and many other implementations, and also serves as the
foundation for the upcoming SSO system OpenID Connect. Despite the popularity
of OAuth, so far analysis efforts were mostly targeted at finding bugs in
specific implementations and were based on formal models which abstract from
many web features or did not provide a formal treatment at all. In this paper,
we carry out the first extensive formal analysis of the OAuth 2.0 standard in
an expressive web model. Our analysis aims at establishing strong authorization
and authentication guarantees, for which we provide formal definitions. In our
formal analysis, all four OAuth grant types are covered. They may even run
simultaneously in the same and different relying parties and identity
providers, where malicious relying parties and identity providers are
considered as well. While proving security, we found two previously unknown
attacks on OAuth, which both break authorization and authentication in OAuth.
The underlying vulnerabilities are present also in the new OpenID Connect
standard and can be exploited in practice. We propose fixes for the identified
vulnerabilities, and then, for the first time, actually prove the security of
OAuth in an expressive web model. In particular, we show that the fixed version
of OAuth provides the authorization and authentication properties we specify.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01230</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01230</id><created>2016-01-06</created><authors><author><keyname>Downer</keyname><forenames>K.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>BYOD Security: A New Business Challenge</title><categories>cs.CR cs.NI</categories><comments>K. Downer and Maumita Bhattacharya. &quot;BYOD Security: A New Business
  Challenge&quot;, accepted for publication in Proceedings of The 5th International
  Symposium on Cloud and Service Computing (SC2 2015), IEEE CS Press</comments><msc-class>68Uxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bring Your Own Device (BYOD) is a rapidly growing trend in businesses
concerned with information technology. BYOD presents a unique list of security
concerns for businesses implementing BYOD policies. Recent publications
indicate a definite awareness of risks involved in incorporating BYOD into
business, however it is still an underrated issue compared to other IT security
concerns. This paper focuses on two key BYOD security issues: security
challenges and available frameworks. A taxonomy specifically classifying BYOD
security challenges is introduced alongside comprehensive frameworks and
solutions which are also analysed to gauge their limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01231</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01231</id><created>2016-01-06</created><authors><author><keyname>Sawhney</keyname><forenames>Mehtaab</forenames></author><author><keyname>Weed</keyname><forenames>Jonathan</forenames></author></authors><title>Further results on arc and bar k-visibility graphs</title><categories>math.CO cs.DM</categories><comments>20 pages</comments><msc-class>05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider visibility graphs involving bars and arcs in which lines of sight
can pass through up to k objects. We prove a new edge bound for arc
k-visibility graphs, provide maximal constructions for arc and semi-arc
k-visibility graphs, and give a complete characterization of semi-arc
visibility graphs. We show that the family of arc i-visibility graphs is never
contained in the family of bar j-visibility graphs for any i and j, and that
the family of bar i-visibility graphs is not contained in the family of bar
j-visibility graphs for $i \neq j$. We also give the first thickness bounds for
arc and semi-arc k-visibility graphs. Finally, we introduce a model for random
semi-bar and semi-arc k-visibility graphs and analyze its properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01232</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01232</id><created>2016-01-06</created><authors><author><keyname>Allain</keyname><forenames>Benjamin</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author><author><keyname>Franco</keyname><forenames>Jean-Sebastien</forenames></author><author><keyname>Hetroy</keyname><forenames>Franck</forenames></author><author><keyname>Boyer</keyname><forenames>Edmond</forenames></author></authors><title>Shape Animation with Combined Captured and Simulated Dynamics</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel volumetric animation generation framework to create new
types of animations from raw 3D surface or point cloud sequence of captured
real performances. The framework considers as input time incoherent 3D
observations of a moving shape, and is thus particularly suitable for the
output of performance capture platforms. In our system, a suitable virtual
representation of the actor is built from real captures that allows seamless
combination and simulation with virtual external forces and objects, in which
the original captured actor can be reshaped, disassembled or reassembled from
user-specified virtual physics. Instead of using the dominant surface-based
geometric representation of the capture, which is less suitable for volumetric
effects, our pipeline exploits Centroidal Voronoi tessellation decompositions
as unified volumetric representation of the real captured actor, which we show
can be used seamlessly as a building block for all processing stages, from
capture and tracking to virtual physic simulation. The representation makes no
human specific assumption and can be used to capture and re-simulate the actor
with props or other moving scenery elements. We demonstrate the potential of
this pipeline for virtual reanimation of a real captured event with various
unprecedented volumetric visual effects, such as volumetric distortion,
erosion, morphing, gravity pull, or collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01233</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01233</id><created>2016-01-06</created><updated>2016-03-08</updated><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames><affiliation>INRIA</affiliation></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames><affiliation>University of Bologna &amp; INRIA</affiliation></author></authors><title>(Leftmost-Outermost) Beta Reduction is Invariant, Indeed</title><categories>cs.PL cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1405.3311</comments><proxy>LMCS</proxy><journal-ref>LMCS 12 (1:4) 2016</journal-ref><doi>10.2168/LMCS-12(1:4)2016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slot and van Emde Boas' weak invariance thesis states that reasonable
machines can simulate each other within a polynomially overhead in time. Is
lambda-calculus a reasonable machine? Is there a way to measure the
computational complexity of a lambda-term? This paper presents the first
complete positive answer to this long-standing problem. Moreover, our answer is
completely machine-independent and based over a standard notion in the theory
of lambda-calculus: the length of a leftmost-outermost derivation to normal
form is an invariant cost model. Such a theorem cannot be proved by directly
relating lambda-calculus with Turing machines or random access machines,
because of the size explosion problem: there are terms that in a linear number
of steps produce an exponentially long output. The first step towards the
solution is to shift to a notion of evaluation for which the length and the
size of the output are linearly related. This is done by adopting the linear
substitution calculus (LSC), a calculus of explicit substitutions modeled after
linear logic proof nets and admitting a decomposition of leftmost-outermost
derivations with the desired property. Thus, the LSC is invariant with respect
to, say, random access machines. The second step is to show that LSC is
invariant with respect to the lambda-calculus. The size explosion problem seems
to imply that this is not possible: having the same notions of normal form,
evaluation in the LSC is exponentially longer than in the lambda-calculus. We
solve such an impasse by introducing a new form of shared normal form and
shared reduction, deemed useful. Useful evaluation avoids those steps that only
unshare the output without contributing to beta-redexes, i.e. the steps that
cause the blow-up in size. The main technical contribution of the paper is
indeed the definition of useful reductions and the thorough analysis of their
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01245</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01245</id><created>2016-01-06</created><authors><author><keyname>Singh</keyname><forenames>Rashmi</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author><author><keyname>Yadav</keyname><forenames>Anita</forenames></author></authors><title>Loop Free Multipath Routing Algorithm</title><categories>cs.NI</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Single path routing that is currently used in the internet routers,is easy to
implement as it simplifies the routing tables and packet flow paths. However it
is not optimal and has shortcomings in utilizing the network resources
optimally, load balancing &amp; fast recovery in case of faults (fault tolerance).
The given algorithm resolves all these problems by using all possible multiple
paths for transfer of information, while retaining loop-free property. We have
proposed a new dynamic loop-free multipath routing algorithm which improves
network throughput and network resource utilization, reduces average
transmission delay, and is not affected by faults in the links and router
nodes. The main idea of this algorithm is to maintain multiple possible next
hops for a destination along with weights. At every node, the traffic to a
destination is split among multiple next hops in proportion to the estimated
weights. The number of multiple next hops also changes depending on the traffic
conditions, but it is never less than one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01271</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01271</id><created>2016-01-06</created><authors><author><keyname>Sassano</keyname><forenames>Mario</forenames></author><author><keyname>Zaccarian</keyname><forenames>Luca</forenames></author></authors><title>Hierarchical stability of nonlinear hybrid systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note we prove a hierarchical stability result that applies to
hybrid dynamical systems satisfying the hybrid basic conditions of (Goebel et
al., 2012). In particular, we establish sufficient conditions for uniform
asymptotic stability of a compact set based on some hierarchical stability
assumptions involving two nested closed sets containing such a compact set.
Moreover, mimicking the well known result for cascaded systems, we prove that
the basin of attraction of such compact set coincides with the largest set from
which all solutions are bounded. The result appears to be useful when applied
to several recent works involving hierarchical control architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01272</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01272</id><created>2016-01-06</created><authors><author><keyname>Tran</keyname><forenames>Ke</forenames></author><author><keyname>Bisazza</keyname><forenames>Arianna</forenames></author><author><keyname>Monz</keyname><forenames>Christof</forenames></author></authors><title>Recurrent Memory Network for Language Modeling</title><categories>cs.CL</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNN) have obtained excellent result in many
natural language processing (NLP) tasks. However, understanding and
interpreting the source of this success remains a challenge. In this paper, we
propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only
amplifies the power of RNN but also facilitates our understanding of its
internal functioning and allows us to discover underlying patterns in data. We
demonstrate the power of RMN on language modeling and sentence completion
tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)
network on three large German, Italian, and English dataset. Additionally we
perform in-depth analysis of various linguistic dimensions that RMN captures.
On Sentence Completion Challenge, for which it is essential to capture sentence
coherence, our RMN obtains 69.2% accuracy, surpassing the previous
state-of-the-art by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01274</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01274</id><created>2016-01-06</created><authors><author><keyname>Liu</keyname><forenames>Hui</forenames></author><author><keyname>Cui</keyname><forenames>Tao</forenames></author><author><keyname>Leng</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Linbo</forenames></author></authors><title>Encoding and Decoding Algorithms for Arbitrary Dimensional Hilbert Order</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hilbert order is widely applied in many areas. However, most of the
algorithms are confined to low dimensional cases. In this paper, algorithms for
encoding and decoding arbitrary dimensional Hilbert order are presented. Eight
algorithms are proposed. Four algorithms are based on arithmetic operations and
the other four algorithms are based on bit operations. For the algorithms
complexities, four of them are linear and the other four are constant for given
inputs. In the end of the paper, algorithms for two dimensional Hilbert order
are presented to demonstrate the usage of the algorithms introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01278</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01278</id><created>2016-01-06</created><updated>2016-01-13</updated><authors><author><keyname>Lutz</keyname><forenames>Roman</forenames></author></authors><title>Security and Privacy in Future Internet Architectures - Benefits and
  Challenges of Content Centric Networks</title><categories>cs.CR cs.NI</categories><comments>11 pages</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the shortcomings of our current Internet become more and more obvious,
researchers have started creating alternative approaches for the Internet of
the future. Their design goals are mainly content-orientation, security,
support for mobility and cloud computing. The probably most popular
architecture is called Content Centric Networking. Every communication is
treated as a distribution of content and caches are used within the network to
improve the effectiveness. While the performance gain of Content Centric
Networks is undoubted, there are questions about security and especially
privacy since it is not one of its main design principle. In this work, we
compare the Content Centric Networking approach with the current Internet with
respect to security and privacy. We analyze improvements that have been made
and new problems that have yet to be resolved. The Internet of the future could
be content-oriented, so it is essential to identify potential security and
privacy issues that are inherent to the architecture early on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01280</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01280</id><created>2016-01-06</created><authors><author><keyname>Dong</keyname><forenames>Li</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author></authors><title>Language to Logical Form with Neural Attention</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic parsing aims at mapping natural language to machine interpretable
meaning representations. Traditional approaches rely on high-quality lexicons,
manually-built templates, and linguistic features which are either domain- or
representation-specific. In this paper, we present a general method based on an
attention-enhanced sequence-to-sequence model. We encode input sentences into
vector representations using recurrent neural networks, and generate their
logical forms by conditioning the output on the encoding vectors. The model is
trained in an end-to-end fashion to maximize the likelihood of target logical
forms given the natural language inputs. Experimental results on four datasets
show that our approach performs competitively without using hand-engineered
features and is easy to adapt across domains and meaning representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01286</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01286</id><created>2016-01-06</created><authors><author><keyname>Goldfeld</keyname><forenames>Ziv</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Strong Secrecy for Cooperative Broadcast Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A broadcast channel (BC) where the decoders cooperate via a one-sided link is
considered. One common and two private messages are transmitted and the private
message to the cooperative user should be kept secret from the
cooperation-aided user. The secrecy level is measured in terms of
strong-secrecy, i.e., a vanishing information leakage. An inner bound on the
capacity region is derived by using a channel-resolvability-based code that
double-bins the codebook of the secret message, and by using a likelihood
encoder to choose the transmitted codeword. The inner bound is shown to be
tight for semi-deterministic and physically degraded BCs and the results are
compared to those of the corresponding BCs without a secrecy constraint.
Blackwell and Gaussian BC examples illustrate the impact of secrecy on the rate
regions. Unlike the case without secrecy, where sharing information about both
private messages via the cooperative link is optimal, our protocol conveys
parts of the common and non-confidential messages only. This restriction
reduces the transmission rates more than the usual rate loss due to secrecy
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01289</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01289</id><created>2016-01-06</created><updated>2016-02-01</updated><authors><author><keyname>Gharibi</keyname><forenames>Mirmojtaba</forenames></author><author><keyname>Boutaba</keyname><forenames>Raouf</forenames></author><author><keyname>Waslander</keyname><forenames>Steven L.</forenames></author></authors><title>Internet of Drones</title><categories>cs.NI cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Internet of Drones (IoD) is a layered network control architecture
designed mainly for coordinating the access of unmanned aerial vehicles to
controlled airspace, and providing navigation services between locations
referred to as nodes. The IoD provides generic services for various drone
applications such as package delivery, traffic surveillance, search and rescue
and more. In this paper, we present a conceptual model of how such an
architecture can be organized and we specify the features that an IoD system
based on our architecture should implement. For doing so, we extract key
concepts from three existing large scale networks, namely the air traffic
control network, the cellular network, and the Internet and explore their
connections to our novel architecture for drone traffic management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01297</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01297</id><created>2016-01-06</created><updated>2016-01-06</updated><authors><author><keyname>Ibarra</keyname><forenames>Imanol Arrieta</forenames></author><author><keyname>Ramos</keyname><forenames>Bernardo</forenames></author><author><keyname>Roemheld</keyname><forenames>Lars</forenames></author></authors><title>Angrier Birds: Bayesian reinforcement learning</title><categories>cs.AI cs.LG</categories><comments>Stanford University CS221 Final Project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We train a reinforcement learner to play a simplified version of the game
Angry Birds. The learner is provided with a game state in a manner similar to
the output that could be produced by computer vision algorithms. We improve on
the efficiency of regular {\epsilon}-greedy Q-Learning with linear function
approximation through more systematic exploration in Randomized Least Squares
Value Iteration (RLSVI), an algorithm that samples its policy from a posterior
distribution on optimal policies. With larger state-action spaces, efficient
exploration becomes increasingly important, as evidenced by the faster learning
in RLSVI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01298</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01298</id><created>2016-01-06</created><authors><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Snoeyink</keyname><forenames>Jack</forenames></author><author><keyname>Vosoughpour</keyname><forenames>Hamideh</forenames></author></authors><title>Visibility Graphs, Dismantlability, and the Cops and Robbers Game</title><categories>cs.CG</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study versions of cop and robber pursuit-evasion games on the visibility
graphs of polygons, and inside polygons with straight and curved sides. Each
player has full information about the other player's location, players take
turns, and the robber is captured when the cop arrives at the same point as the
robber. In visibility graphs we show the cop can always win because visibility
graphs are dismantlable, which is interesting as one of the few results
relating visibility graphs to other known graph classes. We extend this to show
that the cop wins games in which players move along straight line segments
inside any polygon and, more generally, inside any simply connected planar
region with a reasonable boundary. Essentially, our problem is a type of
pursuit-evasion using the link metric rather than the Euclidean metric, and our
result provides an interesting class of infinite cop-win graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01336</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01336</id><created>2016-01-06</created><authors><author><keyname>Lotfifar</keyname><forenames>Foad</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author></authors><title>A Serial Multilevel Hypergraph Partitioning Algorithm</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph partitioning problem has many applications in scientific computing
such as computer aided design, data mining, image compression and other
applications with sparse-matrix vector multiplications as a kernel operation.
In many cases it is advantageous to use hypergraphs as they, compared to
graphs, have a more general structure and can be used to model more complex
relationships between groups of objects. This motivates our focus on the
less-studied hypergraph partitioning problem.
  In this paper, we propose a serial multi-level bipartitioning algorithm. One
important step in current heuristics for hypergraph partitioning is clustering
during which similar vertices must be recognized. This can be particularly
difficult in irregular hypergraphs with high variation of vertex degree and
hyperedge size; heuristics that rely on local vertex clustering decisions often
give poor partitioning quality. A novel feature of the proposed algorithm is to
use the techniques of rough set clustering to address this problem. We show
that our proposed algorithm gives on average between 18.8 per cent and 71.1 per
cent better quality on these irregular hypergraphs by comparing it to
state-of-the-art hypergraph partitioning algorithms on benchmarks taken from
real applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01339</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01339</id><created>2016-01-06</created><authors><author><keyname>Shu</keyname><forenames>Xiao</forenames></author><author><keyname>Wu</keyname><forenames>Xiaolin</forenames></author></authors><title>Quality Adaptive Low-Rank Based JPEG Decoding with Applications</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small compression noises, despite being transparent to human eyes, can
adversely affect the results of many image restoration processes, if left
unaccounted for. Especially, compression noises are highly detrimental to
inverse operators of high-boosting (sharpening) nature, such as deblurring and
superresolution against a convolution kernel. By incorporating the non-linear
DCT quantization mechanism into the formulation for image restoration, we
propose a new sparsity-based convex programming approach for joint compression
noise removal and image restoration. Experimental results demonstrate
significant performance gains of the new approach over existing image
restoration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01343</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01343</id><created>2016-01-06</created><authors><author><keyname>Yamada</keyname><forenames>Ikuya</forenames></author><author><keyname>Shindo</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Takeda</keyname><forenames>Hideaki</forenames></author><author><keyname>Takefuji</keyname><forenames>Yoshiyasu</forenames></author></authors><title>Joint Learning of the Embedding of Words and Entities for Named Entity
  Disambiguation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named Entity Disambiguation (NED) refers to the task of resolving multiple
named entity mentions in a document to their correct references in a knowledge
base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method
specifically designed for NED. The proposed method jointly maps words and
entities into the same continuous vector space. We extend the skip-gram model
by using two models. The KB graph model learns the relatedness of entities
using the link structure of the KB, whereas the anchor context model aims to
align vectors such that similar words and entities occur close to one another
in the vector space by leveraging KB anchors and their context words. By
combining contexts based on the proposed embedding with standard NED features,
we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset
and 85.2% on the TAC 2010 dataset. Our code and pre-trained vectors will be
made available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01348</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01348</id><created>2016-01-06</created><authors><author><keyname>Kumar</keyname><forenames>Akshay</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>An Online Delay Efficient Packet Scheduler for M2M Traffic in Industrial
  Automation</title><categories>cs.NI cs.PF</categories><comments>6 pages, 7 figures, Accepted for publication in IEEE Systems
  Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some Machine-to-Machine (M2M) communication links particularly those in a
industrial automation plant have stringent latency requirements. In this paper,
we study the delay-performance for the M2M uplink from the sensors to a
Programmable Logic Controller (PLC) in a industrial automation scenario. The
uplink traffic can be broadly classified as either Periodic Update (PU) and
Event Driven (ED). The PU arrivals from different sensors are periodic,
synchronized by the PLC and need to be processed by a prespecified firm latency
deadline. On the other hand, the ED arrivals are random, have low-arrival rate,
but may need to be processed quickly depending upon the criticality of the
application. To accommodate these contrasting Quality-of-Service (QoS)
requirements, we model the utility of PU and ED packets using step function and
sigmoidal functions of latency respectively. Our goal is to maximize the
overall system utility while being proportionally fair to both PU and ED data.
To this end, we propose a novel online QoS-aware packet scheduler that gives
priority to ED data as long as that results the latency deadline is met for PU
data. However as the size of networks increases, we drop the PU packets that
fail to meet latency deadline which reduces congestion and improves overall
system utility. Using extensive simulations, we compare the performance of our
scheme with various scheduling policies such as First-Come-First-Serve (FCFS),
Earliest-Due-Date (EDD) and (preemptive) priority. We show that our scheme
outperforms the existing schemes for various simulation scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01356</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01356</id><created>2016-01-06</created><updated>2016-03-06</updated><authors><author><keyname>Ozsoy</keyname><forenames>Makbule Gulcin</forenames></author></authors><title>From Word Embeddings to Item Recommendation</title><categories>cs.LG cs.CL cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network platforms can archive data produced by their users. Then, the
archived data is used to provide better services to the users. One of the
services that these platforms provide is the recommendation service.
Recommendation systems can predict the future preferences of users using
various different techniques. One of the most popular technique for
recommendation is matrix-factorization, which uses low-rank approximation of
input data. Similarly, word embedding methods from natural language processing
literature learn low-dimensional vector space representation of input elements.
Noticing the similarities among word embedding and matrix factorization
techniques and based on the previous works that apply techniques from text
processing to recommendation, Word2Vec's skip-gram technique is employed to
make recommendations. The aim of this work is to make recommendation on next
check-in venues. Unlike previous works that use Word2Vec for recommendation, in
this work non-textual features are used. For the experiments, a Foursquare
check-in dataset is used. The results show that use of vector space
representations of items modeled by skip-gram technique is promising for making
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01360</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01360</id><created>2016-01-06</created><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Grant</keyname><forenames>Steven L.</forenames></author></authors><title>Proportionate Affine Projection Algorithms for Block-sparse System
  Identification</title><categories>cs.IT math.IT</categories><comments>ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new family of block-sparse proportionate affine projection algorithms
(BS-PAPA) is proposed to improve the performance for block-sparse systems. This
is motivated by the recent block-sparse proportionate normalized least mean
square (BS-PNLMS) algorithm. It is demonstrated that the affine projection
algorithm (APA), proportionate APA (PAPA), BS-PNLMS and PNLMS are all special
cases of the proposed BS-PAPA algorithm. Meanwhile, an efficient implementation
of the proposed BS-PAPA and block-sparse memory PAPA (BS-MPAPA) are also
presented to reduce computational complexity. Simulation results demonstrate
that the proposed BS-PAPA and BS-MPAPA algorithms outperform the APA, PAPA and
MPAPA algorithms for block-sparse system identification in terms of both faster
convergence speed and better tracking ability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01363</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01363</id><created>2016-01-06</created><authors><author><keyname>Lin</keyname><forenames>Rongrong</forenames></author><author><keyname>Zhang</keyname><forenames>Haizhang</forenames></author></authors><title>Convergence Analysis of the Gaussian Regularized Shannon Sampling
  Formula</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the reconstruction of a bandlimited function from its finite
localized sample data. Truncating the classical Shannon sampling series results
in an unsatisfactory convergence rate due to the slow decayness of the sinc
function. To overcome this drawback, a simple and highly effective method,
called the Gaussian regularization of the Shannon series, was proposed in the
engineering and has received remarkable attention. It works by multiplying the
sinc function in the Shannon series with a regularized Gaussian function. L.
Qian (Proc. Amer. Math. Soc., 2003) established the convergence rate of
$O(\sqrt{n}\exp(-\frac{\pi-\delta}2n))$ for this method, where $\delta&lt;\pi$ is
the bandwidth and $n$ is the number of sample data. C. Micchelli {\it et al.}
(J. Complexity, 2009) proposed a different regularized method and obtained the
corresponding convergence rate of
$O(\frac1{\sqrt{n}}\exp(-\frac{\pi-\delta}2n))$. This latter rate is by far the
best among all regularized methods for the Shannon series. However, their
regularized method involves the solving of a linear system and is implicit and
more complicated. The main objective of this note is to show that the Gaussian
regularization of the Shannon series can also achieve the same best convergence
rate as that by C. Micchelli {\it et al}. We also show that the Gaussian
regularization method can improve the convergence rate for the useful average
sampling. Finally, the outstanding performance of numerical experiments
justifies our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01372</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01372</id><created>2016-01-06</created><authors><author><keyname>Marx</keyname><forenames>Daniel</forenames></author><author><keyname>Salmasi</keyname><forenames>Ario</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Anastasios</forenames></author></authors><title>Constant-factor approximations for asymmetric TSP on nearly-embeddable
  graphs</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Asymmetric Traveling Salesperson Problem (ATSP) the goal is to find a
closed walk of minimum cost in a directed graph visiting every vertex. We
consider the approximability of ATSP on topologically restricted graphs. It has
been shown by [Oveis Gharan and Saberi 2011] that there exists polynomial-time
constant-factor approximations on planar graphs and more generally graphs of
constant orientable genus. This result was extended to non-orientable genus by
[Erickson and Sidiropoulos 2014].
  We show that for any class of \emph{nearly-embeddable} graphs, ATSP admits a
polynomial-time constant-factor approximation. More precisely, we show that for
any fixed $k\geq 0$, there exist $\alpha, \beta&gt;0$, such that ATSP on
$n$-vertex $k$-nearly-embeddable graphs admits a $\alpha$-approximation in time
$O(n^\beta)$. The class of $k$-nearly-embeddable graphs contains graphs with at
most $k$ apices, $k$ vortices of width at most $k$, and an underlying surface
of either orientable or non-orientable genus at most $k$. Prior to our work,
even the case of graphs with a single apex was open. Our algorithm combines
tools from rounding the Held-Karp LP via thin trees with dynamic programming.
  We complement our upper bounds by showing that solving ATSP exactly on graphs
of pathwidth $k$ (and hence on $k$-nearly embeddable graphs) requires time
$n^{\Omega(k)}$, assuming the Exponential-Time Hypothesis (ETH). This is
surprising in light of the fact that both TSP on undirected graphs and Minimum
Cost Hamiltonian Cycle on directed graphs are FPT parameterized by treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01376</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01376</id><created>2016-01-06</created><authors><author><keyname>Chen</keyname><forenames>Zheng</forenames></author><author><keyname>Qiu</keyname><forenames>Ling</forenames></author><author><keyname>Liang</keyname><forenames>Xiaowen</forenames></author></authors><title>Area Spectral Efficiency Analysis and Energy Consumption Minimization in
  Multi-Antenna Poisson Distributed Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications, Major
  Revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at answering two fundamental questions: how area spectral
efficiency (ASE) behaves with different system parameters; how to design an
energy-efficient network. Based on stochastic geometry, we obtain the
expression and a tight lower-bound for ASE of Poisson distributed networks
considering multi-user MIMO (MU-MIMO) transmission. With the help of the
lower-bound, some interesting results are observed. These results are validated
via numerical results for the original expression. We find that ASE can be
viewed as a concave function with respect to the number of antennas and active
users. For the purpose of maximizing ASE, we demonstrate that the optimal
number of active users is a fixed portion of the number of antennas. With
optimal number of active users, we observe that ASE increases linearly with the
number of antennas. Another work of this paper is joint optimization of the
base station (BS) density, the number of antennas and active users to minimize
the network energy consumption. It is discovered that the optimal combination
of the number of antennas and active users is the solution that maximizes the
energy-efficiency. Besides the optimal algorithm, we propose a suboptimal
algorithm to reduce the computational complexity, which can achieve near
optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01379</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01379</id><created>2016-01-06</created><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Wu</keyname><forenames>Yueping</forenames></author><author><keyname>Jiang</keyname><forenames>Dongdong</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>User-Centric Interference Nulling in Downlink Multi-Antenna
  Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>31 pages, 8 figures, submitted to Transactions on Wireless
  Communications. arXiv admin note: text overlap with arXiv:1504.05283</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks (HetNets) are subject to strong interference due to
spectrum reuse. This affects the signal-to-interference ratio (SIR) of each
user, and hence is one of the limiting factors of network performance. However,
in previous works, interference management approaches in HetNets are mainly
based on signal or interference level, and thus cannot effectively improve
network performance. In this paper, we propose a user-centric interference
nulling (IN) scheme in downlink large-scale HetNets to improve network
performance by improving each user's SIR. This scheme has three design
parameters: the maximum degree of freedom for IN (i.e., maximum IN DoF), and
the IN thresholds for macro and pico users, respectively. Using tools from
stochastic geometry, we first obtain a tractable expression of the coverage
(equivalently outage) probability. Then, we obtain the asymptotic expressions
of the outage and coverage probabilities in the low and high SIR threshold
regimes, respectively. The asymptotic expressions indicate that the maximum IN
DoF and the IN thresholds affect the asymptotic outage (coverage) probability
in dramatically different ways. Moreover, we characterize the optimal maximum
IN DoF which optimizes the asymptotic outage (coverage) probability. The
optimization results reveal that the IN scheme can linearly improve the
performance in the low SIR threshold regime, but cannot improve the performance
in the high SIR threshold regime. Finally, numerical results show that the
user-centric IN scheme can achieve good performance gains over existing
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01386</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01386</id><created>2016-01-06</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Farman</keyname><forenames>Haleem</forenames></author><author><keyname>Jan</keyname><forenames>Zahoor</forenames></author></authors><title>A New Image Steganographic Technique using Pattern based Bits Shuffling
  and Magic LSB for Grayscale Images</title><categories>cs.MM</categories><comments>A short paper of 6 pages</comments><journal-ref>Sindh University Research Journal-SURJ (Science Series) 47.4
  (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image Steganography is a growing research area of information security where
secret information is embedded in innocent-looking public communication. This
paper proposes a novel crystographic technique for grayscale images in spatial
domain. The secret data is encrypted and shuffled using pattern based bits
shuffling algorithm (PBSA) and a secret key. The encrypted data is then
embedded in the cover image using magic least significant bit (M-LSB) method.
Experimentally, the proposed method is evaluated by qualitative and
quantitative analysis which validates the effectiveness of the proposed method
in contrast to several state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01394</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01394</id><created>2016-01-06</created><updated>2016-01-08</updated><authors><author><keyname>Nakagawa</keyname><forenames>Kenji</forenames></author><author><keyname>Watabe</keyname><forenames>Kohei</forenames></author><author><keyname>Sabu</keyname><forenames>Takuto</forenames></author></authors><title>On the Search Algorithm for the Output Distribution that Achieves the
  Channel Capacity</title><categories>cs.IT math.IT</categories><comments>45 pages, 9 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a search algorithm for the output distribution that achieves the
channel capacity of a discrete memoryless channel. We will propose an algorithm
by iterated projections of an output distribution onto affine subspaces in the
set of output distributions. The problem of channel capacity has a similar
geometric structure as that of smallest enclosing circle for a finite number of
points in the Euclidean space. The metric in the Euclidean space is the
Euclidean distance and the metric in the space of output distributions is the
Kullback-Leibler divergence. We consider these two problems based on Amari's
$\alpha$-geometry. Then, we first consider the smallest enclosing circle in the
Euclidean space and develop an algorithm to find the center of the smallest
enclosing circle. Based on the investigation, we will apply the obtained
algorithm to the problem of channel capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01396</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01396</id><created>2016-01-06</created><authors><author><keyname>Tsatsanifos</keyname><forenames>George</forenames></author></authors><title>On the Computation of the Optimal Connecting Points in Road Networks</title><categories>cs.DS</categories><comments>20 pages, Graph algorithms</comments><msc-class>05C85</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a set of travelers, starting from likely different
locations towards a common destination within a road network, and propose
solutions to find the optimal connecting points for them. A connecting point is
a vertex of the network where a subset of the travelers meet and continue
traveling together towards the next connecting point or the destination. The
notion of optimality is with regard to a given aggregated travel cost, e.g.,
travel distance or shared fuel cost. This problem by itself is new and we make
it even more interesting (and complex) by considering affinity factors among
the users, i.e., how much a user likes to travel together with another one.
This plays a fundamental role in determining where the connecting points are
and how subsets of travelers are formed. We propose three methods for
addressing this problem, one that relies on a fast and greedy approach that
finds a sub-optimal solution, and two others that yield globally optimal
solution. We evaluate all proposed approaches through experiments, where
collections of real datasets are used to assess the trade-offs, behavior and
characteristics of each method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01398</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01398</id><created>2016-01-06</created><authors><author><keyname>Singh</keyname><forenames>Vibhutesh Kumar</forenames></author><author><keyname>Chawla</keyname><forenames>Hardik</forenames></author><author><keyname>Bohara</keyname><forenames>Vivek Ashok</forenames></author></authors><title>A Proof-of-Concept Device-to-Device Communication Testbed</title><categories>cs.NI</categories><comments>8th International Conference on COMmunication Systems &amp; NETworkS
  (COMSNETS 2016), Demos &amp; Exhibits Session</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design and development of proof-of-concept
Device-to-Device (D2D) Communication testbed. This testbed also seeks to
address the design issues involved in the implementation of a D2D network in a
realistic scenario. The performance of this testbed has been validated by
emulating a Cellular network consisting of a Base Staion (BTS) and many D2D
devices in its proximity. The devices and the BTS coordinate and communicate
with each other to select the optimum communication range, mode of
communication and transmit parameters. Through the experimental results it has
been shown that the proposed testbed has a communication radius of 120m and a
D2D communication range of 62m with over 90% efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01405</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01405</id><created>2016-01-07</created><authors><author><keyname>Noyes</keyname><forenames>Charles</forenames></author></authors><title>BitAV: Fast Anti-Malware by Distributed Blockchain Consensus and
  Feedforward Scanning</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present the design and implementation of a novel anti-malware environment
called BitAV. BitAV allows for the decentralization of the update and
maintenance mechanisms of the software, traditionally performed by a central
host, and uses a staggered scanning mechanism in order to improve performance.
The peer-to-peer network maintenance mechanism lowered the average update
propagation speed by 500% and is far less susceptible to targeted
denial-of-service attacks. The feedforward scanning mechanism significantly
improved end-to-end performance of the malware matching system, to a degree of
an average 14x increase, by decomposing the file matching process into
efficient queries that operate in verifiably constant time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01408</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01408</id><created>2016-01-07</created><authors><author><keyname>Mustafa</keyname><forenames>Suleiman</forenames></author><author><keyname>Xiao</keyname><forenames>Hannan</forenames></author></authors><title>Comparison of cinepak, intel, microsoft video and indeo codec for video
  compression</title><categories>cs.MM</categories><comments>13 pages, 1 figure, 7 tables, journal paper</comments><journal-ref>The International Journal of Multimedia and Its Applications
  (IJMA), Volume 7, Number 6 December 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The file size and picture quality are factors to be considered for streaming,
storage and transmitting videos over networks. This work compares Cinepak,
Intel, Microsoft Video and Indeo Codec for video compression. The peak signal
to noise ratio is used to compare the quality of such video compressed using
AVI codecs. The most widely used objective measurement by developers of video
processing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise
Ration is measured on a logarithmic scale and depends on the mean squared error
(MSE) between an original and an impaired image or video, relative to (2n-1)2.
  Previous research done regarding assessing of video quality has been mainly
by the use of subjective methods, and there is still no standard method for
objective assessments. Although it has been considered that compression might
not be significant in future as storage and transmission capabilities improve,
but at low bandwidths compression makes communication possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01410</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01410</id><created>2016-01-07</created><authors><author><keyname>Gamble</keyname><forenames>Geoffrey George</forenames></author><author><keyname>Yazdani</keyname><forenames>Mehrdad</forenames></author></authors><title>Sparse signals for the control of human movements using the infinity
  norm</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal control models have been successful in describing many aspects of
human movement. The interpretation of such models regarding neuronal
implementation of the human motor system is not clear. An important aspects of
optimal control policies is the notion of cost. Optimal control seeks to
minimize a notion of cost, while meeting certain goals. We offer a method to
transform current methods in the literature from their traditional form by
changing the norm by which cost is assessed. We show how sparsity can be
introduced into current optimal approaches that use continuous control signals.
We assess cost using the infinity norm. This results in optimal signals which
can be represented by a small amount of Dirac delta functions. Sparsity has
played an important role in theoretical neuroscience for information processing
(such as vision). In this work, to obtain sparse control signals, the infinity
norm is used as a penalty on the control signal, which is then encoded with
Dirac delta functions. We show, for a basic physical system, a point mass can
be moved between two points in a way that resembles human fast reaching
movements. Despite the sparse nature of the control signal, the movements that
result are continuous and smooth. These control signals are simpler than their
non-sparse counterparts, yet yield comparable results when applied towards
modeling reaching movements. In addition, such sparse control signals resemble
a sequence of spikes, giving this approach a biological interpretation. Actual
neuronal implementations are more complex. However, this work shows, in
principle, that sparsely encoded control signals are a plausible implementation
for the control of reaching movements. Leading techniques for modeling human
movements can easily be adjusted in order to introduce sparsity, with a
biological interpretation and the simplified information content of the control
signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01411</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01411</id><created>2016-01-07</created><authors><author><keyname>Tonde</keyname><forenames>Chetan</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Learning Kernels for Structured Prediction using Polynomial Kernel
  Transformations</title><categories>cs.LG stat.ML</categories><report-no>21 pages, 10 figures</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the kernel functions used in kernel methods has been a vastly
explored area in machine learning. It is now widely accepted that to obtain
'good' performance, learning a kernel function is the key challenge. In this
work we focus on learning kernel representations for structured regression. We
propose use of polynomials expansion of kernels, referred to as Schoenberg
transforms and Gegenbaur transforms, which arise from the seminal result of
Schoenberg (1938). These kernels can be thought of as polynomial combination of
input features in a high dimensional reproducing kernel Hilbert space (RKHS).
We learn kernels over input and output for structured data, such that,
dependency between kernel features is maximized. We use Hilbert-Schmidt
Independence Criterion (HSIC) to measure this. We also give an efficient,
matrix decomposition-based algorithm to learn these kernel transformations, and
demonstrate state-of-the-art results on several real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01419</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01419</id><created>2016-01-07</created><authors><author><keyname>Awasthi</keyname><forenames>Sateesh Kumar</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Absolute Trust: Algorithm for Aggregation of Trust in Peer-to- Peer
  Networks</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To mitigate the attacks by malicious peers and to motivate the peers to share
the resources in peer-to-peer networks, several reputation systems have been
proposed in the past. In most of them, the peers evaluate other peers based on
their past interactions and then aggregate this information in the whole
network. However such an aggregation process requires approximations in order
to converge at some global consensus. It may not be the true reflection of past
behavior of the peers. Moreover such type of aggregation gives only the
relative ranking of peers without any absolute evaluation of their past. This
is more significant when all the peers responding to a query, are malicious. In
such a situation, we can only know that who is better among them without
knowing their rank in the whole network. In this paper, we are proposing a new
algorithm which accounts for the past behavior of the peers and will estimate
the absolute value of the trust of peers. Consequently, we can suitably
identify them as a good peers or malicious peers. Our algorithm converges at
some global consensus much faster by choosing suitable parameters. Because of
its absolute nature it will equally load all the peers in network. It will also
reduce the inauthentic download in the network which was not possible in
existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01421</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01421</id><created>2016-01-07</created><updated>2016-01-22</updated><authors><author><keyname>Liu</keyname><forenames>Li</forenames></author><author><keyname>Li</keyname><forenames>Lanqiang</forenames></author><author><keyname>Kai</keyname><forenames>Xiaoshan</forenames></author><author><keyname>Zhu</keyname><forenames>Shixin</forenames></author></authors><title>Repeated-root constacyclic codes of length $3lp^{s}$ and their dual
  codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p\neq3$ be any prime and $l\neq3$ be any odd prime with $gcd(p,l)=1$.
$F_{q}^{*}=\langle\xi\rangle$ is decomposed into mutually disjoint union of
$gcd(q-1,3lp^{s})$ coset over the subgroup $\langle\xi^{3lp^{s}}\rangle$, where
$\xi$ is a primitive $(q-1)$th root of unity. We classify all repeated-root
constacyclic codes of length $3lp^{s}$ over the finite field $F_{q}$ into some
equivalence classes by the decomposition, where $q=p^{m}$, $s$ and $m$ are
positive integers. According to the equivalence classes, we explicitly
determine the generator polynomials of all repeated-root constacyclic codes of
length $3lp^{s}$ over $F_{q}$ and their dual codes. Self-dual
cyclic(negacyclic) codes of length $3lp^{s}$ over $F_{q}$ exist only when
$p=2$. And we give all self-dual cyclic(negacyclic) codes of length
$3l2^{s}$over $F_{2^{m}}$ and its enumeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01422</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01422</id><created>2016-01-07</created><authors><author><keyname>Matsuzawa</keyname><forenames>Tomoki</forenames></author><author><keyname>Relator</keyname><forenames>Raissa</forenames></author><author><keyname>Sese</keyname><forenames>Jun</forenames></author><author><keyname>Kato</keyname><forenames>Tsuyoshi</forenames></author></authors><title>Stochastic Dykstra Algorithms for Metric Learning on Positive
  Semi-Definite Cone</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, covariance descriptors have received much attention as powerful
representations of set of points. In this research, we present a new metric
learning algorithm for covariance descriptors based on the Dykstra algorithm,
in which the current solution is projected onto a half-space at each iteration,
and runs at O(n^3) time. We empirically demonstrate that randomizing the order
of half-spaces in our Dykstra-based algorithm significantly accelerates the
convergence to the optimal solution. Furthermore, we show that our approach
yields promising experimental results on pattern recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01423</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01423</id><created>2016-01-07</created><authors><author><keyname>Zhang</keyname><forenames>Huijuan</forenames></author><author><keyname>Liu</keyname><forenames>Kai</forenames></author></authors><title>A Routing Mechanism Based on Social Networks and Betweenness Centrality
  in Delay-Tolerant Networks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  With the growing popularity of mobile smart devices, the existing networks
are unable to meet the requirement of many complex scenarios; current network
architectures and protocols do not work well with the network with high latency
and frequent disconnections. To improve the performance of these networks some
scholars opened up a new research field, delay-tolerant networks, in which one
of the important research subjects is the forwarding and routing mechanism of
data packets. This paper presents a routing scheme based on social networks
owing to the fact that nodes in computer networks and social networks have high
behavioural similarity. To further improve efficiency this paper also suggests
a mechanism, which is the improved version of an existing betweenness
centrality based routing algorithm. The experiments showed that the proposed
scheme has better performance than the existing friendship routing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01427</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01427</id><created>2016-01-07</created><authors><author><keyname>Wang</keyname><forenames>Hui-Ming</forenames></author><author><keyname>Zheng</keyname><forenames>Tong-Xing</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Lee</keyname><forenames>Moon Ho</forenames></author></authors><title>Physical Layer Security in Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><comments>two-column 15 pages, 12 figures, accepted for publication in IEEE
  Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneous cellular network (HCN) is a promising approach to the
deployment of 5G cellular networks. This paper comprehensively studies physical
layer security in a multi-tier HCN where base stations (BSs), authorized users
and eavesdroppers are all randomly located. We first propose an access
threshold based secrecy mobile association policy that associates each user
with the BS providing the maximum \emph{truncated average received signal
power} beyond a threshold. Under the proposed policy, we investigate the
connection probability and secrecy probability of a randomly located user, and
provide tractable expressions for the two metrics. Asymptotic analysis reveals
that setting a larger access threshold increases the connection probability
while decreases the secrecy probability. We further evaluate the network-wide
secrecy throughput and the minimum secrecy throughput per user with both
connection and secrecy probability constraints. We show that introducing a
properly chosen access threshold significantly enhances the secrecy throughput
performance of a HCN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01431</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01431</id><created>2016-01-07</created><authors><author><keyname>Ju</keyname><forenames>Fujiao</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Liu</keyname><forenames>Simeng</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author></authors><title>Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal
  Component Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic principal component analysis (PPCA) is built upon a global
linear mapping, with which it is insufficient to model complex data variation.
This paper proposes a mixture of bilateral-projection probabilistic principal
component analysis model (mixB2DPPCA) on 2D data. With multi-components in the
mixture, this model can be seen as a soft cluster algorithm and has capability
of modeling data with complex structures. A Bayesian inference scheme has been
proposed based on the variational EM (Expectation-Maximization) approach for
learning model parameters. Experiments on some publicly available databases
show that the performance of mixB2DPPCA has been largely improved, resulting in
more accurate reconstruction errors and recognition rates than the existing
PCA-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01432</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01432</id><created>2016-01-07</created><authors><author><keyname>Piao</keyname><forenames>Xinglin</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author></authors><title>Block-Diagonal Sparse Representation by Learning a Linear Combination
  Dictionary for Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a sparse representation based recognition scheme, it is critical to learn
a desired dictionary, aiming both good representational power and
discriminative performance. In this paper, we propose a new dictionary learning
model for recognition applications, in which three strategies are adopted to
achieve these two objectives simultaneously. First, a block-diagonal constraint
is introduced into the model to eliminate the correlation between classes and
enhance the discriminative performance. Second, a low-rank term is adopted to
model the coherence within classes for refining the sparse representation of
each class. Finally, instead of using the conventional over-complete
dictionary, a specific dictionary constructed from the linear combination of
the training samples is proposed to enhance the representational power of the
dictionary and to improve the robustness of the sparse representation model.
The proposed method is tested on several public datasets. The experimental
results show the method outperforms most state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01435</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01435</id><created>2016-01-07</created><authors><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Artificial Noise Aided Secrecy Information and Power Transfer in OFDMA
  Systems</title><categories>cs.IT math.IT</categories><comments>To appeal in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study simultaneous wireless information and power transfer
(SWIPT) in orthogonal frequency division multiple access (OFDMA) systems with
the coexistence of information receivers (IRs) and energy receivers (ERs). The
IRs are served with best-effort secrecy data and the ERs harvest energy with
minimum required harvested power. To enhance the physical layer security for
IRs and yet satisfy energy harvesting requirements for ERs, we propose a new
frequency domain artificial noise (AN) aided transmission strategy. With the
new strategy, we study the optimal resource allocation for the weighted sum
secrecy rate maximization for IRs by power and subcarrier allocation at the
transmitter. The studied problem is shown to be a mixed integer programming
problem and thus non-convex, while we propose an efficient algorithm for
solving it based on the Lagrange duality method. To further reduce the
computational complexity, we also propose a suboptimal algorithm of lower
complexity. The simulation results illustrate the effectiveness of proposed
algorithms as compared against other heuristic schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01447</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01447</id><created>2016-01-07</created><authors><author><keyname>Isong</keyname><forenames>Bassey</forenames></author><author><keyname>Ekabua</keyname><forenames>Obeten</forenames></author></authors><title>State-Of-The-Art In Empirical Validation Of Software Metrics For Fault
  Proneness Prediction: Systematic Review</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  With the sharp rise in software dependability and failure cost, high quality
has been in great demand. However, guaranteeing high quality in software
systems which have grown in size and complexity coupled with the constraints
imposed on their development has become increasingly difficult, time and
resource consuming activity. Consequently, it becomes inevitable to deliver
software that have no serious faults. In this case, object-oriented (OO)
products being the de facto standard of software development with their unique
features could have some faults that are hard to find or pinpoint the impacts
of changes. The earlier faults are identified, found and fixed, the lesser the
costs and the higher the quality. To assess product quality, software metrics
are used. Many OO metrics have been proposed and developed. Furthermore, many
empirical studies have validated metrics and class fault proneness (FP)
relationship. The challenge is which metrics are related to class FP and what
activities are performed. Therefore, this study bring together the
state-of-the-art in fault prediction of FP that utilizes CK and size metrics.
We conducted a systematic literature review over relevant published empirical
validation articles. The results obtained are analysed and presented. It
indicates that 29 relevant empirical studies exist and measures such as
complexity, coupling and size were found to be strongly related to FP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01453</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01453</id><created>2016-01-07</created><updated>2016-01-11</updated><authors><author><keyname>Cai</keyname><forenames>Shijie</forenames></author><author><keyname>Che</keyname><forenames>Yueling</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Green 5G Heterogeneous Networks through Dynamic Small-Cell Operation</title><categories>cs.IT math.IT</categories><comments>The longer version of a paper to appear in IEEE Journal on Selected
  Areas in Communications, green communications and networking series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional macro-cell networks are experiencing an upsurge of data traffic,
and small-cells are deployed to help offload the traffic from macro-cells.
Given the massive deployment of small-cells in a macro-cell, the aggregate
power consumption of small-cells (though being low individually) can be larger
than that of the macro-cell. Compared to the macro-cell base station (MBS)
whose power consumption increases significantly with its traffic load, the
power consumption of a small-cell base station (SBS) is relatively flat and
independent of its load. To reduce the total power consumption of the
heterogeneous networks (HetNets), we dynamically change the operating states
(on and off) of the SBSs, while keeping the MBS on to avoid any service failure
outside active small-cells. First, we consider that the wireless users are
uniformly distributed in the network, and propose an optimal location-based
operation scheme by gradually turning off the SBSs closer to the MBS. We then
extend the operation problem to a more general case where users are
non-uniformly distributed in the network. Although this problem is NP-hard, we
propose a joint location and user density based operation scheme to achieve
near-optimum (with less than 1\% performance loss in our simulations) in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01463</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01463</id><created>2016-01-07</created><authors><author><keyname>Agrawal</keyname><forenames>Ajay</forenames></author><author><keyname>Gamad</keyname><forenames>R. S.</forenames></author></authors><title>Design of a Low-Power 1.65 Gbps Data Channel for HDMI Transmitter</title><categories>cs.AR</categories><comments>TMDS, HDMI, USB, Gbps, data-dependent jitter, supply current, UMC180,
  low-power consumption, single serial clock</comments><journal-ref>International Journal of VLSI Design &amp; Communication Systems
  (VLSICS), December 2015, Volume 6, Number 6</journal-ref><doi>10.5121/vlsic.2015.6603</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a design of low power data channel for application in
High Definition Multimedia Interface (HDMI) Transmitter circuit. The input is
10 bit parallel data and output is serial data at 1.65 Gbps. This circuit uses
only a single frequency of serial clock input. All other timing signals are
derived within the circuit from the serial clock. This design has dedicated
lines to disable and enable all its channels within two pixel-clock periods
only. A pair of disable and enable functions performed immediately after
power-on of the circuit serves as the reset function. The presented design is
immune to data-dependent switching spikes in supply current and pushes them in
the range of serial frequency and its multiples. Thus filtering requirements
are relaxed. The output stage uses a bias voltage of 2.8 volts for a receiver
pull-up voltage of 3.3 volts. The reported data channel is designed using UMC
180 nm CMOS Technology. The design is modifiable for other inter-board serial
interfaces like USB and LAN with different number of bits at the parallel
input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01465</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01465</id><created>2016-01-07</created><authors><author><keyname>Yao</keyname><forenames>Bing</forenames></author><author><keyname>Liu</keyname><forenames>Xia</forenames></author><author><keyname>Xu</keyname><forenames>Jin</forenames></author></authors><title>Maximum Leaf Spanning Trees of Growing Sierpinski Networks Models</title><categories>cs.DS cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamical phenomena of complex networks are very difficult to predict
from local information due to the rich microstructures and corresponding
complex dynamics. On the other hands, it is a horrible job to compute some
stochastic parameters of a large network having thousand and thousand nodes. We
design several recursive algorithms for finding spanning trees having maximal
leaves (MLS-trees) in investigation of topological structures of Sierpinski
growing network models, and use MLS-trees to determine the kernels, dominating
and balanced sets of the models. We propose a new stochastic method for the
models, called the edge-cumulative distribution, and show that it obeys a power
law distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01467</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01467</id><created>2016-01-07</created><updated>2016-03-04</updated><authors><author><keyname>Martyushev</keyname><forenames>Evgeniy</forenames></author></authors><title>On Some Properties of Calibrated Trifocal Tensors</title><categories>cs.CV</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two-view geometry, the essential matrix describes the relative position
and orientation of two calibrated images. In three views, a similar role is
assigned to the calibrated trifocal tensor. It is a particular case of the
(uncalibrated) trifocal tensor and thus it inherits all its properties but, due
to the smaller degrees of freedom, satisfies a number of additional algebraic
constraints. Some of them are described in this paper. More specifically, we
define a new notion - the trifocal essential matrix. On the one hand, it is a
generalization of the ordinary (bifocal) essential matrix, and, on the other
hand, it is closely related to the calibrated trifocal tensor. We prove the two
necessary and sufficient conditions that characterize the set of trifocal
essential matrices. Based on this characterization, we propose three necessary
conditions on a calibrated trifocal tensor. They have a form of 15 quartic and
99 quintic polynomial equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01469</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01469</id><created>2016-01-07</created><authors><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Zhang</keyname><forenames>Shuzhong</forenames></author></authors><title>Tensor and Its Tucker Core: the Invariance Relationships</title><categories>math.OC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [12], Hillar and Lim famously demonstrated that &quot;multilinear (tensor)
analogues of many efficiently computable problems in numerical linear algebra
are NP-hard&quot;. Despite many recent advancements, the state-of-the-art methods
for computing such `tensor analogues' still suffer severely from the curse of
dimensionality. In this paper we show that the Tucker core of a tensor however,
retains many properties of the original tensor, including the CP rank, the
border rank, the tensor Schatten quasi norms, and the Z-eigenvalues. Since the
core is typically smaller than the original tensor, this property leads to
considerable computational advantages, as confirmed by our numerical
experiments. In our analysis, we in fact work with a generalized Tucker-like
decomposition that can accommodate any full column-rank factorization matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01478</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01478</id><created>2016-01-07</created><authors><author><keyname>Groote</keyname><forenames>Jan Friso</forenames></author><author><keyname>Wijs</keyname><forenames>Anton</forenames></author></authors><title>An O(m log n) Algorithm for Stuttering Equivalence and Branching
  Bisimulation</title><categories>cs.LO cs.DS</categories><comments>A shortened version of this technical report has been published in
  the proceedings of TACAS 2016</comments><report-no>CSR-15-06</report-no><msc-class>68Q60, 68Q85</msc-class><acm-class>D.2.4; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new algorithm to determine stuttering equivalence with time
complexity $O(m \log n)$, where $n$ is the number of states and $m$ is the
number of transitions of a Kripke structure. This algorithm can also be used to
determine branching bisimulation in $O(m(\log |\mathit{Act}|+ \log n))$ time
where $\mathit{Act}$ is the set of actions in a labelled transition system.
Theoretically, our algorithm substantially improves upon existing algorithms
which all have time complexity $O(m n)$ at best. Moreover, it has better or
equal space complexity. Practical results confirm these findings showing that
our algorithm can outperform existing algorithms with orders of magnitude,
especially when the sizes of the Kripke structures are large. The importance of
our algorithm stretches far beyond stuttering equivalence and branching
bisimulation. The known $O(m n)$ algorithms were already far more efficient
(both in space and time) than most other algorithms to determine behavioural
equivalences (including weak bisimulation) and therefore it was often used as
an essential preprocessing step. This new algorithm makes this use of
stuttering equivalence and branching bisimulation even more attractive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01483</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01483</id><created>2016-01-07</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames><affiliation>DEDUCTEAM</affiliation></author></authors><title>Rules and derivations in an elementary logic course</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When teaching an elementary logic course to students who have a general
scientific background but have never been exposed to logic, we have to face the
problem that the notions of deduction rule and of derivation are completely new
to them, and are related to nothing they already know, unlike, for instance,
the notion of model, that can be seen as a generalization of the notion of
algebraic structure. In this note, we defend the idea that one strategy to
introduce these notions is to start with the notion of inductive definition
[1]. Then, the notion of derivation comes naturally. We also defend the idea
that derivations are pervasive in logic and that defining precisely this notion
at an early stage is a good investment to later define other notions in proof
theory, computability theory, automata theory, ... Finally, we defend the idea
that to define the notion of derivation precisely, we need to distinguish two
notions of derivation: labeled with elements and labeled with rule names. This
approach has been taken in [2].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01484</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01484</id><created>2016-01-07</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames><affiliation>DEDUCTEAM</affiliation></author><author><keyname>Jiang</keyname><forenames>Ying</forenames><affiliation>LCS</affiliation></author></authors><title>Decidability, Introduction Rules and Automata</title><categories>cs.LO</categories><comments>International Conferences on Logic for Programming, Artificial
  Intelligence and Reasoning, Nov 2015, Bula, Fiji. 2015, International
  Conferences on Logic for Programming, Artificial Intelligence and Reasoning</comments><proxy>ccsd</proxy><doi>10.1007/978-3-662-48899-7_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to prove the decidability of provability in several
well-known inference systems. This method generalizes both cut-elimination and
the construction of an automaton recognizing the provable propositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01488</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01488</id><created>2016-01-07</created><authors><author><keyname>Hejazi</keyname><forenames>Mohsen</forenames></author><author><keyname>Azimi-Abarghouyi</keyname><forenames>Seyed Mohammad</forenames></author><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author></authors><title>Robust Successive Compute-and-Forward over Multi-User Multi-Relay
  Networks</title><categories>cs.IT math.IT</categories><comments>44 pages, 10 figures, 1 table, accepted to be published in IEEE
  Trans. on Vehicular Tech</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops efficient Compute-and-forward (CMF) schemes in multi-user
multi-relay networks. To solve the rank failure problem in CMF setups and to
achieve full diversity of the network, we introduce two novel CMF methods,
namely, extended CMF and successive CMF. The former, having low complexity, is
based on recovering multiple equations at relays. The latter utilizes
successive interference cancellation (SIC) to enhance the system performance
compared to the state-of-the-art schemes. Both methods can be utilized in a
network with different number of users, relays, and relay antennas, with
negligible feedback channels or signaling overhead. We derive new concise
formulations and explicit framework for the successive CMF method as well as an
approach to reduce its computational complexity. Our theoretical analysis and
computer simulations demonstrate the superior performance of our proposed CMF
methods over the conventional schemes. Furthermore, based on our simulation
results, the successive CMF method yields additional signal-to-noise ratio
gains and shows considerable robustness against channel estimation error,
compared to the extended CMF method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01492</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01492</id><created>2016-01-07</created><authors><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>Complexity of Shift Bribery in Committee Elections</title><categories>cs.AI cs.GT</categories><comments>27 Pages. An extended abstract of this paper appears in the
  Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI 16)</comments><msc-class>91B14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the (parameterized) complexity of SHIFT BRIBERY for multiwinner
voting rules. We focus on SNTV, Bloc, k-Borda, and Chamberlin-Courant, as well
as on approximate variants of Chamberlin-Courant, since the original rule is
NP-hard to compute. We show that SHIFT BRIBERY tends to be significantly harder
in the multiwinner setting than in the single-winner one by showing settings
where SHIFT BRIBERY is easy in the single-winner cases, but is hard (and hard
to approximate) in the multiwinner ones. Moreover, we show that the
non-monotonicity of those rules which are based on approximation algorithms for
the Chamberlin-Courant rule sometimes affects the complexity of SHIFT BRIBERY.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01497</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01497</id><created>2016-01-07</created><authors><author><keyname>Yankovskaya</keyname><forenames>Anna</forenames></author><author><keyname>Yamshanov</keyname><forenames>Artem</forenames></author></authors><title>Family of 2-simplex cognitive tools and their application for
  decision-making and its justifications</title><categories>cs.HC</categories><comments>14 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urgency of application and development of cognitive graphic tools for usage
in intelligent systems of data analysis, decision making and its justifications
is given. Cognitive graphic tool &quot;2-simplex prism&quot; and examples of its usage
are presented. Specificity of program realization of cognitive graphics tools
invariant to problem areas is described. Most significant results are given and
discussed. Future investigations are connected with usage of new approach to
rendering, cross-platform realization, cognitive features improving and
expanding of n-simplex family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01498</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01498</id><created>2016-01-07</created><authors><author><keyname>Gholami</keyname><forenames>Ali</forenames></author><author><keyname>Laure</keyname><forenames>Erwin</forenames></author></authors><title>Security and Privacy of Sensitive Data in Cloud Computing: A Survey of
  Recent Developments</title><categories>cs.CR cs.DC</categories><doi>10.5121/csit.2015.51611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is revolutionizing many ecosystems by providing organizations
with computing resources featuring easy deployment, connectivity,
configuration, automation and scalability. This paradigm shift raises a broad
range of security and privacy issues that must be taken into consideration.
Multi-tenancy, loss of control, and trust are key challenges in cloud computing
environments. This paper reviews the existing technologies and a wide array of
both earlier and state-of-the-art projects on cloud security and privacy. We
categorize the existing research according to the cloud reference architecture
orchestration, resource control, physical resource, and cloud service
management layers, in addition to reviewing the existing developments in
privacy-preserving sensitive data approaches in cloud computing such as privacy
threat modeling and privacy enhancing protocols and solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01500</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01500</id><created>2016-01-07</created><authors><author><keyname>Gholami</keyname><forenames>Ali</forenames></author><author><keyname>Laure</keyname><forenames>Erwin</forenames></author></authors><title>Advanced Cloud Privacy Threat Modeling</title><categories>cs.SE cs.CR cs.DC</categories><doi>10.5121/csit.2016.60120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy-preservation for sensitive data has become a challenging issue in
cloud computing. Threat modeling as a part of requirements engineering in
secure software development provides a structured approach for identifying
attacks and proposing countermeasures against the exploitation of
vulnerabilities in a system . This paper describes an extension of Cloud
Privacy Threat Modeling (CPTM) methodology for privacy threat modeling in
relation to processing sensitive data in cloud computing environments. It
describes the modeling methodology that involved applying Method Engineering to
specify characteristics of a cloud privacy threat modeling methodology,
different steps in the proposed methodology and corresponding products. We
believe that the extended methodology facilitates the application of a
privacy-preserving cloud software development approach from requirements
engineering to design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01502</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01502</id><created>2016-01-07</created><updated>2016-01-17</updated><authors><author><keyname>Ai</keyname><forenames>Jingmei</forenames></author><author><keyname>Honold</keyname><forenames>Thomas</forenames></author><author><keyname>Liu</keyname><forenames>Haiteng</forenames></author></authors><title>The Expurgation-Augmentation Method for Constructing Good Plane Subspace
  Codes</title><categories>math.CO cs.IT math.IT</categories><comments>44 pages, 3 tables, 1 figure; part of the results was presented at
  the International Workshop on Algebraic Combinatorics at Zhejiang University,
  Hangzhou, September 2015; Version 2 contains minor corrections</comments><msc-class>94B05, 05B25, 51E20 (Primary), 51E14, 51E22, 51E23 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As shown in [28], one of the five isomorphism types of optimal binary
subspace codes of size 77 for packet length v=6, constant dimension k=3 and
minimum subspace distance d=4 can be constructed by first expurgating and then
augmenting the corresponding lifted Gabidulin code in a fairly simple way. The
method was refined in [32,26] to yield an essentially computer-free
construction of a currently best-known plane subspace code of size 329 for
(v,k,d)=(7,3,4). In this paper we generalize the expurgation-augmentation
approach to arbitrary packet length v, providing both a detailed theoretical
analysis of our method and computational results for small parameters. As it
turns out, our method is capable of producing codes larger than those obtained
by the echelon-Ferrers construction and its variants. We are able to prove this
observation rigorously for packet lengths v = 3 mod 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01504</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01504</id><created>2016-01-07</created><authors><author><keyname>Johnsen</keyname><forenames>Trygve</forenames></author><author><keyname>Verdure</keyname><forenames>Hugues</forenames></author></authors><title>Generalized Hamming weights for almost affine codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define generalized Hamming weights for almost affine codes. We show how
various aspects and applications of generalized Hamming weights for linear
codes, such as Wei duality, generalized Kung's bound, profiles, connection to
wire-tap channels of type II, apply to the larger class of almost affine codes
in general. In addition we discuss duality of almost affine codes,and of the
smaller class of multilinear codes. We also give results about weight
distributions of infinite series of almost affine codes, each series obtained
from a fixed code by extending the code alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01507</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01507</id><created>2016-01-07</created><authors><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author></authors><title>Fast Kronecker product kernel methods via sampled vec trick</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kronecker product kernel provides the standard approach in the kernel methods
literature for learning from pair-input data, where both data points and
prediction tasks have their own feature representations. The methods allow
simultaneous generalization to both new tasks and data unobserved in the
training set, a setting known as zero-shot or zero-data learning. Such a
setting occurs in numerous applications, including drug-target interaction
prediction, collaborative filtering and information retrieval. Efficient
training algorithms based on the so-called vec trick, that makes use of the
special structure of the Kronecker product, are known for the case where the
output matrix for the training set is fully observed, i.e. the correct output
for each data point-task combination is available. In this work we generalize
these results, proposing an efficient algorithm for sampled Kronecker product
multiplication, where only a subset of the full Kronecker product is computed.
This allows us to derive a general framework for training Kronecker kernel
methods, as specific examples we implement Kronecker ridge regression and
support vector machine algorithms. Experimental results demonstrate that the
proposed approach leads to accurate models, while allowing order of magnitude
improvements in training and prediction time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01517</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01517</id><created>2016-01-07</created><authors><author><keyname>Razafindramintsa</keyname><forenames>Jean Luc</forenames></author><author><keyname>Mahatody</keyname><forenames>Thomas</forenames></author><author><keyname>Razafimandimby</keyname><forenames>Josvah Paul</forenames></author></authors><title>Elaborate lexicon extended language with a lot of conceptual information</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Sciences , Engineering and
  Application (IJCSEA) december 2015</journal-ref><doi>10.5121/ijcsea.2015.5601</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The use of model such as LEL (Lexicon Extended Language) in natural language
is very interesting in Requirements Engineering. But LEL, even if it is derived
from the Universe of Discourse (UofD) does not provide further details on the
concepts it describes. However, we believe that the elements inherent in the
conceptual level of a system are already defined in the Universe of Discourse.
Therefore, in this work we propose a more elaborate natural language model
called eLEL. It is a model that describes the concepts in a domain in more
detail than the conventional LEL. We also propose a modeling process of a
domain using an eLEL model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01523</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01523</id><created>2016-01-07</created><authors><author><keyname>Dross</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Montassier</keyname><forenames>Mickael</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>Partitioning a triangle-free planar graph into a forest and a forest of
  bounded degree</title><categories>cs.DM math.CO</categories><comments>16 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $({\cal F},{\cal F}_d)$-partition of a graph is a vertex-partition into
two sets $F$ and $F_d$ such that the graph induced by $F$ is a forest and the
one induced by $F_d$ is a forest with maximum degree at most $d$. We prove that
every triangle-free planar graph admits an $({\cal F},{\cal F}_5)$-partition.
Moreover we show that if for some integer $d$ there exists a triangle-free
planar graph that does not admit an $({\cal F},{\cal F}_d)$-partition, then it
is an NP-complete problem to decide whether a triangle-free planar graph admits
such a partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01526</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01526</id><created>2016-01-07</created><authors><author><keyname>Xu</keyname><forenames>Shengfeng</forenames></author><author><keyname>Zhu</keyname><forenames>Gang</forenames></author><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Shichao</forenames></author><author><keyname>Zhong</keyname><forenames>Zhangdui</forenames></author></authors><title>Delay-Aware Dynamic Resource Management for High-Speed Railway Wireless
  Communications</title><categories>cs.NI</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the delay-aware dynamic resource management
problem for multi-service transmission in high-speed railway wireless
communications, with a focus on resource allocation among the services and
power control along the time. By taking account of average delay requirements
and power constraints, the considered problem is formulated into a stochastic
optimization problem, rather than pursuing the traditional convex optimization
means. Inspired by Lyapunov optimization theory, the intractable stochastic
optimization problem is transformed into a tractable deterministic optimization
problem, which is a mixed-integer resource management problem. By exploiting
the specific problem structure, the mixed-integer resource management problem
is equivalently transformed into a single variable problem, which can be
effectively solved by the golden section search method with guaranteed global
optimality. Finally, we propose a dynamic resource management algorithm to
solve the original stochastic optimization problem. Simulation results show the
advantage of the proposed dynamic algorithm and reveal that there exists a
fundamental tradeoff between delay requirements and power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01527</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01527</id><created>2016-01-07</created><authors><author><keyname>Oham</keyname><forenames>Chuka</forenames></author><author><keyname>Radenkovic</keyname><forenames>Milena</forenames></author></authors><title>Congestion Aware Spray And Wait Protocol: A Congestion Control Mechanism
  For The Vehicular Delay Tolerant Network</title><categories>cs.NI</categories><comments>13 pages, 9 figures</comments><journal-ref>IJCSIT Vol 7, No 6, December 2015, pp. 83-95</journal-ref><doi>10.5121/ijcsit.2015.7607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years, the Vehicular Ad-hoc Network (VANET) has come to be an
important area of research. Significant research has been conducted to improve
the performance of VANETS. One output of further research conducted on VANET is
the Vehicular Delay Tolerant Network (VDTN). It is an application of the mobile
DTN where nodes relay messages in the network using a store-carry-forward
approach. Due to its high mobility, it suffers frequent disconnections and also
congestions at nodes which leads to message drops. To minimize the rate of
message drops and so optimize the probability of message delivery so that
drivers are increasingly aware of the situation of the road, we propose a
congestion control mechanism: Congestion Aware Spray and Wait (CASaW) protocol
in this work so as to optimize the rate of message delivery to its destination
and so increase the awareness of drivers in the vehicular environment thereby
improve road safety. The results have shown that our proposition performed
better than other classical VDTN protocols in terms of message delivery
probability and rate of packet drops performance measures. We used the
Opportunistic Networking Environment (ONE) simulator to implement the classical
VDTN protocols: the PROPHET protocol, the Epidemic protocol, the MaxProp
protocol and the Spray and Wait Protocol. The simulation scenarios shows a
better performance for the congestion control mechanism we propose as it
maintains a good message delivery rate as well as minimize the rate of packet
losses thereby optimizing the chances of messages getting to their destinations
and so improve road safety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01530</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01530</id><created>2016-01-07</created><authors><author><keyname>Kurata</keyname><forenames>Gakuto</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author><author><keyname>Yu</keyname><forenames>Mo</forenames></author></authors><title>Leveraging Sentence-level Information with Encoder LSTM for Natural
  Language Understanding</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Network (RNN) and one of its specific architectures, Long
Short-Term Memory (LSTM), have been widely used for sequence labeling. In this
paper, we first enhance LSTM-based sequence labeling to explicitly model label
dependencies. Then we propose another enhancement to incorporate the global
information spanning over the whole input sequence. The latter proposed method,
encoder-labeler LSTM, first encodes the whole input sequence into a fixed
length vector with the encoder LSTM, and then uses this encoded vector as the
initial state of another LSTM for sequence labeling. Combining these methods,
we can predict the label sequence with considering label dependencies and
information of whole input sequence. In the experiments of a slot filling task,
which is an essential component of natural language understanding, with using
the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01532</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01532</id><created>2016-01-07</created><updated>2016-02-09</updated><authors><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Pattinson</keyname><forenames>Dirk</forenames></author><author><keyname>Wi&#xdf;mann</keyname><forenames>Thorsten</forenames></author></authors><title>A New Foundation for Finitary Corecursion</title><categories>math.CT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contributes to a uniform theory of the behaviour of &quot;finite-state&quot;
systems. We propose that such systems are modeled as coalgebras with a finitely
generated carrier for an endofunctor on a locally finitely presentable
category. Their behaviour gives rise to a new fixpoint of the coalgebraic type
functor called locally finite fixpoint (LFF). We prove that if the given
endofunctor preserves monos the LFF always exists and is a subcoalgebra of the
final coalgebra (unlike the rational fixpoint previously studied by Adamek,
Milius and Velebil). Moreover, we show that the LFF is characterized by two
universal properties: 1. as the final locally finitely generated coalgebra, and
2. as the initial fg-iterative algebra. As examples of LFF's we first obtain
the known instances of the rational fixpoint, e.g. regular languages, rational
streams and formal power-series, regular trees etc. And we obtain a number of
new examples, e.g. context-free languages, context-free formal power-series
(and any other instance of the generalized powerset construction by Silva,
Bonchi, Bonsangue, and Rutten) and the monad of Courcelle's algebraic trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01539</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01539</id><created>2016-01-07</created><authors><author><keyname>Simon</keyname><forenames>Joerg</forenames></author><author><keyname>Schmidt</keyname><forenames>Peter</forenames></author><author><keyname>Pammer-Schindler</keyname><forenames>Viktoria</forenames></author></authors><title>Analysis of Differential Synchronisation's Energy Consumption on Mobile
  Devices</title><categories>cs.NI</categories><comments>this is pre-published work, article submitted to the EAI Endorsed
  Transactions on 24.12.2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronisation algorithms are central to collaborative editing software. As
collaboration is increasingly mediated by mobile devices, the energy efficiency
for such algorithms is interest to a wide community of application developers.
In this paper we explore the differential synchronisation (diffsync) algorithm
with respect to energy consumption on mobile devices. Discussions within this
paper are based on real usage data of PDF annotations via the Mendeley iOS app,
which requires realtime synchronisation.
  We identify three areas for optimising diffsync: a.) Empty cycles in which no
changes need to be processed b.) tail energy by adapting cycle intervals and
c.) computational complexity. Following these considerations, we propose a
push-based diffsync strategy in which synchronisation cycles are triggered when
a device connects to the network or when a device is notified of changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01544</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01544</id><created>2016-01-07</created><authors><author><keyname>Benavoli</keyname><forenames>Alessio</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>State Space representation of non-stationary Gaussian Processes</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state space (SS) representation of Gaussian processes (GP) has recently
gained a lot of interest. The main reason is that it allows to compute GPs
based inferences in O(n), where $n$ is the number of observations. This
implementation makes GPs suitable for Big Data. For this reason, it is
important to provide a SS representation of the most important kernels used in
machine learning. The aim of this paper is to show how to exploit the transient
behaviour of SS models to map non-stationary kernels to SS models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01546</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01546</id><created>2016-01-07</created><updated>2016-01-11</updated><authors><author><keyname>Luttik</keyname><forenames>Bas</forenames></author><author><keyname>Yang</keyname><forenames>Fei</forenames></author></authors><title>On the Executability of Interactive Computation</title><categories>cs.LO</categories><comments>15 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of interactive Turing machines (ITMs) has been proposed to
characterise which stream translations are interactively computable; the model
of reactive Turing machines (RTMs) has been proposed to characterise which
behaviours are reactively executable. In this article we provide a comparison
of the two models. We show, on the one hand, that the behaviour exhibited by
ITMs is reactively executable, and, on the other hand, that the stream
translations naturally associated with RTMs are interactively computable. We
conclude from these results that the theory of reactive executability subsumes
the theory of interactive computability. Inspired by the existing model of ITMs
with advice, which provides a model of evolving computation, we also consider
RTMs with advice and we establish that a facility of advice considerably
upgrades the behavioural expressiveness of RTMs: every countable transition
system can be simulated by some RTM with advice up to a fine notion of
behavioural equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01549</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01549</id><created>2016-01-07</created><authors><author><keyname>Abeywickrama</keyname><forenames>Tenindra</forenames></author><author><keyname>Cheema</keyname><forenames>Muhammad Aamir</forenames></author><author><keyname>Taniar</keyname><forenames>David</forenames></author></authors><title>k-Nearest Neighbors on Road Networks: A Journey in Experimentation and
  In-Memory Implementation</title><categories>cs.DS</categories><comments>To appear in PVLDB 2016</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A k nearest neighbor (kNN) query on road networks retrieves the k closest
points of interest (POIs) by their network distances from a given location.
Today, in the era of ubiquitous mobile computing, this is a highly pertinent
query. While Euclidean distance has been used as a heuristic to search for the
closest POIs by their road network distance, its efficacy has not been
thoroughly investigated. The most recent methods have shown significant
improvement in query performance. Earlier studies, which proposed disk-based
indexes, were compared to the current state-of-the-art in main memory. However,
recent studies have shown that main memory comparisons can be challenging and
require careful adaptation. This paper presents an extensive experimental
investigation in main memory to settle these and several other issues. We use
efficient and fair memory-resident implementations of each method to reproduce
past experiments and conduct additional comparisons for several overlooked
evaluations. Notably we revisit a previously discarded technique (IER) showing
that, through a simple improvement, it is often the best performing technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01556</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01556</id><created>2016-01-07</created><updated>2016-01-12</updated><authors><author><keyname>Grangel-Gonz&#xe1;lez</keyname><forenames>Irl&#xe1;n</forenames></author><author><keyname>Halilaj</keyname><forenames>Lavdim</forenames></author><author><keyname>Coskun</keyname><forenames>G&#xf6;khan</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Collarana</keyname><forenames>Diego</forenames></author><author><keyname>Hoffmeister</keyname><forenames>Michael</forenames></author></authors><title>Towards a Semantic Administrative Shell for Industry 4.0 Components</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the engineering and manufacturing domain, there is currently an atmosphere
of departure to a new era of digitized production. In different regions,
initiatives in these directions are known under different names, such as
industrie du futur in France, industrial internet in the US or Industrie 4.0 in
Germany. While the vision of digitizing production and manufacturing gained
much traction lately, it is still relatively unclear how this vision can
actually be implemented with concrete standards and technologies. Within the
German Industry 4.0 initiative, the concept of an Administrative Shell was
devised to respond to these requirements. The Administrative Shell is planned
to provide a digital representation of all information being available about
and from an object which can be a hardware system or a software platform. In
this paper, we present an approach to developing such a digital representation
based on semantic knowledge representation formalisms such as RDF, RDF Schema
and OWL. We present our concept of a Semantic I4.0 Component which addresses
the communication and comprehension challenges in Industry 4.0 scenarios using
semantic technologies. Our approach is illustrated with a concrete example
showing its benefits in a real-world use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01566</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01566</id><created>2016-01-07</created><authors><author><keyname>Miseikis</keyname><forenames>Justinas</forenames></author><author><keyname>Glette</keyname><forenames>Kyrre</forenames></author><author><keyname>Elle</keyname><forenames>Ole Jakob</forenames></author><author><keyname>Torresen</keyname><forenames>Jim</forenames></author></authors><title>Automatic Calibration of a Robot Manipulator and Multi 3D Camera System</title><categories>cs.RO</categories><comments>ICRA 2016 Submission, 6 pages, 8 figures, ROBIN group, University of
  Oslo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With 3D sensing becoming cheaper, environment-aware robot arms capable of
safely working in collaboration with humans will become common. However, a
reliable calibration is needed, both for camera internal calibration, as well
as Eye-to-Hand calibration, to make sure the whole system functions correctly.
We present a framework, using a novel combination of well proven methods,
allowing a quick automatic calibration of the system consisting of the robot
and a varying number of 3D cameras by using a standard checkerboard calibration
grid. It is based on a modular design for an easy adaptation after any hardware
or algorithm changes. Our approach allows a quick camera-to-robot recalibration
after any changes to the setup, for example when cameras or robot were
repositioned. The framework has been proven to work by practical experiments to
analyze the quality of the calibration versus the number of positions of the
checkerboard used for each of the calibration procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01576</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01576</id><created>2016-01-07</created><updated>2016-02-29</updated><authors><author><keyname>Klimek</keyname><forenames>Peter</forenames></author><author><keyname>Diakonova</keyname><forenames>Marina</forenames></author><author><keyname>Eguiluz</keyname><forenames>Victor</forenames></author><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Dynamical origins of the community structure of multi-layer societies</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social structures emerge as a result of individuals managing a variety of
different of social relationships. Societies can be represented as highly
structured dynamic multiplex networks. Here we study the dynamical origins of
the specific community structures of a large-scale social multiplex network of
a human society that interacts in a virtual world of a massive multiplayer
online game. There we find substantial differences in the community structures
of different social actions, represented by the various network layers in the
multiplex. Community size distributions are either similar to a power-law or
appear to be centered around a size of 50 individuals. To understand these
observations we propose a voter model that is built around the principle of
triadic closure. It explicitly models the co-evolution of node- and
link-dynamics across different layers of the multiplex. Depending on link- and
node fluctuation rates, the model exhibits an anomalous shattered fragmentation
transition, where one layer fragments from one large component into many small
components. The observed community size distributions are in good agreement
with the predicted fragmentation in the model. We show that the empirical
pairwise similarities of network layers, in terms of link overlap and degree
correlations, practically coincide with the model. This suggests that several
detailed features of the fragmentation in societies can be traced back to the
triadic closure processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01577</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01577</id><created>2016-01-07</created><authors><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Fiaz</keyname><forenames>Mustansar</forenames></author><author><keyname>Kwon</keyname><forenames>Soon-il</forenames></author><author><keyname>Sodanil</keyname><forenames>Maleerat</forenames></author><author><keyname>Vo</keyname><forenames>Bay</forenames></author><author><keyname>Baik</keyname><forenames>Sung Wook</forenames></author></authors><title>Gender Identification using MFCC for Telephone Applications - A
  Comparative Study</title><categories>cs.SD</categories><journal-ref>International Journal of Computer Science and Electronics
  Engineering 3.5 (2015): 351-355</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gender recognition is an essential component of automatic speech recognition
and interactive voice response systems. Determining gender of the speaker
reduces the computational burden of such systems for any further processing.
Typical methods for gender recognition from speech largely depend on features
extraction and classification processes. The purpose of this study is to
evaluate the performance of various state-of-the-art classification methods
along with tuning their parameters for helping selection of the optimal
classification methods for gender recognition tasks. Five classification
schemes including k-nearest neighbor, na\&quot;ive Bayes, multilayer perceptron,
random forest, and support vector machine are comprehensively evaluated for
determination of gender from telephonic speech using the Mel-frequency cepstral
coefficients. Different experiments were performed to determine the effects of
training data sizes, length of the speech streams, and parameter tuning on
classification performance. Results suggest that SVM is the best classifier
among all the five schemes for gender recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01579</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01579</id><created>2016-01-07</created><authors><author><keyname>Jacquemard</keyname><forenames>Florent</forenames></author><author><keyname>Segoufin</keyname><forenames>Luc</forenames></author><author><keyname>Dimino</keyname><forenames>Jer&#xe9;mie</forenames></author></authors><title>FO2(&lt;,+1,~) on data trees, data tree automata and branching vector
  addition systems</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A data tree is an unranked ordered tree where each node carries a label from
a finite alphabet and a datum from some infinite domain. We consider the two
variable first order logic FO2(&lt;,+1,~) over data trees. Here +1 refers to the
child and the next sibling relations while &lt; refers to the descendant and
following sibling relations. Moreover, ~ is a binary predicate testing data
equality. We exhibit an automata model, denoted DAD# that is more expressive
than FO2(&lt;,+1,~) but such that emptiness of DAD# and satisfiability of
FO2(&lt;,+1,~) are inter-reducible. This is proved via a model of counter tree
automata, denoted EBVASS, that extends Branching Vector Addition Systems with
States (BVASS) with extra features for merging counters. We show that, as
decision problems, reachability for EBVASS, satisfiability of FO2(&lt;,+1,~) and
emptiness of DAD# are equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01582</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01582</id><created>2016-01-07</created><authors><author><keyname>Antonoyiannakis</keyname><forenames>Manolis</forenames></author></authors><title>Highlighting Impact and the Impact of Highlighting: PRB Editors'
  Suggestions</title><categories>physics.soc-ph cs.DL</categories><comments>Editorial</comments><journal-ref>Physical Review B 92, 210001 (2015)</journal-ref><doi>10.1103/PhysRevB.92.210001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associate Editor Manolis Antonoyiannakis discusses the highlighting, as
Editors' Suggestions, of a small percentage of the papers published each week.
We highlight papers primarily for their importance and impact in their
respective fields, or because we find them particularly interesting or elegant.
It turns out that the additional layer of scrutiny involved in the selection of
papers as Editors' Suggestions is associated with a significantly elevated and
sustained citation impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01586</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01586</id><created>2016-01-07</created><authors><author><keyname>Bizjak</keyname><forenames>Ale&#x161;</forenames></author><author><keyname>Grathwohl</keyname><forenames>Hans Bugge</forenames></author><author><keyname>Clouston</keyname><forenames>Ranald</forenames></author><author><keyname>M&#xf8;gelberg</keyname><forenames>Rasmus E.</forenames></author><author><keyname>Birkedal</keyname><forenames>Lars</forenames></author></authors><title>Guarded Dependent Type Theory with Coinductive Types</title><categories>cs.LO cs.PL</categories><comments>This is the technical report version of a paper to appear in the
  proceedings of FoSSaCS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present guarded dependent type theory, gDTT, an extensional dependent type
theory with a `later' modality and clock quantifiers for programming and
proving with guarded recursive and coinductive types. The later modality is
used to ensure the productivity of recursive definitions in a modular, type
based, way. Clock quantifiers are used for controlled elimination of the later
modality and for encoding coinductive types using guarded recursive types. Key
to the development of gDTT are novel type and term formers involving what we
call `delayed substitutions'. These generalise the applicative functor rules
for the later modality considered in earlier work, and are crucial for
programming and proving with dependent types. We show soundness of the type
theory with respect to a denotational model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01587</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01587</id><created>2016-01-07</created><authors><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>Vester</keyname><forenames>Steen</forenames></author></authors><title>Distributed Synthesis in Continuous Time</title><categories>cs.DC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a formalism modelling communication of distributed agents
strictly in continuous-time. Within this framework, we study the problem of
synthesising local strategies for individual agents such that a specified set
of goal states is reached, or reached with at least a given probability. The
flow of time is modelled explicitly based on continuous-time randomness, with
two natural implications: First, the non-determinism stemming from interleaving
disappears. Second, when we restrict to a subclass of non-urgent models, the
quantitative value problem for two players can be solved in EXPTIME. Indeed,
the explicit continuous time enables players to communicate their states by
delaying synchronisation (which is unrestricted for non-urgent models). In
general, the problems are undecidable already for two players in the
quantitative case and three players in the qualitative case. The qualitative
undecidability is shown by a reduction to decentralized POMDPs for which we
provide the strongest (and rather surprising) undecidability result so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01597</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01597</id><created>2016-01-07</created><authors><author><keyname>Turner</keyname><forenames>Jonathan</forenames></author></authors><title>Grafalgo - A Library of Graph Algorithms and Supporting Data Structures
  (revised)</title><categories>cs.DS</categories><report-no>WUCSE-2016-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report provides an (updated) overview of {\sl Grafalgo}, an open-source
library of graph algorithms and the data structures used to implement them. The
programs in this library were originally written to support a graduate class in
advanced data structures and algorithms at Washington University. Because the
code's primary purpose was pedagogical, it was written to be as straightforward
as possible, while still being highly efficient. Grafalgo is implemented in C++
and incorporates some features of C++11.
  The library is available on an open-source basis and may be downloaded from
https://code.google.com/p/grafalgo/. Source code documentation is at
www.arl.wustl.edu/\textasciitilde jst/doc/grafalgo. While not designed as
production code, the library is suitable for use in larger systems, so long as
its limitations are understood. The readability of the code also makes it
relatively straightforward to extend it for other purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01598</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01598</id><created>2016-01-07</created><authors><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author><author><keyname>Igamberdiev</keyname><forenames>Alexander</forenames></author><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Klemz</keyname><forenames>Boris</forenames></author><author><keyname>Mchedlidze</keyname><forenames>Tamara</forenames></author><author><keyname>Scheucher</keyname><forenames>Manfred</forenames></author></authors><title>Strongly Monotone Drawings of Planar Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A straight-line drawing of a graph is a monotone drawing if for each pair of
vertices there is a path which is monotonically increasing in some direction,
and it is called a strongly monotone drawing if the direction of monotonicity
is given by the direction of the line segment connecting the two vertices.
  We present algorithms to compute crossing-free strongly monotone drawings for
some classes of planar graphs; namely, 3-connected planar graphs, outerplanar
graphs, and 2-trees. The drawings of 3-connected planar graphs are based on
primal-dual circle packings. Our drawings of outerplanar graphs are based on a
new algorithm that constructs strongly monotone drawings of trees which are
also convex. For irreducible trees, these drawings are strictly convex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01607</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01607</id><created>2016-01-07</created><authors><author><keyname>Merelo</keyname><forenames>Juan-J.</forenames></author><author><keyname>Garc&#xed;a-Valdez</keyname><forenames>Mario</forenames></author><author><keyname>Castillo</keyname><forenames>Pedro A.</forenames></author><author><keyname>Garc&#xed;a-S&#xe1;nchez</keyname><forenames>Pablo</forenames></author><author><keyname>Cuevas</keyname><forenames>P. de las</forenames></author><author><keyname>Rico</keyname><forenames>Nuria</forenames></author></authors><title>NodIO, a JavaScript framework for volunteer-based evolutionary
  algorithms : first results</title><categories>cs.DC cs.NE</categories><comments>GeNeura 2006-01</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  JavaScript is an interpreted language mainly known for its inclusion in web
browsers, making them a container for rich Internet based applications. This
has inspired its use, for a long time, as a tool for evolutionary algorithms,
mainly so in browser-based volunteer computing environments. Several libraries
have also been published so far and are in use. However, the last years have
seen a resurgence of interest in the language, becoming one of the most popular
and thus spawning the improvement of its implementations, which are now the
foundation of many new client-server applications. We present such an
application for running distributed volunteer-based evolutionary algorithm
experiments, and we make a series of measurements to establish the speed of
JavaScript in evolutionary algorithms that can serve as a baseline for
comparison with other distributed computing experiments. These experiments use
different integer and floating point problems, and prove that the speed of
JavaScript is actually competitive with other languages commonly used by the
evolutionary algorithm practitioner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01608</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01608</id><created>2016-01-07</created><authors><author><keyname>Khan</keyname><forenames>Nabeel</forenames></author><author><keyname>Al-Yasiri</keyname><forenames>Adil</forenames></author></authors><title>Framework for cloud computing adoption: A road map for Smes to cloud
  migration</title><categories>cs.CY</categories><comments>15 Pages in International Journal on cloud computing : Services and
  Architecture</comments><journal-ref>International Journal on Cloud Computing: Services and
  Architecture (IJCCSA) Vol. 5, No. 5/6, December 2015</journal-ref><doi>10.5121/ijccsa.2015.5601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small and Medium size Enterprises (SME) are considered as a backbone of many
developing and developed economies of the world; they are the driving force to
any major economy across the globe. Through Cloud Computing firms outsource
their entire information technology (IT) process while concentrating more on
their core business. It allows businesses to cut down heavy cost incurred over
IT infrastructure without losing focus on customer needs. However, Cloud
industry to an extent has struggled to grow among SMEs due to the reluctance
and concerns expressed by them. Throughout the course of this study several
interviews were conducted and the literature was reviewed to understand how
cloud providers offer services and what challenges SMEs are facing. The study
identified issues like cloud knowledge, interoperability, security and
contractual concerns to be hindering SMEs adoption of cloud services. From the
interviews common practices followed by cloud vendors and what concerns SMEs
have were identified as a basis for a cloud framework which will bridge gaps
between cloud vendors and SMEs. A stepwise framework for cloud adoption is
formulated which identifies and provides recommendation to four most
predominant challenges which are hurting cloud industry and taking SMEs away
from cloud computing, as well as guide SMEs aiding in successful cloud
adoption. Moreover, this framework streamlines the cloud adoption process for
SMEs by removing ambiguity in regards to fundamentals associated with their
organisation and cloud adoption process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01611</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01611</id><created>2016-01-07</created><authors><author><keyname>Krstovski</keyname><forenames>Kriste</forenames></author><author><keyname>Smith</keyname><forenames>David A.</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author></authors><title>Automatic Construction of Evaluation Sets and Evaluation of Document
  Similarity Models in Large Scholarly Retrieval Systems</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieval systems for scholarly literature offer the ability for the
scientific community to search, explore and download scholarly articles across
various scientific disciplines. Mostly used by the experts in the particular
field, these systems contain user community logs including information on user
specific downloaded articles. In this paper we present a novel approach for
automatically evaluating document similarity models in large collections of
scholarly publications. Unlike typical evaluation settings that use test
collections consisting of query documents and human annotated relevance
judgments, we use download logs to automatically generate pseudo-relevant set
of similar document pairs. More specifically we show that consecutively
downloaded document pairs, extracted from a scholarly information retrieval
(IR) system, could be utilized as a test collection for evaluating document
similarity models. Another novel aspect of our approach lies in the method that
we employ for evaluating the performance of the model by comparing the
distribution of consecutively downloaded document pairs and random document
pairs in log space. Across two families of similarity models, that represent
documents in the term vector and topic spaces, we show that our evaluation
approach achieves very high correlation with traditional performance metrics
such as Mean Average Precision (MAP), while being more efficient to compute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01612</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01612</id><created>2015-12-30</created><authors><author><keyname>Marszalek</keyname><forenames>Wieslaw</forenames></author></authors><title>Memristive fingerprints of electric arcs</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the memristive fingerprints of the hybrid Cassie-Mayr model of
electric arcs. In particular, it is shown that (i) the voltage-current
characteristic of the model has the pinched hysteresis nature, (ii) the voltage
and current zero crossings occur at the same instants, and, (iii) when the
frequency $f$ of the power supply increases, the voltage-current pinched
hysteresis characteristic tends closer to a single-valued one, meaning that the
voltage-current graph becomes that of a resistor (with an increased linearity
for $f\rightarrow \infty$). The conductance $g$ of the Cassie-Mayr model
decreases when the frequency increases. The hybrid Cassie-Mayr model describes
therefore an interesting case of a memristive phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01614</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01614</id><created>2016-01-07</created><authors><author><keyname>Cl&#xe9;ment</keyname><forenames>Duhart</forenames></author><author><keyname>Cyrille</keyname><forenames>Bertelle</forenames></author></authors><title>Toward Organic Computing Approach for Cybernetic Responsive Environment</title><categories>cs.AI cs.MA cs.NI</categories><journal-ref>International Journal of Ambient Systems and Applications (IJASA),
  december 2015, Volume 3, Number 4</journal-ref><doi>10.5121/ijasa.2015.3401.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The developpment of the Internet of Things (IoT) concept revives Responsive
Environments (RE) technologies. Nowadays, the idea of a permanent connection
between physical and digital world is technologically possible. The capillar
Internet relates to the Internet extension into daily appliances such as they
become actors of Internet like any hu-man. The parallel development of
Machine-to-Machine communications and Arti cial Intelligence (AI) technics
start a new area of cybernetic. This paper presents an approach for Cybernetic
Organism (Cyborg) for RE based on Organic Computing (OC). In such approach,
each appli-ance is a part of an autonomic system in order to control a physical
environment. The underlying idea is that such systems must have self-x
properties in order to adapt their behavior to external disturbances with a
high-degree of autonomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01635</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01635</id><created>2016-01-07</created><updated>2016-02-16</updated><authors><author><keyname>Terletskyi</keyname><forenames>D. A.</forenames></author><author><keyname>Provotar</keyname><forenames>A. I.</forenames></author></authors><title>Fuzzy Object-Oriented Dynamic Networks. I</title><categories>cs.AI</categories><acm-class>I.2.4; D.1.5; D.3.3; F.4.1; E.2</acm-class><journal-ref>Cybernetics and Systems Analysis, 2015, Volume 51, Issue 1, pp
  34-40</journal-ref><doi>10.1007/s10559-015-9694-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concepts of fuzzy objects and their classes are described that make it
possible to structurally represent knowledge about fuzzy and partially-defined
objects and their classes. Operations over such objects and classes are also
proposed that make it possible to obtain sets and new classes of fuzzy objects
and also to model variations in object structures under the influence of
external factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01645</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01645</id><created>2016-01-07</created><authors><author><keyname>Valente</keyname><forenames>Luis</forenames><affiliation>Universidade Federal Fluminense</affiliation></author><author><keyname>Clua</keyname><forenames>Esteban</forenames><affiliation>Universidade Federal Fluminense</affiliation></author><author><keyname>Silva</keyname><forenames>Alexandre Ribeiro</forenames><affiliation>Instituto Federal do Tri&#xe2;ngulo Mineiro</affiliation></author><author><keyname>Feij&#xf3;</keyname><forenames>Bruno</forenames><affiliation>PUC-Rio</affiliation></author></authors><title>Live-action Virtual Reality Games</title><categories>cs.HC</categories><comments>10 pages, technical report published at &quot;Monografias em Ci\^encia da
  Computa\c{c}\~ao, PUC-Rio&quot; (ISSN 0103-9741), MCC03/15, July 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the concept of &quot;live-action virtual reality games&quot; as a
new genre of digital games based on an innovative combination of live-action,
mixed-reality, context-awareness, and interaction paradigms that comprise
tangible objects, context-aware input devices, and embedded/embodied
interactions. Live-action virtual reality games are &quot;live-action games&quot; because
a player physically acts out (using his/her real body and senses) his/her
&quot;avatar&quot; (his/her virtual representation) in the game stage, which is the
mixed-reality environment where the game happens. The game stage is a kind of
&quot;augmented virtuality&quot;; a mixed-reality where the virtual world is augmented
with real-world information. In live-action virtual reality games, players wear
HMD devices and see a virtual world that is constructed using the physical
world architecture as the basic geometry and context information. Physical
objects that reside in the physical world are also mapped to virtual elements.
Live-action virtual reality games keeps the virtual and real-worlds
superimposed, requiring players to physically move in the environment and to
use different interaction paradigms (such as tangible and embodied interaction)
to complete game activities. This setup enables the players to touch physical
architectural elements (such as walls) and other objects, &quot;feeling&quot; the game
stage. Players have free movement and may interact with physical objects placed
in the game stage, implicitly and explicitly. Live-action virtual reality games
differ from similar game concepts because they sense and use contextual
information to create unpredictable game experiences, giving rise to emergent
gameplay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01648</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01648</id><created>2016-01-07</created><authors><author><keyname>Damm</keyname><forenames>Werner</forenames></author><author><keyname>Horbach</keyname><forenames>Matthias</forenames></author><author><keyname>Sofronie-Stokkermans</keyname><forenames>Viorica</forenames></author></authors><title>Decidability of Verification of Safety Properties of Spatial Families of
  Linear Hybrid Automata</title><categories>cs.LO</categories><comments>50 pages, AVACS Technical Report No. 111 (SFB/TR 14 AVACS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider systems composed of an unbounded number of uniformly designed
linear hybrid automata, whose dynamic behavior is determined by their relation
to neighboring systems. We present a class of such systems and a class of
safety properties whose verification can be reduced to the verification of
(small) families of neighbouring systems of bounded size, and identify
situations in which such verification problems are decidable, resp. fixed
parameter tractable. We illustrate the approach with an example from
coordinated vehicle guidance, and describe an implementation which allows us to
perform such verification tasks automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01653</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01653</id><created>2016-01-07</created><authors><author><keyname>Ma'ayan</keyname><forenames>Avi</forenames></author><author><keyname>Clark</keyname><forenames>Neil R.</forenames></author></authors><title>Large Collection of Diverse Gene Set Search Queries Recapitulate Known
  Protein-Protein Interactions and Gene-Gene Functional Associations</title><categories>q-bio.MN cs.AI cs.SI q-bio.GN stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular online enrichment analysis tools from the field of molecular systems
biology provide users with the ability to submit their experimental results as
gene sets for individual analysis. Such queries are kept private, and have
never before been considered as a resource for integrative analysis. By
harnessing gene set query submissions from thousands of users, we aim to
discover biological knowledge beyond the scope of an individual study. In this
work, we investigated a large collection of gene sets submitted to the tool
Enrichr by thousands of users. Based on co-occurrence, we constructed a global
gene-gene association network. We interpret this inferred network as providing
a summary of the structure present in this crowdsourced gene set library, and
show that this network recapitulates known protein-protein interactions and
functional associations between genes. This finding implies that this network
also offers predictive value. Furthermore, we visualize this gene-gene
association network using a new edge-pruning algorithm that retains both the
local and global structures of large-scale networks. Our ability to make
predictions for currently unknown gene associations, that may not be captured
by individual researchers and data sources, is a demonstration of the potential
of harnessing collective knowledge from users of popular tools in the field of
molecular systems biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01654</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01654</id><created>2016-01-07</created><authors><author><keyname>Rezagah</keyname><forenames>Farideh Ebrahim</forenames></author><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Compression-Based Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern compression algorithms exploit complex structures that are present in
signals to describe them very efficiently. On the other hand, the field of
compressed sensing is built upon the observation that &quot;structured&quot; signals can
be recovered from their under-determined set of linear projections. Currently,
there is a large gap between the complexity of the structures studied in the
area of compressed sensing and those employed by the state-of-the-art
compression codes. Recent results in the literature on deterministic signals
aim at bridging this gap through devising compressed sensing decoders that
employ compression codes. This paper focuses on structured stochastic processes
and studies the application of rate-distortion codes to compressed sensing of
such signals. The performance of the formerly-proposed compressible signal
pursuit (CSP) algorithm is studied in this stochastic setting. It is proved
that in the very low distortion regime, as the blocklength grows to infinity,
the CSP algorithm reliably and robustly recovers $n$ instances of a stationary
process from random linear projections as long as their count is slightly more
than $n$ times the rate-distortion dimension (RDD) of the source. It is also
shown that under some regularity conditions, the RDD of a stationary process is
equal to its information dimension (ID). This connection establishes the
optimality of the CSP algorithm at least for memoryless stationary sources, for
which the fundamental limits are known. Finally, it is shown that the CSP
algorithm combined by a family of universal variable-length fixed-distortion
compression codes yields a family of universal compressed sensing recovery
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01660</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01660</id><created>2016-01-07</created><authors><author><keyname>Neider</keyname><forenames>Daniel</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>An Automaton Learning Approach to Solving Safety Games over Infinite
  Graphs</title><categories>cs.FL cs.LG cs.LO</categories><msc-class>05C57</msc-class><acm-class>F.1.1; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to construct finite-state reactive controllers for
systems whose interactions with their adversarial environment are modeled by
infinite-duration two-player games over (possibly) infinite graphs. The
proposed method targets safety games with infinitely many states or with such a
large number of states that it would be impractical---if not impossible---for
conventional synthesis techniques that work on the entire state space. We
resort to constructing finite-state controllers for such systems through an
automata learning approach, utilizing a symbolic representation of the
underlying game that is based on finite automata. Throughout the learning
process, the learner maintains an approximation of the winning region
(represented as a finite automaton) and refines it using different types of
counterexamples provided by the teacher until a satisfactory controller can be
derived (if one exists). We present a symbolic representation of safety games
(inspired by regular model checking), propose implementations of the learner
and teacher, and evaluate their performance on examples motivated by robotic
motion planning in dynamic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01675</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01675</id><created>2016-01-07</created><authors><author><keyname>Zhukov</keyname><forenames>Alexei</forenames></author><author><keyname>Kurbatsky</keyname><forenames>Victor</forenames></author><author><keyname>Tomin</keyname><forenames>Nikita</forenames></author><author><keyname>Sidorov</keyname><forenames>Denis</forenames></author><author><keyname>Panasetsky</keyname><forenames>Daniil</forenames></author><author><keyname>Foley</keyname><forenames>Aoife</forenames></author></authors><title>Ensemble Methods of Classification for Power Systems Security Assessment</title><categories>cs.AI cs.LG</categories><comments>6 pages, 4 figures, 4 tables. Submitted to PSSC</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most promising approaches for complex technical systems analysis
employs ensemble methods of classification. Ensemble methods enable to build a
reliable decision rules for feature space classification in the presence of
many possible states of the system. In this paper, novel techniques based on
decision trees are used for evaluation of the reliability of the regime of
electric power systems. We proposed hybrid approach based on random forests
models and boosting models. Such techniques can be applied to predict the
interaction of increasing renewable power, storage devices and swiching of
smart loads from intelligent domestic appliances, heaters and air-conditioning
units and electric vehicles with grid for enhanced decision making. The
ensemble classification methods were tested on the modified 118-bus IEEE power
system showing that proposed technique can be employed to examine whether the
power system is secured under steady-state operating conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01700</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01700</id><created>2016-01-08</created><authors><author><keyname>Murphy</keyname><forenames>Robert A.</forenames></author></authors><title>A Predictive Model using the Markov Property</title><categories>stat.ME cs.AI</categories><msc-class>60-04, 60-08, 60J10 60J20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a data set of numerical values which are sampled from some unknown
probability distribution, we will show how to check if the data set exhibits
the Markov property and we will show how to use the Markov property to predict
future values from the same distribution, with probability 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01705</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01705</id><created>2016-01-07</created><authors><author><keyname>Andreas</keyname><forenames>Jacob</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Klein</keyname><forenames>Dan</forenames></author></authors><title>Learning to Compose Neural Networks for Question Answering</title><categories>cs.CL cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a question answering model that applies to both images and
structured knowledge bases. The model uses natural language strings to
automatically assemble neural networks from a collection of composable modules.
Parameters for these modules are learned jointly with network-assembly
parameters via reinforcement learning, with only (world, question, answer)
triples as supervision. Our approach, which we term a dynamic neural model
network, achieves state-of-the-art results on benchmark datasets in both visual
and structured domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01722</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01722</id><created>2016-01-07</created><authors><author><keyname>Waern</keyname><forenames>Jonatan</forenames></author><author><keyname>Ekemark</keyname><forenames>Per</forenames></author><author><keyname>Koukos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Kaxiras</keyname><forenames>Stefanos</forenames></author><author><keyname>Jimborean</keyname><forenames>Alexandra</forenames></author></authors><title>Profiling-Assisted Decoupled Access-Execute</title><categories>cs.AR cs.DC</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As energy efficiency became a critical factor in the embedded systems domain,
dynamic voltage and frequency scaling (DVFS) techniques have emerged as means
to control the system's power and energy efficiency. Additionally, due to the
compact design, thermal issues become prominent. State of the art work promotes
software decoupled access-execution (DAE) that statically generates code
amenable to DVFS techniques. The compiler builds memory-bound access phases,
designed to prefetch data in the cache at low frequency, and compute-bound
phases, that consume the data and perform computations at high frequency. This
work investigates techniques to find the optimal balance between lightweight
and efficient access phases. A profiling step guides the selection of loads to
be prefetched in the access phase. For applications whose behavior vary
significantly with respect to the input data, the profiling can be performed
online, accompanied by just-in-time compilation. We evaluated the benefits in
energy efficiency and performance for both static and dynamic code generation
and showed that precise prefetching of critical loads can result in 20% energy
improvements, on average. DAE is particularly beneficial for embedded systems
as by alternating access phases (executed at low frequency) and execute phases
(at high frequency) DAE proactively reduces the temperature and therefore
prevents thermal emergencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01725</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01725</id><created>2016-01-07</created><authors><author><keyname>D'Osualdo</keyname><forenames>Emanuele</forenames></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>On Hierarchical Communication Topologies in the pi-calculus</title><categories>cs.PL cs.LO</categories><comments>42 pages, ESOP16. arXiv admin note: text overlap with
  arXiv:1502.00944</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the shape invariants satisfied by the
communication topology of {\pi}-terms, and the automatic inference of these
invariants. A {\pi}-term P is hierarchical if there is a finite forest T such
that the communication topology of every term reachable from P satisfies a
T-shaped invariant. We design a static analysis to prove a term hierarchical by
means of a novel type system that enjoys decidable inference. The soundness
proof of the type system employs a non-standard view of {\pi}-calculus
reactions. The coverability problem for hierarchical terms is decidable. This
is proved by showing that every hierarchical term is depth-bounded, an
undecidable property known in the literature. We thus obtain an expressive
static fragment of the {\pi}-calculus with decidable safety verification
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01736</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01736</id><created>2016-01-07</created><authors><author><keyname>Csirmaz</keyname><forenames>Elod Pal</forenames></author></authors><title>Algebraic File Synchronization: Adequacy and Completeness</title><categories>cs.DC cs.DM cs.DS</categories><msc-class>08A70 (Primary) 68M07, 68M14, 68P05, 68P20 (Secondary)</msc-class><acm-class>D.4.3, F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With distributed computing and mobile applications, synchronizing diverging
replicas of data structures is a more and more common problem. We use algebraic
methods to reason about filesystem operations, and introduce a simplified
definition of conflicting updates to filesystems. We also define algorithms for
update detection and reconciliation and present rigorous proofs that they not
only work as intended, but also cannot be improved on.
  To achieve this, we introduce a novel, symmetric set of filesystem commands
with higher information content, which removes edge cases and increases the
predictive powers of our algebraic model. We also present a number of generally
useful classes and properties of sequences of commands.
  These results are often intuitive, but providing exact proofs for them is far
from trivial. They contribute to our understanding of this special type of
algebraic model, and toward building more complete algebras of filesystem trees
and extending algebraic approaches to other data storage protocols. They also
form a theoretical basis for specifying and guaranteeing the error-free
operation of applications that implement an algebraic approach to
synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01744</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01744</id><created>2016-01-07</created><authors><author><keyname>Lin</keyname><forenames>Cedric Yen-Yu</forenames></author><author><keyname>Zhu</keyname><forenames>Yechao</forenames></author></authors><title>Performance of QAOA on Typical Instances of Constraint Satisfaction
  Problems with Bounded Degree</title><categories>quant-ph cs.CC cs.DS</categories><comments>18 pages</comments><report-no>MIT-CTP-4751</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider constraint satisfaction problems of bounded degree, with a good
notion of &quot;typicality&quot;, e.g. the negation of the variables in each constraint
is taken independently at random. Using the quantum approximate optimization
algorithm (QAOA), we show that $ \mu+\Omega(1/\sqrt{D}) $ fraction of the
constraints can be satisfied for typical instances, with the assignment
efficiently produced by QAOA. We do so by showing that the averaged fraction of
constraints being satisfied is $ \mu+\Omega(1/\sqrt{D}) $, with small variance.
Here $ \mu $ is the fraction that would be satisfied by a uniformly random
assignment, and $ D $ is the number of constraints that each variable can
appear. CSPs with typicality include Max-$ k $XOR and Max-$ k $SAT. We point
out how it can be applied to determine the typical ground-state energy of some
local Hamiltonians. We also give a similar result for instances with &quot;no
overlapping constraints&quot;, using the quantum algorithm. We sketch how the
classical algorithm might achieve some partial result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01746</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01746</id><created>2016-01-07</created><authors><author><keyname>Yu</keyname><forenames>Shoujian</forenames></author><author><keyname>Zhou</keyname><forenames>Yiyang</forenames></author></authors><title>A Prefixed-Itemset-Based Improvement For Apriori Algorithm</title><categories>cs.DS cs.DB</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Association rules is a very important part of data mining. It is used to find
the interesting patterns from transaction databases. Apriori algorithm is one
of the most classical algorithms of association rules, but it has the
bottleneck in efficiency. In this article, we proposed a prefixed-itemset-based
data structure for candidate itemset generation, with the help of the structure
we managed to improve the efficiency of the classical Apriori algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01747</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01747</id><created>2016-01-07</created><authors><author><keyname>Xu</keyname><forenames>Tianyin</forenames></author><author><keyname>Pandey</keyname><forenames>Vineet</forenames></author><author><keyname>Klemmer</keyname><forenames>Scott</forenames></author></authors><title>An HCI View of Configuration Problems</title><categories>cs.HC cs.SE</categories><comments>9 pages of exploratory research on understanding system configuration
  problems using Human-Computer Interaction principles</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, configuration problems have drawn tremendous attention
because of their increasing prevalence and their big impact on system
availability. We believe that many of these problems are attributable to
today's configuration interfaces that have not evolved to accommodate the
enormous shift of the system administrator group. Plain text files, as the de
facto configuration interfaces, assume administrators' understanding of the
system under configuration. They ask administrators to directly edit the
corresponding entries with little guidance or assistance. However, this
assumption no longer holds for todays administrator group which has expanded
greatly to include non- and semi-professional administrators. In this paper, we
provide an HCI view of today's configuration problems, and articulate system
configuration as a new HCI problem. Moreover, we present the top obstacles to
correctly and efficiently configuring software systems, and most importantly
their implications on the design and implementation of new-generation
configuration interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01750</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01750</id><created>2016-01-07</created><updated>2016-02-23</updated><authors><author><keyname>Son</keyname><forenames>Kilho</forenames></author><author><keyname>Liu</keyname><forenames>Ming-Yu</forenames></author><author><keyname>Taguchi</keyname><forenames>Yuichi</forenames></author></authors><title>Learning to Remove Multipath Distortions in Time-of-Flight Range Images
  for a Robotic Arm Setup</title><categories>cs.CV cs.RO</categories><comments>8 pages, 11 figures, will be presented to ICRA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Range images captured by Time-of-Flight (ToF) cameras are corrupted with
multipath distortions due to interaction between modulated light signals and
scenes. The interaction is often complicated, which makes a model-based
solution elusive. We propose a learning-based approach for removing the
multipath distortions for a ToF camera in a robotic arm setup. Our approach is
based on deep learning. We use the robotic arm to automatically collect a large
amount of ToF range images containing various multipath distortions. The
training images are automatically labeled by leveraging a high precision
structured light sensor available only in the training time. In the test time,
we apply the learned model to remove the multipath distortions. This allows our
robotic arm setup to enjoy the speed and compact form of the ToF camera without
compromising with its range measurement errors. We conduct extensive
experimental validations and compare the proposed method to several baseline
algorithms. The experiment results show that our method achieves 55% error
reduction in range estimation and largely outperforms the baseline algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01754</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01754</id><created>2016-01-07</created><authors><author><keyname>Matsuda</keyname><forenames>Genki</forenames></author><author><keyname>Kaji</keyname><forenames>Shizuo</forenames></author><author><keyname>Ochiai</keyname><forenames>Hiroyuki</forenames></author></authors><title>Anti-commutative Dual Complex Numbers and 2D Rigid Transformation</title><categories>cs.GR cs.CG</categories><acm-class>I.3.5; I.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new presentation of the two dimensional rigid transformation
which is more concise and efficient than the standard matrix presentation. By
modifying the ordinary dual number construction for the complex numbers, we
define the ring of the anti-commutative dual complex numbers, which
parametrizes two dimensional rotation and translation all together. With this
presentation, one can easily interpolate or blend two or more rigid
transformations at a low computational cost. We developed a library for C++
with the MIT-licensed source code and demonstrate its facility by an
interactive deformation tool developed for iPad.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01768</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01768</id><created>2016-01-07</created><authors><author><keyname>Demange</keyname><forenames>Marc</forenames></author><author><keyname>de Werra</keyname><forenames>Dominique</forenames></author></authors><title>Choosability with limited number of colors</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is $\ell$-choosable if, for any choice of lists of $\ell$ colors for
each vertex, there is a list coloring, which is a coloring where each vertex
receives a color from its list. We study complexity issues of choosability of
graphs when the number $k$ of colors is limited. We get results which differ
surprisingly from the usual case where $k$ is implicit and which extend known
results for the usual case. We also exhibit some classes of graphs (defined by
structural properties of their blocks) which are choosable. Finally we show
that for any $\ell\geq 3$ and any $k\geq 2\ell-2$ there is a bipartite graph
which is $\ell$-choosable with $k$ colors but not with $k+1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01770</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01770</id><created>2016-01-08</created><authors><author><keyname>Haque</keyname><forenames>Albert</forenames></author></authors><title>A MapReduce Approach to NoSQL RDF Databases</title><categories>cs.DB</categories><comments>Undergraduate Honors Thesis, December 2013, The University of Texas
  at Austin, Department of Computer Science. Report# HR-13-13 (honors theses)</comments><report-no>HR-13-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the increased need to house and process large volumes of
data has prompted the need for distributed storage and querying systems. The
growth of machine-readable RDF triples has prompted both industry and academia
to develop new database systems, called NoSQL, with characteristics that differ
from classical databases. Many of these systems compromise ACID properties for
increased horizontal scalability and data availability. This thesis concerns
the development and evaluation of a NoSQL triplestore. Triplestores are
database management systems central to emerging technologies such as the
Semantic Web and linked data. The evaluation spans several benchmarks,
including the two most commonly used in triplestore evaluation, the Berlin
SPARQL Benchmark, and the DBpedia benchmark, a query workload that operates an
RDF representation of Wikipedia. Results reveal that the join algorithm used by
the system plays a critical role in dictating query runtimes. Distributed graph
databases must carefully optimize queries before generating MapReduce query
plans as network traffic for large datasets can become prohibitive if the query
is executed naively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01778</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01778</id><created>2016-01-08</created><authors><author><keyname>Huang</keyname><forenames>Jiacai</forenames></author><author><keyname>Chen</keyname><forenames>Yangquan</forenames></author><author><keyname>Li</keyname><forenames>Haibin</forenames></author><author><keyname>Shi</keyname><forenames>Xinxin</forenames></author></authors><title>Fractional Order Modeling of Human Operator Behavior with Second Order
  Controlled Plant and Experiment Research</title><categories>cs.SY</categories><comments>9 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling human operator's dynamic plays a very important role in the manual
closed-loop control system, and it is an active research area for several
decades. Based on the characteristics of human brain and behaviour, a new kind
of fractional order mathematical model for human operator in SISO systems is
proposed. Compared with the traditional models based on the commonly used
quasi-linear transfer function method or the optimal control theory method, the
proposed fractional order model has simple structure with only few parameters,
and each parameter has explicit physical meanings. The actual data and
experiment results with the second-order controlled element illustrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01782</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01782</id><created>2016-01-08</created><authors><author><keyname>Dowek</keyname><forenames>Gilles</forenames><affiliation>DEDUCTEAM</affiliation></author></authors><title>On the definition of the classical connectives and quantifiers</title><categories>cs.LO</categories><comments>Why is this a Proof?, Festschrift for Luiz Carlos Pereira , 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical logic is embedded into constructive logic, through a definition of
the classical connectives and quantifiers in terms of the constructive ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01786</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01786</id><created>2016-01-08</created><updated>2016-01-21</updated><authors><author><keyname>Lamali</keyname><forenames>Mohamed Lamine</forenames><affiliation>LRI</affiliation></author><author><keyname>Fergani</keyname><forenames>Nasreddine</forenames><affiliation>LRI</affiliation></author><author><keyname>Cohen</keyname><forenames>Johanne</forenames><affiliation>LRI</affiliation></author><author><keyname>Pouyllau</keyname><forenames>H&#xe9;lia</forenames><affiliation>TRT</affiliation></author></authors><title>Path computation in multi-layer networks: Complexity and algorithms</title><categories>cs.NI</categories><comments>IEEE INFOCOM 2016, Apr 2016, San Francisco, United States. To be
  published in IEEE INFOCOM 2016, \&amp;lt;http://infocom2016.ieee-infocom.org/\&amp;gt</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carrier-grade networks comprise several layers where different protocols
coexist. Nowadays, most of these networks have different control planes to
manage routing on different layers, leading to a suboptimal use of the network
resources and additional operational costs. However, some routers are able to
encapsulate, decapsulate and convert protocols and act as a liaison between
these layers. A unified control plane would be useful to optimize the use of
the network resources and automate the routing configurations. Software-Defined
Networking (SDN) based architectures, such as OpenFlow, offer a chance to
design such a control plane. One of the most important problems to deal with in
this design is the path computation process. Classical path computation
algorithms cannot resolve the problem as they do not take into account
encapsulations and conversions of protocols. In this paper, we propose
algorithms to solve this problem and study several cases: Path computation
without bandwidth constraint, under bandwidth constraint and under other
Quality of Service constraints. We study the complexity and the scalability of
our algorithms and evaluate their performances on real topologies. The results
show that they outperform the previous ones proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01796</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01796</id><created>2016-01-08</created><authors><author><keyname>Wankhede</keyname><forenames>Hansaraj S.</forenames></author><author><keyname>Gandhi</keyname><forenames>Sanil S.</forenames></author><author><keyname>Kiwelekar</keyname><forenames>Arvind W</forenames></author></authors><title>On Which Skills do Indian Universities Evaluate Software Engineering
  Students?</title><categories>cs.SE</categories><comments>Six Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universities conduct examinations to evaluate acquired skills and knowledge
gained by students. An assessment of skills and knowledge levels evaluated
during Software Engineering examinations is presented in this paper. The
question items asked during examinations are analyzed from three dimensions
that are cognitive levels, knowledge levels and knowledge areas. The Revised
Bloom's Taxonomy is used to classify question items along the dimensions of
cognitive levels and knowledge levels. Question items are also classified in
various knowledge areas specified in ACM/IEEE's Computer Science Curricula. The
analysis presented in this paper will be useful for software engineering
educators to devise corrective interventions and employers of fresh graduates
to design pre-induction training programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01798</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01798</id><created>2016-01-08</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Pernet</keyname><forenames>Clement</forenames><affiliation>ARIC,MOAIS</affiliation></author><author><keyname>Sultan</keyname><forenames>Ziad</forenames><affiliation>MOAIS</affiliation></author></authors><title>Fast Computation of the Rank Profile Matrix and the Generalized Bruhat
  Decomposition</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The row (resp. column) rank profile of a matrix describes the stair-case
shape of its row (resp. column) echelon form.We here propose a new matrix
invariant, the rank profile matrix, summarizing all information on the row
andcolumn rank profiles of all the leading sub-matrices.We show that this
normal form exists and is unique over any ring, provided that the notion of
McCoy's rank is used, in the presence of zero divisors.We then explore the
conditions for a Gaussian elimination algorithm to compute all or part of this
invariant,through the corresponding PLUQ decomposition. This enlarges the set
of known Elimination variants that compute row or column rank profiles.As a
consequence a new Crout base case variant significantly improves the practical
efficiency of previously known implementations over a finite field.With
matrices of very small rank, we also generalize the techniques ofStorjohann and
Yang to the computation of the rank profile matrix, achieving an
$(r^\omega+mn)^{1+o(1)}$ time complexity for an $m \times n$ matrix of rank
$r$, where $\omega$ is the exponent of matrix multiplication. Finally, by give
connections to the Bruhat decomposition, and severalof its variants and
generalizations. Thus, our algorithmicimprovements for the PLUQ factorization,
and their implementations,directly apply to these decompositions.In particular,
we show how a PLUQ decomposition revealing the rankprofile matrix also reveals
both a row and a column echelon form ofthe input matrix or of any of its
leading sub-matrices, by a simplepost-processing made of row and column
permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01799</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01799</id><created>2016-01-08</created><updated>2016-01-13</updated><authors><author><keyname>Bailly</keyname><forenames>Adeline</forenames><affiliation>LETG - Costel, OBELIX</affiliation></author><author><keyname>Malinowski</keyname><forenames>Simon</forenames><affiliation>LinkMedia</affiliation></author><author><keyname>Tavenard</keyname><forenames>Romain</forenames><affiliation>LETG - Costel, OBELIX</affiliation></author><author><keyname>Guyet</keyname><forenames>Thomas</forenames><affiliation>DREAM</affiliation></author><author><keyname>Chapel</keyname><forenames>Laetitia</forenames><affiliation>OBELIX</affiliation></author></authors><title>Dense Bag-of-Temporal-SIFT-Words for Time Series Classification</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series classification is an application of particular interest with the
increase of data to monitor. Classical techniques for time series
classification rely on point-to-point distances. Recently, Bag-of-Words
approaches have been used in this context. Words are quantized versions of
simple features extracted from sliding windows. The SIFT framework has proved
efficient for image classification. In this paper, we design a time series
classification scheme that builds on the SIFT framework adapted to time series
to feed a Bag-of-Words. We then refine our method by studying the impact of
normalized Bag-of-Words, as well as densely extract point descriptors. Proposed
adjustements achieve better performance. The evaluation shows that our method
outperforms classical techniques in terms of classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01815</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01815</id><created>2016-01-08</created><authors><author><keyname>Kucharski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Design and implementation of a user interface for a multi-device
  spatially-aware mobile system</title><categories>cs.HC</categories><comments>PhD thesis, Lodz University of Technology, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The aim of this thesis was the design and development of an interactive
system enhancing collaborative sensemaking. The system design was based on
related work research and preliminary user study. The system is based on
multiple spatially-aware mobile devices. The spatial awareness is established
with a motion tracking system. The information about position of tablets is
received by a server. The server communicates with the devices and manages the
content of each device. The implemented system supports managing the elements
of information across the devices basing on their relative spatial arrangement.
The evaluation of the system was done by a user study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01816</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01816</id><created>2016-01-08</created><authors><author><keyname>Abrahamsen</keyname><forenames>Mikkel</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Outer common tangents and nesting of convex hulls of two disjoint simple
  polygons in linear time and constant workspace</title><categories>cs.CG</categories><msc-class>68U05, 65D18</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm for computing the outer common tangents of two
disjoint simple polygons using linear time and only constant workspace. A
tangent of a polygon is a line touching the polygon such that all of the
polygon lies on the same side of the line. An outer common tangent of two
polygons is a tangent of both polygons such that the polygons lie on the same
side of the tangent. Each polygon is given as a read-only array of its corners
in cyclic order. The algorithm detects if an outer common tangent does not
exist, which is the case if and only if the convex hull of one of the polygons
is contained in the convex hull of the other. Otherwise, two corners defining
an outer common tangent are returned. This was not known to be possible in
linear time and constant workspace prior to this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01824</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01824</id><created>2016-01-08</created><updated>2016-01-18</updated><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Wahlstrom</keyname><forenames>Magnus</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Acyclicity in Edge-Colored Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A walk $W$ in edge-colored graphs is called properly colored (PC) if every
pair of consecutive edges in $W$ is of different color. We introduce and study
five types of PC acyclicity in edge-colored graphs such that graphs of PC
acyclicity of type $i$ is a proper superset of graphs of acyclicity of type
$i+1$, $i=1,2,3,4.$ The first three types are equivalent to the absence of PC
cycles, PC trails, and PC walks, respectively. While graphs of types 1, 2 and 3
can be recognized in polynomial time, the problem of recognizing graphs of type
4 is, somewhat surprisingly, NP-hard even for 2-edge-colored graphs (i.e., when
only two colors are used). The same problem with respect to type 5 is
polynomial-time solvable for all edge-colored graphs. Using the five types, we
investigate the border between intractability and tractability for the problems
of finding the maximum number of internally vertex disjoint PC paths between
two vertices and the minimum number of vertices to meet all PC paths between
two vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01825</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01825</id><created>2016-01-08</created><authors><author><keyname>Elyengui</keyname><forenames>Saida</forenames></author><author><keyname>Bouhouchi</keyname><forenames>Riadh</forenames></author><author><keyname>Ezzedine</keyname><forenames>Tahar</forenames></author></authors><title>A comparative performance study of the routing protocols RPL, LOADng and
  LOADng-CTP with bidirectional traffic for AMI scenario</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1506.06357</comments><journal-ref>2015 IEEE International Conference on Intelligent Computer
  Communication and Processing (ICCP), 3-5 Sept. 2015</journal-ref><doi>10.1109/ICCP.2015.7312719</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the introduction of the smart grid, Advanced Metering Infrastructure
(AMI) has become a main component in the present power system. The effective
implementation of AMI depends widely on its communication infrastructure and
protocols providing trustworthy two-way communications. In this paper we study
two routing protocols philosophies for low power and lossy networks (LLNs) and
their application for a smart metering scenario. This study purposes a detailed
evaluation of two routing protocols proposed by IETF the proactive candidate
namely RPL (IPv6 Routing Protocol for Low-Power and Lossy Networks) and the
reactive candidate named LOADng (LLN On-demand Ad-hoc Distance vector routing
protocol - next generation) recently proposed as an Internet Draft, still in
its design phase and is part of the ITU-T G.9903 recommendation. In the course
of this study, we also implemented an extension version of LOADng named
LOADng-CTP specified by an IETF draft extended with a collection tree for
efficient data acquisition in LLNs. We performed checks on control overhead;
End to End Delay and Packet delivery ratio for the two protocols related to
multipoint-to-point (MP2P), and point-to-multi point (P2MP) traffic flow in a
realistic smart metering architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01856</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01856</id><created>2016-01-08</created><authors><author><keyname>Ablinger</keyname><forenames>Jakob</forenames></author><author><keyname>Bluemlein</keyname><forenames>Johannes</forenames></author><author><keyname>de Freitas</keyname><forenames>Abilio</forenames></author><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>A toolbox to solve coupled systems of differential and difference
  equations</title><categories>cs.SC hep-ph hep-th math-ph math.MP</categories><report-no>DESY 16-003, DO-TH 16/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms to solve coupled systems of linear differential
equations, arising in the calculation of massive Feynman diagrams with local
operator insertions at 3-loop order, which do {\it not} request special choices
of bases. Here we assume that the desired solution has a power series
representation and we seek for the coefficients in closed form. In particular,
if the coefficients depend on a small parameter $\ep$ (the dimensional
parameter), we assume that the coefficients themselves can be expanded in
formal Laurent series w.r.t.\ $\ep$ and we try to compute the first terms in
closed form. More precisely, we have a decision algorithm which solves the
following problem: if the terms can be represented by an indefinite nested
hypergeometric sum expression (covering as special cases the harmonic sums,
cyclotomic sums, generalized harmonic sums or nested binomial sums), then we
can calculate them. If the algorithm fails, we obtain a proof that the terms
cannot be represented by the class of indefinite nested hypergeometric sum
expressions. Internally, this problem is reduced by holonomic closure
properties to solving a coupled system of linear difference equations. The
underlying method in this setting relies on decoupling algorithms, difference
ring algorithms and recurrence solving. We demonstrate by a concrete example
how this algorithm can be applied with the new Mathematica package
\texttt{SolveCoupledSystem} which is based on the packages \texttt{Sigma},
\texttt{HarmonicSums} and \texttt{OreSys}. In all applications the
representation in $x$-space is obtained as an iterated integral representation
over general alphabets, generalizing Poincar\'{e} iterated integrals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01858</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01858</id><created>2016-01-08</created><updated>2016-01-11</updated><authors><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Milleth</keyname><forenames>J. Klutto</forenames></author></authors><title>Joint Backhaul-Access Analysis of Full Duplex Self-Backhauling
  Heterogeneous Networks</title><categories>cs.NI</categories><comments>29 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the successful demonstration of in-band full-duplex (IBFD) transceivers,
a new research dimension has been added to wireless networks. This paper
proposes an interesting use case of this capability for IBFD self-backhauling
heterogeneous networks (HetNet). IBFD self-backhauling in a HetNet refers to
IBFD-enabled small cells backhauling themselves with macro cells over the
wireless channel. Owing to their IBFD capability, the small cells
simultaneously communicate over the access and backhaul links, using the same
frequency band. The idea is doubly advantageous, as it obviates the need for
fiber backhauling small cells every hundred meters and allows the access
spectrum to be reused for backhauling at no extra cost. This work considers the
case of a two-tier cellular network with IBFD-enabled small cells, wirelessly
backhauling themselves with conventional macro cells. For clear exposition, the
case considered is that of FDD network, where within access and backhaul links,
the downlink (DL) and uplink (UL) are frequency duplexed ($f1$, $f2$
respectively), while the total frequency spectrum used at access and backhaul
($f1+f2$) is the same. Analytical expressions for coverage and average downlink
(DL) rate in such a network are derived using tools from the field of
stochastic geometry. It is shown that DL rate in such networks could be close
to double that of a conventional TDD/FDD self-backhauling network, at the
expense of reduced coverage due to higher interference in IBFD networks. For
the proposed IBFD network, the conflicting aspects of increased interference on
one side and high spectral efficiency on the other are captured into a
mathematical model. The mathematical model introduces an end-to-end joint
analysis of backhaul (or fronthaul) and access links, in contrast to the
largely available access-centric studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01876</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01876</id><created>2016-01-08</created><authors><author><keyname>Bekhouche</keyname><forenames>Salah Eddine</forenames><affiliation>Laboratory of LESIA, University of Biskra, Algeria</affiliation></author><author><keyname>Ouafi</keyname><forenames>Abdelkrim</forenames><affiliation>Laboratory of LESIA, University of Biskra, Algeria</affiliation></author><author><keyname>Taleb-Ahmed</keyname><forenames>Abdelmalik</forenames><affiliation>LAMIH, University of Valenciennes, France</affiliation></author><author><keyname>Hadid</keyname><forenames>Abdenour</forenames><affiliation>Center for Machine Vision Research, University of Oulu, Finland</affiliation></author><author><keyname>Benlamoudi</keyname><forenames>Azeddine</forenames><affiliation>Laboratory of LESIA, University of Biskra, Algeria</affiliation></author></authors><title>Facial age estimation using BSIF and LBP</title><categories>cs.CV</categories><comments>5 pages, 8 figures</comments><doi>10.13140/RG.2.1.1933.6483/1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human face aging is irreversible process causing changes in human face
characteristics such us hair whitening, muscles drop and wrinkles. Due to the
importance of human face aging in biometrics systems, age estimation became an
attractive area for researchers. This paper presents a novel method to estimate
the age from face images, using binarized statistical image features (BSIF) and
local binary patterns (LBP)histograms as features performed by support vector
regression (SVR) and kernel ridge regression (KRR). We applied our method on
FG-NET and PAL datasets. Our proposed method has shown superiority to that of
the state-of-the-art methods when using the whole PAL database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01881</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01881</id><created>2016-01-08</created><authors><author><keyname>Ortigoza</keyname><forenames>Jammily</forenames></author><author><keyname>Lopez-Pires</keyname><forenames>Fabio</forenames></author><author><keyname>Baran</keyname><forenames>Benjam&#x131;n</forenames></author></authors><title>Dynamic Environments for Virtual Machine Placement considering
  Elasticity and Overbooking</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1507.00090</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing datacenters provide millions of virtual machines in actual
cloud markets. In this context, Virtual Machine Placement (VMP) is one of the
most challenging problems in cloud infrastructure management, considering the
large number of possible optimization criteria and different formulations that
could be studied. Considering the on-demand model of cloud computing, the VMP
problem should be solved dynamically to efficiently attend typical workload of
modern applications. This work proposes a taxonomy in order to understand
possible challenges for Cloud Service Providers (CSPs) in dynamic environments,
based on the most relevant dynamic parameters studied so far in the VMP
literature. Based on the proposed taxonomy, several unexplored environments
have been identified. To further study those research opportunities, sample
workload traces for each particular environment are required; therefore, basic
examples illustrate a preliminary work on dynamic workload trace generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01885</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01885</id><created>2016-01-08</created><authors><author><keyname>Nicolaou</keyname><forenames>Anguelos</forenames></author><author><keyname>Bagdanov</keyname><forenames>Andrew</forenames></author><author><keyname>Gomez-Bigorda</keyname><forenames>Lluis</forenames></author><author><keyname>Karatzas</keyname><forenames>Dimosthenis</forenames></author></authors><title>Visual Script and Language Identification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a script identification method based on
hand-crafted texture features and an artificial neural network. The proposed
pipeline achieves near state-of-the-art performance for script identification
of video-text and state-of-the-art performance on visual language
identification of handwritten text. More than using the deep network as a
classifier, the use of its intermediary activations as a learned metric
demonstrates remarkable results and allows the use of discriminative models on
unknown classes. Comparative experiments in video-text and text in the wild
datasets provide insights on the internals of the proposed deep network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01886</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01886</id><created>2016-01-08</created><authors><author><keyname>G&#x105;gol</keyname><forenames>Adam</forenames></author><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Kozik</keyname><forenames>Jakub</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author></authors><title>Pathwidth and nonrepetitive list coloring</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A vertex coloring of a graph is nonrepetitive if there is no path in the
graph whose first half receives the same sequence of colors as the second half.
While every tree can be nonrepetitively colored with a bounded number of colors
(4 colors is enough), Fiorenzi, Ochem, Ossona de Mendez, and Zhu recently
showed that this does not extend to the list version of the problem, that is,
for every $\ell \geq 1$ there is a tree that is not nonrepetitively
$\ell$-choosable. In this paper we prove the following positive result, which
complements the result of Fiorenzi et al.: There exists a function $f$ such
that every tree of pathwidth $k$ is nonrepetitively $f(k)$-choosable. We also
show that such a property is specific to trees by constructing a family of
pathwidth-2 graphs that are not nonrepetitively $\ell$-choosable for any fixed
$\ell$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01887</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01887</id><created>2016-01-08</created><authors><author><keyname>Tagiew</keyname><forenames>Rustam</forenames></author></authors><title>Research Project: Text Engineering Tool for Ontological Scientometry</title><categories>cs.CL cs.DL</categories><comments>5 pages, 2 figure</comments><doi>10.13140/RG.2.1.1619.1847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of scientific papers grows exponentially in many disciplines. The
share of online available papers grows as well. At the same time, the period of
time for a paper to loose at chance to be cited anymore shortens. The decay of
the citing rate shows similarity to ultradiffusional processes as for other
online contents in social networks. The distribution of papers per author shows
similarity to the distribution of posts per user in social networks. The rate
of uncited papers for online available papers grows while some papers 'go
viral' in terms of being cited. Summarized, the practice of scientific
publishing moves towards the domain of social networks. The goal of this
project is to create a text engineering tool, which can semi-automatically
categorize a paper according to its type of contribution and extract
relationships between them into an ontological database. Semi-automatic
categorization means that the mistakes made by automatic pre-categorization and
relationship-extraction will be corrected through a wikipedia-like front-end by
volunteers from general public. This tool should not only help researchers and
the general public to find relevant supplementary material and peers faster,
but also provide more information for research funding agencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01891</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01891</id><created>2016-01-08</created><authors><author><keyname>Berardi</keyname><forenames>Stefano</forenames></author><author><keyname>Steila</keyname><forenames>Silvia</forenames></author></authors><title>Ramsey's Theorem for Pairs and $k$ Colors as a Sub-Classical Principle
  of Arithmetic</title><categories>math.LO cs.LO</categories><comments>17 pages</comments><msc-class>03F55, 03B30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose is to study the strength of Ramsey's Theorem for pairs restricted
to recursive assignments of $k$-many colors, with respect to Intuitionistic
Heyting Arithmetic. We prove that for every natural number $k \geq 2$, Ramsey's
Theorem for pairs and recursive assignments of $k$ colors is equivalent to the
Limited Lesser Principle of Omniscience for $\Sigma^0_3$ formulas over Heyting
Arithmetic. Alternatively, the same theorem over intuitionistic arithmetic is
equivalent to: for every recursively enumerable infinite $k$-ary tree there is
some $i &lt; k$ and some branch with infinitely many children of index $i$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01892</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01892</id><created>2016-01-08</created><updated>2016-01-13</updated><authors><author><keyname>Benzi</keyname><forenames>Kirell</forenames></author><author><keyname>Kalofolias</keyname><forenames>Vassilis</forenames></author><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Song Recommendation with Non-Negative Matrix Factorization and Graph
  Total Variation</title><categories>stat.ML cs.IR cs.LG physics.data-an</categories><comments>Code available at: https://github.com/kikohs/recog</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work formulates a novel song recommender system as a matrix completion
problem that benefits from collaborative filtering through Non-negative Matrix
Factorization (NMF) and content-based filtering via total variation (TV) on
graphs. The graphs encode both playlist proximity information and song
similarity, using a rich combination of audio, meta-data and social features.
As we demonstrate, our hybrid recommendation system is very versatile and
incorporates several well-known methods while outperforming them. Particularly,
we show on real-world data that our model overcomes w.r.t. two evaluation
metrics the recommendation of models solely based on low-rank information,
graph-based information or a combination of both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01909</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01909</id><created>2016-01-08</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Karim</keyname><forenames>Mohammad S.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Sorour</keyname><forenames>Sameh</forenames></author></authors><title>Delivery Time Reduction for Order-Constrained Applications using Binary
  Network Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a radio access network wherein a base-station is required to deliver
a set of order-constrained messages to a set of users over independent erasure
channels. This paper studies the delivery time reduction problem using
instantly decodable network coding (IDNC). Motivated by time-critical and
order-constrained applications, the delivery time is defined, at each
transmission, as the number of undelivered messages. The delivery time
minimization problem being computationally intractable, most of the existing
literature on IDNC propose sub-optimal online solutions. This paper suggests a
novel method for solving the problem by introducing the delivery delay as a
measure of distance to optimality. An expression characterizing the delivery
time using the delivery delay is derived, allowing the approximation of the
delivery time minimization problem by an optimization problem involving the
delivery delay. The problem is, then, formulated as a maximum weight clique
selection problem over the IDNC graph wherein the weight of each vertex
reflects its corresponding user and message's delay. Simulation results suggest
that the proposed solution achieves lower delivery and completion times as
compared to the best-known heuristics for delivery time reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01910</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01910</id><created>2016-01-08</created><authors><author><keyname>Richter</keyname><forenames>Harald</forenames></author></authors><title>About the Suitability of Clouds in High-Performance Computing</title><categories>cs.DC</categories><comments>10 pages</comments><journal-ref>Journal Computer Science and Information Technology (CC&amp;IT),
  Volume 6, Number 1, January 2016, pp. 23-33</journal-ref><doi>10.5121/csit.2016.60103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has become the ubiquitous computing and storage paradigm. It
is also attractive for scientists, because they do not have to care any more
for their own IT infrastructure, but can outsource it to a Cloud Service
Provider of their choice. However, for the case of High-Performance Computing
(HPC) in a cloud, as it is needed in simulations or for Big Data analysis,
things are getting more intricate, because HPC codes must stay highly
efficient, even when executed by many virtual cores (vCPUs). Older clouds or
new standard clouds can fulfil this only under special precautions, which are
given in this article. The results can be extrapolated to other cloud OSes than
OpenStack and to other codes than OpenFOAM, which were used as examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01912</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01912</id><created>2016-01-08</created><authors><author><keyname>Ahmadi</keyname><forenames>Mohamad</forenames></author><author><keyname>Ghodselahi</keyname><forenames>Abdolhamid</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author><author><keyname>Molla</keyname><forenames>Anisur Rahaman</forenames></author></authors><title>The Cost of Global Broadcast in Dynamic Radio Networks</title><categories>cs.DC</categories><comments>17 pages, conference version appeared in OPODIS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the single-message broadcast problem in dynamic radio networks. We
show that the time complexity of the problem depends on the amount of stability
and connectivity of the dynamic network topology and on the adaptiveness of the
adversary providing the dynamic topology. More formally, we model communication
using the standard graph-based radio network model. To model the dynamic
network, we use a generalization of the synchronous dynamic graph model
introduced in [Kuhn et al., STOC 2010]. For integer parameters $T\geq 1$ and
$k\geq 1$, we call a dynamic graph $T$-interval $k$-connected if for every
interval of $T$ consecutive rounds, there exists a $k$-vertex-connected stable
subgraph. Further, for an integer parameter $\tau\geq 0$, we say that the
adversary providing the dynamic network is $\tau$-oblivious if for constructing
the graph of some round $t$, the adversary has access to all the randomness
(and states) of the algorithm up to round $t-\tau$.
  As our main result, we show that for any $T\geq 1$, any $k\geq 1$, and any
$\tau\geq 1$, for a $\tau$-oblivious adversary, there is a distributed
algorithm to broadcast a single message in time
$O\big(\big(1+\frac{n}{k\cdot\min\left\{\tau,T\right\}}\big)\cdot n\log^3
n\big)$. We further show that even for large interval $k$-connectivity,
efficient broadcast is not possible for the usual adaptive adversaries. For a
$1$-oblivious adversary, we show that even for any $T\leq
(n/k)^{1-\varepsilon}$ (for any constant $\varepsilon&gt;0$) and for any $k\geq
1$, global broadcast in $T$-interval $k$-connected networks requires at least
$\Omega(n^2/(k^2\log n))$ time. Further, for a $0$ oblivious adversary,
broadcast cannot be solved in $T$-interval $k$-connected networks as long as
$T&lt;n-k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01917</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01917</id><created>2016-01-08</created><authors><author><keyname>Castagnos</keyname><forenames>Sylvain</forenames><affiliation>KIWI</affiliation></author><author><keyname>'Huillier</keyname><forenames>Amaury L</forenames><affiliation>KIWI</affiliation></author><author><keyname>Boyer</keyname><forenames>Anne</forenames><affiliation>KIWI</affiliation></author></authors><title>Toward a Robust Diversity-Based Model to Detect Changes of Context</title><categories>cs.IR cs.AI</categories><comments>27th IEEE International Conference on Tools with Artificial
  Intelligence (ICTAI 2015), Nov 2015, Vietri sul Mare, Italy</comments><proxy>ccsd</proxy><doi>10.1109/ICTAI.2015.84</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to automatically and quickly understand the user context during a
session is a main issue for recommender systems. As a first step toward
achieving that goal, we propose a model that observes in real time the
diversity brought by each item relatively to a short sequence of consultations,
corresponding to the recent user history. Our model has a complexity in
constant time, and is generic since it can apply to any type of items within an
online service (e.g. profiles, products, music tracks) and any application
domain (e-commerce, social network, music streaming), as long as we have
partial item descriptions. The observation of the diversity level over time
allows us to detect implicit changes. In the long term, we plan to characterize
the context, i.e. to find common features among a contiguous sub-sequence of
items between two changes of context determined by our model. This will allow
us to make context-aware and privacy-preserving recommendations, to explain
them to users. As this is an ongoing research, the first step consists here in
studying the robustness of our model while detecting changes of context. In
order to do so, we use a music corpus of 100 users and more than 210,000
consultations (number of songs played in the global history). We validate the
relevancy of our detections by finding connections between changes of context
and events, such as ends of session. Of course, these events are a subset of
the possible changes of context, since there might be several contexts within a
session. We altered the quality of our corpus in several manners, so as to test
the performances of our model when confronted with sparsity and different types
of items. The results show that our model is robust and constitutes a promising
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01920</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01920</id><created>2016-01-08</created><authors><author><keyname>Akanbi</keyname><forenames>Adeyinka K.</forenames></author><author><keyname>Masinde</keyname><forenames>Muthoni</forenames></author></authors><title>Towards Semantic Integration of Heterogeneous Sensor Data with
  Indigenous Knowledge for Drought Forecasting</title><categories>cs.AI cs.NI cs.SE</categories><comments>5 pages, 3 figures, In Proceedings of the Doctoral Symposium of the
  16th International Middleware Conference (Middleware Doct Symposium 2015),
  Ivan Beschastnikh and Wouter Joosen (Eds.). ACM, New York, NY, USA</comments><doi>10.1145/2843966.2843968</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Internet of Things (IoT) domain, various heterogeneous ubiquitous
devices would be able to connect and communicate with each other seamlessly,
irrespective of the domain. Semantic representation of data through detailed
standardized annotation has shown to improve the integration of the
interconnected heterogeneous devices. However, the semantic representation of
these heterogeneous data sources for environmental monitoring systems is not
yet well supported. To achieve the maximum benefits of IoT for drought
forecasting, a dedicated semantic middleware solution is required. This
research proposes a middleware that semantically represents and integrates
heterogeneous data sources with indigenous knowledge based on a unified
ontology for an accurate IoT-based drought early warning system (DEWS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01923</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01923</id><created>2016-01-08</created><authors><author><keyname>Alouini</keyname><forenames>M. S.</forenames></author><author><keyname>Biglieri</keyname><forenames>E.</forenames></author><author><keyname>Divsalar</keyname><forenames>D.</forenames></author><author><keyname>Dolinar</keyname><forenames>S.</forenames></author><author><keyname>Goldsmith</keyname><forenames>A.</forenames></author><author><keyname>Milstein</keyname><forenames>L.</forenames></author></authors><title>The life and work of Marvin Kenneth Simon</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a measure of the importance and profundity of Marvin Kenneth Simon's
contributions to communication theory that this tribute article and tutorial
about his life and work is of current research relevance in spite of the
continually accelerating rate of evolution in this area. Marv, as the entire
community affectionately knew him, was one of the most prolific and influential
communications researchers of his generation. Moreover, he laid the foundation
for many of the techniques used in communication systems today. Marv's tragic
death on September 23, 2007 continues to engender pangs not only of sadness at
the passing of a great friend to many in our community, but also of regret that
he is no longer with us to help in resolving the many challenges facing
communication systems today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01928</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01928</id><created>2016-01-08</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Hoffmann</keyname><forenames>Philipp</forenames></author></authors><title>Reduction Rules for Colored Workflow Nets</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Colored Workflow nets, a model based on Workflow nets enriched with
data. Based on earlier work by Esparza and
Desel[arXiv:1307.2145,arXiv:1403.4958] on the negotiation model of concurrency,
we present reduction rules for our model. Contrary to previous work, our rules
preserve not only soundness, but also the data flow semantics. For free choice
nets, the rules reduce all sound nets (and only them) to a net with one single
transition and the same data flow semantics. We give an explicit algorithm that
requires only a polynomial number of rule applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.01944</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.01944</id><created>2016-01-08</created><authors><author><keyname>Jain</keyname><forenames>Shantanu</forenames></author><author><keyname>White</keyname><forenames>Martha</forenames></author><author><keyname>Trosset</keyname><forenames>Michael W.</forenames></author><author><keyname>Radivojac</keyname><forenames>Predrag</forenames></author></authors><title>Nonparametric semi-supervised learning of class proportions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of developing binary classifiers from positive and unlabeled data
is often encountered in machine learning. A common requirement in this setting
is to approximate posterior probabilities of positive and negative classes for
a previously unseen data point. This problem can be decomposed into two steps:
(i) the development of accurate predictors that discriminate between positive
and unlabeled data, and (ii) the accurate estimation of the prior probabilities
of positive and negative examples. In this work we primarily focus on the
latter subproblem. We study nonparametric class prior estimation and formulate
this problem as an estimation of mixing proportions in two-component mixture
models, given a sample from one of the components and another sample from the
mixture itself. We show that estimation of mixing proportions is generally
ill-defined and propose a canonical form to obtain identifiability while
maintaining the flexibility to model any distribution. We use insights from
this theory to elucidate the optimization surface of the class priors and
propose an algorithm for estimating them. To address the problems of
high-dimensional density estimation, we provide practical transformations to
low-dimensional spaces that preserve class priors. Finally, we demonstrate the
efficacy of our method on univariate and multivariate data.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="89000" completeListSize="102538">1122234|90001</resumptionToken>
</ListRecords>
</OAI-PMH>
