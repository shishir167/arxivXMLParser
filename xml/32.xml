<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:59:42Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|31001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4656</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4656</id><created>2012-04-20</created><updated>2012-06-19</updated><authors><author><keyname>Ambat</keyname><forenames>Sooraj K.</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Fusion of Greedy Pursuits for Compressed Sensing Signal Reconstruction</title><categories>stat.AP cs.IT math.IT</categories><comments>Accepted, &quot;20th European Signal Processing Conference 2012 (EUSIPCO
  2012)&quot;, Bucharest, Romania,27 Aug,2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy Pursuits are very popular in Compressed Sensing for sparse signal
recovery. Though many of the Greedy Pursuits possess elegant theoretical
guarantees for performance, it is well known that their performance depends on
the statistical distribution of the non-zero elements in the sparse signal. In
practice, the distribution of the sparse signal may not be known a priori. It
is also observed that performance of Greedy Pursuits degrades as the number of
available measurements decreases from a threshold value which is method
dependent. To improve the performance in these situations, we introduce a novel
fusion framework for Greedy Pursuits and also propose two algorithms for sparse
recovery. Through Monte Carlo simulations we show that the proposed schemes
improve sparse signal recovery in clean as well as noisy measurement cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4659</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4659</id><created>2012-04-20</created><authors><author><keyname>de Bondt</keyname><forenames>Michiel</forenames></author></authors><title>The computational complexity of Minesweeper</title><categories>cs.CC</categories><comments>14 pages, LaTeX =&gt; DVI =&gt; PS =&gt; PDF</comments><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Minesweeper game is PP-hard, when the object is to locate
all mines with the highest probability. When the probability of locating all
mines may be infinitesimal, the Minesweeper game is even PSPACE-complete. In
our construction, the player can reveal a boolean circuit in polynomial time,
after guessing an initial square with no surrounding mines, a guess that has 99
percent probability of success. Subsequently, the mines must be located with a
maximum probability of success.
  Furthermore, we show that determining the solvability of a partially
uncovered Minesweeper board is NP-complete with hexagonal and triangular grids
as well as a square grid, extending a similar result for square grids only by
R. Kaye. Actually finding the mines with a maximum probability of success is
again PP-hard or PSPACE-complete respectively.
  Our constructions are in such a way that the number of mines can be computed
in polynomial time and hence a possible mine counter does not provide
additional information. The results are obtained by replacing the dyadic gates
in [3] by two primitives which makes life more easy in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4666</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4666</id><created>2012-04-20</created><authors><author><keyname>Kwok</keyname><forenames>Tsz Chiu</forenames></author><author><keyname>Lau</keyname><forenames>Lap Chi</forenames></author></authors><title>Finding Small Sparse Cuts Locally by Random Walk</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding a small sparse cut in an undirected graph.
Given an undirected graph G=(V,E) and a parameter k &lt;= |E|, the small sparsest
cut problem is to find a subset of vertices S with minimum conductance among
all sets with volume at most k. Using ideas developed in local graph
partitioning algorithms, we obtain the following bicriteria approximation
algorithms for the small sparsest cut problem:
  - If there is a subset U with conductance \phi and vol(U) &lt;= k, then there is
a polynomial time algorithm to find a set S with conductance
O(\sqrt{\phi/\epsilon}) and vol(S) &lt;= k^{1+\epsilon} for any \epsilon &gt; 1/k.
  - If there is a subset U with conductance \phi and vol(U) &lt;= k, then there is
a polynomial time algorithm to find a set S with conductance O(\sqrt{\phi
ln(k)/\epsilon}) and vol(S) &lt;= (1+\epsilon)k for any \epsilon &gt; 2ln(k)/k.
  These algorithms can be implemented locally using truncated random walk, with
running time almost linear to the output size. This provides a local graph
partitioning algorithm with a better conductance guarantee when k is sublinear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4672</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4672</id><created>2012-04-20</created><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Lauser</keyname><forenames>Alexander</forenames></author></authors><title>The Join Levels of the Trotter-Weil Hierarchy are Decidable</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variety DA of finite monoids has a huge number of different
characterizations, ranging from two-variable first-order logic FO^2 to
unambiguous polynomials. In order to study the structure of the subvarieties of
DA, Trotter and Weil considered the intersection of varieties of finite monoids
with bands, i.e., with idempotent monoids. The varieties of idempotent monoids
are very well understood and fully classified. Trotter and Weil showed that for
every band variety V there exists a unique maximal variety W inside DA such
that the intersection with bands yields the given band variety V. These maximal
varieties W define the Trotter-Weil hierarchy. This hierarchy is infinite and
it exhausts DA; induced by band varieties, it naturally has a zigzag shape. In
their paper, Trotter and Weil have shown that the corners and the intersection
levels of this hierarchy are decidable.
  In this paper, we give a single identity of omega-terms for every join level
of the Trotter-Weil hierarchy; this yields decidability. Moreover, we show that
the join levels and the subsequent intersection levels do not coincide. Almeida
and Azevedo have shown that the join of R-trivial and L-trivial finite monoids
is decidable; this is the first non-trivial join level of the Trotter-Weil
hierarchy. We extend this result to the other join levels of the Trotter-Weil
hierarchy. At the end of the paper, we give two applications. First, we show
that the hierarchy of deterministic and codeterministic products is decidable.
And second, we show that the direction alternation depth of unambiguous
interval logic is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4679</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4679</id><created>2012-04-20</created><updated>2013-06-14</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Robust Geometric Spanners</title><categories>cs.CG cs.NI</categories><comments>18 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly connected and yet sparse graphs (such as expanders or graphs of high
treewidth) are fundamental, widely applicable and extensively studied
combinatorial objects. We initiate the study of such highly connected graphs
that are, in addition, geometric spanners. We define a property of spanners
called robustness. Informally, when one removes a few vertices from a robust
spanner, this harms only a small number of other vertices. We show that robust
spanners must have a superlinear number of edges, even in one dimension. On the
positive side, we give constructions, for any dimension, of robust spanners
with a near-linear number of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4685</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4685</id><created>2012-04-20</created><authors><author><keyname>Rabe</keyname><forenames>Florian</forenames></author></authors><title>A Query Language for Formal Mathematical Libraries</title><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most promising applications of mathematical knowledge management
is search: Even if we restrict attention to the tiny fragment of mathematics
that has been formalized, the amount exceeds the comprehension of an individual
human.
  Based on the generic representation language MMT, we introduce the
mathematical query language QMT: It combines simplicity, expressivity, and
scalability while avoiding a commitment to a particular logical formalism. QMT
can integrate various search paradigms such as unification, semantic web, or
XQuery style queries, and QMT queries can span different mathematical
libraries.
  We have implemented QMT as a part of the MMT API. This combination provides a
scalable indexing and query engine that can be readily applied to any library
of mathematical knowledge. While our focus here is on libraries that are
available in a content markup language, QMT naturally extends to presentation
and narration markup languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4686</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4686</id><created>2012-04-20</created><authors><author><keyname>S&#xf8;rensen</keyname><forenames>Jesper H.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>&#xd8;stergaard</keyname><forenames>Jan</forenames></author></authors><title>Analysis of LT Codes with Unequal Recovery Time</title><categories>cs.IT math.IT</categories><comments>33 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze a specific class of rateless codes, called LT codes
with unequal recovery time. These codes provide the option of prioritizing
different segments of the transmitted data over other. The result is that
segments are decoded in stages during the rateless transmission, where higher
prioritized segments are decoded at lower overhead. Our analysis focuses on
quantifying the expected amount of received symbols, which are redundant
already upon arrival, i.e. all input symbols contained in the received symbols
have already been decoded. This analysis gives novel insights into the
probabilistic mechanisms of LT codes with unequal recovery time, which has not
yet been available in the literature. We show that while these rateless codes
successfully provide the unequal recovery time, they do so at a significant
price in terms of redundancy in the lower prioritized segments. We propose and
analyze a modification where a single intermediate feedback is transmitted,
when the first segment is decoded in a code with two segments. Our analysis
shows that this modification provides a dramatic improvement on the decoding
performance of the lower prioritized segment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4688</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4688</id><created>2012-04-20</created><updated>2013-11-04</updated><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Witmer</keyname><forenames>David</forenames></author></authors><title>Markov chain methods for small-set expansion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a finite irreducible Markov chain with invariant distribution $\pi$.
We use the inner product induced by $\pi$ and the associated heat operator to
simplify and generalize some results related to graph partitioning and the
small-set expansion problem. For example, Steurer showed a tight connection
between the number of small eigenvalues of a graph's Laplacian and the
expansion of small sets in that graph. We give a simplified proof which
generalizes to the nonregular, directed case. This result implies an
approximation algorithm for an &quot;analytic&quot; version of the Small-Set Expansion
Problem, which, in turn, immediately gives an approximation algorithm for
Small-Set Expansion. We also give a simpler proof of a lower bound on the
probability that a random walk stays within a set; this result was used in some
recent works on finding small sparse cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4691</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4691</id><created>2012-04-20</created><authors><author><keyname>Cleve</keyname><forenames>Richard</forenames></author><author><keyname>Iwama</keyname><forenames>Kazuo</forenames></author><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author><author><keyname>Nishimura</keyname><forenames>Harumichi</forenames></author><author><keyname>Tani</keyname><forenames>Seiichiro</forenames></author><author><keyname>Teruyama</keyname><forenames>Junichi</forenames></author><author><keyname>Yamashita</keyname><forenames>Shigeru</forenames></author></authors><title>Reconstructing Strings from Substrings with Quantum Queries</title><categories>quant-ph cs.CC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the number of quantum queries made to solve the
problem of reconstructing an unknown string from its substrings in a certain
query model. More concretely, the goal of the problem is to identify an unknown
string $S$ by making queries of the following form: &quot;Is $s$ a substring of
$S$?&quot;, where $s$ is a query string over the given alphabet. The number of
queries required to identify the string $S$ is the query complexity of this
problem.
  First we show a quantum algorithm that exactly identifies the string $S$ with
at most $3/4N + o(N)$ queries, where $N$ is the length of $S$. This contrasts
sharply with the classical query complexity $N$. Our algorithm uses Skiena and
Sundaram's classical algorithm and the Grover search as subroutines. To make
them effectively work, we develop another subroutine that finds a string
appearing only once in $S$, which may have an independent interest. We also
prove two lower bounds. The first one is a general lower bound of
$\Omega(\frac{N}{\log^2{N}})$, which means we cannot achieve a query complexity
of $O(N^{1-\epsilon})$ for any constant $\epsilon$. The other one claims that
if we cannot use queries of length roughly between $\log N$ and $3 \log N$,
then we cannot achieve a query complexity of any sublinear function in $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4693</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4693</id><created>2012-04-20</created><authors><author><keyname>Kadish</keyname><forenames>Harlan</forenames></author><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author></authors><title>Padded polynomials, their cousins, and geometric complexity theory</title><categories>math.AG cs.CC math.RT</categories><comments>8 pages</comments><msc-class>14MXX, 68Q15, 14L30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish basic facts about the varieties of homogeneous polynomials
divisible by powers of linear forms, and explain consequences for geometric
complexity theory. This includes quadratic set-theoretic equations, a
description of the ideal in terms of the kernel of a linear map that
generalizes the Foulkes-Howe map, and an explicit description of the coordinate
ring of the normalization. We also prove asymptotic injectivity of the
Foulkes-Howe map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4710</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4710</id><created>2012-04-20</created><updated>2013-03-29</updated><authors><author><keyname>Audibert</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Lugosi</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Regret in Online Combinatorial Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address online linear optimization problems when the possible actions of
the decision maker are represented by binary vectors. The regret of the
decision maker is the difference between her realized loss and the best loss
she would have achieved by picking, in hindsight, the best possible action. Our
goal is to understand the magnitude of the best possible (minimax) regret. We
study the problem under three different assumptions for the feedback the
decision maker receives: full information, and the partial information models
of the so-called &quot;semi-bandit&quot; and &quot;bandit&quot; problems. Combining the Mirror
Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we
are able to prove optimal bounds for the semi-bandit case. We also recover the
optimal bounds for the full information setting. In the bandit case we discuss
existing results in light of a new lower bound, and suggest a conjecture on the
optimal regret in that case. Finally we also prove that the standard
exponentially weighted average forecaster is provably suboptimal in the setting
of online combinatorial optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4714</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4714</id><created>2012-04-20</created><updated>2013-02-22</updated><authors><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Simons</keyname><forenames>Joe</forenames></author><author><keyname>Strash</keyname><forenames>Darren</forenames></author></authors><title>Dynamic Planar Point Location with Sub-Logarithmic Local Updates</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study planar point location in a collection of disjoint fat regions, and
investigate the complexity of \emph {local updates}: replacing any region by a
different region that is &quot;similar&quot; to the original region. (i.e., the size
differs by at most a constant factor, and distance between the two regions is a
constant times that size). We show that it is possible to create a linear size
data structure that allows for insertions, deletions, and queries in
logarithmic time, and allows for local updates in sub-logarithmic time on a
pointer machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4717</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4717</id><created>2012-04-20</created><authors><author><keyname>Aswani</keyname><forenames>Anil</forenames></author><author><keyname>Master</keyname><forenames>Neal</forenames></author><author><keyname>Taneja</keyname><forenames>Jay</forenames></author><author><keyname>Krioukov</keyname><forenames>Andrew</forenames></author><author><keyname>Culler</keyname><forenames>David</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire</forenames></author></authors><title>Energy-Efficient Building HVAC Control Using Hybrid System LBMPC</title><categories>math.OC cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving the energy-efficiency of heating, ventilation, and air-conditioning
(HVAC) systems has the potential to realize large economic and societal
benefits. This paper concerns the system identification of a hybrid system
model of a building-wide HVAC system and its subsequent control using a hybrid
system formulation of learning-based model predictive control (LBMPC). Here,
the learning refers to model updates to the hybrid system model that
incorporate the heating effects due to occupancy, solar effects, outside air
temperature (OAT), and equipment, in addition to integrator dynamics inherently
present in low-level control. Though we make significant modeling
simplifications, our corresponding controller that uses this model is able to
experimentally achieve a large reduction in energy usage without any
degradations in occupant comfort. It is in this way that we justify the
modeling simplifications that we have made. We conclude by presenting results
from experiments on our building HVAC testbed, which show an average of 1.5MWh
of energy savings per day (p = 0.002) with a 95% confidence interval of 1.0MWh
to 2.1MWh of energy savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4734</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4734</id><created>2012-04-20</created><authors><author><keyname>Racette</keyname><forenames>Chantal</forenames></author></authors><title>Numerical Analysis of Diagonal-Preserving, Ripple-Minimizing and
  Low-Pass Image Resampling Methods</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image resampling is a necessary component of any operation that changes the
size of an image or its geometry.
  Methods tuned for natural image upsampling (roughly speaking, image
enlargement) are analyzed and developed with a focus on their ability to
preserve diagonal features and suppress overshoots. Monotone, locally bounded
and almost monotone &quot;direct&quot; interpolation and filtering methods, as well as
face split and vertex split surface subdivision methods, alone or in
combination, are studied. Key properties are established by way of proofs and
counterexamples as well as numerical experiments involving 1D curve and 2D
diagonal data resampling.
  In addition, the Remez minimax method for the computation of low-cost
polynomial approximations of low-pass filter kernels tuned for natural image
downsampling (roughly speaking, image reduction) is refactored for relative
error minimization in the presence of roots in the interior of the interval of
approximation and so that even and odd functions are approximated with like
polynomials. The accuracy and frequency response of the approximations are
tabulated and plotted against the original, establishing their rapid
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4736</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4736</id><created>2012-04-20</created><authors><author><keyname>Gorlin</keyname><forenames>Andrey</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>C. R.</forenames></author><author><keyname>Smolka</keyname><forenames>Scott A.</forenames></author></authors><title>Model Checking with Probabilistic Tabled Logic Programming</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formulation of the problem of probabilistic model checking as
one of query evaluation over probabilistic logic programs. To the best of our
knowledge, our formulation is the first of its kind, and it covers a rich class
of probabilistic models and probabilistic temporal logics. The inference
algorithms of existing probabilistic logic-programming systems are well defined
only for queries with a finite number of explanations. This restriction
prohibits the encoding of probabilistic model checkers, where explanations
correspond to executions of the system being model checked. To overcome this
restriction, we propose a more general inference algorithm that uses finite
generative structures (similar to automata) to represent families of
explanations. The inference algorithm computes the probability of a possibly
infinite set of explanations directly from the finite generative structure. We
have implemented our inference algorithm in XSB Prolog, and use this
implementation to encode probabilistic model checkers for a variety of temporal
logics, including PCTL and GPL (which subsumes PCTL*). Our experiment results
show that, despite the highly declarative nature of their encodings, the model
checkers constructed in this manner are competitive with their native
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4753</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4753</id><created>2012-04-20</created><authors><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author><author><keyname>Sanita</keyname><forenames>Laura</forenames></author></authors><title>0/1 Polytopes with Quadratic Chvatal Rank</title><categories>math.CO cs.CG cs.DM</categories><comments>15 pages, 2 figures</comments><msc-class>52B11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a polytope P, the Chvatal closure P' is obtained by simultaneously
strengthening all feasible inequalities cx &lt;= b (with integral c) to cx &lt;=
floor(b). The number of iterations of this procedure that are needed until the
integral hull of P is reached is called the Chvatal rank. If P is a subset of
[0,1]^n, then it is known that O(n^2 log n) iterations always suffice
(Eisenbrand and Schulz (1999)) and at least (1+1/e-o(1))n iterations are
sometimes needed (Pokutta and Stauffer (2011)), leaving a huge gap between
lower and upper bounds.
  We prove that there is a polytope contained in the 0/1 cube that has Chvatal
rank Omega(n^2), closing the gap up to a logarithmic factor. In fact, even a
superlinear lower bound was mentioned as an open problem by several authors.
Our choice of P is the convex hull of a semi-random Knapsack polytope and a
single fractional vertex. The main technical ingredient is linking the Chvatal
rank to simultaneous Diophantine approximations w.r.t. the L1-norm of the
normal vector defining P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4758</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4758</id><created>2012-04-20</created><updated>2012-07-16</updated><authors><author><keyname>Xu</keyname><forenames>Yongchao</forenames></author><author><keyname>G&#xe9;raud</keyname><forenames>Thierry</forenames></author><author><keyname>Najman</keyname><forenames>Laurent</forenames></author></authors><title>Morphological Filtering in Shape Spaces: Applications using Tree-Based
  Image Representations</title><categories>cs.CV math.OA</categories><comments>4 pages, will appear in 21st International Conference on Pattern
  Recognition (ICPR 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connected operators are filtering tools that act by merging elementary
regions of an image. A popular strategy is based on tree-based image
representations: for example, one can compute an attribute on each node of the
tree and keep only the nodes for which the attribute is sufficiently strong.
This operation can be seen as a thresholding of the tree, seen as a graph whose
nodes are weighted by the attribute. Rather than being satisfied with a mere
thresholding, we propose to expand on this idea, and to apply connected filters
on this latest graph. Consequently, the filtering is done not in the space of
the image, but on the space of shapes build from the image. Such a processing
is a generalization of the existing tree-based connected operators. Indeed, the
framework includes classical existing connected operators by attributes. It
also allows us to propose a class of novel connected operators from the
leveling family, based on shape attributes. Finally, we also propose a novel
class of self-dual connected operators that we call morphological shapings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4763</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4763</id><created>2012-04-20</created><authors><author><keyname>Owen</keyname><forenames>Art B.</forenames></author></authors><title>Better estimation of small Sobol' sensitivity indices</title><categories>stat.ME cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method for estimating Sobol' indices is proposed. The new method makes
use of 3 independent input vectors rather than the usual 2. It attains much
greater accuracy on problems where the target Sobol' index is small, even
outperforming some oracles which adjust using the true but unknown mean of the
function. When the target Sobol' index is quite large, the oracles do better
than the new method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4765</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4765</id><created>2012-04-20</created><authors><author><keyname>D'souza</keyname><forenames>Julius</forenames></author></authors><title>String Trees</title><categories>cs.DS</categories><comments>5 pages</comments><acm-class>E.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A string-like compact data structure for unlabelled rooted trees is given
using 2n bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4779</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4779</id><created>2012-04-21</created><updated>2012-05-16</updated><authors><author><keyname>Muranushi</keyname><forenames>Takayuki</forenames></author></authors><title>Paraiso : An Automated Tuning Framework for Explicit Solvers of Partial
  Differential Equations</title><categories>astro-ph.IM cs.DC cs.NE</categories><comments>52 pages, 14 figures, accepted for publications in Computational
  Science and Discovery</comments><msc-class>65M22 (Primary) 68N15, 68N18, 65K10 (Secondary)</msc-class><doi>10.1088/1749-4699/5/1/015003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Paraiso, a domain specific language embedded in functional
programming language Haskell, for automated tuning of explicit solvers of
partial differential equations (PDEs) on GPUs as well as multicore CPUs. In
Paraiso, one can describe PDE solving algorithms succinctly using tensor
equations notation. Hydrodynamic properties, interpolation methods and other
building blocks are described in abstract, modular, re-usable and combinable
forms, which lets us generate versatile solvers from little set of Paraiso
source codes.
  We demonstrate Paraiso by implementing a compressive hydrodynamics solver. A
single source code less than 500 lines can be used to generate solvers of
arbitrary dimensions, for both multicore CPUs and GPUs. We demonstrate both
manual annotation based tuning and evolutionary computing based automated
tuning of the program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4783</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4783</id><created>2012-04-21</created><authors><author><keyname>Sobana</keyname><forenames>S.</forenames></author><author><keyname>Prabha</keyname><forenames>S. Krishna</forenames></author></authors><title>An Efficient Method For Multichannel Wireless Mesh Networks With Pulse
  Coupled Neural Network</title><categories>cs.NI</categories><comments>4 pages, 4 figures</comments><msc-class>68M10</msc-class><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 3, No. 1, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi cast communication is a key technology for wireless mesh networks.
Multicast provides efficient data distribution among a group of nodes,
Generally sensor networks and MANETs uses multicast algorithms which are
designed to be energy efficient and to achieve optimal route discovery among
mobile nodes whereas wireless mesh networks needs to maximize throughput. Here
we propose two multicast algorithms: The Level Channel Assignment (LCA)
algorithm and the Multi-Channel Multicast (MCM) algorithm to improve the
throughput for multichannel sand multi interface mesh networks. The algorithm
builds efficient multicast trees by minimizing the number of relay nodes and
total hop count distance of the trees. Shortest path computation is a classical
combinatorial optimization problem. Neural networks have been used for
processing path optimization problem. Pulse Coupled Neural Networks (PCNNS)
suffer from high computational cast for very long paths we propose a new PCNN
modal called dual source PCNN (DSPCNN) which can improve the computational
efficiency two auto waves are produced by DSPCNN one comes from source neuron
and other from goal neuron when the auto waves from these two sources meet the
DSPCNN stops and then the shortest path is found by backtracking the two auto
waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4804</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4804</id><created>2012-04-21</created><authors><author><keyname>Berdine</keyname><forenames>Josh</forenames></author><author><keyname>Calcagno</keyname><forenames>Cristiano</forenames></author><author><keyname>O'Hearn</keyname><forenames>Peter W.</forenames></author></authors><title>Verification Condition Generation and Variable Conditions in Smallfoot</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes are a companion to [1] which describe - the variable conditions
that Smallfoot checks, - the analysis used to check them, - the algorithm used
to compute a set of verification conditions corresponding to an annotated
program, and - the treatment of concurrent resource initialization code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4805</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4805</id><created>2012-04-21</created><authors><author><keyname>Hastings</keyname><forenames>Janna</forenames></author><author><keyname>Batchelor</keyname><forenames>Colin</forenames></author><author><keyname>Neuhaus</keyname><forenames>Fabian</forenames></author><author><keyname>Steinbeck</keyname><forenames>Christoph</forenames></author></authors><title>What's in an `is about' link? Chemical diagrams and the Information
  Artifact Ontology</title><categories>cs.AI cs.LO</categories><comments>10 pages, 5 figures, presented at the 2nd International Conference on
  Biomedical Ontology (ICBO) 2011</comments><journal-ref>CEUR-WS Volume 833, 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Information Artifact Ontology is an ontology in the domain of information
entities. Core to the definition of what it is to be an information entity is
the claim that an information entity must be `about' something, which is
encoded in an axiom expressing that all information entities are about some
entity. This axiom comes into conflict with ontological realism, since many
information entities seem to be about non-existing entities, such as
hypothetical molecules. We discuss this problem in the context of diagrams of
molecules, a kind of information entity pervasively used throughout
computational chemistry. We then propose a solution that recognizes that
information entities such as diagrams are expressions of diagrammatic
languages. In so doing, we not only address the problem of classifying diagrams
that seem to be about non-existing entities but also allow a more sophisticated
categorisation of information entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4809</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4809</id><created>2012-04-21</created><authors><author><keyname>Bai</keyname><forenames>Shuotian</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author><author><keyname>Cheng</keyname><forenames>Li</forenames></author></authors><title>Big-Five Personality Prediction Based on User Behaviors at Social
  Network Sites</title><categories>cs.CY</categories><comments>7 pages, predicting user's Big-five personality based on Social
  Network behaviors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many customer services are already available at Social Network Sites (SNSs),
including user recommendation and media interaction, to name a few. There are
strong desires to provide online users more dedicated and personalized services
that fit into individual's need, usually strongly depending on the inner
personalities of the user. However, little has been done to conduct proper
psychological analysis, crucial for explaining the user's outer behaviors from
their inner personality. In this paper, we propose an approach that intends to
facilitate this line of research by directly predicting the so called Big-Five
Personality from user's SNS behaviors. Comparing to the conventional
inventory-based psychological analysis, we demonstrate via experimental studies
that users' personalities can be predicted with reasonable precision based on
their online behaviors. Except for proving some former behavior-personality
correlation results, our experiments show that extraversion is positively
related to one's status republishing proportion and neuroticism is positively
related to the proportion of one's angry blogs (blogs making people angry).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4826</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4826</id><created>2012-04-21</created><authors><author><keyname>Kalla</keyname><forenames>C.</forenames></author><author><keyname>Klein</keyname><forenames>C.</forenames></author></authors><title>Computation of the topological type of a real Riemann surface</title><categories>math.AG cs.CG</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for the computation of the topological type of a real
compact Riemann surface associated to an algebraic curve, i.e., its genus and
the properties of the set of fixed points of the anti-holomorphic involution
$\tau$, namely, the number of its connected components, and whether this set
divides the surface into one or two connected components. This is achieved by
transforming an arbitrary canonical homology basis to a homology basis where
the $\mathcal{A}$-cycles are invariant under the anti-holomorphic involution
$\tau$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4827</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4827</id><created>2012-04-21</created><authors><author><keyname>Liang</keyname><forenames>Hongyu</forenames></author></authors><title>On the Signed (Total) $k$-Domination Number of a Graph</title><categories>cs.DM math.CO</categories><comments>Accepted by JCMCC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k$ be a positive integer and $G=(V,E)$ be a graph of minimum degree at
least $k-1$. A function $f:V\rightarrow \{-1,1\}$ is called a \emph{signed
$k$-dominating function} of $G$ if $\sum_{u\in N_G[v]}f(u)\geq k$ for all $v\in
V$. The \emph{signed $k$-domination number} of $G$ is the minimum value of
$\sum_{v\in V}f(v)$ taken over all signed $k$-dominating functions of $G$. The
\emph{signed total $k$-dominating function} and \emph{signed total
$k$-domination number} of $G$ can be similarly defined by changing the closed
neighborhood $N_G[v]$ to the open neighborhood $N_G(v)$ in the definition. The
\emph{upper signed $k$-domination number} is the maximum value of $\sum_{v\in
V}f(v)$ taken over all \emph{minimal} signed $k$-dominating functions of $G$.
In this paper, we study these graph parameters from both algorithmic complexity
and graph-theoretic perspectives. We prove that for every fixed $k\geq 1$, the
problems of computing these three parameters are all \NP-hard. We also present
sharp lower bounds on the signed $k$-domination number and signed total
$k$-domination number for general graphs in terms of their minimum and maximum
degrees, generalizing several known results about signed domination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4829</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4829</id><created>2012-04-21</created><authors><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author></authors><title>Improved Balas and Mazzola Linearization for Quadratic 0-1 Programs with
  Application in a New Cutting Plane Algorithm</title><categories>cs.DS math.OC</categories><journal-ref>Int. J. Communications, Network and System Sciences, 2012</journal-ref><doi>10.4236/ijcns.2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Balas and Mazzola linearization (BML) is widely used in devising cutting
plane algorithms for quadratic 0-1 programs. In this article, we improve BML by
first strengthening the primal formulation of BML and then considering the dual
formulation. Additionally, a new cutting plane algorithm is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4835</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4835</id><created>2012-04-21</created><authors><author><keyname>Farzan</keyname><forenames>Arash</forenames></author><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author></authors><title>Succinct Indices for Range Queries with applications to Orthogonal Range
  Maxima</title><categories>cs.DS</categories><comments>To appear in ICALP 2012</comments><report-no>Leicester CS-TR-12-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of preprocessing $N$ points in 2D, each endowed with
a priority, to answer the following queries: given a axis-parallel rectangle,
determine the point with the largest priority in the rectangle. Using the ideas
of the \emph{effective entropy} of range maxima queries and \emph{succinct
indices} for range maxima queries, we obtain a structure that uses O(N) words
and answers the above query in $O(\log N \log \log N)$ time. This is a direct
improvement of Chazelle's result from FOCS 1985 for this problem -- Chazelle
required $O(N/\epsilon)$ words to answer queries in $O((\log N)^{1+\epsilon})$
time for any constant $\epsilon &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4840</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4840</id><created>2012-04-21</created><authors><author><keyname>Rebeiz</keyname><forenames>Eric</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Energy-Delay Tradeoff and Dynamic Sleep Switching for Bluetooth-Like
  Body-Area Sensor Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Extended version (with proofs details in the Appendix) of a paper
  accepted for publication on the IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless technology enables novel approaches to healthcare, in particular the
remote monitoring of vital signs and other parameters indicative of people's
health. This paper considers a system scenario relevant to such applications,
where a smart-phone acts as a data-collecting hub, gathering data from a number
of wireless-capable body sensors, and relaying them to a healthcare provider
host through standard existing cellular networks. Delay of critical data and
sensors' energy efficiency are both relevant and conflicting issues. Therefore,
it is important to operate the wireless body-area sensor network at some
desired point close to the optimal energy-delay tradeoff curve. This tradeoff
curve is a function of the employed physical-layer protocol: in particular, it
depends on the multiple-access scheme and on the coding and modulation schemes
available. In this work, we consider a protocol closely inspired by the
widely-used Bluetooth standard. First, we consider the calculation of the
minimum energy function, i.e., the minimum sum energy per symbol that
guarantees the stability of all transmission queues in the network. Then, we
apply the general theory developed by Neely to develop a dynamic scheduling
policy that approaches the optimal energy-delay tradeoff for the network at
hand. Finally, we examine the queue dynamics and propose a novel policy that
adaptively switches between connected and disconnected (sleeping) modes. We
demonstrate that the proposed policy can achieve significant gains in the
realistic case where the control &quot;NULL&quot; packets necessary to maintain the
connection alive, have a non-zero energy cost, and the data arrival statistics
corresponding to the sensed physical process are bursty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4847</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4847</id><created>2012-04-21</created><authors><author><keyname>Antoniadis</keyname><forenames>Panayotis</forenames></author><author><keyname>Fdida</keyname><forenames>Serge</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Jin</keyname><forenames>Youngmi</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>CSMA Local Area Networking under Dynamic Altruism</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider medium access control of local area networks
(LANs) under limited-information conditions as befits a distributed system.
Rather than assuming &quot;by rule&quot; conformance to a protocol designed to regulate
packet-flow rates (e.g., CSMA windowing), we begin with a non-cooperative game
framework and build a dynamic altruism term into the net utility. The effects
of altruism are analyzed at Nash equilibrium for both the ALOHA and CSMA
frameworks in the quasistationary (fictitious play) regime. We consider either
power or throughput based costs of networking, and the cases of identical or
heterogeneous (independent) users/players. In a numerical study we consider
diverse players, and we see that the effects of altruism for similar players
can be beneficial in the presence of significant congestion, but excessive
altruism may lead to underuse of the channel when demand is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4864</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4864</id><created>2012-04-22</created><authors><author><keyname>GuoHua</keyname><forenames>Zhang</forenames></author><author><keyname>XinMei</keyname><forenames>Wang</forenames></author></authors><title>Tight lower bound of consecutive lengths for QC-LDPC codes with girth
  twelve</title><categories>cs.IT math.IT</categories><comments>9 pages,1 figure. the chinese version of this paper has been
  published in Chinese Sci Bull,2011, 56 (19):1578-1582</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an arbitrary (3,L) QC-LDPC code with a girth of twelve, a tight lower
bound of the consecutive lengths is proposed. For an arbitrary length above the
bound the resultant code necessarily has a girth of twelve, and for the length
meeting the bound, the corresponding code inevitably has a girth smaller than
twelve. The conclusion can play an important role in the proofs of the
existence of large-girth QC-LDPC codes, the construction of large-girth QC-LDPC
codes based on the Chinese remainder theorem, and the construction of LDPC
codes with the guaranteed error correction capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4865</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4865</id><created>2012-04-22</created><updated>2013-04-11</updated><authors><author><keyname>Farivar</keyname><forenames>Masoud</forenames></author><author><keyname>Low</keyname><forenames>Steven H.</forenames></author></authors><title>Branch Flow Model: Relaxations and Convexification (Parts I, II)</title><categories>cs.SY math.OC</categories><comments>A preliminary and abridged version has appeared in IEEE CDC, December
  2012</comments><journal-ref>IEEE Transactions on Power Systems, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a branch flow model for the anal- ysis and optimization of mesh as
well as radial networks. The model leads to a new approach to solving optimal
power flow (OPF) that consists of two relaxation steps. The first step
eliminates the voltage and current angles and the second step approximates the
resulting problem by a conic program that can be solved efficiently. For radial
networks, we prove that both relaxation steps are always exact, provided there
are no upper bounds on loads. For mesh networks, the conic relaxation is always
exact but the angle relaxation may not be exact, and we provide a simple way to
determine if a relaxed solution is globally optimal. We propose convexification
of mesh networks using phase shifters so that OPF for the convexified network
can always be solved efficiently for an optimal solution. We prove that
convexification requires phase shifters only outside a spanning tree of the
network and their placement depends only on network topology, not on power
flows, generation, loads, or operating constraints. Part I introduces our
branch flow model, explains the two relaxation steps, and proves the conditions
for exact relaxation. Part II describes convexification of mesh networks, and
presents simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4867</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4867</id><created>2012-04-22</created><authors><author><keyname>Bagon</keyname><forenames>Shai</forenames></author><author><keyname>Galun</keyname><forenames>Meirav</forenames></author></authors><title>A Unified Multiscale Framework for Discrete Energy Minimization</title><categories>cs.CV cs.DM</categories><comments>11 pages, 8 figures, 6 tables, submitted to IJCV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete energy minimization is a ubiquitous task in computer vision, yet is
NP-hard in most cases. In this work we propose a multiscale framework for
coping with the NP-hardness of discrete optimization. Our approach utilizes
algebraic multiscale principles to efficiently explore the discrete solution
space, yielding improved results on challenging, non-submodular energies for
which current methods provide unsatisfactory approximations. In contrast to
popular multiscale methods in computer vision, that builds an image pyramid,
our framework acts directly on the energy to construct an energy pyramid.
Deriving a multiscale scheme from the energy itself makes our framework
application independent and widely applicable. Our framework gives rise to two
complementary energy coarsening strategies: one in which coarser scales involve
fewer variables, and a more revolutionary one in which the coarser scales
involve fewer discrete labels. We empirically evaluated our unified framework
on a variety of both non-submodular and submodular energies, including energies
from Middlebury benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4874</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4874</id><created>2012-04-22</created><authors><author><keyname>Thuan</keyname><forenames>Le Quang</forenames></author><author><keyname>Camlibel</keyname><forenames>Kanat</forenames></author></authors><title>On the existence, uniqueness and nature of Caratheodory and Filippov
  solutions for bimodal piecewise affine dynamical systems</title><categories>cs.SY math.CA</categories><comments>18 pages, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the well-posedness (in the sense of existence and
uniqueness of solutions) and nature of solutions for discontinuous bimodal
piecewise affine systems in a differential inclusion setting. First, we show
that the conditions guaranteeing uniqueness of Filippov solutions in the
context of general differential inclusions are quite restrictive when applied
to bimodal piecewise affine systems. Later, we present a set of necessary and
sufficient conditions for uniqueness of Filippov solutions for bimodal
piecewise affine systems. We also study the so-called Zeno behavior
(possibility of infinitely many switchings within a finite time interval) for
Filippov solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4880</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4880</id><created>2012-04-22</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author><author><keyname>Villanger</keyname><forenames>Yngve</forenames></author></authors><title>A Polynomial kernel for Proper Interval Vertex Deletion</title><categories>cs.DS</categories><comments>13 pages, 1 figure</comments><msc-class>05C85 (Primary) 05C75 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the problem of deleting at most k vertices to obtain a
proper interval graph (Proper Interval Vertex Deletion) is fixed parameter
tractable. However, whether the problem admits a polynomial kernel or not was
open. Here, we answers this question in affirmative by obtaining a polynomial
kernel for Proper Interval Vertex Deletion. This resolves an open question of
van Bevern, Komusiewicz, Moser, and Niedermeier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4905</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4905</id><created>2012-04-22</created><authors><author><keyname>Rajesh</keyname><forenames>R</forenames></author><author><keyname>K</keyname><forenames>Deekshith P</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Capacity of Gaussian MAC Powered by Energy Harvesters without Storage
  Buffer</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Gaussian multiple access channel (GMAC) where the users are
sensor nodes powered by energy harvesters. The energy harvester has no buffer
to store the harvested energy and hence the energy need to be expended
immediately. We assume that the decoder has perfect knowledge of the energy
harvesting process. We characterize the capacity region of such a GMAC. We also
provide the capacity region when one of the users has infinite buffer to store
the energy harvested. Next we find the achievable rates when the energy
harvesting information is not available at the decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4906</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4906</id><created>2012-04-22</created><authors><author><keyname>Bojanczyk</keyname><forenames>Miko\laj</forenames></author><author><keyname>Szawiel</keyname><forenames>Stanis\law</forenames></author><author><keyname>Zawadowski</keyname><forenames>Marek</forenames></author></authors><title>Rigidity is undecidable</title><categories>math.LO cs.LO math.CT</categories><comments>8 pages</comments><msc-class>03D35, 03C05, 03G30, 18C10, 18C15</msc-class><doi>10.1017/S096012951300087X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem `whether a finite set of regular-linear axioms
defines a rigid theory' is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4909</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4909</id><created>2012-04-22</created><authors><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author><author><keyname>Qureshi</keyname><forenames>Waseem</forenames></author></authors><title>Evaluation of the Design Metric to Reduce the Number of Defects in
  Software Development</title><categories>cs.SE</categories><comments>9 Pages</comments><journal-ref>International Journal of Information Technology and Computer
  Science (IJITCS), Vol. 4/4, pp. 9-17, April 2012</journal-ref><doi>10.5815/ijitcs.2012.04.02</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software design is one of the most important and key activities in the system
development life cycle (SDLC) phase that ensures the quality of software.
Different key areas of design are very vital to be taken into consideration
while designing software. Software design describes how the software system is
decomposed and managed in smaller components. Object-oriented (OO) paradigm has
facilitated software industry with more reliable and manageable software and
its design. The quality of the software design can be measured through
different metrics such as Chidamber and Kemerer (CK) design metrics, Mood
Metrics &amp; Lorenz and Kidd metrics. CK metrics is one of the oldest and most
reliable metrics among all metrics available to software industry to evaluate
OO design. This paper presents an evaluation of CK metrics to propose an
improved CK design metrics values to reduce the defects during software design
phase in software. This paper will also describe that whether a significant
effect of any CK design metrics exists on total number of defects per module or
not. This is achieved by conducting survey in two software development
companies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4914</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4914</id><created>2012-04-22</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Quantum Interference in Cognition: Structural Aspects of the Brain</title><categories>cs.AI cs.CL quant-ph</categories><comments>15 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1004.2530</comments><journal-ref>In V. Ovchinnikov and P. Dini (Eds.), IARIA, Proceedings of the
  Sixth International Conference on Quantum, Nano and Micro Technologies, pp.
  33-41, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify the presence of typically quantum effects, namely 'superposition'
and 'interference', in what happens when human concepts are combined, and
provide a quantum model in complex Hilbert space that represents faithfully
experimental data measuring the situation of combining concepts. Our model
shows how 'interference of concepts' explains the effects of underextension and
overextension when two concepts combine to the disjunction of these two
concepts. This result supports our earlier hypothesis that human thought has a
superposed two-layered structure, one layer consisting of 'classical logical
thought' and a superposed layer consisting of 'quantum conceptual thought'.
Possible connections with recent findings of a 'grid-structure' for the brain
are analyzed, and influences on the mind/brain relation, and consequences on
applied disciplines, such as artificial intelligence and quantum computation,
are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4927</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4927</id><created>2012-04-22</created><authors><author><keyname>Bennett</keyname><forenames>Casey</forenames></author><author><keyname>Doub</keyname><forenames>Tom</forenames></author><author><keyname>Selove</keyname><forenames>Rebecca</forenames></author></authors><title>EHRs Connect Research and Practice: Where Predictive Modeling,
  Artificial Intelligence, and Clinical Decision Support Intersect</title><categories>cs.AI cs.DB stat.ML</categories><comments>Keywords: Data Mining; Decision Support Systems, Clinical; Electronic
  Health Records; Implementation; Evidence-Based Medicine; Data Warehouse;
  (2012). EHRs Connect Research and Practice: Where Predictive Modeling,
  Artificial Intelligence, and Clinical Decision Support Intersect. Health
  Policy and Technology. arXiv admin note: substantial text overlap with
  arXiv:1112.1668</comments><journal-ref>Health Policy and Technology 1(2): 105-114 (2012)</journal-ref><doi>10.1016/j.hlpt.2012.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objectives: Electronic health records (EHRs) are only a first step in
capturing and utilizing health-related data - the challenge is turning that
data into useful information. Furthermore, EHRs are increasingly likely to
include data relating to patient outcomes, functionality such as clinical
decision support, and genetic information as well, and, as such, can be seen as
repositories of increasingly valuable information about patients' health
conditions and responses to treatment over time. Methods: We describe a case
study of 423 patients treated by Centerstone within Tennessee and Indiana in
which we utilized electronic health record data to generate predictive
algorithms of individual patient treatment response. Multiple models were
constructed using predictor variables derived from clinical, financial and
geographic data. Results: For the 423 patients, 101 deteriorated, 223 improved
and in 99 there was no change in clinical condition. Based on modeling of
various clinical indicators at baseline, the highest accuracy in predicting
individual patient response ranged from 70-72% within the models tested. In
terms of individual predictors, the Centerstone Assessment of Recovery Level -
Adult (CARLA) baseline score was most significant in predicting outcome over
time (odds ratio 4.1 + 2.27). Other variables with consistently significant
impact on outcome included payer, diagnostic category, location and provision
of case management services. Conclusions: This approach represents a promising
avenue toward reducing the current gap between research and practice across
healthcare, developing data-driven clinical decision support based on
real-world populations, and serving as a component of embedded clinical
artificial intelligences that &quot;learn&quot; over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4928</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4928</id><created>2012-04-22</created><authors><author><keyname>Miguel</keyname><forenames>Maxi San</forenames></author><author><keyname>Johnson</keyname><forenames>Jeffrey H.</forenames></author><author><keyname>Kertesz</keyname><forenames>Janos</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>D&#xed;az-Guilera</keyname><forenames>Albert</forenames></author><author><keyname>MacKay</keyname><forenames>Robert S.</forenames></author><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author><author><keyname>Erdi</keyname><forenames>Peter</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Challenges in Complex Systems Science</title><categories>nlin.AO cs.SI physics.soc-ph</categories><doi>10.1140/epjst/e2012-01694-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FuturICT foundations are social science, complex systems science, and ICT.
The main concerns and challenges in the science of complex systems in the
context of FuturICT are laid out in this paper with special emphasis on the
Complex Systems route to Social Sciences. This include complex systems having:
many heterogeneous interacting parts; multiple scales; complicated transition
laws; unexpected or unpredicted emergence; sensitive dependence on initial
conditions; path-dependent dynamics; networked hierarchical connectivities;
interaction of autonomous agents; self-organisation; non-equilibrium dynamics;
combinatorial explosion; adaptivity to changing environments; co-evolving
subsystems; ill-defined boundaries; and multilevel dynamics. In this context,
science is seen as the process of abstracting the dynamics of systems from
data. This presents many challenges including: data gathering by large-scale
experiment, participatory sensing and social computation, managing huge
distributed dynamic and heterogeneous databases; moving from data to dynamical
models, going beyond correlations to cause-effect relationships, understanding
the relationship between simple and comprehensive models with appropriate
choices of variables, ensemble modeling and data assimilation, modeling systems
of systems of systems with many levels between micro and macro; and formulating
new approaches to prediction, forecasting, and risk, especially in systems that
can reflect on and change their behaviour in response to predictions, and
systems whose apparently predictable behaviour is disrupted by apparently
unpredictable rare or extreme events. These challenges are part of the FuturICT
agenda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4948</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4948</id><created>2012-04-22</created><updated>2012-04-28</updated><authors><author><keyname>Michaliszyn</keyname><forenames>Jakub</forenames></author><author><keyname>Muscholl</keyname><forenames>Anca</forenames></author><author><keyname>Staworko</keyname><forenames>S&#x142;awek</forenames></author><author><keyname>Wieczorek</keyname><forenames>Piotr</forenames></author><author><keyname>Wu</keyname><forenames>Zhilin</forenames></author></authors><title>On Injective Embeddings of Tree Patterns</title><categories>cs.DB</categories><comments>Under conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study three different kinds of embeddings of tree patterns:
weakly-injective, ancestor-preserving, and lca-preserving. While each of them
is often referred to as injective embedding, they form a proper hierarchy and
their computational properties vary (from P to NP-complete). We present a
thorough study of the complexity of the model checking problem i.e., is there
an embedding of a given tree pattern in a given tree, and we investigate the
impact of various restrictions imposed on the tree pattern: bound on the degree
of a node, bound on the height, and type of allowed labels and edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4951</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4951</id><created>2012-04-22</created><authors><author><keyname>Widanapathirana</keyname><forenames>C. H.</forenames></author><author><keyname>Goi</keyname><forenames>Bok-Min</forenames></author><author><keyname>Lim</keyname><forenames>Sim Moh</forenames></author></authors><title>MPIFA: A Modified Protocol Independent Fairness Algorithm for Community
  Wireless Mesh Networks</title><categories>cs.NI</categories><comments>Innovative Technologies in Intelligent Systems and Industrial
  Applications(CITISIA) 2009</comments><doi>10.1109/CITISIA.2009.5224195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community Wireless Mesh Networks (WMN) is a paradigm in wireless
communication of 21st centuary as means of providing high speed braodband
access. Un-cooperative nodes, both selfish and malicious proves to be a
significant threat in Community WMN that require a solution independent of
routing protocols being used. We propose to implement Modified PIFA (MPIFA), an
Improved version of Protocol Independent Fairness Algorithm (PIFA) proposed by
Younghwan Yoo, Sanghyun and P. Agrawal [6] with ability to cater specific
requirements in Community WMN. MPIFA has malicious nodes detection rate
improvement of 50% when nodes demonstrate low probabilistic malicious behavior
of 10% to circumvent the security measures in place. Improvements were also
made to reduce false malicious node detections to 4% when node-to-node link
failures occur in Community WMN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4982</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4982</id><created>2012-04-23</created><authors><author><keyname>Gruber</keyname><forenames>Hermann</forenames></author><author><keyname>Lee</keyname><forenames>Jonathan</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Enumerating regular expressions and their languages</title><categories>cs.FL math.CO</categories><comments>Chapter 13 in the handbook &quot;AutoMathA&quot;. 30 + 1 pages, 3 figures. To
  view the source code attachments, please download and extract the gzipped tar
  source file listed under &quot;Other formats&quot;</comments><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we discuss the problem of enumerating distinct regular
expressions by size and the regular languages they represent. We discuss
various notions of the size of a regular expression that appear in the
literature and their advantages and disadvantages. We consider a formal
definition of regular expressions using a context-free grammar.
  We then show how to enumerate strings generated by an unambiguous
context-free grammar using the Chomsky-Sch\&quot;utzenberger theorem. This theorem
allows one to construct an algebraic equation whose power series expansion
provides the enumeration. Classical tools from complex analysis, such as
singularity analysis, can then be used to determine the asymptotic behavior of
the enumeration.
  We use these algebraic and analytic methods to obtain asymptotic estimates on
the number of regular expressions of size n. A single regular language can
often be described by several regular expressions, and we estimate the number
of distinct languages denoted by regular expressions of size n. We also give
asymptotic estimates for these quantities. For the first few values, we provide
exact enumeration results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4988</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4988</id><created>2012-04-23</created><authors><author><keyname>Emmanuel</keyname><forenames>Jeandel</forenames><affiliation>LIRMM</affiliation></author><author><keyname>Vanier</keyname><forenames>Pascal</forenames><affiliation>LIF</affiliation></author></authors><title>Hardness of conjugacy and factorization of multidimensional subshifts of
  finite type</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate here the hardness of conjugacy and factorization of subshifts
of finite type (SFTs) in dimension $d&gt;1$. In particular, we prove that the
factorization problem is $\Sigma^0_3$-complete and the conjugacy problem
$\Sigma^0_1$-complete in the arithmetical hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4989</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4989</id><created>2012-04-23</created><authors><author><keyname>Taillandier</keyname><forenames>Patrick</forenames><affiliation>COGIT, UMMISCO</affiliation></author><author><keyname>Duch&#xea;ne</keyname><forenames>C&#xe9;cile</forenames><affiliation>COGIT</affiliation></author><author><keyname>Drogoul</keyname><forenames>Alexis</forenames><affiliation>UMMISCO, MSI</affiliation></author></authors><title>Using Belief Theory to Diagnose Control Knowledge Quality. Application
  to cartographic generalisation</title><categories>cs.AI</categories><comments>Best paper award, International Conference on Computing and
  Communication Technologies (IEEE-RIVF), Danang : Viet Nam (2009)</comments><proxy>ccsd</proxy><doi>10.1109/RIVF.2009.5174663</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both humans and artificial systems frequently use trial and error methods to
problem solving. In order to be effective, this type of strategy implies having
high quality control knowledge to guide the quest for the optimal solution.
Unfortunately, this control knowledge is rarely perfect. Moreover, in
artificial systems-as in humans-self-evaluation of one's own knowledge is often
difficult. Yet, this self-evaluation can be very useful to manage knowledge and
to determine when to revise it. The objective of our work is to propose an
automated approach to evaluate the quality of control knowledge in artificial
systems based on a specific trial and error strategy, namely the informed tree
search strategy. Our revision approach consists in analysing the system's
execution logs, and in using the belief theory to evaluate the global quality
of the knowledge. We present a real-world industrial application in the form of
an experiment using this approach in the domain of cartographic generalisation.
Thus far, the results of using our approach have been encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4990</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4990</id><created>2012-04-23</created><authors><author><keyname>Taillandier</keyname><forenames>Patrick</forenames><affiliation>UMMISCO</affiliation></author><author><keyname>Gaffuri</keyname><forenames>Julien</forenames><affiliation>COGIT</affiliation></author></authors><title>Objective Function Designing Led by User Preferences Acquisition</title><categories>cs.LG cs.AI cs.HC</categories><comments>International Conference on Information Technology and Applications,
  Hanoi : Viet Nam (2009)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world problems can be defined as optimisation problems in which the
aim is to maximise an objective function. The quality of obtained solution is
directly linked to the pertinence of the used objective function. However,
designing such function, which has to translate the user needs, is usually
fastidious. In this paper, a method to help user objective functions designing
is proposed. Our approach, which is highly interactive, is based on man machine
dialogue and more particularly on the comparison of problem instance solutions
by the user. We propose an experiment in the domain of cartographic
generalisation that shows promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4991</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4991</id><created>2012-04-23</created><authors><author><keyname>Taillandier</keyname><forenames>Patrick</forenames><affiliation>COGIT, UMMISCO</affiliation></author><author><keyname>Duch&#xea;ne</keyname><forenames>C&#xe9;cile</forenames><affiliation>COGIT</affiliation></author><author><keyname>Drogoul</keyname><forenames>Alexis</forenames><affiliation>UMMISCO, MSI</affiliation></author></authors><title>Knowledge revision in systems based on an informed tree search strategy
  : application to cartographic generalisation</title><categories>cs.AI cs.LG</categories><comments>Knowledge Revision; Problem Solving; Informed Tree Search Strategy;
  Cartographic Generalisation., Paris : France (2008)</comments><proxy>ccsd</proxy><doi>10.1145/1456223.1456281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real world problems can be expressed as optimisation problems. Solving
this kind of problems means to find, among all possible solutions, the one that
maximises an evaluation function. One approach to solve this kind of problem is
to use an informed search strategy. The principle of this kind of strategy is
to use problem-specific knowledge beyond the definition of the problem itself
to find solutions more efficiently than with an uninformed strategy. This kind
of strategy demands to define problem-specific knowledge (heuristics). The
efficiency and the effectiveness of systems based on it directly depend on the
used knowledge quality. Unfortunately, acquiring and maintaining such knowledge
can be fastidious. The objective of the work presented in this paper is to
propose an automatic knowledge revision approach for systems based on an
informed tree search strategy. Our approach consists in analysing the system
execution logs and revising knowledge based on these logs by modelling the
revision problem as a knowledge space exploration problem. We present an
experiment we carried out in an application domain where informed search
strategies are often used: cartographic generalisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4997</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4997</id><created>2012-04-23</created><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Optimal Orthogonal Graph Drawing with Convex Bend Costs</title><categories>cs.DS cs.DM</categories><comments>31 pages, 14 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, the quality of orthogonal planar drawings is quantified by
either the total number of bends, or the maximum number of bends per edge.
However, this neglects that in typical applications, edges have varying
importance. Moreover, as bend minimization over all planar embeddings is
NP-hard, most approaches focus on a fixed planar embedding.
  We consider the problem OptimalFlexDraw that is defined as follows. Given a
planar graph G on n vertices with maximum degree 4 and for each edge e a cost
function cost_e : N_0 --&gt; R defining costs depending on the number of bends on
e, compute an orthogonal drawing of G of minimum cost. Note that this optimizes
over all planar embeddings of the input graphs, and the cost functions allow
fine-grained control on the bends of edges.
  In this generality OptimalFlexDraw is NP-hard. We show that it can be solved
efficiently if 1) the cost function of each edge is convex and 2) the first
bend on each edge does not cause any cost (which is a condition similar to the
positive flexibility for the decision problem FlexDraw). Moreover, we show the
existence of an optimal solution with at most three bends per edge except for a
single edge per block (maximal biconnected component) with up to four bends.
For biconnected graphs we obtain a running time of O(n T_flow(n)), where
T_flow(n) denotes the time necessary to compute a minimum-cost flow in a planar
flow network with multiple sources and sinks. For connected graphs that are not
biconnected we need an additional factor of O(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5001</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5001</id><created>2012-04-23</created><updated>2012-04-24</updated><authors><author><keyname>Grigorescu</keyname><forenames>Andrea</forenames></author><author><keyname>Rudnicki</keyname><forenames>Marek</forenames></author><author><keyname>Isik</keyname><forenames>Michael</forenames></author><author><keyname>Hemmert</keyname><forenames>Werner</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author></authors><title>Improving the Entropy Estimate of Neuronal Firings of Modeled Cochlear
  Nucleus Neurons</title><categories>q-bio.NC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence information theoretical tools are used to investigate
the statistical properties of modeled cochlear nucleus globular bushy cell
spike trains. The firing patterns are obtained from a simulation software that
generates sample spike trains from any auditory input. Here we analyze for the
first time the responses of globular bushy cells to voiced and unvoiced speech
sounds. Classical entropy estimates, such as the direct method, are improved
upon by considering a time-varying and time-dependent entropy estimate. With
this method we investigated the relationship between the predictability of the
neuronal response and the frequency content in the auditory signals. The
analysis quantifies the temporal precision of the neuronal coding and the
memory in the neuronal response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5023</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5023</id><created>2012-04-23</created><authors><author><keyname>Singh</keyname><forenames>Niraj Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Mita</forenames></author><author><keyname>Chakraborty</keyname><forenames>Soubhik</forenames></author></authors><title>The Parameterized Complexity Analysis of Partition Sort for Negative
  Binomial Distribution Inputs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper makes a study on Partition sort algorithm for negative
binomial inputs. Comparing the results with those for binomial inputs in our
previous work, we find that this algorithm is sensitive to parameters of both
distributions. But the main effects as well as the interaction effects
involving these parameters and the input size are more significant for negative
binomial case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5028</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5028</id><created>2012-04-23</created><updated>2013-07-26</updated><authors><author><keyname>Jiekak</keyname><forenames>Steve</forenames></author><author><keyname>Kermarrec</keyname><forenames>Anne-Marie</forenames></author><author><keyname>Scouarnec</keyname><forenames>Nicolas Le</forenames></author><author><keyname>Straub</keyname><forenames>Gilles</forenames></author><author><keyname>Van Kempen</keyname><forenames>Alexandre</forenames></author></authors><title>Regenerating Codes: A System Perspective</title><categories>cs.DC cs.IT math.IT</categories><comments>10 pages, 24 figures; Published in ACM SIGOPS Operating System Review
  (July 2013). Extended version of a paper published at DISCCO 2012 (IEEE :
  http://dx.doi.org/10.1109/SRDS.2012.58)</comments><doi>10.1145/2506164.2506170</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosion of the amount of data stored in cloud systems calls for more
efficient paradigms for redundancy. While replication is widely used to ensure
data availability, erasure correcting codes provide a much better trade-off
between storage and availability. Regenerating codes are good candidates for
they also offer low repair costs in term of network bandwidth. While they have
been proven optimal, they are difficult to understand and parameterize. In this
paper we provide an analysis of regenerating codes for practitioners to grasp
the various trade-offs. More specifically we make two contributions: (i) we
study the impact of the parameters by conducting an analysis at the level of
the system, rather than at the level of a single device; (ii) we compare the
computational costs of various implementations of codes and highlight the most
efficient ones. Our goal is to provide system designers with concrete
information to help them choose the best parameters and design for regenerating
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5043</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5043</id><created>2012-04-23</created><updated>2012-06-12</updated><authors><author><keyname>Argyriou</keyname><forenames>Andreas</forenames></author><author><keyname>Foygel</keyname><forenames>Rina</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Sparse Prediction with the $k$-Support Norm</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a novel norm that corresponds to the tightest convex relaxation of
sparsity combined with an $\ell_2$ penalty. We show that this new {\em
$k$-support norm} provides a tighter relaxation than the elastic net and is
thus a good replacement for the Lasso or the elastic net in sparse prediction
problems. Through the study of the $k$-support norm, we also bound the
looseness of the elastic net, thus shedding new light on it and providing
justification for its use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5046</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5046</id><created>2012-04-23</created><updated>2012-05-09</updated><authors><author><keyname>Ho</keyname><forenames>Zuleita</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Instantaneous Relaying: Optimal Strategies and Interference
  Neutralization</title><categories>cs.IT math.IT math.OC</categories><comments>30 pages, journal version, complete proofs</comments><doi>10.1109/TSP.2012.2210709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-user wireless network equipped with multiple relay nodes, some
relays are more intelligent than other relay nodes. The intelligent relays are
able to gather channel state information, perform linear processing and forward
signals whereas the dumb relays is only able to serve as amplifiers. As the
dumb relays are oblivious to the source and destination nodes, the wireless
network can be modeled as a relay network with *smart instantaneous relay*
only: the signals of source-destination arrive at the same time as
source-relay-destination. Recently, instantaneous relaying is shown to improve
the degrees-of-freedom of the network as compared to classical cut-set bound.
In this paper, we study an achievable rate region and its boundary of the
instantaneous interference relay channel in the scenario of (a) uninformed
non-cooperative source-destination nodes (source and destination nodes are not
aware of the existence of the relay and are non-cooperative) and (b) informed
and cooperative source-destination nodes. Further, we examine the performance
of interference neutralization: a relay strategy which is able to cancel
interference signals at each destination node in the air. We observe that
interference neutralization, although promise to achieve desired
degrees-of-freedom, may not be feasible if relay has limited power. Simulation
results show that the optimal relay strategies improve the achievable rate
region and provide better user-fairness in both uninformed non-cooperative and
informed cooperative scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5056</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5056</id><created>2012-04-23</created><authors><author><keyname>Manzalini</keyname><forenames>Antonio</forenames></author></authors><title>Mitigating Systemic Risks in Future Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper elaborates about the potential risk of systemic instabilities in
future networks and proposes a methodology to mitigate it. The starting concept
is modeling the network as a complex environment (e.g. ecosystem) of resources
and associated functional controllers in a continuous and dynamic game of
cooperation - competition. Methodology foresees defining and associating
utility functions to these controllers and elaborating a global utility
function (as a function of the controllers' utility functions) for the overall
network. It is conjectured that the optimization of the global utility function
ensures network stability and security evaluations. Paper concludes arguing
that self-governance (with limited human intervention) is possible provided
that proper local, global control rules are coded into these utility functions
optimization processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5057</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5057</id><created>2012-04-23</created><authors><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>Esparza</keyname><forenames>Javier</forenames></author></authors><title>Deterministic Automata for the (F,G)-fragment of LTL</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with linear temporal logic properties in the setting of e.g.
games or probabilistic systems, one often needs to express them as
deterministic omega-automata. In order to translate LTL to deterministic
omega-automata, the traditional approach first translates the formula to a
non-deterministic B\&quot;uchi automaton. Then a determinization procedure such as
of Safra is performed yielding a deterministic \omega-automaton. We present a
direct translation of the (F,G)-fragment of LTL into deterministic
\omega-automata with no determinization procedure involved. Since our approach
is tailored to LTL, we often avoid the typically unnecessarily large blowup
caused by general determinization algorithms. We investigate the complexity of
this translation and provide experimental results and compare them to the
traditional method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5059</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5059</id><created>2012-04-23</created><updated>2013-02-06</updated><authors><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Computation over Mismatched Channels</title><categories>cs.IT math.IT</categories><comments>23 pages, to appear in IEEE Journal on Selected Areas in
  Communications</comments><journal-ref>IEEE Journal on Selected Areas in Communications, vol. 31, pp. 666
  - 677, April 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed computation of a target function over
a multiple-access channel. If the target and channel functions are matched
(i.e., compute the same function), significant performance gains can be
obtained by jointly designing the computation and communication tasks. However,
in most situations there is mismatch between these two functions. In this work,
we analyze the impact of this mismatch on the performance gains achievable with
joint computation and communication designs over separation-based designs. We
show that for most pairs of target and channel functions there is no such gain,
and separation of computation and communication is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5072</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5072</id><created>2012-04-23</created><updated>2012-07-25</updated><authors><author><keyname>Kelling</keyname><forenames>Jeffrey</forenames></author><author><keyname>&#xd3;dor</keyname><forenames>G&#xe9;za</forenames></author><author><keyname>Nagy</keyname><forenames>M&#xe1;t&#xe9; Ferenc</forenames></author><author><keyname>Schulz</keyname><forenames>Henrik</forenames></author><author><keyname>Heinig</keyname><forenames>Karl-Heinz</forenames></author></authors><title>Comparison of Different Parallel Implementations of the 2+1-Dimensional
  KPZ Model and the 3-Dimensional KMC Model</title><categories>cs.DC cond-mat.mtrl-sci cond-mat.stat-mech nlin.CG physics.comp-ph</categories><comments>14 pages, 8 figures, to be published in a forthcoming EPJST special
  issue on &quot;Computer simulations on GPU&quot;</comments><journal-ref>The European Physical Journal - Special Topics 210, Number 1
  (2012), 175-187</journal-ref><doi>10.1140/epjst/e2012-01645-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that efficient simulations of the Kardar-Parisi-Zhang interface
growth in 2 + 1 dimensions and of the 3-dimensional Kinetic Monte Carlo of
thermally activated diffusion can be realized both on GPUs and modern CPUs. In
this article we present results of different implementations on GPUs using CUDA
and OpenCL and also on CPUs using OpenCL and MPI. We investigate the runtime
and scaling behavior on different architectures to find optimal solutions for
solving current simulation problems in the field of statistical physics and
materials science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5074</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5074</id><created>2012-04-23</created><authors><author><keyname>Belovs</keyname><forenames>Aleksandrs</forenames></author></authors><title>Adversary Lower Bound for Element Distinctness</title><categories>quant-ph cs.CC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we construct an explicit optimal (negative-weight) adversary
matrix for the element distinctness problem, given that the size of the
alphabet is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5083</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5083</id><created>2012-04-23</created><authors><author><keyname>Singh</keyname><forenames>Niraj Kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Soubhik</forenames></author></authors><title>Smart Sort: Design and Analysis of a Fast, Efficient and Robust
  Comparison Based Internal Sort Algorithm</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart Sort algorithm is a &quot;smart&quot; fusion of heap construction procedures (of
Heap sort algorithm) into the conventional &quot;Partition&quot; function (of Quick sort
algorithm) resulting in a robust version of Quick sort algorithm. We have also
performed empirical analysis of average case behavior of our proposed algorithm
along with the necessary theoretical analysis for best and worst cases. Its
performance was checked against some standard probability distributions, both
uniform and non-uniform, like Binomial, Poisson, Discrete &amp; Continuous Uniform,
Exponential, and Standard Normal. The analysis exhibited the desired robustness
coupled with excellent performance of our algorithm. Although this paper
assumes the static partition ratios, its dynamic version is expected to yield
still better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5086</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5086</id><created>2012-04-23</created><authors><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Ion</keyname><forenames>Patrick</forenames></author><author><keyname>Dimou</keyname><forenames>Anastasia</forenames></author><author><keyname>Bratsas</keyname><forenames>Charalampos</forenames></author><author><keyname>Corneli</keyname><forenames>Joseph</forenames></author><author><keyname>Sperber</keyname><forenames>Wolfram</forenames></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author><author><keyname>Antoniou</keyname><forenames>Ioannis</forenames></author></authors><title>Reimplementing the Mathematical Subject Classification (MSC) as a Linked
  Open Dataset</title><categories>cs.DL cs.MS</categories><comments>Conference on Intelligent Computer Mathematics, July 9-14, Bremen,
  Germany. Published as number 7362 in Lecture Notes in Artificial
  Intelligence, Springer</comments><msc-class>68T30</msc-class><acm-class>H.3.5; I.2.4; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Mathematics Subject Classification (MSC) is a widely used scheme for
classifying documents in mathematics by subject. Its traditional, idiosyncratic
conceptualization and representation makes the scheme hard to maintain and
requires custom implementations of search, query and annotation support. This
limits uptake e.g. in semantic web technologies in general and the creation and
exploration of connections between mathematics and related domains (e.g.
science) in particular.
  This paper presents the new official implementation of the MSC2010 as a
Linked Open Dataset, building on SKOS (Simple Knowledge Organization System).
We provide a brief overview of the dataset's structure, its available
implementations, and first applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5093</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5093</id><created>2012-04-23</created><authors><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Kutz</keyname><forenames>Oliver</forenames></author><author><keyname>Mossakowski</keyname><forenames>Till</forenames></author><author><keyname>Gr&#xfc;ninger</keyname><forenames>Michael</forenames></author></authors><title>The Distributed Ontology Language (DOL): Ontology Integration and
  Interoperability Applied to Mathematical Formalization</title><categories>cs.LO</categories><comments>Conference on Intelligent Computer Mathematics, July 9-14, Bremen,
  Germany. Published as number 7362 in Lecture Notes in Artificial
  Intelligence, Springer</comments><msc-class>68T30, 03B10, 03B70, 16B50</msc-class><acm-class>F.4.1; F.4.m; H.3.5; H.5.3; H.5.4; I.2.4; I.7.1; I.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Distributed Ontology Language (DOL) is currently being standardized
within the OntoIOp (Ontology Integration and Interoperability) activity of
ISO/TC 37/SC 3. It aims at providing a unified framework for (1) ontologies
formalized in heterogeneous logics, (2) modular ontologies, (3) links between
ontologies, and (4) annotation of ontologies.
  This paper focuses on an application of DOL's meta-theoretical features in
mathematical formalization: validating relationships between ontological
formalizations of mathematical concepts in COLORE (Common Logic Repository),
which provide the foundation for formalizing real-world notions such as spatial
and temporal relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5094</identifier>
 <datestamp>2012-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5094</id><created>2012-04-23</created><updated>2012-07-10</updated><authors><author><keyname>Tankink</keyname><forenames>Carst</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>Point-and-write --- Documenting Formal Mathematics by Reference</title><categories>cs.MS cs.DL</categories><comments>Conference on Intelligent Computer Mathematics, July 8--13, Bremen,
  Germany. Published as number 7362 in Lecture Notes in Artificial
  Intelligence, Springer</comments><msc-class>68T30, 68T35, 03B35</msc-class><acm-class>F.4.1; F.4.m; H.3.5; H.5.3; H.5.4; I.2.4; I.7.1; I.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the design and implementation of mechanisms for
light-weight inclusion of formal mathematics in informal mathematical writings,
particularly in a Web-based setting. This is conceptually done in three stages:
(i) by choosing a suitable representation layer (based on RDF) for encoding the
information about available resources of formal mathematics, (ii) by exporting
this information from formal libraries, and (iii) by providing syntax and
implementation for including formal mathematics in informal writings.
  We describe the use case of an author referring to formal text from an
informal narrative, and discuss design choices entailed by this use case.
Furthermore, we describe an implementation of the use case within the Agora
prototype: a Wiki for collaborating on formalized mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5113</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5113</id><created>2012-04-23</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Paulusma</keyname><forenames>Daniel</forenames></author></authors><title>Obtaining Planarity by Contracting Few Edges</title><categories>cs.DS cs.DM math.CO</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Planar Contraction problem is to test whether a given graph can be made
planar by using at most k edge contractions. This problem is known to be
NP-complete. We show that it is fixed-parameter tractable when parameterized by
k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5136</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5136</id><created>2012-04-23</created><authors><author><keyname>Eftekhari</keyname><forenames>Yaser</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author><author><keyname>Lambadaris</keyname><forenames>Ioannis</forenames></author></authors><title>Analysis and Design of Irregular Graphs for Node-Based
  Verification-Based Recovery Algorithms in Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>5 Pages, to be presented at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a probabilistic analysis of iterative node-based
verification-based (NB-VB) recovery algorithms over irregular graphs in the
context of compressed sensing. Verification-based algorithms are particularly
interesting due to their low complexity (linear in the signal dimension $n$).
The analysis predicts the average fraction of unverified signal elements at
each iteration $\ell$ where the average is taken over the ensembles of input
signals and sensing matrices. The analysis is asymptotic ($n \rightarrow
\infty$) and is similar in nature to the well-known density evolution technique
commonly used to analyze iterative decoding algorithms. Compared to the
existing technique for the analysis of NB-VB algorithms, which is based on
numerically solving a large system of coupled differential equations, the
proposed method is much simpler and more accurate. This allows us to design
irregular sensing graphs for such recovery algorithms. The designed irregular
graphs outperform the corresponding regular graphs substantially. For example,
for the same recovery complexity per iteration, we design irregular graphs that
can recover up to about 40% more non-zero signal elements compared to the
regular graphs. Simulation results are also provided which demonstrate that the
proposed asymptotic analysis matches the performance of recovery algorithms for
large but finite values of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5156</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5156</id><created>2012-04-23</created><authors><author><keyname>Farooque</keyname><forenames>Mahfuza</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Lengrand</keyname><forenames>St&#xe9;phane</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author></authors><title>A sequent calculus with procedure calls</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the sequent calculus LKF into a calculus LK(T),
allowing calls to a decision procedure. We prove cut-elimination of LK(T).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5159</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5159</id><created>2012-04-23</created><authors><author><keyname>Farooque</keyname><forenames>Mahfuza</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Lengrand</keyname><forenames>St&#xe9;phane</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Mahboubi</keyname><forenames>Assia</forenames><affiliation>LIX, MSR - INRIA, INRIA Saclay - Ile de France</affiliation></author></authors><title>Two simulations about DPLL(T)</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we relate different formulations of the DPLL(T) procedure. The
first formulation is based on a system of rewrite rules, which we denote
DPLL(T). The second formulation is an inference system of, which we denote
LKDPLL(T). The third formulation is the application of a standard proof-search
mechanism in a sequent calculus LKp(T) introduced here. We formalise an
encoding from DPLL(T) to LKDPLL(T) that was, to our knowledge, never explicitly
given and, in the case where DPLL(T) is extended with backjumping and Lemma
learning, never even implicitly given. We also formalise an encoding from
LKDPLL(T) to LKp(T), building on Ivan Gazeau's previous work: we extend his
work in that we handle the &quot;-modulo-Theory&quot; aspect of SAT-modulo-theory, by
extending the sequent calculus to allow calls to a theory solver (seen as a
blackbox). We also extend his work in that we handle advanced features of DPLL
such as backjumping and Lemma learning, etc. Finally, we re fine the approach
by starting to formalise quantitative aspects of the simulations: the
complexity is preserved (number of steps to build complete proofs). Other
aspects remain to be formalised (non-determinism of the search / width of
search space).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5174</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5174</id><created>2012-04-23</created><updated>2012-05-04</updated><authors><author><keyname>Barchiesi</keyname><forenames>Maximiliano</forenames></author><author><keyname>Sangroni</keyname><forenames>Mercedes</forenames></author><author><keyname>Renaudo</keyname><forenames>Carlos</forenames></author><author><keyname>Rossi</keyname><forenames>Pablo</forenames></author><author><keyname>Pramparo</keyname><forenames>Mar&#xed;a de Carmen</forenames></author><author><keyname>Nepote</keyname><forenames>Valeria</forenames></author><author><keyname>Grosso</keyname><forenames>Nelson Ruben</forenames></author><author><keyname>Gayol</keyname><forenames>Mar&#xed;a Fernanda</forenames></author></authors><title>Christhin: Quantitative Analysis of Thin Layer Chromatography</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manual for Christhin 0.1.36 Christhin (Chromatography Riser Thin) is software
developed for the quantitative analysis of data obtained from thin-layer
chromatographic techniques (TLC). Once installed on your computer, the program
is very easy to use, and provides data quickly and accurately. This manual
describes the program, and reading should be enough to use it properly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5192</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5192</id><created>2012-04-23</created><updated>2013-05-23</updated><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Excluded Forest Minors and the Erd\H{o}s-P\'osa Property</title><categories>math.CO cs.DM</categories><comments>v3: referee's comments implemented</comments><journal-ref>Combinatorics, Probability, and Computing, 22/5:700--721, 2013</journal-ref><doi>10.1017/S0963548313000266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical result of Robertson and Seymour states that the set of graphs
containing a fixed planar graph $H$ as a minor has the so-called
Erd\H{o}s-P\'osa property; namely, there exists a function $f$ depending only
on $H$ such that, for every graph $G$ and every positive integer $k$, the graph
$G$ has $k$ vertex-disjoint subgraphs each containing $H$ as a minor, or there
exists a subset $X$ of vertices of $G$ with $|X| \leq f(k)$ such that $G - X$
has no $H$-minor. While the best function $f$ currently known is exponential in
$k$, a $O(k \log k)$ bound is known in the special case where $H$ is a forest.
This is a consequence of a theorem of Bienstock, Robertson, Seymour, and Thomas
on the pathwidth of graphs with an excluded forest-minor. In this paper we show
that the function $f$ can be taken to be linear when $H$ is a forest. This is
best possible in the sense that no linear bound is possible if $H$ has a cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5194</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5194</id><created>2012-04-23</created><updated>2015-03-31</updated><authors><author><keyname>Gajarsky</keyname><forenames>Jakub</forenames><affiliation>FI MU Brno</affiliation></author><author><keyname>Hlineny</keyname><forenames>Petr</forenames><affiliation>FI MU Brno</affiliation></author></authors><title>Kernelizing MSO Properties of Trees of Fixed Height, and Some
  Consequences</title><categories>cs.DM cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (April 1,
  2015) lmcs:748</journal-ref><doi>10.2168/LMCS-11(1:19)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fix an integer h&gt;=1. In the universe of coloured trees of height at most h,
we prove that for any graph decision problem defined by an MSO formula with r
quantifiers, there exists a set of kernels, each of size bounded by an
elementary function of r and the number of colours. This yields two noteworthy
consequences. Consider any graph class G having a one-dimensional MSO
interpretation in the universe of coloured trees of height h (equivalently, G
is a class of shrub-depth h). First, class G admits an MSO model checking
algorithm whose runtime has an elementary dependence on the formula size.
Second, on G the expressive powers of FO and MSO coincide (which extends a 2012
result of Elberfeld, Grohe, and Tantau).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5213</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5213</id><created>2012-04-23</created><updated>2013-07-01</updated><authors><author><keyname>de Keijzer</keyname><forenames>Bart</forenames></author><author><keyname>Klos</keyname><forenames>Tomas B.</forenames></author><author><keyname>Zhang</keyname><forenames>Yingqian</forenames></author></authors><title>Solving Weighted Voting Game Design Problems Optimally: Representations,
  Synthesis, and Enumeration</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the inverse power index problem for weighted voting games: the
problem of finding a weighted voting game in which the power of the players is
as close as possible to a certain target distribution. Our goal is to find
algorithms that solve this problem exactly. Thereto, we study various
subclasses of simple games, and their associated representation methods. We
survey algorithms and impossibility results for the synthesis problem, i.e.,
converting a representation of a simple game into another representation.
  We contribute to the synthesis problem by showing that it is impossible to
compute in polynomial time the list of ceiling coalitions (also known as
shift-maximal losing coalitions) of a game from its list of roof coalitions
(also known as shift-minimal winning coalitions), and vice versa.
  Then, we proceed by studying the problem of enumerating the set of weighted
voting games. We present first a naive algorithm for this, running in doubly
exponential time. Using our knowledge of the synthesis problem, we then improve
on this naive algorithm, and we obtain an enumeration algorithm that runs in
quadratic exponential time (that is, O(2^(n^2) p(n)) for a polynomial p).
Moreover, we show that this algorithm runs in output-polynomial time, making it
the best possible enumeration algorithm up to a polynomial factor.
  Finally, we propose an exact anytime algorithm for the inverse power index
problem that runs in exponential time. This algorithm is straightforward and
general: it computes the error for each game enumerated, and outputs the game
that minimizes this error. By the genericity of our approach, our algorithm can
be used to find a weighted voting game that optimizes any exponential time
computable function. We implement our algorithm for the case of the normalized
Banzhaf index, and we perform experiments in order to study performance and
error convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5224</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5224</id><created>2012-04-23</created><updated>2015-03-16</updated><authors><author><keyname>Bruner</keyname><forenames>Marie-Louise</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author></authors><title>A Fast Algorithm for Permutation Pattern Matching Based on Alternating
  Runs</title><categories>cs.DS cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-complete Permutation Pattern Matching problem asks whether a
$k$-permutation $P$ is contained in a $n$-permutation $T$ as a pattern. This is
the case if there exists an order-preserving embedding of $P$ into $T$. In this
paper, we present a fixed-parameter algorithm solving this problem with a
worst-case runtime of $\mathcal{O}(1.79^{\mathsf{run}(T)}\cdot n\cdot k)$,
where $\mathsf{run}(T)$ denotes the number of alternating runs of $T$. This
algorithm is particularly well-suited for instances where $T$ has few runs,
i.e., few ups and downs. Moreover, since $\mathsf{run}(T)&lt;n$, this can be seen
as a $\mathcal{O}(1.79^{n}\cdot n\cdot k)$ algorithm which is the first to beat
the exponential $2^n$ runtime of brute-force search. Furthermore, we prove that
under standard complexity theoretic assumptions such a fixed-parameter
tractability result is not possible for $\mathsf{run}(P)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5226</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5226</id><created>2012-04-23</created><updated>2015-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Lam</keyname><forenames>Albert Y. S.</forenames></author><author><keyname>Dominguez-Garcia</keyname><forenames>Alejandro</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>An Optimal and Distributed Method for Voltage Regulation in Power
  Distribution Systems</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>To Appear in IEEE Transaction on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of voltage regulation in power distribution
networks with deep-penetration of distributed energy resources, e.g.,
renewable-based generation, and storage-capable loads such as plug-in hybrid
electric vehicles. We cast the problem as an optimization program, where the
objective is to minimize the losses in the network subject to constraints on
bus voltage magnitudes, limits on active and reactive power injections,
transmission line thermal limits and losses. We provide sufficient conditions
under which the optimization problem can be solved via its convex relaxation.
Using data from existing networks, we show that these sufficient conditions are
expected to be satisfied by most networks. We also provide an efficient
distributed algorithm to solve the problem. The algorithm adheres to a
communication topology described by a graph that is the same as the graph that
describes the electrical network topology. We illustrate the operation of the
algorithm, including its robustness against communication link failures,
through several case studies involving 5-, 34-, and 123-bus power distribution
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5229</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5229</id><created>2012-04-23</created><updated>2012-08-28</updated><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Talmon</keyname><forenames>Nimrod</forenames></author></authors><title>Selection in the Presence of Memory Faults, with Applications to
  In-place Resilient Sorting</title><categories>cs.DS</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The selection problem, where one wishes to locate the $k^{th}$ smallest
element in an unsorted array of size $n$, is one of the basic problems studied
in computer science. The main focus of this work is designing algorithms for
solving the selection problem in the presence of memory faults. These can
happen as the result of cosmic rays, alpha particles, or hardware failures.
  Specifically, the computational model assumed here is a faulty variant of the
RAM model (abbreviated as FRAM), which was introduced by Finocchi and Italiano.
In this model, the content of memory cells might get corrupted adversarially
during the execution, and the algorithm is given an upper bound $\delta$ on the
number of corruptions that may occur.
  The main contribution of this work is a deterministic resilient selection
algorithm with optimal O(n) worst-case running time. Interestingly, the running
time does not depend on the number of faults, and the algorithm does not need
to know $\delta$.
  The aforementioned resilient selection algorithm can be used to improve the
complexity bounds for resilient $k$-d trees developed by Gieseke, Moruz and
Vahrenhold. Specifically, the time complexity for constructing a $k$-d tree is
improved from $O(n\log^2 n + \delta^2)$ to $O(n \log n)$.
  Besides the deterministic algorithm, a randomized resilient selection
algorithm is developed, which is simpler than the deterministic one, and has
$O(n + \alpha)$ expected time complexity and O(1) space complexity (i.e., is
in-place). This algorithm is used to develop the first resilient sorting
algorithm that is in-place and achieves optimal $O(n\log n + \alpha\delta)$
expected running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5244</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5244</id><created>2012-04-23</created><authors><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Powell</keyname><forenames>Thomas</forenames></author></authors><title>A Game-Theoretic Computational Interpretation of Proofs in Classical
  Analysis</title><categories>math.LO cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that a functional interpretation of proofs in mathematical
analysis can be given by the product of selection functions, a mode of
recursion that has an intuitive reading in terms of the computation of optimal
strategies in sequential games. We argue that this result has genuine practical
value by interpreting some well-known theorems of mathematics and demonstrating
that the product gives these theorems a natural computational interpretation
that can be clearly understood in game theoretic terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5249</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5249</id><created>2012-04-23</created><authors><author><keyname>Toyota</keyname><forenames>Norihito</forenames></author></authors><title>Does Parrondo Paradox occur in Scale Free Networks? -A simple
  Consideration-</title><categories>physics.soc-ph cs.GT</categories><comments>11 pages, 11 figures</comments><journal-ref>Bulletin of Hokkaido Information University, Vol23.No1, 2011 Oct</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parrondo's paradox occurs in sequences of games in which a winning
expectation may be obtained by playing the games in a random order, even though
each game in the sequence may be lost when played individually. Several
variations of Parrondo's games apparently with paradoxical property have been
introduced; history dependence, one dimensional line, two dimensional lattice
and so on. In this article, we examine whether Parrondo's paradox occurs or not
in scale free networks. This is interesting as an empirical study, since scale
free networks are ubiquitous in our real world. First some simulation results
are given and after that theoretical studies are made. As a result, we mostly
confirm that Parrondo's paradox can not occur in the naive case, where the game
has the same number of parameters as the original Parrondo's game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5253</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5253</id><created>2012-04-23</created><authors><author><keyname>Barbosa</keyname><forenames>Felipe Cinelli</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Costa</keyname><forenames>Max H. M.</forenames></author></authors><title>An Algebraic Framework for Concatenated Linear Block Codes in Side
  Information Based Problems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides an algebraic framework for source coding with decoder side
information and its dual problem, channel coding with encoder side information,
showing that nested concatenated codes can achieve the corresponding
rate-distortion and capacity-noise bounds. We show that code concatenation
preserves the nested properties of codes and that only one of the concatenated
codes needs to be nested, which opens up a wide range of possible new code
combinations for these side information based problems. In particular, the
practically important binary version of these problems can be addressed by
concatenating binary inner and non-binary outer linear codes. By observing that
list decoding with folded Reed- Solomon codes is asymptotically optimal for
encoding IID q-ary sources and that in concatenation with inner binary codes it
can asymptotically achieve the rate-distortion bound for a Bernoulli symmetric
source, we illustrate our findings with a new algebraic construction which
comprises concatenated nested cyclic codes and binary linear block codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5267</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5267</id><created>2012-04-24</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Francis</keyname><forenames>Leena Mary</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>WILI - Web Interface for people with Lowvision Issues</title><categories>cs.HC</categories><comments>8 Pages; 6 Figures, International Journal on Computational Sciences &amp;
  Applications (IJCSA) Vo2, No.2, April 2012, ISSN: 2200-0011</comments><msc-class>68U35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though World Wide Web is the single largest source of information, it is
ill-equipped to serve the people with vision related problems. With the
prolific increase in the interest to make the web accessible to all sections of
the society, solving this accessibility problem becomes mandatory. This paper
presents a technique for making web pages accessible for people with low vision
issues. A model for making web pages accessible, WILI (Web Interface for people
with Low-vision Issues) has been proposed. The approach followed in this work
is to automatically replace the existing display style of a web page with a new
skin following the guidelines given by Clear Print Booklet provided by Royal
National Institute of Blind. &quot;Single Click Solution&quot; is one of the primary
advantages provided by WILI. A prototype using the WILI model is implemented
and various experiments are conducted. The results of experiments conducted on
WILI indicate 82% effective conversion rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5280</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5280</id><created>2012-04-24</created><authors><author><keyname>Junior</keyname><forenames>Armando Gon&#xe7;alves Da Silva</forenames><affiliation>CIn</affiliation></author><author><keyname>Deransart</keyname><forenames>Pierre</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Menezes</keyname><forenames>Luis-Carlos</forenames><affiliation>CIn</affiliation></author><author><keyname>Da Silva</keyname><forenames>Marcos-Aur&#xe9;lio Almeida</forenames><affiliation>LPMC</affiliation></author><author><keyname>Robin</keyname><forenames>Jacques</forenames><affiliation>TRT</affiliation></author></authors><title>Towards a Generic Trace for Rule Based Constraint Reasoning</title><categories>cs.PL</categories><proxy>ccsd</proxy><report-no>RR-7939</report-no><journal-ref>N&amp;deg; RR-7939 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CHR is a very versatile programming language that allows programmers to
declaratively specify constraint solvers. An important part of the development
of such solvers is in their testing and debugging phases. Current CHR
implementations support those phases by offering tracing facilities with
limited information. In this report, we propose a new trace for CHR which
contains enough information to analyze any aspects of \CHRv\ execution at some
useful abstract level, common to several implementations. %a large family of
rule based solvers. This approach is based on the idea of generic trace. Such a
trace is formally defined as an extension of the $\omega_r^\lor$ semantics of
CHR. We show that it can be derived form the SWI Prolog CHR trace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5281</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5281</id><created>2012-04-24</created><authors><author><keyname>Zhong</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Stochastic Analysis of Mean Interference for RTS/CTS Mechanism</title><categories>cs.NI cs.IT cs.PF math.IT</categories><comments>technical report, in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The RTS/CTS handshake mechanism in WLAN is studied using stochastic geometry.
The effect of RTS/CTS is treated as a thinning procedure for a spatially
distributed point process that models the potential transceivers in a WLAN, and
the resulting concurrent transmission processes are described. Exact formulas
for the intensity of the concurrent transmission processes and the mean
interference experienced by a typical receiver are established. The analysis
yields useful results for understanding how the design parameters of RTS/CTS
affect the network interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5284</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5284</id><created>2012-04-24</created><authors><author><keyname>Jiang</keyname><forenames>Heping</forenames></author></authors><title>Non-Hamiltonian Holes in Grid Graphs</title><categories>cs.DM</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend general grid graphs to the grid graphs consist of
polygons tiling on a plane, named polygonal grid graphs. With a cycle basis
satisfied polygons tiling, we study the cyclic structure of Hamilton graphs. A
Hamilton cycle can be expressed as a symmetric difference of a subset of cycles
in the basis. From the combinatorial relations of vertices in the subset of
cycles in the basis, we deduce the formula of inside faces in Grinberg theorem,
called Grinberg equation, and derive a kind of cycles whose existence make a
polygonal grid graph non-Hamiltonian, called non-Hamiltonian holes, and then we
characterize the existence condition of non-Hamiltonian holes and obtain the
necessary and sufficient condition of a polygonal grid graph to be Hamiltonian.
The result in this paper provides a new starting point for developing a
polynomial-time algorithm for Hamilton problem in general grid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5306</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5306</id><created>2012-04-24</created><authors><author><keyname>Bernasconi</keyname><forenames>Anna</forenames></author><author><keyname>Ciriani</keyname><forenames>Valentina</forenames></author><author><keyname>Luccio</keyname><forenames>Fabrizio</forenames></author><author><keyname>Pagli</keyname><forenames>Linda</forenames></author></authors><title>Compact DSOP and partial DSOP Forms</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a Boolean function f on n variables, a Disjoint Sum-of-Products (DSOP)
of f is a set of products (ANDs) of subsets of literals whose sum (OR) equals
f, such that no two products cover the same minterm of f. DSOP forms are a
special instance of partial DSOPs, i.e. the general case where a subset of
minterms must be covered exactly once and the other minterms (typically
corresponding to don't care conditions of $f$) can be covered any number of
times. We discuss finding DSOPs and partial DSOP with a minimal number of
products, a problem theoretically connected with various properties of Boolean
functions and practically relevant in the synthesis of digital circuits.
Finding an absolute minimum is hard, in fact we prove that the problem of
absolute minimization of partial DSOPs is NP-hard. Therefore it is crucial to
devise a polynomial time heuristic that compares favorably with the known
minimization tools. To this end we develop a further piece of theory starting
from the definition of the weight of a product p as a functions of the number
of fragments induced on other cubes by the selection of p, and show how product
weights can be exploited for building a class of minimization heuristics for
DSOP and partial DSOP synthesis. A set of experiments conducted on major
benchmark functions show that our method, with a family of variants, always
generates better results than the ones of previous heuristics, including the
method based on a BDD representation of f.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5309</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5309</id><created>2012-04-24</created><updated>2013-03-26</updated><authors><author><keyname>Hawe</keyname><forenames>Simon</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author><author><keyname>Diepold</keyname><forenames>Klaus</forenames></author></authors><title>Analysis Operator Learning and Its Application to Image Reconstruction</title><categories>cs.LG cs.CV</categories><comments>12 pages, 7 figures</comments><acm-class>I.4.5</acm-class><doi>10.1109/TIP.2013.2246175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting a priori known structural information lies at the core of many
image reconstruction methods that can be stated as inverse problems. The
synthesis model, which assumes that images can be decomposed into a linear
combination of very few atoms of some dictionary, is now a well established
tool for the design of image reconstruction algorithms. An interesting
alternative is the analysis model, where the signal is multiplied by an
analysis operator and the outcome is assumed to be the sparse. This approach
has only recently gained increasing interest. The quality of reconstruction
methods based on an analysis model severely depends on the right choice of the
suitable operator.
  In this work, we present an algorithm for learning an analysis operator from
training images. Our method is based on an $\ell_p$-norm minimization on the
set of full rank matrices with normalized columns. We carefully introduce the
employed conjugate gradient method on manifolds, and explain the underlying
geometry of the constraints. Moreover, we compare our approach to
state-of-the-art methods for image denoising, inpainting, and single image
super-resolution. Our numerical results show competitive performance of our
general approach in all presented applications compared to the specialized
state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5314</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5314</id><created>2012-04-24</created><authors><author><keyname>Chandra</keyname><forenames>Joydeep</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>A Tunable Mechanism for Identifying Trusted Nodes in Large Scale
  Distributed Networks</title><categories>cs.SI physics.soc-ph</categories><comments>18 pages, 4 figures, accepted for IEEE TrustCom 2012</comments><acm-class>C.2; H.3.5; H.3.4</acm-class><doi>10.1109/TrustCom.2012.63</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a simple randomized protocol for identifying
trusted nodes based on personalized trust in large scale distributed networks.
The problem of identifying trusted nodes, based on personalized trust, in a
large network setting stems from the huge computation and message overhead
involved in exhaustively calculating and propagating the trust estimates by the
remote nodes. However, in any practical scenario, nodes generally communicate
with a small subset of nodes and thus exhaustively estimating the trust of all
the nodes can lead to huge resource consumption. In contrast, our mechanism can
be tuned to locate a desired subset of trusted nodes, based on the allowable
overhead, with respect to a particular user. The mechanism is based on a simple
exchange of random walk messages and nodes counting the number of times they
are being hit by random walkers of nodes in their neighborhood. Simulation
results to analyze the effectiveness of the algorithm show that using the
proposed algorithm, nodes identify the top trusted nodes in the network with a
very high probability by exploring only around 45% of the total nodes, and in
turn generates nearly 90% less overhead as compared to an exhaustive trust
estimation mechanism, named TrustWebRank. Finally, we provide a measure of the
global trustworthiness of a node; simulation results indicate that the measures
generated using our mechanism differ by only around 0.6% as compared to
TrustWebRank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5316</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5316</id><created>2012-04-24</created><authors><author><keyname>Lefran&#xe7;ois</keyname><forenames>Maxime</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Gandon</keyname><forenames>Fabien</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>ILexicOn: toward an ECD-compliant interlingual lexical ontology
  described with semantic web formalisms</title><categories>cs.CL cs.AI</categories><proxy>ccsd</proxy><journal-ref>MTT - 5th International Conference on Meaning-Text Theory - 2011
  (2011) 155-164</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in bridging the world of natural language and the world of
the semantic web in particular to support natural multilingual access to the
web of data. In this paper we introduce a new type of lexical ontology called
interlingual lexical ontology (ILexicOn), which uses semantic web formalisms to
make each interlingual lexical unit class (ILUc) support the projection of its
semantic decomposition on itself. After a short overview of existing lexical
ontologies, we briefly introduce the semantic web formalisms we use. We then
present the three layered architecture of our approach: i) the interlingual
lexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formally
defined; iii) the data layer. We illustrate our approach with a standalone
ILexicOn, and introduce and explain a concise human-readable notation to
represent ILexicOns. Finally, we show how semantic web formalisms enable the
projection of a semantic decomposition on the decomposed ILUc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5317</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5317</id><created>2012-04-24</created><updated>2012-05-24</updated><authors><author><keyname>Duda</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Korus</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Correction Trees as an Alternative to Turbo Codes and Low Density Parity
  Check Codes</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapidly improving performance of modern hardware renders convolutional
codes obsolete, and allows for the practical implementation of more
sophisticated correction codes such as low density parity check (LDPC) and
turbo codes (TC). Both are decoded by iterative algorithms, which require a
disproportional computational effort for low channel noise. They are also
unable to correct higher noise levels, still below the Shannon theoretical
limit. In this paper, we discuss an enhanced version of a convolutional-like
decoding paradigm which adopts very large spaces of possible system states, of
the order of $2^{64}$. Under such conditions, the traditional convolution
operation is rendered useless and needs to be replaced by a carefully designed
state transition procedure. The size of the system state space completely
changes the correction philosophy, as state collisions are virtually impossible
and the decoding procedure becomes a correction tree. The proposed decoding
algorithm is practically cost-free for low channel noise. As the channel noise
approaches the Shannon limit, it is still possible to perform correction,
although its cost increases to infinity. In many applications, the implemented
decoder can essentially outperform both LDPC and TC. This paper describes the
proposed correction paradigm and theoretically analyzes the asymptotic
correction performance. The considered encoder and decoder were verified
experimentally for the binary symmetric channel. The correction process remains
practically cost-free for channel error rates below 0.05 and 0.13 for the 1/2
and 1/4 rate codes, respectively. For the considered resource limit, the output
bit error rates reach the order of $10^{-3}$ for channel error rates 0.08 and
0.18. The proposed correction paradigm can be easily extended to other
communication channels; the appropriate generalizations are also discussed in
this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5318</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5318</id><created>2012-04-24</created><authors><author><keyname>Escobar</keyname><forenames>Santiago</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia</affiliation></author></authors><title>Proceedings 10th International Workshop on Reduction Strategies in
  Rewriting and Programming</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 82, 2012</journal-ref><doi>10.4204/EPTCS.82</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains a selection of the papers presented at the 10th
International Workshop on Reduction Strategies in Rewriting and Programming
(WRS'2011), held on 29 May 2011 in Novi Sad, Serbia. Previous editions of the
workshop were held in Utrecht (2001), Copenhagen (2002), Valencia (2003),
Aachen (2004), Nara (2005), Seattle (2006), Paris (2007), Hagenberg (2008),
Brasilia (2009), and Edinburgh (2010); the last one as a joint workshop with
the STRATEGIES workshop.
  The WRS 2011 workshop was part of the Federated Conference on Rewriting,
Deduction, and Programming (RDP'1), which grouped together different events
including the 22th International Conference on Rewriting Techniques and
Applications (RTA'11) and the 10th International Conference on Typed Lambda
Calculi and Applications (TLCA'11).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5320</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5320</id><created>2012-04-24</created><updated>2014-04-14</updated><authors><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Pascal</keyname><forenames>Frederic</forenames></author><author><keyname>Silverstein</keyname><forenames>Jack W.</forenames></author></authors><title>Robust Estimates of Covariance Matrices in the Large Dimensional Regime</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the limiting behavior of a class of robust population
covariance matrix estimators, originally due to Maronna in 1976, in the regime
where both the number of available samples and the population size grow large.
Using tools from random matrix theory, we prove that, for sample vectors made
of independent entries having some moment conditions, the difference between
the sample covariance matrix and (a scaled version of) such robust estimator
tends to zero in spectral norm, almost surely. This result can be applied to
various statistical methods arising from random matrix theory that can be made
robust without altering their first order behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5333</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5333</id><created>2012-04-24</created><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Avraham</keyname><forenames>Rinat Ben</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Computing the Discrete Fr\'echet Distance in Subquadratic Time</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fr\'echet distance is a similarity measure between two curves $A$ and
$B$: Informally, it is the minimum length of a leash required to connect a dog,
constrained to be on $A$, and its owner, constrained to be on $B$, as they walk
without backtracking along their respective curves from one endpoint to the
other. The advantage of this measure on other measures such as the Hausdorff
distance is that it takes into account the ordering of the points along the
curves.
  The discrete Fr\'echet distance replaces the dog and its owner by a pair of
frogs that can only reside on $n$ and $m$ specific pebbles on the curves $A$
and $B$, respectively. These frogs hop from a pebble to the next without
backtracking. The discrete Fr\'echet distance can be computed by a rather
straightforward quadratic dynamic programming algorithm. However, despite a
considerable amount of work on this problem and its variations, there is no
subquadratic algorithm known, even for approximation versions of the problem.
  In this paper we present a subquadratic algorithm for computing the discrete
Fr\'echet distance between two sequences of points in the plane, of respective
lengths $m\le n$. The algorithm runs in $O(\dfrac{mn\log\log n}{\log n})$ time
and uses $O(n+m)$ storage. Our approach uses the geometry of the problem in a
subtle way to encode legal positions of the frogs as states of a finite
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5335</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5335</id><created>2012-04-24</created><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Rucinski</keyname><forenames>Andrzej</forenames></author><author><keyname>Szymanska</keyname><forenames>Edyta</forenames></author></authors><title>Approximate Counting of Matchings in Sparse Uniform Hypergraphs</title><categories>cs.DS cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.5885</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a fully polynomial randomized approximation scheme
(FPRAS) for the number of matchings in k-uniform hypergraphs whose intersection
graphs contain few claws. Our method gives a generalization of the canonical
path method of Jerrum and Sinclair to hypergraphs satisfying a local
restriction. Our proof method depends on an application of the Euler tour
technique for the canonical paths of the underlying Markov chains. On the other
hand, we prove that it is NP-hard to approximate the number of matchings even
for the class of k-uniform, 2-regular and linear hypergraphs, for all k &gt;= 6,
without the above restriction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5345</identifier>
 <datestamp>2012-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5345</id><created>2012-04-24</created><authors><author><keyname>Stevens</keyname><forenames>William M.</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Jahan</keyname><forenames>Ishrat</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author></authors><title>Time-dependent wave selection for information processing in excitable
  media</title><categories>nlin.PS cs.CL</categories><journal-ref>Phys. Rev. E 85, 066129 (2012)</journal-ref><doi>10.1103/PhysRevE.85.066129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate an improved technique for implementing logic circuits in
light-sensitive chemical excitable media. The technique makes use of the
constant-speed propagation of waves along defined channels in an excitable
medium based on the Belousov-Zhabotinsky reaction, along with the mutual
annihilation of colliding waves. What distinguishes this work from previous
work in this area is that regions where channels meet at a junction can
periodically alternate between permitting the propagation of waves and blocking
them. These valve-like areas are used to select waves based on the length of
time that it takes waves to propagate from one valve to another. In an
experimental implementation, the channels which make up the circuit layout are
projected by a digital projector connected to a computer. Excitable channels
are projected as dark areas, unexcitable regions as light areas. Valves
alternate between dark and light: every valve has the same period and phase,
with a 50% duty cycle. This scheme can be used to make logic gates based on
combinations of OR and AND-NOT operations, with few geometrical constraints.
Because there are few geometrical constraints, compact circuits can be
implemented. Experimental results from an implementation of a 4-bit input,
2-bit output integer square root circuit are given. This is the most complex
logic circuit that has been implemented in BZ excitable media to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5347</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5347</id><created>2012-04-24</created><authors><author><keyname>Cleju</keyname><forenames>Nicolae</forenames></author><author><keyname>Jafari</keyname><forenames>Maria G.</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Analysis-based sparse reconstruction with synthesis-based solvers</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure, presented at IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis based reconstruction has recently been introduced as an alternative
to the well-known synthesis sparsity model used in a variety of signal
processing areas. In this paper we convert the analysis exact-sparse
reconstruction problem to an equivalent synthesis recovery problem with a set
of additional constraints. We are therefore able to use existing
synthesis-based algorithms for analysis-based exact-sparse recovery. We call
this the Analysis-By-Synthesis (ABS) approach. We evaluate our proposed
approach by comparing it against the recent Greedy Analysis Pursuit (GAP)
analysis-based recovery algorithm. The results show that our approach is a
viable option for analysis-based reconstruction, while at the same time
allowing many algorithms that have been developed for synthesis reconstruction
to be directly applied for analysis reconstruction as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5357</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5357</id><created>2012-04-24</created><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Jose M.</forenames></author></authors><title>Learning AMP Chain Graphs under Faithfulness</title><categories>stat.ML cs.AI math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with chain graphs under the alternative
Andersson-Madigan-Perlman (AMP) interpretation. In particular, we present a
constraint based algorithm for learning an AMP chain graph a given probability
distribution is faithful to. We also show that the extension of Meek's
conjecture to AMP chain graphs does not hold, which compromises the development
of efficient and correct score+search learning algorithms under assumptions
weaker than faithfulness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5368</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5368</id><created>2012-04-24</created><updated>2012-06-27</updated><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Lauser</keyname><forenames>Alexander</forenames></author></authors><title>The Join of the Varieties of R-trivial and L-trivial Monoids via
  Combinatorics on Words</title><categories>cs.FL</categories><journal-ref>Discrete Mathematics &amp; Theoretical Computer Science, Volume 14 No
  1, pages 141-146, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The join of two varieties is the smallest variety containing both. In finite
semigroup theory, the varieties of R-trivial and L-trivial monoids are two of
the most prominent classes of finite monoids. Their join is known to be
decidable due to a result of Almeida and Azevedo. In this paper, we give a new
proof for Almeida and Azevedo's effective characterization of the join of
R-trivial and L-trivial monoids. This characterization is a single identity of
omega-terms using three variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5369</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5369</id><created>2012-04-24</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Strapparava</keyname><forenames>Carlo</forenames></author><author><keyname>Stock</keyname><forenames>Oliviero</forenames></author></authors><title>Ecological Evaluation of Persuasive Messages Using Google AdWords</title><categories>cs.CL cs.SI</categories><comments>To appear at ACL 2012. 9 pages, 2 figures</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been a growing interest in crowdsourcing
methodologies to be used in experimental research for NLP tasks. In particular,
evaluation of systems and theories about persuasion is difficult to accommodate
within existing frameworks. In this paper we present a new cheap and fast
methodology that allows fast experiment building and evaluation with
fully-automated analysis at a low cost. The central idea is exploiting existing
commercial tools for advertising on the web, such as Google AdWords, to measure
message impact in an ecological setting. The paper includes a description of
the approach, tips for how to use AdWords for scientific research, and results
of pilot experiments on the impact of affective text variations which confirm
the effectiveness of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5371</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5371</id><created>2012-04-24</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Geometry and Dynamics of the Besicovitch and Weyl Spaces</title><categories>math.DS cs.DM cs.FL math.AT</categories><comments>15 pages. Submitted to DLT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the geometric properties of Cantor subshifts in the Besicovitch
space, proving that sofic shifts occupy exactly the homotopy classes of
simplicial complexes. In addition, we study canonical projections into
subshifts, characterize the cellular automata that are contracting or isometric
in the Besicovitch or Weyl spaces, study continuous functions that locally look
like cellular automata, and present a new proof for the nonexistence of
transitive cellular automata in the Besicovitch space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5373</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5373</id><created>2012-04-24</created><authors><author><keyname>Geva</keyname><forenames>Shlomo</forenames></author><author><keyname>De Vries</keyname><forenames>Christopher M.</forenames></author></authors><title>TopSig: Topology Preserving Document Signatures</title><categories>cs.IR</categories><comments>12 pages, 8 figures, CIKM 2011</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance comparisons between File Signatures and Inverted Files for text
retrieval have previously shown several significant shortcomings of file
signatures relative to inverted files. The inverted file approach underpins
most state-of-the-art search engine algorithms, such as Language and
Probabilistic models. It has been widely accepted that traditional file
signatures are inferior alternatives to inverted files. This paper describes
TopSig, a new approach to the construction of file signatures. Many advances in
semantic hashing and dimensionality reduction have been made in recent times,
but these were not so far linked to general purpose, signature file based,
search engines. This paper introduces a different signature file approach that
builds upon and extends these recent advances. We are able to demonstrate
significant improvements in the performance of signature file based indexing
and retrieval, performance that is comparable to that of state of the art
inverted file based systems, including Language models and BM25. These findings
suggest that file signatures offer a viable alternative to inverted files in
suitable settings and from the theoretical perspective it positions the file
signatures model in the class of Vector Space retrieval models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5383</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5383</id><created>2012-04-21</created><updated>2012-06-13</updated><authors><author><keyname>Serra</keyname><forenames>Jean</forenames></author><author><keyname>Kiran</keyname><forenames>Bangalore Ravi</forenames></author></authors><title>Climbing on Pyramids</title><categories>math.OC cs.SY</categories><comments>Few more errata fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach is proposed for finding the &quot;best cut&quot; in a hierarchy of
partitions by energy minimization. Said energy must be &quot;climbing&quot; i.e. it must
be hierarchically and scale increasing. It encompasses separable energies and
those composed under supremum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5388</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5388</id><created>2012-04-24</created><authors><author><keyname>Ickowicz</keyname><forenames>Adrien</forenames></author></authors><title>Track estimation with binary derivative observations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus in this paper in the estimation of a target trajectory defined by
whether a time constant parameter in a simple stochastic process or a random
walk with binary observations. The binary observation comes from binary
derivative sensors, that is, the target is getting closer or moving away. Such
a binary observation has a time property that will be used to ensure the
quality of a max-likelihood estimation, through single index model or
classification for the constant velocity movement. In the second part of this
paper we present a new algorithm for target tracking within a binary sensor
network when the target trajectory is assumed to be modelled by a random walk.
For a given target, this algorithm provides an estimation of its velocity and
its position. The greatest improvements are made through a position correction
and velocity analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5393</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5393</id><created>2012-04-24</created><updated>2012-08-31</updated><authors><author><keyname>Durand</keyname><forenames>Fabien</forenames><affiliation>LAMFA</affiliation></author></authors><title>Decidability of uniform recurrence of morphic sequences</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the uniform recurrence of morphic sequences is decidable. For
this we show that the number of derived sequences of uniformly recurrent
morphic sequences is bounded. As a corollary we obtain that uniformly recurrent
morphic sequences are primitive substitutive sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5399</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5399</id><created>2012-04-24</created><updated>2013-04-09</updated><authors><author><keyname>Carvalho</keyname><forenames>Arthur</forenames></author><author><keyname>Larson</keyname><forenames>Kate</forenames></author></authors><title>A Consensual Linear Opinion Pool</title><categories>cs.MA math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important question when eliciting opinions from experts is how to
aggregate the reported opinions. In this paper, we propose a pooling method to
aggregate expert opinions. Intuitively, it works as if the experts were
continuously updating their opinions in order to accommodate the expertise of
others. Each updated opinion takes the form of a linear opinion pool, where the
weight that an expert assigns to a peer's opinion is inversely related to the
distance between their opinions. In other words, experts are assumed to prefer
opinions that are close to their own opinions. We prove that such an updating
process leads to consensus, \textit{i.e.}, the experts all converge towards the
same opinion. Further, we show that if rational experts are rewarded using the
quadratic scoring rule, then the assumption that they prefer opinions that are
close to their own opinions follows naturally. We empirically demonstrate the
efficacy of the proposed method using real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5402</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5402</id><created>2012-04-24</created><authors><author><keyname>Aldinucci</keyname><forenames>Marco</forenames></author><author><keyname>Danelutto</keyname><forenames>Marco</forenames></author><author><keyname>Torquati</keyname><forenames>Massimo</forenames></author></authors><title>FastFlow tutorial</title><categories>cs.DC</categories><comments>49 pages + cover</comments><report-no>TR-12-04</report-no><acm-class>D.1.3; D.3.2; C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FastFlow is a structured parallel programming framework targeting shared
memory multicores. Its layered design and the optimized implementation of the
communication mechanisms used to implement the FastFlow streaming networks
provided to the application programmer as algorithmic skeletons support the
development of efficient fine grain parallel applications. FastFlow is
available (open source) at SourceForge
(http://sourceforge.net/projects/mc-fastflow/). This work introduces FastFlow
programming techniques and points out the different ways used to parallelize
existing C/C++ code using FastFlow as a software accelerator. In short: this is
a kind of tutorial on FastFlow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5407</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5407</id><created>2012-04-24</created><authors><author><keyname>Singla</keyname><forenames>Pradeep</forenames></author><author><keyname>Malik</keyname><forenames>Naveen Kr.</forenames></author></authors><title>Reversible Programmable Logic Array (RPLA) using Feynman &amp; MUX Gates for
  Low Power Industrial Applications</title><categories>cs.AR</categories><comments>9 Pages, 9 Figures</comments><journal-ref>Pradeep Singla and Naveen Kr. Malik. Article: Reversible
  Programmable Logic Array (RPLA) using Feynman &amp; MUX Gates for Low Power
  Industrial Applications. Proceedinggs of ICIAICT-2012,pp 411-419, March 2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper present the research work directed towards the design of
reversible programmable logic array using very high speed integrated circuit
hardware description language (VHDL). Reversible logic circuits have
significant importance in bioinformatics, optical information processing, CMOS
design etc. In this paper the authors propose the design of new RPLA using
Feynman &amp; MUX gate.VHDL based codes of reversible gates with simulating results
are shown .This proposed RPLA may be further used to design any reversible
logic function or Boolean function (Adder, subtractor etc.) which dissipate
very low or ideally no heat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5416</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5416</id><created>2012-04-24</created><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author><author><keyname>Kaushik</keyname><forenames>Vikas</forenames></author><author><keyname>Singla</keyname><forenames>Pradeep</forenames></author></authors><title>A New Approach of Improving CFA Image for Digital Camera's</title><categories>cs.CV</categories><comments>4 Pages, 6 Figures</comments><journal-ref>Proceedings of ETEIC-2012:pp 256-259, April-2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper work directly towards the improving the quality of the image for
the digital cameras and other visual capturing products. In this Paper, the
authors clearly defines the problems occurs in the CFA image. A different
methodology for removing the noise is discuses in the paper for color
correction and color balancing of the image. At the same time, the authors also
proposed a new methodology of providing denoisiing process before the
demosaickingfor the improving the image quality of CFA which is much efficient
then the other previous defined. The demosaicking process for producing the
colors in the image in a best way is also discuss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5429</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5429</id><created>2012-04-24</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>Understanding differential equations through diffusion point of view</title><categories>cs.NA math.NA</categories><comments>8 pages</comments><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new adaptation of the D-iteration algorithm to
numerically solve the differential equations. This problem can be reinterpreted
in 2D or 3D (or higher dimensions) as a limit of a diffusion process where the
boundary or initial conditions are replaced by fluid catalysts. Pre-computing
the diffusion process for an elementary catalyst case as a fundamental block of
a class of differential equations, we show that the computation efficiency can
be greatly improved. The method can be applied on the class of problems that
can be addressed by the Gauss-Seidel iteration, based on the linear
approximation of the differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5431</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5431</id><created>2012-04-24</created><updated>2012-05-12</updated><authors><author><keyname>Tofighi</keyname><forenames>Mohammad</forenames></author><author><keyname>Kalbkhani</keyname><forenames>Hashem</forenames></author><author><keyname>Shayesteh</keyname><forenames>Mahrokh G.</forenames></author><author><keyname>Ghasemzadeh</keyname><forenames>Mehdi</forenames></author></authors><title>Robust Head Pose Estimation Using Contourlet Transform</title><categories>cs.CV</categories><comments>5 pages, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating pose of the head is an important preprocessing step in many
pattern recognition and computer vision systems such as face recognition. Since
the performance of the face recognition systems is greatly affected by the
poses of the face, how to estimate the accurate pose of the face in human face
image is still a challenging problem. In this paper, we represent a novel
method for head pose estimation. To enhance the efficiency of the estimation we
use contourlet transform for feature extraction. Contourlet transform is
multi-resolution, multi-direction transform. In order to reduce the feature
space dimension and obtain appropriate features we use LDA (Linear Discriminant
Analysis) and PCA (Principal Component Analysis) to remove ineffcient features.
Then, we apply different classifiers such as k-nearest neighborhood (knn) and
minimum distance. We use the public available FERET database to evaluate the
performance of proposed method. Simulation results indicate the superior
robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5436</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5436</id><created>2012-04-24</created><authors><author><keyname>Braude</keyname><forenames>Eric</forenames></author></authors><title>Weakest Preconditions and Cumulative Subgoal Fulfillment: A Comparison</title><categories>cs.SE cs.DS</categories><comments>12 pages</comments><acm-class>D.1; D.2.3; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We contrast the use of weakest preconditions for the correct construction of
procedures with the cumulative subgoal fulfillment (CSF) approach. An example
of Cohen and Monin is used for this purpose. The CSF construction process is
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5442</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5442</id><created>2012-04-24</created><authors><author><keyname>Xin</keyname><forenames>Qin</forenames></author></authors><title>Faster Treasure Hunt and Better Strongly Universal Exploration Sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the explicit deterministic treasure hunt
problem in a $n$-vertex network. This problem was firstly introduced by Ta-Shma
and Zwick in \cite{TZ07} [SODA'07]. Note also it is a variant of the well known
rendezvous problem in which one of the robot (the treasure) is always
stationary. In this paper, we propose an $O(n^{c(1+\frac{1}{\lambda})})$-time
algorithm for the treasure hunt problem, which significantly improves the
currently best known result of running time $O(n^{2c})$ in \cite{TZ07}, where
$c$ is a constant induced from the construction of an universal exploration
sequence in \cite{R05,TZ07}, and $\lambda \gg 1$ is an arbitrary large, but
fixed, integer constant. The treasure hunt problem also motivates the study of
strongly universal exploration sequences. In this paper, we also propose a much
better explicit construction for strongly universal exploration sequences
compared to the one in \cite{TZ07}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5443</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5443</id><created>2012-04-24</created><authors><author><keyname>Kogan</keyname><forenames>Kirill</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Nikolenko</keyname><forenames>Sergey I.</forenames></author><author><keyname>Sirotkin</keyname><forenames>Alexander V.</forenames></author><author><keyname>Tugaryov</keyname><forenames>Denis</forenames></author></authors><title>FIFO Queueing Policies for Packets with Heterogeneous Processing</title><categories>cs.NI</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of managing a bounded size First-In-First-Out (FIFO)
queue buffer, where each incoming unit-sized packet requires several rounds of
processing before it can be transmitted out. Our objective is to maximize the
total number of successfully transmitted packets. We consider both push-out
(when the policy is permitted to drop already admitted packets) and
non-push-out cases. In particular, we provide analytical guarantees for the
throughput performance of our algorithms. We further conduct a comprehensive
simulation study which experimentally validates the predicted theoretical
behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5446</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5446</id><created>2012-04-24</created><updated>2012-12-17</updated><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Nguyen</keyname><forenames>Duy</forenames></author><author><keyname>Ohrimenko</keyname><forenames>Olga</forenames></author><author><keyname>Papamanthou</keyname><forenames>Charalampos</forenames></author><author><keyname>Tamassia</keyname><forenames>Roberto</forenames></author><author><keyname>Triandopoulos</keyname><forenames>Nikos</forenames></author><author><keyname>Lopes</keyname><forenames>Cristina Videira</forenames></author></authors><title>Verifying Search Results Over Web Collections</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching accounts for one of the most frequently performed computations over
the Internet as well as one of the most important applications of outsourced
computing, producing results that critically affect users' decision-making
behaviors. As such, verifying the integrity of Internet-based searches over
vast amounts of web contents is essential.
  We provide the first solution to this general security problem. We introduce
the concept of an authenticated web crawler and present the design and
prototype implementation of this new concept. An authenticated web crawler is a
trusted program that computes a special &quot;signature&quot; $s$ of a collection of web
contents it visits. Subject to this signature, web searches can be verified to
be correct with respect to the integrity of their produced results. This
signature also allows the verification of complicated queries on web pages,
such as conjunctive keyword searches. In our solution, along with the web pages
that satisfy any given search query, the search engine also returns a
cryptographic proof. This proof, together with the signature $s$, enables any
user to efficiently verify that no legitimate web pages are omitted from the
result computed by the search engine, and that no pages that are non-conforming
with the query are included in the result. An important property of our
solution is that the proof size and the verification time both depend solely on
the sizes of the query description and the query result, but not on the number
or sizes of the web pages over which the search is performed.
  Our authentication protocols are based on standard Merkle trees and the more
involved bilinear-map accumulators. As we experimentally demonstrate, the
prototype implementation of our system gives a low communication overhead
between the search engine and the user, and allows for fast verification of the
returned results on the user side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5447</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5447</id><created>2012-04-24</created><updated>2012-04-25</updated><authors><author><keyname>Shayda</keyname><forenames>Dara O.</forenames></author></authors><title>Kolmogorov Complexity, Causality And Spin</title><categories>cs.CC</categories><comments>Higher resolution images available, plus the Mathematica code that
  generated them</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel topological and computational method for 'motion' is described.
Motion is constrained by inequalities in terms of Kolmogorov Complexity.
Causality is obtained as the output of a high-pass filter, passing through only
high values of Kolmogorov Complexity. Motion under the electromagnetic field
described with immediate relationship with Subscript[G, 2] Holonomy group and
its corresponding dense free 2-subgroup. Similar to Causality, Spin emerges as
an immediate and inevitable consequence of high values of Kolmogorov
Complexity. Consequently, the physical laws are nothing but a low-pass filter
for small values of Kolmogorov Complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5462</identifier>
 <datestamp>2012-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5462</id><created>2012-04-24</created><updated>2012-07-18</updated><authors><author><keyname>Graben</keyname><forenames>Peter beim</forenames></author><author><keyname>Potthast</keyname><forenames>Roland</forenames></author></authors><title>Implementing Turing Machines in Dynamic Field Architectures</title><categories>cs.FL</categories><comments>5 pages, no figure</comments><journal-ref>In: M. &amp; Erden, Y. J. (Eds.) Proceedings of AISB12 World Congress
  2012 - Alan Turing 2012, 5th AISB Symposium on Computing and Philosophy:
  Computing, Philosophy and the Question of Bio-Machine Hybrids, 36 - 40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive computation such as e.g. language processing, is conventionally
regarded as Turing computation, and Turing machines can be uniquely implemented
as nonlinear dynamical systems using generalized shifts and subsequent G\&quot;odel
encoding of the symbolic repertoire. The resulting nonlinear dynamical automata
(NDA) are piecewise affine-linear maps acting on the unit square that is
partitioned into rectangular domains. Iterating a single point, i.e. a
microstate, by the dynamics yields a trajectory of, in principle, infinitely
many points scattered through phase space. Therefore, the NDAs microstate
dynamics does not necessarily terminate in contrast to its counterpart, the
symbolic dynamics obtained from the rectangular partition. In order to regain
the proper symbolic interpretation, one has to prepare ensembles of randomly
distributed microstates with rectangular supports. Only the resulting
macrostate evolution corresponds then to the original Turing machine
computation. However, the introduction of random initial conditions into a
deterministic dynamics is not really satisfactory. As a possible solution for
this problem we suggest a change of perspective. Instead of looking at point
dynamics in phase space, we consider functional dynamics of probability
distributions functions (p.d.f.s) over phase space. This is generally described
by a Frobenius-Perron integral transformation that can be regarded as a neural
field equation over the unit square as feature space of a dynamic field theory
(DFT). Solving the Frobenius-Perron equation, yields that uniform p.d.f.s with
rectangular support are mapped onto uniform p.d.f.s with rectangular support,
again. Thus, the symbolically meaningful NDA macrostate dynamics becomes
represented by iterated function dynamics in DFT; hence we call the resulting
representation dynamic field automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5467</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5467</id><created>2012-04-24</created><authors><author><keyname>Ron-Zewi</keyname><forenames>Noga</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>A new upper bound on the query complexity for testing generalized
  Reed-Muller codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over a finite field $\F_q$ the $(n,d,q)$-Reed-Muller code is the code given
by evaluations of $n$-variate polynomials of total degree at most $d$ on all
points (of $\F_q^n$). The task of testing if a function $f:\F_q^n \to \F_q$ is
close to a codeword of an $(n,d,q)$-Reed-Muller code has been of central
interest in complexity theory and property testing. The query complexity of
this task is the minimal number of queries that a tester can make (minimum over
all testers of the maximum number of queries over all random choices) while
accepting all Reed-Muller codewords and rejecting words that are $\delta$-far
from the code with probability $\Omega(\delta)$. (In this work we allow the
constant in the $\Omega$ to depend on $d$.) In this work we give a new upper
bound of $(c q)^{(d+1)/q}$ on the query complexity, where $c$ is a universal
constant. In the process we also give new upper bounds on the &quot;spanning weight&quot;
of the dual of the Reed-Muller code (which is also a Reed-Muller code). The
spanning weight of a code is the smallest integer $w$ such that codewords of
Hamming weight at most $w$ span the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5489</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5489</id><created>2012-04-24</created><updated>2012-10-25</updated><authors><author><keyname>Chlamtac</keyname><forenames>Eden</forenames></author><author><keyname>Friggstad</keyname><forenames>Zac</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author></authors><title>Understanding Set Cover: Sub-exponential Time Approximations and
  Lift-and-Project Methods</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Cygan, Kowalik, and Wykurz [IPL 2009] gave sub-exponential-time
approximation algorithms for the Set-Cover problem with approximation ratios
better than ln(n). In light of this result, it is natural to ask whether such
improvements can be achieved using lift-and-project methods. We present a
simpler combinatorial algorithm which has nearly the same time-approximation
tradeoff as the algorithm of Cygan et al., and which lends itself naturally to
a lift-and-project based approach.
  At a high level, our approach is similar to the recent work of Karlin,
Mathieu, and Nguyen [IPCO 2011], who examined a known PTAS for Knapsack
(similar to our combinatorial Set-Cover algorithm) and its connection to
hierarchies of LP and SDP relaxations for Knapsack. For Set-Cover, we show
that, indeed, using the trick of &quot;lifting the objective function&quot;, we can match
the performance of our combinatorial algorithm using the LP hierarchy of Lovasz
and Schrijver. We also show that this trick is essential: even in the stronger
LP hierarchy of Sherali and Adams, the integrality gap remains at least (1-eps)
ln(n) at level Omega(n) (when the objective function is not lifted).
  As shown by Aleknovich, Arora, and Tourlakis [STOC 2005], Set-Cover
relaxations stemming from SDP hierarchies (specifically, LS+) have similarly
large integrality gaps. This stands in contrast to Knapsack, where Karlin et
al. showed that the (much stronger) Lasserre SDP hierarchy reduces the
integrality gap to (1+eps) at level O(1). For completeness, we show that LS+
also reduces the integrality gap for Knapsack to (1+eps). This result may be of
independent interest, as our LS+ based rounding and analysis are rather
different from those of Karlin et al., and to the best of our knowledge this is
the first explicit demonstration of such a reduction in the integrality gap of
LS+ relaxations after few rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5490</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5490</id><created>2012-04-24</created><authors><author><keyname>Britnell</keyname><forenames>John R.</forenames></author><author><keyname>Wildon</keyname><forenames>Mark</forenames></author></authors><title>Finding a princess in a palace: A pursuit-evasion problem</title><categories>math.CO cs.DM</categories><comments>8 pages</comments><msc-class>05C57, (secondary) 91A24, 91A43</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper solves a pursuit-evasion problem in which a prince must find a
princess who is constrained to move on each day from one vertex of a finite
graph to another. Unlike the related and much studied `Cops and Robbers Game',
the prince has no knowledge of the position of the princess; he may, however,
visit any single room he wishes on each day. We characterize the graphs for
which the prince has a winning strategy, and determine, for each such graph,
the minimum number of days the prince requires to guarantee to find the
princess.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5500</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5500</id><created>2012-04-24</created><updated>2014-04-11</updated><authors><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author></authors><title>On the Complexity of the Monte Carlo Method for Incremental PageRank</title><categories>cs.DS</categories><journal-ref>Information Processing Letters, Volume 114, Issue 3, March 2014,
  Pages 104-106</journal-ref><doi>10.1016/j.ipl.2013.11.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note extends the analysis of incremental PageRank in [B. Bahmani, A.
Chowdhury, and A. Goel. Fast Incremental and Personalized PageRank. VLDB 2011].
In that work, the authors prove a running time of $O(\frac{nR}{\epsilon^2}
\ln(m))$ to keep PageRank updated over $m$ edge arrivals in a graph with $n$
nodes when the algorithm stores $R$ random walks per node and the PageRank
teleport probability is $\epsilon$. To prove this running time, they assume
that edges arrive in a random order, and leave it to future work to extend
their running time guarantees to adversarial edge arrival. In this note, we
show that the random edge order assumption is necessary by exhibiting a graph
and adversarial edge arrival order in which the running time is $\Omega \left(R
n m^{\lg{\frac{3}{2}(1-\epsilon)}}\right)$. More generally, for any integer $d
\geq 2$, we construct a graph and adversarial edge order in which the running
time is $\Omega \left(R n m^{\log_d(H_d (1-\epsilon))}\right)$, where $H_d$ is
the $d$th harmonic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5507</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5507</id><created>2012-04-24</created><updated>2012-11-11</updated><authors><author><keyname>Rajawat</keyname><forenames>Ketan</forenames></author><author><keyname>Dall'Anese</keyname><forenames>Emiliano</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Dynamic Network Delay Cartography</title><categories>cs.NI</categories><comments>Part of this paper has been published in the \emph{IEEE Statistical
  Signal Processing Workshop}, Ann Arbor, MI, Aug. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path delays in IP networks are important metrics, required by network
operators for assessment, planning, and fault diagnosis. Monitoring delays of
all source-destination pairs in a large network is however challenging and
wasteful of resources. The present paper advocates a spatio-temporal Kalman
filtering approach to construct network-wide delay maps using measurements on
only a few paths. The proposed network cartography framework allows efficient
tracking and prediction of delays by relying on both topological as well as
historical data. Optimal paths for delay measurement are selected in an online
fashion by leveraging the notion of submodularity. The resulting predictor is
optimal in the class of linear predictors, and outperforms competing
alternatives on real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5508</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5508</id><created>2012-04-24</created><authors><author><keyname>Aehlig</keyname><forenames>Klaus</forenames></author><author><keyname>Cook</keyname><forenames>Stephen</forenames></author><author><keyname>Nguyen</keyname><forenames>Phuong</forenames></author></authors><title>Relativizing Small Complexity Classes and their Theories</title><categories>cs.CC</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing definitions of the relativizations of \NCOne, \L\ and \NL\ do not
preserve the inclusions $\NCOne \subseteq \L$, $\NL\subseteq \ACOne$. We start
by giving the first definitions that preserve them. Here for \L\ and \NL\ we
define their relativizations using Wilson's stack oracle model, but limit the
height of the stack to a constant (instead of $\log(n)$). We show that the
collapse of any two classes in $\{\ACZm, \TCZ, \NCOne, \L, \NL\}$ implies the
collapse of their relativizations. Next we exhibit an oracle $\alpha$ that
makes $\ACk(\alpha)$ a proper hierarchy. This strengthens and clarifies the
separations of the relativized theories in [Takeuti, 1995]. The idea is that a
circuit whose nested depth of oracle gates is bounded by $k$ cannot compute
correctly the $(k+1)$ compositions of every oracle function. Finally we develop
theories that characterize the relativizations of subclasses of \Ptime\ by
modifying theories previously defined by the second two authors. A function is
provably total in a theory iff it is in the corresponding relativized class,
and hence the oracle separations imply separations for the relativized
theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5513</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5513</id><created>2012-04-24</created><updated>2016-02-01</updated><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>The impulse cutoff an entropy functional measure on trajectories of
  Markov diffusion process integrating in information path functional</title><categories>nlin.AO cs.IT math.IT math.OC math.PR</categories><comments>54 pages,1 figure</comments><msc-class>58J65, 60J65, 93B52, 93E02, 93E15, 93E30</msc-class><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The impulses, cutting entropy functional (EF) measure on trajectories Markov
diffusion process, integrate information path functional (IPF) composing
discrete information Bits extracted from observing random process. Each cut
brings memory of the cutting entropy, which provides both reduction of the
process entropy and discrete unit of the cutting entropy a Bit. Consequently,
information is memorized entropy cutting in random observations which process
interactions. The origin of information associates with anatomy creation of
impulse enables both cut entropy and stipulate random process generating
information under the cut. Memory of the impulse cutting time interval freezes
the observing events dynamics in information processes. Diffusion process
additive functional defines EF reducing it to a regular integral functional.
Compared to Shannon entropy measure of random state, cutting process on
separated states decreases quantity information concealed in the states
correlation holding hidden process information. Infinite dimensional process
cutoffs integrate finite information in IPF whose information approaches EF
restricting process maximal information. Within the impulse reversible
microprocess, conjugated entropy increments are entangling up to the cutoff
converting entropy in irreversible information. Extracting maximum of minimal
impulse information and transferring minimal entropy between impulses implement
maxmin-minimax principle of optimal conversion process entropy to information.
Macroprocess extremals integrate entropy of microprocess and cutoff information
of impulses in the IPF information physical process. IPF measures Feller kernel
information. Estimation extracting information confirms nonadditivity of EF
measured process increments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5519</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5519</id><created>2012-04-24</created><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author></authors><title>Optimal Mechanisms for Selling Information</title><categories>cs.GT</categories><comments>accepted to EC'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The buying and selling of information is taking place at a scale
unprecedented in the history of commerce, thanks to the formation of online
marketplaces for user data. Data providing agencies sell user information to
advertisers to allow them to match ads to viewers more effectively. In this
paper we study the design of optimal mechanisms for a monopolistic data
provider to sell information to a buyer, in a model where both parties have
(possibly correlated) private signals about a state of the world, and the buyer
uses information learned from the seller, along with his own signal, to choose
an action (e.g., displaying an ad) whose payoff depends on the state of the
world.
  We provide sufficient conditions under which there is a simple one-round
protocol (i.e. a protocol where the buyer and seller each sends a single
message, and there is a single money transfer) achieving optimal revenue. In
these cases we present a polynomial-time algorithm that computes the optimal
mechanism. Intriguingly, we show that multiple rounds of partial information
disclosure (interleaved by payment to the seller) are sometimes necessary to
achieve optimal revenue if the buyer is allowed to abort his interaction with
the seller prematurely. We also prove some negative results about the inability
of simple mechanisms for selling information to approximate more complicated
ones in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5524</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5524</id><created>2012-04-24</created><updated>2013-05-26</updated><authors><author><keyname>Yamamoto</keyname><forenames>Jun'ichi</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Time and Space Efficient Lempel-Ziv Factorization based on Run Length
  Encoding</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for calculating the Lempel-Ziv factorization of a
string, based on run length encoding (RLE). We present a conceptually simple
off-line algorithm based on a variant of suffix arrays, as well as an on-line
algorithm based on a variant of directed acyclic word graphs (DAWGs). Both
algorithms run in $O(N+n\log n)$ time and O(n) extra space, where N is the size
of the string, $n\leq N$ is the number of RLE factors. The time dependency on N
is only in the conversion of the string to RLE, which can be computed very
efficiently in O(N) time and O(1) extra space (excluding the output). When the
string is compressible via RLE, i.e., $n = o(N)$, our algorithms are, to the
best of our knowledge, the first algorithms which require only o(N) extra space
while running in $o(N\log N)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5525</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5525</id><created>2012-04-24</created><authors><author><keyname>Singla</keyname><forenames>Pradeep</forenames></author><author><keyname>Malik</keyname><forenames>Naveen Kr.</forenames></author></authors><title>A Cost- Effective Design of Reversible Programmable Logic Array</title><categories>cs.OH</categories><comments>6 Pages, 9 Figures</comments><journal-ref>Pradeep Singla and Naveen Kr. Malik. Article: A Cost- Effective
  Design of Reversible Programmable Logic Array. IJCA 41(15):41-46, March 2012.
  Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/5619-7911</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the recent era, Reversible computing is a growing field having
applications in nanotechnology, optical information processing, quantum
networks etc. In this paper, the authors show the design of a cost effective
reversible programmable logic array using VHDL. It is simulated on xilinx ISE
8.2i and results are shown. The proposed reversible Programming logic array
called RPLA is designed by MUX gate [10] &amp; Feynman gate for 3- inputs, which is
able to perform any reversible 3- input logic function or Boolean function.
Furthermore the quantized analysis with camparitive finding is shown for the
realized RPLA against the existing one. The result shows improvement in the
quantum cost and total logical caculation in proposed RPLA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5526</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5526</id><created>2012-04-24</created><updated>2012-05-27</updated><authors><author><keyname>Singla</keyname><forenames>Pradeep</forenames></author><author><keyname>Satyan</keyname></author></authors><title>Towards the Solution of Power Dissipation in Electronics Systems through
  Thermodynamics</title><categories>cs.OH</categories><comments>4 Pages, 10 Figures</comments><journal-ref>Pradeep Singla and Satyan. Article:Towards the Solution of Power
  Dissipation in Electronics Systems through Thermodynamics. Proceedings of
  ETEIC-2012: pp 300-303, April-2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Power loss in the electronic system is a very crucial limiting factor that
can be reduced or minimized with the help of using the reversible logics &quot;a
concept came from Thermodynamics&quot;. In this paper the authors shows the concept
of reversible logics for the Electronics system. The logical and physical
designing approach is given in the paper in detail. The contradiction of
logical and physical reversibility with the conventional CMOS designing is also
shows and the solution of that contradiction is also proposed by the authors
using adiabatic logic. This Paper gives a complete and clear idea if the
thermodynamical concept for the electronics industries for power reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5541</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5541</id><created>2012-04-24</created><authors><author><keyname>Plump</keyname><forenames>Detlef</forenames><affiliation>The University of York</affiliation></author></authors><title>The Design of GP 2</title><categories>cs.PL cs.LO</categories><comments>In Proceedings WRS 2011, arXiv:1204.5318</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 82, 2012, pp. 1-16</journal-ref><doi>10.4204/EPTCS.82.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This papers defines the syntax and semantics of GP 2, a revised version of
the graph programming language GP. New concepts are illustrated and explained
with example programs. Changes to the first version of GP include an improved
type system for labels, a built-in marking mechanism for nodes and edges, a
more powerful edge predicate for conditional rule schemata, and functions
returning the indegree and outdegree of matched nodes. Moreover, the semantics
of the branching and loop statement have been simplified to allow their
efficient implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5542</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5542</id><created>2012-04-24</created><authors><author><keyname>Verdejo</keyname><forenames>Alberto</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author><author><keyname>Mart&#xed;-Oliet</keyname><forenames>Narciso</forenames><affiliation>Universidad Complutense de Madrid</affiliation></author></authors><title>Basic completion strategies as another application of the Maude strategy
  language</title><categories>cs.LO cs.PL</categories><comments>In Proceedings WRS 2011, arXiv:1204.5318</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 82, 2012, pp. 17-36</journal-ref><doi>10.4204/EPTCS.82.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two levels of data and actions on those data provided by the separation
between equations and rules in rewriting logic are completed by a third level
of strategies to control the application of those actions. This level is
implemented on top of Maude as a strategy language, which has been successfully
used in a wide range of applications. First we summarize the Maude strategy
language design and review some of its applications; then, we describe a new
case study, namely the description of completion procedures as transition rules
+ control, as proposed by Lescanne.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5543</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5543</id><created>2012-04-24</created><authors><author><keyname>Belkhir</keyname><forenames>Walid</forenames><affiliation>University of Franche-Comt&#xe9;</affiliation></author><author><keyname>Giorgetti</keyname><forenames>Alain</forenames><affiliation>University of Franche-Comt&#xe9;</affiliation></author></authors><title>Lazy AC-Pattern Matching for Rewriting</title><categories>cs.LO cs.PL</categories><comments>In Proceedings WRS 2011, arXiv:1204.5318</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 82, 2012, pp. 37-51</journal-ref><doi>10.4204/EPTCS.82.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a lazy pattern-matching mechanism modulo associativity and
commutativity. The solutions of a pattern-matching problem are stored in a lazy
list composed of a first substitution at the head and a non-evaluated object
that encodes the remaining computations. We integrate the lazy AC-matching in a
strategy language: rewriting rule and strategy application produce a lazy list
of terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5544</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5544</id><created>2012-04-24</created><authors><author><keyname>Raffelsieper</keyname><forenames>Matthias</forenames><affiliation>TU Eindhoven</affiliation></author></authors><title>Productivity of Non-Orthogonal Term Rewrite Systems</title><categories>cs.LO cs.PL</categories><comments>In Proceedings WRS 2011, arXiv:1204.5318</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 82, 2012, pp. 53-67</journal-ref><doi>10.4204/EPTCS.82.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Productivity is the property that finite prefixes of an infinite constructor
term can be computed using a given term rewrite system. Hitherto, productivity
has only been considered for orthogonal systems, where non-determinism is not
allowed. This paper presents techniques to also prove productivity of
non-orthogonal term rewrite systems. For such systems, it is desired that one
does not have to guess the reduction steps to perform, instead any
outermost-fair reduction should compute an infinite constructor term in the
limit. As a main result, it is shown that for possibly non-orthogonal term
rewrite systems this kind of productivity can be concluded from
context-sensitive termination. This result can be applied to prove
stabilization of digital circuits, as will be illustrated by means of an
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5545</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5545</id><created>2012-04-24</created><authors><author><keyname>Zantema</keyname><forenames>Hans</forenames><affiliation>University of Technology Eindhoven</affiliation></author></authors><title>Strategy Independent Reduction Lengths in Rewriting and Binary
  Arithmetic</title><categories>cs.LO cs.PL</categories><comments>In Proceedings WRS 2011, arXiv:1204.5318</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 82, 2012, pp. 69-76</journal-ref><doi>10.4204/EPTCS.82.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a criterion by which one can conclude that every
reduction of a basic term to normal form has the same length. As a consequence,
the number of steps to reach the normal form is independent of the chosen
strategy. In particular this holds for TRSs computing addition and
multiplication of natural numbers, both in unary and binary notation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5547</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5547</id><created>2012-04-24</created><updated>2013-05-20</updated><authors><author><keyname>Ghorpade</keyname><forenames>Sudhir R.</forenames></author><author><keyname>Kaipa</keyname><forenames>Krishna V.</forenames></author></authors><title>Automorphism groups of Grassmann codes</title><categories>cs.IT math.AG math.IT</categories><comments>revised version</comments><msc-class>14M15, 20B25, 94B05, 94B27</msc-class><journal-ref>Finite Fields Appl. 23 (2013), 80-102</journal-ref><doi>10.1016/j.ffa.2013.04.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a theorem of Chow (1949) on line-preserving bijections of
Grassmannians to determine the automorphism group of Grassmann codes. Further,
we analyze the automorphisms of the big cell of a Grassmannian and then use it
to settle an open question of Beelen et al. (2010) concerning the permutation
automorphism groups of affine Grassmann codes. Finally, we prove an analogue of
Chow's theorem for the case of Schubert divisors in Grassmannians and then use
it to determine the automorphism group of linear codes associated to such
Schubert divisors. In the course of this work, we also give an alternative
short proof of MacWilliams theorem concerning the equivalence of linear codes
and a characterization of maximal linear subspaces of Schubert divisors in
Grassmannians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5551</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5551</id><created>2012-04-24</created><updated>2013-10-07</updated><authors><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>A lower bound on seller revenue in single buyer monopoly auctions</title><categories>cs.GT</categories><comments>5 pages. To appear in Operations Research Letters</comments><journal-ref>Operations Research Letters, 2013, Volume 41, Issue 5, Pages
  474--476</journal-ref><doi>10.1016/j.orl.2013.05.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a monopoly seller who optimally auctions a single object to a
single potential buyer, with a known distribution of valuations. We show that a
tight lower bound on the seller's expected revenue is $1/e$ times the geometric
expectation of the buyer's valuation, and that this bound is uniquely achieved
for the equal revenue distribution. We show also that when the valuation's
expectation and geometric expectation are close, then the seller's expected
revenue is close to the expected valuation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5563</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5563</id><created>2012-04-25</created><authors><author><keyname>Gowanlock</keyname><forenames>Michael G.</forenames></author><author><keyname>Gazan</keyname><forenames>Rich</forenames></author></authors><title>Assessing Researcher Interdisciplinarity: A Case Study of the University
  of Hawaii NASA Astrobiology Institute</title><categories>physics.soc-ph cs.DL</categories><comments>Accepted for publication in Scientometrics. 41 pages, 12 figures, 9
  tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we combine bibliometric techniques with a machine learning
algorithm, the sequential Information Bottleneck, to assess the
interdisciplinarity of research produced by the University of Hawaii NASA
Astrobiology Institute (UHNAI). In particular, we cluster abstract data to
evaluate Thomson Reuters Web of Knowledge subject categories as descriptive
labels for astrobiology documents, assess individual researcher
interdisciplinarity, and determine where collaboration opportunities might
occur. We find that the majority of the UHNAI team is engaged in
interdisciplinary research, and suggest that our method could be applied to
additional NASA Astrobiology Institute teams in particular, or other
interdisciplinary research teams more broadly, to identify and facilitate
collaboration opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5576</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5576</id><created>2012-04-25</created><updated>2012-10-06</updated><authors><author><keyname>Zhou</keyname><forenames>YuQian</forenames></author></authors><title>Efficient programs of NPC problems should be length upper-bounded, and a
  thought experiment to search for them by machine enumeration</title><categories>cs.CC</categories><comments>6 pages, 2 figures</comments><acm-class>C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a thought experiment to search for efficient bounded
algorithms of NPC problems by machine enumeration. The key contributions are:
  -- On Universal Turing Machines, a program's time complexity should be
characterized as: execution time(n) = loading time(n) + running time(n).
  -- Introduces the concept of bounded algorithms; proposes a comparison based
criterion to decide if a bounded algorithm is inefficient; and establishes the
length upper bound of efficient bounded programs.
  -- Introduces the growth rate characteristic function to evaluate program
complexity, which is more easily machine checkable based on observations.
  -- Raises the theoretical question: if there exists any bounded algorithm
with polynomial execution time for NPC problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5577</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5577</id><created>2012-04-25</created><updated>2013-10-16</updated><authors><author><keyname>Farrell</keyname><forenames>Patrick E.</forenames><affiliation>Department of Earth Science and Engineering Imperial College London</affiliation></author><author><keyname>Ham</keyname><forenames>David A.</forenames><affiliation>Department of Computing Imperial College London</affiliation><affiliation>Grantham Institute for Climate Change Imperial College London</affiliation></author><author><keyname>Funke</keyname><forenames>Simon F.</forenames><affiliation>Department of Earth Science and Engineering Imperial College London</affiliation><affiliation>Grantham Institute for Climate Change Imperial College London</affiliation></author><author><keyname>Rognes</keyname><forenames>Marie E.</forenames><affiliation>Simula Research Laboratory Lysaker Norway</affiliation></author></authors><title>Automated derivation of the adjoint of high-level transient finite
  element programs</title><categories>cs.MS</categories><msc-class>65N30, 68N20, 49M29</msc-class><journal-ref>SIAM Journal on Scientific Computing 2013 35:4, C369-C393</journal-ref><doi>10.1137/120873558 10.1137/120873558 10.1137/120873558 10.1137/120873558
  10.1137/120873558 10.1137/120873558</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate a new technique for deriving discrete adjoint
and tangent linear models of finite element models. The technique is
significantly more efficient and automatic than standard algorithmic
differentiation techniques. The approach relies on a high-level symbolic
representation of the forward problem. In contrast to developing a model
directly in Fortran or C++, high-level systems allow the developer to express
the variational problems to be solved in near-mathematical notation. As such,
these systems have a key advantage: since the mathematical structure of the
problem is preserved, they are more amenable to automated analysis and
manipulation. The framework introduced here is implemented in a freely
available software package named dolfin-adjoint, based on the FEniCS Project.
Our approach to automated adjoint derivation relies on run-time annotation of
the temporal structure of the model, and employs the FEniCS finite element form
compiler to automatically generate the low-level code for the derived models.
The approach requires only trivial changes to a large class of forward models,
including complicated time-dependent nonlinear models. The adjoint model
automatically employs optimal checkpointing schemes to mitigate storage
requirements for nonlinear models, without any user management or intervention.
Furthermore, both the tangent linear and adjoint models naturally work in
parallel, without any need to differentiate through calls to MPI or to parse
OpenMP directives. The generality, applicability and efficiency of the approach
are demonstrated with examples from a wide range of scientific applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5580</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5580</id><created>2012-04-25</created><authors><author><keyname>Manubens</keyname><forenames>Montserrat</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wenger</keyname><forenames>Philippe</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Cusp Points in the Parameter Space of Degenerate 3-RPR Planar Parallel
  Manipulators</title><categories>cs.RO</categories><comments>ASME Journal of Mechanisms and Robotics (2012) 1-10</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the conditions in the design parameter space for the
existence and distribution of the cusp locus for planar parallel manipulators.
Cusp points make possible non-singular assembly-mode changing motion, which
increases the maximum singularity-free workspace. An accurate algorithm for the
determination is proposed amending some imprecisions done by previous existing
algorithms. This is combined with methods of Cylindric Algebraic Decomposition,
Gr\&quot;obner bases and Discriminant Varieties in order to partition the parameter
space into cells with constant number of cusp points. These algorithms will
allow us to classify a family of degenerate 3-RPR manipulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5590</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5590</id><created>2012-04-25</created><authors><author><keyname>Gupta</keyname><forenames>B. B.</forenames></author><author><keyname>Joshi</keyname><forenames>R. C.</forenames></author><author><keyname>Misra</keyname><forenames>Manoj</forenames></author></authors><title>An Efficient Analytical Solution to Thwart DDoS Attacks in Public Domain</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.2400</comments><journal-ref>Proceedings of ACM International Conference on Advances in
  Computer, Communication and Computing (ICAC3-2008), pp. 503-509, Jan. 23-24,
  2009,India</journal-ref><doi>10.1145/1523103.1523203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an analytical model for DDoS attacks detection is proposed, in
which propagation of abrupt traffic changes inside public domain is monitored
to detect a wide range of DDoS attacks. Although, various statistical measures
can be used to construct profile of the traffic normally seen in the network to
identify anomalies whenever traffic goes out of profile, we have selected
volume and flow measure. Consideration of varying tolerance factors make
proposed detection system scalable to the varying network conditions and attack
loads in real time. NS-2 network simulator on Linux platform is used as
simulation testbed. Simulation results show that our proposed solution gives a
drastic improvement in terms of detection rate and false positive rate.
However, the mammoth volume generated by DDoS attacks pose the biggest
challenge in terms of memory and computational overheads as far as monitoring
and analysis of traffic at single point connecting victim is concerned. To
address this problem, a distributed cooperative technique is proposed that
distributes memory and computational overheads to all edge routers for
detecting a wide range of DDoS attacks at early stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5592</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5592</id><created>2012-04-25</created><authors><author><keyname>Gupta</keyname><forenames>B. B.</forenames></author><author><keyname>Joshi</keyname><forenames>R. C.</forenames></author><author><keyname>Misra</keyname><forenames>Manoj</forenames></author></authors><title>Dynamic and Auto Responsive Solution for Distributed Denial-of-Service
  Attacks Detection in ISP Network</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.2400</comments><journal-ref>International Journal of Computer Theory and Engineering, Vol. 1,
  No. 1, April 2009 1793-821X</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denial of service (DoS) attacks and more particularly the distributed ones
(DDoS) are one of the latest threat and pose a grave danger to users,
organizations and infrastructures of the Internet. Several schemes have been
proposed on how to detect some of these attacks, but they suffer from a range
of problems, some of them being impractical and others not being effective
against these attacks. This paper reports the design principles and evaluation
results of our proposed framework that autonomously detects and accurately
characterizes a wide range of flooding DDoS attacks in ISP network. Attacks are
detected by the constant monitoring of propagation of abrupt traffic changes
inside ISP network. For this, a newly designed flow-volume based approach
(FVBA) is used to construct profile of the traffic normally seen in the
network, and identify anomalies whenever traffic goes out of profile.
Consideration of varying tolerance factors make proposed detection system
scalable to the varying network conditions and attack loads in real time.
Six-sigma method is used to identify threshold values accurately for malicious
flows characterization. FVBA has been extensively evaluated in a controlled
test-bed environment. Detection thresholds and efficiency is justified using
receiver operating characteristics (ROC) curve. For validation, KDD 99, a
publicly available benchmark dataset is used. The results show that our
proposed system gives a drastic improvement in terms of detection and false
alarm rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5602</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5602</id><created>2012-04-25</created><updated>2013-12-16</updated><authors><author><keyname>Saramaki</keyname><forenames>J.</forenames></author><author><keyname>Leicht</keyname><forenames>E. A.</forenames></author><author><keyname>Lopez</keyname><forenames>E.</forenames></author><author><keyname>Roberts</keyname><forenames>S. G. B.</forenames></author><author><keyname>Reed-Tsochas</keyname><forenames>F.</forenames></author><author><keyname>Dunbar</keyname><forenames>R. I. M.</forenames></author></authors><title>The persistence of social signatures in human communication</title><categories>physics.soc-ph cs.SI</categories><comments>Revised version, SI Appendix added</comments><journal-ref>Proc.Natl.Acad.Sci. U.S.A. 111 (2014) 942-947</journal-ref><doi>10.1073/pnas.1308540110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social network maintained by a focal individual, or ego, is intrinsically
dynamic and typically exhibits some turnover in membership over time as
personal circumstances change. However, the consequences of such changes on the
distribution of an ego's network ties are not well understood. Here we use a
unique 18-month data set that combines mobile phone calls and survey data to
track changes in the ego networks and communication patterns of students making
the transition from school to university or work. Our analysis reveals that
individuals display a distinctive and robust social signature, captured by how
interactions are distributed across different alters. Notably, for a given ego,
these social signatures tend to persist over time, despite considerable
turnover in the identity of alters in the ego network. Thus as new network
members are added, some old network members are either replaced or receive
fewer calls, preserving the overall distribution of calls across network
members. This is likely to reflect the consequences of finite resources such as
the time available for communication, the cognitive and emotional effort
required to sustain close relationships, and the ability to make emotional
investments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5613</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5613</id><created>2012-04-25</created><authors><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author></authors><title>Rerouting shortest paths in planar graphs</title><categories>cs.DS</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rerouting sequence is a sequence of shortest st-paths such that consecutive
paths differ in one vertex. We study the the Shortest Path Rerouting Problem,
which asks, given two shortest st-paths P and Q in a graph G, whether a
rerouting sequence exists from P to Q. This problem is PSPACE-hard in general,
but we show that it can be solved in polynomial time if G is planar. To this
end, we introduce a dynamic programming method for reconfiguration problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5631</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5631</id><created>2012-04-25</created><updated>2012-06-01</updated><authors><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author><author><keyname>Powell</keyname><forenames>Thomas</forenames></author></authors><title>A Constructive Interpretation of Ramsey's Theorem via the Product of
  Selection Functions</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use G\&quot;{o}del's Dialectica interpretation to produce a computational
version of the well known proof of Ramsey's theorem by Erd\H{o}s and Rado. Our
proof makes use of the product of selection functions, which forms an intuitive
alternative to Spector's bar recursion when interpreting proofs in analysis.
This case study is another instance of the application of proof theoretic
techniques in mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5635</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5635</id><created>2012-04-25</created><updated>2012-12-05</updated><authors><author><keyname>Ram&#xed;rez</keyname><forenames>D.</forenames></author><author><keyname>V&#xed;a</keyname><forenames>J.</forenames></author><author><keyname>Santamar&#xed;a</keyname><forenames>I.</forenames></author><author><keyname>Scharf</keyname><forenames>L. L.</forenames></author></authors><title>Locally Most Powerful Invariant Tests for Correlation and Sphericity of
  Gaussian Vectors</title><categories>cs.IT math.IT stat.OT</categories><doi>10.1109/TIT.2012.2232705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the existence of locally most powerful invariant tests
(LMPIT) for the problem of testing the covariance structure of a set of
Gaussian random vectors. The LMPIT is the optimal test for the case of close
hypotheses, among those satisfying the invariances of the problem, and in
practical scenarios can provide better performance than the typically used
generalized likelihood ratio test (GLRT). The derivation of the LMPIT usually
requires one to find the maximal invariant statistic for the detection problem
and then derive its distribution under both hypotheses, which in general is a
rather involved procedure. As an alternative, Wijsman's theorem provides the
ratio of the maximal invariant densities without even finding an explicit
expression for the maximal invariant. We first consider the problem of testing
whether a set of $N$-dimensional Gaussian random vectors are uncorrelated or
not, and show that the LMPIT is given by the Frobenius norm of the sample
coherence matrix. Second, we study the case in which the vectors under the null
hypothesis are uncorrelated and identically distributed, that is, the
sphericity test for Gaussian vectors, for which we show that the LMPIT is given
by the Frobenius norm of a normalized version of the sample covariance matrix.
Finally, some numerical examples illustrate the performance of the proposed
tests, which provide better results than their GLRT counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5636</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5636</id><created>2012-04-25</created><updated>2013-04-16</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author></authors><title>Social Networks with Competing Products</title><categories>cs.SI physics.soc-ph</categories><comments>To appear in Fundamenta Informaticae. 32 pages. A preliminary version
  of this paper appeared as arXiv:1105.2434</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new threshold model of social networks, in which the nodes
influenced by their neighbours can adopt one out of several alternatives. We
characterize social networks for which adoption of a product by the whole
network is possible (respectively necessary) and the ones for which a unique
outcome is guaranteed. These characterizations directly yield polynomial time
algorithms that allow us to determine whether a given social network satisfies
one of the above properties.
  We also study algorithmic questions for networks without unique outcomes. We
show that the problem of determining whether a final network exists in which
all nodes adopted some product is NP-complete. In turn, the problems of
determining whether a given node adopts some (respectively, a given) product in
some (respectively, all) network(s) are either co-NP complete or can be solved
in polynomial time.
  Further, we show that the problem of computing the minimum possible spread of
a product is NP-hard to approximate with an approximation ratio better than
$\Omega(n)$, in contrast to the maximum spread, which is efficiently
computable. Finally, we clarify that some of the above problems can be solved
in polynomial time when there are only two products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5639</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5639</id><created>2012-04-25</created><authors><author><keyname>Kindermann</keyname><forenames>Roland</forenames></author><author><keyname>Junttila</keyname><forenames>Tommi</forenames></author><author><keyname>Niemel&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>SMT-based Induction Methods for Timed Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling time related aspects is important in many applications of
verification methods. For precise results, it is necessary to interpret time as
a dense domain, e.g. using timed automata as a formalism, even though the
system's resulting infinite state space is challenging for verification
methods. Furthermore, fully symbolic treatment of both timing related and
non-timing related elements of the state space seems to offer an attractive
approach to model checking timed systems with a large amount of
non-determinism. This paper presents an SMT-based timed system extension to the
IC3 algorithm, a SAT-based novel, highly efficient, complete verification
method for untimed systems. Handling of the infinite state spaces of timed
system in the extended IC3 algorithm is based on suitably adapting the
well-known region abstraction for timed systems. Additionally, $k$-induction,
another symbolic verification method for discrete time systems, is extended in
a similar fashion to support timed systems. Both new methods are evaluated and
experimentally compared to a booleanization-based verification approach that
uses the original discrete time IC3 algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5641</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5641</id><created>2012-04-25</created><authors><author><keyname>Dimitri</keyname><forenames>Papadimitriou</forenames><affiliation>UPC</affiliation></author><author><keyname>Albert</keyname><forenames>Cabellos</forenames><affiliation>UPC</affiliation></author></authors><title>Stability Analysis of Path-vector Routing</title><categories>cs.NI</categories><comments>14\`emes Rencontres Francophones sur les Aspects Algorithmiques des
  T\'el\'ecommunications (AlgoTel), La Grande Motte : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most studies on path-vector routing stability have been conducted empirically
by means of ad-hoc analysis of BGP data traces. None of them consider prior
specification of an analytic method including the use of stability measurement
metrics for the systematic analysis of BGP traces and associated
meta-processing for determining the local state of the routing system. In this
paper, we define a set of metrics that characterize the local stability
properties of path-vector routing such as BGP (Border Gateway Protocol). By
means of these stability metrics, we propose a method to analyze the effects of
BGP policy- and protocol-induced instability on local routers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5642</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5642</id><created>2012-04-25</created><authors><author><keyname>Dimitri</keyname><forenames>Papadimitriou</forenames><affiliation>UPC</affiliation></author><author><keyname>Albert</keyname><forenames>Cabellos</forenames><affiliation>UPC</affiliation></author></authors><title>Analysis of Path-vector Routing Stability</title><categories>cs.NI</categories><comments>14\`emes Rencontres Francophones sur les Aspects Algorithmiques des
  T\'el\'ecommunications (AlgoTel), La Grande Motte : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most studies on path-vector routing stability have been conducted empirically
by means of ad-hoc analysis of BGP data traces. None of them consider prior
specification of an analytic method including the use of stability measurement
metrics for the systematic analysis of BGP traces and associated
meta-processing for determining the local state of the routing system. In this
paper, we define a set of metrics that characterize the local stability
properties of path-vector routing such as BGP (Border Gateway Protocol). By
means of these stability metrics, we propose a method to analyze the effects of
BGP policy- and protocol-induced instability on local routers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5648</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5648</id><created>2012-04-25</created><authors><author><keyname>Bitar</keyname><forenames>Ibrahim El</forenames></author><author><keyname>Belouadha</keyname><forenames>Fatima Zahra</forenames></author><author><keyname>Roudies</keyname><forenames>Ounsa</forenames></author></authors><title>Taxonomy and synthesis of Web services querying languages</title><categories>cs.DB</categories><comments>7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most works on Web services has focused on discovery, composition and
selection processes of these kinds of services. Other few works were interested
in how to represent Web services search queries. However, these queries cannot
be processed by ensuring a high level of performance without being adequately
represented first. To this end, different query languages were designed. Even
so, in the absence of a standard, these languages are quite various. Their
diversity makes it difficult choosing the most suitable language. In fact, this
language should be able to cover all types of preferences or requirements of
clients such as their functional, nonfunctional,temporal or even specific
constraints as is the case of geographical or spatial constraints and meet
their needs and preferences helping to provide them the best answer. It must
also be mutually simple and imposes no restrictions or at least not too many
constraints in terms of prior knowledge to use and also provide a formal or
semi-formal queries presentation to support their automatic post-processing. A
comparative study is eventually established to allow to reveal the advantages
and limitations of various existing languages in this context. It is a
synthesis of this category of languages discussing their performance level and
their capability to respond to various needs related to the Web services
research and discovery case. The criterions identified at this stage may, in
our opinion, constitute then the main pre-requisite that a language should
satisfy to be called perfect or to be a future standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5652</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5652</id><created>2012-04-25</created><authors><author><keyname>Rajashekar</keyname><forenames>Rakshith</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>ML Decoding Complexity Reduction in STBCs Using Time-Orthogonal Pulse
  Shaping</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent developments in the Space Shift Keying (SSK) and
Spatial Modulation (SM) systems which employ Time-Orthogonal Pulse Shaping
(TOPS) filters to achieve transmit diversity gains, we propose TOPS for
Space-Time Block Codes (STBC). We show that any STBC whose set of weight
matrices partitions into P subsets under the equivalence relation termed as
Common Support Relation can be made P -group decodable by properly employing
TOPS waveforms across space and time. Furthermore, by considering some of the
well known STBCs in the literature we show that the order of their Maximum
Likelihood decoding complexity can be greatly reduced by the application of
TOPS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5653</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5653</id><created>2012-04-25</created><authors><author><keyname>Kuske</keyname><forenames>Dietrich</forenames></author></authors><title>Isomorphisms of scattered automatic linear orders</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the isomorphism of scattered tree automatic linear orders as
well as the existence of automorphisms of scattered word automatic linear
orders are undecidable. For the existence of automatic automorphisms of word
automatic linear orders, we determine the exact level of undecidability in the
arithmetical hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5661</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5661</id><created>2012-04-25</created><updated>2012-11-22</updated><authors><author><keyname>Maeno</keyname><forenames>Yoshiharu</forenames></author><author><keyname>Morinaga</keyname><forenames>Satoshi</forenames></author><author><keyname>Matsushima</keyname><forenames>Hirokazu</forenames></author><author><keyname>Amagai</keyname><forenames>Kenichi</forenames></author></authors><title>Transmission of distress in a bank credit network</title><categories>q-fin.RM cs.AI</categories><comments>presented at the 4th World Congress on Social Simulation, Taipei,
  September 2012</comments><journal-ref>presented at the 4th World Congress on Social Simulation, Taipei,
  September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The European sovereign debt crisis has impaired many European banks. The
distress on the European banks may transmit worldwide, and result in a
large-scale knock-on default of financial institutions. This study presents a
computer simulation model to analyze the risk of insolvency of banks and
defaults in a bank credit network. Simulation experiments reproduce the
knock-on default, and quantify the impact which is imposed on the number of
bank defaults by heterogeneity of the bank credit network, the equity capital
ratio of banks, and the capital surcharge on big banks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5662</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5662</id><created>2012-04-25</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>H&#xe5;stad</keyname><forenames>Johan</forenames></author></authors><title>On the Usefulness of Predicates</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the pervasiveness of strong inapproximability results for
Max-CSPs, we introduce a relaxed notion of an approximate solution of a
Max-CSP. In this relaxed version, loosely speaking, the algorithm is allowed to
replace the constraints of an instance by some other (possibly real-valued)
constraints, and then only needs to satisfy as many of the new constraints as
possible.
  To be more precise, we introduce the following notion of a predicate $P$
being \emph{useful} for a (real-valued) objective $Q$: given an almost
satisfiable Max-$P$ instance, there is an algorithm that beats a random
assignment on the corresponding Max-$Q$ instance applied to the same sets of
literals. The standard notion of a nontrivial approximation algorithm for a
Max-CSP with predicate $P$ is exactly the same as saying that $P$ is useful for
$P$ itself.
  We say that $P$ is useless if it is not useful for any $Q$. This turns out to
be equivalent to the following pseudo-randomness property: given an almost
satisfiable instance of Max-$P$ it is hard to find an assignment such that the
induced distribution on $k$-bit strings defined by the instance is not
essentially uniform.
  Under the Unique Games Conjecture, we give a complete and simple
characterization of useful Max-CSPs defined by a predicate: such a Max-CSP is
useless if and only if there is a pairwise independent distribution supported
on the satisfying assignments of the predicate. It is natural to also consider
the case when no negations are allowed in the CSP instance, and we derive a
similar complete characterization (under the UGC) there as well.
  Finally, we also include some results and examples shedding additional light
on the approximability of certain Max-CSPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5663</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5663</id><created>2012-04-25</created><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>Cognitive Interference Channels with Confidential Messages under
  Randomness Constraint</title><categories>cs.IT math.IT</categories><comments>10 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1201.6468</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cognitive interference channel with confidential messages (CICC) proposed
by Liang et. al. is investigated. When the security is considered in coding
systems, it is well known that the sender needs to use a stochastic encoding to
avoid the information about the transmitted confidential message to be leaked
to an eavesdropper. For the CICC, the trade-off between the rate of the random
number to realize the stochastic encoding and the communication rates is
investigated, and the optimal trade-off is completely characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5666</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5666</id><created>2012-04-25</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>A new point of NP-hardness for 2-to-1 Label Cover</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that given a satisfiable instance of the 2-to-1 Label Cover problem,
it is NP-hard to find a $(23/24 + \eps)$-satisfying assignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5677</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5677</id><created>2011-11-15</created><authors><author><keyname>Ferreira</keyname><forenames>C.</forenames></author><author><keyname>Monteiro</keyname><forenames>S.</forenames></author><author><keyname>Monteiro</keyname><forenames>J.</forenames></author></authors><title>Automatic Generation of C-code or PLD Circuits under SFC Graphical
  Environment</title><categories>cs.SY</categories><comments>IEEE International Symposium on Industrial Electronics (ISIE'97)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a framework for automatic development of control systems
from a high level specification based in Grafcet formalism. Grafcet, or
Sequential Function Charts (SFC), is a special class of Petri Nets and is
becoming the standard representation for sequential control systems. The
proposed framework accepts a graphical (through ISaGRAPH) or textual
behavioural specification of the control system to be implemented. It follows
the usual procedure in software specification: the first step is to formally
validate the initial specification. Then the initial specification is
translated through automated processes into an implementation. At the moment
there are two possible output languages: C and Palasm [1]. The target processor
for the C code language are microcontroller based systems that require extended
time constrains and access to external peripherals. The goal of including PLD's
is the possibility of automatically design mixed hardware and software systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5702</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5702</id><created>2012-04-25</created><updated>2014-12-04</updated><authors><author><keyname>Cameron</keyname><forenames>Kathie</forenames></author><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Ho&#xe0;ng</keyname><forenames>Ch&#xed;nh T.</forenames></author></authors><title>Edge Intersection Graphs of L-Shaped Paths in Grids</title><categories>cs.DM</categories><comments>14 pages, to appear in DAM special issue for LAGOS'13</comments><doi>10.1016/j.dam.2015.01.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we continue the study of the edge intersection graphs of one
(or zero) bend paths on a rectangular grid. That is, the edge intersection
graphs where each vertex is represented by one of the following shapes:
$\llcorner$,$\ulcorner$, $\urcorner$, $\lrcorner$, and we consider zero bend
paths (i.e., | and $-$) to be degenerate $\llcorner$s. These graphs, called
$B_1$-EPG graphs, were first introduced by Golumbic et al (2009). We consider
the natural subclasses of $B_1$-EPG formed by the subsets of the four single
bend shapes (i.e., {$\llcorner$}, {$\llcorner$,$\ulcorner$},
{$\llcorner$,$\urcorner$}, and {$\llcorner$,$\ulcorner$,$\urcorner$}) and we
denote the classes by [$\llcorner$], [$\llcorner$,$\ulcorner$],
[$\llcorner$,$\urcorner$], and [$\llcorner$,$\ulcorner$,$\urcorner$]
respectively. Note: all other subsets are isomorphic to these up to 90 degree
rotation. We show that testing for membership in each of these classes is
NP-complete and observe the expected strict inclusions and incomparability
(i.e., [$\llcorner$] $\subsetneq$ [$\llcorner$,$\ulcorner$],
[$\llcorner$,$\urcorner$] $\subsetneq$ [$\llcorner$,$\ulcorner$,$\urcorner$]
$\subsetneq$ $B_1$-EPG; also, [$\llcorner$,$\ulcorner$] is incomparable with
[$\llcorner$,$\urcorner$]). Additionally, we give characterizations and
polytime recognition algorithms for special subclasses of Split $\cap$
[$\llcorner$].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5703</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5703</id><created>2012-04-25</created><updated>2013-10-19</updated><authors><author><keyname>Yedla</keyname><forenames>Arvind</forenames></author><author><keyname>Jian</keyname><forenames>Yung-Yih</forenames></author><author><keyname>Nguyen</keyname><forenames>Phong S.</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>A Simple Proof of Threshold Saturation for Coupled Scalar Recursions</title><categories>cs.IT math.IT</categories><comments>In this update, there are a few small changes to Def. 5, Def. 6, and
  Remark 1. These changes avoid a pathological counterexample that is described
  in arXiv:1309.7910. The original version appears in the proceedings of ISTC
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LDPC) convolutional codes (or spatially-coupled
codes) have been shown to approach capacity on the binary erasure channel (BEC)
and binary-input memoryless symmetric channels. The mechanism behind this
spectacular performance is the threshold saturation phenomenon, which is
characterized by the belief-propagation threshold of the spatially-coupled
ensemble increasing to an intrinsic noise threshold defined by the uncoupled
system.
  In this paper, we present a simple proof of threshold saturation that applies
to a broad class of coupled scalar recursions. The conditions of the theorem
are verified for the density-evolution (DE) equations of irregular LDPC codes
on the BEC, a class of generalized LDPC codes, and the joint iterative decoding
of LDPC codes on intersymbol-interference channels with erasure noise. Our
approach is based on potential functions and was motivated mainly by the ideas
of Takeuchi et al. The resulting proof is surprisingly simple when compared to
previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5707</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5707</id><created>2012-04-25</created><authors><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Analysis of MMSE Estimation for Compressive Sensing of Block Sparse
  Signals</title><categories>cs.IT math.IT</categories><comments>6 pages, 1 figure. This is a corrected and updated version of the
  paper published in IEEE ITW'11, Paraty Brazil, October 16-20, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum mean square error (MMSE) estimation of block sparse signals from
noisy linear measurements is considered. Unlike in the standard compressive
sensing setup where the non-zero entries of the signal are independently and
uniformly distributed across the vector of interest, the information bearing
components appear here in large mutually dependent clusters. Using the replica
method from statistical physics, we derive a simple closed-form solution for
the MMSE obtained by the optimum estimator. We show that the MMSE is a version
of the Tse-Hanly formula with system load and MSE scaled by parameters that
depend on the sparsity pattern of the source. It turns out that this is equal
to the MSE obtained by a genie-aided MMSE estimator which is informed in
advance about the exact locations of the non-zero blocks. The asymptotic
results obtained by the non-rigorous replica method are found to have an
excellent agreement with finite sized numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5710</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5710</id><created>2012-04-25</created><authors><author><keyname>Courtade</keyname><forenames>Thomas</forenames></author></authors><title>Information Masking and Amplification: The Source Coding Setting</title><categories>cs.IT math.IT</categories><comments>6 pages, 1 figure, to appear at the IEEE 2012 International Symposium
  on Information Theory (ISIT 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complementary problems of masking and amplifying channel state
information in the Gel'fand-Pinsker channel have recently been solved by Merhav
and Shamai, and Kim et al., respectively. In this paper, we study a related
source coding problem. Specifically, we consider the two-encoder source coding
setting where one source is to be amplified, while the other source is to be
masked. In general, there is a tension between these two objectives which is
characterized by the amplification-masking tradeoff. In this paper, we give a
single-letter description of this tradeoff.
  We apply this result, together with a recent theorem by Courtade and Weissman
on multiterminal source coding, to solve a fundamental entropy characterization
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5714</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5714</id><created>2012-04-25</created><authors><author><keyname>McQuillan</keyname><forenames>Colin</forenames></author></authors><title>Degree two approximate Boolean #CSPs with variable weights</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A counting constraint satisfaction problem (#CSP) asks for the number of ways
to satisfy a given list of constraints, drawn from a fixed constraint language
\Gamma. We study how hard it is to evaluate this number approximately. There is
an interesting partial classification, due to Dyer, Goldberg, Jalsenius and
Richerby, of Boolean constraint languages when the degree of instances is
bounded by d&gt;=3 - every variable appears in at most d constraints - under the
assumption that &quot;pinning&quot; is allowed as part of the instance. We study the d=2
case under the stronger assumption that &quot;variable weights&quot; are allowed as part
of the instance. We give a dichotomy: in each case, either the #CSP is
tractable, or one of two important open problems, #BIS or #PM, reduces to the
#CSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5717</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5717</id><created>2012-04-25</created><updated>2013-01-06</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>LaValle</keyname><forenames>Steven M.</forenames></author></authors><title>Multi-agent Path Planning and Network Flow</title><categories>cs.DS cs.RO cs.SY</categories><comments>Corrected an inaccuracy on time optimal solution for average arrival
  time</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper connects multi-agent path planning on graphs (roadmaps) to network
flow problems, showing that the former can be reduced to the latter, therefore
enabling the application of combinatorial network flow algorithms, as well as
general linear program techniques, to multi-agent path planning problems on
graphs. Exploiting this connection, we show that when the goals are permutation
invariant, the problem always has a feasible solution path set with a longest
finish time of no more than $n + V - 1$ steps, in which $n$ is the number of
agents and $V$ is the number of vertices of the underlying graph. We then give
a complete algorithm that finds such a solution in $O(nVE)$ time, with $E$
being the number of edges of the graph. Taking a further step, we study time
and distance optimality of the feasible solutions, show that they have a
pairwise Pareto optimal structure, and again provide efficient algorithms for
optimizing two of these practical objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5721</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5721</id><created>2012-04-25</created><updated>2012-11-03</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author></authors><title>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
  Problems</title><categories>cs.LG stat.ML</categories><comments>To appear in Foundations and Trends in Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-armed bandit problems are the most basic examples of sequential
decision problems with an exploration-exploitation trade-off. This is the
balance between staying with the option that gave highest payoffs in the past
and exploring new options that might give higher payoffs in the future.
Although the study of bandit problems dates back to the Thirties,
exploration-exploitation trade-offs arise in several modern applications, such
as ad placement, website optimization, and packet routing. Mathematically, a
multi-armed bandit is defined by the payoff process associated with each
option. In this survey, we focus on two extreme cases in which the analysis of
regret is particularly simple and elegant: i.i.d. payoffs and adversarial
payoffs. Besides the basic setting of finitely many actions, we also analyze
some of the most important variants and extensions, such as the contextual
bandit model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5780</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5780</id><created>2012-04-25</created><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Onkar</forenames></author><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author></authors><title>Friendship, Altruism, and Reward Sharing in Stable Matching and
  Contribution Games</title><categories>cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study stable matching problems in networks where players are embedded in a
social context, and may incorporate friendship relations or altruism into their
decisions. Each player is a node in a social network and strives to form a good
match with a neighboring player. We consider the existence, computation, and
inefficiency of stable matchings from which no pair of players wants to
deviate. When the benefits from a match are the same for both players, we show
that incorporating the well-being of other players into their matching
decisions significantly decreases the price of stability, while the price of
anarchy remains unaffected. Furthermore, a good stable matching achieving the
price of stability bound always exists and can be reached in polynomial time.
We extend these results to more general matching rewards, when players matched
to each other may receive different utilities from the match. For this more
general case, we show that incorporating social context (i.e., &quot;caring about
your friends&quot;) can make an even larger difference, and greatly reduce the price
of anarchy. We show a variety of existence results, and present upper and lower
bounds on the prices of anarchy and stability for various matching utility
structures. Finally, we extend most of our results to network contribution
games, in which players can decide how much effort to contribute to each
incident edge, instead of simply choosing a single node to match with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5791</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5791</id><created>2012-04-25</created><authors><author><keyname>Widanapathirana</keyname><forenames>Chathuranga</forenames></author><author><keyname>Sekercioglu</keyname><forenames>Y. Ahmet</forenames></author><author><keyname>Goi</keyname><forenames>Bok-Min</forenames></author></authors><title>Hybrid FPMS: A New Fairness Protocol Management Scheme for Community
  Wireless Mesh Networks</title><categories>cs.NI</categories><comments>KSII Transactions on Internet and Information Systems, 2011</comments><doi>10.3837/tiis.2011.11.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node cooperation during packet forwarding operations is critically important
for fair resource utilization in Community Wireless Mesh Networks (CoWMNs). In
a CoWMN, node cooperation is achieved by using fairness protocols specifically
designed to detect and isolate malicious nodes, discourage unfair behavior, and
encourage node participation in forwarding packets. In general, these protocols
can be split into two groups: Incentive-based ones, which are managed
centrally, and use credit allocation schemes. In contrast, reputation-based
protocols that are decentralized, and rely on information exchange among
neighboring nodes. Centrally managed protocols inevitably suffer from
scalability problems. The decentralized, reputation-based protocols lacks in
detection capability, suffer from false detections and error propagation
compared to the centralized, incentive-based protocols. In this study, we
present a new fairness protocol management scheme, called Hybrid FPMS that
captures the superior detection capability of incentive-based fairness
protocols without the scalability problems inherently expected from a
centralized management scheme as a network's size and density grows. Simulation
results show that Hybrid FPMS is more efficient than the current centralized
approach and significantly reduces the network delays and overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5796</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5796</id><created>2012-04-25</created><authors><author><keyname>Bryans</keyname><forenames>Jeremy</forenames><affiliation>Newcastle University</affiliation></author><author><keyname>Fitzgerald</keyname><forenames>John</forenames><affiliation>Newcastle University</affiliation></author></authors><title>Proceedings Third Workshop on Formal Aspects of Virtual Organisations</title><categories>cs.MA</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 83, 2012</journal-ref><doi>10.4204/EPTCS.83</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 3rd International Workshop on
Formal Aspects of Virtual Organisations (FAVO 2011). The workshop was held in
Sao Paulo, Brazil on October 18th, 2011 as a satellite event to the 12th IFIP
Working Conference on Virtual Enterprises (PRO-VE'11). The FAVO workshop aims
to provide a forum for researchers interested in the application of formal
techniques in the design and analysis of Virtual Organisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5801</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5801</id><created>2012-04-25</created><updated>2014-02-13</updated><authors><author><keyname>Barbay</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author></authors><title>Optimal Prefix Free Code in Linear Time</title><categories>cs.DS</categories><comments>The algorithm TopDown is incorrect, and it is not clear how to
  correct it</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm computing an optimal prefix free code from $N$
unsorted positive integer weights in time linear in the number of machine words
holding those weights. This algorithm takes advantage of common non-algebraic
instructions, and of specific results on optimal prefix free codes. This result
improves over the state of the art complexities of $O(N\lg N)$ in the algebraic
decision tree model and $O(N\lg\lg N)$ in the RAM model for the computation of
Huffman's codes, a landmark in compression and coding since 1952.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5802</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5802</id><created>2012-04-25</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Quantitative Concept Analysis</title><categories>cs.LG math.CT</categories><comments>16 pages, 3 figures, ICFCA 2012</comments><msc-class>18D20, 06B23</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal Concept Analysis (FCA) begins from a context, given as a binary
relation between some objects and some attributes, and derives a lattice of
concepts, where each concept is given as a set of objects and a set of
attributes, such that the first set consists of all objects that satisfy all
attributes in the second, and vice versa. Many applications, though, provide
contexts with quantitative information, telling not just whether an object
satisfies an attribute, but also quantifying this satisfaction. Contexts in
this form arise as rating matrices in recommender systems, as occurrence
matrices in text analysis, as pixel intensity matrices in digital image
processing, etc. Such applications have attracted a lot of attention, and
several numeric extensions of FCA have been proposed. We propose the framework
of proximity sets (proxets), which subsume partially ordered sets (posets) as
well as metric spaces. One feature of this approach is that it extracts from
quantified contexts quantified concepts, and thus allows full use of the
available information. Another feature is that the categorical approach allows
analyzing any universal properties that the classical FCA and the new versions
may have, and thus provides structural guidance for aligning and combining the
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5805</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5805</id><created>2012-04-25</created><authors><author><keyname>Widanapathirana</keyname><forenames>C.</forenames></author><author><keyname>Li</keyname><forenames>J.</forenames></author><author><keyname>Sekercioglu</keyname><forenames>Y. A.</forenames></author><author><keyname>Ivanovich</keyname><forenames>M.</forenames></author><author><keyname>Fitzpatrick</keyname><forenames>P.</forenames></author></authors><title>Intelligent Automated Diagnosis of Client Device Bottlenecks in Private
  Clouds</title><categories>cs.NI cs.AI</categories><comments>2011 Fourth IEEE International Conference on Utility and Cloud
  Computing (UCC)</comments><journal-ref>Fourth IEEE International Conference on Utility and Cloud
  Computing (UCC), 2011</journal-ref><doi>10.1109/UCC.2011.42</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an automated solution for rapid diagnosis of client device
problems in private cloud environments: the Intelligent Automated Client
Diagnostic (IACD) system. Clients are diagnosed with the aid of Transmission
Control Protocol (TCP) packet traces, by (i) observation of anomalous artifacts
occurring as a result of each fault and (ii) subsequent use of the inference
capabilities of soft-margin Support Vector Machine (SVM) classifiers. The IACD
system features a modular design and is extendible to new faults, with
detection capability unaffected by the TCP variant used at the client.
Experimental evaluation of the IACD system in a controlled environment
demonstrated an overall diagnostic accuracy of 98%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5810</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5810</id><created>2012-04-25</created><authors><author><keyname>Molinaro</keyname><forenames>Marco</forenames></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author></authors><title>Geometry of Online Packing Linear Programs</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider packing LP's with $m$ rows where all constraint coefficients are
normalized to be in the unit interval. The n columns arrive in random order and
the goal is to set the corresponding decision variables irrevocably when they
arrive so as to obtain a feasible solution maximizing the expected reward.
Previous (1 - \epsilon)-competitive algorithms require the right-hand side of
the LP to be Omega((m/\epsilon^2) log (n/\epsilon)), a bound that worsens with
the number of columns and rows. However, the dependence on the number of
columns is not required in the single-row case and known lower bounds for the
general case are also independent of n.
  Our goal is to understand whether the dependence on n is required in the
multi-row case, making it fundamentally harder than the single-row version. We
refute this by exhibiting an algorithm which is (1 - \epsilon)-competitive as
long as the right-hand sides are Omega((m^2/\epsilon^2) log (m/\epsilon)). Our
techniques refine previous PAC-learning based approaches which interpret the
online decisions as linear classifications of the columns based on sampled dual
prices. The key ingredient of our improvement comes from a non-standard
covering argument together with the realization that only when the columns of
the LP belong to few 1-d subspaces we can obtain small such covers; bounding
the size of the cover constructed also relies on the geometry of linear
classifiers. General packing LP's are handled by perturbing the input columns,
which can be seen as making the learning problem more robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5823</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5823</id><created>2012-04-25</created><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Umboh</keyname><forenames>Seeun</forenames></author></authors><title>A Bicriteria Approximation for the Reordering Buffer Problem</title><categories>cs.DS</categories><comments>13 pages</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the reordering buffer problem (RBP), a server is asked to process a
sequence of requests lying in a metric space. To process a request the server
must move to the corresponding point in the metric. The requests can be
processed slightly out of order; in particular, the server has a buffer of
capacity k which can store up to k requests as it reads in the sequence. The
goal is to reorder the requests in such a manner that the buffer constraint is
satisfied and the total travel cost of the server is minimized. The RBP arises
in many applications that require scheduling with a limited buffer capacity,
such as scheduling a disk arm in storage systems, switching colors in paint
shops of a car manufacturing plant, and rendering 3D images in computer
graphics.
  We study the offline version of RBP and develop bicriteria approximations.
When the underlying metric is a tree, we obtain a solution of cost no more than
9OPT using a buffer of capacity 4k + 1 where OPT is the cost of an optimal
solution with buffer capacity k. Constant factor approximations were known
previously only for the uniform metric (Avigdor-Elgrabli et al., 2012). Via
randomized tree embeddings, this implies an O(log n) approximation to cost and
O(1) approximation to buffer size for general metrics. Previously the best
known algorithm for arbitrary metrics by Englert et al. (2007) provided an
O(log^2 k log n) approximation without violating the buffer constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5828</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5828</id><created>2012-04-26</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author></authors><title>The traveling salesman problem for lines and rays in the plane</title><categories>cs.CG math.MG</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of
$n$ regions (neighborhoods) and we seek a shortest tour that visits each
region. In the path variant, we seek a shortest path that visits each region.
We present several linear-time approximation algorithms with improved ratios
for these problems for two cases of neighborhoods that are (infinite) lines,
and respectively, (half-infinite) rays. Along the way we derive a tight bound
on the minimum perimeter of a rectangle enclosing an open curve of length $L$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5834</identifier>
 <datestamp>2012-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5834</id><created>2012-04-26</created><updated>2012-07-12</updated><authors><author><keyname>Blanca</keyname><forenames>Antonio</forenames></author><author><keyname>Mihail</keyname><forenames>Milena</forenames></author></authors><title>Efficient Generation \epsilon-close to G(n,p) and Generalizations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an efficient algorithm to generate a graph from a distribution
$\epsilon$-close to $G(n,p)$, in the sense of total variation distance. In
particular, if $p$ is represented with $O(\log n)$-bit accuracy, then, with
high probability, the running time is linear in the expected number of edges of
the output graph (up to poly-logarithmic factors). All our running times
include the complexity of the arithmetic involved in the corresponding
algorithms. Previous standard methods for exact $G(n,p)$ sampling (see e.g.
Batagelj and Brandes, 2005) achieve similar running times, however, under the
assumption that performing real number arithmetic with arbitrary accuracy takes
constant time. We note that the actual accuracy required by these methods is
O(n)-bit per step, which results in quadratic running times.
  The main idea of our $G(n,p)$ generation algorithm is a Metropolis Markov
chain to sample $\epsilon$-close from the binomial distribution. This is a new
method for sampling from the binomial distribution: it is of separate interest
and may find other useful applications. Our analysis accounts for all necessary
bit-accuracy and arithmetic, and our running times are comparable to known
methods for exact binomial sampling.
  We further obtain efficient generation algorithms for random graphs with
given arbitrary degree distributions, Inhomogeneous Random Graphs when the
kernel function is the inner product, and Stochastic Kronecker Graphs. To the
best our knowledge, our work can be viewed as the first effort to simulate
efficient generation of graphs from classical random graph models, while taking
into account implementational considerations as fundamental computational
aspects, and quantifying the tradeoff between accuracy and running time in a
way that can be useful in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5839</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5839</id><created>2012-04-26</created><authors><author><keyname>Yadav</keyname><forenames>Shrikrishan</forenames></author><author><keyname>Jani</keyname><forenames>Shuchi</forenames></author><author><keyname>Pal</keyname><forenames>B. L.</forenames></author></authors><title>Analysis of Various Symbol Detection Techniques in Multiple-Input
  Multiple-Output System (MIMO)</title><categories>cs.NI</categories><comments>11 pages,9 figures</comments><journal-ref>Advanced Computing: An International Journal ( ACIJ ), Vol.3,
  No.2, March 2012</journal-ref><doi>10.5121/acij.2012.3211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication is the fastest growing area of the communication
industry. To keep swiftness with the indefinite increase in customers' demands
and expectations, and the market competition among companies for the services
offered,there is need for higher data rate along with reliable communication at
low cost so that the applications can reach all. Until now, many technical
challenges remain in designing robust and fast wireless systems that deliver
the performance necessary to support emerging applications, due to the fact
that wireless channels are frequency selective, power-limited, susceptible to
noise and interference. Demand for high data rate and increasing applications
offered by a wireless device calls for an effective method. Due to limit on the
available bandwidth, there is a need for exploiting the available bandwidth in
a way so that we get maximum advantage. Multiple-Input Multiple-Output system
does exactly this thing by multiplying the data rate without any expansion in
the bandwidth. This system utilizes the spatial diversity property of the multi
channel system. The reliable transmission requires symbols to be effectively
recovered at the receiving end. V-BLAST detection technique is employed for
this purpose. This paper depicted the advantages of using multiple antennas by
exploiting signal diversity offered by multipath effect and the system offers
high spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5852</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5852</id><created>2012-04-26</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Alwani</keyname><forenames>Mohammad</forenames></author></authors><title>Context-sensitive Spelling Correction Using Google Web 1T 5-Gram
  Information</title><categories>cs.CL</categories><comments>LACSC - Lebanese Association for Computational Sciences -
  http://www.lacsc.org</comments><journal-ref>Computer and Information Science, Vol. 5, No. 3, May 2012</journal-ref><doi>10.5539/cis.v5n3p37</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computing, spell checking is the process of detecting and sometimes
providing spelling suggestions for incorrectly spelled words in a text.
Basically, a spell checker is a computer program that uses a dictionary of
words to perform spell checking. The bigger the dictionary is, the higher is
the error detection rate. The fact that spell checkers are based on regular
dictionaries, they suffer from data sparseness problem as they cannot capture
large vocabulary of words including proper names, domain-specific terms,
technical jargons, special acronyms, and terminologies. As a result, they
exhibit low error detection rate and often fail to catch major errors in the
text. This paper proposes a new context-sensitive spelling correction method
for detecting and correcting non-word and real-word errors in digital text
documents. The approach hinges around data statistics from Google Web 1T 5-gram
data set which consists of a big volume of n-gram word sequences, extracted
from the World Wide Web. Fundamentally, the proposed method comprises an error
detector that detects misspellings, a candidate spellings generator based on a
character 2-gram model that generates correction suggestions, and an error
corrector that performs contextual error correction. Experiments conducted on a
set of text documents from different domains and containing misspellings,
showed an outstanding spelling error correction rate and a drastic reduction of
both non-word and real-word errors. In a further study, the proposed algorithm
is to be parallelized so as to lower the computational cost of the error
detection and correction processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5853</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5853</id><created>2012-04-26</created><updated>2015-06-19</updated><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Simultaneous Embedding of Planar Graphs</title><categories>cs.DS cs.DM</categories><comments>survey, 35 pages, 12 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous embedding is concerned with simultaneously representing a series
of graphs sharing some or all vertices. This forms the basis for the
visualization of dynamic graphs and thus is an important field of research.
Recently there has been a great deal of work investigating simultaneous
embedding problems both from a theoretical and a practical point of view. We
survey recent work on this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5859</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5859</id><created>2012-04-26</created><updated>2015-05-14</updated><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author><author><keyname>Schaerf</keyname><forenames>Marco</forenames></author></authors><title>On the Complexity of Finding Second-Best Abductive Explanations</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While looking for abductive explanations of a given set of manifestations, an
ordering between possible solutions is often assumed. The complexity of
finding/verifying optimal solutions is already known. In this paper we consider
the computational complexity of finding second-best solutions. We consider
different orderings, and consider also different possible definitions of what a
second-best solution is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5887</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5887</id><created>2012-04-26</created><authors><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Numeration Systems: a Link between Number Theory and Formal Language
  Theory</title><categories>cs.FL cs.DM</categories><comments>21 pages, 3 figures, invited talk DLT'2010</comments><msc-class>68Q45, 68R15, 11U99, 91A46</msc-class><journal-ref>Proceedings of Developments in Language Theory, London, Ontario,
  Canada (2010), Lect. Notes in Comput. Sci. 6224, 33-53 Springer-Verlag (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey facts mostly emerging from the seminal results of Alan Cobham
obtained in the late sixties and early seventies. We do not attempt to be
exhaustive but try instead to give some personal interpretations and some
research directions. We discuss the notion of numeration systems, recognizable
sets of integers and automatic sequences. We briefly sketch some results about
transcendence related to the representation of real numbers. We conclude with
some applications to combinatorial game theory and verification of
infinite-state systems and present a list of open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5920</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5920</id><created>2012-04-26</created><authors><author><keyname>Benzmueller</keyname><forenames>Christoph</forenames></author><author><keyname>Genovese</keyname><forenames>Valerio</forenames></author></authors><title>Quantified Conditional Logics are Fragments of HOL</title><categories>cs.AI cs.LO</categories><comments>This work has been presented at the conference on Non-classical Modal
  and Predicate Logics 2011, Guangzhou (Canton), China, 5-9 December 2011</comments><msc-class>03B60, 03B15, 68T27, 68T30, 68T15</msc-class><acm-class>I.2.3; I.2.4; I.2.0; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semantic embedding of (constant domain) quantified conditional logic in
classical higher-order logic is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5929</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5929</id><created>2012-04-26</created><authors><author><keyname>Luccio</keyname><forenames>Fabrizio</forenames></author><author><keyname>Pagli</keyname><forenames>Linda</forenames></author></authors><title>Chain Rotations: a New Look at Tree Distance</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As well known the rotation distance D(S,T) between two binary trees S, T of n
vertices is the minimum number of rotations of pairs of vertices to transform S
into T. We introduce the new operation of chain rotation on a tree, involving
two chains of vertices, that requires changing exactly three pointers in the
data structure as for a standard rotation, and define the corresponding chain
distance C(S,T). As for D(S,T), no polynomial time algorithm to compute C(S,T)
is known. We prove a constructive upper bound and an analytical lower bound on
C(S,T) based on the number of maximal chains in the two trees. In terms of n we
prove the general upper bound C(S,T)&lt;= n-1 and we show that there are pairs of
trees for which this bound is tight. No similar result is known for D(S,T)
where the best upper and lower bounds are 2n-6 and 5n/3-4 respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5952</identifier>
 <datestamp>2012-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5952</id><created>2012-04-26</created><authors><author><keyname>Albeverio</keyname><forenames>S.</forenames></author><author><keyname>Kozyrev</keyname><forenames>S. V.</forenames></author></authors><title>Clustering by hypergraphs and dimensionality of cluster systems</title><categories>cs.DS q-bio.PE</categories><comments>15 pages</comments><journal-ref>p-Adic Numbers, Ultrametric Analysis and Applications, 4 (2012)
  no. 3, 167--178</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we discuss the clustering procedure in the case where
instead of a single metric we have a family of metrics. In this case we can
obtain a partially ordered graph of clusters which is not necessarily a tree.
We discuss a structure of a hypergraph above this graph. We propose two
definitions of dimension for hyperedges of this hypergraph and show that for
the multidimensional p-adic case both dimensions are reduced to the number of
p-adic parameters.
  We discuss the application of the hypergraph clustering procedure to the
construction of phylogenetic graphs in biology. In this case the dimension of a
hyperedge will describe the number of sources of genetic diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5958</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5958</id><created>2012-04-26</created><authors><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Sparse Signal Processing with Frame Theory</title><categories>math.FA cs.IT math.IT</categories><comments>PhD thesis</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Many emerging applications involve sparse signals, and their processing is a
subject of active research. We desire a large class of sensing matrices which
allow the user to discern important properties of the measured sparse signal.
Of particular interest are matrices with the restricted isometry property
(RIP). RIP matrices are known to enable efficient and stable reconstruction of
sufficiently sparse signals, but the deterministic construction of such
matrices has proven very difficult. In this thesis, we discuss this matrix
design problem in the context of a growing field of study known as frame
theory. In the first two chapters, we build large families of equiangular tight
frames and full spark frames, and we discuss their relationship to RIP matrices
as well as their utility in other aspects of sparse signal processing. In
Chapter 3, we pave the road to deterministic RIP matrices, evaluating various
techniques to demonstrate RIP, and making interesting connections with graph
theory and number theory. We conclude in Chapter 4 with a coherence-based
alternative to RIP, which provides near-optimal probabilistic guarantees for
various aspects of sparse signal processing while at the same time admitting a
whole host of deterministic constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5963</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5963</id><created>2012-04-26</created><updated>2013-06-26</updated><authors><author><keyname>Carvalho</keyname><forenames>Arthur</forenames></author><author><keyname>Larson</keyname><forenames>Kate</forenames></author></authors><title>On a Reliable Peer-Review Process</title><categories>cs.GT stat.AP stat.OT</categories><comments>This paper has been withdrawn by the author due to some errors in the
  basic model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an enhanced peer-review process where the reviewers are encouraged
to truthfully disclose their reviews. We start by modelling that process using
a Bayesian model where the uncertainty regarding the quality of the manuscript
is taken into account. After that, we introduce a scoring function to evaluate
the reported reviews. Under mild assumptions, we show that reviewers strictly
maximize their expected scores by telling the truth. We also show how those
scores can be used in order to reach consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.5981</identifier>
 <datestamp>2012-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.5981</id><created>2012-04-26</created><authors><author><keyname>Madelaine</keyname><forenames>Florent</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>Containment, Equivalence and Coreness from CSP to QCSP and beyond</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The constraint satisfaction problem (CSP) and its quantified extensions,
whether without (QCSP) or with disjunction (QCSP_or), correspond naturally to
the model checking problem for three increasingly stronger fragments of
positive first-order logic. Their complexity is often studied when
parameterised by a fixed model, the so-called template.
  It is a natural question to ask when two templates are equivalent, or more
generally when one &quot;contain&quot; another, in the sense that a satisfied instance of
the first will be necessarily satisfied in the second. One can also ask for a
smallest possible equivalent template: this is known as the core for CSP.
  We recall and extend previous results on containment, equivalence and
&quot;coreness&quot; for QCSP_or before initiating a preliminary study of cores for QCSP
which we characterise for certain structures and which turns out to be more
elusive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6049</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6049</id><created>2012-04-26</created><updated>2014-03-21</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Channel Capacity under Sub-Nyquist Nonuniform Sampling</title><categories>cs.IT math.IT</categories><comments>accepted to IEEE Transactions on Information Theory, 2014</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 60, No. 8, pp.
  4739-4756, August 2014</journal-ref><doi>10.1109/TIT.2014.2323406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the effect of sub-Nyquist sampling upon the capacity
of an analog channel. The channel is assumed to be a linear time-invariant
Gaussian channel, where perfect channel knowledge is available at both the
transmitter and the receiver. We consider a general class of right-invertible
time-preserving sampling methods which include irregular nonuniform sampling,
and characterize in closed form the channel capacity achievable by this class
of sampling methods, under a sampling rate and power constraint. Our results
indicate that the optimal sampling structures extract out the set of
frequencies that exhibits the highest signal-to-noise ratio among all spectral
sets of measure equal to the sampling rate. This can be attained through
filterbank sampling with uniform sampling at each branch with possibly
different rates, or through a single branch of modulation and filtering
followed by uniform sampling. These results reveal that for a large class of
channels, employing irregular nonuniform sampling sets, while typically
complicated to realize, does not provide capacity gain over uniform sampling
sets with appropriate preprocessing. Our findings demonstrate that aliasing or
scrambling of spectral components does not provide capacity gain, which is in
contrast to the benefits obtained from random mixing in spectrum-blind
compressive sampling schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6070</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6070</id><created>2012-04-26</created><updated>2015-06-24</updated><authors><author><keyname>Abello</keyname><forenames>James</forenames></author><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Vysko&#x10d;il</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>MSOL Restricted Contractibility to Planar Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of graph planarization via edge
contraction. The problem CONTRACT asks whether there exists a set $S$ of at
most $k$ edges that when contracted produces a planar graph. We work with a
more general problem called $P$-RESTRICTEDCONTRACT in which $S$, in addition,
is required to satisfy a fixed MSOL formula $P(S,G)$. We give an FPT algorithm
in time $O(n^2 f(k))$ which solves $P$-RESTRICTEDCONTRACT, where $P(S,G)$ is
(i) inclusion-closed and (ii) inert contraction-closed (where inert edges are
the edges non-incident to any inclusion minimal solution $S$).
  As a specific example, we can solve the $\ell$-subgraph contractibility
problem in which the edges of a set $S$ are required to form disjoint connected
subgraphs of size at most $\ell$. This problem can be solved in time $O(n^2
f'(k,\ell))$ using the general algorithm. We also show that for $\ell \ge 2$
the problem is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6076</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6076</id><created>2012-04-26</created><authors><author><keyname>Mouratidis</keyname><forenames>Kyriakos</forenames></author><author><keyname>Yiu</keyname><forenames>Man Lung</forenames></author></authors><title>Shortest Path Computation with No Information Leakage</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  692-703 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shortest path computation is one of the most common queries in location-based
services (LBSs). Although particularly useful, such queries raise serious
privacy concerns. Exposing to a (potentially untrusted) LBS the client's
position and her destination may reveal personal information, such as social
habits, health condition, shopping preferences, lifestyle choices, etc. The
only existing method for privacy-preserving shortest path computation follows
the obfuscation paradigm; it prevents the LBS from inferring the source and
destination of the query with a probability higher than a threshold. This
implies, however, that the LBS still deduces some information (albeit not
exact) about the client's location and her destination. In this paper we aim at
strong privacy, where the adversary learns nothing about the shortest path
query. We achieve this via established private information retrieval
techniques, which we treat as black-box building blocks. Experiments on real,
large-scale road networks assess the practicality of our schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6077</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6077</id><created>2012-04-26</created><authors><author><keyname>Metwally</keyname><forenames>Ahmed</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity
  Joins of Multisets and Vectors</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  704-715 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes V-SMART-Join, a scalable MapReduce-based framework for
discovering all pairs of similar entities. The V-SMART-Join framework is
applicable to sets, multisets, and vectors. V-SMART-Join is motivated by the
observed skew in the underlying distributions of Internet traffic, and is a
family of 2-stage algorithms, where the first stage computes and joins the
partial results, and the second stage computes the similarity exactly for all
candidate pairs. The V-SMART-Join algorithms are very efficient and scalable in
the number of entities, as well as their cardinalities. They were up to 30
times faster than the state of the art algorithm, VCL, when compared on a real
dataset of a small size. We also established the scalability of the proposed
algorithms by running them on a dataset of a realistic size, on which VCL never
succeeded to finish. Experiments were run using real datasets of IPs and
cookies, where each IP is represented as a multiset of cookies, and the goal is
to discover similar IPs to identify Internet proxies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6078</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6078</id><created>2012-04-26</created><authors><author><keyname>Low</keyname><forenames>Yucheng</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph</forenames></author><author><keyname>Kyrola</keyname><forenames>Aapo</forenames></author><author><keyname>Bickson</keyname><forenames>Danny</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author></authors><title>Distributed GraphLab: A Framework for Machine Learning in the Cloud</title><categories>cs.DB cs.LG</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  716-727 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While high-level data parallel frameworks, like MapReduce, simplify the
design and implementation of large-scale data processing systems, they do not
naturally or efficiently support many important data mining and machine
learning algorithms and can lead to inefficient learning systems. To help fill
this critical void, we introduced the GraphLab abstraction which naturally
expresses asynchronous, dynamic, graph-parallel computation while ensuring data
consistency and achieving a high degree of parallel performance in the
shared-memory setting. In this paper, we extend the GraphLab framework to the
substantially more challenging distributed setting while preserving strong data
consistency guarantees. We develop graph based extensions to pipelined locking
and data versioning to reduce network congestion and mitigate the effect of
network latency. We also introduce fault tolerance to the GraphLab abstraction
using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can
be easily implemented by exploiting the GraphLab abstraction itself. Finally,
we evaluate our distributed implementation of the GraphLab abstraction on a
large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains
over Hadoop-based implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6079</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6079</id><created>2012-04-26</created><authors><author><keyname>Singh</keyname><forenames>Rishabh</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author></authors><title>Learning Semantic String Transformations from Examples</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  740-751 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of performing semantic transformations on strings,
which may represent a variety of data types (or their combination) such as a
column in a relational table, time, date, currency, etc. Unlike syntactic
transformations, which are based on regular expressions and which interpret a
string as a sequence of characters, semantic transformations additionally
require exploiting the semantics of the data type represented by the string,
which may be encoded as a database of relational tables. Manually performing
such transformations on a large collection of strings is error prone and
cumbersome, while programmatic solutions are beyond the skill-set of end-users.
We present a programming by example technology that allows end-users to
automate such repetitive tasks. We describe an expressive transformation
language for semantic manipulation that combines table lookup operations and
syntactic manipulations. We then present a synthesis algorithm that can learn
all transformations in the language that are consistent with the user-provided
set of input-output examples. We have implemented this technology as an add-in
for the Microsoft Excel Spreadsheet system and have evaluated it successfully
over several benchmarks picked from various Excel help-forums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6080</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6080</id><created>2012-04-26</created><authors><author><keyname>Liu</keyname><forenames>Changbin</forenames></author><author><keyname>Ren</keyname><forenames>Lu</forenames></author><author><keyname>Loo</keyname><forenames>Boon Thau</forenames></author><author><keyname>Mao</keyname><forenames>Yun</forenames></author><author><keyname>Basu</keyname><forenames>Prithwish</forenames></author></authors><title>Cologne: A Declarative Distributed Constraint Optimization Platform</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  752-763 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Cologne, a declarative optimization platform that enables
constraint optimization problems (COPs) to be declaratively specified and
incrementally executed in distributed systems. Cologne integrates a declarative
networking engine with an off-the-shelf constraint solver. We have developed
the Colog language that combines distributed Datalog used in declarative
networking with language constructs for specifying goals and constraints used
in COPs. Cologne uses novel query processing strategies for processing Colog
programs, by combining the use of bottom-up distributed Datalog evaluation with
top-down goal-oriented constraint solving. Using case studies based on cloud
and wireless network optimizations, we demonstrate that Cologne (1) can
flexibly support a wide range of policy-based optimizations in distributed
systems, (2) results in orders of magnitude less code compared to imperative
implementations, and (3) is highly efficient with low overhead and fast
convergence times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6081</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6081</id><created>2012-04-26</created><authors><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Yang</keyname><forenames>Jun</forenames></author></authors><title>Optimizing I/O for Big Array Analytics</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  764-775 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big array analytics is becoming indispensable in answering important
scientific and business questions. Most analysis tasks consist of multiple
steps, each making one or multiple passes over the arrays to be analyzed and
generating intermediate results. In the big data setting, I/O optimization is a
key to efficient analytics. In this paper, we develop a framework and
techniques for capturing a broad range of analysis tasks expressible in
nested-loop forms, representing them in a declarative way, and optimizing their
I/O by identifying sharing opportunities. Experiment results show that our
optimizer is capable of finding execution plans that exploit nontrivial I/O
sharing opportunities with significant savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6082</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6082</id><created>2012-04-26</created><authors><author><keyname>Bailis</keyname><forenames>Peter</forenames></author><author><keyname>Venkataraman</keyname><forenames>Shivaram</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>Probabilistically Bounded Staleness for Practical Partial Quorums</title><categories>cs.DB cs.DC</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.
  776-787 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data store replication results in a fundamental trade-off between operation
latency and data consistency. In this paper, we examine this trade-off in the
context of quorum-replicated data stores. Under partial, or non-strict quorum
replication, a data store waits for responses from a subset of replicas before
answering a query, without guaranteeing that read and write replica sets
intersect. As deployed in practice, these configurations provide only basic
eventual consistency guarantees, with no limit to the recency of data returned.
However, anecdotally, partial quorums are often &quot;good enough&quot; for practitioners
given their latency benefits. In this work, we explain why partial quorums are
regularly acceptable in practice, analyzing both the staleness of data they
return and the latency benefits they offer. We introduce Probabilistically
Bounded Staleness (PBS) consistency, which provides expected bounds on
staleness with respect to both versions and wall clock time. We derive a
closed-form solution for versioned staleness as well as model real-time
staleness for representative Dynamo-style systems under internet-scale
production workloads. Using PBS, we measure the latency-consistency trade-off
for partial quorum systems. We quantitatively demonstrate how eventually
consistent systems frequently return consistent data within tens of
milliseconds while offering significant latency benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6089</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6089</id><created>2012-04-26</created><authors><author><keyname>Hilbert</keyname><forenames>Frank</forenames><affiliation>Institute of Construction Informatics Dresden University of Technology Dresden, Germany</affiliation></author><author><keyname>Scherer</keyname><forenames>Raimar J.</forenames><affiliation>Institute of Construction Informatics Dresden University of Technology Dresden, Germany</affiliation></author><author><keyname>Araujo</keyname><forenames>Larissa</forenames><affiliation>Engineering School of Sao Carlos University of Sao Paulo Sao Paulo, Brazil</affiliation></author></authors><title>Multi-model-based Access Control in Construction Projects</title><categories>cs.MA cs.CE cs.CY cs.SE</categories><comments>In Proceedings FAVO 2011, arXiv:1204.5796</comments><proxy>EPTCS</proxy><acm-class>H.5.3</acm-class><journal-ref>EPTCS 83, 2012, pp. 1-9</journal-ref><doi>10.4204/EPTCS.83.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the execution of large scale construction projects performed by
Virtual Organizations (VO), relatively complex technical models have to be
exchanged between the VO members. For linking the trade and transfer of these
models, a so-called multi-model container format was developed. Considering the
different skills and tasks of the involved partners, it is not necessary for
them to know all the models in every technical detailing. Furthermore, the
model size can lead to a delay in communication. In this paper an approach is
presented for defining model cut-outs according to the current project context.
Dynamic dependencies to the project context as well as static dependencies on
the organizational structure are mapped in a context-sensitive rule. As a
result, an approach for dynamic filtering of multi-models is obtained which
ensures, together with a filtering service, that the involved VO members get a
simplified view of complex multi-models as well as sufficient permissions
depending on their tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6090</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6090</id><created>2012-04-26</created><authors><author><keyname>Bab</keyname><forenames>Sebastian</forenames><affiliation>TU Berlin</affiliation></author><author><keyname>Sarrouh</keyname><forenames>Nadim</forenames><affiliation>TU Berlin</affiliation></author></authors><title>Towards a Formal Model of Privacy-Sensitive Dynamic Coalitions</title><categories>cs.MA cs.DC</categories><comments>In Proceedings FAVO 2011, arXiv:1204.5796</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 83, 2012, pp. 10-21</journal-ref><doi>10.4204/EPTCS.83.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of dynamic coalitions (also virtual organizations) describes the
temporary interconnection of autonomous agents, who share information or
resources in order to achieve a common goal. Through modern technologies these
coalitions may form across company, organization and system borders. Therefor
questions of access control and security are of vital significance for the
architectures supporting these coalitions.
  In this paper, we present our first steps to reach a formal framework for
modeling and verifying the design of privacy-sensitive dynamic coalition
infrastructures and their processes. In order to do so we extend existing
dynamic coalition modeling approaches with an access-control-concept, which
manages access to information through policies. Furthermore we regard the
processes underlying these coalitions and present first works in formalizing
these processes. As a result of the present paper we illustrate the usefulness
of the Abstract State Machine (ASM) method for this task. We demonstrate a
formal treatment of privacy-sensitive dynamic coalitions by two example ASMs
which model certain access control situations. A logical consideration of these
ASMs can lead to a better understanding and a verification of the ASMs
according to the aspired specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6091</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6091</id><created>2012-04-26</created><authors><author><keyname>Reiff-Marganiec</keyname><forenames>Stephan</forenames><affiliation>University of Leicester</affiliation></author></authors><title>A structured approach to VO reconfigurations through Policies</title><categories>cs.MA cs.SE</categories><comments>In Proceedings FAVO 2011, arXiv:1204.5796</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 83, 2012, pp. 22-31</journal-ref><doi>10.4204/EPTCS.83.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the strength of Virtual Organisations is their ability to dynamically
and rapidly adapt in response to changing environmental conditions. Dynamic
adaptability has been studied in other system areas as well and system
management through policies has crystallized itself as a very prominent
solution in system and network administration. However, these areas are often
concerned with very low-level technical aspects. Previous work on the APPEL
policy language has been aimed at dynamically adapting system behaviour to
satisfy end-user demands and - as part of STPOWLA - APPEL was used to adapt
workflow instances at runtime. In this paper we explore how the ideas of APPEL
and STPOWLA can be extended from workflows to the wider scope of Virtual
Organisations. We will use a Travel Booking VO as example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6098</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6098</id><created>2012-04-26</created><authors><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>On Locality in Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Information Theory Workshop (ITW) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the design of codes for distributed storage systems (DSS)
that enable local repair in the event of node failure. This paper presents
locally repairable codes based on low degree multivariate polynomials. Its code
construction mechanism extends work on Noisy Interpolating Set by Dvir et al.
\cite{dvir2011}. The paper presents two classes of codes that allow node repair
to be performed by contacting 2 and 3 surviving nodes respectively. It further
shows that both classes are good in terms of their rate and minimum distance,
and allow their rate to be bartered for greater flexibility in the repair
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6100</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6100</id><created>2012-04-26</created><updated>2012-07-31</updated><authors><author><keyname>Ayach</keyname><forenames>Omar El</forenames></author><author><keyname>Lozano</keyname><forenames>Angel</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>On the Overhead of Interference Alignment: Training, Feedback, and
  Cooperation</title><categories>cs.IT math.IT</categories><comments>22 pages, submitted to IEEE Transactions on Wireless Communications</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 11, no. 11, pp.
  4192-4203, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is a cooperative transmission strategy that,
under some conditions, achieves the interference channel's maximum number of
degrees of freedom. Realizing IA gains, however, is contingent upon providing
transmitters with sufficiently accurate channel knowledge. In this paper, we
study the performance of IA in multiple-input multiple-output systems where
channel knowledge is acquired through training and analog feedback. We design
the training and feedback system to maximize IA's effective sum-rate: a
non-asymptotic performance metric that accounts for estimation error, training
and feedback overhead, and channel selectivity. We characterize effective
sum-rate with overhead in relation to various parameters such as
signal-to-noise ratio, Doppler spread, and feedback channel quality. A main
insight from our analysis is that, by properly designing the CSI acquisition
process, IA can provide good sum-rate performance in a very wide range of
fading scenarios. Another observation from our work is that such overhead-aware
analysis can help solve a number of practical network design problems. To
demonstrate the concept of overhead-aware network design, we consider the
example problem of finding the optimal number of cooperative IA users based on
signal power and mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6105</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6105</id><created>2012-04-26</created><updated>2012-11-26</updated><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Garcia</keyname><forenames>Alfredo</forenames></author></authors><title>Mechanism Design for Base Station Association and Resource Allocation in
  Downlink OFDMA Network</title><categories>cs.IT cs.GT math.IT</categories><comments>Technical Report, contains the proof of NP-hardness which is omitted
  in the Journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a resource management problem in a multi-cell downlink OFDMA
network, whereby the goal is to find the optimal per base station resource
allocation and user-base station assignment. The users are assumed to be
strategic/selfish who have private information on downlink channel states and
noise levels. To induce truthfulness among the users as well as to enhance the
spectrum efficiency, the resource management strategy needs to be both
incentive compatible and efficient. However, due to the mixed (discrete and
continuous) nature of resource management in this context, the implementation
of any incentive compatible mechanism that maximizes the system throughput is
NP-hard. We consider the dominant strategy implementation of an approximately
optimal resource management scheme via a computationally tractable mechanism.
The proposed mechanism is decentralized and dynamic. More importantly, it
ensures the truthfulness of the users and it implements a resource allocation
solution that yields at least 1/2 of the optimal throughput. Simulations are
provided to illustrate the effectiveness of the performance of the proposed
mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6106</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6106</id><created>2012-04-26</created><authors><author><keyname>Shi</keyname><forenames>Peng</forenames></author><author><keyname>Zhao</keyname><forenames>Shengmei</forenames></author><author><keyname>Wang</keyname><forenames>Bei</forenames></author></authors><title>Performance of Polar Codes on wireless communications Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, to be submitted to GlobeCom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the performance of polar codes, the capacity-achieving channel
codes, on wireless communication channel in this paper. By generalizing the
definition of Bhattacharyya Parameter in discrete memoryless channel, we
present the special expression of the parameter for Gaussian and Rayleigh
fading the two continuous channels, including the recursive formulas and the
initial values. We analyze the applications of polar codes with the defined
parameter over Rayleigh fading channel by transmitting image and speech. By
comparing with low density parity-check codes(LDPC) at the same cases, our
simulation results show that polar codes have better performance than that of
LDPC codes. Polar codes will be good candidate for wireless communication
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6120</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6120</id><created>2012-04-27</created><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author></authors><title>Geometric Separation by Single-Pass Alternating Thresholding</title><categories>math.FA cs.IT math.IT math.NA</categories><comments>35 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern data is customarily of multimodal nature, and analysis tasks typically
require separation into the single components. Although a highly ill-posed
problem, the morphological difference of these components sometimes allow a
very precise separation such as, for instance, in neurobiological imaging a
separation into spines (pointlike structures) and dendrites (curvilinear
structures). Recently, applied harmonic analysis introduced powerful
methodologies to achieve this task, exploiting specifically designed
representation systems in which the components are sparsely representable,
combined with either performing $\ell_1$ minimization or thresholding on the
combined dictionary.
  In this paper we provide a thorough theoretical study of the separation of a
distributional model situation of point- and curvilinear singularities
exploiting a surprisingly simple single-pass alternating thresholding method
applied to the two complementary frames: wavelets and curvelets. Utilizing the
fact that the coefficients are clustered geometrically, thereby exhibiting
clustered/geometric sparsity in the chosen frames, we prove that at
sufficiently fine scales arbitrarily precise separation is possible. Even more
surprising, it turns out that the thresholding index sets converge to the
wavefront sets of the point- and curvilinear singularities in phase space and
that those wavefront sets are perfectly separated by the thresholding
procedure. Main ingredients of our analysis are the novel notion of cluster
coherence and clustered/geometric sparsity as well as a microlocal analysis
viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6123</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6123</id><created>2012-04-27</created><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author></authors><title>Clustered Sparsity and Separation of Cartoon and Texture</title><categories>math.FA cs.IT math.IT math.NA</categories><comments>25 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural images are typically a composition of cartoon and texture structures.
A medical image might, for instance, show a mixture of gray matter and the
skull cap. One common task is to separate such an image into two single images,
one containing the cartoon part and the other containing the texture part.
Recently, a powerful class of algorithms using sparse approximation and
$\ell_1$ minimization has been introduced to resolve this problem, and numerous
inspiring empirical results have already been obtained.
  In this paper we provide the first thorough theoretical study of the
separation of a combination of cartoon and texture structures in a model
situation using this class of algorithms. The methodology we consider expands
the image in a combined dictionary consisting of a curvelet tight frame and a
Gabor tight frame and minimizes the $\ell_1$ norm on the analysis side. Sparse
approximation properties then force the cartoon components into the curvelet
coefficients and the texture components into the Gabor coefficients, thereby
separating the image. Utilizing the fact that the coefficients are clustered
geometrically, we prove that at sufficiently fine scales arbitrarily precise
separation is possible. Main ingredients of our analysis are the novel notion
of cluster coherence and clustered/geometric sparsity. Our analysis also
provides a deep understanding on when separation is still possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6170</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6170</id><created>2012-04-27</created><updated>2012-05-31</updated><authors><author><keyname>Hesselink</keyname><forenames>Wim H.</forenames></author></authors><title>A distributed resource allocation algorithm for many processes</title><categories>cs.DC</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource allocation is the problem that a process may enter a critical
section CS of its code only when its resource requirements are not in conflict
with those of other processes in their critical sections. For each execution of
CS, these requirements are given anew. In the resource requirements, levels can
be distinguished, such as e.g. read access or write access. We allow infinitely
many processes that communicate by reliable asynchronous messages and have
finite memory. A simple starvation-free solution is presented. Processes only
wait for one another when they have conflicting resource requirements. The
correctness of the solution is argued with invariants and temporal logic. It
has been verified with the proof assistant PVS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6174</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6174</id><created>2012-04-27</created><updated>2013-02-15</updated><authors><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Jungers</keyname><forenames>Raphael M.</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Sou</keyname><forenames>Kin Cheong</forenames></author></authors><title>Efficient Computations of a Security Index for False Data Attacks in
  Power Networks</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The resilience of Supervisory Control and Data Acquisition (SCADA) systems
for electric power networks for certain cyber-attacks is considered. We analyze
the vulnerability of the measurement system to false data attack on
communicated measurements. The vulnerability analysis problem is shown to be
NP-hard, meaning that unless $P = NP$ there is no polynomial time algorithm to
analyze the vulnerability of the system. Nevertheless, we identify situations,
such as the full measurement case, where it can be solved efficiently. In such
cases, we show indeed that the problem can be cast as a generalization of the
minimum cut problem involving costly nodes. We further show that it can be
reformulated as a standard minimum cut problem (without costly nodes) on a
modified graph of proportional size. An important consequence of this result is
that our approach provides the first exact efficient algorithm for the
vulnerability analysis problem under the full measurement assumption.
Furthermore, our approach also provides an efficient heuristic algorithm for
the general NP-hard problem. Our results are illustrated by numerical studies
on benchmark systems including the IEEE 118-bus system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6178</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6178</id><created>2012-04-27</created><updated>2013-09-17</updated><authors><author><keyname>Feyzmahdavian</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Gattami</keyname><forenames>Ather</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Distributed Output-Feedback LQG Control with Delayed Information Sharing</title><categories>cs.SY</categories><comments>25 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper develops a controller synthesis method for distributed LQG control
problems under output-feedback. We consider a system consisting of three
interconnected linear subsystems with a delayed information sharing structure.
While the state-feedback case has previously been solved, the extension to
output-feedback is nontrivial as the classical separation principle fails. To
find the optimal solution, the controller is decomposed into two independent
components: a centralized LQG-optimal controller under delayed state
observations, and a sum of correction terms based on additional local
information available to decision makers. Explicit discrete-time equations are
derived whose solutions are the gains of the optimal controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6179</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6179</id><created>2012-04-27</created><updated>2012-05-04</updated><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Sreejith</keyname><forenames>A. V.</forenames></author></authors><title>Non-definability of languages by generalized first-order formulas over
  (N,+)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider first-order logic with monoidal quantifiers over words. We show
that all languages with a neutral letter, definable using the addition
numerical predicate are also definable with the order predicate as the only
numerical predicate. Let S be a subset of monoids.
  Let LS be the logic closed under quantification over the monoids in S and N
be the class of neutral letter languages. Then we show that: LS[&lt;,+] cap N =
LS[&lt;] Our result can be interpreted as the Crane Beach conjecture to hold for
the logic LS[&lt;,+]. As a corollary of our result we get the result of Roy and
Straubing that FO+MOD[&lt;,+] collapses to FO+MOD[&lt;].
  For cyclic groups, we answer an open question of Roy and Straubing, proving
that MOD[&lt;,+] collapses to MOD[&lt;]. Our result also shows that multiplication is
necessary for Barrington's theorem to hold.
  All these results can be viewed as separation results for very uniform
circuit classes. For example we separate FO[&lt;,+]-uniform CC0 from
FO[&lt;,+]-uniform ACC0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6181</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6181</id><created>2012-04-27</created><authors><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author></authors><title>The conduciveness of CA-rule graphs</title><categories>nlin.CG cs.NE</categories><journal-ref>Artificial Life 19 (2013), 255-266</journal-ref><doi>10.1162/ARTL_a_00107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two subsets A and B of nodes in a directed graph, the conduciveness of
the graph from A to B is the ratio representing how many of the edges outgoing
from nodes in A are incoming to nodes in B. When the graph's nodes stand for
the possible solutions to certain problems of combinatorial optimization,
choosing its edges appropriately has been shown to lead to conduciveness
properties that provide useful insight into the performance of algorithms to
solve those problems. Here we study the conduciveness of CA-rule graphs, that
is, graphs whose node set is the set of all CA rules given a cell's number of
possible states and neighborhood size. We consider several different edge sets
interconnecting these nodes, both deterministic and random ones, and derive
analytical expressions for the resulting graph's conduciveness toward rules
having a fixed number of non-quiescent entries. We demonstrate that one of the
random edge sets, characterized by allowing nodes to be sparsely interconnected
across any Hamming distance between the corresponding rules, has the potential
of providing reasonable conduciveness toward the desired rules. We conjecture
that this may lie at the bottom of the best strategies known to date for
discovering complex rules to solve specific problems, all of an evolutionary
nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6216</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6216</id><created>2012-04-24</created><updated>2012-09-12</updated><authors><author><keyname>Crane</keyname><forenames>Keenan</forenames></author><author><keyname>Weischedel</keyname><forenames>Clarisse</forenames></author><author><keyname>Wardetzky</keyname><forenames>Max</forenames></author></authors><title>Geodesics in Heat</title><categories>cs.GR</categories><journal-ref>ACM Trans. Graph. 32 (5), 2013</journal-ref><doi>10.1145/2516971.2516977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the heat method for computing the shortest geodesic distance to
a specified subset (e.g., point or curve) of a given domain. The heat method is
robust, efficient, and simple to implement since it is based on solving a pair
of standard linear elliptic problems. The method represents a significant
breakthrough in the practical computation of distance on a wide variety of
geometric domains, since the resulting linear systems can be prefactored once
and subsequently solved in near-linear time. In practice, distance can be
updated via the heat method an order of magnitude faster than with
state-of-the-art methods while maintaining a comparable level of accuracy. We
provide numerical evidence that the method converges to the exact geodesic
distance in the limit of refinement; we also explore smoothed approximations of
distance suitable for applications where more regularity is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6233</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6233</id><created>2012-04-27</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Strong Backdoors to Bounded Treewidth SAT</title><categories>cs.DS cs.AI cs.CC cs.DM math.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.4331</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are various approaches to exploiting &quot;hidden structure&quot; in instances of
hard combinatorial problems to allow faster algorithms than for general
unstructured or random instances. For SAT and its counting version #SAT, hidden
structure has been exploited in terms of decomposability and strong backdoor
sets. Decomposability can be considered in terms of the treewidth of a graph
that is associated with the given CNF formula, for instance by considering
clauses and variables as vertices of the graph, and making a variable adjacent
with all the clauses it appears in. On the other hand, a strong backdoor set of
a CNF formula is a set of variables such that each possible partial assignment
to this set moves the formula into a fixed class for which (#)SAT can be solved
in polynomial time.
  In this paper we combine the two above approaches. In particular, we study
the algorithmic question of finding a small strong backdoor set into the class
W_t of CNF formulas whose associated graphs have treewidth at most t. The main
results are positive:
  (1) There is a cubic-time algorithm that, given a CNF formula F and two
constants k,t\ge 0, either finds a strong W_t-backdoor set of size at most 2^k,
or concludes that F has no strong W_t-backdoor set of size at most k.
  (2) There is a cubic-time algorithm that, given a CNF formula F, computes the
number of satisfying assignments of F or concludes that sb_t(F)&gt;k, for any pair
of constants k,t\ge 0. Here, sb_t(F) denotes the size of a smallest strong
W_t-backdoor set of F.
  The significance of our results lies in the fact that they allow us to
exploit algorithmically a hidden structure in formulas that is not accessible
by any one of the two approaches (decomposability, backdoors) alone. Already a
backdoor size 1 on top of treewidth 1 (i.e., sb_1(F)=1) entails formulas of
arbitrarily large treewidth and arbitrarily large cycle cutsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6236</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6236</id><created>2012-04-27</created><authors><author><keyname>Baelde</keyname><forenames>David</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>Combining Deduction Modulo and Logics of Fixed-Point Definitions</title><categories>cs.LO</categories><comments>Long version of a paper accepted at LICS'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inductive and coinductive specifications are widely used in formalizing
computational systems. Such specifications have a natural rendition in logics
that support fixed-point definitions. Another useful formalization device is
that of recursive specifications. These specifications are not directly
complemented by fixed-point reasoning techniques and, correspondingly, do not
have to satisfy strong monotonicity restrictions. We show how to incorporate a
rewriting capability into logics of fixed-point definitions towards
additionally supporting recursive specifications. In particular, we describe a
natural deduction calculus that adds a form of &quot;closed-world&quot; equality - a key
ingredient to supporting fixed-point definitions - to deduction modulo, a
framework for extending a logic with a rewriting layer operating on formulas.
We show that our calculus enjoys strong normalizability when the rewrite system
satisfies general properties and we demonstrate its usefulness in specifying
and reasoning about syntax-based descriptions. The integration of closed-world
equality into deduction modulo leads us to reconfigure the elimination
principle for this form of equality in a way that, for the first time, resolves
issues regarding the stability of finite proofs under reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6248</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6248</id><created>2012-01-07</created><updated>2013-02-18</updated><authors><author><keyname>Merler</keyname><forenames>Stefano</forenames></author><author><keyname>Jurman</keyname><forenames>Giuseppe</forenames></author></authors><title>A combinatorial model of malware diffusion via Bluetooth connections</title><categories>cs.CR math.CO</categories><comments>In press on PlosONE</comments><doi>10.1371/journal.pone.0059468</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline here the mathematical expression of a diffusion model for
cellphones malware transmitted through Bluetooth channels. In particular, we
provide the deterministic formula underlying the proposed infection model, in
its equivalent recursive (simple but computationally heavy) and closed form
(more complex but efficiently computable) expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6249</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6249</id><created>2012-04-27</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>Understanding differential equations through diffusion point of view:
  non-symmetric discrete equations</title><categories>cs.NA cs.DM math.AP math.CA math.NA</categories><comments>4 pages</comments><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new adaptation of the D-iteration algorithm to
numerically solve the differential equations. This problem can be reinterpreted
in 2D or 3D (or higher dimensions) as a limit of a diffusion process where the
boundary or initial conditions are replaced by fluid catalysts. It has been
shown that pre-computing the diffusion process for an elementary catalyst case
as a fundamental block of a class of differential equations, the computation
efficiency can be greatly improved. Here, we explain how the diffusion point of
view can be applied to decompose the fluid diffusion process per direction and
how to handle non-symmetric discrete equations. The method can be applied on
the class of problems that can be addressed by the Gauss-Seidel iteration,
based on the linear approximation of the differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6250</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6250</id><created>2011-11-28</created><authors><author><keyname>Abro</keyname><forenames>Abdul Ghani</forenames></author><author><keyname>Saleh</keyname><forenames>Junita Mohamad</forenames></author></authors><title>Feature Selection for Generator Excitation Neurocontroller Development
  Using Filter Technique</title><categories>cs.SY cs.LG</categories><comments>10-Pages, 10-Figures, 8-Tables, International Journal of Computer
  Science Issues, Vol. 8, Issue 5, No 3, September 2011</comments><journal-ref>International Journal of Computer Science Issues,PP. 108-117, Vol.
  8, Issue 5, No 3, September 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Essentially, motive behind using control system is to generate suitable
control signal for yielding desired response of a physical process. Control of
synchronous generator has always remained very critical in power system
operation and control. For certain well known reasons power generators are
normally operated well below their steady state stability limit. This raises
demand for efficient and fast controllers. Artificial intelligence has been
reported to give revolutionary outcomes in the field of control engineering.
Artificial Neural Network (ANN), a branch of artificial intelligence has been
used for nonlinear and adaptive control, utilizing its inherent observability.
The overall performance of neurocontroller is dependent upon input features
too. Selecting optimum features to train a neurocontroller optimally is very
critical. Both quality and size of data are of equal importance for better
performance. In this work filter technique is employed to select independent
factors for ANN training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6255</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6255</id><created>2012-04-27</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author><author><keyname>Burnside</keyname><forenames>G&#xe9;rard</forenames></author><author><keyname>Raoult</keyname><forenames>Philippe</forenames></author></authors><title>Revisiting the D-iteration method: runtime comparison</title><categories>cs.NA cs.PF</categories><comments>7 pages</comments><acm-class>G.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the D-iteration algorithm in order to better
explain different performance results that were observed for the numerical
computation of the eigenvector associated to the PageRank score. We revisit
here the practical computation cost based on the execution runtime compared to
the theoretical number of iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6284</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6284</id><created>2011-11-24</created><authors><author><keyname>Mazzega</keyname><forenames>Pierre</forenames><affiliation>LMTG</affiliation></author><author><keyname>Bourcier</keyname><forenames>Dani&#xe8;le</forenames><affiliation>CERSA</affiliation></author><author><keyname>Boulet</keyname><forenames>Romain</forenames><affiliation>LMTG</affiliation></author></authors><title>The Network of French Legal Codes</title><categories>cs.AI cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>12th International Conference on Artificial Intelligence and Law
  (ICAIL 2009), Barcelona : Espagne (2009)</journal-ref><doi>10.1145/1568234.1568271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an analysis of the codified Law of France as a structured system.
Fifty two legal codes are selected on the basis of explicit legal criteria and
considered as vertices with their mutual quotations forming the edges in a
network which properties are analyzed relying on graph theory. We find that a
group of 10 codes are simultaneously the most citing and the most cited by
other codes, and are also strongly connected together so forming a &quot;rich club&quot;
sub-graph. Three other code communities are also found that somewhat partition
the legal field is distinct thematic sub-domains. The legal interpretation of
this partition is opening new untraditional lines of research. We also
conjecture that many legal systems are forming such new kind of networks that
share some properties in common with small worlds but are far denser. We
propose to call &quot;concentrated world&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6291</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6291</id><created>2012-04-27</created><authors><author><keyname>Elberfeld</keyname><forenames>Michael</forenames></author><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Tantau</keyname><forenames>Till</forenames></author></authors><title>Where First-Order and Monadic Second-Order Logic Coincide</title><categories>cs.LO cs.CC</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study on which classes of graphs first-order logic (FO) and monadic
second-order logic (MSO) have the same expressive power. We show that for all
classes C of graphs that are closed under taking subgraphs, FO and MSO have the
same expressive power on C if, and only if, C has bounded tree depth. Tree
depth is a graph invariant that measures the similarity of a graph to a star in
a similar way that tree width measures the similarity of a graph to a tree. For
classes just closed under taking induced subgraphs, we show an analogous result
for guarded second-order logic (GSO), the variant of MSO that not only allows
quantification over vertex sets but also over edge sets. A key tool in our
proof is a Feferman-Vaught-type theorem that is constructive and still works
for unbounded partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6304</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6304</id><created>2012-04-27</created><authors><author><keyname>Nagarajan</keyname><forenames>Sathya Narayanan</forenames></author><author><keyname>Ravikumar</keyname><forenames>Srijith</forenames></author></authors><title>Model for Predicting End User Web Page Response Time</title><categories>cs.PF</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Perceived responsiveness of a web page is one of the most important and least
understood metrics of web page design, and is critical for attracting and
maintaining a large audience. Web pages can be designed to meet performance
SLAs early in the product lifecycle if there is a way to predict the apparent
responsiveness of a particular page layout. Response time of a web page is
largely influenced by page layout and various network characteristics. Since
the network characteristics vary widely from country to country, accurately
modeling and predicting the perceived responsiveness of a web page from the end
user's perspective has traditionally proven very difficult. We propose a model
for predicting end user web page response time based on web page, network,
browser download and browser rendering characteristics. We start by
understanding the key parameters that affect perceived response time. We then
model each of these parameters individually using experimental tests and
statistical techniques. Finally, we demonstrate the effectiveness of this model
by conducting an experimental study with Yahoo! web pages in two countries and
compare it with 3rd party measurement application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6321</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6321</id><created>2012-04-27</created><authors><author><keyname>Leftheriotis</keyname><forenames>Ioannis</forenames></author><author><keyname>Gkonela</keyname><forenames>Chrysoula</forenames></author><author><keyname>Chorianopoulos</keyname><forenames>Konstantinos</forenames></author></authors><title>Efficient Video Indexing on the Web: A System that Leverages User
  Interactions with a Video Player</title><categories>cs.MM cs.DL cs.HC cs.IR</categories><comments>9 pages, 3 figures, UCMedia 2010: 2nd International ICST Conference
  on User Centric Media</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a user-based video indexing method, that
automatically generates thumbnails of the most important scenes of an online
video stream, by analyzing users' interactions with a web video player. As a
test bench to verify our idea we have extended the YouTube video player into
the VideoSkip system. In addition, VideoSkip uses a web-database (Google
Application Engine) to keep a record of some important parameters, such as the
timing of basic user actions (play, pause, skip). Moreover, we implemented an
algorithm that selects representative thumbnails. Finally, we populated the
system with data from an experiment with nine users. We found that the
VideoSkip system indexes video content by leveraging implicit users
interactions, such as pause and thirty seconds skip. Our early findings point
toward improvements of the web video player and its thumbnail generation
technique. The VideSkip system could compliment content-based algorithms, in
order to achieve efficient video-indexing in difficult videos, such as lectures
or sports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6325</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6325</id><created>2012-04-27</created><updated>2012-06-03</updated><authors><author><keyname>Chorianopoulos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Tsaknaki</keyname><forenames>Vassiliki</forenames></author></authors><title>CELL: Connecting Everyday Life in an archipeLago</title><categories>cs.HC cs.LG</categories><comments>This paper has been withdrawn by the author due to some errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the design of a seamless broadcast communication system that
brings together the distributed community of remote secondary education
schools. In contrast to higher education, primary and secondary education
establishments should remain distributed, in order to maintain a balance of
urban and rural life in the developing and the developed world. We plan to
deploy an ambient and social interactive TV platform (physical installation,
authoring tools, interactive content) that supports social communication in a
positive way. In particular, we present the physical design and the conceptual
model of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6326</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6326</id><created>2012-04-27</created><updated>2012-05-17</updated><authors><author><keyname>Jodoin</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Bilodeau</keyname><forenames>Guillaume-Alexandre</forenames></author><author><keyname>Saunier</keyname><forenames>Nicolas</forenames></author></authors><title>Background subtraction based on Local Shape</title><categories>cs.CV</categories><comments>4 pages, 5 figures, 3 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to background subtraction that is based on the
local shape of small image regions. In our approach, an image region centered
on a pixel is mod-eled using the local self-similarity descriptor. We aim at
obtaining a reliable change detection based on local shape change in an image
when foreground objects are moving. The method first builds a background model
and compares the local self-similarities between the background model and the
subsequent frames to distinguish background and foreground objects.
Post-processing is then used to refine the boundaries of moving objects.
Results show that this approach is promising as the foregrounds obtained are
com-plete, although they often include shadows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6341</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6341</id><created>2012-04-27</created><updated>2012-10-24</updated><authors><author><keyname>Lee</keyname><forenames>Junghoon</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>Stochastic Ordering of Interferences in Large-scale Wireless Networks</title><categories>cs.IT math.IT</categories><comments>28 pages, 6 figures, submitted to IEEE Trans. Signal Process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic orders are binary relations defined on probability distributions
which capture intuitive notions like being larger or being more variable. This
paper introduces stochastic ordering of interference distributions in
large-scale networks modeled as point process. Interference is the main
performance-limiting factor in most wireless networks, thus it is important to
understand its statistics. Since closed-form results for the distribution of
interference for such networks are only available in limited cases,
interference of networks are compared using stochastic orders, even when closed
form expressions for interferences are not tractable. We show that the
interference from a large-scale network depends on the fading distributions
with respect to the stochastic Laplace transform order. The condition for
path-loss models is also established to have stochastic ordering between
interferences. The stochastic ordering of interferences between different
networks are also shown. Monte-Carlo simulations are used to supplement our
analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6346</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6346</id><created>2012-04-27</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author></authors><title>Magic Sets for Disjunctive Datalog Programs</title><categories>cs.AI cs.LO</categories><comments>67 pages, 19 figures, preprint submitted to Artificial Intelligence</comments><msc-class>68T27</msc-class><acm-class>F.4.1</acm-class><doi>10.1007/978-3-642-28148-8_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new technique for the optimization of (partially) bound
queries over disjunctive Datalog programs with stratified negation is
presented. The technique exploits the propagation of query bindings and extends
the Magic Set (MS) optimization technique.
  An important feature of disjunctive Datalog is nonmonotonicity, which calls
for nondeterministic implementations, such as backtracking search. A
distinguishing characteristic of the new method is that the optimization can be
exploited also during the nondeterministic phase. In particular, after some
assumptions have been made during the computation, parts of the program may
become irrelevant to a query under these assumptions. This allows for dynamic
pruning of the search space. In contrast, the effect of the previously defined
MS methods for disjunctive Datalog is limited to the deterministic portion of
the process. In this way, the potential performance gain by using the proposed
method can be exponential, as could be observed empirically.
  The correctness of MS is established thanks to a strong relationship between
MS and unfounded sets that has not been studied in the literature before. This
knowledge allows for extending the method also to programs with stratified
negation in a natural way.
  The proposed method has been implemented in DLV and various experiments have
been conducted. Experimental results on synthetic data confirm the utility of
MS for disjunctive Datalog, and they highlight the computational gain that may
be obtained by the new method w.r.t. the previously proposed MS methods for
disjunctive Datalog programs. Further experiments on real-world data show the
benefits of MS within an application scenario that has received considerable
attention in recent years, the problem of answering user queries over possibly
inconsistent databases originating from integration of autonomous sources of
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6350</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6350</id><created>2012-04-27</created><updated>2012-06-18</updated><authors><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Shashank</keyname><forenames>V.</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author></authors><title>Secure Computation in a Bidirectional Relay</title><categories>cs.IT math.IT</categories><comments>This work is now subsumed by arXiv:1206.3392</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional relaying, where a relay helps two user nodes to exchange equal
length binary messages, has been an active area of recent research. A popular
strategy involves a modified Gaussian MAC, where the relay decodes the XOR of
the two messages using the naturally-occurring sum of symbols simultaneously
transmitted by user nodes. In this work, we consider the Gaussian MAC in
bidirectional relaying with an additional secrecy constraint for protection
against a honest but curious relay. The constraint is that, while the relay
should decode the XOR, it should be fully ignorant of the individual messages
of the users. We exploit the symbol addition that occurs in a Gaussian MAC to
design explicit strategies that achieve perfect independence between the
received symbols and individual transmitted messages. Our results actually hold
for a more general scenario where the messages at the two user nodes come from
a finite Abelian group, and the relay must decode the sum within the group of
the two messages. We provide a lattice coding strategy and study optimal rate
versus average power trade-offs for asymptotically large dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6362</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6362</id><created>2012-04-27</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Elsayed</keyname><forenames>Adel</forenames></author></authors><title>A Corpus-based Evaluation of Lexical Components of a Domainspecific Text
  to Knowledge Mapping Prototype</title><categories>cs.IR cs.CL</categories><comments>2008 IEEE International Conference on Computer and Information
  Technology (ICCIT 2008)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of this paper is to evaluate the lexical components of a Text to
Knowledge Mapping (TKM) prototype. The prototype is domain-specific, the
purpose of which is to map instructional text onto a knowledge domain. The
context of the knowledge domain of the prototype is physics, specifically DC
electrical circuits. During development, the prototype has been tested with a
limited data set from the domain. The prototype now reached a stage where it
needs to be evaluated with a representative linguistic data set called corpus.
A corpus is a collection of text drawn from typical sources which can be used
as a test data set to evaluate NLP systems. As there is no available corpus for
the domain, we developed a representative corpus and annotated it with
linguistic information. The evaluation of the prototype considers one of its
two main components- lexical knowledge base. With the corpus, the evaluation
enriches the lexical knowledge resources like vocabulary and grammar structure.
This leads the prototype to parse a reasonable amount of sentences in the
corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6364</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6364</id><created>2012-04-27</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Elsayed</keyname><forenames>Adel</forenames></author><author><keyname>Akter</keyname><forenames>Quazi Mah-Zereen</forenames></author></authors><title>A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping
  Prototype</title><categories>cs.CL</categories><comments>Journal of Computers, Academy Publishers 2010</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)
Prototype. The prototype is domain-specific, the purpose of which is to map
instructional text onto a knowledge domain. The context of the knowledge domain
is DC electrical circuit. During development, the prototype has been tested
with a limited data set from the domain. The prototype reached a stage where it
needs to be evaluated with a representative linguistic data set called corpus.
A corpus is a collection of text drawn from typical sources which can be used
as a test data set to evaluate NLP systems. As there is no available corpus for
the domain, we developed and annotated a representative corpus. The evaluation
of the prototype considers two of its major components- lexical components and
knowledge model. Evaluation on lexical components enriches the lexical
resources of the prototype like vocabulary and grammar structures. This leads
the prototype to parse a reasonable amount of sentences in the corpus. While
dealing with the lexicon was straight forward, the identification and
extraction of appropriate semantic relations was much more involved. It was
necessary, therefore, to manually develop a conceptual structure for the domain
to formulate a domain-specific framework of semantic relations. The framework
of semantic relationsthat has resulted from this study consisted of 55
relations, out of which 42 have inverse relations. We also conducted rhetorical
analysis on the corpus to prove its representativeness in conveying semantic.
Finally, we conducted a topical and discourse analysis on the corpus to analyze
the coverage of discourse by the prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6376</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6376</id><created>2012-04-28</created><authors><author><keyname>Weinan</keyname><forenames>E.</forenames></author><author><keyname>Lu</keyname><forenames>Jianfeng</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author></authors><title>The Landscape of Complex Networks</title><categories>stat.ME cs.SI physics.soc-ph q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological landscape is introduced for networks with functions defined on
the nodes. By extending the notion of gradient flows to the network setting,
critical nodes of different indices are defined. This leads to a concise and
hierarchical representation of the network. Persistent homology from
computational topology is used to design efficient algorithms for performing
such analysis. Applications to some examples in social and biological networks
are demonstrated, which show that critical nodes carry important information
about structures and dynamics of such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6385</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6385</id><created>2012-04-28</created><authors><author><keyname>Sun</keyname><forenames>Yankui</forenames></author><author><keyname>Zhang</keyname><forenames>Tian</forenames></author></authors><title>A 3D Segmentation Method for Retinal Optical Coherence Tomography Volume
  Data</title><categories>cs.CV physics.optics</categories><comments>4 pages, 9 figures</comments><journal-ref>China Patent Application (201110247341.5), 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the introduction of spectral-domain optical coherence tomography (OCT),
much larger image datasets are routinely acquired compared to what was possible
using the previous generation of time-domain OCT. Thus, the need for 3-D
segmentation methods for processing such data is becoming increasingly
important. We present a new 3D segmentation method for retinal OCT volume data,
which generates an enhanced volume data by using pixel intensity, boundary
position information, intensity changes on both sides of the border
simultaneously, and preliminary discrete boundary points are found from all
A-Scans and then the smoothed boundary surface can be obtained after removing a
small quantity of error points. Our experiments show that this method is
efficient, accurate and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6389</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6389</id><created>2012-04-28</created><updated>2012-12-02</updated><authors><author><keyname>Gaspar</keyname><forenames>Merse E.</forenames></author><author><keyname>Csermely</keyname><forenames>Peter</forenames></author></authors><title>Rigidity and flexibility of biological networks</title><categories>physics.bio-ph cond-mat.dis-nn cs.CE cs.CG nlin.PS q-bio.MN</categories><comments>21 pages, 4 figures, 1 table</comments><journal-ref>Briefings in Functional Genomics (2012) 11: 443-456</journal-ref><doi>10.1093/bfgp/els023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network approach became a widely used tool to understand the behaviour of
complex systems in the last decade. We start from a short description of
structural rigidity theory. A detailed account on the combinatorial rigidity
analysis of protein structures, as well as local flexibility measures of
proteins and their applications in explaining allostery and thermostability is
given. We also briefly discuss the network aspects of cytoskeletal tensegrity.
Finally, we show the importance of the balance between functional flexibility
and rigidity in protein-protein interaction, metabolic, gene regulatory and
neuronal networks. Our summary raises the possibility that the concepts of
flexibility and rigidity can be generalized to all networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6391</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6391</id><created>2012-04-28</created><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Extending partial representations of function graphs and permutation
  graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>Submitted to ESA 2012, track A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Function graphs are graphs representable by intersections of continuous
real-valued functions on the interval [0,1] and are known to be exactly the
complements of comparability graphs. As such they are recognizable in
polynomial time. Function graphs generalize permutation graphs, which arise
when all functions considered are linear.
  We focus on the problem of extending partial representations, which
generalizes the recognition problem. We observe that for permutation graphs an
easy extension of Golumbic's comparability graph recognition algorithm can be
exploited. This approach fails for function graphs. Nevertheless, we present a
polynomial-time algorithm for extending a partial representation of a graph by
functions defined on the entire interval [0,1] provided for some of the
vertices. On the other hand, we show that if a partial representation consists
of functions defined on subintervals of [0,1], then the problem of extending
this representation to functions on the entire interval [0,1] becomes
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6396</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6396</id><created>2012-04-28</created><authors><author><keyname>Bhatnagar</keyname><forenames>Roheet</forenames></author><author><keyname>Ghose</keyname><forenames>Mrinal Kanti</forenames></author></authors><title>Comparing Soft Computing Techniques For Early Stage Software Development
  Effort Estimations</title><categories>cs.SE</categories><comments>09 PAGES</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.3, No.2, March 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Accurately estimating the software size, cost, effort and schedule is
probably the biggest challenge facing software developers today. It has major
implications for the management of software development because both the
overestimates and underestimates have direct impact for causing damage to
software companies. Lot of models have been proposed over the years by various
researchers for carrying out effort estimations. Also some of the studies for
early stage effort estimations suggest the importance of early estimations. New
paradigms offer alternatives to estimate the software development effort, in
particular the Computational Intelligence (CI) that exploits mechanisms of
interaction between humans and processes domain knowledge with the intention of
building intelligent systems (IS). Among IS, Artificial Neural Network and
Fuzzy Logic are the two most popular soft computing techniques for software
development effort estimation. In this paper neural network models and Mamdani
FIS model have been used to predict the early stage effort estimations using
the student dataset. It has been found that Mamdani FIS was able to predict the
early stage efforts more efficiently in comparison to the neural network models
based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6408</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6408</id><created>2012-04-28</created><authors><author><keyname>Gritschacher</keyname><forenames>Tobias</forenames></author><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author></authors><title>Standing on the Shoulders of Their Peers: Success Factors for Massive
  Cooperation Among Children Creating Open Source Animations and Games on Their
  Smartphones</title><categories>cs.CY cs.SI</categories><comments>4 pages; short paper at the 11th International Conference on
  Interaction Design and Children (IDC2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a website for kids where they can share new as well as remixed
animations and games, e.g., interactive music videos, which they created on
their smartphones or tablets using a visual &quot;LEGO-style&quot; programming
environment called Catroid. Online communities for children like our website
have unique requirements, and keeping the commitment of kids on a high level is
a continuous challenge. For instance, one key motivator for kids is the ability
to entertain their friends. Another success factor is the ability to learn from
and cooperate with other children. In this short position paper we attempt at
identifying the requirements for the success of such an online community, both
from the point of view of the kids as well as of their parents, and at finding
ways to make it attractive for both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6411</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6411</id><created>2012-04-28</created><authors><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author></authors><title>Catroid: A Mobile Visual Programming System for Children</title><categories>cs.PL cs.CY cs.HC cs.RO</categories><comments>4 pages. Demo paper at the 11th International Conference on
  Interaction Design and Children (IDC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Catroid is a free and open source visual programming language, programming
environment, image manipulation program, and website. Catroid allows casual and
first-time users starting from age eight to develop their own animations and
games solely using their Android phones or tablets. Catroid also allows to
wirelessly control external hardware such as Lego Mindstorms robots via
Bluetooth, Bluetooth Arduino boards, as well as Parrot's popular and
inexpensive AR.Drone quadcopters via WiFi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6415</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6415</id><created>2012-04-28</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author></authors><title>A Fuzzy Model for Analogical Problem Solving</title><categories>cs.AI</categories><comments>10 pages, 1 Table</comments><msc-class>03E72, 97C30</msc-class><journal-ref>International Journal of Fuzzy Logic Systems Vol. 2, No. 1, pp.
  1-10, February 2012</journal-ref><doi>10.5121/ijfls.2012.2101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a fuzzy model for the description of the process of
Analogical Reasoning by representing its main steps as fuzzy subsets of a set
of linguistic labels characterizing the individuals' performance in each step
and we use the Shannon- Wiener diversity index as a measure of the individuals'
abilities in analogical problem solving. This model is compared with a
stochastic model presented in author's earlier papers by introducing a finite
Markov chain on the steps of the process of Analogical Reasoning. A classroom
experiment is also presented to illustrate the use of our results in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6416</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6416</id><created>2012-04-28</created><authors><author><keyname>Borgohain</keyname><forenames>Rajdeep</forenames></author></authors><title>FuGeIDS: Fuzzy Genetic paradigms in Intrusion Detection Systems</title><categories>cs.CR</categories><comments>7 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase in the number of security threats, Intrusion Detection
Systems have evolved as a significant countermeasure against these threats. And
as such, the topic of Intrusion Detection Systems has become one of the most
prominent research topics in recent years. This paper gives an overview of the
Intrusion Detection System and looks at two major machine learning paradigms
used in Intrusion Detection System, Genetic Algorithms and Fuzzy Logic and how
to apply them for intrusion detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6422</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6422</id><created>2012-04-28</created><authors><author><keyname>Cheilaris</keyname><forenames>Panagiotis</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Shakhar</forenames></author></authors><title>Conflict-free coloring with respect to a subset of intervals</title><categories>math.CO cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a hypergraph H = (V, E), a coloring of its vertices is said to be
conflict-free if for every hyperedge S \in E there is at least one vertex in S
whose color is distinct from the colors of all other vertices in S. The
discrete interval hypergraph Hn is the hypergraph with vertex set {1,...,n} and
hyperedge set the family of all subsets of consecutive integers in {1,...,n}.
We provide a polynomial time algorithm for conflict-free coloring any
subhypergraph of Hn, we show that the algorithm has approximation ratio 2, and
we prove that our analysis is tight, i.e., there is a subhypergraph for which
the algorithm computes a solution which uses twice the number of colors of the
optimal solution. We also show that the problem of deciding whether a given
subhypergraph of Hn can be colored with at most k colors has a quasipolynomial
time algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6423</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6423</id><created>2012-04-28</created><updated>2013-11-27</updated><authors><author><keyname>Pandey</keyname><forenames>Gaurav</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>Minimum Description Length Principle for Maximum Entropy Model Selection</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures, 4 tables, submitted to Uncertainty in Artificial
  Intelligence</comments><doi>10.1109/ISIT.2013.6620481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model selection is central to statistics, and many learning problems can be
formulated as model selection problems. In this paper, we treat the problem of
selecting a maximum entropy model given various feature subsets and their
moments, as a model selection problem, and present a minimum description length
(MDL) formulation to solve this problem. For this, we derive normalized maximum
likelihood (NML) codelength for these models. Furthermore, we prove that the
minimax entropy principle is a special case of maximum entropy model selection,
where one assumes that complexity of all the models are equal. We apply our
approach to gene selection problem and present simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6441</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6441</id><created>2012-04-28</created><authors><author><keyname>Gayo-Avello</keyname><forenames>Daniel</forenames></author></authors><title>&quot;I Wanted to Predict Elections with Twitter and all I got was this Lousy
  Paper&quot; -- A Balanced Survey on Election Prediction using Twitter Data</title><categories>cs.CY cs.CL cs.SI physics.soc-ph</categories><comments>13 pages, no figures. Annotated bibliography of 25 papers regarding
  electoral prediction from Twitter data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting X from Twitter is a popular fad within the Twitter research
subculture. It seems both appealing and relatively easy. Among such kind of
studies, electoral prediction is maybe the most attractive, and at this moment
there is a growing body of literature on such a topic. This is not only an
interesting research problem but, above all, it is extremely difficult.
However, most of the authors seem to be more interested in claiming positive
results than in providing sound and reproducible methods. It is also especially
worrisome that many recent papers seem to only acknowledge those studies
supporting the idea of Twitter predicting elections, instead of conducting a
balanced literature review showing both sides of the matter. After reading many
of such papers I have decided to write such a survey myself. Hence, in this
paper, every study relevant to the matter of electoral prediction using social
media is commented. From this review it can be concluded that the predictive
power of Twitter regarding elections has been greatly exaggerated, and that
hard research problems still lie ahead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6445</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6445</id><created>2012-04-28</created><updated>2013-07-22</updated><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Williams</keyname><forenames>Tyson</forenames></author></authors><title>A Complete Dichotomy Rises from the Capture of Vanishing Signatures</title><categories>cs.CC</categories><comments>58 pages, 14 figures, extended abstract appeared at STOC 2013, new in
  this arXiv version: added section 3, greatly improved section 8.1, many small
  fixes</comments><msc-class>68Q17</msc-class><acm-class>F.1.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a complexity dichotomy theorem for Holant problems over an arbitrary
set of complex-valued symmetric constraint functions F on Boolean variables.
This extends and unifies all previous dichotomies for Holant problems on
symmetric constraint functions (taking values without a finite modulus). We
define and characterize all symmetric vanishing signatures. They turned out to
be essential to the complete classification of Holant problems. The dichotomy
theorem has an explicit tractability criterion expressible in terms of
holographic transformations. A Holant problem defined by a set of constraint
functions F is solvable in polynomial time if it satisfies this tractability
criterion, and is #P-hard otherwise. The tractability criterion can be
intuitively stated as follows: A set F is tractable if (1) every function in F
has arity at most two, or (2) F is transformable to an affine type, or (3) F is
transformable to a product type, or (4) F is vanishing, combined with the right
type of binary functions, or (5) F belongs to a special category of vanishing
type Fibonacci gates. The proof of this theorem utilizes many previous
dichotomy theorems on Holant problems and Boolean #CSP. Holographic
transformations play an indispensable role as both a proof technique and in the
statement of the tractability criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6447</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6447</id><created>2012-04-28</created><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author></authors><title>Open Problems in Analysis of Boolean Functions</title><categories>cs.DM math.CO math.PR</categories><comments>27 problems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A list of open problems in the field of analysis of boolean functions,
compiled February 2012 for the Simons Symposium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6453</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6453</id><created>2012-04-29</created><authors><author><keyname>Arslan</keyname><forenames>Oktay</forenames></author><author><keyname>Tsiotras</keyname><forenames>Panagiotis</forenames></author></authors><title>The Role of Vertex Consistency in Sampling-based Algorithms for Optimal
  Motion Planning</title><categories>cs.RO</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion planning problems have been studied by both the robotics and the
controls research communities for a long time, and many algorithms have been
developed for their solution. Among them, incremental sampling-based motion
planning algorithms, such as the Rapidly-exploring Random Trees (RRTs), and the
Probabilistic Road Maps (PRMs) have become very popular recently, owing to
their implementation simplicity and their advantages in handling
high-dimensional problems. Although these algorithms work very well in
practice, the quality of the computed solution is often not good, i.e., the
solution can be far from the optimal one. A recent variation of RRT, namely the
RRT* algorithm, bypasses this drawback of the traditional RRT algorithm, by
ensuring asymptotic optimality as the number of samples tends to infinity.
Nonetheless, the convergence rate to the optimal solution may still be slow.
This paper presents a new incremental sampling-based motion planning algorithm
based on Rapidly-exploring Random Graphs (RRG), denoted RRT# (RRT &quot;sharp&quot;)
which also guarantees asymptotic optimality but, in addition, it also ensures
that the constructed spanning tree of the geometric graph is consistent after
each iteration. In consistent trees, the vertices which have the potential to
be part of the optimal solution have the minimum cost-come-value. This implies
that the best possible solution is readily computed if there are some vertices
in the current graph that are already in the goal region. Numerical results
compare with the RRT* algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6455</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6455</id><created>2012-04-29</created><authors><author><keyname>Durand</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LINCS, INRIA Rocquencourt</affiliation></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames><affiliation>LINCS, INRIA Rocquencourt</affiliation></author><author><keyname>Noirie</keyname><forenames>Ludovic</forenames><affiliation>LINCS</affiliation></author></authors><title>On the Manipulability of Voting Systems: Application to Multi-Carrier
  Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>N&amp;deg; 2012-04-001 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, Internet involves many actors who are making revenues on it
(operators, companies, service providers,...). It is therefore important to be
able to make fair decisions in this large-scale and highly competitive
economical ecosystem. One of the main issues is to prevent actors from
manipulating the natural outcome of the decision process. For that purpose,
game theory is a natural framework. In that context, voting systems represent
an interesting alternative that, to our knowledge, has not yet been considered.
They allow competing entities to decide among different options. Strong
theoretical results showed that all voting systems are susceptible to be
manipulated by one single voter, except for some &quot;degenerated&quot; and
non-acceptable cases. However, very little is known about how much a voting
system is manipulable in practical scenarios. In this paper, we investigate
empirically the use of voting systems for choosing end-to-end paths in
multi-carrier networks, analyzing their manipulability and their economical
efficiency. We show that one particular system, called \Single Transferable
Vote (STV), is largely more resistant to manipulability than the natural system
which tries to get the economical optimum. Moreover, STV manages to select
paths close to the economical optimum, whether the participants try to cheat or
not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6458</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6458</id><created>2012-04-29</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Chan</keyname><forenames>Kap Luk</forenames></author></authors><title>Active Contour with A Tangential Component</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Conventional edge-based active contours often require the normal component of
an edge indicator function on the optimal contours to approximate zero, while
the tangential component can still be significant. In real images, the full
gradients of the edge indicator function along the object boundaries are often
small. Hence, the curve evolution of edge-based active contours can terminate
early before converging to the object boundaries with a careless contour
initialization. We propose a novel Geodesic Snakes (GeoSnakes) active contour
that requires the full gradients of the edge indicator to vanish at the optimal
solution. Besides, the conventional curve evolution approach for minimizing
active contour energy cannot fully solve the Euler-Lagrange (EL) equation of
our GeoSnakes active contour, causing a Pseudo Stationary Phenomenon (PSP). To
address the PSP problem, we propose an auxiliary curve evolution equation,
named the equilibrium flow (EF) equation. Based on the EF and the conventional
curve evolution, we obtain a solution to the full EL equation of GeoSnakes
active contour. Experimental results validate the proposed geometrical
interpretation of the early termination problem, and they also show that the
proposed method overcomes the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6459</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6459</id><created>2012-04-29</created><authors><author><keyname>Gauthier</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>A Distinguisher-Based Attack on a Variant of McEliece's Cryptosystem
  Based on Reed-Solomon Codes</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.6686</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Baldi et \textit{al.} proposed a variant of McEliece's cryptosystem. The main
idea is to replace its permutation matrix by adding to it a rank 1 matrix. The
motivation for this change is twofold: it would allow the use of codes that
were shown to be insecure in the original McEliece's cryptosystem, and it would
reduce the key size while keeping the same security against generic decoding
attacks. The authors suggest to use generalized Reed-Solomon codes instead of
Goppa codes. The public code built with this method is not anymore a
generalized Reed-Solomon code. On the other hand, it contains a very large
secret generalized Reed-Solomon code. In this paper we present an attack that
is built upon a distinguisher which is able to identify elements of this secret
code. The distinguisher is constructed by considering the code generated by
component-wise products of codewords of the public code (the so-called &quot;square
code&quot;). By using square-code dimension considerations, the initial generalized
Reed-Solomon code can be recovered which permits to decode any ciphertext. A
similar technique has already been successful for mounting an attack against a
homomorphic encryption scheme suggested by Bogdanoc et \textit{al.}. This work
can be viewed as another illustration of how a distinguisher of Reed-Solomon
codes can be used to devise an attack on cryptosystems based on them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6482</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6482</id><created>2012-04-29</created><authors><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Koh</keyname><forenames>Chung Ha</forenames></author></authors><title>Tradeoff Analysis of Delay-Power-CSIT Quality of Dynamic BackPressure
  Algorithm for Energy Efficient OFDM Systems</title><categories>cs.SY</categories><comments>30 pages</comments><journal-ref>IEEE Transactions on Signal Processing 2012</journal-ref><doi>10.1109/TSP.2012.2198817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the fundamental power-delay tradeoff in
point-to-point OFDM systems under imperfect channel state information quality
and non-ideal circuit power. We consider the dynamic back- pressure (DBP)
algorithm, where the transmitter determines the rate and power control actions
based on the instantaneous channel state information (CSIT) and the queue state
information (QSI). We exploit a general fluid queue dynamics using a continuous
time dynamic equation. Using the sample-path approach and renewal theory, we
decompose the average delay in terms of multiple unfinished works along a
sample path, and derive an upper bound on the average delay under the DBP power
control, which is asymptotically accurate at small delay regime. We show that
despite imperfect CSIT quality and non-ideal circuit power, the average power
(P) of the DBP policy scales with delay (D) as P = O(Dexp(1/D)) at small delay
regime. While the impacts of CSIT quality and circuit power appears as the
coefficients of the scaling law, they may be significant in some operating
regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6484</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6484</id><created>2012-04-29</created><authors><author><keyname>Feige</keyname><forenames>Uriel</forenames></author><author><keyname>Jozeph</keyname><forenames>Shlomo</forenames></author></authors><title>Universal Factor Graphs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The factor graph of an instance of a symmetric constraint satisfaction
problem on n Boolean variables and m constraints (CSPs such as k-SAT, k-AND,
k-LIN) is a bipartite graph describing which variables appear in which
constraints. The factor graph describes the instance up to the polarity of the
variables, and hence there are up to 2km instances of the CSP that share the
same factor graph. It is well known that factor graphs with certain structural
properties make the underlying CSP easier to either solve exactly (e.g., for
tree structures) or approximately (e.g., for planar structures). We are
interested in the following question: is there a factor graph for which if one
can solve every instance of the CSP with this particular factor graph, then one
can solve every instance of the CSP regardless of the factor graph (and
similarly, for approximation)? We call such a factor graph universal. As one
needs different factor graphs for different values of n and m, this gives rise
to the notion of a family of universal factor graphs. We initiate a systematic
study of universal factor graphs, and present some results for max-kSAT. Our
work has connections with the notion of preprocessing as previously studied for
closest codeword and closest lattice-vector problems, with proofs for the PCP
theorem, and with tests for the long code. Many questions remain open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6492</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6492</id><created>2012-04-29</created><authors><author><keyname>Pessoa</keyname><forenames>Tiago</forenames></author><author><keyname>Abreu</keyname><forenames>Fernando Brito e</forenames></author><author><keyname>Monteiro</keyname><forenames>Miguel Pessoa</forenames></author><author><keyname>Bryton</keyname><forenames>Sergio</forenames></author></authors><title>An Eclipse Plugin to Support Code Smells Detection</title><categories>cs.SE cs.PL</categories><comments>12 pages, 7 figures</comments><acm-class>D.2.3; D.2.7</acm-class><journal-ref>INFORUM'2011 conference proceedings, Luis Caires e Raul Barbosa
  (eds.), 8-9 September, Coimbra, Portugal, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eradication of code smells is often pointed out as a way to improve
readability, extensibility and design in existing software. However, code smell
detection in large systems remains time consuming and error-prone, partly due
to the inherent subjectivity of the detection processes presently available. In
view of mitigating the subjectivity problem, this paper presents a tool that
automates a technique for the detection and assessment of code smells in Java
source code, developed as an Eclipse plug-in. The technique is based upon a
Binary Logistic Regression model and calibrated by expert's knowledge. A short
overview of the technique is provided and the tool is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6508</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6508</id><created>2012-04-29</created><updated>2012-05-27</updated><authors><author><keyname>Sharma</keyname><forenames>Neeraj</forenames></author><author><keyname>Sen</keyname><forenames>Sandeep</forenames></author></authors><title>Efficient cache oblivious algorithms for randomized divide-and-conquer
  on the multicore model</title><categories>cs.DS cs.CG cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present randomized algorithms for sorting and convex hull
that achieves optimal performance (for speed-up and cache misses) on the
multicore model with private cache model. Our algorithms are cache oblivious
and generalize the randomized divide and conquer strategy given by Reischuk and
Reif and Sen. Although the approach yielded optimal speed-up in the PRAM model,
we require additional techniques to optimize cache-misses in an oblivious
setting. Under a mild assumption on input and number of processors our
algorithm will have optimal time and cache misses with high probability.
Although similar results have been obtained recently for sorting, we feel that
our approach is simpler and general and we apply it to obtain an optimal
parallel algorithm for 3D convex hulls with similar bounds. We also present a
simple randomized processor allocation technique without the explicit knowledge
of the number of processors that is likely to find additional applications in
resource oblivious environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6509</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6509</id><created>2012-04-29</created><authors><author><keyname>Conan-Guez</keyname><forenames>Brieuc</forenames><affiliation>LITA</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Dissimilarity Clustering by Hierarchical Multi-Level Refinement</title><categories>stat.ML cs.LG</categories><comments>20-th European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN 2012), Bruges : Belgium (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce in this paper a new way of optimizing the natural extension of
the quantization error using in k-means clustering to dissimilarity data. The
proposed method is based on hierarchical clustering analysis combined with
multi-level heuristic refinement. The method is computationally efficient and
achieves better quantization errors than the
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6512</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6512</id><created>2012-04-29</created><authors><author><keyname>Wang</keyname><forenames>Bei</forenames></author><author><keyname>Miller</keyname><forenames>Greg</forenames></author><author><keyname>Colella</keyname><forenames>Phil</forenames></author></authors><title>An adaptive, high-order phase-space remapping for the two-dimensional
  Vlasov-Poisson equations</title><categories>math.NA cs.CE physics.comp-ph</categories><journal-ref>SIAM Journal on Scientific Computing, 34(6), 2012</journal-ref><doi>10.1137/120872954</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The numerical solution of high dimensional Vlasov equation is usually
performed by particle-in-cell (PIC) methods. However, due to the well-known
numerical noise, it is challenging to use PIC methods to get a precise
description of the distribution function in phase space. To control the
numerical error, we introduce an adaptive phase-space remapping which
regularizes the particle distribution by periodically reconstructing the
distribution function on a hierarchy of phase-space grids with high-order
interpolations. The positivity of the distribution function can be preserved by
using a local redistribution technique. The method has been successfully
applied to a set of classical plasma problems in one dimension. In this paper,
we present the algorithm for the two dimensional Vlasov-Poisson equations. An
efficient Poisson solver with infinite domain boundary conditions is used. The
parallel scalability of the algorithm on massively parallel computers will be
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6521</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6521</id><created>2012-04-29</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author></authors><title>Harnessing Folksonomies for Resource Classification</title><categories>cs.DL cs.IR cs.SI</categories><comments>PhD thesis</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In our daily lives, organizing resources into a set of categories is a common
task. Categorization becomes more useful as the collection of resources
increases. Large collections of books, movies, and web pages, for instance, are
cataloged in libraries, organized in databases and classified in directories,
respectively. However, the usual largeness of these collections requires a vast
endeavor and an outrageous expense to organize manually.
  Recent research is moving towards developing automated classifiers that
reduce the increasing costs and effort of the task. Little work has been done
analyzing the appropriateness of and exploring how to harness the annotations
provided by users on social tagging systems as a data source. Users on these
systems save resources as bookmarks in a social environment by attaching
annotations in the form of tags. It has been shown that these tags facilitate
retrieval of resources not only for the annotators themselves but also for the
whole community. Likewise, these tags provide meaningful metadata that refers
to the content of the resources.
  In this thesis, we deal with the utilization of these user-provided tags in
search of the most accurate classification of resources as compared to
expert-driven categorizations. To the best of our knowledge, this is the first
research work performing actual classification experiments utilizing social
tags. By exploring the characteristics and nature of these systems and the
underlying folksonomies, this thesis sheds new light on the way of getting the
most out of social tags for the sake of automated resource classification
tasks. Therefore, we believe that the contributions in this work are of utmost
interest for future researchers in the field, as well as for the scientific
community in order to better understand these systems and further utilize the
knowledge garnered from social tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6527</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6527</id><created>2012-04-29</created><updated>2014-10-22</updated><authors><author><keyname>Despotakis</keyname><forenames>Stylianos C.</forenames></author><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author></authors><title>An upper bound on Euclidean embeddings of rigid graphs with 8 vertices</title><categories>cs.CG</categories><comments>This paper has been withdrawn by the authors because there was a bug
  in the code used to produce the sub-systems in Section 4. More specifically
  some of the sub-systems don't satisfy the Laman property. For an update on
  this problem, see arXiv:1402.1484</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is called (generically) rigid in R^d if, for any choice of
sufficiently generic edge lengths, it can be embedded in R^d in a finite number
of distinct ways, modulo rigid transformations. Here, we deal with the problem
of determining the maximum number of planar Euclidean embeddings of minimally
rigid graphs with 8 vertices, because this is the smallest unknown case in the
plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6529</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6529</id><created>2012-04-29</created><updated>2013-01-20</updated><authors><author><keyname>Gwynne</keyname><forenames>Matthew</forenames></author><author><keyname>Kullmann</keyname><forenames>Oliver</forenames></author></authors><title>Generalising unit-refutation completeness and SLUR via nested input
  resolution</title><categories>cs.LO cs.AI</categories><comments>41 pages; second version improved formulations and added examples,
  and more details regarding future directions, third version further examples,
  improved and extended explanations, and more on SLUR, fourth version various
  additional remarks and editorial improvements, fifth version more
  explanations and references, typos corrected, improved wording</comments><journal-ref>Journal of Automated Reasoning 52(1): 31-65 (2014)</journal-ref><doi>10.1007/s10817-013-9275-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two hierarchies of clause-sets, SLUR_k and UC_k, based on the
classes SLUR (Single Lookahead Unit Refutation), introduced in 1995, and UC
(Unit refutation Complete), introduced in 1994.
  The class SLUR, introduced in [Annexstein et al, 1995], is the class of
clause-sets for which unit-clause-propagation (denoted by r_1) detects
unsatisfiability, or where otherwise iterative assignment, avoiding obviously
false assignments by look-ahead, always yields a satisfying assignment. It is
natural to consider how to form a hierarchy based on SLUR. Such investigations
were started in [Cepek et al, 2012] and [Balyo et al, 2012]. We present what we
consider the &quot;limit hierarchy&quot; SLUR_k, based on generalising r_1 by r_k, that
is, using generalised unit-clause-propagation introduced in [Kullmann, 1999,
2004].
  The class UC, studied in [Del Val, 1994], is the class of Unit refutation
Complete clause-sets, that is, those clause-sets for which unsatisfiability is
decidable by r_1 under any falsifying assignment. For unsatisfiable clause-sets
F, the minimum k such that r_k determines unsatisfiability of F is exactly the
&quot;hardness&quot; of F, as introduced in [Ku 99, 04]. For satisfiable F we use now an
extension mentioned in [Ansotegui et al, 2008]: The hardness is the minimum k
such that after application of any falsifying partial assignments, r_k
determines unsatisfiability. The class UC_k is given by the clause-sets which
have hardness &lt;= k. We observe that UC_1 is exactly UC.
  UC_k has a proof-theoretic character, due to the relations between hardness
and tree-resolution, while SLUR_k has an algorithmic character. The
correspondence between r_k and k-times nested input resolution (or tree
resolution using clause-space k+1) means that r_k has a dual nature: both
algorithmic and proof theoretic. This corresponds to a basic result of this
paper, namely SLUR_k = UC_k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6535</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6535</id><created>2012-04-29</created><updated>2014-08-30</updated><authors><author><keyname>Gupta</keyname><forenames>Sandeep</forenames></author></authors><title>Citations, Sequence Alignments, Contagion, and Semantics: On Acyclic
  Structures and their Randomness</title><categories>cs.DM cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datasets from several domains, such as life-sciences, semantic web, machine
learning, natural language processing, etc. are naturally structured as acyclic
graphs. These datasets, particularly those in bio-informatics and computational
epidemiology, have grown tremendously over the last decade or so. Increasingly,
as a consequence, there is a need to build and evaluate various strategies for
processing acyclic structured graphs. Most of the proposed research models the
real world acyclic structures as random graphs, i.e., they are generated by
randomly selecting a subset of edges from all possible edges. Unfortunately the
graphs thus generated have predictable and degenerate structures, i.e., the
resulting graphs will always have almost the same degree distribution and very
short paths.
  Specifically, we show that if $O(n \log n \log n)$ edges are added to a
binary tree of $n$ nodes then with probability more than $O(1/(\log n)^{1/n})$
the depth of all but $O({\log \log n} ^{\log \log n})$ vertices of the dag
collapses to 1. Experiments show that irregularity, as measured by distribution
of length of random walks from root to leaves, is also predictable and small.
The degree distribution and random walk length properties of real world graphs
from these domains are significantly different from random graphs of similar
vertex and edge size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6537</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6537</id><created>2012-04-29</created><authors><author><keyname>Mardani</keyname><forenames>Morteza</forenames></author><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Recovery of Low-Rank Plus Compressed Sparse Matrices with Application to
  Unveiling Traffic Anomalies</title><categories>cs.IT cs.NI math.IT stat.ML</categories><comments>38 pages, submitted to the IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2013.2257913</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the superposition of a low-rank matrix plus the product of a known fat
compression matrix times a sparse matrix, the goal of this paper is to
establish deterministic conditions under which exact recovery of the low-rank
and sparse components becomes possible. This fundamental identifiability issue
arises with traffic anomaly detection in backbone networks, and subsumes
compressed sensing as well as the timely low-rank plus sparse matrix recovery
tasks encountered in matrix decomposition problems. Leveraging the ability of
$\ell_1$- and nuclear norms to recover sparse and low-rank matrices, a convex
program is formulated to estimate the unknowns. Analysis and simulations
confirm that the said convex program can recover the unknowns for sufficiently
low-rank and sparse enough components, along with a compression matrix
possessing an isometry property when restricted to operate on sparse vectors.
When the low-rank, sparse, and compression matrices are drawn from certain
random ensembles, it is established that exact recovery is possible with high
probability. First-order algorithms are developed to solve the nonsmooth convex
optimization problem with provable iteration complexity guarantees. Insightful
tests with synthetic and real network data corroborate the effectiveness of the
novel approach in unveiling traffic anomalies across flows and time, and its
ability to outperform existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6549</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6549</id><created>2012-04-30</created><authors><author><keyname>O'Neale</keyname><forenames>D. R. J.</forenames></author><author><keyname>Hendy</keyname><forenames>S. C.</forenames></author></authors><title>Power Law Distributions of Patents as Indicators of Innovation</title><categories>physics.soc-ph cs.SI</categories><doi>10.1371/journal.pone.0049501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total number of patents produced by a country (or the number of patents
produced per capita) is often used as an indicator for innovation. Here we
present evidence that the distribution of patents amongst applicants within
many OECD countries is well-described by power laws with exponents that vary
between 1.66 (Japan) and 2.37 (Poland). Using simulations based on simple
preferential attachment-type rules that generate power laws, we find we can
explain some of the variation in exponents between countries, with countries
that have larger numbers of patents per applicant generally exhibiting smaller
exponents in both the simulated and actual data. Similarly we find that the
exponents for most countries are inversely correlated with other indicators of
innovation, such as R&amp;D intensity or the ubiquity of export baskets. This
suggests that in more advanced economies, which tend to have smaller values of
the exponent, a greater proportion of the total number of patents are filed by
large companies than in less advanced countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6552</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6552</id><created>2012-04-30</created><updated>2013-01-30</updated><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Katz</keyname><forenames>Jonathan</forenames></author><author><keyname>Mukherjee</keyname><forenames>Koyel</forenames></author></authors><title>A Game-Theoretic Model Motivated by the DARPA Network Challenge</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a game-theoretic model to analyze events similar to
the 2009 \emph{DARPA Network Challenge}, which was organized by the Defense
Advanced Research Projects Agency (DARPA) for exploring the roles that the
Internet and social networks play in incentivizing wide-area collaborations.
The challenge was to form a group that would be the first to find the locations
of ten moored weather balloons across the United States. We consider a model in
which $N$ people (who can form groups) are located in some topology with a
fixed coverage volume around each person's geographical location. We consider
various topologies where the players can be located such as the Euclidean
$d$-dimension space and the vertices of a graph. A balloon is placed in the
space and a group wins if it is the first one to report the location of the
balloon. A larger team has a higher probability of finding the balloon, but we
assume that the prize money is divided equally among the team members. Hence
there is a competing tension to keep teams as small as possible.
  \emph{Risk aversion} is the reluctance of a person to accept a bargain with
an uncertain payoff rather than another bargain with a more certain, but
possibly lower, expected payoff. In our model we consider the \emph{isoelastic}
utility function derived from the Arrow-Pratt measure of relative risk
aversion. The main aim is to analyze the structures of the groups in Nash
equilibria for our model. For the $d$-dimensional Euclidean space ($d\geq 1$)
and the class of bounded degree regular graphs we show that in any Nash
Equilibrium the \emph{richest} group (having maximum expected utility per
person) covers a constant fraction of the total volume.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6563</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6563</id><created>2012-04-30</created><updated>2012-05-02</updated><authors><author><keyname>Kaliamoorthi</keyname><forenames>Prabhu</forenames></author><author><keyname>Kakarala</keyname><forenames>Ramakrishna</forenames></author></authors><title>Parametric annealing: a stochastic search method for human pose tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model based methods to marker-free motion capture have a very high
computational overhead that make them unattractive. In this paper we describe a
method that improves on existing global optimization techniques to tracking
articulated objects. Our method improves on the state-of-the-art Annealed
Particle Filter (APF) by reusing samples across annealing layers and by using
an adaptive parametric density for diffusion. We compare the proposed method
with APF on a scalable problem and study how the two methods scale with the
dimensionality, multi-modality and the range of search. Then we perform
sensitivity analysis on the parameters of our algorithm and show that it
tolerates a wide range of parameter settings. We also show results on tracking
human pose from the widely-used Human Eva I dataset. Our results show that the
proposed method reduces the tracking error despite using less than 50% of the
computational resources as APF. The tracked output also shows a significant
qualitative improvement over APF as demonstrated through image and video
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6564</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6564</id><created>2012-04-30</created><updated>2013-01-01</updated><authors><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Fiorina</keyname><forenames>Jocelyn</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author></authors><title>A New Family of Low-Complexity STBCs for Four Transmit Antennas</title><categories>cs.IT math.IT</categories><comments>12 pages, 8 figures, 5 tables, accepted for publication in IEEE
  Transactions on Wireless Communications. arXiv admin note: substantial text
  overlap with arXiv:1204.4000</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space-Time Block Codes (STBCs) suffer from a prohibitively high decoding
complexity unless the low-complexity decodability property is taken into
consideration in the STBC design. For this purpose, several families of STBCs
that involve a reduced decoding complexity have been proposed, notably the
multi-group decodable and the fast decodable (FD) codes. Recently, a new family
of codes that combines both of these families namely the fast group decodable
(FGD) codes was proposed. In this paper, we propose a new construction scheme
for rate-1 FGD codes for 2^a transmit antennas. The proposed scheme is then
applied to the case of four transmit antennas and we show that the new rate-1
FGD code has the lowest worst-case decoding complexity among existing
comparable STBCs. The coding gain of the new rate-1 code is optimized through
constellation stretching and proved to be constant irrespective of the
underlying QAM constellation prior to normalization. Next, we propose a new
rate-2 FD STBC by multiplexing two of our rate-1 codes by the means of a
unitary matrix. Also a compromise between rate and complexity is obtained
through puncturing our rate-2 FD code giving rise to a new rate-3/2 FD code.
The proposed codes are compared to existing codes in the literature and
simulation results show that our rate-3/2 code has a lower average decoding
complexity while our rate-2 code maintains its lower average decoding
complexity in the low SNR region. If a time-out sphere decoder is employed, our
proposed codes outperform existing codes at high SNR region thanks to their
lower worst-case decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6583</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6583</id><created>2012-04-30</created><authors><author><keyname>Kanamori</keyname><forenames>Takafumi</forenames></author><author><keyname>Takeda</keyname><forenames>Akiko</forenames></author><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author></authors><title>A Conjugate Property between Loss Functions and Uncertainty Sets in
  Classification Problems</title><categories>stat.ML cs.LG</categories><comments>41 pages, 4 figures. The shorter version is accepted by COLT2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In binary classification problems, mainly two approaches have been proposed;
one is loss function approach and the other is uncertainty set approach. The
loss function approach is applied to major learning algorithms such as support
vector machine (SVM) and boosting methods. The loss function represents the
penalty of the decision function on the training samples. In the learning
algorithm, the empirical mean of the loss function is minimized to obtain the
classifier. Against a backdrop of the development of mathematical programming,
nowadays learning algorithms based on loss functions are widely applied to
real-world data analysis. In addition, statistical properties of such learning
algorithms are well-understood based on a lots of theoretical works. On the
other hand, the learning method using the so-called uncertainty set is used in
hard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. In
the learning algorithm, firstly, the uncertainty set is defined for each binary
label based on the training samples. Then, the best separating hyperplane
between the two uncertainty sets is employed as the decision function. This is
regarded as an extension of the maximum-margin approach. The uncertainty set
approach has been studied as an application of robust optimization in the field
of mathematical programming. The statistical properties of learning algorithms
with uncertainty sets have not been intensively studied. In this paper, we
consider the relation between the above two approaches. We point out that the
uncertainty set is described by using the level set of the conjugate of the
loss function. Based on such relation, we study statistical properties of
learning algorithms using uncertainty sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6588</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6588</id><created>2012-04-30</created><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Karnin</keyname><forenames>Zohar</forenames></author></authors><title>A note on: No need to choose: How to get both a PTAS and Sublinear Query
  Complexity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit various PTAS's (Polynomial Time Approximation Schemes) for
minimization versions of dense problems, and show that they can be performed
with sublinear query complexity. This means that not only do we obtain a
(1+eps)-approximation to the NP-Hard problems in polynomial time, but also
avoid reading the entire input. This setting is particularly advantageous when
the price of reading parts of the input is high, as is the case, for examples,
where humans provide the input. Trading off query complexity with approximation
is the raison d'etre of the field of learning theory, and of the ERM (Empirical
Risk Minimization) setting in particular. A typical ERM result, however, does
not deal with computational complexity. We discuss two particular problems for
which (a) it has already been shown that sublinear querying is sufficient for
obtaining a (1 + eps)-approximation using unlimited computational power (an ERM
result), and (b) with full access to input, we could get a
(1+eps)-approximation in polynomial time (a PTAS). Here we show that neither
benefit need be sacrificed. We get a PTAS with efficient query complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6610</identifier>
 <datestamp>2013-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6610</id><created>2012-04-30</created><authors><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Cao</keyname><forenames>Xiao-Qin</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author></authors><title>Residual Belief Propagation for Topic Modeling</title><categories>cs.LG cs.IR</categories><comments>6 pages, 8 figures</comments><journal-ref>Advanced Data Mining and Applications Lecture Notes in Computer
  Science Volume 7713, 739-752, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast convergence speed is a desired property for training latent Dirichlet
allocation (LDA), especially in online and parallel topic modeling for massive
data sets. This paper presents a novel residual belief propagation (RBP)
algorithm to accelerate the convergence speed for training LDA. The proposed
RBP uses an informed scheduling scheme for asynchronous message passing, which
passes fast-convergent messages with a higher priority to influence those
slow-convergent messages at each learning iteration. Extensive empirical
studies confirm that RBP significantly reduces the training time until
convergence while achieves a much lower predictive perplexity than other
state-of-the-art training algorithms for LDA, including variational Bayes (VB),
collapsed Gibbs sampling (GS), loopy belief propagation (BP), and residual VB
(RVB).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6615</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6615</id><created>2012-04-30</created><updated>2012-05-01</updated><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Escape to Mizar for ATPs</title><categories>cs.LO</categories><comments>10 pages. Submitted to PAAR 2012 (Practical Aspects of Automated
  Reasoning, Manchester, June 2012,
  http://www.eprover.org/EVENTS/PAAR-2012.html)</comments><msc-class>03F07, 68T15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We announce a tool for mapping derivations of the E theorem prover to Mizar
proofs. Our mapping complements earlier work that generates problems for
automated theorem provers from Mizar inference checking problems. We describe
the tool, explain the mapping, and show how we solved some of the difficulties
that arise in mapping proofs between different logical formalisms, even when
they are based on the same notion of logical consequence, as Mizar and E are
(namely, first-order classical logic with identity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6623</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6623</id><created>2012-04-30</created><authors><author><keyname>Gilad</keyname><forenames>Yossi</forenames></author><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author></authors><title>Off-Path Attacking the Web</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how an off-path (spoofing-only) attacker can perform cross-site
scripting (XSS), cross-site request forgery (CSRF) and site spoofing/defacement
attacks, without requiring vulnerabilities in either web-browser or server and
circumventing known defenses. Attacker can also launch devastating denial of
service (DoS) attacks, even when the connection between the client and the
server is secured with SSL/TLS. The attacks are practical and require a puppet
(malicious script in browser sandbox) running on a the victim client machine,
and attacker capable of IP-spoofing on the Internet. Our attacks use a
technique allowing an off-path attacker to learn the sequence numbers of both
client and server in a TCP connection. The technique exploits the fact that
many computers, in particular those running Windows, use a global IP-ID
counter, which provides a side channel allowing efficient exposure of the
connection sequence numbers. We present results of experiments evaluating the
learning technique and the attacks that exploit it. Finally, we present
practical defenses that can be deployed at the firewall level; no changes to
existing TCP/IP stacks are required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6628</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6628</id><created>2012-04-30</created><authors><author><keyname>Licari</keyname><forenames>Daniele</forenames></author><author><keyname>Calzolari</keyname><forenames>Federico</forenames></author></authors><title>The Anatomy of a Grid portal</title><categories>cs.DC</categories><comments>6 pages</comments><acm-class>D.2.2</acm-class><journal-ref>Journal of Physics: Conference Series 331 (2011) 072043</journal-ref><doi>10.1088/1742-6596/331/7/072043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new way to deal with Grid portals referring to
our implementation. L-GRID is a light portal to access the EGEE/EGI Grid
infrastructure via Web, allowing users to submit their jobs from a common Web
browser in a few minutes, without any knowledge about the Grid infrastructure.
It provides the control over the complete lifecycle of a Grid Job, from its
submission and status monitoring, to the output retrieval. The system,
implemented as client-server architecture, is based on the Globus Grid
middleware. The client side application is based on a java applet; the server
relies on a Globus User Interface. There is no need of user registration on the
server side, and the user needs only his own X.509 personal certificate. The
system is user-friendly, secure (it uses SSL protocol, mechanism for dynamic
delegation and identity creation in public key infrastructures), highly
customizable, open source, and easy to install. The X.509 personal certificate
does not get out from the local machine. It allows to reduce the time spent for
the job submission, granting at the same time a higher efficiency and a better
security level in proxy delegation and management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6629</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6629</id><created>2012-04-30</created><authors><author><keyname>Calzolari</keyname><forenames>Federico</forenames></author><author><keyname>Licari</keyname><forenames>Daniele</forenames></author></authors><title>Proxy dynamic delegation in grid gateway</title><categories>cs.CR cs.DC</categories><comments>6 pages</comments><acm-class>D.4.6</acm-class><journal-ref>PoS(ISGC 2011 &amp; OGF 31)027</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays one of the main obstacles the research comes up against is the
difficulty in accessing the required computational resources. Grid is able to
offer the user a wide set of resources, even if they are often too hard to
exploit for non expert end user. Use simplification has today become a common
practice in the access and utilization of Cloud, Grid, and data center
resources. With the launch of L-GRID gateway, we introduced a new way to deal
with Grid portals. L-GRID is an extremely light portal developed in order to
access the EGI Grid infrastructure via Web, allowing users to submit their jobs
from whatever Web browser in a few minutes, without any knowledge about the
underlying Grid infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6631</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6631</id><created>2012-04-30</created><authors><author><keyname>Calzolari</keyname><forenames>Federico</forenames></author><author><keyname>Volpe</keyname><forenames>Silvia</forenames></author></authors><title>A new job migration algorithm to improve data center efficiency</title><categories>cs.DC</categories><comments>7 pages</comments><acm-class>D.4.7</acm-class><journal-ref>PoS(ISGC 2011 &amp; OGF 31)008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The under exploitation of the available resources risks to be one of the main
problems for a computing center. The growing demand of computational power
necessarily entails more complex approaches in the management of the computing
resources, with particular attention to the batch queue system scheduler. In a
heterogeneous batch queue system, available for both serial single core
processes and parallel multi core jobs, it may happen that one or more
computational nodes composing the cluster are not fully occupied, running a
number of jobs lower than their actual capability. A typical case is
represented by more single core jobs running each one over a different multi
core server, while more parallel jobs - requiring all the available cores of a
host - are queued. A job rearrangement executed at runtime is able to free
extra resources, in order to host new processes. We present an efficient method
to improve the computing resources exploitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6638</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6638</id><created>2012-04-30</created><authors><author><keyname>Yang</keyname><forenames>Jung-Hun</forenames></author><author><keyname>Ettema</keyname><forenames>Dick</forenames></author><author><keyname>Frenken</keyname><forenames>Koen</forenames></author></authors><title>Modelling the emergence of spatial patterns of economic activity</title><categories>cs.MA cs.SI physics.soc-ph q-fin.GN</categories><comments>Conference Proceeding in European Regional Science Association,
  Liverpool, England, August, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how spatial configurations of economic activity emerge is
important when formulating spatial planning and economic policy. A simple model
was proposed by Simon, who assumed that firms grow at a rate proportional to
their size, and that new divisions of firms with certain probabilities relocate
to other firms or to new centres of economic activity. Simon's model produces
realistic results in the sense that the sizes of economic centres follow a Zipf
distribution, which is also observed in reality. It lacks realism in the sense
that mechanisms such as cluster formation, congestion (defined as an overly
high density of the same activities) and dependence on the spatial distribution
of external parties (clients, labour markets) are ignored.
  The present paper proposed an extension of the Simon model that includes both
centripetal and centrifugal forces. Centripetal forces are included in the
sense that firm divisions are more likely to settle in locations that offer a
higher accessibility to other firms. Centrifugal forces are represented by an
aversion of a too high density of activities in the potential location. The
model is implemented as an agent-based simulation model in a simplified spatial
setting. By running both the Simon model and the extended model, comparisons
are made with respect to their effects on spatial configurations. To this end a
series of metrics are used, including the rank-size distribution and indices of
the degree of clustering and concentration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6645</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6645</id><created>2012-04-30</created><updated>2013-10-16</updated><authors><author><keyname>Conlon</keyname><forenames>David</forenames></author><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Zhao</keyname><forenames>Yufei</forenames></author></authors><title>Extremal results in sparse pseudorandom graphs</title><categories>math.CO cs.DM</categories><comments>70 pages, accepted for publication in Adv. Math</comments><journal-ref>Adv. Math. 256 (2014), 206-290</journal-ref><doi>10.1016/j.aim.2013.12.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Szemer\'edi's regularity lemma is a fundamental tool in extremal
combinatorics. However, the original version is only helpful in studying dense
graphs. In the 1990s, Kohayakawa and R\&quot;odl proved an analogue of Szemer\'edi's
regularity lemma for sparse graphs as part of a general program toward
extending extremal results to sparse graphs. Many of the key applications of
Szemer\'edi's regularity lemma use an associated counting lemma. In order to
prove extensions of these results which also apply to sparse graphs, it
remained a well-known open problem to prove a counting lemma in sparse graphs.
  The main advance of this paper lies in a new counting lemma, proved following
the functional approach of Gowers, which complements the sparse regularity
lemma of Kohayakawa and R\&quot;odl, allowing us to count small graphs in regular
subgraphs of a sufficiently pseudorandom graph. We use this to prove sparse
extensions of several well-known combinatorial theorems, including the removal
lemmas for graphs and groups, the Erd\H{o}s-Stone-Simonovits theorem and
Ramsey's theorem. These results extend and improve upon a substantial body of
previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6653</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6653</id><created>2012-04-30</created><authors><author><keyname>Katyal</keyname><forenames>Vini</forenames></author><author><keyname>Aviral</keyname></author><author><keyname>Srivastava</keyname><forenames>Deepesh</forenames></author></authors><title>Elimination of Glass Artifacts and Object Segmentation</title><categories>cs.CV</categories><doi>10.5120/6215-8919</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many images nowadays are captured from behind the glasses and may have
certain stains discrepancy because of glass and must be processed to make
differentiation between the glass and objects behind it. This research paper
proposes an algorithm to remove the damaged or corrupted part of the image and
make it consistent with other part of the image and to segment objects behind
the glass. The damaged part is removed using total variation inpainting method
and segmentation is done using kmeans clustering, anisotropic diffusion and
watershed transformation. The final output is obtained by interpolation. This
algorithm can be useful to applications in which some part of the images are
corrupted due to data transmission or needs to segment objects from an image
for further processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6662</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6662</id><created>2012-04-30</created><authors><author><keyname>Kallel</keyname><forenames>Emna</forenames></author><author><keyname>Aoudni</keyname><forenames>Yassine</forenames></author><author><keyname>Baklouti</keyname><forenames>Mouna</forenames></author><author><keyname>Abid</keyname><forenames>Mohamed</forenames></author></authors><title>Mppsocgen: A framework for automatic generation of mppsoc architecture</title><categories>cs.DC cs.AR</categories><comments>16 pages; International Journal of Computer Science &amp; Information
  Technology (IJCSIT) Vol 4, No 2, April 2012</comments><doi>10.5121/ijcsit.2012.4201</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Automatic code generation is a standard method in software engineering since
it improves the code consistency and reduces the overall development time. In
this context, this paper presents a design flow for automatic VHDL code
generation of mppSoC (massively parallel processing System-on-Chip)
configuration. Indeed, depending on the application requirements, a framework
of Netbeans Platform Software Tool named MppSoCGEN was developed in order to
accelerate the design process of complex mppSoC. Starting from an architecture
parameters design, VHDL code will be automatically generated using parsing
method. Configuration rules are proposed to have a correct and valid VHDL
syntax configuration. Finally, an automatic generation of Processor Elements
and network topologies models of mppSoC architecture will be done for Stratix
II device family. Our framework improves its flexibility on Netbeans 5.5
version and centrino duo Core 2GHz with 22 Kbytes and 3 seconds average
runtime. Experimental results for reduction algorithm validate our MppSoCGEN
design flow and demonstrate the efficiency of generated architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6671</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6671</id><created>2012-04-30</created><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund</forenames></author></authors><title>Delta-Decidability over the Reals</title><categories>cs.LO</categories><comments>A short version appears in LICS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given any collection F of computable functions over the reals, we show that
there exists an algorithm that, given any L_F-sentence \varphi containing only
bounded quantifiers, and any positive rational number \delta, decides either
&quot;\varphi is true&quot;, or &quot;a \delta-strengthening of \varphi is false&quot;. Under mild
assumptions, for a C-computable signature F, the \delta-decision problem for
bounded \Sigma_k-sentences in L_F resides in (\Sigma_k^P)^C. The results stand
in sharp contrast to the well-known undecidability results, and serve as a
theoretical basis for the use of numerical methods in decision procedures for
nonlinear first-order theories over the reals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6675</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6675</id><created>2012-04-30</created><authors><author><keyname>Barenboim</keyname><forenames>Leonid</forenames></author></authors><title>On the Locality of Some NP-Complete Problems</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the distributed message-passing {LOCAL} model. In this model a
communication network is represented by a graph where vertices host processors,
and communication is performed over the edges. Computation proceeds in
synchronous rounds. The running time of an algorithm is the number of rounds
from the beginning until all vertices terminate. Local computation is free. An
algorithm is called {local} if it terminates within a constant number of
rounds. The question of what problems can be computed locally was raised by
Naor and Stockmayer \cite{NS93} in their seminal paper in STOC'93. Since then
the quest for problems with local algorithms, and for problems that cannot be
computed locally, has become a central research direction in the field of
distributed algorithms \cite{KMW04,KMW10,LOW08,PR01}.
  We devise the first local algorithm for an {NP-complete} problem.
Specifically, our randomized algorithm computes, with high probability, an
O(n^{1/2 + epsilon} \cdot chi)-coloring within O(1) rounds, where epsilon &gt; 0
is an arbitrarily small constant, and chi is the chromatic number of the input
graph. (This problem was shown to be NP-complete in \cite{Z07}.) On our way to
this result we devise a constant-time algorithm for computing (O(1), O(n^{1/2 +
epsilon}))-network-decompositions. Network-decompositions were introduced by
Awerbuch et al. \cite{AGLP89}, and are very useful for solving various
distributed problems. The best previously-known algorithm for
network-decomposition has a polylogarithmic running time (but is applicable for
a wider range of parameters) \cite{LS93}. We also devise a Delta^{1 +
epsilon}-coloring algorithm for graphs with sufficiently large maximum degree
Delta that runs within O(1) rounds. It improves the best previously-known
result for this family of graphs, which is O(\log-star n) \cite{SW10}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6681</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6681</id><created>2012-04-30</created><authors><author><keyname>Hartnell</keyname><forenames>Bert L.</forenames></author><author><keyname>Rall</keyname><forenames>Douglas F.</forenames></author></authors><title>On the Cartesian product of non well-covered graphs</title><categories>math.CO cs.DM</categories><msc-class>05C69, 05C76</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is well-covered if every maximal independent set has the same
cardinality, namely the vertex independence number. We answer a question of
Topp and Volkmann and prove that if the Cartesian product of two graphs is
well-covered, then at least one of them must be well-covered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6687</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6687</id><created>2012-04-30</created><authors><author><keyname>Grytczuk</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Szafruga</keyname><forenames>Piotr</forenames></author><author><keyname>Zmarz</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Online version of the theorem of Thue</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequence S is nonrepetitive if no two adjacent blocks of S are the same. In
1906 Thue proved that there exist arbitrarily long nonrepetitive sequences over
3 symbols. We consider the online variant of this result in which a
nonrepetitive sequence is constructed during a play between two players: Bob is
choosing a position in a sequence and Alice is inserting a symbol on that
position taken from a fixed set A. The goal of Bob is to force Alice to create
a repetition, and if he succeeds, then the game stops. The goal of Alice is
naturally to avoid that and thereby to construct a nonrepetitive sequence of
any given length. We prove that Alice has a strategy to play arbitrarily long
provided the size of the set A is at least 12. This is the online version of
the Theorem of Thue. The proof is based on nonrepetitive colorings of
outerplanar graphs. On the other hand, one can prove that even over 4 symbols
Alice has no chance to play for too long. The minimum size of the set of
symbols needed for the online version of Thue's theorem remains unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6691</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6691</id><created>2012-04-30</created><authors><author><keyname>Lucanin</keyname><forenames>Drazen</forenames></author><author><keyname>Maurer</keyname><forenames>Michael</forenames></author><author><keyname>Mastelic</keyname><forenames>Toni</forenames></author><author><keyname>Brandic</keyname><forenames>Ivona</forenames></author></authors><title>Energy Efficient Service Delivery in Clouds in Compliance with the Kyoto
  Protocol</title><categories>cs.DC</categories><doi>10.1007/978-3-642-33645-4_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is revolutionizing the ICT landscape by providing scalable
and efficient computing resources on demand. The ICT industry - especially data
centers, are responsible for considerable amounts of CO2 emissions and will
very soon be faced with legislative restrictions, such as the Kyoto protocol,
defining caps at different organizational levels (country, industry branch
etc.) A lot has been done around energy efficient data centers, yet there is
very little work done in defining flexible models considering CO2. In this
paper we present a first attempt of modeling data centers in compliance with
the Kyoto protocol. We discuss a novel approach for trading credits for
emission reductions across data centers to comply with their constraints. CO2
caps can be integrated with Service Level Agreements and juxtaposed to other
computing commodities (e.g. computational power, storage), setting a foundation
for implementing next-generation schedulers and pricing models that support
Kyoto-compliant CO2 trading schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6695</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6695</id><created>2012-04-30</created><updated>2014-03-31</updated><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Merry</keyname><forenames>Alex</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Soloviev</keyname><forenames>Matvey</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Pattern graph rewrite systems</title><categories>math.CT cs.LO</categories><comments>In Proceedings DCM 2012, arXiv:1403.7579</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 143, 2014, pp. 54-66</journal-ref><doi>10.4204/EPTCS.143.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  String diagrams are a powerful tool for reasoning about physical processes,
logic circuits, tensor networks, and many other compositional structures.
Dixon, Duncan and Kissinger introduced string graphs, which are a combinatoric
representations of string diagrams, amenable to automated reasoning about
diagrammatic theories via graph rewrite systems. In this extended abstract, we
show how the power of such rewrite systems can be greatly extended by
introducing pattern graphs, which provide a means of expressing infinite
families of rewrite rules where certain marked subgraphs, called !-boxes (&quot;bang
boxes&quot;), on both sides of a rule can be copied any number of times or removed.
After reviewing the string graph formalism, we show how string graphs can be
extended to pattern graphs and how pattern graphs and pattern rewrite rules can
be instantiated to concrete string graphs and rewrite rules. We then provide
examples demonstrating the expressive power of pattern graphs and how they can
be applied to study interacting algebraic structures that are central to
categorical quantum mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6696</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6696</id><created>2012-04-30</created><authors><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Nonuniform Kolmogorov extractors</title><categories>cs.CC</categories><comments>This is a part of the conference paper from CCC 2011. It corrects an
  erroneus result from there, namely Theorem 4.8 (the new version has weaker
  parameters)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish tight bounds on the amount on nonuniformity that is necessary
for extracting a string with randomness rate 1 from a single source of
randomness with lower randomness rate. More precisely, as instantiations of
more general results, we show that while O(1) amount of advice regarding the
source is not enough for extracting a string with randomness rate 1 from a
source string with constant subunitary random rate, \omega(1) amount of advice
is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6699</identifier>
 <datestamp>2012-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6699</id><created>2012-04-30</created><updated>2012-09-12</updated><authors><author><keyname>Ding</keyname><forenames>Hu</forenames></author><author><keyname>Xu</keyname><forenames>Jinhui</forenames></author></authors><title>Chromatic Clustering in High Dimensional Space</title><categories>cs.CG</categories><comments>20 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a new type of clustering problem, called {\em
Chromatic Clustering}, in high dimensional space. Chromatic clustering seeks to
partition a set of colored points into groups (or clusters) so that no group
contains points with the same color and a certain objective function is
optimized. In this paper, we consider two variants of the problem, chromatic
$k$-means clustering (denoted as $k$-CMeans) and chromatic $k$-medians
clustering (denoted as $k$-CMedians), and investigate their hardness and
approximation solutions. For $k$-CMeans, we show that the additional coloring
constraint destroys several key properties (such as the locality property) used
in existing $k$-means techniques (for ordinary points), and significantly
complicates the problem. There is no FPTAS for the chromatic clustering
problem, even if $k=2$. To overcome the additional difficulty, we develop a
standalone result, called {\em Simplex Lemma}, which enables us to efficiently
approximate the mean point of an unknown point set through a fixed dimensional
simplex. A nice feature of the simplex is its independence with the
dimensionality of the original space, and thus can be used for problems in very
high dimensional space. With the simplex lemma, together with several random
sampling techniques, we show that a $(1+\epsilon)$-approximation of $k$-CMeans
can be achieved in near linear time through a sphere peeling algorithm. For
$k$-CMedians, we show that a similar sphere peeling algorithm exists for
achieving constant approximation solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6703</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6703</id><created>2012-04-30</created><updated>2013-01-17</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Foster</keyname><forenames>Dean P.</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Liu</keyname><forenames>Yi-Kai</forenames></author></authors><title>A Spectral Algorithm for Latent Dirichlet Allocation</title><categories>cs.LG stat.ML</categories><comments>Changed title to match conference version, which appears in Advances
  in Neural Information Processing Systems 25, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of topic modeling can be seen as a generalization of the
clustering problem, in that it posits that observations are generated due to
multiple latent factors (e.g., the words in each document are generated as a
mixture of several active topics, as opposed to just one). This increased
representational power comes at the cost of a more challenging unsupervised
learning problem of estimating the topic probability vectors (the distributions
over words for each topic), when only the words are observed and the
corresponding topics are hidden.
  We provide a simple and efficient learning procedure that is guaranteed to
recover the parameters for a wide class of mixture models, including the
popular latent Dirichlet allocation (LDA) model. For LDA, the procedure
correctly recovers both the topic probability vectors and the prior over the
topics, using only trigram statistics (i.e., third order moments, which may be
estimated with documents containing just three words). The method, termed
Excess Correlation Analysis (ECA), is based on a spectral decomposition of low
order moments (third and fourth order) via two singular value decompositions
(SVDs). Moreover, the algorithm is scalable since the SVD operations are
carried out on $k\times k$ matrices, where $k$ is the number of latent factors
(e.g. the number of topics), rather than in the $d$-dimensional observed space
(typically $d \gg k$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6716</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6716</id><created>2012-04-30</created><authors><author><keyname>Biemesderfer</keyname><forenames>Chris</forenames></author></authors><title>Fully Digital: Policy and Process Implications for the AAS</title><categories>astro-ph.IM cs.DL</categories><doi>10.1007/978-1-4419-8369-5_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades, every scholarly publisher has migrated at least
the mechanical aspects of their journal publishing so that they utilize digital
means. The academy was comfortable with that for a while, but publishers are
under increasing pressure to adapt further. At the American Astronomical
Society (AAS), we think that means bringing our publishing program to the point
of being fully digital, by establishing procedures and policies that regard the
digital objects of publication primarily. We have always thought about our
electronic journals as databases of digital articles, from which we can publish
and syndicate articles one at a time, and we must now put flesh on those bones
by developing practices that are consistent with the realities of article at a
time publication online. As a learned society that holds the long-term rights
to the literature, we have actively taken responsibility for the preservation
of the digital assets that constitute our journals, and in so doing we have not
forsaken the legacy pre-digital assets. All of us who serve as the long-term
stewards of scholarship must begin to evolve into fully digital publishers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6717</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6717</id><created>2012-04-30</created><updated>2012-09-12</updated><authors><author><keyname>Ding</keyname><forenames>Hu</forenames></author><author><keyname>Xu</keyname><forenames>Jinhui</forenames></author></authors><title>Linear Time Algorithm for Projective Clustering</title><categories>cs.CG</categories><comments>22 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Projective clustering is a problem with both theoretical and practical
importance and has received a great deal of attentions in recent years. Given a
set of points $P$ in $\mathbb{R}^{d}$ space, projective clustering is to find a
set $\mathbb{F}$ of $k$ lower dimensional $j$-flats so that the average
distance (or squared distance) from points in $P$ to their closest flats is
minimized. Existing approaches for this problem are mainly based on
adaptive/volume sampling or core-sets techniques which suffer from several
limitations. In this paper, we present the first uniform random sampling based
approach for this challenging problem and achieve linear time solutions for
three cases, general projective clustering, regular projective clustering, and
$L_{\tau}$ sense projective clustering. For the general projective clustering
problem, we show that for any given small numbers $0&lt;\gamma, \epsilon &lt;1$, our
approach first removes $\gamma|P|$ points as outliers and then determines $k$
$j$-flats to cluster the remaining points into $k$ clusters with an objective
value no more than $(1+\epsilon)$ times of the optimal for all points. For
regular projective clustering, we demonstrate that when the input points
satisfy some reasonable assumption on its input, our approach for the general
case can be extended to yield a PTAS for all points. For $L_{\tau}$ sense
projective clustering, we show that our techniques for both the general and
regular cases can be naturally extended to the $L_{\tau}$ sense projective
clustering problem for any $1 \le \tau &lt; \infty$. Our results are based on
several novel techniques, such as slab partition, $\Delta$-rotation, symmetric
sampling, and recursive projection, and can be easily implemented for
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6719</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6719</id><created>2012-04-30</created><authors><author><keyname>Grigore</keyname><forenames>Radu</forenames></author></authors><title>The Design and Algorithms of a Verification Condition Generator</title><categories>cs.SE</categories><comments>PhD Thesis from University College Dublin</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation discusses several problems loosely related, because they
all involve a verification condition generator. The Boogie language is
introduced; the architecture of a verification-generator is described. Then
come more interesting parts. (1) Moving to a passive form representation can be
seen as an automatic transformation into a pure functional language. How to
formalize this transformation and what is its complexity? (2) How do various
ways of describing the semantics of procedural languages (predicate
transformers, operational semantics) relate to each other? (3) How to do
incremental verification? That is, how to work less when re-verifying a program
that changed only a little since the verifier was last run. (4) How to detect
unreachable code, taking into account formal specifications?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6725</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6725</id><created>2012-04-30</created><updated>2012-06-08</updated><authors><author><keyname>Mokhov</keyname><forenames>Serguei A.</forenames></author><author><keyname>Sun</keyname><forenames>Yankui</forenames></author></authors><title>OCT Segmentation Survey and Summary Reviews and a Novel 3D Segmentation
  Algorithm and a Proof of Concept Implementation</title><categories>cs.CV physics.optics</categories><comments>51 pages, 22 figures, TOC, index, code excerpts; v2 refreshes
  references and fixes some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We overview the existing OCT work, especially the practical aspects of it. We
create a novel algorithm for 3D OCT segmentation with the goals of speed and/or
accuracy while remaining flexible in the design and implementation for future
extensions and improvements. The document at this point is a running draft
being iteratively &quot;developed&quot; as a progress report as the work and survey
advance. It contains the review and summarization of select OCT works, the
design and implementation of the OCTMARF experimentation application and some
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.6729</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.6729</id><created>2012-04-30</created><authors><author><keyname>Salahddine</keyname><forenames>Krit</forenames></author><author><keyname>Jalal</keyname><forenames>Laassiri</forenames></author><author><keyname>Said</keyname><forenames>El Hajji</forenames></author></authors><title>Specification and Verification of Uplink Framework for Application of
  Software Engineering using RM-ODP</title><categories>cs.OH</categories><comments>6 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 1, September 2011, 149-154</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper present a survey and discussion of the Reference Model for Open
Distributed Processing (RM-ODP) viewpoints; oriented approaches to requirements
engineering viewpoint and a presentation of new work in the application
wireless mobile phone, this area which has been designed with practical
application using the Unified Modelling Language (UML)/VHDL_AMS (VHSIC Hardware
Description Language Analog and Mixed-Signal). We mainly focus on rising and
fulling time, action, uplink behaviour constraints (sequentiality, non
determinism and concurrency constraints).We discuss the practical problems of
introducing viewpoint; oriented requirements engineering into industrial
software engineering practice and why these have prevented the widespread use
of existing approaches. The goal of this article is to check the uplink path
using the MIC (Microphone amplifier) with all analog inputs, and check the
amplifier gain. This paper provides an example of using the Uplink Framework to
build a comprehensive, good solution for Application Wireless Mobile Phone.
Finally, we discuss how well this approach addresses some outstanding problems
in requirements engineering (RE) and the practical industrial problems of
introducing new requirements engineering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0003</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0003</id><created>2012-04-30</created><authors><author><keyname>Abdel-Aty</keyname><forenames>Mahmoud</forenames></author></authors><title>Indices to Quantify the Ranking of Arabic Journals and Research Output</title><categories>cs.DL</categories><comments>4 pages, 3 figures</comments><journal-ref>Inf. Sci. Lett. 1, (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I propose two simple indices to classify journals, published in Arabic
language, and different researchers. These indices depend upon the known impact
factor and h-index. The new indices give an easy way to judge the rank of any
journal (output of any researcher) without looking for other journals (output
of other researchers).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0030</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0030</id><created>2012-04-30</created><authors><author><keyname>Aperjis</keyname><forenames>Christina</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>A Market for Unbiased Private Data: Paying Individuals According to
  their Privacy Attitudes</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since there is, in principle, no reason why third parties should not pay
individuals for the use of their data, we introduce a realistic market that
would allow these payments to be made while taking into account the privacy
attitude of the participants. And since it is usually important to use unbiased
samples to obtain credible statistical results, we examine the properties that
such a market should have and suggest a mechanism that compensates those
individuals that participate according to their risk attitudes. Equally
important, we show that this mechanism also benefits buyers, as they pay less
for the data than they would if they compensated all individuals with the same
maximum fee that the most concerned ones expect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0036</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0036</id><created>2012-04-30</created><updated>2013-05-07</updated><authors><author><keyname>Rosenbaum</keyname><forenames>David</forenames></author></authors><title>Optimal Quantum Circuits for Nearest-Neighbor Architectures</title><categories>quant-ph cs.CC</categories><comments>24 pages, 6 figures. v1 introduces all the results. v2 and v3 make
  minor improvements to the presentation and add additional references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the depth of quantum circuits in the realistic architecture
where a classical controller determines which local interactions to apply on
the kD grid Z^k where k &gt;= 2 is the same (up to a constant factor) as in the
standard model where arbitrary interactions are allowed. This allows
minimum-depth circuits (up to a constant factor) for the nearest-neighbor
architecture to be obtained from minimum-depth circuits in the standard
abstract model. Our work therefore justifies the standard assumption that
interactions can be performed between arbitrary pairs of qubits. In particular,
our results imply that Shor's algorithm, controlled operations and fanouts can
be implemented in constant depth, polynomial size and polynomial width in this
architecture.
  We also present optimal non-adaptive quantum circuits for controlled
operations and fanouts on a kD grid. These circuits have depth Theta(n^(1 /
k)), size Theta(n) and width Theta(n). Our lower bound also applies to a more
general class of operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0038</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0038</id><created>2012-04-30</created><authors><author><keyname>Reid</keyname><forenames>Fergal</forenames></author><author><keyname>McDaid</keyname><forenames>Aaron</forenames></author><author><keyname>Hurley</keyname><forenames>Neil</forenames></author></authors><title>Percolation Computation in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 8 figures. Supporting source code available:
  http://sites.google.com/site/cliqueperccomp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K-clique percolation is an overlapping community finding algorithm which
extracts particular structures, comprised of overlapping cliques, from complex
networks. While it is conceptually straightforward, and can be elegantly
expressed using clique graphs, certain aspects of k-clique percolation are
computationally challenging in practice. In this paper we investigate aspects
of empirical social networks, such as the large numbers of overlapping maximal
cliques contained within them, that make clique percolation, and clique graph
representations, computationally expensive. We motivate a simple algorithm to
conduct clique percolation, and investigate its performance compared to current
best-in-class algorithms. We present improvements to this algorithm, which
allow us to perform k-clique percolation on much larger empirical datasets. Our
approaches perform much better than existing algorithms on networks exhibiting
pervasively overlapping community structure, especially for higher values of k.
However, clique percolation remains a hard computational problem; current
algorithms still scale worse than some other overlapping community finding
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0040</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0040</id><created>2012-04-30</created><authors><author><keyname>Cohen</keyname><forenames>Eyal</forenames></author><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Frenkel</keyname><forenames>Sergey</forenames></author><author><keyname>Kryzhanovsky</keyname><forenames>Boris</forenames></author><author><keyname>Palagushkin</keyname><forenames>Alexandr</forenames></author><author><keyname>Rosenblit</keyname><forenames>Michael</forenames></author><author><keyname>Zakharov</keyname><forenames>Victor</forenames></author></authors><title>Optical Solver of Combinatorial Problems: Nano-Technological Approach</title><categories>cs.ET</categories><doi>10.1364/JOSAA.30.001845</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the first steps in creating an optical computing system. This
system may solve NP-Hard problems by utilizing a setup of exponential sized
masks. This is exponential space complexity but the production of those masks
is done with a polynomial time preprocessing. These masks are later used to
solve the problem in polynomial time. We propose to reduced the size of the
masks to nano-scaled density. Simulations were done to choose a proper design,
and actual implementations show the feasibility of such a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0042</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0042</id><created>2012-04-30</created><authors><author><keyname>Ujma</keyname><forenames>Mateusz</forenames></author><author><keyname>Shafiei</keyname><forenames>Nastaran</forenames></author></authors><title>jpf-concurrent: An extension of Java PathFinder for java.util.concurrent</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges when verifying multi-threaded Java applications is
the state space explosion problem. Due to thread interleavings, the number of
states that the model checker has to verify can grow rapidly and impede the
feasibility of verification. In the Java language, the source of thread
interleavings can be the system under test as well as the Java Development Kit
(JDK) itself. In our paper, we propose a method to minimize the state space
explosion problem for applications verified under the Java PathFinder (JPF)
model checker. Our method is based on abstracting the state of the application
to a smaller domain and implementing application behavior using the Model Java
Interface (MJI) of JPF. To show the capabilities of our approach, we have
created a JPF extension called jpf-concurrent which abstracts classes from the
Java Concurrency Utilities. Several benchmarks proved the usefulness of our
approach. In all cases, our implementation was faster than the JDK
implementation when running under the JPF model checker. Moreover, our
implementation led to significantly smaller state spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0044</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0044</id><created>2012-04-30</created><authors><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author></authors><title>A Singly-Exponential Time Algorithm for Computing Nonnegative Rank</title><categories>cs.DS cs.IR cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we give an algorithm for deciding if the nonnegative rank of a matrix
$M$ of dimension $m \times n$ is at most $r$ which runs in time
$(nm)^{O(r^2)}$. This is the first exact algorithm that runs in time
singly-exponential in $r$. This algorithm (and earlier algorithms) are built on
methods for finding a solution to a system of polynomial inequalities (if one
exists). Notably, the best algorithms for this task run in time exponential in
the number of variables but polynomial in all of the other parameters (the
number of inequalities and the maximum degree).
  Hence these algorithms motivate natural algebraic questions whose solution
have immediate {\em algorithmic} implications: How many variables do we need to
represent the decision problem, does $M$ have nonnegative rank at most $r$? A
naive formulation uses $nr + mr$ variables and yields an algorithm that is
exponential in $n$ and $m$ even for constant $r$. (Arora, Ge, Kannan, Moitra,
STOC 2012) recently reduced the number of variables to $2r^2 2^r$, and here we
exponentially reduce the number of variables to $2r^2$ and this yields our main
algorithm. In fact, the algorithm that we obtain is nearly-optimal (under the
Exponential Time Hypothesis) since an algorithm that runs in time $(nm)^{o(r)}$
would yield a subexponential algorithm for 3-SAT .
  Our main result is based on establishing a normal form for nonnegative matrix
factorization - which in turn allows us to exploit algebraic dependence among a
large collection of linear transformations with variable entries. Additionally,
we also demonstrate that nonnegative rank cannot be certified by even a very
large submatrix of $M$, and this property also follows from the intuition
gained from viewing nonnegative rank through the lens of systems of polynomial
inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0047</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0047</id><created>2012-04-30</created><updated>2012-10-24</updated><authors><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Moura</keyname><forenames>Jose' M. F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>$QD$-Learning: A Collaborative Distributed Strategy for Multi-Agent
  Reinforcement Learning Through Consensus + Innovations</title><categories>stat.ML cs.LG cs.MA math.OC math.PR</categories><comments>Submitted to the IEEE Transactions on Signal Processing, 33 pages</comments><doi>10.1109/TSP.2013.2241057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers a class of multi-agent Markov decision processes (MDPs),
in which the network agents respond differently (as manifested by the
instantaneous one-stage random costs) to a global controlled state and the
control actions of a remote controller. The paper investigates a distributed
reinforcement learning setup with no prior information on the global state
transition and local agent cost statistics. Specifically, with the agents'
objective consisting of minimizing a network-averaged infinite horizon
discounted cost, the paper proposes a distributed version of $Q$-learning,
$\mathcal{QD}$-learning, in which the network agents collaborate by means of
local processing and mutual information exchange over a sparse (possibly
stochastic) communication network to achieve the network goal. Under the
assumption that each agent is only aware of its local online cost data and the
inter-agent communication network is \emph{weakly} connected, the proposed
distributed scheme is almost surely (a.s.) shown to yield asymptotically the
desired value function and the optimal stationary control policy at each
network agent. The analytical techniques developed in the paper to address the
mixed time-scale stochastic dynamics of the \emph{consensus + innovations}
form, which arise as a result of the proposed interactive distributed scheme,
are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0076</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0076</id><created>2012-04-30</created><authors><author><keyname>Como</keyname><forenames>Giacomo</forenames></author><author><keyname>Savla</keyname><forenames>Ketan</forenames></author><author><keyname>Acemoglu</keyname><forenames>Daron</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A.</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Robust Distributed Routing in Dynamical Networks with Cascading Failures</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness of routing policies for networks is a central problem which is
gaining increased attention with a growing awareness to safeguard critical
infrastructure networks against natural and man-induced disruptions. Routing
under limited information and the possibility of cascades through the network
adds serious challenges to this problem. This abstract considers the framework
of dynamical networks introduced in our earlier work [1,2], where the network
is modeled by a system of ordinary differential equations derived from mass
conservation laws on directed acyclic graphs with a single origin-destination
pair and a constant inflow at the origin. The rate of change of the particle
density on each link of the network equals the difference between the inflow
and the outflow on that link. The latter is modeled to depend on the current
particle density on that link through a flow function. The novel modeling
element in this paper is that every link is assumed to have finite capacity for
particle density and that the flow function is modeled to be strictly
increasing as density increases from zero up to the maximum density capacity,
and is discontinuous at the maximum density capacity, with the flow function
value being zero at that point. This feature, in particular, allows for the
possibility of spill-backs in our model. In this paper, we present our results
on resilience of such networks under distributed routing, towards perturbations
that reduce link-wise flow functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0079</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0079</id><created>2012-04-30</created><updated>2012-05-19</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>Complexity Analysis of the Lasso Regularization Path</title><categories>stat.ML cs.LG math.OC</categories><comments>To appear in the proceedings of 29th International Conference on
  Machine Learning (ICML 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The regularization path of the Lasso can be shown to be piecewise linear,
making it possible to &quot;follow&quot; and explicitly compute the entire path. We
analyze in this paper this popular strategy, and prove that its worst case
complexity is exponential in the number of variables. We then oppose this
pessimistic result to an (optimistic) approximate analysis: We show that an
approximate path with at most O(1/sqrt(epsilon)) linear segments can always be
obtained, where every point on the path is guaranteed to be optimal up to a
relative epsilon-duality gap. We complete our theoretical analysis with a
practical algorithm to compute these approximate paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0085</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0085</id><created>2012-05-01</created><authors><author><keyname>Lee</keyname><forenames>Keonkook</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author></authors><title>Spectrum Leasing via Cooperation for Enhanced Physical-Layer Secrecy</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures, Part of this work was presented at the ICC 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum leasing via cooperation refers to the possibility of primary users
leasing a portion of the spectral resources to secondary users in exchange for
cooperation. In the presence of an eavesdropper, this correspondence proposes a
novel application of this concept in which the secondary cooperation aims at
improving secrecy of the primary network by creating more interference to the
eavesdropper than to the primary receiver. To generate the interference in a
positive way, this work studies an optimal design of a beamformer at the
secondary transmitter with multiple antennas that maximizes a secrecy rate of
the primary network while satisfying a required rate for the secondary network.
Moreover, we investigate two scenarios depending upon the operation of the
eavesdropper: i) the eavesdropper treats the interference by the secondary
transmission as an additive noise (single-user decoding) and ii) the
eavesdropper tries to decode and remove the secondary signal (joint decoding).
Numerical results confirm that, for a wide range of required secondary rate
constraints, the proposed spectrum-leasing strategy increases the secrecy rate
of the primary network compared to the case of no spectrum leasing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0088</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0088</id><created>2012-05-01</created><updated>2012-05-19</updated><authors><author><keyname>Lai</keyname><forenames>Ranch Y. Q.</forenames></author><author><keyname>Yuen</keyname><forenames>Pong C.</forenames></author></authors><title>ProPPA: A Fast Algorithm for $\ell_1$ Minimization and Low-Rank Matrix
  Completion</title><categories>cs.LG math.OC</categories><comments>update needed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Projected Proximal Point Algorithm (ProPPA) for solving a class
of optimization problems. The algorithm iteratively computes the proximal point
of the last estimated solution projected into an affine space which itself is
parallel and approaching to the feasible set. We provide convergence analysis
theoretically supporting the general algorithm, and then apply it for solving
$\ell_1$-minimization problems and the matrix completion problem. These
problems arise in many applications including machine learning, image and
signal processing. We compare our algorithm with the existing state-of-the-art
algorithms. Experimental results on solving these problems show that our
algorithm is very efficient and competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0103</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0103</id><created>2012-05-01</created><authors><author><keyname>&#x160;krbina</keyname><forenames>Neboj&#x161;a</forenames></author><author><keyname>Stojanovski</keyname><forenames>Toni</forenames></author></authors><title>Using parallel processing for file carving</title><categories>cs.CR</categories><comments>CIIT Conference, April 2012, Bitola Macedonia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  File carving is one of the most important procedures in Digital Forensic
Investigation (DFI). But it is also requires the most computational resources.
Parallel processing on Graphics Processing Units have proven to be many times
faster than when executed on standard CPU. This paper is inspecting the
algorithms and methods to use parallel processing for development of file
carving tools that will do their job much faster than the conventional DFI
tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0104</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0104</id><created>2012-05-01</created><authors><author><keyname>Vu&#x10d;kovi&#x107;</keyname><forenames>Marko</forenames></author><author><keyname>Stojanovski</keyname><forenames>Toni</forenames></author></authors><title>Migration of data for iKnow application at EURM - a case study</title><categories>cs.SE</categories><comments>CIIT Conference, April 2012, Bitola Macedonia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software evolves. After many revisions and improvements software gets retired
and replaced. When replacement takes place, one needs to migrate the data from
the old database into the new database, so the new application can replace the
old application. Student administration application (SAA) currently used by
European University (EURM) has been outgrown by the university, and needs
replacement. iKnow application developed as part of the iKnow Tempus project is
scheduled to replace the existing Student Administration application at EURM.
This paper describes the problems that were encountered while migrating the
data from the old databases of SAA to the new database designed for the iKnow
application. The problems were resolved using the well-known solutions typical
for an ETL process, since data migration can be considered as a type of ETL
process. In this paper we describe the solutions for the problems that we
encountered while migrating the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0106</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0106</id><created>2012-05-01</created><authors><author><keyname>Cvetanoska</keyname><forenames>Verche</forenames></author><author><keyname>Stojanovski</keyname><forenames>Toni</forenames></author></authors><title>Using high performance computing and Monte Carlo simulation for pricing
  american options</title><categories>cs.DC q-fin.CP</categories><comments>CIIT Conference, April 2012, Bitola Macedonia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High performance computing (HPC) is a very attractive and relatively new area
of research, which gives promising results in many applications. In this paper
HPC is used for pricing of American options. Although the American options are
very significant in computational finance; their valuation is very challenging,
especially when the Monte Carlo simulation techniques are used. For getting the
most accurate price for these types of options we use Quasi Monte Carlo
simulation, which gives the best convergence. Furthermore, this algorithm is
implemented on both GPU and CPU. Additionally, the CUDA architecture is used
for harnessing the power and the capability of the GPU for executing the
algorithm in parallel which is later compared with the serial implementation on
the CPU. In conclusion this paper gives the reasons and the advantages of
applying HPC in computational finance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0110</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0110</id><created>2012-05-01</created><authors><author><keyname>Yang</keyname><forenames>Jung-Hun</forenames></author><author><keyname>Ettema</keyname><forenames>Dick</forenames></author><author><keyname>Frenken</keyname><forenames>Koen</forenames></author><author><keyname>Van Oort</keyname><forenames>Frank</forenames></author><author><keyname>Visser</keyname><forenames>Evert-Jan</forenames></author></authors><title>Modelling spatial patterns of economic activity in the Netherlands</title><categories>cs.MA</categories><comments>16 pages</comments><journal-ref>Proceedings of Computers on Urban Planning and Urban Management,
  HongKong, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how spatial configurations of economic activity emerge is
important when formulating spatial planning and economic policy. Not only
micro-simulation and agent-based model such as UrbanSim, ILUMAS and SIMFIRMS,
but also Simon's model of hierarchical concentration have widely applied, for
this purpose. These models, however, have limitations with respect to
simulating structural changes in spatial economic systems and the impact of
proximity. The present paper proposes a model of firm development that is based
on behavioural rules such as growth, closure, spin-off and relocation. An
important aspect of the model is that locational preferences of firms are based
on agglomeration advantages, accessibility of markets and congestion, allowing
for a proper description of concentration and deconcentration tendencies. By
comparing the outcomes of the proposed model with real world data, we will
calibrate the parameters and assess how well the model predicts existing
spatial configurations and decide. The model is implemented as an agent-based
simulation model describing firm development in the Netherlands in 21
industrial sectors from 1950 to 2004.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0111</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0111</id><created>2012-05-01</created><authors><author><keyname>Simon</keyname><forenames>Emile</forenames></author></authors><title>Alternatives for optimization in systems and control: convex and
  non-convex approaches</title><categories>math.OC cs.SY</categories><comments>4 pages, extended abstract, for the 20th International Symposium on
  Mathematical Theory of Networks and Systems (MTNS 2012)</comments><msc-class>90C30, 93B51</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this presentation, we will develop a short overview of main trends of
optimization in systems and control, and from there outline some new
perspectives emerging today. More specifically, we will focus on the current
situation, where it is clear that convex and Linear Matrix Inequality (LMI)
methods have become the most common option. However, because of its vast
success, the convex approach is often the only direction considered, despite
the underlying problem is non-convex and that other optimization methods
specifically equipped to handle such problems should have been used instead. We
will present key points on this topic, and as a side result we will propose a
method to produce a virtually infinite number of papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0124</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0124</id><created>2012-05-01</created><authors><author><keyname>Singh</keyname><forenames>Jagbeer</forenames></author><author><keyname>Singh</keyname><forenames>Satyendra Prasad</forenames></author></authors><title>Schedulability Test for Soft Real-Time Systems under Multiprocessor
  Environment by using an Earliest Deadline First Scheduling Algorithm</title><categories>cs.OS</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the study of Earliest Deadline First (EDF) which is an
optimal scheduling algorithm for uniprocessor real time systems use for
scheduling the periodic task in soft real-time multiprocessor systems. In hard
real-time systems, a significant disparity exists EDF-based schemes and RMA
scheduling (which is the only known way of optimally scheduling recurrent
real-time tasks on multiprocessors): on M processors, all known EDF variants
have utilization-based schedulability bounds of approximately M/2, while RMA
algorithms can fully utilize all processors. This is unfortunate because EDF
based algorithms entail lower scheduling and task migration overheads. In work
on hard real-time systems, it has been shown that this disparity in
Schedulability can be lessened by placing caps on per task utilizations. Our
main contribution is a new EDF based scheme that ensures bounded deadline
tardiness. In this scheme, per-task utilizations must be focused,but overall
utilization need not be stricted. Our scheme should enable a wide range of soft
real-time applications to be scheduled with no constraints on total
utilization. Also propose techniques and heuristics that can be used to reduce
tardiness as well as increase the efficiency of task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0125</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0125</id><created>2012-05-01</created><authors><author><keyname>Khachatryan</keyname><forenames>A. M.</forenames></author><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>On the parameter $\mu_{21}$ of a complete bipartite graph</title><categories>cs.DM math.CO</categories><comments>5 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper edge $t$-coloring of a graph $G$ is a coloring of edges of $G$ with
colors $1,2,...,t$ such that all colors are used, and no two adjacent edges
receive the same color. The set of colors of edges incident with a vertex $x$
is called a spectrum of $x$. An arbitrary nonempty subset of consecutive
integers is called an interval.
  Suppose that all edges of a graph $G$ are colored in the game of Alice and
Bob with asymmetric distribution of roles. Alice determines the number $t$ of
colors in the future proper edge coloring of $G$ and aspires to minimize the
number of vertices with an interval spectrum in it. Bob colors edges of $G$
with $t$ colors and aspires to maximize that number. $\mu_{21}(G)$ is equal to
the number of vertices of $G$ with an interval spectrum at the finish of the
game on the supposition that both players choose their best strategies.
  In this paper, for arbitrary positive integers $m$ and $n$, the exact value
of the parameter $\mu_{21}(K_{m,n})$ is found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0126</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0126</id><created>2012-05-01</created><updated>2012-05-31</updated><authors><author><keyname>Mio</keyname><forenames>Matteo</forenames><affiliation>LIX, Ecole Polytechnique</affiliation></author></authors><title>On the equivalence of game and denotational semantics for the
  probabilistic mu-calculus</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.2.4, F.3.0, F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 1,
  2012) lmcs:787</journal-ref><doi>10.2168/LMCS-8(2:7)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic (or quantitative) modal mu-calculus is a fixed-point logic
de- signed for expressing properties of probabilistic labeled transition
systems (PLTS). Two semantics have been studied for this logic, both assigning
to every process state a value in the interval [0,1] representing the
probability that the property expressed by the formula holds at the state. One
semantics is denotational and the other is a game semantics, specified in terms
of two-player stochastic games. The two semantics have been proved to coincide
on all finite PLTS's, but the equivalence of the two semantics on arbitrary
models has been open in literature. In this paper we prove that the equivalence
indeed holds for arbitrary infinite models, and thus our result strengthens the
fruitful connection between denotational and game semantics. Our proof adapts
the unraveling or unfolding method, a general proof technique for proving
result of parity games by induction on their complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0128</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0128</id><created>2012-05-01</created><authors><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>It was not known about simple cycles</title><categories>cs.DM math.CO</categories><comments>7 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper edge $t$-coloring of a graph is a coloring of its edges with colors
$1,2,...,t$ such that all colors are used, and no two adjacent edges receive
the same color. For any integer $n\geq 3$, all possible values of $t$ are
found, for which there exists such a proper edge $t$-coloring of the simple
cycle C(n), which uses for each pair of adjacent edges either consecutive
colors or the first and the last ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0130</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0130</id><created>2012-05-01</created><authors><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>On one-sided interval edge colorings of biregular bipartite graphs</title><categories>cs.DM math.CO</categories><comments>6 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper edge $t$-coloring of a graph $G$ is a coloring of edges of $G$ with
colors $1,2,...,t$ such that all colors are used, and no two adjacent edges
receive the same color. The set of colors of edges incident with a vertex $x$
is called a spectrum of $x$. An arbitrary nonempty subset of consecutive
integers is called an interval. We say that a proper edge $t$-coloring of a
graph $G$ is interval in the vertex $x$ if the spectrum of $x$ is an interval.
We say that a proper edge $t$-coloring $\varphi$ of a graph $G$ is interval on
a subset $R_0$ of vertices of $G$, if for an arbitrary $x\in R_0$, $\varphi$ is
interval in $x$. We say that a subset $R$ of vertices of $G$ has an
$i$-property if there is a proper edge $t$-coloring of $G$ which is interval on
$R$. If $G$ is a graph, and a subset $R$ of its vertices has an $i$-property,
then the minimum value of $t$ for which there is a proper edge $t$-coloring of
$G$ interval on $R$ is denoted by $w_R(G)$.
  In this paper, for some bipartite graphs, we estimate the value of this
parameter in that cases when $R$ coincides with the set of all vertices of one
part of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0131</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0131</id><created>2012-05-01</created><authors><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>Estimates for the number of vertices with an interval spectrum in proper
  edge colorings of some graphs</title><categories>cs.DM math.CO</categories><comments>10 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper edge $t$-coloring of a graph $G$ is a coloring of edges of $G$ with
colors $1,2,...,t$ such that each of $t$ colors is used, and adjacent edges are
colored differently. The set of colors of edges incident with a vertex $x$ of
$G$ is called a spectrum of $x$. A proper edge $t$-coloring of a graph $G$ is
interval for its vertex $x$ if the spectrum of $x$ is an interval of integers.
A proper edge $t$-coloring of a graph $G$ is persistent-interval for its vertex
$x$ if the spectrum of $x$ is an interval of integers beginning from the color
1.
  For graphs $G$ from some classes of graphs, we obtain estimates for the
possible number of vertices for which a proper edge $t$-coloring of $G$ can be
interval or persistent-interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0139</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0139</id><created>2012-05-01</created><updated>2012-05-23</updated><authors><author><keyname>Buliga</keyname><forenames>Marius</forenames></author></authors><title>$\lambda$-Scale, a lambda calculus for spaces with dilations</title><categories>cs.LO math.LO math.MG math.RA</categories><comments>massively re-written version, condensed, relative calculus introduced</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\lambda$-Scale is an enrichment of lambda calculus which is adapted to
emergent algebras. It can be used therefore in metric spaces with dilations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0144</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0144</id><created>2012-05-01</created><authors><author><keyname>Chlamtac</keyname><forenames>Eden</forenames></author><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Everywhere-Sparse Spanners via Dense Subgraphs</title><categories>cs.DS</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The significant progress in constructing graph spanners that are sparse
(small number of edges) or light (low total weight) has skipped spanners that
are everywhere-sparse (small maximum degree). This disparity is in line with
other network design problems, where the maximum-degree objective has been a
notorious technical challenge. Our main result is for the Lowest Degree
2-Spanner (LD2S) problem, where the goal is to compute a 2-spanner of an input
graph so as to minimize the maximum degree. We design a polynomial-time
algorithm achieving approximation factor $\tilde O(\Delta^{3-2\sqrt{2}})
\approx \tilde O(\Delta^{0.172})$, where $\Delta$ is the maximum degree of the
input graph. The previous $\tilde O(\Delta^{1/4})$ -approximation was proved
nearly two decades ago by Kortsarz and Peleg [SODA 1994, SICOMP 1998].
  Our main conceptual contribution is to establish a formal connection between
LD2S and a variant of the Densest k-Subgraph (DkS) problem. Specifically, we
design for both problems strong relaxations based on the Sherali-Adams linear
programming (LP) hierarchy, and show that &quot;faithful&quot; randomized rounding of the
DkS-variant can be used to round LD2S solutions. Our notion of faithfulness
intuitively means that all vertices and edges are chosen with probability
proportional to their LP value, but the precise formulation is more subtle.
  Unfortunately, the best algorithms known for DkS use the Lov\'asz-Schrijver
LP hierarchy in a non-faithful way [Bhaskara, Charikar, Chlamtac, Feige, and
Vijayaraghavan, STOC 2010]. Our main technical contribution is to overcome this
shortcoming, while still matching the gap that arises in random graphs by
planting a subgraph with same log-density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0149</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0149</id><created>2012-05-01</created><authors><author><keyname>Sumour</keyname><forenames>M. A.</forenames></author><author><keyname>Radwan</keyname><forenames>M. A.</forenames></author></authors><title>Non-Universality in Semi-Directed Barabasi-Albert Networks</title><categories>physics.comp-ph cs.SI physics.soc-ph</categories><comments>5 pages including 2 figures and computer program</comments><doi>10.1142/S0129183112500623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In usual scale-free networks of Barabasi-Albert type, a newly added node
selects randomly m neighbors from the already existing network nodes,
proportionally to the number of links these had before. Then the number N(k) of
nodes with k links each decays as 1/k^gamma where gamma=3 is universal, i.e.
independent of m. Now we use a limited directedness in the construction of the
network, as a result of which the exponent gamma decreases from 3 to 2 for
increasing m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0157</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0157</id><created>2012-05-01</created><authors><author><keyname>Habeeb</keyname><forenames>Maggie</forenames></author><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>A Secret Sharing Scheme Based on Group Presentations and the Word
  Problem</title><categories>cs.CR math.GR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A (t,n)-threshold secret sharing scheme is a method to distribute a secret
among n participants in such a way that any t participants can recover the
secret, but no t-1 participants can. In this paper, we propose two secret
sharing schemes using non-abelian groups. One scheme is the special case where
all the participants must get together to recover the secret. The other one is
a (t,n)-threshold scheme that is a combination of Shamir's scheme and the
group-theoretic scheme proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0162</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0162</id><created>2012-05-01</created><authors><author><keyname>Shaqfeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Alnuweiri</keyname><forenames>Hussein</forenames></author></authors><title>Joint Power and Resource Allocation for Block-Fading Relay-Assisted
  Broadcast Channels</title><categories>cs.IT math.IT math.OC</categories><comments>IEEE Transactions on Wireless Communications, June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the solution for optimizing the power and resource allocation over
block-fading relay-assisted broadcast channels in order to maximize the long
term average achievable rates region of the users. The problem formulation
assumes regenerative (repetition coding) decode-and-forward (DF) relaying
strategy, long-term average total transmitted power constraint, orthogonal
multiplexing of the users messages within the channel blocks, possibility to
use a direct transmission (DT) mode from the base station to the user terminal
directly or a relaying (DF) transmission mode, and partial channel state
information. We show that our optimization problem can be transformed into an
equivalent &quot;no-relaying&quot; broadcast channel optimization problem with each
actual user substituted by two virtual users having different channel qualities
and multiplexing weights. The proposed power and resource allocation strategies
are expressed in closed-form that can be applied practically in centralized
relay-assisted wireless networks. Furthermore, we show by numerical examples
that our scheme enlarges the achievable rates region significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0170</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0170</id><created>2012-04-30</created><authors><author><keyname>Bylinski</keyname><forenames>Czeslaw</forenames></author><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>New developments in parsing Mizar</title><categories>cs.PL math.LO</categories><comments>5 pages. Accepted at Mathematical Knowledge Management 2012 Track D
  (Systems and Projects), Bremen, Germany, July 2012</comments><acm-class>D.3.4</acm-class><doi>10.1007/978-3-642-31374-5_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Mizar language aims to capture mathematical vernacular by providing a
rich language for mathematics. From the perspective of a user, the richness of
the language is welcome because it makes writing texts more &quot;natural&quot;. But for
the developer, the richness leads to syntactic complexity, such as dealing with
overloading.
  Recently the Mizar team has been making a fresh approach to the problem of
parsing the Mizar language. One aim is to make the language accessible to users
and other developers. In this paper we describe these new parsing efforts and
some applications thereof, such as large-scale text refactorings,
pretty-printing, HTTP parsing services, and normalizations of Mizar texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0175</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0175</id><created>2012-05-01</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author></authors><title>Approximating Sparse Covering Integer Programs Online</title><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A covering integer program (CIP) is a mathematical program of the form: min
{c^T x : Ax &gt;= 1, 0 &lt;= x &lt;= u, x integer}, where A is an m x n matrix, and c
and u are n-dimensional vectors, all having non-negative entries. In the online
setting, the constraints (i.e., the rows of the constraint matrix A) arrive
over time, and the algorithm can only increase the coordinates of vector x to
maintain feasibility. As an intermediate step, we consider solving the covering
linear program (CLP) online, where the integrality requirement on x is dropped.
  Our main results are (a) an O(log k)-competitive online algorithm for solving
the CLP, and (b) an O(log k log L)-competitive randomized online algorithm for
solving the CIP. Here k&lt;=n and L&lt;=m respectively denote the maximum number of
non-zero entries in any row and column of the constraint matrix A. By a result
of Feige and Korman, this is the best possible for polynomial-time online
algorithms, even in the special case of set cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0178</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0178</id><created>2012-05-01</created><updated>2013-11-29</updated><authors><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author></authors><title>A Survey of Multi-Tape Automata</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes the fundamental expressiveness, closure, and
decidability properties of various finite-state automata classes with multiple
input tapes. It also includes an original algorithm for the intersection of
one-way nondeterministic finite-state automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0181</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0181</id><created>2012-05-01</created><updated>2012-09-19</updated><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Distributed Linear Precoder Optimization and Base Station Selection for
  an Uplink Heterogeneous Network</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2013.2252169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a heterogeneous wireless cellular network, each user may be covered by
multiple access points such as macro/pico/relay/femto base stations (BS). An
effective approach to maximize the sum utility (e.g., system throughput) in
such a network is to jointly optimize users' linear procoders as well as their
base station associations. In this paper we first show that this joint
optimization problem is NP-hard and thus is difficult to solve to global
optimality. To find a locally optimal solution, we formulate the problem as a
noncooperative game in which the users and the BSs both act as players. We
introduce a set of new utility functions for the players and show that every
Nash equilibrium (NE) of the resulting game is a stationary solution of the
original sum utility maximization problem. Moreover, we develop a best-response
type algorithm that allows the players to distributedly reach a NE of the game.
Simulation results show that the proposed distributed algorithm can effectively
relieve local BS congestion and simultaneously achieve high throughput and load
balancing in a heterogeneous network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0192</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0192</id><created>2012-05-01</created><updated>2012-05-11</updated><authors><author><keyname>Cox</keyname><forenames>Anthony J.</forenames></author><author><keyname>Bauer</keyname><forenames>Markus J.</forenames></author><author><keyname>Jakobi</keyname><forenames>Tobias</forenames></author><author><keyname>Rosone</keyname><forenames>Giovanna</forenames></author></authors><title>Large-scale compression of genomic sequence databases with the
  Burrows-Wheeler transform</title><categories>cs.DS q-bio.GN</categories><comments>Version here is as submitted to Bioinformatics and is same as the
  previously archived version. This submission registers the fact that the
  advanced access version is now available at
  http://bioinformatics.oxfordjournals.org/content/early/2012/05/02/bioinformatics.bts173.abstract
  . Bioinformatics should be considered as the original place of publication of
  this article, please cite accordingly</comments><doi>10.1093/bioinformatics/bts173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation
  The Burrows-Wheeler transform (BWT) is the foundation of many algorithms for
compression and indexing of text data, but the cost of computing the BWT of
very large string collections has prevented these techniques from being widely
applied to the large sets of sequences often encountered as the outcome of DNA
sequencing experiments. In previous work, we presented a novel algorithm that
allows the BWT of human genome scale data to be computed on very moderate
hardware, thus enabling us to investigate the BWT as a tool for the compression
of such datasets.
  Results
  We first used simulated reads to explore the relationship between the level
of compression and the error rate, the length of the reads and the level of
sampling of the underlying genome and compare choices of second-stage
compression algorithm.
  We demonstrate that compression may be greatly improved by a particular
reordering of the sequences in the collection and give a novel `implicit
sorting' strategy that enables these benefits to be realised without the
overhead of sorting the reads. With these techniques, a 45x coverage of real
human genome sequence data compresses losslessly to under 0.5 bits per base,
allowing the 135.3Gbp of sequence to fit into only 8.2Gbytes of space (trimming
a small proportion of low-quality bases from the reads improves the compression
still further).
  This is more than 4 times smaller than the size achieved by a standard
BWT-based compressor (bzip2) on the untrimmed reads, but an important further
advantage of our approach is that it facilitates the building of compressed
full text indexes such as the FM-index on large-scale DNA sequence collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0193</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0193</id><created>2012-05-01</created><authors><author><keyname>Kamalian</keyname><forenames>R. R.</forenames></author></authors><title>On cyclically-interval edge colorings of trees</title><categories>cs.DM math.CO</categories><comments>9 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an undirected, simple, finite, connected graph $G$, we denote by $V(G)$
and $E(G)$ the sets of its vertices and edges, respectively. A function
$\varphi:E(G)\rightarrow\{1,2,\ldots,t\}$ is called a proper edge $t$-coloring
of a graph $G$ if adjacent edges are colored differently and each of $t$ colors
is used. An arbitrary nonempty subset of consecutive integers is called an
interval. If $\varphi$ is a proper edge $t$-coloring of a graph $G$ and $x\in
V(G)$, then $S_G(x,\varphi)$ denotes the set of colors of edges of $G$ which
are incident with $x$. A proper edge $t$-coloring $\varphi$ of a graph $G$ is
called a cyclically-interval $t$-coloring if for any $x\in V(G)$ at least one
of the following two conditions holds: a) $S_G(x,\varphi)$ is an interval, b)
$\{1,2,\ldots,t\}\setminus S_G(x,\varphi)$ is an interval. For any $t\in
\mathbb{N}$, let $\mathfrak{M}_t$ be the set of graphs for which there exists a
cyclically-interval $t$-coloring, and let
$$\mathfrak{M}\equiv\bigcup_{t\geq1}\mathfrak{M}_t.$$ For an arbitrary tree
$G$, it is proved that $G\in\mathfrak{M}$ and all possible values of $t$ are
found for which $G\in\mathfrak{M}_t.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0207</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0207</id><created>2012-05-01</created><updated>2012-09-08</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author></authors><title>Shortest Path Set Induced Vertex Ordering and its Application to
  Distributed Distance Optimal Multi-agent Formation Path Planning</title><categories>cs.RO cs.SY</categories><comments>Extended the earlier version to 8 Pages, complete with literature
  review. One additional section on a distributed scheduling algorithm is added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the task of moving a group of indistinguishable agents on a connected
graph with unit edge lengths into an arbitrary goal formation, it was
previously shown that distance optimal paths can be scheduled to complete with
a tight convergence time guarantee, using a fully centralized algorithm. In
this study, we show that the problem formulation in fact induces a more
fundamental ordering of the vertices on the underlying graph network, which
directly leads to a more intuitive scheduling algorithm that assures the same
convergence time and runs faster. More importantly, this structure enables a
distributed scheduling algorithm once individual paths are assigned to the
agents, which was not possible before. The vertex ordering also readily extends
to more general graphs - those with non-unit capacities and edge lengths - for
which we again guarantee the convergence time until the desired formation is
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0211</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0211</id><created>2012-05-01</created><updated>2012-07-19</updated><authors><author><keyname>Sen</keyname><forenames>Parongama</forenames></author></authors><title>Non-conservative kinetic exchange model of opinion dynamics with
  randomness and bounded confidence</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>Figure and new text material added; paper restructured; version
  accepted in PRE</comments><doi>10.1103/PhysRevE.86.016115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of a bounded confidence level is incorporated in a
nonconservative kinetic exchange model of opinion dynamics model where opinions
have continuous values $\in [-1,1]$. The characteristics of the unrestricted
model, which has one parameter $\lambda$ representing conviction, undergo
drastic changes with the introduction of bounded confidence parametrised by
$\delta$. Three distinct regions are identified in the phase diagram in the
$\delta-\lambda$ plane and the evidences of a first order phase transition for
$\delta \geq 0.3$ are presented. A neutral state with all opinions equal to
zero occurs for $\lambda \leq \lambda_{c_1} \simeq 2/3$, independent of
$\delta$, while for $\lambda_{c_1} \leq \lambda \leq \lambda_{c_2}(\delta)$, an
ordered region is seen to exist where opinions of only one sign prevail. At
$\lambda_{c_2}(\delta)$, a transition to a disordered state is observed, where
individual opinions of both signs coexist and move closer to the extreme values
($\pm 1$) as $\lambda$ is increased. For confidence level $\delta &lt; 0.3$, the
ordered phase exists for a narrow range of $\lambda$ only. The line $\delta =
0$ is apparently a line of discontinuity and this limit is discussed in some
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0213</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0213</id><created>2012-05-01</created><updated>2012-06-01</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Seuret</keyname><forenames>Alexandre</forenames></author></authors><title>Convex dwell-time characterizations for uncertain linear impulsive
  systems</title><categories>math.OC cs.SY math.CA math.DS</categories><comments>Accepted at IEEE Transactions on Automatic Control</comments><doi>10.1109/TAC.2012.2200379</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New sufficient conditions for the characterization of dwell-times for linear
impulsive systems are proposed and shown to coincide with continuous decrease
conditions of a certain class of looped-functionals, a recently introduced type
of functionals suitable for the analysis of hybrid systems. This approach
allows to consider Lyapunov functions that evolve non-monotonically along the
flow of the system in a new way, broadening then the admissible class of
systems which may be analyzed. As a byproduct, the particular structure of the
obtained conditions makes the method is easily extendable to uncertain systems
by exploiting some convexity properties. Several examples illustrate the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0243</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0243</id><created>2012-04-30</created><updated>2012-05-06</updated><authors><author><keyname>Maseleno</keyname><forenames>Andino</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mahmud</forenames></author></authors><title>Poultry Diseases Expert System using Dempster-Shafer Theory</title><categories>cs.AI stat.AP</categories><comments>Brunei International Conference and Engineering Technology 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Based on World Health Organization (WHO) fact sheet in the 2011, outbreaks of
poultry diseases especially Avian Influenza in poultry may raise global public
health concerns due to their effect on poultry populations, their potential to
cause serious disease in people, and their pandemic potential. In this
research, we built a Poultry Diseases Expert System using Dempster-Shafer
Theory. In this Poultry Diseases Expert System We describe five symptoms which
include depression, combs, wattle, bluish face region, swollen face region,
narrowness of eyes, and balance disorders. The result of the research is that
Poultry Diseases Expert System has been successfully identifying poultry
diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0260</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0260</id><created>2012-05-01</created><updated>2013-09-18</updated><authors><author><keyname>Jedwab</keyname><forenames>Jonathan</forenames></author><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author></authors><title>Littlewood Polynomials with Small $L^4$ Norm</title><categories>math.NT cs.IT math.CO math.IT</categories><comments>minor revisions</comments><msc-class>Primary: 11B83, Secondary: 94A55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Littlewood asked how small the ratio $||f||_4/||f||_2$ (where $||.||_\alpha$
denotes the $L^\alpha$ norm on the unit circle) can be for polynomials $f$
having all coefficients in $\{1,-1\}$, as the degree tends to infinity. Since
1988, the least known asymptotic value of this ratio has been $\sqrt[4]{7/6}$,
which was conjectured to be minimum. We disprove this conjecture by showing
that there is a sequence of such polynomials, derived from the Fekete
polynomials, for which the limit of this ratio is less than $\sqrt[4]{22/19}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0263</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0263</id><created>2012-05-01</created><updated>2012-05-16</updated><authors><author><keyname>Impagliazzo</keyname><forenames>Russell</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>An Entropic Proof of Chang's Inequality</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chang's lemma is a useful tool in additive combinatorics and the analysis of
Boolean functions. Here we give an elementary proof using entropy. The constant
we obtain is tight, and we give a slight improvement in the case where the
variables are highly biased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0273</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0273</id><created>2012-05-01</created><authors><author><keyname>Jorgensen</keyname><forenames>Allan</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Geometric Computations on Indecisive and Uncertain Points</title><categories>cs.CG</categories><comments>26 pages, 30 figures. This replaces a paper here
  (http://arXiv.org/abs/0812.2967) that was split, extended, and published in
  two venues (ESA 2009 and WADS 2011); although the old version contains some
  minor content that was omitted to make a more coherent story</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computing geometric problems on uncertain points. An uncertain point
is a point that does not have a fixed location, but rather is described by a
probability distribution. When these probability distributions are restricted
to a finite number of locations, the points are called indecisive points. In
particular, we focus on geometric shape-fitting problems and on building
compact distributions to describe how the solutions to these problems vary with
respect to the uncertainty in the points. Our main results are: (1) a simple
and efficient randomized approximation algorithm for calculating the
distribution of any statistic on uncertain data sets; (2) a polynomial,
deterministic and exact algorithm for computing the distribution of answers for
any LP-type problem on an indecisive point set; and (3) the development of
shape inclusion probability (SIP) functions which captures the ambient
distribution of shapes fit to uncertain or indecisive point sets and are
admissible to the two algorithmic constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0281</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0281</id><created>2012-05-01</created><updated>2012-05-09</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Yetis</keyname><forenames>Cenk M.</forenames></author><author><keyname>Gunawan</keyname><forenames>Erry</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Improving Achievable Rate for the Two-User SISO Interference Channel
  with Improper Gaussian Signaling</title><categories>cs.IT math.IT</categories><comments>Version 2, Invited paper, submitted to Asilomar 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the achievable rate region of the two-user
single-input-single-output (SISO) Gaussian interference channel, when the
improper Gaussian signaling is applied. Under the assumption that the
interference is treated as additive Gaussian noise, we show that the user's
achievable rate can be expressed as a summation of the rate achievable by the
conventional proper Gaussian signaling, which depends on the users' input
covariances only, and an additional term, which is a function of both the
users' covariances and pseudo-covariances. The additional degree of freedom
given by the pseudo-covariance, which is conventionally set to be zero for the
case of proper Gaussian signaling, provides an opportunity to improve the
achievable rate by employing the improper Gaussian signaling. Since finding the
optimal solution for the joint covariance and pseudo-covariance optimization is
difficult, we propose a sub-optimal but efficient algorithm by separately
optimizing these two sets of parameters. Numerical results show that the
proposed algorithm provides a close-to-optimal performance as compared to the
exhaustive search method, and significantly outperforms the optimal proper
Gaussian signaling and other existing improper Gaussian signaling schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0282</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0282</id><created>2012-05-01</created><authors><author><keyname>Hassan</keyname><forenames>A. H.</forenames></author><author><keyname>Fluke</keyname><forenames>C. J.</forenames></author><author><keyname>Barnes</keyname><forenames>D. G.</forenames></author></authors><title>A Distributed GPU-based Framework for real-time 3D Volume Rendering of
  Large Astronomical Data Cubes</title><categories>astro-ph.IM cs.DC cs.GR</categories><comments>13 Pages, 7 figures, has been accepted for publication in
  Publications of the Astronomical Society of Australia</comments><doi>10.1071/AS12025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework to interactively volume-render three-dimensional data
cubes using distributed ray-casting and volume bricking over a cluster of
workstations powered by one or more graphics processing units (GPUs) and a
multi-core CPU. The main design target for this framework is to provide an
in-core visualization solution able to provide three-dimensional interactive
views of terabyte-sized data cubes. We tested the presented framework using a
computing cluster comprising 64 nodes with a total of 128 GPUs. The framework
proved to be scalable to render a 204 GB data cube with an average of 30 frames
per second. Our performance analyses also compare between using NVIDIA Tesla
1060 and 2050 GPU architectures and the effect of increasing the visualization
output resolution on the rendering performance. Although our initial focus, and
the examples presented in this work, is volume rendering of spectral data cubes
from radio astronomy, we contend that our approach has applicability to other
disciplines where close to real-time volume rendering of terabyte-order 3D data
sets is a requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0288</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0288</id><created>2012-05-01</created><updated>2013-01-07</updated><authors><author><keyname>Afkanpour</keyname><forenames>Arash</forenames></author><author><keyname>Gy&#xf6;rgy</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel
  Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of simultaneously learning to linearly combine a very
large number of kernels and learn a good predictor based on the learnt kernel.
When the number of kernels $d$ to be combined is very large, multiple kernel
learning methods whose computational cost scales linearly in $d$ are
intractable. We propose a randomized version of the mirror descent algorithm to
overcome this issue, under the objective of minimizing the group $p$-norm
penalized empirical risk. The key to achieve the required exponential speed-up
is the computationally efficient construction of low-variance estimates of the
gradient. We propose importance sampling based estimates, and find that the
ideal distribution samples a coordinate with a probability proportional to the
magnitude of the corresponding gradient. We show the surprising result that in
the case of learning the coefficients of a polynomial kernel, the combinatorial
structure of the base kernels to be combined allows the implementation of
sampling from this distribution to run in $O(\log(d))$ time, making the total
computational cost of the method to achieve an $\epsilon$-optimal solution to
be $O(\log(d)/\epsilon^2)$, thereby allowing our method to operate for very
large values of $d$. Experiments with simulated and real data confirm that the
new algorithm is computationally more efficient than its state-of-the-art
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0306</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0306</id><created>2012-05-01</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>An index formula for simple graphs</title><categories>math.DG cs.CG math.GN</categories><comments>24 pages, 6 figures</comments><msc-class>05C10, 57M15, 68R10, 53A55 60B99, 94C99, 97K30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gauss-Bonnet for simple graphs G assures that the sum of curvatures K(x) over
the vertex set V of G is the Euler characteristic X(G). Poincare-Hopf tells
that for any injective function f on V the sum of i(f,x) is X(G). We also know
that averaging the indices E[i(f,x)] over all functions gives curvature K(x).
  We explore here the situation when G is geometric of dimension d: that is if
each unit sphere S(x) is geometric of dimension d-1 and that X(S(x))=0 for even
d and X(S(x))=2 for odd d. The dimension of G is inductively defined as the
average of 1+dim(S(x)) over all S(x) assuming the empty graph has dimension -1.
  We prove that any odd dimensional geometric graph G has zero curvature. This
is done with the help of an index formula j(f,x) = 1-X(S(x))/2-X(B(f,x))/2,
where j(x)=[i(f,x)+i(-f,x)]/2. The graph B(f,x) is the discrete level surface
{y | f(y) = f(x)} intersected with S(x). It is a subgraph of the line graph of
G and geometric if G is geometric.
  The index formula simplifies for geometric graphs: for even d it is j(f,x) =
1-X(B(f,x))/2, where B(f,x) is a (d-2)-dimensional graph. For odd d it becomes
j(f,x) =-X(B(f,x))/2, where B(f,x) is an odd dimensional graph. Because by
induction with respect to d, the X(B(f,x))=0 we know now that that j(f,x) is
zero for all x and so, by taking expectation over f that curvature K(x) is zero
for all x.
  We also point out that all these results hold almost verbatim for compact
Riemannian manifolds and actually are much simpler there. The same integral
geometric index formula is valid if f is a Morse function, i(f,x) is the index
of the gradient vector field and if S(x) is a sufficiently small geodesic
sphere around x and B(f,x) which is S(x) intersected with the level surface {y
| f(y)=f(x)}. Also in the continuum, the symmetric index j(f,x) is constant
zero everywhere if d is odd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0312</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0312</id><created>2012-05-01</created><authors><author><keyname>Ke</keyname><forenames>Weimao</forenames></author></authors><title>Least Information Modeling for Information Retrieval</title><categories>cs.IR cs.IT math.IT</categories><comments>10 pages, 3 figures</comments><acm-class>H.3.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed a Least Information theory (LIT) to quantify meaning of
information in probability distribution changes, from which a new information
retrieval model was developed. We observed several important characteristics of
the proposed theory and derived two quantities in the IR context for document
representation. Given probability distributions in a collection as prior
knowledge, LI Binary (LIB) quantifies least information due to the binary
occurrence of a term in a document whereas LI Frequency (LIF) measures least
information based on the probability of drawing a term from a bag of words.
Three fusion methods were also developed to combine LIB and LIF quantities for
term weighting and document ranking. Experiments on four benchmark TREC
collections for ad hoc retrieval showed that LIT-based methods demonstrated
very strong performances compared to classic TF*IDF and BM25, especially for
verbose queries and hard search topics. The least information theory offers a
new approach to measuring semantic quantities of information and provides
valuable insight into the development of new IR models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0314</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0314</id><created>2012-05-01</created><authors><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>Analysis of Boolean Functions</title><categories>cs.CC</categories><comments>43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scribe notes from the 2012 Barbados Workshop on Computational Complexity. A
series of lectures on Analysis of Boolean Functions by Ryan O'Donnell, with a
guest lecture by Per Austrin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0325</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0325</id><created>2012-05-02</created><updated>2012-05-08</updated><authors><author><keyname>Wu</keyname><forenames>Yuan</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Tsang</keyname><forenames>Danny H. K.</forenames></author><author><keyname>Qian</keyname><forenames>Liping</forenames></author></authors><title>Energy-Efficient Delay-Constrained Transmission and Sensing for
  Cognitive Radio Systems</title><categories>cs.NI</categories><comments>waiting for the agreements from all the coauthors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study energy-efficient transmission for Cognitive Radio (CR)
which opportunistically operates on Primary User's (PU's) channel through
spectrum sensing. Spectrum sensing and compulsory idling (for incumbent
protection) introduce energy-overheads for Secondary User's (SU's) operations,
and thus an appropriate balance between energy consumption in data transmission
and energy-overheads is required. We formulate this problem as a discrete-time
Markov Decision Process (MDP) in which the SU aims at minimizing its average
cost (including both energy consumption and delay cost) to finish a target
traffic payload through an appropriate rate allocation. Based on Certainty
Equivalent Control, we propose a low-complexity rate-adaptation policy that
achieves comparable performance as the optimal policy. With the low-complexity
policy, we quantify the impact of energy-overheads (including the power
consumption for spectrum sensing and compulsory idling) on the SU transmission
strategy. Specifically, the SU rate increases with the increase of
energy-overheads, whose marginal impact, however, diminishes. Moreover, the
marginal impact of energy-overheads is more significant for delay-insensitive
traffic compared to that for delay-sensitive traffic. To mitigate the loss due
to imperfect spectrum sensing, we quantify that the SU decreases (increases)
its rate with a larger mis-detection probability (false alarm probability).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0326</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0326</id><created>2012-05-02</created><authors><author><keyname>Bhatnagar</keyname><forenames>Manav R.</forenames></author></authors><title>Performance Analysis of Decode-and-Forward Relaying in Gamma-Gamma
  Fading Channels</title><categories>cs.IT math.IT</categories><comments>3 pages, 1 figure, journal</comments><journal-ref>IEEE Photonics Technology Letters, volume 24, number 7, pages
  545-547, April 2012</journal-ref><doi>10.1109/LPT.2011.2176330</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decode-and-forward (DF) cooperative communication based on free space optical
(FSO) links is studied in this letter. We analyze performance of the DF
protocol in the FSO links following the Gamma-Gamma distribution. The
cumulative distribution function (CDF) and probability density function (PDF)
of a random variable containing mixture of the Gamma- Gamma and Gaussian random
variables is derived. By using the derived CDF and PDF, average bit error rate
of the DF relaying is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0329</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0329</id><created>2012-05-02</created><updated>2012-05-03</updated><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi Prasad</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>An Adaptive Conditional Zero-Forcing Decoder with Full-diversity, Least
  Complexity and Essentially-ML Performance for STBCs</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures. Corrected a minor typographical error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A low complexity, essentially-ML decoding technique for the Golden code and
the 3 antenna Perfect code was introduced by Sirianunpiboon, Howard and
Calderbank. Though no theoretical analysis of the decoder was given, the
simulations showed that this decoding technique has almost maximum-likelihood
(ML) performance. Inspired by this technique, in this paper we introduce two
new low complexity decoders for Space-Time Block Codes (STBCs) - the Adaptive
Conditional Zero-Forcing (ACZF) decoder and the ACZF decoder with successive
interference cancellation (ACZF-SIC), which include as a special case the
decoding technique of Sirianunpiboon et al. We show that both ACZF and ACZF-SIC
decoders are capable of achieving full-diversity, and we give sufficient
conditions for an STBC to give full-diversity with these decoders. We then show
that the Golden code, the 3 and 4 antenna Perfect codes, the 3 antenna Threaded
Algebraic Space-Time code and the 4 antenna rate 2 code of Srinath and Rajan
are all full-diversity ACZF/ACZF-SIC decodable with complexity strictly less
than that of their ML decoders. Simulations show that the proposed decoding
method performs identical to ML decoding for all these five codes. These STBCs
along with the proposed decoding algorithm outperform all known codes in terms
of decoding complexity and error performance for 2,3 and 4 transmit antennas.
We further provide a lower bound on the complexity of full-diversity
ACZF/ACZF-SIC decoding. All the five codes listed above achieve this lower
bound and hence are optimal in terms of minimizing the ACZF/ACZF-SIC decoding
complexity. Both ACZF and ACZF-SIC decoders are amenable to sphere decoding
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0337</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0337</id><created>2012-05-02</created><authors><author><keyname>Goudarzi</keyname><forenames>Pejman</forenames></author></authors><title>Stochastic TCO minimization for Video Transmission over IP Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the viewpoint of service operators the Total Cost of Ownership (TCO) for
developing a communication service comprises from two parts; CAPital
EXpenditure (CAPEX) and OPerational EXpenditure (OPEX). These two types of
costs are interrelated and affect any service provider's deployment strategy.
In many traditional methods, selection of critical elements of a new service is
performed in a heuristic manner aimed at reducing only the OPEX part of the TCO
which is not necessarily optimal. Furthermore, exact cost modeling for such
services is not always possible and contains some uncertainties. In the current
work, after cost modeling of each video streaming element by capturing the
effect of the model uncertainties, the TCO optimization problem for video
streaming over IP networks is formulated as a stochastic optimization problem.
The solution of the proposed optimization problem can cope with the cost
modeling uncertainties and track the dynamism in the TCO and lead to a
time-varying optimal solution. Numerical analysis results verify the developed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0343</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0343</id><created>2012-05-02</created><authors><author><keyname>Liang</keyname><forenames>Hongyu</forenames></author></authors><title>Signed and Minus Domination in Complete Multipartite Graphs</title><categories>cs.DM math.CO</categories><comments>Accepted to Ars Combinatoria</comments><msc-class>05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we determine the exact values of the signed domination number,
signed total domination number, and minus domination number of complete
multipartite graphs, which substantially generalizes some previous results
obtained for special subclasses of complete multipartite graphs such as cliques
and complete bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0345</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0345</id><created>2012-05-02</created><authors><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames><affiliation>NT, IRMAR</affiliation></author></authors><title>Bounds on List Decoding Gabidulin Codes</title><categories>cs.IT math.IT</categories><comments>Thirteenth International Workshop on Algebraic and Combinatorial
  Coding Theory (ACCT 2012), Pomorie : Bulgaria (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open question about Gabidulin codes is whether polynomial-time list
decoding beyond half the minimum distance is possible or not. In this
contribution, we give a lower and an upper bound on the list size, i.e., the
number of codewords in a ball around the received word. The lower bound shows
that if the radius of this ball is greater than the Johnson radius, this list
size can be exponential and hence, no polynomial-time list decoding is
possible. The upper bound on the list size uses subspace properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0357</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0357</id><created>2012-05-02</created><updated>2012-05-31</updated><authors><author><keyname>Bahr</keyname><forenames>Patrick</forenames><affiliation>Department of Computer Science, University of Copenhagen</affiliation></author></authors><title>Modes of Convergence for Term Graph Rewriting</title><categories>cs.LO cs.PL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1107.0666</comments><proxy>LMCS</proxy><acm-class>F.4.2, F.1.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 1,
  2012) lmcs:935</journal-ref><doi>10.2168/LMCS-8(2:6)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Term graph rewriting provides a simple mechanism to finitely represent
restricted forms of infinitary term rewriting. The correspondence between
infinitary term rewriting and term graph rewriting has been studied to some
extent. However, this endeavour is impaired by the lack of an appropriate
counterpart of infinitary rewriting on the side of term graphs. We aim to fill
this gap by devising two modes of convergence based on a partial order
respectively a metric on term graphs. The thus obtained structures generalise
corresponding modes of convergence that are usually studied in infinitary term
rewriting. We argue that this yields a common framework in which both term
rewriting and term graph rewriting can be studied. In order to substantiate our
claim, we compare convergence on term graphs and on terms. In particular, we
show that the modes of convergence on term graphs are conservative extensions
of the corresponding modes of convergence on terms and are preserved under
unravelling term graphs to terms. Moreover, we show that many of the properties
known from infinitary term rewriting are preserved. This includes the intrinsic
completeness of both modes of convergence and the fact that convergence via the
partial order is a conservative extension of the metric convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0376</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0376</id><created>2012-05-02</created><updated>2012-07-15</updated><authors><author><keyname>Hermanns</keyname><forenames>Holger</forenames></author><author><keyname>Turrini</keyname><forenames>Andrea</forenames></author></authors><title>Deciding Probabilistic Automata Weak Bisimulation in Polynomial Time</title><categories>cs.FL</categories><comments>Polished version with a more complete running example and typo fixes</comments><acm-class>G.3; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deciding in an efficient way weak probabilistic bisimulation in the context
of Probabilistic Automata is an open problem for about a decade. In this work
we close this problem by proposing a procedure that checks in polynomial time
the existence of a weak combined transition satisfying the step condition of
the bisimulation. We also present several extensions of weak combined
transitions, such as hyper-transitions and the new concepts of allowed weak
combined and hyper-transitions and of equivalence matching, that turn out to be
verifiable in polynomial time as well. These results set the ground for the
development of more effective compositional analysis algorithms for
probabilistic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0406</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0406</id><created>2012-05-02</created><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Tang</keyname><forenames>Ke</forenames></author></authors><title>Minimax Classifier for Uncertain Costs</title><categories>cs.LG</categories><comments>6 pages, more materials will be added into the manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies on the cost-sensitive learning assumed that a unique cost matrix
is known for a problem. However, this assumption may not hold for many
real-world problems. For example, a classifier might need to be applied in
several circumstances, each of which associates with a different cost matrix.
Or, different human experts have different opinions about the costs for a given
problem. Motivated by these facts, this study aims to seek the minimax
classifier over multiple cost matrices. In summary, we theoretically proved
that, no matter how many cost matrices are involved, the minimax problem can be
tackled by solving a number of standard cost-sensitive problems and
sub-problems that involve only two cost matrices. As a result, a general
framework for achieving minimax classifier over multiple cost matrices is
suggested and justified by preliminary empirical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0411</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0411</id><created>2012-05-02</created><updated>2012-05-21</updated><authors><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Sriperumbudur</keyname><forenames>Bharath</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Hypothesis testing using pairwise distances and associated kernels (with
  Appendix)</title><categories>cs.LG stat.ME stat.ML</categories><comments>Appearing in Proceedings of the 29th International Conference on
  Machine Learning, Edinburgh, Scotland, UK, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a unifying framework linking two classes of statistics used in
two-sample and independence testing: on the one hand, the energy distances and
distance covariances from the statistics literature; on the other, distances
between embeddings of distributions to reproducing kernel Hilbert spaces
(RKHS), as established in machine learning. The equivalence holds when energy
distances are computed with semimetrics of negative type, in which case a
kernel may be defined such that the RKHS distance between distributions
corresponds exactly to the energy distance. We determine the class of
probability distributions for which kernels induced by semimetrics are
characteristic (that is, for which embeddings of the distributions to an RKHS
are injective). Finally, we investigate the performance of this family of
kernels in two-sample and independence tests: we show in particular that the
energy distance most commonly employed in statistics is just one member of a
parametric family of kernels, and that other choices from this family can yield
more powerful tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0435</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0435</id><created>2012-05-02</created><updated>2012-08-17</updated><authors><author><keyname>Chen</keyname><forenames>Jianjun</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Varghese</keyname><forenames>George</forenames></author></authors><title>Scalable Social Coordination using Enmeshed Queries</title><categories>cs.DB cs.SI physics.soc-ph</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social coordination allows users to move beyond awareness of their friends to
efficiently coordinating physical activities with others. While specific forms
of social coordination can be seen in tools such as Evite, Meetup and Groupon,
we introduce a more general model using what we call enmeshed queries. An
enmeshed query allows users to declaratively specify an intent to coordinate by
specifying social attributes such as the desired group size and who/what/when,
and the database returns matching queries. Enmeshed queries are continuous, but
new queries (and not data) answer older queries; the variable group size also
makes enmeshed queries different from entangled queries, publish-subscribe
systems, and dating services.
  We show that even offline group coordination using enmeshed queries is
NP-hard. We then introduce efficient heuristics that use selective indices such
as location and time to reduce the space of possible matches; we also add
refinements such as delayed evaluation and using the relative matchability of
users to determine search order. We describe a centralized implementation and
evaluate its performance against an optimal algorithm. We show that the
combination of not stopping prematurely (after finding a match) and delayed
evaluation results in an algorithm that finds 86% of the matches found by an
optimal algorithm, and takes an average of 40 usec per query using 1 core of a
2.5 Ghz server machine. Further, the algorithm has good latency, is reasonably
fair to large group size requests, and can be scaled to global workloads using
multiple cores and multiple servers. We conclude by describing potential
generalizations that add prices, recommendations, and data mining to basic
enmeshed queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0439</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0439</id><created>2012-05-02</created><authors><author><keyname>Mohamed</keyname><forenames>Aridj</forenames></author><author><keyname>Eddine</keyname><forenames>Zegour Djamel</forenames></author></authors><title>TH*:Scalable Distributed Trie Hashing</title><categories>cs.DS cs.DB cs.DC</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 7, Issue
  6, November 2010 ISSN (Online): 1694-0814 http://www.IJCSI.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world of computers, dealing with huge amounts of data is not
unusual. The need to distribute this data in order to increase its availability
and increase the performance of accessing it is more urgent than ever. For
these reasons it is necessary to develop scalable distributed data structures.
In this paper we propose a TH* distributed variant of the Trie Hashing data
structure. First we propose Thsw new version of TH without node Nil in digital
tree (trie), then this version will be adapted to multicomputer environment.
The simulation results reveal that TH* is scalable in the sense that it grows
gracefully, one bucket at a time, to a large number of servers, also TH* offers
a good storage space utilization and high query efficiency special for ordering
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0451</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0451</id><created>2012-05-02</created><updated>2012-05-14</updated><authors><author><keyname>Abolfazli</keyname><forenames>Saeid</forenames></author><author><keyname>Sanaei</keyname><forenames>Zohreh</forenames></author><author><keyname>Gani</keyname><forenames>Abdullah</forenames></author></authors><title>Mobile Cloud Computing: A Review on Smartphone Augmentation Approaches</title><categories>cs.DC</categories><journal-ref>Saeid Abolfazli, Zohreh Sanaei, Abdullah Gani, Mobile Cloud
  Computing: A Review on Smartphone Augmentation Approaches, 1st International
  Conference on Computing, Information Systems, and Communications(CISCO'12),
  May 2012, Singapore</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Smartphones have recently gained significant popularity in heavy mobile
processing while users are increasing their expectations toward rich computing
experience. However, resource limitations and current mobile computing
advancements hinder this vision. Therefore, resource-intensive application
execution remains a challenging task in mobile computing that necessitates
device augmentation. In this article, smartphone augmentation approaches are
reviewed and classified in two main groups, namely hardware and software.
Generating high-end hardware is a subset of hardware augmentation approaches,
whereas conserving local resource and reducing resource requirements approaches
are grouped under software augmentation methods. Our study advocates that
consreving smartphones' native resources, which is mainly done via task
offloading, is more appropriate for already-developed applications than new
ones, due to costly re-development process. Cloud computing has recently
obtained momentous ground as one of the major cornerstone technologies in
augmenting smartphones. We present sample execution model for intensive mobile
applications and devised taxonomy of augmentation approaches. For better
comprehension, the results of this study are summarized in a table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0456</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0456</id><created>2012-05-02</created><authors><author><keyname>Lella</keyname><forenames>Paolo</forenames></author></authors><title>An efficient implementation of the algorithm computing the Borel-fixed
  points of a Hilbert scheme</title><categories>cs.SC math.AC math.AG math.CO</categories><comments>11 pages. Comments are welcome. Accepted for ISSAC 2012</comments><msc-class>14C05, 05E40, 13P99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Borel-fixed ideals play a key role in the study of Hilbert schemes. Indeed
each component and each intersection of components of a Hilbert scheme contains
at least one Borel-fixed point, i.e. a point corresponding to a subscheme
defined by a Borel-fixed ideal. Moreover Borel-fixed ideals have good
combinatorial properties, which make them very interesting in an algorithmic
perspective. In this paper, we propose an implementation of the algorithm
computing all the saturated Borel-fixed ideals with number of variables and
Hilbert polynomial assigned, introduced from a theoretical point of view in the
paper &quot;Segment ideals and Hilbert schemes of points&quot;, Discrete Mathematics 311
(2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0458</identifier>
 <datestamp>2012-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0458</id><created>2012-05-02</created><updated>2012-07-05</updated><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>Benabbas</keyname><forenames>Siavosh</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author></authors><title>Better Balance by Being Biased: A 0.8776-Approximation for Max Bisection</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently Raghavendra and Tan (SODA 2012) gave a 0.85-approximation algorithm
for the Max Bisection problem. We improve their algorithm to a
0.8776-approximation. As Max Bisection is hard to approximate within
$\alpha_{GW} + \epsilon \approx 0.8786$ under the Unique Games Conjecture
(UGC), our algorithm is nearly optimal. We conjecture that Max Bisection is
approximable within $\alpha_{GW}-\epsilon$, i.e., the bisection constraint
(essentially) does not make Max Cut harder.
  We also obtain an optimal algorithm (assuming the UGC) for the analogous
variant of Max 2-Sat. Our approximation ratio for this problem exactly matches
the optimal approximation ratio for Max 2-Sat, i.e., $\alpha_{LLZ} + \epsilon
\approx 0.9401$, showing that the bisection constraint does not make Max 2-Sat
harder. This improves on a 0.93-approximation for this problem due to
Raghavendra and Tan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0477</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0477</id><created>2012-05-02</created><updated>2013-03-03</updated><authors><author><keyname>Denysyuk</keyname><forenames>Oksana</forenames></author><author><keyname>Rodrigues</keyname><forenames>Luis</forenames></author></authors><title>Order-preserving Renaming in Synchronous Message Passing Systems with
  Byzantine Faults</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renaming is a fundamental problem in distributed computing, which consists of
a set of processes picking distinct names from a given namespace. The paper
presents algorithms that solve order-preserving renaming in synchronous message
passing systems with Byzantine processes. To the best of our knowledge, this
work is the first to address order-preserving renaming in the given model.
Although this problem can be solved by using consensus, it is known that
renaming is &quot;weaker&quot; than consensus, therefore we are mainly concerned with the
efficiency of performing renaming and make three contributions in this
direction. We present an order-preserving renaming algorithm for $N &gt; 3t$ with
target namespace of size $N+t-1$ and logarithmic step complexity (where $N$ is
the number of processes and $t$ is an upper bound on the number of faults).
Similarly to the existing crash-tolerant solution, our algorithm employs the
ideas from the approximate agreement problem. We show that our algorithm has
constant step complexity if $N&gt;t^2+2t$ and achieves tight namespace of size
$N$. Finally, we present an algorithm that solves order-preserving renaming in
just 2 communication steps, if $N &gt; 2t^2 + t$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0480</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0480</id><created>2012-05-02</created><authors><author><keyname>Keshavarz</keyname><forenames>Hassan</forenames></author><author><keyname>Sattari</keyname><forenames>Mohammad Reza Jabbarpour</forenames></author><author><keyname>Noor</keyname><forenames>Rafidah Md</forenames></author></authors><title>Session Initiation Protocol Attacks and Challenges</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In recent years, Session Initiation Protocol (SIP) has become widely used in
current internet protocols. It is a text-based protocol much like Hyper Text
Transport Protocol (HTTP) and Simple Mail Transport Protocol (SMTP). SIP is a
strong enough signaling protocol on the internet for establishing, maintaining,
and terminating session. In this paper the areas of security and attacks in SIP
are discussed. We consider attacks from diverse related perspectives. The
authentication schemes are compared, the representative existing solutions are
highlighted, and several remaining research challenges are identified. Finally,
the taxonomy of SIP threat will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0483</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0483</id><created>2012-05-02</created><authors><author><keyname>Calzolari</keyname><forenames>Federico</forenames></author><author><keyname>Arezzini</keyname><forenames>Silvia</forenames></author><author><keyname>Ciampa</keyname><forenames>Alberto</forenames></author><author><keyname>Mazzoni</keyname><forenames>Enrico</forenames></author><author><keyname>Domenici</keyname><forenames>Andrea</forenames></author><author><keyname>Vaglini</keyname><forenames>Gigliola</forenames></author></authors><title>High availability using virtualization - 3RC</title><categories>cs.SY</categories><comments>10 pages</comments><acm-class>C.4</acm-class><journal-ref>J. Phys.: Conf. Ser. 2010 219 052017</journal-ref><doi>10.1088/1742-6596/219/5/052017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High availability has always been one of the main problems for a data center.
Till now high availability was achieved by host per host redundancy, a highly
expensive method in terms of hardware and human costs. A new approach to the
problem can be offered by virtualization. Using virtualization, it is possible
to achieve a redundancy system for all the services running on a data center.
This new approach to high availability allows the running virtual machines to
be distributed over a small number of servers, by exploiting the features of
the virtualization layer: start, stop and move virtual machines between
physical hosts. The 3RC system is based on a finite state machine, providing
the possibility to restart each virtual machine over any physical host, or
reinstall it from scratch. A complete infrastructure has been developed to
install operating system and middleware in a few minutes. To virtualize the
main servers of a data center, a new procedure has been developed to migrate
physical to virtual hosts. The whole Grid data center SNS-PISA is running at
the moment in virtual environment under the high availability system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0537</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0537</id><created>2012-05-02</created><updated>2012-12-18</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>A greedy-navigator approach to navigable city plans</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 9 figures, 3 tables</comments><journal-ref>Eur. Phys. J.-Spec. Top. 215, 135 (2013)</journal-ref><doi>10.1140/epjst/e2013-01720-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a set of four theoretical navigability indices for street maps to
investigate the shape of the resulting street networks, if they are grown by
optimizing these indices. The indices compare the performance of simulated
navigators (having a partial information about the surroundings, like humans in
many real situations) to the performance of optimally navigating individuals.
We show that our simple greedy shortcut construction strategy generates the
emerging structures that are different from real road network, but not
inconceivable. The resulting city plans, for all navigation indices, share
common qualitative properties such as the tendency for triangular blocks to
appear, while the more quantitative features, such as degree distributions and
clustering, are characteristically different depending on the type of metrics
and routing strategies. We show that it is the type of metrics used which
determines the overall shapes characterized by structural heterogeneity, but
the routing schemes contribute to more subtle details of locality, which is
more emphasized in case of unrestricted connections when the edge crossing is
allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0540</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0540</id><created>2012-05-02</created><authors><author><keyname>Ke</keyname><forenames>Weimao</forenames></author></authors><title>A Fitness Model for Scholarly Impact Analysis</title><categories>stat.AP cs.DL cs.SI physics.soc-ph</categories><comments>19 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a model to analyze citation growth and influences of fitness
(competitiveness) factors in an evolving citation network. Applying the
proposed method to modeling citations to papers and scholars in the InfoVis
2004 data, a benchmark collection about a 31-year history of information
visualization, leads to findings consistent with citation distributions in
general and observations of the domain in particular. Fitness variables based
on prior impacts and the time factor have significant influences on citation
outcomes. We find considerably large effect sizes from the fitness modeling,
which suggest inevitable bias in citation analysis due to these factors. While
raw citation scores offer little insight into the growth of InfoVis,
normalization of the scores by influences of time and prior fitness offers a
reasonable depiction of the field's development. The analysis demonstrates the
proposed model's ability to produce results consistent with observed data and
to support meaningful comparison of citation scores over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0541</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0541</id><created>2012-05-02</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Percolation threshold determines the optimal population density for
  public cooperation</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>4 two-column pages, 5 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 85 (2012) 037101</journal-ref><doi>10.1103/PhysRevE.85.037101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While worldwide census data provide statistical evidence that firmly link the
population density with several indicators of social welfare, the precise
mechanisms underlying these observations are largely unknown. Here we study the
impact of population density on the evolution of public cooperation in
structured populations and find that the optimal density is uniquely related to
the percolation threshold of the host graph irrespective of its topological
details. We explain our observations by showing that spatial reciprocity peaks
in the vicinity of the percolation threshold, when the emergence of a giant
cooperative cluster is hindered neither by vacancy nor by invading defectors,
thus discovering an intuitive yet universal law that links the population
density with social prosperity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0561</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0561</id><created>2012-05-02</created><updated>2013-11-15</updated><authors><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author></authors><title>Multi-level agent-based modeling - A literature survey</title><categories>cs.MA</categories><comments>v2. Ref 102 added. v3-4 Many refs and text added v5-6 bibliographic
  statistics updated. v7 Change of the name of the paper to reflect what it
  became, many refs and text added, bibliographic statistics updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During last decade, multi-level agent-based modeling has received significant
and dramatically increasing interest. In this article we present a
comprehensive and structured review of literature on the subject. We present
the main theoretical contributions and application domains of this concept,
with an emphasis on social, flow, biological and biomedical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0581</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0581</id><created>2012-05-02</created><authors><author><keyname>Dani</keyname><forenames>Varsha</forenames></author><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author></authors><title>Scalable Mechanisms for Rational Secret Sharing</title><categories>cs.DS</categories><comments>30 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical secret sharing problem in the case where all agents
are selfish but rational. In recent work, Kol and Naor show that, when there
are two players, in the non-simultaneous communication model, i.e. when rushing
is possible, there is no Nash equilibrium that ensures both players learn the
secret. However, they describe a mechanism for this problem, for any number of
players, that is an epsilon-Nash equilibrium, in that no player can gain more
than epsilon utility by deviating from it. Unfortunately, the Kol and Naor
mechanism, and, to the best of our knowledge, all previous mechanisms for this
problem require each agent to send O(n) messages in expectation, where n is the
number of agents. This may be problematic for some applications of rational
secret sharing such as secure multi-party computation and simulation of a
mediator.
  We address this issue by describing mechanisms for rational secret sharing
that are designed for large n. Both of our results hold for n &gt; 2, and are Nash
equilbria, rather than just epsilon-Nash equilbria.
  Our first result is a mechanism for n-out-of-n rational secret sharing that
is scalable in the sense that it requires each agent to send only an expected
O(log n) bits. Moreover, the latency of this mechanism is O(log n) in
expectation, compared to O(n) expected latency for the Kol and Naor result. Our
second result is a mechanism for a relaxed variant of rational m-out-of-n
secret sharing where m = Theta(n). It requires each processor to send O(log n)
bits and has O(log n) latency. Both of our mechanisms are non-cryptographic,
and are not susceptible to backwards induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0586</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0586</id><created>2012-05-02</created><authors><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Xie</keyname><forenames>Hongmei</forenames></author></authors><title>Enhanced Algebraic Error Control for Random Linear Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error control is significant to network coding, since when unchecked, errors
greatly deteriorate the throughput gains of network coding and seriously
undermine both reliability and security of data. Two families of codes,
subspace and rank metric codes, have been used to provide error control for
random linear network coding. In this paper, we enhance the error correction
capability of these two families of codes by using a novel two-tier decoding
scheme. While the decoding of subspace and rank metric codes serves a
second-tier decoding, we propose to perform a first-tier decoding on the packet
level by taking advantage of Hamming distance properties of subspace and rank
metric codes. This packet-level decoding can also be implemented by
intermediate nodes to reduce error propagation. To support the first-tier
decoding, we also investigate Hamming distance properties of three important
families of subspace and rank metric codes, Gabidulin codes,
Kotter--Kschischang codes, and Mahdavifar--Vardy codes. Both the two-tier
decoding scheme and the Hamming distance properties of these codes are novel to
the best of our knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0591</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0591</id><created>2012-05-02</created><authors><author><keyname>Agarwal</keyname><forenames>Deepak</forenames></author><author><keyname>Chen</keyname><forenames>Bee-Chung</forenames></author><author><keyname>Wang</keyname><forenames>Xuanhui</forenames></author></authors><title>Multi-Faceted Ranking of News Articles using Post-Read Actions</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalized article recommendation is important to improve user engagement
on news sites. Existing work quantifies engagement primarily through click
rates. We argue that quality of recommendations can be improved by
incorporating different types of &quot;post-read&quot; engagement signals like sharing,
commenting, printing and e-mailing article links. More specifically, we propose
a multi-faceted ranking problem for recommending news articles where each facet
corresponds to a ranking problem to maximize actions of a post-read action
type. The key technical challenge is to estimate the rates of post-read action
types by mitigating the impact of enormous data sparsity, we do so through
several variations of factor models. To exploit correlations among post-read
action types we also introduce a novel variant called locally augmented tensor
(LAT) model. Through data obtained from a major news site in the US, we show
that factor models significantly outperform a few baseline IR models and the
LAT model significantly outperforms several other variations of factor models.
Our findings show that it is possible to incorporate post-read signals that are
commonly available on online news sites to improve quality of recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0596</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0596</id><created>2012-05-02</created><updated>2013-01-21</updated><authors><author><keyname>Southwell</keyname><forenames>Richard</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Cannings</keyname><forenames>Chris</forenames></author></authors><title>Complex Networks from Simple Rewrite Systems</title><categories>cs.SI nlin.AO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks are all around us, and they can be generated by simple
mechanisms. Understanding what kinds of networks can be produced by following
simple rules is therefore of great importance. We investigate this issue by
studying the dynamics of extremely simple systems where are `writer' moves
around a network, and modifies it in a way that depends upon the writer's
surroundings. Each vertex in the network has three edges incident upon it,
which are colored red, blue and green. This edge coloring is done to provide a
way for the writer to orient its movement. We explore the dynamics of a space
of 3888 of these `colored trinet automata' systems. We find a large variety of
behaviour, ranging from the very simple to the very complex. We also discover
simple rules that generate forms which are remarkably similar to a wide range
of natural objects. We study our systems using simulations (with appropriate
visualization techniques) and analyze selected rules mathematically. We arrive
at an empirical classification scheme which reveals a lot about the kinds of
dynamics and networks that can be generated by these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0606</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0606</id><created>2012-05-02</created><updated>2015-01-22</updated><authors><author><keyname>Hupp</keyname><forenames>Philipp</forenames></author><author><keyname>Jacob</keyname><forenames>Riko</forenames></author></authors><title>Tight Bounds for Low Dimensional Star Stencils in the Parallel External
  Memory Model</title><categories>cs.CC</categories><comments>64 pages, 8 figures, 4 tables</comments><report-no>eth-5672-01</report-no><journal-ref>WADS 2013, LNCS 8037, pp. 415-426, 2013</journal-ref><doi>10.1007/978-3-642-40104-6_36</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stencil computations on low dimensional grids are kernels of many scientific
applications including finite difference methods used to solve partial
differential equations. On typical modern computer architectures, such stencil
computations are limited by the performance of the memory subsystem, namely by
the bandwidth between main memory and the cache. This work considers the
computation of star stencils, like the 5-point and 7-point stencil, in the
external memory model and parallel external memory model and analyses the
constant of the leading term of the non-compulsory I/Os. While optimizing
stencil computations is an active field of research, there has been a
significant gap between the lower bounds and the performance of the algorithms
so far. In two dimensions, this work provides matching constants for lower and
upper bounds closing a multiplicative gap of 4. In three dimensions, the bounds
match up to a factor of $\sqrt{2}$ improving the known results by a factor of
$2 \sqrt{3}\sqrt{B}$, where $B$ is the block (cache line) size of the external
memory model. For dimensions $d\geq 4$, the lower bound is improved between a
factor of $4$ and $6$. For arbitrary dimension~$d$, the first analysis of the
constant of the leading term of the non-compulsory I/Os is presented. For
$d\geq 3$ the lower and upper bound match up to a factor of
$\sqrt[d-1]{d!}\approx \frac{d}{e}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0610</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0610</id><created>2012-05-03</created><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Corso</keyname><forenames>Jason</forenames></author></authors><title>Greedy Multiple Instance Learning via Codebook Learning and Nearest
  Neighbor Voting</title><categories>cs.LG</categories><comments>12 pages</comments><acm-class>I.5.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Multiple instance learning (MIL) has attracted great attention recently in
machine learning community. However, most MIL algorithms are very slow and
cannot be applied to large datasets. In this paper, we propose a greedy
strategy to speed up the multiple instance learning process. Our contribution
is two fold. First, we propose a density ratio model, and show that maximizing
a density ratio function is the low bound of the DD model under certain
conditions. Secondly, we make use of a histogram ratio between positive bags
and negative bags to represent the density ratio function and find codebooks
separately for positive bags and negative bags by a greedy strategy. For
testing, we make use of a nearest neighbor strategy to classify new bags. We
test our method on both small benchmark datasets and the large TRECVID MED11
dataset. The experimental results show that our method yields comparable
accuracy to the current state of the art, while being up to at least one order
of magnitude faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0618</identifier>
 <datestamp>2013-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0618</id><created>2012-05-03</created><updated>2013-09-05</updated><authors><author><keyname>Zhou</keyname><forenames>Xun</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author></authors><title>Wireless Information and Power Transfer: Architecture Design and
  Rate-Energy Tradeoff</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous information and power transfer over the wireless channels
potentially offers great convenience to mobile users. Yet practical receiver
designs impose technical constraints on its hardware realization, as practical
circuits for harvesting energy from radio signals are not yet able to decode
the carried information directly. To make theoretical progress, we propose a
general receiver operation, namely, dynamic power splitting (DPS), which splits
the received signal with adjustable power ratio for energy harvesting and
information decoding, separately. Three special cases of DPS, namely, time
switching (TS), static power splitting (SPS) and on-off power splitting (OPS)
are investigated. The TS and SPS schemes can be treated as special cases of
OPS. Moreover, we propose two types of practical receiver architectures,
namely, separated versus integrated information and energy receivers. The
integrated receiver integrates the front-end components of the separated
receiver, thus achieving a smaller form factor. The rate-energy tradeoff for
the two architectures are characterized by a so-called rate-energy (R-E)
region. The optimal transmission strategy is derived to achieve different
rate-energy tradeoffs. With receiver circuit power consumption taken into
account, it is shown that the OPS scheme is optimal for both receivers. For the
ideal case when the receiver circuit does not consume power, the SPS scheme is
optimal for both receivers. In addition, we study the performance for the two
types of receivers under a realistic system setup that employs practical
modulation. Our results provide useful insights to the optimal practical
receiver design for simultaneous wireless information and power transfer
(SWIPT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0622</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0622</id><created>2012-05-03</created><authors><author><keyname>Lanctot</keyname><forenames>Marc</forenames></author><author><keyname>Gibson</keyname><forenames>Richard</forenames></author><author><keyname>Burch</keyname><forenames>Neil</forenames></author><author><keyname>Zinkevich</keyname><forenames>Martin</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>No-Regret Learning in Extensive-Form Games with Imperfect Recall</title><categories>cs.GT cs.AI</categories><comments>21 pages, 4 figures, expanded version of article to appear in
  Proceedings of the Twenty-Ninth International Conference on Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterfactual Regret Minimization (CFR) is an efficient no-regret learning
algorithm for decision problems modeled as extensive games. CFR's regret bounds
depend on the requirement of perfect recall: players always remember
information that was revealed to them and the order in which it was revealed.
In games without perfect recall, however, CFR's guarantees do not apply. In
this paper, we present the first regret bound for CFR when applied to a general
class of games with imperfect recall. In addition, we show that CFR applied to
any abstraction belonging to our general class results in a regret bound not
just for the abstract game, but for the full game as well. We verify our theory
and show how imperfect recall can be used to trade a small increase in regret
for a significant reduction in memory in three domains: die-roll poker, phantom
tic-tac-toe, and Bluff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0626</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0626</id><created>2012-05-03</created><updated>2013-01-24</updated><authors><author><keyname>Jedwab</keyname><forenames>Jonathan</forenames></author><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author></authors><title>Advances in the merit factor problem for binary sequences</title><categories>math.CO cs.IT math.IT</categories><comments>31 pages, minor revisions</comments><msc-class>94A55, 11B83 (Primary) 11T24 (Secondary)</msc-class><journal-ref>J. Combin. Theory Ser. A, 120(4), 882-906, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of binary sequences with large merit factor (small
mean-squared aperiodic autocorrelation) is an old problem of complex analysis
and combinatorial optimization, with practical importance in digital
communications engineering and condensed matter physics. We establish the
asymptotic merit factor of several families of binary sequences and thereby
prove various conjectures, explain numerical evidence presented by other
authors, and bring together within a single framework results previously
appearing in scattered form. We exhibit, for the first time, families of
skew-symmetric sequences whose asymptotic merit factor is as large as the best
known value (an algebraic number greater than 6.34) for all binary sequences;
this is interesting in light of Golay's conjecture that the subclass of
skew-symmetric sequences has asymptotically optimal merit factor. Our methods
combine Fourier analysis, estimation of character sums, and estimation of the
number of lattice points in polyhedra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0627</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0627</id><created>2012-05-03</created><authors><author><keyname>Ponty</keyname><forenames>Yann</forenames><affiliation>LIX, INRIA Saclay - Ile de France</affiliation></author></authors><title>Rule-weighted and terminal-weighted context-free grammars have identical
  expressivity</title><categories>cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two formalisms, both based on context-free grammars, have recently been
proposed as a basis for a non-uniform random generation of combinatorial
objects. The former, introduced by Denise et al, associates weights with
letters, while the latter, recently explored by Weinberg et al in the context
of random generation, associates weights to transitions. In this short note, we
use a simple modification of the Greibach Normal Form transformation algorithm,
due to Blum and Koch, to show the equivalent expressivities, in term of their
induced distributions, of these two formalisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0642</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0642</id><created>2012-05-03</created><updated>2013-12-11</updated><authors><author><keyname>Rosenbaum</keyname><forenames>David</forenames></author></authors><title>Breaking the n^(log n) Barrier for Solvable-Group Isomorphism</title><categories>cs.DS cs.DM quant-ph</categories><comments>24 pages for the main body, 29 pages of appendices and references, 2
  figures. v1 shows the algorithm for nilpotent groups. v2 makes a minor
  improvement. v3 generalizes to solvable groups. v4 contains additional
  discussions, clarifications and new side results. v5 makes the construction
  of the canonical forms explicit (expanding on a remark in v4). v6 makes
  various minor corrections. The portion of this paper on p-groups has been
  replaced by arXiv:1312.1755</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the group isomorphism problem: given two finite groups G and H
specified by their multiplication tables, decide if G and H are isomorphic. The
n^(log n) barrier for group isomorphism has withstood all attacks --- even for
the special cases of p-groups and solvable groups --- ever since the n^(log n +
O(1)) generator-enumeration algorithm. In this work, we present the first
significant improvement over n^(log n) by showing that group isomorphism is
n^((1 / 2) log_p n + O(1)) Turing reducible to composition-series isomorphism
where p is the smallest prime dividing the order of the group. Combining our
reduction with an n^(O(p / log p)) algorithm for p-group composition-series
isomorphism, we obtain an n^((1 / 2) log n + O(1)) algorithm for p-group
isomorphism. We then generalize our techniques from p-groups using Sylow bases
to derive an n^((1 / 2) log n + O(log n / log log n)) algorithm for
solvable-group isomorphism. Finally, we relate group isomorphism to the
collision problem which allows us replace the 1 / 2 in the exponents with 1 / 4
using randomized algorithms and 1 / 6 using quantum algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0646</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0646</id><created>2012-05-03</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>On the calculation of percentile-based bibliometric indicators</title><categories>cs.DL physics.soc-ph</categories><journal-ref>Journal of the American Society for Information Science and
  Technology 64 (2013) 372-379</journal-ref><doi>10.1002/asi.22775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A percentile-based bibliometric indicator is an indicator that values
publications based on their position within the citation distribution of their
field. The most straightforward percentile-based indicator is the proportion of
frequently cited publications, for instance the proportion of publications that
belong to the top 10% most frequently cited of their field. Recently, more
complex percentile-based indicators were proposed. A difficulty in the
calculation of percentile-based indicators is caused by the discrete nature of
citation distributions combined with the presence of many publications with the
same number of citations. We introduce an approach to calculating
percentile-based indicators that deals with this difficulty in a more
satisfactory way than earlier approaches suggested in the literature. We show
in a formal mathematical framework that our approach leads to indicators that
do not suffer from biases in favor of or against particular fields of science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0651</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0651</id><created>2012-05-03</created><updated>2013-12-30</updated><authors><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Pandey</keyname><forenames>Gaurav</forenames></author><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Koley</keyname><forenames>Paramita</forenames></author><author><keyname>Sriram</keyname><forenames>D. M. V. Satya</forenames></author></authors><title>Generative Maximum Entropy Learning for Multiclass Classification</title><categories>cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum entropy approach to classification is very well studied in applied
statistics and machine learning and almost all the methods that exists in
literature are discriminative in nature. In this paper, we introduce a maximum
entropy classification method with feature selection for large dimensional data
such as text datasets that is generative in nature. To tackle the curse of
dimensionality of large data sets, we employ conditional independence
assumption (Naive Bayes) and we perform feature selection simultaneously, by
enforcing a `maximum discrimination' between estimated class conditional
densities. For two class problems, in the proposed method, we use Jeffreys
($J$) divergence to discriminate the class conditional densities. To extend our
method to the multi-class case, we propose a completely new approach by
considering a multi-distribution divergence: we replace Jeffreys divergence by
Jensen-Shannon ($JS$) divergence to discriminate conditional densities of
multiple classes. In order to reduce computational complexity, we employ a
modified Jensen-Shannon divergence ($JS_{GM}$), based on AM-GM inequality. We
show that the resulting divergence is a natural generalization of Jeffreys
divergence to a multiple distributions case. As far as the theoretical
justifications are concerned we show that when one intends to select the best
features in a generative maximum entropy approach, maximum discrimination using
$J-$divergence emerges naturally in binary classification. Performance and
comparative study of the proposed algorithms have been demonstrated on large
dimensional text and gene expression datasets that show our methods scale up
very well with large dimensional datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0652</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0652</id><created>2012-05-03</created><updated>2014-06-27</updated><authors><author><keyname>Yuan</keyname><forenames>Peiyan</forenames></author><author><keyname>Ma</keyname><forenames>Huadong</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author></authors><title>On Exploiting Hotspot and Entropy for Data Forwarding in Delay Tolerant
  Networks</title><categories>cs.DC</categories><comments>This paper has been withdrawn by the author due to further studies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance of data forwarding in Delay Tolerant Networks (DTNs) benefits
considerably if one can make use of human mobility in terms of social
structures. However, it is difficult and time-consuming to calculate the
centrality and similarity of nodes by using solutions for traditional social
networks, this is mainly because of the transient node contact and the
intermittently connected environment. In this work, we are interested in the
following question: Can we explore some other stable social attributes to
quantify the centrality and similarity of nodes? Taking GPS traces of human
walks from the real world, we find that there exist two known phenomena. One is
public hotspot, the other is personal hotspot. Motivated by this observation,
we present Hoten (hotspot and entropy), a novel routing metric to improve
routing performance in DTNs. First, we use the relative entropy between the
public hotspots and the personal hotspots to compute the centrality of nodes.
Then we utilize the inverse symmetrized entropy of the personal hotspots
between two nodes to compute the similarity between them. Third, we exploit the
entropy of personal hotspots of a node to estimate its personality. Besides, we
propose a method to ascertain the optimized size of hotspot. Finally, we
compare our routing strategy with other state-of-the-art routing schemes
through extensive trace-driven simulations, the results show that Hoten largely
outperforms other solutions, especially in terms of combined overhead/packet
delivery ratio and the average number of hops per message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0668</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0668</id><created>2012-05-03</created><updated>2012-06-18</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Zhou</keyname><forenames>Ping</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>How Can Journal Impact Factors be Normalized across Fields of Science?
  An Assessment in terms of Percentile Ranks and Fractional Counts</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the CD-ROM version of the Science Citation Index 2010 (N = 3,705
journals), we study the (combined) effects of (i) fractional counting on the
impact factor (IF) and (ii) transformation of the skewed citation distributions
into a distribution of 100 percentiles and six percentile rank classes (top-1%,
top-5%, etc.). Do these approaches lead to field-normalized impact measures for
journals? In addition to the two-year IF (IF2), we consider the five-year IF
(IF5), the respective numerators of these IFs, and the number of Total Cites,
counted both as integers and fractionally. These various indicators are tested
against the hypothesis that the classification of journals into 11 broad fields
by PatentBoard/National Science Foundation provides statistically significant
between-field effects. Using fractional counting the between-field variance is
reduced by 91.7% in the case of IF5, and by 79.2% in the case of IF2. However,
the differences in citation counts are not significantly affected by fractional
counting. These results accord with previous studies, but the longer citation
window of a fractionally counted IF5 can lead to significant improvement in the
normalization across fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0679</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0679</id><created>2012-05-03</created><updated>2013-10-07</updated><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Lower Bounds for Existential Pebble Games and k-Consistency Tests</title><categories>cs.LO cs.CC</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (October 8,
  2013) lmcs:1010</journal-ref><doi>10.2168/LMCS-9(4:2)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existential k-pebble game characterizes the expressive power of the
existential-positive k-variable fragment of first-order logic on finite
structures. The winner of the existential k-pebble game on two given finite
structures can be determined in time O(n2k) by dynamic programming on the graph
of game configurations. We show that there is no O(n(k-3)/12)-time algorithm
that decides which player can win the existential k-pebble game on two given
structures. This lower bound is unconditional and does not rely on any
complexity-theoretic assumptions. Establishing strong k-consistency is a
well-known heuristic for solving the constraint satisfaction problem (CSP). By
the game characterization of Kolaitis and Vardi our result implies that there
is no O(n(k-3)/12)-time algorithm that decides if strong k-consistency can be
established for a given CSP-instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0680</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0680</id><created>2012-05-03</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Citation impact of papers published from six prolific countries: A
  national comparison based on InCites data</title><categories>cs.DL stat.AP</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the InCites tool of Thomson Reuters, this study compares normalized
citation impact values calculated for China, Japan, France, Germany, United
States, and the UK throughout the time period from 1981 to 2010. The citation
impact values are normalized to four subject areas: natural sciences;
engineering and technology; medical and health sciences; and agricultural
sciences. The results show an increasing trend in citation impact values for
France, the UK and especially for Germany across the last thirty years in all
subject areas. The citation impact of papers from China is still at a
relatively low level (mostly below the world average), but the country follows
an increasing trend line. The USA exhibits a relatively stable pattern of high
citation impact values across the years. With small impact differences between
the publication years, the US trend is increasing in engineering and technology
but decreasing in medical and health sciences as well as in agricultural
sciences. Similar to the USA, Japan follows increasing as well as decreasing
trends in different subject areas, but the variability across the years is
small. In most of the years, papers from Japan perform below or approximately
at the world average in each subject area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0699</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0699</id><created>2012-05-03</created><updated>2012-05-11</updated><authors><author><keyname>Duyck</keyname><forenames>Dieter</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author><author><keyname>Takawira</keyname><forenames>Fambirai</forenames></author><author><keyname>Boutros</keyname><forenames>Joseph J.</forenames></author><author><keyname>Moeneclaey</keyname><forenames>Marc</forenames></author></authors><title>Time-Varying Space-Only Codes for Coded MIMO</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on Inf. Theory, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple antenna (MIMO) devices are widely used to increase reliability and
information bit rate. Optimal error rate performance (full diversity and large
coding gain), for unknown channel state information at the transmitter and for
maximal rate, can be achieved by approximately universal space-time codes, but
comes at a price of large detection complexity, infeasible for most practical
systems. We propose a new coded modulation paradigm: error-correction outer
code with space-only but time-varying precoder (as inner code). We refer to the
latter as Ergodic Mutual Information (EMI) code. The EMI code achieves the
maximal multiplexing gain and full diversity is proved in terms of the outage
probability. Contrary to most of the literature, our work is not based on the
elegant but difficult classical algebraic MIMO theory. Instead, the relation
between MIMO and parallel channels is exploited. The theoretical proof of full
diversity is corroborated by means of numerical simulations for many MIMO
scenarios, in terms of outage probability and word error rate of LDPC coded
systems. The full-diversity and full-rate at low detection complexity comes at
a price of a small coding gain loss for outer coding rates close to one, but
this loss vanishes with decreasing coding rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0703</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0703</id><created>2012-05-03</created><authors><author><keyname>Hurley</keyname><forenames>Barry</forenames></author><author><keyname>Hurley</keyname><forenames>Ted</forenames></author></authors><title>Paraunitary Matrices</title><categories>cs.IT math.IT</categories><msc-class>15B99, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design methods for paraunitary matrices from complete orthogonal sets of
idempotents and related matrix structures are presented. These include
techniques for designing non-separable multidimensional paraunitary matrices.
Properties of the structures are obtained and proofs given. Paraunitary
matrices play a central role in signal processing, in particular in the areas
of filterbanks and wavelets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0722</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0722</id><created>2012-05-03</created><authors><author><keyname>Meier</keyname><forenames>Arne</forenames></author></authors><title>Generalized Complexity of ALC Subsumption</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subsumption problem with respect to terminologies in the description
logic ALC is EXPTIME-complete. We investigate the computational complexity of
fragments of this problem by means of allowed Boolean operators. Hereto we make
use of the notion of clones in the context of Post's lattice. Furthermore we
consider all four possible quantifier combinations for each fragment
parameterized by a clone. We will see that depending on what quantifiers are
available the classification will be either tripartite or a quartering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0724</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0724</id><created>2011-12-14</created><authors><author><keyname>Nguyen</keyname><forenames>Phuc V.</forenames><affiliation>Hung Vuong Univesity, Ho Chi Minh City</affiliation></author></authors><title>Using Data Warehouse to Support Building Strategy or Forecast Business
  Tend</title><categories>cs.DB</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data warehousing is becoming increasingly important in terms of strategic
decision making through their capacity to integrate heterogeneous data from
multiple information sources in a common storage space, for querying and
analysis. So it can evolve into a multi-tier structure where parts of the
organization take information from the main data warehouse into their own
systems. These may include analysis databases or dependent data marts. As the
data warehouse evolves and the organization gets better at capturing
information on all interactions with the customer. Data warehouse can track
customer interactions over the whole of the customer's lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0730</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0730</id><created>2012-05-03</created><updated>2012-10-29</updated><authors><author><keyname>Fouquet</keyname><forenames>Jean-Luc</forenames><affiliation>LIFO</affiliation></author><author><keyname>Vanherpe</keyname><forenames>Jean-Marie</forenames><affiliation>LIFO</affiliation></author></authors><title>Reed's conjecture on some special classes of graphs</title><categories>cs.DM</categories><comments>Submitted to Discrete Mathematics</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reed conjectured that for any graph $G$, $\chi(G) \leq \lceil
\frac{\omega(G)+\Delta(G)+1}{2}\rceil$, where $\chi(G)$, $\omega(G)$, and
$\Delta(G)$ respectively denote the chromatic number, the clique number and the
maximum degree of $G$. In this paper, we verify this conjecture for some
special classes of graphs, in particular for subclasses of $P_5$-free graphs or
$Chair$-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0731</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0731</id><created>2012-05-03</created><updated>2012-10-29</updated><authors><author><keyname>Fouquet</keyname><forenames>Jean-Luc</forenames><affiliation>LIFO</affiliation></author><author><keyname>Vanherpe</keyname><forenames>Jean-Marie</forenames><affiliation>LIFO</affiliation></author></authors><title>Reed's Conjecture on hole expansions</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1998, Reed conjectured that for any graph $G$, $\chi(G) \leq \lceil
\frac{\omega(G) + \Delta(G)+1}{2}\rceil$, where $\chi(G)$, $\omega(G)$, and
$\Delta(G)$ respectively denote the chromatic number, the clique number and the
maximum degree of $G$. In this paper, we study this conjecture for some
expansions of graphs, that is graphs obtained with the well known operation
composition of graphs.
  We prove that Reed's Conjecture holds for expansions of bipartite graphs, for
expansions of odd holes where the minimum chromatic number of the components is
even, when some component of the expansion has chromatic number 1 or when a
component induces a bipartite graph. Moreover, Reed's Conjecture holds if all
components have the same chromatic number, if the components have chromatic
number at most 4 and when the odd hole has length 5. Finally, when $G$ is an
odd hole expansion, we prove
$\chi(G)\leq\lceil\frac{\omega(G)+\Delta(G)+1}{2}\rceil+1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0732</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0732</id><created>2012-05-03</created><authors><author><keyname>Kryzhanovsky</keyname><forenames>Boris</forenames></author><author><keyname>Kryzhanovsky</keyname><forenames>Mikhail</forenames></author><author><keyname>Malsagov</keyname><forenames>Magomed</forenames></author></authors><title>Discretization of a matrix in the problem of quadratic functional binary
  minimization</title><categories>cs.NE</categories><comments>11 pages, 4 figures, in Russian language</comments><journal-ref>Doklady Mathematics, 2011,vol. 83, No. 3, pp.1-5</journal-ref><doi>10.1134/S1064562411030197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capability of discretization of matrix elements in the problem of
quadratic functional minimization with linear member built on matrix in
N-dimensional configuration space with discrete coordinates is researched. It
is shown, that optimal procedure of replacement matrix elements by the integer
quantities with the limited number of gradations exist, and the efficient of
minimization does not reduce. Parameter depends on matrix properties, which
allows estimate the capability of using described procedure for given type of
matrix, is found. Computational complexities of algorithm and RAM requirements
are reduced by 16 times, correct using of integer elements allows increase
minimization algorithm speed by the orders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0739</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0739</id><created>2012-05-03</created><authors><author><keyname>Uhler</keyname><forenames>Caroline</forenames></author><author><keyname>Slavkovic</keyname><forenames>Aleksandra B.</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E.</forenames></author></authors><title>Privacy-Preserving Data Sharing for Genome-Wide Association Studies</title><categories>stat.ME cs.CR</categories><msc-class>62F03, 68P25, 92D20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional statistical methods for confidentiality protection of statistical
databases do not scale well to deal with GWAS (genome-wide association studies)
databases especially in terms of guarantees regarding protection from linkage
to external information. The more recent concept of differential privacy,
introduced by the cryptographic community, is an approach which provides a
rigorous definition of privacy with meaningful privacy guarantees in the
presence of arbitrary external information, although the guarantees come at a
serious price in terms of data utility. Building on such notions, we propose
new methods to release aggregate GWAS data without compromising an individual's
privacy. We present methods for releasing differentially private minor allele
frequencies, chi-square statistics and p-values. We compare these approaches on
simulated data and on a GWAS study of canine hair length involving 685 dogs. We
also propose a privacy-preserving method for finding genome-wide associations
based on a differentially-private approach to penalized logistic regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0750</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0750</id><created>2012-05-03</created><authors><author><keyname>Fernandez-y-Fernandez</keyname><forenames>Carlos Alberto</forenames></author></authors><title>Towards a new metamodel for the Task Flow Model of the Discovery Method</title><categories>cs.SE</categories><comments>7 pages, ISBN: 978-607-607-082-6</comments><journal-ref>Congreso Internacional de Investigacion e Innovacion en Ingenieria
  de Software (Conisoft 2012), Guadalajara, Jalisco, April 25-27, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our proposal for the evolution of the metamodel for the
Task Algebra in the Task Flow model for the Discovery Method. The original Task
Algebra is based on simple and compound tasks structured using operators such
as sequence, selection, and parallel composition. Recursion and encapsulation
were also considered. We propose additional characteristics to improve the
capabilities of the metamodel to represent accurately the Task Flow Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0751</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0751</id><created>2012-05-03</created><authors><author><keyname>Fernandez-y-Fernandez</keyname><forenames>Carlos Alberto</forenames></author><author><keyname>Quintanar</keyname><forenames>Jose Angel</forenames></author></authors><title>Integrated Development Environment Gesture for modeling workflow
  diagrams</title><categories>cs.HC cs.SE</categories><comments>8 pages, ISBN: 978-607-607-082-6</comments><journal-ref>Congreso Internacional de Investigacion e Innovacion en Ingenieria
  de Software (Conisoft 2012), Guadalajara, Jalisco, April 25-27, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current software development tools show the same form of interaction as
when they started back, in the mid 70's. However, since the appearance of
visual languages and due to their own nature, they can be handled by tools
which have different input methods to conventional ones. By incorporating new
motion detection technology, it is intended that new forms of interaction are
established. Interactions which respond to the free movement of hands,
therefore the software's developer will have a substantial improvement in the
user experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0768</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0768</id><created>2012-05-03</created><authors><author><keyname>Poroseva</keyname><forenames>S. V.</forenames><affiliation>U. of New Mexico</affiliation></author><author><keyname>Rikvold</keyname><forenames>P. A.</forenames><affiliation>Florida State U.</affiliation></author></authors><title>Optimization of Survivability Analysis for Large-Scale Engineering
  Networks</title><categories>math.OC cs.SI physics.soc-ph</categories><comments>10 pages</comments><journal-ref>Proceedings of the Eighth International Conference on Engineering
  Computational Technology (Civil-Comp Press, Stirlingshire, United Kingdom,
  2012), paper 53</journal-ref><doi>10.4203/ccp.100.53</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Engineering networks fall into the category of large-scale networks with
heterogeneous nodes such as sources and sinks. The survivability analysis of
such networks requires the analysis of the connectivity of the network
components for every possible combination of faults to determine a network
response to each combination of faults. From the computational complexity point
of view, the problem belongs to the class of exponential time problems at
least. Partially, the problem complexity can be reduced by mapping the initial
topology of a complex large-scale network with multiple sources and multiple
sinks onto a set of smaller sub-topologies with multiple sources and a single
sink connected to the network of sources by a single link. In this paper, the
mapping procedure is applied to the Florida power grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0790</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0790</id><created>2012-05-03</created><updated>2012-05-15</updated><authors><author><keyname>Pawlowski</keyname><forenames>Roger P.</forenames></author><author><keyname>Phipps</keyname><forenames>Eric T.</forenames></author><author><keyname>Salinger</keyname><forenames>Andrew G.</forenames></author></authors><title>Automating embedded analysis capabilities and managing software
  complexity in multiphysics simulation part I: template-based generic
  programming</title><categories>cs.MS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach for incorporating embedded simulation and analysis capabilities
in complex simulation codes through template-based generic programming is
presented. This approach relies on templating and operator overloading within
the C++ language to transform a given calculation into one that can compute a
variety of additional quantities that are necessary for many state-of-the-art
simulation and analysis algorithms. An approach for incorporating these ideas
into complex simulation codes through general graph-based assembly is also
presented. These ideas have been implemented within a set of packages in the
Trilinos framework and are demonstrated on a simple problem from chemical
engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0792</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0792</id><created>2012-05-03</created><updated>2012-11-07</updated><authors><author><keyname>Leistedt</keyname><forenames>B.</forenames></author><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author></authors><title>Exact Wavelets on the Ball</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>13 pages, 10 figures, accepted for publication in IEEE Trans. Sig.
  Proc. The code is publicly available from http://www.flaglets.org</comments><journal-ref>IEEE Trans. Signal. Process., 60 (2012), 6257-6269</journal-ref><doi>10.1109/TSP.2012.2215030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an exact wavelet transform on the three-dimensional ball (i.e. on
the solid sphere), which we name the flaglet transform. For this purpose we
first construct an exact transform on the radial half-line using damped
Laguerre polynomials and develop a corresponding quadrature rule. Combined with
the spherical harmonic transform, this approach leads to a sampling theorem on
the ball and a novel three-dimensional decomposition which we call the
Fourier-Laguerre transform. We relate this new transform to the well-known
Fourier-Bessel decomposition and show that band-limitedness in the
Fourier-Laguerre basis is a sufficient condition to compute the Fourier-Bessel
decomposition exactly. We then construct the flaglet transform on the ball
through a harmonic tiling, which is exact thanks to the exactness of the
Fourier-Laguerre transform (from which the name flaglets is coined). The
corresponding wavelet kernels are well localised in real and Fourier-Laguerre
spaces and their angular aperture is invariant under radial translation. We
introduce a multiresolution algorithm to perform the flaglet transform rapidly,
while capturing all information at each wavelet scale in the minimal number of
samples on the ball. Our implementation of these new tools achieves
floating-point precision and is made publicly available. We perform numerical
experiments demonstrating the speed and accuracy of these libraries and
illustrate their capabilities on a simple denoising example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0820</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0820</id><created>2012-05-03</created><authors><author><keyname>Kanuparthy</keyname><forenames>Partha</forenames></author><author><keyname>Matthews</keyname><forenames>Warren</forenames></author><author><keyname>Dovrolis</keyname><forenames>Constantine</forenames></author></authors><title>DNS-based Ingress Load Balancing: An Experimental Evaluation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multihomed services can load-balance their incoming connection requests using
DNS, resolving the name of the server with different addresses depending on the
link load that corresponds to each address. Previous work has studied a number
of problems with this approach, e.g., due to Time-to-Live duration violations
and client proximity to local DNS servers. In this paper, we experimentally
evaluate a DNS-based ingress traffic engineering system that we deployed at
Georgia Tech. Our objective is to understand whether simple and robust load
balancing algorithms can be accurate in practice, despite aforementioned
problems with DNS-based load balancing methods. In particular, we examine the
impact of various system parameters and of the main workload characteristics.
We show that a window-based measurement scheme can be fairly accurate in
practice, as long as its window duration has been appropriately configured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0831</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0831</id><created>2012-05-03</created><authors><author><keyname>Maseleno</keyname><forenames>Andino</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mahmud</forenames></author></authors><title>African Trypanosomiasis Detection using Dempster-Shafer Theory</title><categories>cs.AI stat.CO</categories><journal-ref>International Journal of Emerging Trends in Computing and
  Information Sciences, Vol. 3, No. 4, 2012, pp. 480 - 487</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  World Health Organization reports that African Trypanosomiasis affects mostly
poor populations living in remote rural areas of Africa that can be fatal if
properly not treated. This paper presents Dempster-Shafer Theory for the
detection of African trypanosomiasis. Sustainable elimination of African
trypanosomiasis as a public-health problem is feasible and requires continuous
efforts and innovative approaches. In this research, we implement
Dempster-Shafer theory for detecting African trypanosomiasis and displaying the
result of detection process. We describe eleven symptoms as major symptoms
which include fever, red urine, skin rash, paralysis, headache, bleeding around
the bite, joint the paint, swollen lymph nodes, sleep disturbances, meningitis
and arthritis. Dempster-Shafer theory to quantify the degree of belief, our
approach uses Dempster-Shafer theory to combine beliefs under conditions of
uncertainty and ignorance, and allows quantitative measurement of the belief
and plausibility in our identification result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0835</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0835</id><created>2012-05-03</created><updated>2013-03-29</updated><authors><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Parameter Tracking via Optimal Distributed Beamforming in an Analog
  Sensor Network</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, accepted by IEEE Asilomar 2012, Jul. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimal distributed beamforming in a sensor
network where the sensors observe a dynamic parameter in noise and coherently
amplify and forward their observations to a fusion center (FC). The FC uses a
Kalman filter to track the parameter using the observations from the sensors,
and we show how to find the optimal gain and phase of the sensor transmissions
under both global and individual power constraints in order to minimize the
mean squared error (MSE) of the parameter estimate. For the case of a global
power constraint, a closed-form solution can be obtained. A numerical
optimization is required for individual power constraints, but the problem can
be relaxed to a semidefinite programming problem (SDP), and we show how the
optimal solution can be constructed from the solution to the SDP. Simulation
results show that compared with equal power transmission, the use of optimized
power control can significantly reduce the MSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0837</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0837</id><created>2012-05-03</created><authors><author><keyname>Chester</keyname><forenames>Sean</forenames></author><author><keyname>Thomo</keyname><forenames>Alex</forenames></author><author><keyname>Venkatesh</keyname><forenames>S.</forenames></author><author><keyname>Whitesides</keyname><forenames>Sue</forenames></author></authors><title>Indexing Reverse Top-k Queries</title><categories>cs.DB cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recently introduced monochromatic reverse top-k queries which
ask for, given a new tuple q and a dataset D, all possible top-k queries on D
union {q} for which q is in the result. Towards this problem, we focus on
designing indexes in two dimensions for repeated (or batch) querying, a novel
but practical consideration. We present the insight that by representing the
dataset as an arrangement of lines, a critical k-polygon can be identified and
used exclusively to respond to reverse top-k queries. We construct an index
based on this observation which has guaranteed worst-case query cost that is
logarithmic in the size of the k-polygon.
  We implement our work and compare it to related approaches, demonstrating
that our index is fast in practice. Furthermore, we demonstrate through our
experiments that a k-polygon is comprised of a small proportion of the original
data, so our index structure consumes little disk space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0839</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0839</id><created>2012-05-03</created><authors><author><keyname>Shulkin</keyname><forenames>Evgeny V.</forenames></author><author><keyname>Krasnopeyev</keyname><forenames>Sergey M.</forenames></author></authors><title>Development of application for discovering and binding to published
  geospatial processes in distributed environments</title><categories>cs.SE</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, society has recognized that the lack of access to spatial data and
tools for their analysis is the limiting factor of economic development. It
came to the realization that without the single information space, which is
implemented in the form of spatial data infrastructures, a progressive business
development is impossible. Spatial data infrastructures will support a variety
of tasks, which requires the binding of geospatial information from multiple
sources. In the last few years, the rate of progress in spatial data collection
was higher, than in management and analysis of data. Infrastructures allow the
accumulated data to be available to large groups of users, and infrastructure
of analysis allows the data to be effectively used for such tasks as municipal
planning, science research, etc. Moreover, free access to the information
resources and instruments of analysis will serve as an additional impulse to
development of application models in corresponding areas of expertise. The goal
of this paper is to indicate possible solutions to the client-side problems of
spatial data analysis in distributed environments, using the developing
application for data analysis as an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0845</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0845</id><created>2012-05-03</created><authors><author><keyname>Du</keyname><forenames>Lizhi</forenames></author></authors><title>Method Study on the 3x+1 Problem</title><categories>cs.DM</categories><comments>7 pages, I give a possible way for solving the 3x+1 problem utterly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3x+1 problem is one of the most classical problems in computer science,
related to many fields. As it is thought by scientists a highly hard problem,
resolving it successfully not only can improve the research in many relating
fields, but also be meaningful to the method study. By deep analyzing the 3x+1
calculation process with the input positive integer becoming greater, we find a
useful way for solving this problem with high probability. By making use of the
greater calculating ability of great computers and the internet, our way is a
valid and powerful way for utterly solving the 3x+1 problem. This way can be
expressed in three points: 1) If we can find a positive integer N, for any
positive integer less than 2N, the times of dividing 2 out of its stopping time
is less than or equal to N, then the 3x+1 conjecture is true; 2) This N may be
big, so the calculation may be too big. Our way for solving this is: to find a
positive integer K, for all positive integers less than 2K, not all the times
of dividing 2 out of their stopping time for these integers are less than or
equal to K, some part of these are greater than K, but the number of this part
becomes less and less with the K increasing; 3) This K and the calculation may
also be too big, our way for solving this is: to find a positive integer R, for
all positive integers less than 2R, as above, out of their stopping time, the
times of dividing 2 of some part of these integers are greater than R, also the
number of this part integers does not become less immediately with the R
increasing, but the increasing rate of this number is less and less until to
below zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0852</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0852</id><created>2012-05-03</created><updated>2013-01-09</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>On the Parameterized Complexity and Kernelization of the Workflow
  Satisfiability Problem</title><categories>cs.CR cs.CC</categories><acm-class>D.4.6; F.2.2; H.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A workflow specification defines a set of steps and the order in which those
steps must be executed. Security requirements may impose constraints on which
groups of users are permitted to perform subsets of those steps. A workflow
specification is said to be satisfiable if there exists an assignment of users
to workflow steps that satisfies all the constraints. An algorithm for
determining whether such an assignment exists is important, both as a static
analysis tool for workflow specifications, and for the construction of run-time
reference monitors for workflow management systems. Finding such an assignment
is a hard problem in general, but work by Wang and Li in 2010 using the theory
of parameterized complexity suggests that efficient algorithms exist under
reasonable assumptions about workflow specifications. In this paper, we improve
the complexity bounds for the workflow satisfiability problem. We also
generalize and extend the types of constraints that may be defined in a
workflow specification and prove that the satisfiability problem remains
fixed-parameter tractable for such constraints. Finally, we consider
preprocessing for the problem and prove that in an important special case, in
polynomial time, we can reduce the given input into an equivalent one, where
the number of users is at most the number of steps. We also show that no such
reduction exists for two natural extensions of this case, which bounds the
number of users by a polynomial in the number of steps, provided a
widely-accepted complexity-theoretical assumption holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0856</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0856</id><created>2012-05-03</created><authors><author><keyname>Ruan</keyname><forenames>Ning</forenames></author><author><keyname>Gao</keyname><forenames>David Yang</forenames></author></authors><title>Global Optimal Solution to Discrete Value Selection Problem with
  Inequality Constraints</title><categories>math.OC cs.DM</categories><comments>18 pages and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a canonical dual method for solving a quadratic discrete
value selection problem subjected to inequality constraints. The problem is
first transformed into a problem with quadratic objective and 0-1 integer
variables. The dual problem of the 0-1 programming problem is thus constructed
by using the canonical duality theory. Under appropriate conditions, this dual
problem is a maximization problem of a concave function over a convex
continuous space. Numerical simulation studies, including some large scale
problems, are carried out so as to demonstrate the effectiveness and efficiency
of the method proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0858</identifier>
 <datestamp>2013-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0858</id><created>2012-05-04</created><updated>2013-09-04</updated><authors><author><keyname>Nitinawarat</keyname><forenames>Sirin</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Controlled Sensing for Multihypothesis Testing</title><categories>cs.IT math.IT</categories><comments>To appear in the Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of multiple hypothesis testing with observation control is
considered in both fixed sample size and sequential settings. In the fixed
sample size setting, for binary hypothesis testing, the optimal exponent for
the maximal error probability corresponds to the maximum Chernoff information
over the choice of controls, and a pure stationary open-loop control policy is
asymptotically optimal within the larger class of all causal control policies.
For multihypothesis testing in the fixed sample size setting, lower and upper
bounds on the optimal error exponent are derived. It is also shown through an
example with three hypotheses that the optimal causal control policy can be
strictly better than the optimal open-loop control policy. In the sequential
setting, a test based on earlier work by Chernoff for binary hypothesis
testing, is shown to be first-order asymptotically optimal for multihypothesis
testing in a strong sense, using the notion of decision making risk in place of
the overall probability of error. Another test is also designed to meet hard
risk constrains while retaining asymptotic optimality. The role of past
information and randomization in designing optimal control policies is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0879</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0879</id><created>2012-05-04</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Chyzak</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author></authors><title>Fast Computation of Common Left Multiples of Linear Ordinary
  Differential Operators</title><categories>cs.SC</categories><comments>The final version will appear in Proceedings of ISSAC 2012</comments><msc-class>68Q25, 34A30, 68W30</msc-class><acm-class>I.1.2; F.2.1; G.1.7</acm-class><journal-ref>Proceedings ISSAC'12, pages 99-106, 2012</journal-ref><doi>10.1145/2442829.2442847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study tight bounds and fast algorithms for LCLMs of several linear
differential operators with polynomial coefficients. We analyze the arithmetic
complexity of existing algorithms for LCLMs, as well as the size of their
outputs. We propose a new algorithm that recasts the LCLM computation in a
linear algebra problem on a polynomial matrix. This algorithm yields sharp
bounds on the coefficient degrees of the LCLM, improving by one order of
magnitude the best bounds obtained using previous algorithms. The complexity of
the new algorithm is almost optimal, in the sense that it nearly matches the
arithmetic size of the output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0900</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0900</id><created>2012-05-04</created><authors><author><keyname>Talamo</keyname><forenames>Maurizio</forenames></author><author><keyname>Galinium</keyname><forenames>Maulahikmah</forenames></author><author><keyname>Schunck</keyname><forenames>Christian H.</forenames></author><author><keyname>Arcieri</keyname><forenames>Franco</forenames></author></authors><title>Interleaving Commands: a Threat to the Interoperability of Smartcard
  Based Security Applications</title><categories>cs.CR</categories><comments>8 pages</comments><journal-ref>International Journal of Computers and Communications, Issue 1,
  Volume 6, 2012, pp.76-83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although smartcards are widely used, secure smartcard interoperability has
remained a significant challenge. Usually each manufacturer provides a closed
environment for their smartcard based applications including the microchip,
associated firmware and application software. While the security of this
&quot;package&quot; can be tested and certified for example based on the Common Criteria,
the secure and convenient interoperability with other smartcards and smartcard
applications is not guaranteed. Ideally one would have a middleware that can
support various smartcards and smartcard applications. In our ongoing research
we study this scenario with the goal to develop a way to certify secure
smartcard interoperability in such an environment. Here we discuss and
experimentally demonstrate one critical security problem: if several smartcards
are connected via a middleware it is possible that a smartcard of type S
receives commands that were supposed to be executed on a different smartcard of
type S'. Such &quot;external commands&quot; can interleave with the commands that were
supposed to be executed on S. Here we demonstrate this problem experimentally
with a Common Criteria certified digital signature process on two commercially
available smartcards. Importantly, in some of these cases the digital signature
processes terminate without generating an error message or warning to the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0903</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0903</id><created>2012-05-04</created><authors><author><keyname>Wunderlich</keyname><forenames>Henning</forenames></author></authors><title>A note on a problem in communication complexity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we prove a version of Tarui's Theorem in communication
complexity, namely $PH^{cc} \subseteq BP\cdot PP^{cc}$. Consequently, every
measure for $PP^{cc}$ leads to a measure for $PH^{cc}$, subsuming a result of
Linial and Shraibman that problems with high mc-rigidity lie outside the
polynomial hierarchy. By slightly changing the definition of mc-rigidity
(arbitrary instead of uniform distribution), it is then evident that the class
$M^{cc}$ of problems with low mc-rigidity equals $BP\cdot PP^{cc}$. As $BP\cdot
PP^{cc} \subseteq PSPACE^{cc}$, this rules out the possibility, that had been
left open, that even polynomial space is contained in $M^{cc}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0908</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0908</id><created>2012-05-04</created><authors><author><keyname>Karandashev</keyname><forenames>Iakov</forenames></author><author><keyname>Kryzhanovsky</keyname><forenames>Boris</forenames></author><author><keyname>Litinskii</keyname><forenames>Leonid</forenames></author></authors><title>Weighted Patterns as a Tool for Improving the Hopfield Model</title><categories>cond-mat.dis-nn cs.LG cs.NE</categories><comments>19 pages, 17 figures</comments><journal-ref>Physical Review E, 2012, Vol.85, No.4</journal-ref><doi>10.1103/PhysRevE.85.041925</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the standard Hopfield model to the case when a weight is
assigned to each input pattern. The weight can be interpreted as the frequency
of the pattern occurrence at the input of the network. In the framework of the
statistical physics approach we obtain the saddle-point equation allowing us to
examine the memory of the network. In the case of unequal weights our model
does not lead to the catastrophic destruction of the memory due to its
overfilling (that is typical for the standard Hopfield model). The real memory
consists only of the patterns with weights exceeding a critical value that is
determined by the weights distribution. We obtain the algorithm allowing us to
find this critical value for an arbitrary distribution of the weights, and
analyze in detail some particular weights distributions. It is shown that the
memory decreases as compared to the case of the standard Hopfield model.
However, in our model the network can learn online without the catastrophic
destruction of the memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0910</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0910</id><created>2012-05-04</created><updated>2013-11-12</updated><authors><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Strapasson</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Costa</keyname><forenames>Sueli</forenames></author></authors><title>On projections of arbitrary lattices</title><categories>cs.CG cs.IT math.IT</categories><comments>11 pages</comments><doi>10.1016/j.laa.2013.07.022 10.1016/j.laa.2013.07.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that given any two point lattices $\Lambda_1 \subset
\mathbb{R}^n$ and $ \Lambda_2 \subset \nobreak \mathbb{R}^{n-k}$, there is a
set of $k$ vectors $\bm{v}_i \in \Lambda_1$ such that $\Lambda_2$ is, up to
similarity, arbitrarily close to the projection of $\Lambda_1$ onto the
orthogonal complement of the subspace spanned by $\bm{v}_1, \ldots, \bm{v}_k$.
This result extends the main theorem of \cite{Sloane2} and has applications in
communication theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0913</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0913</id><created>2012-05-04</created><updated>2012-05-07</updated><authors><author><keyname>Dawar</keyname><forenames>Anuj</forenames></author><author><keyname>Holm</keyname><forenames>Bjarki</forenames></author></authors><title>Pebble games with algebraic rules</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a general framework of partition games for formulating two-player
pebble games over finite structures. We show that one particular such game,
which we call the invertible-map game, yields a family of polynomial-time
approximations of graph isomorphism that is strictly stronger than the
well-known Weisfeiler-Lehman method. The general framework we introduce
includes as special cases the pebble games for finite-variable logics with and
without counting. It also includes a matrix-equivalence game, introduced here,
which characterises equivalence in the finite-variable fragments of matrix-rank
logic. We show that the equivalence defined by the invertible-map game is a
refinement of the equivalence defined by each of these games for
finite-variable logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0917</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0917</id><created>2012-05-04</created><authors><author><keyname>Boughamoura</keyname><forenames>Radhouane</forenames></author><author><keyname>Hlaoua</keyname><forenames>Lobna</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>VIQI: A New Approach for Visual Interpretation of Deep Web Query
  Interfaces</title><categories>cs.IR cs.AI</categories><comments>8th NCM: 2012 International Conference on Networked Computing and
  Advanced Information Management</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Web databases contain more than 90% of pertinent information of the Web.
Despite their importance, users don't profit of this treasury. Many deep web
services are offering competitive services in term of prices, quality of
service, and facilities. As the number of services is growing rapidly, users
have difficulty to ask many web services in the same time. In this paper, we
imagine a system where users have the possibility to formulate one query using
one query interface and then the system translates query to the rest of query
interfaces. However, interfaces are created by designers in order to be
interpreted visually by users, machines can not interpret query from a given
interface. We propose a new approach which emulates capacity of interpretation
of users and extracts query from deep web query interfaces. Our approach has
proved good performances on two standard datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0919</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0919</id><created>2012-05-04</created><authors><author><keyname>Hlaoua</keyname><forenames>Radhouane Boughammoura Lobna</forenames></author><author><keyname>Omri</keyname><forenames>Mohamed Nazih</forenames></author></authors><title>ViQIE: A New Approach for Visual Query Interpretation and Extraction</title><categories>cs.IR</categories><comments>ICITES 2012 - 2nd International Conference on Information Technology
  and e-Services</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web services are accessed via query interfaces which hide databases
containing thousands of relevant information. User's side, distant database is
a black box which accepts query and returns results, there is no way to access
database schema which reflect data and query meanings. Hence, web services are
very autonomous. Users view this autonomy as a major drawback because they need
often to combine query capabilities of many web services at the same time. In
this work, we will present a new approach which allows users to benefit of
query capabilities of many web services while respecting autonomy of each
service. This solution is a new contribution in Information Retrieval research
axe and has proven good performances on two standard datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0927</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0927</id><created>2012-05-04</created><authors><author><keyname>Glazunov</keyname><forenames>Andr&#xe9;s Alay&#xf3;n</forenames></author></authors><title>An Energy-Efficient MIMO Algorithm with Receive Power Constraint</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to Transactions on Emerging Telecommunications Technologies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the energy-efficiency of Multiple-Input Multiple-Output (MIMO)
systems with constrained received power rather than constrained transmit power.
A Energy-Efficient Water-Filling (EEWF) algorithm that maximizes the ratio of
the transmission rate to the total transmit power has been derived. The EEWF
power allocation policy establishes a trade-off between the transmission rate
and the total transmit power under the total receive power constraint. The
static and the uncorrelated fast fading Rayleigh channels have been considered,
where the maximization is performed on the instantaneous realization of the
channel assuming perfect information at both the transmitter and the receiver
with equal number of antennas. We show, based on Monte Carlo simulations that
the energy-efficiency provided by the EEWF algorithm can be more than an order
of magnitude greater than the energy-efficiency corresponding to capacity
achieving Water-Filling (WF) algorithm. We also show that the energy-efficiency
increases with both the number of antennas and the signal-to-noise ratio. The
corresponding transmission rate also increases but at a slower rate than the
Shannon capacity, while the corresponding total transmit power decreases with
the number of antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0933</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0933</id><created>2012-05-04</created><authors><author><keyname>Glazunov</keyname><forenames>Andr&#xe9;s Alay&#xf3;n</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>A note on the bivariate distribution representation of two perfectly
  correlated random variables by Dirac's $\delta$-function</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss the representation of the joint probability density
function of perfectly correlated continuous random variables, i.e., with
correlation coefficients $\rho=pm1$, by Dirac's $\delta$-function. We also show
how this representation allows to define Dirac's $\delta$-function as the ratio
between bivariate distributions and the marginal distribution in the limit
$\rho\rightarrow \pm1$, whenever this limit exists. We illustrate this with the
example of the bivariate Rice distribution
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0940</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0940</id><created>2012-05-04</created><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Herinckx</keyname><forenames>Audrey</forenames></author></authors><title>A tighter Erd\&quot;os-P\'osa function for long cycles</title><categories>math.CO cs.DM</categories><comments>4 pages</comments><msc-class>05C35, 05C38, 05C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there exists a bivariate function f with f(k,l) = O(l k log k)
such that for every naturals k and l, every graph G has at least k
vertex-disjoint cycles of length at least l or a set of at most f(k,l) vertices
that meets all cycles of length at least l. This improves a result by
Birmel\'e, Bondy and Reed (Combinatorica, 2007), who proved the same result
with f(k,l) = \Theta(l k^2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0946</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0946</id><created>2012-05-04</created><updated>2014-02-11</updated><authors><author><keyname>Bersani</keyname><forenames>Marcello M.</forenames></author><author><keyname>Frigeri</keyname><forenames>Achille</forenames></author><author><keyname>Morzenti</keyname><forenames>Angelo</forenames></author><author><keyname>Pradella</keyname><forenames>Matteo</forenames></author><author><keyname>Rossi</keyname><forenames>Matteo</forenames></author><author><keyname>Pietro</keyname><forenames>Pierluigi San</forenames></author></authors><title>Constraint LTL Satisfiability Checking without Automata</title><categories>cs.LO</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel technique to decide the satisfiability of
formulae written in the language of Linear Temporal Logic with Both future and
past operators and atomic formulae belonging to constraint system D (CLTLB(D)
for short). The technique is based on the concept of bounded satisfiability,
and hinges on an encoding of CLTLB(D) formulae into QF-EUD, the theory of
quantifier-free equality and uninterpreted functions combined with D. Similarly
to standard LTL, where bounded model-checking and SAT-solvers can be used as an
alternative to automata-theoretic approaches to model-checking, our approach
allows users to solve the satisfiability problem for CLTLB(D) formulae through
SMT-solving techniques, rather than by checking the emptiness of the language
of a suitable automaton A_{\phi}. The technique is effective, and it has been
implemented in our Zot formal verification tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0958</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0958</id><created>2012-05-04</created><updated>2012-09-24</updated><authors><author><keyname>B&#xf3;</keyname><forenames>Pedro Dal</forenames></author><author><keyname>Pujals</keyname><forenames>Enrique R.</forenames></author></authors><title>The Evolutionary Robustness of Forgiveness and Cooperation</title><categories>math.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolutionary robustness of strategies in infinitely repeated
prisoners' dilemma games in which players make mistakes with a small
probability and are patient. The evolutionary process we consider is given by
the replicator dynamics. We show that there are strategies with a uniformly
large basin of attraction independently of the size of the population.
Moreover, we show that those strategies forgive defections and, assuming that
they are symmetric, they cooperate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0960</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0960</id><created>2012-05-04</created><authors><author><keyname>Pollner</keyname><forenames>Peter</forenames></author><author><keyname>Palla</keyname><forenames>Gergely</forenames></author><author><keyname>Vicsek</keyname><forenames>Tamas</forenames></author></authors><title>Parallel clustering with CFinder</title><categories>physics.soc-ph cs.DC cs.DS cs.SI physics.data-an</categories><comments>Electronic version of an article published as
  http://www.worldscinet.com/ppl/22/2201/S0129626412400014.html copyright World
  Scientific Publishing Company</comments><journal-ref>Parallel Processing Letters 22:(1) p. 1240001. (2012)</journal-ref><doi>10.1142/S0129626412400014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of available data about complex systems is increasing every year,
measurements of larger and larger systems are collected and recorded. A natural
representation of such data is given by networks, whose size is following the
size of the original system. The current trend of multiple cores in computing
infrastructures call for a parallel reimplementation of earlier methods. Here
we present the grid version of CFinder, which can locate overlapping
communities in directed, weighted or undirected networks based on the clique
percolation method (CPM). We show that the computation of the communities can
be distributed among several CPU-s or computers. Although switching to the
parallel version not necessarily leads to gain in computing time, it definitely
makes the community structure of extremely large networks accessible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0963</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0963</id><created>2012-05-04</created><authors><author><keyname>Itoh</keyname><forenames>Jin-ichi</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author><author><keyname>Vilcu</keyname><forenames>Costin</forenames></author></authors><title>Source Unfoldings of Convex Polyhedra via Certain Closed Curves</title><categories>cs.CG</categories><comments>A preliminary abstract appeared EuroCG: Proc. 25th European Workshop
  Comput. Geom., pages 61--64, March 2009. This full version is 19 pages long,
  with 9 figures</comments><msc-class>52B10</msc-class><acm-class>F.2.2; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the notion of a source unfolding of a convex polyhedron P to be
based on a closed polygonal curve Q in a particular class rather than based on
a point. The class requires that Q &quot;lives on a cone&quot; to both sides; it includes
simple, closed quasigeodesics. Cutting a particular subset of the cut locus of
Q (in P) leads to a non-overlapping unfolding of the polyhedron. This gives a
new general method to unfold the surface of any convex polyhedron to a simple,
planar polygon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0968</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0968</id><created>2012-05-04</created><authors><author><keyname>Chakrabarti</keyname><forenames>Amit</forenames></author><author><keyname>Kondapally</keyname><forenames>Ranganath</forenames></author><author><keyname>Wang</keyname><forenames>Zhenghui</forenames></author></authors><title>Information Complexity versus Corruption and Applications to
  Orthogonality and Gap-Hamming</title><categories>cs.CC cs.IT math.IT math.PR</categories><acm-class>F.2.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three decades of research in communication complexity have led to the
invention of a number of techniques to lower bound randomized communication
complexity. The majority of these techniques involve properties of large
submatrices (rectangles) of the truth-table matrix defining a communication
problem. The only technique that does not quite fit is information complexity,
which has been investigated over the last decade. Here, we connect information
complexity to one of the most powerful &quot;rectangular&quot; techniques: the
recently-introduced smooth corruption (or &quot;smooth rectangle&quot;) bound. We show
that the former subsumes the latter under rectangular input distributions. We
conjecture that this subsumption holds more generally, under arbitrary
distributions, which would resolve the long-standing direct sum question for
randomized communication. As an application, we obtain an optimal $\Omega(n)$
lower bound on the information complexity---under the {\em uniform
distribution}---of the so-called orthogonality problem (ORT), which is in turn
closely related to the much-studied Gap-Hamming-Distance (GHD). The proof of
this bound is along the lines of recent communication lower bounds for GHD, but
we encounter a surprising amount of additional technical detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0974</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0974</id><created>2012-05-04</created><authors><author><keyname>Bonifaci</keyname><forenames>Vincenzo</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>Scheduling Unrelated Machines of Few Different Types</title><categories>cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A very well-known machine model in scheduling allows the machines to be
unrelated, modelling jobs that might have different characteristics on each
machine. Due to its generality, many optimization problems of this form are
very difficult to tackle and typically APX-hard. However, in many applications
the number of different types of machines, such as processor cores, GPUs, etc.
is very limited. In this paper, we address this point and study the assignment
of jobs to unrelated machines in the case that each machine belongs to one of a
fixed number of types and the machines of each type are identical. We present
polynomial time approximation schemes (PTASs) for minimizing the makespan for
multidimensional jobs with a fixed number of dimensions and for minimizing the
L_p-norm. In particular, our results subsume and generalize the existing PTASs
for a constant number of unrelated machines and for an arbitrary number of
identical machines for these problems. We employ a number of techniques which
go beyond the previously known results, including a new counting argument and a
method for making the concept of sparse extreme point solutions usable for a
convex program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0986</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0986</id><created>2012-05-04</created><authors><author><keyname>B&#xf6;hmer</keyname><forenames>Wendelin</forenames></author></authors><title>Robot Navigation using Reinforcement Learning and Slow Feature Analysis</title><categories>cs.AI cs.NE</categories><comments>Diploma Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of reinforcement learning algorithms onto real life problems
always bears the challenge of filtering the environmental state out of raw
sensor readings. While most approaches use heuristics, biology suggests that
there must exist an unsupervised method to construct such filters
automatically. Besides the extraction of environmental states, the filters have
to represent them in a fashion that support modern reinforcement algorithms.
Many popular algorithms use a linear architecture, so one should aim at filters
that have good approximation properties in combination with linear functions.
This thesis wants to propose the unsupervised method slow feature analysis
(SFA) for this task. Presented with a random sequence of sensor readings, SFA
learns a set of filters. With growing model complexity and training examples,
the filters converge against trigonometric polynomial functions. These are
known to possess excellent approximation capabilities and should therfore
support the reinforcement algorithms well. We evaluate this claim on a robot.
The task is to learn a navigational control in a simple environment using the
least square policy iteration (LSPI) algorithm. The only accessible sensor is a
head mounted video camera, but without meaningful filtering, video images are
not suited as LSPI input. We will show that filters learned by SFA, based on a
random walk video of the robot, allow the learned control to navigate
successfully in ca. 80% of the test trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0987</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0987</id><created>2012-05-04</created><authors><author><keyname>Espa&#xf1;a</keyname><forenames>Sergio</forenames></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>Arturo</forenames></author><author><keyname>Pastor</keyname><forenames>&#xd3;scar</forenames></author><author><keyname>Ruiz</keyname><forenames>Marcela</forenames></author></authors><title>Communication Analysis modelling techniques</title><categories>cs.SE</categories><comments>35 pages, 14 figures, 9 tables</comments><report-no>ProS-TR-2012-02</report-no><msc-class>68N01</msc-class><acm-class>D.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes and illustrates several modelling techniques proposed
by Communication Analysis; namely Communicative Event Diagram, Message
Structures and Event Specification Templates. The Communicative Event Diagram
is a business process modelling technique that adopts a communicational
perspective by focusing on communicative interactions when describing the
organizational work practice, instead of focusing on physical activities1; at
this abstraction level, we refer to business activities as communicative
events. Message Structures is a technique based on structured text that allows
specifying the messages associated to communicative events. Event Specification
Templates are a means to organise the requirements concerning a communicative
event. This report can be useful to analysts and business process modellers in
general, since, according to our industrial experience, it is possible to apply
many Communication Analysis concepts, guidelines and criteria to other business
process modelling notations such as BPMN. Also, Message Structures can
complement business process models created with other notations different than
Communicative Event Diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0991</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0991</id><created>2012-05-04</created><updated>2013-01-16</updated><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author></authors><title>On the Complexity of Recognizing S-composite and S-prime Graphs</title><categories>cs.CC cs.DM</categories><comments>This paper has been withdrawn. A full and correct version of this
  paper can be found here:
  http://www.sciencedirect.com/science/article/pii/S0166218X12004271</comments><doi>10.1016/j.dam.2012.11.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  S-prime graphs are graphs that cannot be represented as nontrivial subgraphs
of nontrivial Cartesian products of graphs, i.e., whenever it is a subgraph of
a nontrivial Cartesian product graph it is a subgraph of one the factors. A
graph is S-composite if it is not S-prime. Although linear time recognition
algorithms for determining whether a graph is prime or not with respect to the
Cartesian product are known, it remained unknown if a similar result holds also
for the recognition of S-prime and S-composite graphs.
  In this contribution the computational complexity of recognizing S-composite
and S-prime graphs is considered. Klav\v{z}ar et al. [Discr. Math. 244: 223-230
(2002)] proved that a graph is S-composite if and only if it admits a
nontrivial path-k-coloring. The problem of determining whether there exists a
path-2-coloring for a given graph is shown to be NP-complete, which in turn is
utilized to show that determining whether a graph is S-composite is NP-complete
and thus, determining whether a graph is S-prime is CoNP-complete. A plenty of
other problems are shown to be NP-hard, using the latter results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.0997</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.0997</id><created>2012-05-04</created><updated>2014-09-11</updated><authors><author><keyname>Blaum</keyname><forenames>Mario</forenames></author><author><keyname>Hafner</keyname><forenames>James Lee</forenames></author><author><keyname>Hetzler</keyname><forenames>Steven</forenames></author></authors><title>Partial-MDS Codes and their Application to RAID Type of Architectures</title><categories>cs.IT math.IT</categories><comments>This new version corrects a typo in Theorem 5.5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of codes with a natural two-dimensional structure is presented,
inspired by an application of RAID type of architectures whose units are solid
state drives (SSDs). Arrays of SSDs behave differently to arrays of hard disk
drives (HDDs), since hard errors in sectors are common and traditional RAID
approaches (like RAID 5 or RAID 6) may be either insufficient or excessive. An
efficient solution to this problem is given by the new codes presented, called
partial-MDS (PMDS) codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1010</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1010</id><created>2012-05-04</created><updated>2012-06-19</updated><authors><author><keyname>Conover</keyname><forenames>Michael D.</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Partisan Asymmetries in Online Political Activity</title><categories>cs.SI cs.HC physics.soc-ph</categories><comments>17 pages, 10 figures, 6 tables</comments><journal-ref>EPJ Data Science 1, 6 (2012)</journal-ref><doi>10.1140/epjds6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine partisan differences in the behavior, communication patterns and
social interactions of more than 18,000 politically-active Twitter users to
produce evidence that points to changing levels of partisan engagement with the
American online political landscape. Analysis of a network defined by the
communication activity of these users in proximity to the 2010 midterm
congressional elections reveals a highly segregated, well clustered partisan
community structure. Using cluster membership as a high-fidelity (87% accuracy)
proxy for political affiliation, we characterize a wide range of differences in
the behavior, communication and social connectivity of left- and right-leaning
Twitter users. We find that in contrast to the online political dynamics of the
2008 campaign, right-leaning Twitter users exhibit greater levels of political
activity, a more tightly interconnected social structure, and a communication
network topology that facilitates the rapid and broad dissemination of
political information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1013</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1013</id><created>2012-05-04</created><updated>2013-04-16</updated><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Puy</keyname><forenames>G.</forenames></author><author><keyname>Thiran</keyname><forenames>J. -Ph.</forenames></author><author><keyname>Vandergheynst</keyname><forenames>P.</forenames></author><author><keyname>Van De Ville</keyname><forenames>D.</forenames></author><author><keyname>Wiaux</keyname><forenames>Y.</forenames></author></authors><title>Sparse image reconstruction on the sphere: implications of a new
  sampling theorem</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>11 pages, 5 figures</comments><journal-ref>IEEE Trans. Image Process. 22 (2013) 2275-2285</journal-ref><doi>10.1109/TIP.2013.2249079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the impact of sampling theorems on the fidelity of sparse image
reconstruction on the sphere. We discuss how a reduction in the number of
samples required to represent all information content of a band-limited signal
acts to improve the fidelity of sparse image reconstruction, through both the
dimensionality and sparsity of signals. To demonstrate this result we consider
a simple inpainting problem on the sphere and consider images sparse in the
magnitude of their gradient. We develop a framework for total variation (TV)
inpainting on the sphere, including fast methods to render the inpainting
problem computationally feasible at high-resolution. Recently a new sampling
theorem on the sphere was developed, reducing the required number of samples by
a factor of two for equiangular sampling schemes. Through numerical simulations
we verify the enhanced fidelity of sparse image reconstruction due to the more
efficient sampling of the sphere provided by the new sampling theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1015</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1015</id><created>2012-05-04</created><updated>2014-05-16</updated><authors><author><keyname>Koiran</keyname><forenames>Pascal</forenames></author><author><keyname>Portier</keyname><forenames>Natacha</forenames></author><author><keyname>Tavenas</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>A Wronskian Approach to the real \tau-conjecture</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the real \tau-conjecture, the number of real roots of a sum of
products of sparse polynomials should be polynomially bounded in the size of
such an expression. It is known that this conjecture implies a superpolynomial
lower bound on the arithmetic circuit complexity of the permanent.
  In this paper, we use the Wronksian determinant to give an upper bound on the
number of real roots of sums of products of sparse polynomials. The proof
technique is quite versatile; it can in particular be applied to some sparse
geometric problems that do not originate from arithmetic circuit complexity.
The paper should therefore be of interest to researchers from these two
communities (complexity theory and sparse polynomial systems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1050</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1050</id><created>2012-05-04</created><updated>2012-05-31</updated><authors><author><keyname>Urquhart</keyname><forenames>Alasdair</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Width and size of regular resolution proofs</title><categories>cs.CC math.LO</categories><comments>The article was reformatted using the style file for Logical Methods
  in Computer Science</comments><proxy>LMCS</proxy><acm-class>F2.2,F4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 1,
  2012) lmcs:862</journal-ref><doi>10.2168/LMCS-8(2:8)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the topic of the minimum width of a regular resolution
refutation of a set of clauses. The main result shows that there are examples
having small regular resolution refutations, for which any regular refutation
must contain a large clause. This forms a contrast with corresponding results
for general resolution refutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1053</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1053</id><created>2012-05-03</created><authors><author><keyname>Kim</keyname><forenames>Dongwoo</forenames></author><author><keyname>Chung</keyname><forenames>Yeonseung</forenames></author><author><keyname>Oh</keyname><forenames>Alice</forenames></author></authors><title>Variable Selection for Latent Dirichlet Allocation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In latent Dirichlet allocation (LDA), topics are multinomial distributions
over the entire vocabulary. However, the vocabulary usually contains many words
that are not relevant in forming the topics. We adopt a variable selection
method widely used in statistical modeling as a dimension reduction tool and
combine it with LDA. In this variable selection model for LDA (vsLDA), topics
are multinomial distributions over a subset of the vocabulary, and by excluding
words that are not informative for finding the latent topic structure of the
corpus, vsLDA finds topics that are more robust and discriminative. We compare
three models, vsLDA, LDA with symmetric priors, and LDA with asymmetric priors,
on heldout likelihood, MCMC chain consistency, and document classification. The
performance of vsLDA is better than symmetric LDA for likelihood and
classification, better than asymmetric LDA for consistency and classification,
and about the same in the other comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1055</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1055</id><created>2012-05-04</created><authors><author><keyname>Nosek</keyname><forenames>Brian A.</forenames></author><author><keyname>Bar-Anan</keyname><forenames>Yoav</forenames></author></authors><title>Scientific Utopia: I. Opening scientific communication</title><categories>physics.soc-ph cs.DL</categories><comments>Psychological Inquiry, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing norms for scientific communication are rooted in anachronistic
practices of bygone eras, making them needlessly inefficient. We outline a path
that moves away from the existing model of scientific communication to improve
the efficiency in meeting the purpose of public science - knowledge
accumulation. We call for six changes: (1) full embrace of digital
communication, (2) open access to all published research, (3) disentangling
publication from evaluation, (4) breaking the &quot;one article, one journal&quot; model
with a grading system for evaluation and diversified dissemination outlets, (5)
publishing peer review, and, (6) allowing open, continuous peer review. We
address conceptual and practical barriers to change, and provide examples
showing how the suggested practices are being used already. The critical
barriers to change are not technical or financial; they are social. While
scientists guard the status quo, they also have the power to change it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1069</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1069</id><created>2012-05-04</created><updated>2012-10-19</updated><authors><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author></authors><title>Asymptotic $L^4$ norm of polynomials derived from characters</title><categories>math.NT cs.IT math.CO math.IT</categories><comments>23 pages, corrects errata in and makes small adjustments to previous
  version</comments><msc-class>Primary: 11C08, Secondary: 42A05, 11T24 (2010)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Littlewood investigated polynomials with coefficients in $\{-1,1\}$
(Littlewood polynomials), to see how small their ratio of norms
$||f||_4/||f||_2$ on the unit circle can become as $deg(f)\to\infty$. A small
limit is equivalent to slow growth in the mean square autocorrelation of the
associated binary sequences of coefficients of the polynomials. The
autocorrelation problem for arrays and higher dimensional objects has also been
studied; it is the natural generalization to multivariable polynomials. Here we
find, for each $n &gt; 1$, a family of $n$-variable Littlewood polynomials with
lower asymptotic $||f||_4/||f||_2$ than any known hitherto. We discover these
through a wide survey, infeasible with previous methods, of polynomials whose
coefficients come from finite field characters. This is the first time that the
lowest known asymptotic ratio of norms $||f||_4/||f||_2$ for multivariable
polynomials $f(z_1,...,z_n)$ is strictly less than what could be obtained by
using products $f_1(z_1)... f_n(z_n)$ of the best known univariate polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1098</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1098</id><created>2012-05-05</created><authors><author><keyname>Belter</keyname><forenames>Geoffrey</forenames></author><author><keyname>Jessup</keyname><forenames>Elizabeth</forenames></author><author><keyname>Nelson</keyname><forenames>Thomas</forenames></author><author><keyname>Norris</keyname><forenames>Boyana</forenames></author><author><keyname>Siek</keyname><forenames>Jeremy G.</forenames></author></authors><title>Reliable Generation of High-Performance Matrix Algebra</title><categories>cs.MS cs.PF cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific programmers often turn to vendor-tuned Basic Linear Algebra
Subprograms (BLAS) to obtain portable high performance. However, many numerical
algorithms require several BLAS calls in sequence, and those successive calls
result in suboptimal performance. The entire sequence needs to be optimized in
concert. Instead of vendor-tuned BLAS, a programmer could start with source
code in Fortran or C (e.g., based on the Netlib BLAS) and use a
state-of-the-art optimizing compiler. However, our experiments show that
optimizing compilers often attain only one-quarter the performance of
hand-optimized code. In this paper we present a domain-specific compiler for
matrix algebra, the Build to Order BLAS (BTO), that reliably achieves high
performance using a scalable search algorithm for choosing the best combination
of loop fusion, array contraction, and multithreading for data parallelism. The
BTO compiler generates code that is between 16% slower and 39% faster than
hand-optimized code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1102</identifier>
 <datestamp>2012-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1102</id><created>2012-05-05</created><updated>2012-07-10</updated><authors><author><keyname>Baudry</keyname><forenames>Benoit</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Towards Ecology Inspired Software Engineering</title><categories>cs.SE</categories><comments>No. RR-7952 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ecosystems are complex and dynamic systems. Over billions of years, they have
developed advanced capabilities to provide stable functions, despite changes in
their environment. In this paper, we argue that the laws of organization and
development of ecosystems provide a solid and rich source of inspiration to lay
the foundations for novel software construction paradigms that provide
stability as much as openness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1114</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1114</id><created>2012-05-05</created><authors><author><keyname>Clay</keyname><forenames>James</forenames><suffix>III</suffix></author><author><keyname>Wortman</keyname><forenames>Kevin</forenames></author></authors><title>A Durable Flash Memory Search Tree</title><categories>cs.DS</categories><comments>This is a 2 Page abstract submitted to CompSust'12</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider the task of optimizing the B-tree data structure, used
extensively in operating systems and databases, for sustainable usage on
multi-level flash memory. Empirical evidence shows that this new flash memory
tree, or FM Tree, extends the operational lifespan of each block of flash
memory by a factor of roughly 27 to 70 times, while still supporting
logarithmic-time search tree operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1117</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1117</id><created>2012-05-05</created><authors><author><keyname>Madhulatha</keyname><forenames>T. Soni</forenames></author></authors><title>An Overview on Clustering Methods</title><categories>cs.DS cs.DB</categories><comments>7 pages</comments><journal-ref>IOSR Journal of Engineering, Apr. 2012, Vol. 2(4) pp: 719-725,
  ISSN 2250-3021</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Clustering is a common technique for statistical data analysis, which is used
in many fields, including machine learning, data mining, pattern recognition,
image analysis and bioinformatics. Clustering is the process of grouping
similar objects into different groups, or more precisely, the partitioning of a
data set into subsets, so that the data in each subset according to some
defined distance measure. This paper covers about clustering algorithms,
benefits and its applications. Paper concludes by discussing some limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1125</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1125</id><created>2012-05-05</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author></authors><title>Application Of Data Mining In Bioinformatics</title><categories>cs.CE cs.DB</categories><journal-ref>Indian Journal of Computer Science and Engineering 1(2):114-118
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article highlights some of the basic concepts of bioinformatics and data
mining. The major research areas of bioinformatics are highlighted. The
application of data mining in the domain of bioinformatics is explained. It
also highlights some of the current challenges and opportunities of data mining
in bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1126</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1126</id><created>2012-05-05</created><authors><author><keyname>Farooqi</keyname><forenames>Md. Rashid</forenames></author><author><keyname>Raza</keyname><forenames>Khalid</forenames></author></authors><title>A Comprehensive Study of CRM through Data Mining Techniques</title><categories>cs.DB</categories><comments>Proceedings of the National Conference; NCCIST-2011, September 09,
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's competitive scenario in corporate world, &quot;Customer Retention&quot;
strategy in Customer Relationship Management (CRM) is an increasingly pressed
issue. Data mining techniques play a vital role in better CRM. This paper
attempts to bring a new perspective by focusing the issue of data mining
applications, opportunities and challenges in CRM. It covers the topic such as
customer retention, customer services, risk assessment, fraud detection and
some of the data mining tools which are widely used in CRM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1141</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1141</id><created>2012-05-05</created><authors><author><keyname>Dongre</keyname><forenames>Vikas J</forenames></author><author><keyname>Yenkar</keyname><forenames>Ramkrishna V</forenames></author><author><keyname>Mankar</keyname><forenames>Vijay H</forenames></author></authors><title>Interactive Learning through Hands-on Practice using Electronic Mini -
  Lab (EML): a Case Study</title><categories>cs.CY</categories><comments>6 pages,1 Table, 7 Figures Journal paper</comments><journal-ref>Journal of Engineering, Science &amp; Management Education (JESME)
  Bhopal, India, Vol-5 Issue-I (Jan-Mar 2012) (357-362) www.nitttrbhopal.ac.in</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -- In this paper, a new approach to impart practical skill based technical
education is presented in comprehensive manner. An Electronic Mini-Lab (EML) is
devised containing basic design and test instruments with electronic
components, ICs, connecting wires and battery. Using the EML, students perform
various formal and informal digital and analog circuit practicals as well as
design prototype of projects. This gives them a hands-on experience, sense of
belonging and sense of cooperation. The EML is useful for performing many
practicals of various subjects. The EML also reduces the workload of college
laboratories. Students have their own individual EML at their disposal anytime,
which can be used to design hobby projects as a fun too. This will make them
skilled engineers. This provides tremendous benefits in teaching learning
process. It also boosted the interest, confidence of students and teachers.
Incorporating active/ cooperative learning into traditional instruction can be
a useful pedagogical tool to help students to perform practicals and project
work any time anywhere. This concept is remarkably simple and cost effective
but the dividends can be profound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1143</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1143</id><created>2012-05-05</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;ktun&#xe7;</keyname><forenames>Onur</forenames></author><author><keyname>Saule</keyname><forenames>Erik</forenames></author><author><keyname>Kaya</keyname><forenames>Kamer</forenames></author><author><keyname>&#xc7;ataly&#xfc;rek</keyname><forenames>&#xdc;mit V.</forenames></author></authors><title>Recommendation on Academic Networks using Direction Aware Citation
  Analysis</title><categories>cs.IR cs.DL</categories><comments>10 pages, 8 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The literature search has always been an important part of an academic
research. It greatly helps to improve the quality of the research process and
output, and increase the efficiency of the researchers in terms of their novel
contribution to science. As the number of published papers increases every
year, a manual search becomes more exhaustive even with the help of today's
search engines since they are not specialized for this task. In academics, two
relevant papers do not always have to share keywords, cite one another, or even
be in the same field. Although a well-known paper is usually an easy pray in
such a hunt, relevant papers using a different terminology, especially recent
ones, are not obvious to the eye.
  In this work, we propose paper recommendation algorithms by using the
citation information among papers. The proposed algorithms are direction aware
in the sense that they can be tuned to find either recent or traditional
papers. The algorithms require a set of papers as input and recommend a set of
related ones. If the user wants to give negative or positive feedback on the
suggested paper set, the recommendation is refined. The search process can be
easily guided in that sense by relevance feedback. We show that this slight
guidance helps the user to reach a desired paper in a more efficient way. We
adapt our models and algorithms also for the venue and reviewer recommendation
tasks. Accuracy of the models and algorithms is thoroughly evaluated by
comparison with multiple baselines and algorithms from the literature in terms
of several objectives specific to citation, venue, and reviewer recommendation
tasks. All of these algorithms are implemented within a publicly available
web-service framework (http://theadvisor.osu.edu/) which currently uses the
data from DBLP and CiteSeer to construct the proposed citation graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1144</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1144</id><created>2012-05-05</created><authors><author><keyname>Mangia</keyname><forenames>Mauro</forenames></author><author><keyname>Rovatti</keyname><forenames>Riccardo</forenames></author><author><keyname>Setti</keyname><forenames>Gianluca</forenames></author></authors><title>Rakeness in the design of Analog-to-Information Conversion of Sparse and
  Localized Signals</title><categories>cs.IT cs.CV math.IT</categories><journal-ref>IEEE Transactions on Circuits and Systems, Part I, vol. 59, n. 5,
  pp. 1001-1014, 2012</journal-ref><doi>10.1109/TCSI.2012.2191312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design of Random Modulation Pre-Integration systems based on the
restricted-isometry property may be suboptimal when the energy of the signals
to be acquired is not evenly distributed, i.e. when they are both sparse and
localized. To counter this, we introduce an additional design criterion, that
we call rakeness, accounting for the amount of energy that the measurements
capture from the signal to be acquired. Hence, for localized signals a proper
system tuning increases the rakeness as well as the average SNR of the samples
used in its reconstruction. Yet, maximizing average SNR may go against the need
of capturing all the components that are potentially non-zero in a sparse
signal, i.e., against the restricted isometry requirement ensuring
reconstructability. What we propose is to administer the trade-off between
rakeness and restricted isometry in a statistical way by laying down an
optimization problem. The solution of such an optimization problem is the
statistic of the process generating the random waveforms onto which the signal
is projected to obtain the measurements. The formal definition of such a
problems is given as well as its solution for signals that are either localized
in frequency or in more generic domain. Sample applications, to ECG signals and
small images of printed letters and numbers, show that rakeness-based design
leads to non-negligible improvements in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1171</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1171</id><created>2012-05-05</created><authors><author><keyname>White</keyname><forenames>Jeffrey M.</forenames></author><author><keyname>Wortman</keyname><forenames>Kevin A.</forenames></author></authors><title>Divide-and-Conquer 3D Convex Hulls on the GPU</title><categories>cs.DC cs.CG</categories><comments>11 pages, 9 figures</comments><acm-class>I.3.5; D.1.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe a pure divide-and-conquer parallel algorithm for computing 3D
convex hulls. We implement that algorithm on GPU hardware, and find a
significant speedup over comparable CPU implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1173</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1173</id><created>2012-05-05</created><authors><author><keyname>Viswanatha</keyname><forenames>Kumar</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Subset Typicality Lemmas and Improved Achievable Regions in
  Multiterminal Source Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following information theoretic setup wherein independent
codebooks of N correlated random variables are generated according to their
respective marginals. The problem of determining the conditions on the rates of
codebooks to ensure the existence of at least one codeword tuple which is
jointly typical with respect to a given joint density (called the multivariate
covering lemma) has been studied fairly well and the associated rate regions
have found applications in several source coding scenarios. However, several
multiterminal source coding applications, such as the general multi-user
Gray-Wyner network, require joint typicality only within subsets of codewords
transmitted. Motivated by such applications, we ask ourselves the conditions on
the rates to ensure the existence of at least one codeword tuple which is
jointly typical within subsets according to given per subset joint densities.
This report focuses primarily on deriving a new achievable rate region for this
problem which strictly improves upon the direct extension of the multivariate
covering lemma, which has quite popularly been used in several earlier work.
Towards proving this result, we derive two important results called `subset
typicality lemmas' which can potentially have broader applicability in more
general scenarios beyond what is considered in this report. We finally apply
the results therein to derive a new achievable region for the general
multi-user Gray-Wyner network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1183</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1183</id><created>2012-05-06</created><updated>2013-04-18</updated><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>On the Complexity of Trial and Error</title><categories>cs.CC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by certain applications from physics, biochemistry, economics, and
computer science, in which the objects under investigation are not accessible
because of various limitations, we propose a trial-and-error model to examine
algorithmic issues in such situations. Given a search problem with a hidden
input, we are asked to find a valid solution, to find which we can propose
candidate solutions (trials), and use observed violations (errors), to prepare
future proposals. In accordance with our motivating applications, we consider
the fairly broad class of constraint satisfaction problems, and assume that
errors are signaled by a verification oracle in the format of the index of a
violated constraint (with the content of the constraint still hidden).
  Our discoveries are summarized as follows. On one hand, despite the seemingly
very little information provided by the verification oracle, efficient
algorithms do exist for a number of important problems. For the Nash, Core,
Stable Matching, and SAT problems, the unknown-input versions are as hard as
the corresponding known-input versions, up to a factor of polynomial. We
further give almost tight bounds on the latter two problems' trial
complexities. On the other hand, there are problems whose complexities are
substantially increased in the unknown-input model. In particular, no
time-efficient algorithms exist (under standard hardness assumptions) for Graph
Isomorphism and Group Isomorphism problems. The tools used to achieve these
results include order theory, strong ellipsoid method, and some non-standard
reductions.
  Our model investigates the value of information, and our results demonstrate
that the lack of input information can introduce various levels of extra
difficulty. The model exhibits intimate connections with (and we hope can also
serve as a useful supplement to) certain existing learning and complexity
theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1190</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1190</id><created>2012-05-06</created><authors><author><keyname>Hidayat</keyname><forenames>Sidiq S.</forenames></author><author><keyname>Kim</keyname><forenames>Bong Keung</forenames></author><author><keyname>Ohba</keyname><forenames>Kohtaro</forenames></author></authors><title>An Approach For Robots To Deal With Objects</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding object and its context are very important for robots when
dealing with objects for completion of a mission. In this paper, an
Affordance-based Ontology (ABO) is proposed for easy robot dealing with
substantive and non-substantive objects. An ABO is a machine-understandable
representation of objects and their relationships by what it's related to and
how it's related. By using ABO, when dealing with a substantive object, robots
can understand the representation of its object and its relation with other
non-substantive objects. When the substantive object is not available, the
robots have the understanding ability, in term of objects and their functions
to select a non substantive object in order to complete the mission, such as
giving raincoat or hat instead of getting stuck due to the unavailability of
substantive object, e.g. umbrella. The experiment is done in the Ubiquitous
Robotics Technology (u-RT) Space of National Institute of Advanced Industrial
Science and Technology (AIST), Tsukuba, Japan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1195</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1195</id><created>2012-05-06</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author></authors><title>Sequential-Access FM-Indexes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous authors have shown how to build FM-indexes efficiently in external
memory, but querying them efficiently remains an open problem. Searching
na\&quot;{i}vely for a pattern $P$ requires (\Theta (|P|)) random access. In this
paper we show how, by storing a few small auxiliary tables, we can access data
only in the order in which they appear on disk, which should be faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1196</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1196</id><created>2012-05-06</created><authors><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Shou</keyname><forenames>Biying</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Capacity Allocation and Pricing Strategies for Wireless Femtocell
  Services</title><categories>cs.NI</categories><comments>This paper has been submitted to INFORMS Journal on Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indoor cell phone users often suffer from poor connectivity. One promising
solution, femtocell technology, has been rapidly developed and deployed over
the past few years. One of the biggest challenges for femtocell deployment is
lack of a clear business model. This paper investigates the economic incentive
for the cellular operator (also called macrocell operator) to enable femtocell
service by leasing spectrum resource to an independent femtocell operator. On
the one hand, femtocell services can increase communication service quality and
thus increase the efficiency of the spectrum resource. On the other hand,
femtocell services may introduce more competition to the market. We model the
interactions between a macrocell operator, a femtocell operator, and users as a
three-stage dynamic game, and derive the equilibrium pricing and capacity
allocation decisions. We show that when spectrum resources are very limited,
the macrocell operator has incentive to lease spectrum to femtocell operators,
as femtocell service can provide access to more users and efficiently increase
the coverage. However, when the total spectrum resource is large, femtocell
service offers significant competition to macrocell service. Macrocell operator
thus has less incentive to enable femtocell service. We also investigate the
issue of additional operational cost and limited coverage of femtocell service
on equilibrium decisions, consumer surplus and social welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1203</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1203</id><created>2012-05-06</created><updated>2012-08-10</updated><authors><author><keyname>Valls</keyname><forenames>V&#xed;ctor</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Jos&#xe9; Luis</forenames></author><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>Hierarchical Range Sectoring and Bidirectional Link Quality Estimation
  for On-demand Collections in WSNs</title><categories>cs.NI</categories><comments>22 pages, 11 figures</comments><journal-ref>Ad Hoc Networks 11(3): 894-906 (2013)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper presents two mechanisms for designing an on-demand, reliable and
efficient collection protocol for Wireless Sensor Networks. The former is the
Bidirectional Link Quality Estimation, which allows nodes to easily and quickly
compute the quality of a link between a pair of nodes. The latter, Hierarchical
Range Sectoring, organizes sensors in different sectors based on their location
within the network. Based on this organization, nodes from each sector are
coordinated to transmit in specific periods of time to reduce the hidden
terminal problem. To evaluate these two mechanisms, a protocol called HBCP
(Hierarchical-Based Collection Protocol), that implements both mechanisms, has
been implemented in TinyOS 2.1, and evaluated in a testbed using TelosB motes.
The results show that the HBCP protocol is able to achieve a very high
reliability, especially in large networks and in scenarios with bottlenecks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1212</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1212</id><created>2012-05-06</created><updated>2012-09-23</updated><authors><author><keyname>Ciss</keyname><forenames>Abdoul Aziz</forenames></author><author><keyname>Cheikh</keyname><forenames>Ahmed Youssef Ould</forenames></author><author><keyname>Sow</keyname><forenames>Djiby</forenames></author></authors><title>A Factoring and Discrete Logarithm based Cryptosystem</title><categories>cs.CR math.NT</categories><comments>Something was not correct in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new public key cryptosystem based on two hard
problems : the cube root extraction modulo a composite moduli (which is
equivalent to the factorisation of the moduli) and the discrete logarithm
problem. These two hard problems are combined during the key generation,
encryption and decryption phases. By combining the IFP and the DLP we introduce
a secure and efficient public key cryptosystem. To break the scheme, an
adversary may solve the IFP and the DLP separately which is computationally
infeasible. The key generation is a simple operation based on the discrete
logarithm modulo a composite moduli. The encryption phase is based both on the
cube root computation and the DLP. These operations are computationally
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1223</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1223</id><created>2012-05-06</created><updated>2013-08-04</updated><authors><author><keyname>Han</keyname><forenames>Jingjun</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Proving Inequalities and Solving Global Optimization Problems via
  Simplified CAD Projection</title><categories>cs.SC math.AG</categories><comments>26 pages</comments><msc-class>68W30, 14Q20</msc-class><acm-class>B.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\xx_n=(x_1,\ldots,x_n)$ and $f\in \R[\xx_n,k]$. The problem of finding
all $k_0$ such that $f(\xx_n,k_0)\ge 0$ on $\mathbb{R}^n$ is considered in this
paper, which obviously takes as a special case the problem of computing the
global infimum or proving the semi-definiteness of a polynomial.
  For solving the problems, we propose a simplified Brown's CAD projection
operator, \Nproj, of which the projection scale is always no larger than that
of Brown's. For many problems, the scale is much smaller than that of Brown's.
As a result, the lifting phase is also simplified. Some new algorithms based on
\Nproj\ for solving those problems are designed and proved to be correct.
Comparison to some existing tools on some examples is reported to illustrate
the effectiveness of our new algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1225</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1225</id><created>2012-05-06</created><authors><author><keyname>Sandhu</keyname><forenames>Romeil</forenames></author><author><keyname>Dominitz</keyname><forenames>Ayelet</forenames></author><author><keyname>Gao</keyname><forenames>Yi</forenames></author><author><keyname>Tannenbaum</keyname><forenames>Allen</forenames></author></authors><title>Volumetric Mapping of Genus Zero Objects via Mass Preservation</title><categories>cs.CG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a technique to map any genus zero solid object onto
a hexahedral decomposition of a solid cube. This problem appears in many
applications ranging from finite element methods to visual tracking. From this,
one can then hopefully utilize the proposed technique for shape analysis,
registration, as well as other related computer graphics tasks. More
importantly, given that we seek to establish a one-to-one correspondence of an
input volume to that of a solid cube, our algorithm can naturally generate a
quality hexahedral mesh as an output. In addition, we constrain the mapping
itself to be volume preserving allowing for the possibility of further mesh
simplification. We demonstrate our method both qualitatively and quantitatively
on various 3D solid models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1227</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1227</id><created>2012-05-06</created><authors><author><keyname>Schomisch</keyname><forenames>Siegfried</forenames></author><author><keyname>Zens</keyname><forenames>Maria</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Are e-readers suitable tools for scholarly work?</title><categories>cs.DL cs.HC</categories><comments>22 pages, 6 figures, accepted for publication in Online Information
  Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to offer insights into the usability, acceptance and
limitations of e-readers with regard to the specific requirements of scholarly
text work. To fit into the academic workflow non-linear reading, bookmarking,
commenting, extracting text or the integration of non-textual elements must be
supported. A group of social science students were questioned about their
experiences with electronic publications for study purposes. This same group
executed several text-related tasks with the digitized material presented to
them in two different file formats on four different e-readers. Their
performances were subsequently evaluated by means of frequency analyses in
detail. Findings - e-Publications have made advances in the academic world;
however e-readers do not yet fit seamlessly into the established chain of
scholarly text-processing focusing on how readers use material during and after
reading. Our tests revealed major deficiencies in these techniques. With a
small number of participants (n=26) qualitative insights can be obtained, not
representative results. Further testing with participants from various
disciplines and of varying academic status is required to arrive at more
broadly applicable results. Practical implications - Our test results help to
optimize file conversion routines for scholarly texts. We evaluated our data on
the basis of descriptive statistics and abstained from any statistical
significance test. The usability test of e-readers in a scientific context
aligns with both studies on the prevalence of e-books in the sciences and
technical test reports of portable reading devices. Still, it takes a
distinctive angle in focusing on the characteristics and procedures of textual
work in the social sciences and measures the usability of e-readers and
file-features against these standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1240</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1240</id><created>2012-05-06</created><authors><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>Convex Relaxation for Combinatorial Penalties</title><categories>stat.ML cs.LG</categories><comments>35 page</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an unifying view of several recently proposed
structured sparsity-inducing norms. We consider the situation of a model
simultaneously (a) penalized by a set- function de ned on the support of the
unknown parameter vector which represents prior knowledge on supports, and (b)
regularized in Lp-norm. We show that the natural combinatorial optimization
problems obtained may be relaxed into convex optimization problems and
introduce a notion, the lower combinatorial envelope of a set-function, that
characterizes the tightness of our relaxations. We moreover establish links
with norms based on latent representations including the latent group Lasso and
block-coding, and with norms obtained from submodular functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1242</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1242</id><created>2012-05-06</created><updated>2012-05-08</updated><authors><author><keyname>Nomura</keyname><forenames>Ryo</forenames></author><author><keyname>Matsushima</keyname><forenames>Toshiyasu</forenames></author></authors><title>Information Spectrum Approach to Overflow Probability of Variable-Length
  Codes with Conditional Cost Function</title><categories>cs.IT math.IT</categories><comments>to be presented at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lossless variable-length source coding with unequal cost function is
considered for general sources. In this problem, the codeword cost instead of
codeword length is important. The infimum of average codeword cost has already
been determined for general sources. We consider the overflow probability of
codeword cost and determine the infimum of achievable overflow threshold. Our
analysis is on the basis of information-spectrum methods and hence valid
through the general source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1245</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1245</id><created>2012-05-06</created><updated>2013-02-06</updated><authors><author><keyname>Vincent</keyname><forenames>Martin</forenames></author><author><keyname>Hansen</keyname><forenames>Niels Richard</forenames></author></authors><title>Sparse group lasso and high dimensional multinomial classification</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sparse group lasso optimization problem is solved using a coordinate
gradient descent algorithm. The algorithm is applicable to a broad class of
convex loss functions. Convergence of the algorithm is established, and the
algorithm is used to investigate the performance of the multinomial sparse
group lasso classifier. On three different real data examples the multinomial
group lasso clearly outperforms multinomial lasso in terms of achieved
classification error rate and in terms of including fewer features for the
classification. The run-time of our sparse group lasso implementation is of the
same order of magnitude as the multinomial lasso algorithm implemented in the R
package glmnet. Our implementation scales well with the problem size. One of
the high dimensional examples considered is a 50 class classification problem
with 10k features, which amounts to estimating 500k parameters. The
implementation is available as the R package msgl.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1254</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1254</id><created>2012-05-06</created><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Combinatorial coloring of 3-colorable graphs</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of coloring a 3-colorable graph in polynomial time
using as few colors as possible. We present a combinatorial algorithm getting
down to $\tO(n^{4/11})$ colors. This is the first combinatorial improvement of
Blum's $\tO(n^{3/8})$ bound from FOCS'90. Like Blum's algorithm, our new
algorithm composes nicely with recent semi-definite approaches. The current
best bound is $O(n^{0.2072})$ colors by Chlamtac from FOCS'07. We now bring it
down to $O(n^{0.2038})$ colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1262</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1262</id><created>2012-05-06</created><updated>2012-05-07</updated><authors><author><keyname>Laekhanukit</keyname><forenames>Bundit</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author></authors><title>A Rounding by Sampling Approach to the Minimum Size k-Arc Connected
  Subgraph Problem</title><categories>cs.DS math.CO</categories><comments>10 pages, 2 figures, ICALP2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the k-arc connected subgraph problem, we are given a directed graph G and
an integer k and the goal is the find a subgraph of minimum cost such that
there are at least k-arc disjoint paths between any pair of vertices. We give a
simple (1 + 1/k)-approximation to the unweighted variant of the problem, where
all arcs of G have the same cost. This improves on the 1 + 2/k approximation of
Gabow et al. [GGTW09].
  Similar to the 2-approximation algorithm for this problem [FJ81], our
algorithm simply takes the union of a k in-arborescence and a k
out-arborescence. The main difference is in the selection of the two
arborescences. Here, inspired by the recent applications of the rounding by
sampling method (see e.g. [AGM+ 10, MOS11, OSS11, AKS12]), we select the
arborescences randomly by sampling from a distribution on unions of k
arborescences that is defined based on an extreme point solution of the linear
programming relaxation of the problem. In the analysis, we crucially utilize
the sparsity property of the extreme point solution to upper-bound the size of
the union of the sampled arborescences.
  To complement the algorithm, we also show that the integrality gap of the
minimum cost strongly connected subgraph problem (i.e., when k = 1) is at least
3/2 - c, for any c &gt; 0. Our integrality gap instance is inspired by the
integrality gap example of the asymmetric traveling salesman problem [CGK06],
hence providing further evidence of connections between the approximability of
the two problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1271</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1271</id><created>2012-05-06</created><updated>2014-12-02</updated><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author></authors><title>Directed Subset Feedback Vertex Set is Fixed-Parameter Tractable</title><categories>cs.DS cs.CC</categories><comments>To appear in ACM Transactions on Algorithms. A preliminary version
  appeared in ICALP '12. We would like to thank Marcin Pilipczuk for pointing
  out a missing case in the conference version which has been considered in
  this version. Also, we give an single exponential FPT algorithm improving on
  the double exponential algorithm from the conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$ and an integer $k$, the Feedback Vertex Set (FVS) problem
asks if there is a vertex set $T$ of size at most $k$ that hits all cycles in
the graph. The fixed-parameter tractability status of FVS in directed graphs
was a long-standing open problem until Chen et al. (STOC '08) showed that it is
FPT by giving a $4^{k}k!n^{O(1)}$ time algorithm. In the subset versions of
this problems, we are given an additional subset $S$ of vertices (resp., edges)
and we want to hit all cycles passing through a vertex of $S$ (resp. an edge of
$S$). Recently, the Subset Feedback Vertex Set in undirected graphs was shown
to be FPT by Cygan et al. (ICALP '11) and independently by Kakimura et al.
(SODA '12). We generalize the result of Chen et al. (STOC '08) by showing that
Subset Feedback Vertex Set in directed graphs can be solved in time
$2^{O(k^3)}n^{O(1)}$. By our result, we complete the picture for feedback
vertex set problems and their subset versions in undirected and directed
graphs. Besides proving the fixed-parameter tractability of Directed Subset
Feedback Vertex Set, we reformulate the random sampling of important separators
technique in an abstract way that can be used for a general family of
transversal problems. Moreover, we modify the probability distribution used in
the technique to achieve better running time; in particular, this gives an
improvement from $2^{2^{O(k)}}$ to $2^{O(k^2)}$ in the parameter dependence of
the Directed Multiway Cut algorithm of Chitnis et al. (SODA '12).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1277</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1277</id><created>2012-05-06</created><updated>2014-01-13</updated><authors><author><keyname>Kaplan</keyname><forenames>Nathan</forenames></author></authors><title>MacWilliams Identities for $m$-tuple Weight Enumerators</title><categories>cs.IT math.CO math.IT</categories><comments>17 pages. Accepted to SIAM Journal on Discrete Mathematics</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since MacWilliams proved the original identity relating the Hamming weight
enumerator of a linear code to the weight enumerator of its dual code there
have been many different generalizations, leading to the development of
$m$-tuple support enumerators. We prove a generalization of theorems of Britz
and of Ray-Chaudhuri and Siap, which build on earlier work of Kl{\o}ve,
Shiromoto, Wan, and others. We then give illustrations of these $m$-tuple
weight enumerators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1281</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1281</id><created>2012-05-07</created><updated>2013-04-15</updated><authors><author><keyname>Yan</keyname><forenames>Li</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author></authors><title>LP-rounding Algorithms for the Fault-Tolerant Facility Placement Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fault-Tolerant Facility Placement problem (FTFP) is a generalization of
the classic Uncapacitated Facility Location Problem (UFL). In FTFP we are given
a set of facility sites and a set of clients. Opening a facility at site $i$
costs $f_i$ and connecting client $j$ to a facility at site $i$ costs $d_{ij}$.
We assume that the connection costs (distances) $d_{ij}$ satisfy the triangle
inequality. Multiple facilities can be opened at any site. Each client $j$ has
a demand $r_j$, which means that it needs to be connected to $r_j$ different
facilities (some of which could be located on the same site). The goal is to
minimize the sum of facility opening cost and connection cost.
  The main result of this paper is a 1.575-approximation algorithm for FTFP,
based on LP-rounding. The algorithm first reduces the demands to values
polynomial in the number of sites. Then it uses a technique that we call
adaptive partitioning, which partitions the instance by splitting clients into
unit demands and creating a number of (not yet opened) facilities at each site.
It also partitions the optimal fractional solution to produce a fractional
solution for this new instance. The partitioned instance satisfies a number of
properties that allow us to exploit existing LP-rounding methods for UFL to
round our partitioned solution to an integral solution, preserving the
approximation ratio. In particular, our 1.575-approximation algorithm is based
on the ideas from the 1.575-approximation algorithm for UFL by Byrka et al.,
with changes necessary to satisfy the fault-tolerance requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1287</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1287</id><created>2012-05-07</created><updated>2014-11-02</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Jung</keyname><forenames>Tzyy-Ping</forenames></author><author><keyname>Makeig</keyname><forenames>Scott</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Compressed Sensing for Energy-Efficient Wireless Telemonitoring of
  Noninvasive Fetal ECG via Block Sparse Bayesian Learning</title><categories>stat.ML cs.LG stat.AP</categories><comments>The code and the data can be downloaded from the first author's
  homepage: http://sites.google.com/site/researchbyzhang/bsbl, or
  http://dsp.ucsd.edu/~zhilin/BSBL.html</comments><doi>10.1109/TBME.2012.2226175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. The
design of a telemonitoring system via a wireless body-area network with low
energy consumption for ambulatory use is highly desirable. As an emerging
technique, compressed sensing (CS) shows great promise in
compressing/reconstructing data with low energy consumption. However, due to
some specific characteristics of raw FECG recordings such as non-sparsity and
strong noise contamination, current CS algorithms generally fail in this
application.
  This work proposes to use the block sparse Bayesian learning (BSBL) framework
to compress/reconstruct non-sparse raw FECG recordings. Experimental results
show that the framework can reconstruct the raw recordings with high quality.
Especially, the reconstruction does not destroy the interdependence relation
among the multichannel recordings. This ensures that the independent component
analysis decomposition of the reconstructed recordings has high fidelity.
Furthermore, the framework allows the use of a sparse binary sensing matrix
with much fewer nonzero entries to compress recordings. Particularly, each
column of the matrix can contain only two nonzero entries. This shows the
framework, compared to other algorithms such as current CS algorithms and
wavelet algorithms, can greatly reduce code execution in CPU in the data
compression stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1312</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1312</id><created>2012-05-07</created><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author><author><keyname>Vardi</keyname><forenames>Shai</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author></authors><title>Converting online algorithms to local computation algorithms</title><categories>cs.DS</categories><comments>ICALP 2012, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general method for converting online algorithms to local
computation algorithms by selecting a random permutation of the input, and
simulating running the online algorithm. We bound the number of steps of the
algorithm using a query tree, which models the dependencies between queries. We
improve previous analyses of query trees on graphs of bounded degree, and
extend the analysis to the cases where the degrees are distributed binomially,
and to a special case of bipartite graphs.
  Using this method, we give a local computation algorithm for maximal matching
in graphs of bounded degree, which runs in time and space O(log^3 n).
  We also show how to convert a large family of load balancing algorithms
(related to balls and bins problems) to local computation algorithms. This
gives several local load balancing algorithms which achieve the same
approximation ratios as the online algorithms, but run in O(log n) time and
space.
  Finally, we modify existing local computation algorithms for hypergraph
2-coloring and k-CNF and use our improved analysis to obtain better time and
space bounds, of O(log^4 n), removing the dependency on the maximal degree of
the graph from the exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1331</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1331</id><created>2012-05-07</created><authors><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author></authors><title>Approximation Algorithms for Wireless Link Scheduling with Flexible Data
  Rates</title><categories>cs.NI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider scheduling problems in wireless networks with respect to flexible
data rates. That is, more or less data can be transmitted per time depending on
the signal quality, which is determined by the
signal-to-interference-plus-noise ratio (SINR). Each wireless link has a
utility function mapping SINR values to the respective data rates. We have to
decide which transmissions are performed simultaneously and (depending on the
problem variant) also which transmission powers are used.
  In the capacity-maximization problem, one strives to maximize the overall
network throughput, i.e., the summed utility of all links. For arbitrary
utility functions (not necessarily continuous ones), we present an O(log
n)-approximation when having n communication requests. This algorithm is built
on a constant-factor approximation for the special case of the respective
problem where utility functions only consist of a single step. In other words,
each link has an individual threshold and we aim at maximizing the number of
links whose threshold is satisfied. On the way, this improves the result in
[Kesselheim, SODA 2011] by not only extending it to individual thresholds but
also showing a constant approximation factor independent of assumptions on the
underlying metric space or the network parameters.
  In addition, we consider the latency-minimization problem. Here, each link
has a demand, e.g., representing an amount of data. We have to compute a
schedule of shortest possible length such that for each link the demand is
fulfilled, that is the overall summed utility (or data transferred) is at least
as large as its demand. Based on the capacity-maximization algorithm, we show
an O(log^2 n)-approximation for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1357</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1357</id><created>2012-05-07</created><authors><author><keyname>Menahem</keyname><forenames>Eitan</forenames></author><author><keyname>Puzis</keyname><forenames>Rami</forenames></author></authors><title>Detecting Spammers via Aggregated Historical Data Set</title><categories>cs.CR cs.LG</categories><comments>This is a conference version of the HDS research. 13 pages 10 figures</comments><acm-class>C.2.0; H.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The battle between email service providers and senders of mass unsolicited
emails (Spam) continues to gain traction. Vast numbers of Spam emails are sent
mainly from automatic botnets distributed over the world. One method for
mitigating Spam in a computationally efficient manner is fast and accurate
blacklisting of the senders. In this work we propose a new sender reputation
mechanism that is based on an aggregated historical data-set which encodes the
behavior of mail transfer agents over time. A historical data-set is created
from labeled logs of received emails. We use machine learning algorithms to
build a model that predicts the \emph{spammingness} of mail transfer agents in
the near future. The proposed mechanism is targeted mainly at large enterprises
and email service providers and can be used for updating both the black and the
white lists. We evaluate the proposed mechanism using 9.5M anonymized log
entries obtained from the biggest Internet service provider in Europe.
Experiments show that proposed method detects more than 94% of the Spam emails
that escaped the blacklist (i.e., TPR), while having less than 0.5%
false-alarms. Therefore, the effectiveness of the proposed method is much
higher than of previously reported reputation mechanisms, which rely on emails
logs. In addition, the proposed method, when used for updating both the black
and white lists, eliminated the need in automatic content inspection of 4 out
of 5 incoming emails, which resulted in dramatic reduction in the filtering
computational load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1358</identifier>
 <datestamp>2012-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1358</id><created>2012-05-07</created><updated>2012-07-12</updated><authors><author><keyname>Sankaran</keyname><forenames>Abhisekh</forenames></author><author><keyname>Adsul</keyname><forenames>Bharat</forenames></author><author><keyname>Madan</keyname><forenames>Vivek</forenames></author><author><keyname>Kamath</keyname><forenames>Pritish</forenames></author><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author></authors><title>Preservation under Substructures modulo Bounded Cores</title><categories>cs.LO math.LO</categories><comments>From v2 to v3: Corrected typos, edited sentences for better
  readability; Conjecture 1 of v2 is now resolved so it is now Theorem 4, its
  proof is included in a new section (Section 7), Thm i in v2 is now Thm i+1
  for i &gt;= 4; everything else remains the same. From v1 to v2: Thm i is now Thm
  i-1 for i &gt;= 7, Corrected the proof of Theorem 10 (now Theorem 9) for B &gt; 2
  (statement is still correct)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a model-theoretic property that generalizes the classical
notion of &quot;preservation under substructures&quot;. We call this property
\emph{preservation under substructures modulo bounded cores}, and present a
syntactic characterization via $\Sigma_2^0$ sentences for properties of
arbitrary structures definable by FO sentences. As a sharper characterization,
we further show that the count of existential quantifiers in the $\Sigma_2^0$
sentence equals the size of the smallest bounded core. We also present our
results on the sharper characterization for special fragments of FO and also
over special classes of structures. We present a (not FO-definable) class of
finite structures for which the sharper characterization fails, but for which
the classical {\L}o\'s-Tarski preservation theorem holds. As a fallout of our
studies, we obtain combinatorial proofs of the {\L}o\'s-Tarski theorem for some
of the aforementioned cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1365</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1365</id><created>2012-05-07</created><authors><author><keyname>Mukherjee</keyname><forenames>Aroop</forenames></author><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>Image Enhancement with Statistical Estimation</title><categories>cs.MM cs.CV</categories><comments>9 pages,6 figures; ISSN:0975-5578 (Online); 0975-5934 (Print)</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  April 2012, Volume 4, Number 2, page 59-67</journal-ref><doi>10.5121/ijma.2012.4205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrast enhancement is an important area of research for the image analysis.
Over the decade, the researcher worked on this domain to develop an efficient
and adequate algorithm. The proposed method will enhance the contrast of image
using Binarization method with the help of Maximum Likelihood Estimation (MLE).
The paper aims to enhance the image contrast of bimodal and multi-modal images.
The proposed methodology use to collect mathematical information retrieves from
the image. In this paper, we are using binarization method that generates the
desired histogram by separating image nodes. It generates the enhanced image
using histogram specification with binarization method. The proposed method has
showed an improvement in the image contrast enhancement compare with the other
image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1366</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1366</id><created>2012-05-07</created><updated>2013-04-24</updated><authors><author><keyname>H&#xfc;gel</keyname><forenames>Max</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author></authors><title>Remote sensing via $\ell_1$ minimization</title><categories>cs.IT math.IT math.NA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting the locations of targets in the far
field by sending probing signals from an antenna array and recording the
reflected echoes. Drawing on key concepts from the area of compressive sensing,
we use an $\ell_1$-based regularization approach to solve this, in general
ill-posed, inverse scattering problem. As common in compressed sensing, we
exploit randomness, which in this context comes from choosing the antenna
locations at random. With $n$ antennas we obtain $n^2$ measurements of a vector
$x \in \C^{N}$ representing the target locations and reflectivities on a
discretized grid. It is common to assume that the scene $x$ is sparse due to a
limited number of targets. Under a natural condition on the mesh size of the
grid, we show that an $s$-sparse scene can be recovered via
$\ell_1$-minimization with high probability if $n^2 \geq C s \log^2(N)$. The
reconstruction is stable under noise and under passing from sparse to
approximately sparse vectors. Our theoretical findings are confirmed by
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1373</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1373</id><created>2012-05-07</created><updated>2014-01-17</updated><authors><author><keyname>Polacek</keyname><forenames>Lukas</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Quasi-Polynomial Local Search for Restricted Max-Min Fair Allocation</title><categories>cs.DS</categories><comments>14 pages, 1 figure</comments><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The restricted max-min fair allocation problem (also known as the restricted
Santa Claus problem) is one of few problems that enjoys the intriguing status
of having a better estimation algorithm than approximation algorithm. Indeed,
Asadpour et al. proved that a certain configuration LP can be used to estimate
the optimal value within a factor ${1}/{(4+\epsilon)}$, for any $\epsilon&gt;0$,
but at the same time it is not known how to efficiently find a solution with a
comparable performance guarantee.
  A natural question that arises from their work is if the difference between
these guarantees is inherent or because of a lack of suitable techniques. We
address this problem by giving a quasi-polynomial approximation algorithm with
the mentioned performance guarantee. More specifically, we modify the local
search of Asadpour et al. and provide a novel analysis that lets us
significantly improve the bound on its running time: from $2^{O(n)}$ to
$n^{O(\log n)}$. Our techniques also have the interesting property that
although we use the rather complex configuration LP in the analysis, we never
actually solve it and therefore the resulting algorithm is purely
combinatorial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1389</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1389</id><created>2012-05-07</created><authors><author><keyname>Lomnitz</keyname><forenames>Yuval</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>A simpler derivation of the coding theorem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple proof for the Shannon coding theorem, using only the Markov
inequality, is presented. The technique is useful for didactic purposes, since
it does not require many preliminaries and the information density and mutual
information follow naturally in the proof. It may also be applicable to
situations where typicality is not natural.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1419</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1419</id><created>2012-05-07</created><authors><author><keyname>Wagner</keyname><forenames>Caroline S.</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>An Integrated Impact Indicator (I3): A New Definition of &quot;Impact&quot; with
  Policy Relevance</title><categories>cs.DL</categories><comments>Research Evaluation (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Allocation of research funding, as well as promotion and tenure decisions,
are increasingly made using indicators and impact factors drawn from citations
to published work. A debate among scientometricians about proper normalization
of citation counts has resolved with the creation of an Integrated Impact
Indicator (I3) that solves a number of problems found among previously used
indicators. The I3 applies non-parametric statistics using percentiles,
allowing highly-cited papers to be weighted more than less-cited ones. It
further allows unbundling of venues (i.e., journals or databases) at the
article level. Measures at the article level can be re-aggregated in terms of
units of evaluation. At the venue level, the I3 creates a properly weighted
alternative to the journal impact factor. I3 has the added advantage of
enabling and quantifying classifications such as the six percentile rank
classes used by the National Science Board's Science &amp; Engineering Indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1423</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1423</id><created>2012-05-07</created><updated>2013-10-08</updated><authors><author><keyname>Kueng</keyname><forenames>Richard</forenames></author><author><keyname>Gross</keyname><forenames>David</forenames></author></authors><title>RIPless compressed sensing from anisotropic measurements</title><categories>cs.IT math.IT</categories><comments>19 pages. To appear in Linear Algebra and its Applications, Special
  Issue on Sparse Approximate Solution of Linear Systems</comments><journal-ref>Linear Algebra and its Applications 441 (2014): 110-123</journal-ref><doi>10.1016/j.laa.2013.04.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is the art of reconstructing a sparse vector from its
inner products with respect to a small set of randomly chosen measurement
vectors. It is usually assumed that the ensemble of measurement vectors is in
isotropic position in the sense that the associated covariance matrix is
proportional to the identity matrix. In this paper, we establish bounds on the
number of required measurements in the anisotropic case, where the ensemble of
measurement vectors possesses a non-trivial covariance matrix. Essentially, we
find that the required sampling rate grows proportionally to the condition
number of the covariance matrix. In contrast to other recent contributions to
this problem, our arguments do not rely on any restricted isometry properties
(RIP's), but rather on ideas from convex geometry which have been
systematically studied in the theory of low-rank matrix recovery. This allows
for a simple argument and slightly improved bounds, but may lead to a worse
dependency on noise (which we do not consider in the present paper).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1428</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1428</id><created>2012-05-07</created><authors><author><keyname>Swaddiwudhipong</keyname><forenames>S.</forenames></author><author><keyname>Islam</keyname><forenames>M. J.</forenames></author><author><keyname>Liu</keyname><forenames>Z. S.</forenames></author></authors><title>High Velocity Penetration/Perforation Using Coupled Smooth Particle
  Hydrodynamics-Finite Element Method</title><categories>cs.CE physics.flu-dyn</categories><comments>18 pages; International Journal of Protective Structures 2010</comments><msc-class>74C05</msc-class><acm-class>G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite element method (FEM) suffers from a serious mesh distortion problem
when used for high velocity impact analyses. The smooth particle hydrodynamics
(SPH) method is appropriate for this class of problems involving severe damages
but at considerable computational cost. It is beneficial if the latter is
adopted only in severely distorted regions and FEM further away. The coupled
smooth particle hydrodynamics - finite element method (SFM) has been adopted in
a commercial hydrocode LS-DYNA to study the perforation of Weldox 460E steel
and AA5083-H116 aluminum plates with varying thicknesses and various projectile
nose geometries including blunt, conical and ogival noses. Effects of the SPH
domain size and particle density are studied considering the friction effect
between the projectile and the target materials. The simulated residual
velocities and the ballistic limit velocities from the SFM agree well with the
published experimental data. The study shows that SFM is able to emulate the
same failure mechanisms of the steel and aluminum plates as observed in various
experimental investigations for initial impact velocity of 170 m/s and higher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1456</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1456</id><created>2012-05-07</created><authors><author><keyname>Lakkaraju</keyname><forenames>Himabindu</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Indrajit</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Chiranjib</forenames></author></authors><title>Dynamic Multi-Relational Chinese Restaurant Process for Analyzing
  Influences on Users in Social Media</title><categories>cs.SI cs.LG physics.soc-ph</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of analyzing influence of various factors affecting
individual messages posted in social media. The problem is challenging because
of various types of influences propagating through the social media network
that act simultaneously on any user. Additionally, the topic composition of the
influencing factors and the susceptibility of users to these influences evolve
over time. This problem has not studied before, and off-the-shelf models are
unsuitable for this purpose. To capture the complex interplay of these various
factors, we propose a new non-parametric model called the Dynamic
Multi-Relational Chinese Restaurant Process. This accounts for the user network
for data generation and also allows the parameters to evolve over time.
Designing inference algorithms for this model suited for large scale
social-media data is another challenge. To this end, we propose a scalable and
multi-threaded inference algorithm based on online Gibbs Sampling. Extensive
evaluations on large-scale Twitter and Facebook data show that the extracted
topics when applied to authorship and commenting prediction outperform
state-of-the-art baselines. More importantly, our model produces valuable
insights on topic trends and user personality trends, beyond the capability of
existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1457</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1457</id><created>2012-05-07</created><authors><author><keyname>Dichev</keyname><forenames>Kiril</forenames></author><author><keyname>Reid</keyname><forenames>Fergal</forenames></author><author><keyname>Lastovetsky</keyname><forenames>Alexey</forenames></author></authors><title>Efficient and reliable network tomography in heterogeneous networks
  using BitTorrent broadcasts and clustering algorithms</title><categories>cs.DC cs.NI cs.SI</categories><comments>11pages, 13figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of network performance and discovery, network tomography focuses
on reconstructing network properties using only end-to-end measurements at the
application layer. One challenging problem in network tomography is
reconstructing available bandwidth along all links during multiple
source/multiple destination transmissions. The traditional measurement
procedures used for bandwidth tomography are extremely time consuming. We
propose a novel solution to this problem. Our method counts the fragments
exchanged during a BitTorrent broadcast. While this measurement has a high
level of randomness, it can be obtained very efficiently, and aggregated into a
reliable metric. This data is then analyzed with state-of-the-art algorithms,
which reliably reconstruct logical clusters of nodes inter-connected by high
bandwidth, as well as bottlenecks between these logical clusters. Our
experiments demonstrate that the proposed two-phase approach efficiently solves
the presented problem for a number of settings on a complex grid
infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1462</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1462</id><created>2012-05-07</created><authors><author><keyname>Husain</keyname><forenames>Mohammad Iftekhar</forenames></author><author><keyname>Ko</keyname><forenames>Steve</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author><author><keyname>Uurtamo</keyname><forenames>Steve</forenames></author></authors><title>Almost Universal Hash Families are also Storage Enforcing</title><categories>cs.IT cs.CC math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1104.3025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every almost universal hash function also has the storage
enforcement property. Almost universal hash functions have found numerous
applications and we show that this new storage enforcement property allows the
application of almost universal hash functions in a wide range of remote
verification tasks: (i) Proof of Secure Erasure (where we want to remotely
erase and securely update the code of a compromised machine with memory-bounded
adversary), (ii) Proof of Ownership (where a storage server wants to check if a
client has the data it claims to have before giving access to deduplicated
data) and (iii) Data possession (where the client wants to verify whether the
remote storage server is storing its data). Specifically, storage enforcement
guarantee in the classical data possession problem removes any practical
incentive for the storage server to cheat the client by saving on storage
space.
  The proof of our result relies on a natural combination of Kolmogorov
Complexity and List Decoding. To the best of our knowledge this is the first
work that combines these two techniques. We believe the newly introduced
storage enforcement property of almost universal hash functions will open
promising avenues of exciting research under memory-bounded (bounded storage)
adversary model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1465</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1465</id><created>2011-12-30</created><authors><author><keyname>Yao</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Bing</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Wu</keyname><forenames>Guowei</forenames></author><author><keyname>Lin</keyname><forenames>Qiang</forenames></author></authors><title>A Group Key Management Protocol Based on Weight-Balanced 2-3 Tree for
  Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><msc-class>68M14</msc-class><acm-class>C.2</acm-class><journal-ref>Information, Vol.14, No.10, pp.3261-3278, October 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicast in Wireless Sensor Networks (WSNs) is an attractive mechanism for
delivering data to multiple receivers as it saves bandwidth. To guarantee the
security of multicast, the group key is used to encrypt and decrypt the
packages. However, providing key management services in WSNs is complicated
because sensor nodes possess limited resources of computing, storage and
communication. To address the balance between security and limited resources, a
multicast group key management protocol based on the weight-balanced 2-3 tree
is proposed to generate, distribute, and update the group key securely and
efficiently. The decentralized group key management method is employed. A
weight-balanced 2-3 key tree is formed in every subgroup. Instead of using the
conventional symmetric and non-symmetric encryption algorithms, the Maximum
Distance Separable (MDS) code technique is used to distribute the multicast key
dynamically. During the key updating, a series of adjustment rules are
summarized to keep the tree weight-balanced, where pseudo-nodes as leaves are
added to reduce the computation and communication complexity. Compared with
some other group key management protocols, our scheme shows higher superiority
on security and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1470</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1470</id><created>2012-05-07</created><authors><author><keyname>Gugelmann</keyname><forenames>Luca</forenames></author><author><keyname>Panagiotou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Peter</keyname><forenames>Ueli</forenames></author></authors><title>Random Hyperbolic Graphs: Degree Sequence and Clustering</title><categories>math.CO cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decades, the study of models for large real-world networks has
been a very popular and active area of research. A reasonable model should not
only replicate all the structural properties that are observed in real world
networks (for example, heavy tailed degree distributions, high clustering and
small diameter), but it should also be amenable to mathematical analysis. There
are plenty of models that succeed in the first task but are hard to analyze
rigorously. On the other hand, a multitude of proposed models, like classical
random graphs, can be studied mathematically, but fail in creating certain
aspects that are observed in real-world networks.
  Recently, Papadopoulos, Krioukov, Boguna and Vahdat [INFOCOM'10] introduced a
random geometric graph model that is based on hyperbolic geometry. The authors
argued empirically and by some preliminary mathematical analysis that the
resulting graphs have many of the desired properties. Moreover, by computing
explicitly a maximum likelihood fit of the Internet graph, they demonstrated
impressively that this model is adequate for reproducing the structure of real
graphs with high accuracy.
  In this work we initiate the rigorous study of random hyperbolic graphs. We
compute exact asymptotic expressions for the expected number of vertices of
degree k for all k up to the maximum degree and provide small probabilities for
large deviations. We also prove a constant lower bound for the clustering
coefficient. In particular, our findings confirm rigorously that the degree
sequence follows a power-law distribution with controllable exponent and that
the clustering is nonvanishing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1473</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1473</id><created>2012-05-04</created><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Ku&#x10d;era</keyname><forenames>Anton&#xed;n</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Wojtczak</keyname><forenames>Dominik</forenames></author></authors><title>Minimizing Expected Termination Time in One-Counter Markov Decision
  Processes</title><categories>cs.FL cs.CC</categories><comments>35 pages, this is a full version of a paper accepted for publication
  in proceedings of ICALP 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the value and an optimal strategy for
minimizing the expected termination time in one-counter Markov decision
processes. Since the value may be irrational and an optimal strategy may be
rather complicated, we concentrate on the problems of approximating the value
up to a given error epsilon &gt; 0 and computing a finite representation of an
epsilon-optimal strategy. We show that these problems are solvable in
exponential time for a given configuration, and we also show that they are
computationally hard in the sense that a polynomial-time approximation
algorithm cannot exist unless P=NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1477</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1477</id><created>2012-05-07</created><authors><author><keyname>Buchbinder</keyname><forenames>Niv</forenames><affiliation>Seffi</affiliation></author><author><keyname>Joseph</keyname><affiliation>Seffi</affiliation></author><author><keyname>Naor</keyname></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author></authors><title>Approximation Algorithms for Online Weighted Rank Function Maximization
  under Matroid Constraints</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following online version of the submodular maximization problem
under a matroid constraint: We are given a set of elements over which a matroid
is defined. The goal is to incrementally choose a subset that remains
independent in the matroid over time. At each time, a new weighted rank
function of a different matroid (one per time) over the same elements is
presented; the algorithm can add a few elements to the incrementally
constructed set, and reaps a reward equal to the value of the new weighted rank
function on the current set. The goal of the algorithm as it builds this
independent set online is to maximize the sum of these (weighted rank) rewards.
As in regular online analysis, we compare the rewards of our online algorithm
to that of an offline optimum, namely a single independent set of the matroid
that maximizes the sum of the weighted rank rewards that arrive over time. This
problem is a natural extension of two well-studied streams of earlier work: the
first is on online set cover algorithms (in particular for the max coverage
version) while the second is on approximately maximizing submodular functions
under a matroid constraint.
  In this paper, we present the first randomized online algorithms for this
problem with poly-logarithmic competitive ratio. To do this, we employ the LP
formulation of a scaled reward version of the problem. Then we extend a
weighted-majority type update rule along with uncrossing properties of tight
sets in the matroid polytope to find an approximately optimal fractional LP
solution. We use the fractional solution values as probabilities for a online
randomized rounding algorithm. To show that our rounding produces a
sufficiently large reward independent set, we prove and use new covering
properties for randomly rounded fractional solutions in the matroid polytope
that may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1478</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1478</id><created>2012-04-24</created><authors><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author><author><keyname>Srinivasan</keyname><forenames>Srikanth</forenames></author></authors><title>A Tail Bound for Read-k Families of Functions</title><categories>cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a Chernoff-like large deviation bound on the sum of non-independent
random variables that have the following dependence structure. The variables
$Y_1,...,Y_r$ are arbitrary Boolean functions of independent random variables
$X_1,...,X_m$, modulo a restriction that every $X_i$ influences at most $k$ of
the variables $Y_1,...,Y_r$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1482</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1482</id><created>2012-05-07</created><updated>2012-11-01</updated><authors><author><keyname>Deledalle</keyname><forenames>Charles-Alban</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal</forenames><affiliation>GREYC</affiliation></author><author><keyname>Dossal</keyname><forenames>Charles</forenames><affiliation>IMB</affiliation></author></authors><title>Risk estimation for matrix recovery with spectral regularization</title><categories>math.OC cs.IT cs.LG math.IT math.ST stat.ML stat.TH</categories><comments>This version is an update of our original paper presented at
  ICML'2012 workshop on Sparsity, Dictionaries and Projections in Machine
  Learning and Signal Processing</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop an approach to recursively estimate the quadratic
risk for matrix recovery problems regularized with spectral functions. Toward
this end, in the spirit of the SURE theory, a key step is to compute the (weak)
derivative and divergence of a solution with respect to the observations. As
such a solution is not available in closed form, but rather through a proximal
splitting algorithm, we propose to recursively compute the divergence from the
sequence of iterates. A second challenge that we unlocked is the computation of
the (weak) derivative of the proximity operator of a spectral function. To show
the potential applicability of our approach, we exemplify it on a matrix
completion problem to objectively and automatically select the regularization
parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1483</identifier>
 <datestamp>2012-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1483</id><created>2012-05-07</created><authors><author><keyname>Maleki</keyname><forenames>Hamed</forenames></author><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Index Coding - An Interference Alignment Perspective</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The index coding problem is studied from an interference alignment
perspective, providing new results as well as new insights into, and
generalizations of, previously known results. An equivalence is established
between multiple unicast index coding where each message is desired by exactly
one receiver, and multiple groupcast index coding where a message can be
desired by multiple receivers, which settles the heretofore open question of
insufficiency of linear codes for the multiple unicast index coding problem by
equivalence with multiple groupcast settings where this question has previously
been answered. Necessary and sufficient conditions for the achievability of
rate half per message are shown to be a natural consequence of interference
alignment constraints, and generalizations to feasibility of rate
$\frac{1}{L+1}$ per message when each destination desires at least $L$
messages, are similarly obtained. Finally, capacity optimal solutions are
presented to a series of symmetric index coding problems inspired by the local
connectivity and local interference characteristics of wireless networks. The
solutions are based on vector linear coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1496</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1496</id><created>2012-05-07</created><updated>2012-05-08</updated><authors><author><keyname>Qian</keyname><forenames>Jing</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author><author><keyname>Zhao</keyname><forenames>Manqi</forenames></author></authors><title>Graph-based Learning with Unbalanced Clusters</title><categories>stat.ML cs.LG</categories><comments>21 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph construction is a crucial step in spectral clustering (SC) and
graph-based semi-supervised learning (SSL). Spectral methods applied on
standard graphs such as full-RBF, $\epsilon$-graphs and $k$-NN graphs can lead
to poor performance in the presence of proximal and unbalanced data. This is
because spectral methods based on minimizing RatioCut or normalized cut on
these graphs tend to put more importance on balancing cluster sizes over
reducing cut values. We propose a novel graph construction technique and show
that the RatioCut solution on this new graph is able to handle proximal and
unbalanced data. Our method is based on adaptively modulating the neighborhood
degrees in a $k$-NN graph, which tends to sparsify neighborhoods in low density
regions. Our method adapts to data with varying levels of unbalancedness and
can be naturally used for small cluster detection. We justify our ideas through
limit cut analysis. Unsupervised and semi-supervised experiments on synthetic
and real data sets demonstrate the superiority of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1505</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1505</id><created>2012-05-07</created><authors><author><keyname>Lacasa</keyname><forenames>Lucas</forenames></author><author><keyname>Tagliabue</keyname><forenames>Jacopo</forenames></author><author><keyname>Berdahl</keyname><forenames>Andrew</forenames></author></authors><title>Crossover phenomenon in the performance of an Internet search engine</title><categories>cs.IR physics.data-an</categories><comments>Working paper, comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore the ability of the Google search engine to find
results for random N-letter strings. These random strings, dense over the set
of possible N-letter words, address the existence of typos, acronyms, and other
words without semantic meaning. Interestingly, we find that the probability of
finding such strings sharply drops from one to zero at Nc = 6. The behavior of
such order parameter suggests the presence of a transition-like phenomenon in
the geometry of the search space. Furthermore, we define a susceptibility-like
parameter which reaches a maximum in the neighborhood, suggesting the presence
of criticality. We finally speculate on the possible connections to Ramsey
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1524</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1524</id><created>2012-05-07</created><updated>2013-06-14</updated><authors><author><keyname>Chen</keyname><forenames>Dan</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author></authors><title>Approximating Majority Depth</title><categories>cs.CG</categories><comments>9 pages; no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating the majority depth (Liu and Singh,
1993) of a point q with respect to an n-point set, S, by random sampling. At
the heart of this problem is a data structures question: How can we preprocess
a set of n lines so that we can quickly test whether a randomly selected vertex
in the arrangement of these lines is above or below the median level. We
describe a Monte-Carlo data structure for this problem that can be constructed
in O(nlog n) time, can answer queries O((log n)^{4/3}) expected time, and
answers correctly with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1552</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1552</id><created>2012-05-07</created><authors><author><keyname>Avci</keyname><forenames>Serhat Nazim</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Optimal Algorithms for Near-Hitless Network Restoration via Diversity
  Coding</title><categories>cs.NI</categories><comments>An old version of this paper is submitted to IEEE Globecom 2012
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diversity coding is a network restoration technique which offers near-hitless
restoration, while other state-of-the art techniques are signi?cantly slower.
Furthermore, the extra spare capacity requirement of diversity coding is
competitive with the others. Previously, we developed heuristic algorithms to
employ diversity coding structures in networks with arbitrary topology. This
paper presents two algorithms to solve the network design problems using
diversity coding in an optimal manner. The first technique pre-provisions
static traffic whereas the second technique carries out the dynamic
provisioning of the traffic on-demand. In both cases, diversity coding results
in smaller restoration time, simpler synchronization, and much reduced
signaling complexity than the existing techniques in the literature. A Mixed
Integer Programming (MIP) formulation and an algorithm based on Integer Linear
Programming (ILP) are developed for pre-provisioning and dynamic provisioning,
respectively. Simulation results indicate that diversity coding has
signi?cantly higher restoration speed than Shared Path Protection (SPP) and
p-cycle techniques. It requires more extra capacity than the p-cycle technique
and SPP. However, the increase in the total capacity is negligible compared to
the increase in the restoration speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1556</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1556</id><created>2012-05-07</created><authors><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>Jos&#xe9; Miguel</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author><author><keyname>Ventura</keyname><forenames>Inmaculada</forenames></author></authors><title>Locating a single facility and a high-speed line</title><categories>cs.CG</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a facility location problem in the plane in which a
single point (facility) and a rapid transit line (highway) are simultaneously
located in order to minimize the total travel time from the clients to the
facility, using the $L_1$ or Manhattan metric. The rapid transit line is given
by a segment with any length and orientation, and is an alternative
transportation line that can be used by the clients to reduce their travel time
to the facility. We study the variant of the problem in which clients can enter
and exit the highway at any point. We provide an $O(n^3)$-time algorithm that
solves this variant, where $n$ is the number of clients. We also present a
detailed characterization of the solutions, which depends on the speed given in
the highway.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1564</identifier>
 <datestamp>2013-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1564</id><created>2012-05-07</created><authors><author><keyname>Li</keyname><forenames>Wentian</forenames></author></authors><title>Characterizing Ranked Chinese Syllable-to-Character Mapping Spectrum: A
  Bridge Between the Spoken and Written Chinese Language</title><categories>cs.CL stat.AP</categories><comments>15 pages, 4 figures</comments><journal-ref>Journal of Quantitative Linguistics, 20, 153-167 (2013)</journal-ref><doi>10.1080/09296174.2013.773140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important aspect of the relationship between spoken and written Chinese
is the ranked syllable-to-character mapping spectrum, which is the ranked list
of syllables by the number of characters that map to the syllable. Previously,
this spectrum is analyzed for more than 400 syllables without distinguishing
the four intonations. In the current study, the spectrum with 1280 toned
syllables is analyzed by logarithmic function, Beta rank function, and
piecewise logarithmic function. Out of the three fitting functions, the
two-piece logarithmic function fits the data the best, both by the smallest sum
of squared errors (SSE) and by the lowest Akaike information criterion (AIC)
value. The Beta rank function is the close second. By sampling from a Poisson
distribution whose parameter value is chosen from the observed data, we
empirically estimate the $p$-value for testing the
two-piece-logarithmic-function being better than the Beta rank function
hypothesis, to be 0.16. For practical purposes, the piecewise logarithmic
function and the Beta rank function can be considered a tie.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1579</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1579</id><created>2012-05-07</created><updated>2012-06-28</updated><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author></authors><title>Anonymous Card Shuffling and its Applications to Parallel Mixnets</title><categories>cs.DS cs.CR cs.DC cs.NI</categories><comments>Full version of a paper appearing in ICALP 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the question of how to shuffle $n$ cards when faced with an opponent
who knows the initial position of all the cards {\em and} can track every card
when permuted, {\em except} when one takes $K&lt; n$ cards at a time and shuffles
them in a private buffer &quot;behind your back,&quot; which we call {\em buffer
shuffling}. The problem arises naturally in the context of parallel mixnet
servers as well as other security applications. Our analysis is based on
related analyses of load-balancing processes. We include extensions to
variations that involve corrupted servers and adversarially injected messages,
which correspond to an opponent who can peek at some shuffles in the buffer and
who can mark some number of the cards. In addition, our analysis makes novel
use of a sum-of-squares metric for anonymity, which leads to improved
performance bounds for parallel mixnets and can also be used to bound
well-known existing anonymity measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1580</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1580</id><created>2012-05-07</created><updated>2014-01-10</updated><authors><author><keyname>McCoy</keyname><forenames>Michael B.</forenames></author><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>Sharp recovery bounds for convex demixing, with applications</title><categories>cs.IT math.IT</categories><comments>51 pages, 13 figures, 2 tables. This version accepted to J. Found.
  Comput. Math</comments><msc-class>60D05, 52B55, 52A22 (primary) 94B75 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demixing refers to the challenge of identifying two structured signals given
only the sum of the two signals and prior information about their structures.
Examples include the problem of separating a signal that is sparse with respect
to one basis from a signal that is sparse with respect to a second basis, and
the problem of decomposing an observed matrix into a low-rank matrix plus a
sparse matrix. This paper describes and analyzes a framework, based on convex
optimization, for solving these demixing problems, and many others. This work
introduces a randomized signal model which ensures that the two structures are
incoherent, i.e., generically oriented. For an observation from this model,
this approach identifies a summary statistic that reflects the complexity of a
particular signal. The difficulty of separating two structured, incoherent
signals depends only on the total complexity of the two structures. Some
applications include (i) demixing two signals that are sparse in mutually
incoherent bases; (ii) decoding spread-spectrum transmissions in the presence
of impulsive errors; and (iii) removing sparse corruptions from a low-rank
matrix. In each case, the theoretical analysis of the convex demixing method
closely matches its empirical behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1587</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1587</id><created>2012-05-08</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author></authors><title>Testing Coverage Functions</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coverage function f over a ground set [m] is associated with a universe U
of weighted elements and m subsets A_1,..., A_m of U, and for any subset T of
[m], f(T) is defined as the total weight of the elements in the union
$\cup_{j\in T} A_j$. Coverage functions are an important special case of
submodular functions, and arise in many applications, for instance as a class
of utility functions of agents in combinatorial auctions.
  Set functions such as coverage functions often lack succinct representations,
and in algorithmic applications, an access to a value oracle is assumed. In
this paper, we ask whether one can test if a given oracle is that of a coverage
function or not. We demonstrate an algorithm which makes O(m|U|) queries to an
oracle of a coverage function and completely reconstructs it. This gives a
polytime tester for succinct coverage functions for which |U$ is polynomially
bounded in m. In contrast, we demonstrate a set function which is &quot;far&quot; from
coverage, but requires 2^{\tilde{\Theta}(m)} queries to distinguish it from the
class of coverage functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1600</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1600</id><created>2012-05-08</created><authors><author><keyname>Lim</keyname><forenames>Joanne Mun-Yee</forenames></author><author><keyname>Chow</keyname><forenames>Chee-Onn</forenames></author></authors><title>Smart handover based on fuzzy logic trend in IEEE802.11 mobile IPv6
  networks</title><categories>cs.NI</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A properly designed handoff algorithm is essential in reducing the connection
quality deterioration when a mobile node moves across the cell boundaries.
Therefore, to improve communication quality, we identified three goals in our
paper. The first goal is to minimize unnecessary handovers and increase
communication quality by reducing misrepresentations of RSSI readings due to
multipath and shadow effect with the use of additional parameters. The second
goal is to control the handover decisions depending on the users' mobility by
utilizing location factors as one of the input parameters in a fuzzy logic
handover algorithm. The third goal is to minimize false handover alarms caused
by sudden fluctuations of parameters by monitoring the trend of fuzzy logic
outputs for a period of time before making handover decision. In this paper, we
use RSSI, speed and distance as the input decision criteria of a handover
trigger algorithm by means of fuzzy logic. The fuzzy logic output trend is
monitored for a period of time before handover is triggered. Finally, through
simulations, we show the effectiveness of the proposed handover algorithm in
achieving better communication quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1602</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1602</id><created>2012-05-08</created><authors><author><keyname>Molijy</keyname><forenames>Abdulrahman Al</forenames></author><author><keyname>Hmeidi</keyname><forenames>Ismail</forenames></author><author><keyname>Alsmadi</keyname><forenames>Izzat</forenames></author></authors><title>Indexing of Arabic documents automatically based on lexical analysis</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continuous information explosion through the Internet and all information
sources makes it necessary to perform all information processing activities
automatically in quick and reliable manners. In this paper, we proposed and
implemented a method to automatically create and Index for books written in
Arabic language. The process depends largely on text summarization and
abstraction processes to collect main topics and statements in the book. The
process is developed in terms of accuracy and performance and results showed
that this process can effectively replace the effort of manually indexing books
and document, a process that can be very useful in all information processing
and retrieval applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1603</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1603</id><created>2012-05-08</created><authors><author><keyname>Thant</keyname><forenames>Win Win</forenames></author><author><keyname>Htwe</keyname><forenames>Tin Myat</forenames></author><author><keyname>Thein</keyname><forenames>Ni Lar</forenames></author></authors><title>Parsing of Myanmar sentences with function tagging</title><categories>cs.CL</categories><comments>18 pages, 8 figures, 10 tables. arXiv admin note: substantial text
  overlap with arXiv:1203.1685, and with arXiv:0912.1820 by other authors
  without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the use of Naive Bayes to address the task of assigning
function tags and context free grammar (CFG) to parse Myanmar sentences. Part
of the challenge of statistical function tagging for Myanmar sentences comes
from the fact that Myanmar has free-phrase-order and a complex morphological
system. Function tagging is a pre-processing step for parsing. In the task of
function tagging, we use the functional annotated corpus and tag Myanmar
sentences with correct segmentation, POS (part-of-speech) tagging and chunking
information. We propose Myanmar grammar rules and apply context free grammar
(CFG) to find out the parse tree of function tagged Myanmar sentences.
Experiments show that our analysis achieves a good result with parsing of
simple sentences and three types of complex sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1604</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1604</id><created>2012-05-08</created><authors><author><keyname>Arif</keyname><forenames>Mohammad</forenames></author><author><keyname>Rani</keyname><forenames>Tara</forenames></author></authors><title>ACO based routing for MANETs</title><categories>cs.NI</categories><comments>12 pages, 7 figures, ISSN:0975-3834 (Online); 0975-4679 (Print), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad hoc network (MANET) is a collection of wireless mobile nodes. It
dynamically forms a temporary network without using any pre existing network
infrastructure or centralized administration i.e. with minimal prior planning.
All nodes have routing capabilities and forward data packets to other nodes in
multi-hop fashion. As the network is dynamic, the network topology continuously
experiences alterations during deployment. The biggest challenge in MANETs is
to find a path between communicating nodes. The considerations of the MANET
environment and the nature of the mobile nodes create further complications
which results in the need to develop special routing algorithms to meet these
challenges. Swarm intelligence, a bio-inspired technique, which has proven to
be very adaptable in other problem domains, has been applied to the MANET
routing problem as it forms a good fit to the problem. In ant societies the
activities of the individuals are not regulated by any explicit form of
centralized control but are the result of self-organizing dynamics driven by
local interactions and communications among a number of relatively simple
individuals. This unique characteristic has made ant societies an attractive
and inspiring model for building new algorithms and new multi-agent systems. In
this paper, we have studied and reviewed Ant Colony based routing algorithms
and its variants. Finally, a performance evaluation of the original ARA and the
EARA is carried out with respect to each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1609</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1609</id><created>2012-05-08</created><authors><author><keyname>Pillai</keyname><forenames>Jyothi</forenames></author><author><keyname>Vyas</keyname><forenames>O. P.</forenames></author></authors><title>CSHURI - Modified HURI algorithm for Customer Segmentation and
  Transaction Profitability</title><categories>cs.DB</categories><comments>11 pages, 5 tables, 1 figure, IJCSEIT-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Association rule mining (ARM) is the process of generating rules based on the
correlation between the set of items that the customers purchase.Of late, data
mining researchers have improved upon the quality of association rule mining
for business development by incorporating factors like value (utility),
quantity of items sold (weight) and profit. The rules mined without considering
utility values (profit margin) will lead to a probable loss of profitable
rules. The advantage of wealth of the customers' needs information and rules
aids the retailer in designing his store layout[9]. An algorithm CSHURI,
Customer Segmentation using HURI, is proposed, a modified version of HURI [6],
finds customers who purchase high profitable rare items and accordingly
classify the customers based on some criteria; for example, a retail business
may need to identify valuable customers who are major contributors to a
company's overall profit. For a potential customer arriving in the store, which
customer group one should belong to according to customer needs, what are the
preferred functional features or products that the customer focuses on and what
kind of offers will satisfy the customer, etc., finds the key in targeting
customers to improve sales [9], which forms the base for customer utility
mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1618</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1618</id><created>2012-05-08</created><authors><author><keyname>Samad</keyname><forenames>Md Abdus</forenames></author></authors><title>A Novel Window Function Yielding Suppressed Mainlobe Width and Minimum
  Sidelobe Peak</title><categories>cs.OH</categories><comments>13 pages, 10 figures, AIRCC (International Journal of Computer
  Science, Engineering and Information Technology(IJCSEIT), Md Abdus Samad, &quot;A
  Novel Window Function Yielding Suppressed Mainlobe Width and Minimum Sidelobe
  Peak&quot; International Journal of Computer Science, Engineering and Information
  Technology (IJCSEIT), Vol.2, No.2, April 2012</comments><doi>10.5121/ijcseit.2012.2209</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications like FIR filters, FFT, signal processing and
measurements, we are required (~45 dB) or less side lobes amplitudes. However,
the problem is usual window based FIR filter design lies in its side lobes
amplitudes that are higher than the requirement of application. We propose a
window function, which has better performance like narrower main lobe width,
minimum side lobe peak compared to the several commonly used windows. The
proposed window has slightly larger main lobe width of the commonly used
Hamming window, while featuring 6.2\ sim 22.62 dB smaller side lobe peak. The
proposed window maintains its maximum side lobe peak about -58.4 \sim -52.6 dB
compared to -35.8 \sim -38.8 dB of Hamming window for M=10~14, while offering
roughly equal main lobe width. Our simulated results also show significant
performance upgrading of the proposed window compared to the Kaiser, Gaussian,
and Lanczos windows. The proposed window also shows better performance than
Dolph-Chebyshev window. Finally, the example of designed low pass FIR filter
confirms the efficiency of the proposed window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1621</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1621</id><created>2012-05-08</created><authors><author><keyname>Zhang</keyname><forenames>Jian Yuan Wen-Xia</forenames></author><author><keyname>Zhou</keyname><forenames>Zhou-Hai</forenames></author></authors><title>An optimal consensus tracking control algorithm for autonomous
  underwater vehicles with disturbances</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The optimal disturbance rejection control problem is considered for consensus
tracking systems affected by external persistent disturbances and noise.
Optimal estimated values of system states are obtained by recursive filtering
for the multiple autonomous underwater vehicles modeled to multi-agent systems
with Kalman filter. Then the feedforward-feedback optimal control law is
deduced by solving the Riccati equations and matrix equations. The existence
and uniqueness condition of feedforward-feedback optimal control law is
proposed and the optimal control law algorithm is carried out. Lastly,
simulations show the result is effectiveness with respect to external
persistent disturbances and noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1622</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1622</id><created>2012-05-08</created><authors><author><keyname>Suakanto</keyname><forenames>Sinung</forenames></author><author><keyname>Supangkat</keyname><forenames>Suhono H</forenames></author><author><keyname>Suhardi</keyname></author><author><keyname>Saragih</keyname><forenames>Roberd</forenames></author></authors><title>Performance Measurement of Cloud Computing Services</title><categories>cs.NI cs.DC cs.PF</categories><comments>It was published at International Journal on Cloud Computing:
  Services and Architecture (IJCCSA), April 2012, Volume 2, Number 2
  http://airccse.org/journal/ijccsa/current2012.html
  http://airccse.org/journal/ijccsa/papers/2212ijccsa02.pdf</comments><journal-ref>http://airccse.org/journal/ijccsa/current2012.html
  http://airccse.org/journal/ijccsa/papers/2212ijccsa02.pdf</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing today has now been growing as new technologies and new
business models. In distributed technology perspective, cloud computing most
like client-server services like web-based or web-service but it used virtual
resources to execute. Currently, cloud computing relies on the use of an
elastic virtual machine and the use of network for data exchange. We conduct an
experimental setup to measure the quality of service received by cloud
computing customers. Experimental setup done by creating a HTTP service that
runs in the cloud computing infrastructure. We interest to know about the
impact of increasing the number of users on the average quality received by
users. The qualities received by user measured within two parameters consist of
average response times and the number of requests time out. Experimental
results of this study show that increasing the number of users has increased
the average response time. Similarly, the number of request time out increasing
with increasing number of users. It means that the qualities of service
received by user are decreasing also. We found that the impact of the number of
users on the quality of service is no longer in linear trend. The results of
this study can be used as a reference model for the network operator in
performing services in which a certain number of users in order to obtain
optimal quality services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1628</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1628</id><created>2012-05-08</created><authors><author><keyname>Rybski</keyname><forenames>Diego</forenames></author><author><keyname>Buldyrev</keyname><forenames>Sergey V.</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>Communication activity in a social network: relation between long-term
  correlations and inter-event clustering</title><categories>physics.soc-ph cs.SI</categories><comments>26 pages, 7 figures</comments><journal-ref>Nature Scientific Reports 2, 560 (2012)</journal-ref><doi>10.1038/srep00560</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The timing patterns of human communication in social networks is not random.
On the contrary, communication is dominated by emergent statistical laws such
as non-trivial correlations and clustering. Recently, we found long-term
correlations in the user's activity in social communities. Here, we extend this
work to study collective behavior of the whole community. The goal is to
understand the origin of clustering and long-term persistence. At the
individual level, we find that the correlations in activity are a byproduct of
the clustering expressed in the power-law distribution of inter-event times of
single users. On the contrary, the activity of the whole community presents
long-term correlations that are a true emergent property of the system, i.e.
they are not related to the distribution of inter-event times. This result
suggests the existence of collective behavior, possible arising from nontrivial
communication patterns through the embedding social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1630</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1630</id><created>2012-05-08</created><authors><author><keyname>Abri</keyname><forenames>Karima Ben Hamida El</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ammar</forenames></author></authors><title>A New UWB System Based on a Frequency Domain Transformation Of The
  Received Signal</title><categories>cs.IT math.IT</categories><comments>14 pages,5 figures, journal paper</comments><journal-ref>Karima Ben Hamida El Abri and Ammar Bouallegue,&quot;A New UWB System
  Based on a Frequency Domain Transformation Of The Received Signal&quot;, IJWMN,
  april 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential system for ultra wide band (UWB) transmission is a very
attractive solution from a practical point of view. In this paper, we present a
new direct sequence (DS) UWB system based on the conversion of the received
signal from time domain to frequency domain that's why we called FDR receiver.
Simulation results show that the proposed receiver structure outperforms the
classical differential one for both low and high data rate systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1632</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1632</id><created>2012-05-08</created><authors><author><keyname>Nidhra</keyname><forenames>Srinivas</forenames></author><author><keyname>Poovanna</keyname><forenames>Likith</forenames></author><author><keyname>Ethiraj</keyname><forenames>Vinay Sudha</forenames></author></authors><title>Visitor schedule management system- an intelligent decision support
  system</title><categories>cs.OH</categories><comments>20, International Journal on Cybernetics &amp; Informatics (IJCI) Vol.1,
  No.2, April 2012</comments><doi>10.5121/ijci.2012.1201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Travelling salesman problem is a problem which is of high interest for
researchers, industry professionals, and academicians. Visitor or salesman used
to face lot of problems with respect to scheduling based on meeting top ranked
clients. Even excel sheet made the work tedious. So these flaws propelled us to
design an intelligent decision support system. This paper reports the problem
definition we tried to address and possible solution to this problem. We even
explained the project design and implementation of our visitor schedule
management system.. Our system made a major contribution in terms of valuable
resources such as time and satisfying high ranked clients efficiently. We used
optimization via mathematical programming to solve these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1633</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1633</id><created>2012-05-08</created><authors><author><keyname>Mohamed</keyname><forenames>Samir A. Elsagheer</forenames></author><author><keyname>Nasr</keyname><forenames>A.</forenames></author><author><keyname>Ansari</keyname><forenames>Gufran Ahmad</forenames></author></authors><title>Precise positioning systems for Vehicular Ad-Hoc Networks</title><categories>cs.NI</categories><comments>15 pages, 15 figures, International Journal of Wireless &amp; Mobile
  Networks (IJWMN) Vol. 4, No. 2, April 2012</comments><doi>10.5121/ijwmn.2012.4217</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad Hoc Networks (VANET) is a very promising research venue that can
offers many useful and critical applications including the safety applications.
Most of these applications require that each vehicle knows precisely its
current position in real time. GPS is the most common positioning technique for
VANET. However, it is not accurate. Moreover, the GPS signals cannot be
received in the tunnels, undergrounds, or near tall buildings. Thus, no
positioning service can be obtained in these locations. Even if the Deferential
GPS (DGPS) can provide high accuracy, but still no GPS converge in these
locations. In this paper, we provide positioning techniques for VANET that can
provide accurate positioning service in the areas where GPS signals are
hindered by the obstacles. Experimental results show significant improvement in
the accuracy. This allows when combined with DGPS the continuity of a precise
positioning service that can be used by most of the VANET applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1638</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1638</id><created>2012-05-08</created><authors><author><keyname>S</keyname><forenames>Aji</forenames></author><author><keyname>Kaimal</keyname><forenames>Ramachandra</forenames></author></authors><title>Document summarization using positive pointwise mutual information</title><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degree of success in document summarization processes depends on the
performance of the method used in identifying significant sentences in the
documents. The collection of unique words characterizes the major signature of
the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive
Pointwise Mutual Information, which works well for measuring semantic
similarity in the Term-Sentence-Matrix, is used in our method to assign weights
for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated
from this weighted TSM, is then used to extract a summary from the document.
Our experiments show that such a method would outperform most of the existing
methods in producing summaries from large documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1639</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1639</id><created>2012-05-08</created><authors><author><keyname>Divakaran</keyname><forenames>Sajilal</forenames></author></authors><title>Spectral Analysis of Projection Histogram for Enhancing Close matching
  character Recognition in Malayalam</title><categories>cs.CL cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success rates of Optical Character Recognition (OCR) systems for printed
Malayalam documents is quite impressive with the state of the art accuracy
levels in the range of 85-95% for various. However for real applications,
further enhancement of this accuracy levels are required. One of the bottle
necks in further enhancement of the accuracy is identified as close-matching
characters. In this paper, we delineate the close matching characters in
Malayalam and report the development of a specialised classifier for these
close-matching characters. The output of a state of the art of OCR is taken and
characters falling into the close-matching character set is further fed into
this specialised classifier for enhancing the accuracy. The classifier is based
on support vector machine algorithm and uses feature vectors derived out of
spectral coefficients of projection histogram signals of close-matching
characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1641</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1641</id><created>2012-05-08</created><authors><author><keyname>Patel</keyname><forenames>B V</forenames></author><author><keyname>Meshram</keyname><forenames>B B</forenames></author></authors><title>Content based video retrieval systems</title><categories>cs.MM</categories><comments>18 Pages</comments><doi>10.5121/iju.2012.3202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of multimedia data types and available bandwidth there
is huge demand of video retrieval systems, as users shift from text based
retrieval systems to content based retrieval systems. Selection of extracted
features play an important role in content based video retrieval regardless of
video attributes being under consideration. These features are intended for
selecting, indexing and ranking according to their potential interest to the
user. Good features selection also allows the time and space costs of the
retrieval process to be reduced. This survey reviews the interesting features
that can be extracted from video data for indexing and retrieval along with
similarity measurement methods. We also identify present research issues in
area of content based video retrieval systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1642</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1642</id><created>2012-05-08</created><authors><author><keyname>Texier</keyname><forenames>Jose</forenames></author><author><keyname>Manuel</keyname><forenames>Bermudez</forenames></author></authors><title>Traductor Writing System Web</title><categories>cs.OH</categories><comments>LACCEI 2008, ISBN-10 0-9822896-1-8, ISBN-13 978-0-9822896-1-7</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A compilator is a program which is development in a programming language that
read a file known as source. After this file have to translate and have to
convert in other program known as object or to generate a exit. The best way
for to know any programming language is analizing a compilation process which
is same in all programming paradigm existents. To like to generate a tool that
permit a learning in university course. This course could explain in any
plataform such as Linux o Windows. This goal is posible through development a
Web aplication which is unite with a compilator, it is Traductor Writing System
(Sistema de Escritura de Traductores). This system is complete and permit
extend and modify the compilator. The system is a module in Moodle which is a
Course Management System (CMS) that help teachers for to create comunities of
learning in line. This software is in free software license (GPL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1643</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1643</id><created>2012-05-08</created><authors><author><keyname>Farrokhi</keyname><forenames>Vahid</forenames></author><author><keyname>Pokoradi</keyname><forenames>Laszlo</forenames></author></authors><title>The necessities for building a model to evaluate Business Intelligence
  projects- Literature Review</title><categories>cs.OH</categories><comments>International Journal of Computer Science &amp; Engineering Survey
  (IJCSES) Vol.3, No.2, April 2012</comments><doi>10.5121/ijcses.2012.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years Business Intelligence (BI) systems have consistently been
rated as one of the highest priorities of Information Systems (IS) and business
leaders. BI allows firms to apply information for supporting their processes
and decisions by combining its capabilities in both of organizational and
technical issues. Many of companies are being spent a significant portion of
its IT budgets on business intelligence and related technology. Evaluation of
BI readiness is vital because it serves two important goals. First, it shows
gaps areas where company is not ready to proceed with its BI efforts. By
identifying BI readiness gaps, we can avoid wasting time and resources. Second,
the evaluation guides us what we need to close the gaps and implement BI with a
high probability of success. This paper proposes to present an overview of BI
and necessities for evaluation of readiness. Key words: Business intelligence,
Evaluation, Success, Readiness
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1644</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1644</id><created>2012-05-08</created><authors><author><keyname>Jagadeesh</keyname><forenames>H S</forenames></author><author><keyname>Babu</keyname><forenames>K Suresh</forenames></author><author><keyname>Raja</keyname><forenames>K B</forenames></author></authors><title>DBC based Face Recognition using DWT</title><categories>cs.CV</categories><comments>15 pages,9 figures, 4 tables</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.3, No.2, April 2012</journal-ref><doi>10.5121/sipij.2012.3208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The applications using face biometric has proved its reliability in last
decade. In this paper, we propose DBC based Face Recognition using DWT (DBC-
FR) model. The Poly-U Near Infra Red (NIR) database images are scanned and
cropped to get only the face part in pre-processing. The face part is resized
to 100*100 and DWT is applied to derive LL, LH, HL and HH subbands. The LL
subband of size 50*50 is converted into 100 cells with 5*5 dimention of each
cell. The Directional Binary Code (DBC) is applied on each 5*5 cell to derive
100 features. The Euclidian distance measure is used to compare the features of
test image and database images. The proposed algorithm render better percentage
recognition rate compared to the existing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1645</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1645</id><created>2012-05-08</created><authors><author><keyname>Plu</keyname><forenames>Julien</forenames></author><author><keyname>Scharffe</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Publishing and linking transport data on the Web</title><categories>cs.AI</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/13</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Without Linked Data, transport data is limited to applications exclusively
around transport. In this paper, we present a workflow for publishing and
linking transport data on the Web. So we will be able to develop transport
applications and to add other features which will be created from other
datasets. This will be possible because transport data will be linked to these
datasets. We apply this workflow to two datasets: NEPTUNE, a French standard
describing a transport line, and Passim, a directory containing relevant
information on transport services, in every French city.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1646</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1646</id><created>2012-05-08</created><authors><author><keyname>Okano</keyname><forenames>Keiji</forenames></author></authors><title>On conditions for \rho-value is 1 or not of complete family of
  pairing-friendly elliptic curves</title><categories>math.NT cs.CR</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study whether a complete family of pairing friendly elliptic curves has a
\rho-value 1 or not. We show that, in some cases, \rho-values are not to be 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1648</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1648</id><created>2012-05-08</created><authors><author><keyname>T</keyname><forenames>Manu V</forenames></author><author><keyname>Simon</keyname><forenames>Philomina</forenames></author></authors><title>A novel statistical fusion rule for image fusion and its comparison in
  non subsampled contourlet transform domain and wavelet domain</title><categories>cs.CV math.ST stat.TH</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image fusion produces a single fused image from a set of input images. A new
method for image fusion is proposed based on Weighted Average Merging Method
(WAMM) in the NonSubsampled Contourlet Transform (NSCT) domain. A performance
analysis on various statistical fusion rules are also analysed both in NSCT and
Wavelet domain. Analysis has been made on medical images, remote sensing images
and multi focus images. Experimental results shows that the proposed method,
WAMM obtained better results in NSCT domain than the wavelet domain as it
preserves more edges and keeps the visual quality intact in the fused image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1649</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1649</id><created>2012-05-08</created><authors><author><keyname>Aslam</keyname><forenames>Muhammad Shahzad</forenames></author></authors><title>The impact of pharmacybernetic in reducing medication error</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Doctors and Pharmacists play a foremost role in safe, effective use of
medication in health care. Still, there is no database available through which
Doctor can communicate with all field of pharmacy such as hospital Pharmacy,
Clinical Pharmacy, Community Pharmacy, Nutrition Pharmacy and Drug research
center so that they would like to cooperate with pharmacists in Medication
error prevention, Drug-Disease management, Nutrition management, and
pharmacotherapy. The authors examined the comprehensive project of implementing
Electronic Drug Information Record (EDIR), introduce the new term
Pharmacybernetic and how to reduce the medication error by integrated
management system (IMS). This paper presented EDIR conceptual model and the
flow sheet of the Pharmacybernetic system, which describes the integration of
different Pharmaceutical related aspect in the field of Cybernetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1650</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1650</id><created>2012-05-08</created><authors><author><keyname>Blumensath</keyname><forenames>Thomas</forenames></author></authors><title>Compressed Sensing with Nonlinear Observations and Related Nonlinear
  Optimisation Problems</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-convex constraints have recently proven a valuable tool in many
optimisation problems. In particular sparsity constraints have had a
significant impact on sampling theory, where they are used in Compressed
Sensing and allow structured signals to be sampled far below the rate
traditionally prescribed.
  Nearly all of the theory developed for Compressed Sensing signal recovery
assumes that samples are taken using linear measurements. In this paper we
instead address the Compressed Sensing recovery problem in a setting where the
observations are non-linear. We show that, under conditions similar to those
required in the linear setting, the Iterative Hard Thresholding algorithm can
be used to accurately recover sparse or structured signals from few non-linear
observations.
  Similar ideas can also be developed in a more general non-linear optimisation
framework. In the second part of this paper we therefore present related result
that show how this can be done under sparsity and union of subspaces
constraints, whenever a generalisation of the Restricted Isometry Property
traditionally imposed on the Compressed Sensing system holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1657</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1657</id><created>2012-05-08</created><authors><author><keyname>Heni</keyname><forenames>Maher</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Power control in reactive routing protocol for Mobile Ad Hoc Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to change the routing strategy of AODV protocol (Ad
hoc On Demand Vector) in order to improve the energy consumption in mobile ad
hoc networks (MANET). The purpose is to minimize the regular period of HELLO
messages generated by the AODV protocol used for the research, development and
maintenance of routes. This information is useful to have an idea about battery
power levels of different network hosts. After storing this information, the
node elect the shortest path following the classical model used this
information to elect safest path (make a compromise) in terms of energy.
Transmitter node does not select another node as its battery will be exhausted
soon. Any node of the network can have the same information's about the
neighborhoods as well as other information about the energy level of the
different terminal to avoid routing using a link that will be lost due to an
exhausted battery of a node in this link. Analytical study and simulations by
Jist/SWANS have been conducted to note that no divergence relatively to the
classical AODV, a node can have this type of information that improves the
energy efficiency in ad hoc networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1668</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1668</id><created>2012-05-08</created><authors><author><keyname>Ramesh</keyname><forenames>K.</forenames></author><author><keyname>Somasundaram</keyname><forenames>Dr. K.</forenames></author></authors><title>Improved Fair-Zone technique using Mobility Prediction in WSN</title><categories>cs.NI cs.DC</categories><comments>10 pages, 7 figures, Published in International Journal Of Advanced
  Smart Sensor Network Systems (IJASSN)</comments><msc-class>00B50</msc-class><acm-class>C.2.1; C.2.3; C.2.4</acm-class><journal-ref>International Journal Of Advanced Smart Sensor Network Systems (
  IJASSN ), Vol 2, No.2, April 2012</journal-ref><doi>10.5121/ijassn.2012.2203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The self-organizational ability of ad-hoc Wireless Sensor Networks (WSNs) has
led them to be the most popular choice in ubiquitous computing. Clustering
sensor nodes organizing them hierarchically have proven to be an effective
method to provide better data aggregation and scalability for the sensor
network while conserving limited energy. It has some limitation in energy and
mobility of nodes. In this paper we propose a mobility prediction technique
which tries overcoming above mentioned problems and improves the life time of
the network. The technique used here is Exponential Moving Average for online
updates of nodal contact probability in cluster based network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1670</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1670</id><created>2012-05-08</created><authors><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author></authors><title>Rainbow Colouring of Split and Threshold Graphs</title><categories>cs.DM cs.CC cs.DS math.CO</categories><comments>15 pages, 3 figures, accepted for presentation at the 18th Annual
  International Computing and Combinatorics Conference (COCOON 2012)</comments><msc-class>O5C15, 05C85 (Primary), 05C40 (Secondary)</msc-class><acm-class>G.2.2; F.2.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A rainbow colouring of a connected graph is a colouring of the edges of the
graph, such that every pair of vertices is connected by at least one path in
which no two edges are coloured the same. Such a colouring using minimum
possible number of colours is called an optimal rainbow colouring, and the
minimum number of colours required is called the rainbow connection number of
the graph. In this article, we show the following:
  1. The problem of deciding whether a graph can be rainbow coloured using 3
colours remains NP-complete even when restricted to the class of split graphs.
However, any split graph can be rainbow coloured in linear time using at most
one more colour than the optimum.
  2. For every integer k larger than 2, the problem of deciding whether a graph
can be rainbow coloured using k colours remains NP-complete even when
restricted to the class of chordal graphs.
  3. For every positive integer k, threshold graphs with rainbow connection
number k can be characterised based on their degree sequence alone. Further, we
can optimally rainbow colour a threshold graph in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1671</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1671</id><created>2012-05-08</created><authors><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Submodular Inference of Diffusion Networks from Multiple Trees</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>To appear in the 29th International Conference on Machine Learning
  (ICML), 2012. Website:
  http://www.stanford.edu/~manuelgr/network-inference-multitree/</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion and propagation of information, influence and diseases take place
over increasingly larger networks. We observe when a node copies information,
makes a decision or becomes infected but networks are often hidden or
unobserved. Since networks are highly dynamic, changing and growing rapidly, we
only observe a relatively small set of cascades before a network changes
significantly. Scalable network inference based on a small cascade set is then
necessary for understanding the rapidly evolving dynamics that govern
diffusion. In this article, we develop a scalable approximation algorithm with
provable near-optimal performance based on submodular maximization which
achieves a high accuracy in such scenario, solving an open problem first
introduced by Gomez-Rodriguez et al (2010). Experiments on synthetic and real
diffusion data show that our algorithm in practice achieves an optimal
trade-off between accuracy and running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1672</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1672</id><created>2012-05-08</created><authors><author><keyname>Cocco</keyname><forenames>Giuseppe</forenames></author><author><keyname>Alagha</keyname><forenames>Nader</forenames></author><author><keyname>Ibars</keyname><forenames>Christian</forenames></author><author><keyname>Cioni</keyname><forenames>Stefano</forenames></author></authors><title>A Network-Coded Diversity Protocol for Collision Recovery in Slotted
  ALOHA Networks</title><categories>cs.NI</categories><comments>Submitted to IEEE Journal on Selected Areas in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a collision recovery scheme for symbol-synchronous slotted ALOHA
(SA) based on physical layer network coding over extended Galois Fields.
Information is extracted from colliding bursts allowing to achieve higher
maximum throughput with respect to previously proposed collision recovery
schemes. An energy analysis is also performed, and it is shown that, by
adjusting the transmission probability, high energy efficiency can be achieved.
The paper also addresses several practical aspects, namely frequency, phase,
and amplitude estimation, as well as partial symbol asynchronism. A performance
evaluation is carried out using the proposed algorithms, revealing remarkable
performance in terms of normalized throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1673</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1673</id><created>2012-05-08</created><authors><author><keyname>Ramesh</keyname><forenames>K.</forenames></author><author><keyname>Somasundaram</keyname><forenames>Dr. K.</forenames></author></authors><title>A comparative study of clusterhead selection algorithms in wireless
  sensor networks</title><categories>cs.NI</categories><comments>12 pages, 3 figures, 5 tables, Int JournaL, International Journal of
  Computer Science &amp; Engineering Survey (IJCSES) Vol.2, No.4, November 2011</comments><msc-class>00B55</msc-class><acm-class>D.4.4; D.4.7; D.4.8; H.2.4</acm-class><doi>10.5121/ijcses.2011.2411</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In Wireless Sensor Network, sensor nodes life time is the most critical
parameter. Many researches on these lifetime extension are motivated by LEACH
scheme, which by allowing rotation of cluster head role among the sensor nodes
tries to distribute the energy consumption over all nodes in the network.
Selection of clusterhead for such rotation greatly affects the energy
efficiency of the network. Different communication protocols and algorithms are
investigated to find ways to reduce power consumption. In this paper brief
survey is taken from many proposals, which suggests different clusterhead
selection strategies and a global view is presented. Comparison of their costs
of clusterhead selection in different rounds, transmission method and other
effects like cluster formation, distribution of clusterheads and creation of
clusters shows a need of a combined strategy for better results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1682</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1682</id><created>2012-05-08</created><authors><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Influence Maximization in Continuous Time Diffusion Networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>To appear in the 29th International Conference on Machine Learning
  (ICML), 2012. Website: http://www.stanford.edu/~manuelgr/influmax/</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding the optimal set of source nodes in a diffusion network
that maximizes the spread of information, influence, and diseases in a limited
amount of time depends dramatically on the underlying temporal dynamics of the
network. However, this still remains largely unexplored to date. To this end,
given a network and its temporal dynamics, we first describe how continuous
time Markov chains allow us to analytically compute the average total number of
nodes reached by a diffusion process starting in a set of source nodes. We then
show that selecting the set of most influential source nodes in the continuous
time influence maximization problem is NP-hard and develop an efficient
approximation algorithm with provable near-optimal performance. Experiments on
synthetic and real diffusion networks show that our algorithm outperforms other
state of the art algorithms by at least ~20% and is robust across different
network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1687</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1687</id><created>2012-05-08</created><authors><author><keyname>Sharma</keyname><forenames>Sumit</forenames></author><author><keyname>Sharma</keyname><forenames>Rohitt</forenames></author><author><keyname>Singh</keyname><forenames>Paramjit</forenames></author><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author></authors><title>Age Based User Interface in Mobile Operating System</title><categories>cs.OS</categories><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) 2,2: 177-184 April 2012</journal-ref><doi>10.5121/ijcsea.2012.2215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the creation of different interfaces in the mobile
operating system for different age groups. The different age groups identified
are kids, elderly people and all others. The motive behind creating different
interfaces is to make the smartphones of today's world usable to all age
groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1690</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1690</id><created>2012-05-08</created><updated>2013-01-09</updated><authors><author><keyname>Umeno</keyname><forenames>Ken</forenames></author><author><keyname>Sato</keyname><forenames>Aki-Hiro</forenames></author></authors><title>Chaotic Method for Generating q-Gaussian Random Variables</title><categories>cs.IT cond-mat.stat-mech math.IT nlin.CD</categories><comments>24 pages, 32 figures, and 2 tables. This article is accepted for
  publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes a pseudo random number generator of q-Gaussian random
variables for a range of q values, -infinity &lt; q &lt; 3, based on deterministic
chaotic map dynamics. Our method consists of chaotic maps on the unit circle
and map dynamics based on the piecewise linear map. We perform the q-Gaussian
random number generator for several values of q and conduct both
Kolmogorov-Smirnov (KS) and Anderson-Darling (AD) tests. The q-Gaussian samples
generated by our proposed method pass the KS test at more than 5% significance
level for values of q ranging from -1.0 to 2.7, while they pass the AD test at
more than 5% significance level for q ranging from -1 to 2.4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1701</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1701</id><created>2012-05-08</created><authors><author><keyname>singh</keyname><forenames>Himanshu</forenames></author><author><keyname>Biswas</keyname><forenames>Bhaskar</forenames></author></authors><title>Comparison of CSMA based MAC protocols of wireless sensor networks</title><categories>cs.NI</categories><comments>International Journal of AdHoc Network Systems, Volume 2, Number 2,
  April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy conservation has been an important area of interest in Wireless Sensor
networks (WSNs). Medium Access Control (MAC) protocols play an important role
in energy conservation. In this paper, we describe CSMA based MAC protocols for
WSN and analyze the simulation results of these protocols. We implemented
S-MAC, T-MAC, B-MAC, B-MAC+, X-MAC, DMAC and Wise-MAC in TOSSIM, a simulator
which unlike other simulators simulates the same code running on real hardware.
Previous surveys mainly focused on the classification of MAC protocols
according to the techniques being used or problem dealt with and presented a
theoretical evaluation of protocols. This paper presents the comparative study
of CSMA based protocols for WSNs, showing which MAC protocol is suitable in a
particular environment and supports the arguments with the simulation results.
The comparative study can be used to find the best suited MAC protocol for
wireless sensor networks in different environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1712</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1712</id><created>2012-05-08</created><updated>2012-06-20</updated><authors><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author><author><keyname>Warsi</keyname><forenames>Naqueeb Ahmad</forenames></author></authors><title>On the strong converses for the quantum channel capacity theorems</title><categories>quant-ph cs.IT math.IT</categories><comments>Added the strong converse for the erasure channel for maximally
  entangled inputs and corrected minor typos</comments><doi>10.1103/PhysRevLett.110.080501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A unified approach to prove the converses for the quantum channel capacity
theorems is presented. These converses include the strong converse theorems for
classical or quantum information transfer with error exponents and novel
explicit upper bounds on the fidelity measures reminiscent of the Wolfowitz
strong converse for the classical channel capacity theorems. We provide a new
proof for the error exponents for the classical information transfer. A long
standing problem in quantum information theory has been to find out the strong
converse for the channel capacity theorem when quantum information is sent
across the channel. We give the quantum error exponent thereby giving a
one-shot exponential upper bound on the fidelity. We then apply our results to
show that the strong converse holds for the quantum information transfer across
an erasure channel for maximally entangled channel inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1720</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1720</id><created>2012-05-08</created><updated>2012-05-15</updated><authors><author><keyname>Pan</keyname><forenames>Wei</forenames></author><author><keyname>Yuan</keyname><forenames>Ye</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author></authors><title>Reconstruction of Arbitrary Biochemical Reaction Networks: A Compressive
  Sensing Approach</title><categories>cs.SY physics.bio-ph</categories><comments>Submitted to IEEE Conference on Decision and Control, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction of biochemical reaction networks is a central topic in systems
biology which raises crucial theoretical challenges in system identification.
Nonlinear Ordinary Differential Equations (ODEs) that involve polynomial and
rational functions are typically used to model biochemical reaction networks.
Such nonlinear models make the problem of determining the connectivity of
biochemical networks from time-series experimental data quite difficult. In
this paper, we present a network reconstruction algorithm that can deal with
model descriptions under the form of polynomial and rational functions. Rather
than identifying the parameters of linear or nonlinear ODEs characterised by
pre-defined equation structures, our methodology allows us to determine the
nonlinear ODEs structure together with their associated reaction constants. To
solve the network reconstruction problem, we cast it as a Compressive Sensing
(CS) problem and use Bayesian Sparse Learning (BSL) algorithms as an efficient
way to obtain its solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1721</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1721</id><created>2012-05-08</created><authors><author><keyname>Costello</keyname><forenames>Kevin</forenames></author><author><keyname>Tetali</keyname><forenames>Prasad</forenames></author><author><keyname>Tripathi</keyname><forenames>Pushkar</forenames></author></authors><title>Matching with Commitments</title><categories>cs.DS cs.DM cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following stochastic optimization problem first introduced by
Chen et al. in \cite{chen}. We are given a vertex set of a random graph where
each possible edge is present with probability p_e. We do not know which edges
are actually present unless we scan/probe an edge. However whenever we probe an
edge and find it to be present, we are constrained to picking the edge and both
its end points are deleted from the graph. We wish to find the maximum matching
in this model. We compare our results against the optimal omniscient algorithm
that knows the edges of the graph and present a 0.573 factor algorithm using a
novel sampling technique. We also prove that no algorithm can attain a factor
better than 0.898 in this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1731</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1731</id><created>2012-05-08</created><authors><author><keyname>Fanous</keyname><forenames>Anthony</forenames></author><author><keyname>Ephremides</keyname><forenames>Anthony</forenames></author></authors><title>Stable Throughput in a Cognitive Wireless Network</title><categories>cs.IT math.IT</categories><comments>37 Pages with 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study, from a network layer perspective, the effect of an Ad-Hoc secondary
network with N nodes randomly accessing the spectrum licensed to a primary node
during the idle slots of the primary user. If the sensing is perfect, then the
secondary nodes do not interfere with the primary node and hence do not affect
its stable throughput. In case of imperfect sensing, it is shown that if the
primary user's arrival rate is less than some calculated finite value,
cognitive nodes can employ any transmission power or probabilities without
affecting the primary user's stability; otherwise, the secondary nodes should
control their transmission parameters to reduce the interference on the
primary. It is also shown that in contrast with the primary's maximum stable
throughput which strictly decreases with increased sensing errors, the
throughput of the secondary nodes might increase with sensing errors as more
transmission opportunities become available to them. Finally, we explore the
use of the secondary nodes as relays of the primary node's traffic to
compensate for the interference they might cause. We introduce a relaying
protocol based on distributed space-time coding that forces all the secondary
nodes that are able to decode a primary's unsuccessful packet to relay that
packet whenever the primary is idle. In this case, for appropriate modulation
scheme and under perfect sensing, it is shown that the more secondary nodes in
the system, the better for the primary user in terms of his stable throughput.
Meanwhile, the secondary nodes might benefit from relaying by having access to
a larger number of idle slots due to the increase of the service rate of the
primary. For the case of a single secondary node, the proposed relaying
protocol guarantees that either both the primary and the secondary benefit from
relaying or none of them does.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1733</identifier>
 <datestamp>2012-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1733</id><created>2012-05-08</created><updated>2012-09-05</updated><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Finite-time and Asymptotic Convergence of Distributed Averaging and
  Maximizing Algorithms</title><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formulate and investigate a generalized consensus algorithm
which makes an attempt to unify distributed averaging and maximizing algorithms
considered in the literature. Each node iteratively updates its state as a
time-varying weighted average of its own state, the minimal state, and the
maximal state of its neighbors. We prove that finite-time consensus is almost
impossible for averaging under this uniform model. Both time-dependent and
state-dependent graphs are considered, and various necessary and/or sufficient
conditions are presented on the consensus convergence. For time-dependent
graphs, we show that quasi-strong connectivity is critical for averaging, as is
strong connectivity for maximizing. For state-dependent graphs defined by a
$\mu$-nearest-neighbor rule, where each node interacts with its $\mu$ nearest
smaller neighbors and the $\mu$ nearest larger neighbors, we show that $\mu+1$
is a critical threshold on the total number of nodes for the transit from
finite-time to asymptotic convergence for averaging, in the absence of node
self-confidence. The threshold is $2\mu$ if each node chooses to connect only
to neighbors with unique values. Numerical examples illustrate the tightness of
the conditions. The results characterize some fundamental similarities and
differences between distributed averaging and maximizing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1737</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1737</id><created>2012-05-08</created><updated>2012-07-27</updated><authors><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Saha</keyname><forenames>Sangeet</forenames></author><author><keyname>Zaman</keyname><forenames>Jkm Sadique Uz</forenames></author><author><keyname>Das</keyname><forenames>Suman</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author><author><keyname>Ghosh</keyname><forenames>Ranjan</forenames></author></authors><title>A simple 1-byte 1-clock RC4 design and its efficient implementation in
  FPGA coprocessor for secured ethernet communication</title><categories>cs.AR</categories><comments>Proceedings of National Workshop on Cryptology 2012 Organized by
  CRSI(http://crsind.com/),INDIA Held at VIT,INDIA (06.08.12 - 08.08.12)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of cryptography till date the 1-byte in 1-clock is the best
known RC4 hardware design [1], while the 1-byte in 3clocks is the best known
implementation [2,3]. The design algorithm in [1] considers two consecutive
bytes together and processes them in 2 clocks. The design of 1-byte in 3-clocks
is too much modular and clock hungry. In this paper considering the RC4
algorithm, as it is, a simpler RC4 hardware design providing higher throughput
is proposed in which 1-byte is processed in 1-clock. In the design two
sequential tasks are executed as two independent events during rising and
falling edges of the same clock and the swapping is directly executed using a
MUX-DEMUX combination. The power consumed in behavioral and structural designs
of RC4 are estimated and a power optimization technique is proposed. The NIST
statistical test suite is run on RC4 key streams in order to know its
randomness property. The encryption and decryption designs are respectively
embedded on two FPGA boards with RC4 in a custom coprocessor followed by
Ethernet communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1745</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1745</id><created>2012-05-08</created><authors><author><keyname>Hajiani</keyname><forenames>Pedram</forenames></author><author><keyname>Poshtan</keyname><forenames>Javad</forenames></author></authors><title>Reconfigurable Controller Design For Actuator Faults In A Four-Tank
  System Benchmark</title><categories>cs.SY</categories><comments>8 pages, 5 figures, 3 tables</comments><journal-ref>International Journal of Instrumentation and Control Systems
  (IJICS) Vol.2, No.2, April 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The purpose of this work is to design a state feedback controller using
Parametric Eigenstructure Assignment (PAE) technique that has the capacity to
be reconfigured in the case that partial actuator faults occur. The proposed
controller is capable of compensating the gain losses in actuators and
maintaining the control performance in faulty situations. Simulations show the
performance enhancement in comparison to the non-reconfigurable controller
through Integral Absolute Error (IAE) index for different fault scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1758</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1758</id><created>2012-05-08</created><updated>2014-03-14</updated><authors><author><keyname>Thaler</keyname><forenames>Justin</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Faster Algorithms for Privately Releasing Marginals</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of releasing $k$-way marginals of a database $D \in
(\{0,1\}^d)^n$, while preserving differential privacy. The answer to a $k$-way
marginal query is the fraction of $D$'s records $x \in \{0,1\}^d$ with a given
value in each of a given set of up to $k$ columns. Marginal queries enable a
rich class of statistical analyses of a dataset, and designing efficient
algorithms for privately releasing marginal queries has been identified as an
important open problem in private data analysis (cf. Barak et. al., PODS '07).
  We give an algorithm that runs in time $d^{O(\sqrt{k})}$ and releases a
private summary capable of answering any $k$-way marginal query with at most
$\pm .01$ error on every query as long as $n \geq d^{O(\sqrt{k})}$. To our
knowledge, ours is the first algorithm capable of privately releasing marginal
queries with non-trivial worst-case accuracy guarantees in time substantially
smaller than the number of $k$-way marginal queries, which is $d^{\Theta(k)}$
(for $k \ll d$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1765</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1765</id><created>2012-05-08</created><updated>2013-01-05</updated><authors><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author></authors><title>Chaotic multi-objective optimization based design of fractional order
  PI{\lambda}D{\mu} controller in AVR system</title><categories>cs.SY cs.NE</categories><comments>30 pages, 14 figures</comments><journal-ref>International Journal of Electrical Power &amp; Energy Systems, Volume
  43, Issue 1, December 2012, Pages 393-407</journal-ref><doi>10.1016/j.ijepes.2012.06.034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a fractional order (FO) PI{\lambda}D\mu controller is designed
to take care of various contradictory objective functions for an Automatic
Voltage Regulator (AVR) system. An improved evolutionary Non-dominated Sorting
Genetic Algorithm II (NSGA II), which is augmented with a chaotic map for
greater effectiveness, is used for the multi-objective optimization problem.
The Pareto fronts showing the trade-off between different design criteria are
obtained for the PI{\lambda}D\mu and PID controller. A comparative analysis is
done with respect to the standard PID controller to demonstrate the merits and
demerits of the fractional order PI{\lambda}D\mu controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1768</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1768</id><created>2012-05-07</created><authors><author><keyname>Abrarov</keyname><forenames>S. M.</forenames></author><author><keyname>Quine</keyname><forenames>B. M.</forenames></author></authors><title>On the Fourier expansion method for highly accurate computation of the
  Voigt/complex error function in a rapid algorithm</title><categories>math.NA cs.NA</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent publication [1] we presented an exponential series
approximation suitable for highly accurate computation of the complex error
function in a rapid algorithm. In this Short Communication we describe how a
simplified representation of the proposed complex error function approximation
makes possible further algorithmic optimization resulting in a considerable
computational acceleration without compromise on accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1771</identifier>
 <datestamp>2014-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1771</id><created>2012-05-08</created><updated>2014-08-14</updated><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author><author><keyname>Armano</keyname><forenames>Giuliano</forenames></author></authors><title>Quantum-Classical Transitions in Complex Networks</title><categories>cond-mat.dis-nn cond-mat.quant-gas cs.SI physics.soc-ph</categories><comments>12 pages, 5 figures</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment.
  volume(2013) number (04) P04019</journal-ref><doi>10.1088/1742-5468/2013/04/P04019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inherent properties of specific physical systems can be used as metaphors
for investigation of the behavior of complex networks. This insight has already
been put into practice in previous work, e.g., studying the network evolution
in terms of phase transitions of quantum gases or representing distances among
nodes as if they were particle energies. This paper shows that the emergence of
different structures in complex networks, such as the scale-free and the
winner-takes-all networks, can be represented in terms of a quantum-classical
transition for quantum gases. In particular, we propose a model of fermionic
networks that allows us to investigate the network evolution and its dependence
on the system temperature. Simulations, performed in accordance with the cited
model, clearly highlight the separation between classical random and
winner-takes-all networks, in full correspondence with the separation between
classical and quantum regions for quantum gases. We deem this model useful for
the analysis of synthetic and real complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1775</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1775</id><created>2012-05-08</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM, IMJ</affiliation></author><author><keyname>Todorcevic</keyname><forenames>Stevo</forenames><affiliation>ELM, IMJ</affiliation></author></authors><title>Automatic Ordinals</title><categories>math.LO cs.LO</categories><comments>To appear in a Special Issue on New Worlds of Computation 2011 of the
  International Journal of Unconventional Computing. arXiv admin note: text
  overlap with arXiv:1111.1504</comments><proxy>ccsd</proxy><journal-ref>International Journal of Unconventional Computing 9, 1-2 (2013)
  61-70</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the injectively omega-tree-automatic ordinals are the ordinals
smaller than $\omega^{\omega^\omega}$. Then we show that the injectively
$\omega^n$-automatic ordinals, where $n&gt;0$ is an integer, are the ordinals
smaller than $\omega^{\omega^n}$. This strengthens a recent result of Schlicht
and Stephan who considered in [Schlicht-Stephan11] the subclasses of finite
word $\omega^n$-automatic ordinals. As a by-product we obtain that the
hierarchy of injectively $\omega^n$-automatic structures, n&gt;0, which was
considered in [Finkel-Todorcevic12], is strict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1779</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1779</id><created>2012-05-08</created><authors><author><keyname>Pires</keyname><forenames>Ricardo</forenames></author></authors><title>A Common Evaluation Setting for Just.Ask, Open Ephyra and Aranea QA
  systems</title><categories>cs.IR</categories><comments>Technical Report elaborated by Ricardo Pires for the course Natural
  Language Processing Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Question Answering (QA) is not a new research field in Natural Language
Processing (NLP). However in recent years, QA has been a subject of growing
study. Nowadays, most of the QA systems have a similar pipelined architecture
and each system use a set of unique techniques to accomplish its state of the
art results. However, many things are not clear in the QA processing. It is not
clear the extend of the impact of tasks performed in earlier stages in
following stages of the pipelining process. It is not clear, if techniques used
in a QA system can be used in another QA system to improve its results. And
finally, it is not clear in what setting should be these systems tested in
order to properly analyze their results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1782</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1782</id><created>2012-05-08</created><updated>2012-05-21</updated><authors><author><keyname>Petrik</keyname><forenames>Marek</forenames></author></authors><title>Approximate Dynamic Programming By Minimizing Distributionally Robust
  Bounds</title><categories>stat.ML cs.LG</categories><comments>In Proceedings of International Conference on Machine Learning, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate dynamic programming is a popular method for solving large Markov
decision processes. This paper describes a new class of approximate dynamic
programming (ADP) methods- distributionally robust ADP-that address the curse
of dimensionality by minimizing a pessimistic bound on the policy loss. This
approach turns ADP into an optimization problem, for which we derive new
mathematical program formulations and analyze its properties. DRADP improves on
the theoretical guarantees of existing ADP methods-it guarantees convergence
and L1 norm based error bounds. The empirical evaluation of DRADP shows that
the theoretical guarantees translate well into good performance on benchmark
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1786</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1786</id><created>2012-05-08</created><authors><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Levavi</keyname><forenames>Ariel</forenames></author></authors><title>Tight Lower Bounds on Envy-Free Makespan Approximation</title><categories>cs.DM cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we give a tight lower bound on makespan approximations for
envy-free allocation mechanism dedicated to scheduling tasks on unrelated
machines. Specifically, we show that no mechanism exists that can guarantee an
envy-free allocation of jobs to $m$ machines with a makespan of less than a
factor of $O(\log m)$ of the minimal makespan. Combined with previous results,
this paper definitively proves that the optimal algorithm for obtaining a
minimal makespan for any envy-free division can at best approximate the
makespan to a factor of $O(\log m)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1794</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1794</id><created>2012-05-08</created><authors><author><keyname>Abdolali</keyname><forenames>Behrouz</forenames></author><author><keyname>Sameti</keyname><forenames>Hossein</forenames></author></authors><title>A Novel Method For Speech Segmentation Based On Speakers'
  Characteristics</title><categories>cs.AI cs.CL</categories><comments>14 pages, 8 figures</comments><msc-class>92C55</msc-class><acm-class>C.3.4</acm-class><journal-ref>B. Abdolali, H. Sameti &quot;A Novel Method for Speech Segmentation
  based on Speakers' Specifications&quot;, Signal &amp; Image Processing: An
  International Journal (SIPIJ) Vol.3, No.2, pp. 65-78, April 2012</journal-ref><doi>10.5121/sipij.2012.3205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech Segmentation is the process change point detection for partitioning an
input audio stream into regions each of which corresponds to only one audio
source or one speaker. One application of this system is in Speaker Diarization
systems. There are several methods for speaker segmentation; however, most of
the Speaker Diarization Systems use BIC-based Segmentation methods. The main
goal of this paper is to propose a new method for speaker segmentation with
higher speed than the current methods - e.g. BIC - and acceptable accuracy. Our
proposed method is based on the pitch frequency of the speech. The accuracy of
this method is similar to the accuracy of common speaker segmentation methods.
However, its computation cost is much less than theirs. We show that our method
is about 2.4 times faster than the BIC-based method, while the average accuracy
of pitch-based method is slightly higher than that of the BIC-based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1796</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1796</id><created>2012-05-08</created><authors><author><keyname>Boulmakoul</keyname><forenames>Azedine</forenames></author><author><keyname>Karim</keyname><forenames>Lamia</forenames></author><author><keyname>Lbath</keyname><forenames>Ahmed</forenames></author></authors><title>Moving Object Trajectories Meta-Model And Spatio-Temporal Queries</title><categories>cs.DB</categories><comments>International Journal of Database Management Systems (IJDMS) Vol.4,
  No.2, April 2012</comments><doi>10.5121/ijdms.2012.4203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a general moving object trajectories framework is put forward
to allow independent applications processing trajectories data benefit from a
high level of interoperability, information sharing as well as an efficient
answer for a wide range of complex trajectory queries. Our proposed meta-model
is based on ontology and event approach, incorporates existing presentations of
trajectory and integrates new patterns like space-time path to describe
activities in geographical space-time. We introduce recursive Region of
Interest concepts and deal mobile objects trajectories with diverse
spatio-temporal sampling protocols and different sensors available that
traditional data model alone are incapable for this purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1813</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1813</id><created>2012-05-08</created><authors><author><keyname>Nadakuditi</keyname><forenames>Raj Rao</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Graph spectra and the detectability of community structure in networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>5 pages, 2 figures</comments><journal-ref>Phys. Rev. Lett. 108, 188701 (2012)</journal-ref><doi>10.1103/PhysRevLett.108.188701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study networks that display community structure -- groups of nodes within
which connections are unusually dense. Using methods from random matrix theory,
we calculate the spectra of such networks in the limit of large size, and hence
demonstrate the presence of a phase transition in matrix methods for community
detection, such as the popular modularity maximization method. The transition
separates a regime in which such methods successfully detect the community
structure from one in which the structure is present but is not detected. By
comparing these results with recent analyses of maximum-likelihood methods we
are able to show that spectral modularity maximization is an optimal detection
method in the sense that no other method will succeed in the regime where the
modularity method fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1820</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1820</id><created>2012-01-15</created><authors><author><keyname>Zizzi</keyname><forenames>Paola</forenames></author></authors><title>The non-algorithmic side of the mind</title><categories>cs.AI quant-ph</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of a non-algorithmic side of the mind, conjectured by Penrose
on the basis of G\&quot;odel's first incompleteness theorem, is investigated here in
terms of a quantum metalanguage. We suggest that, besides human ordinary
thought, which can be formalized in a computable, logical language, there is
another important kind of human thought, which is Turing-non-computable. This
is methatought, the process of thinking about ordinary thought. Metathought can
be formalized as a metalanguage, which speaks about and controls the logical
language of ordinary thought. Ordinary thought has two computational modes, the
quantum mode and the classical mode, the latter deriving from decoherence of
the former. In order to control the logical language of the quantum mode, one
needs to introduce a quantum metalanguage, which in turn requires a quantum
version of Tarski Convention T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1823</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1823</id><created>2012-05-08</created><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Pl\&quot;ucker Embedding of Cyclic Orbit Codes</title><categories>cs.IT math.IT</categories><comments>to appear in Proceedings of the 20th International Symposium on
  Mathematical Theory of Networks and Systems 2012, Melbourne, Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic orbit codes are a family of constant dimension codes used for random
network coding. We investigate the Pl\&quot;ucker embedding of these codes and show
how to efficiently compute the Grassmann coordinates of the code words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1824</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1824</id><created>2012-05-08</created><updated>2014-08-19</updated><authors><author><keyname>Laso&#x144;</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Streib</keyname><forenames>Noah</forenames></author><author><keyname>Trotter</keyname><forenames>William T.</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>An extremal problem on crossing vectors</title><categories>math.CO cs.DM</categories><comments>Corrections and improvements</comments><msc-class>05D05, 06A07</msc-class><journal-ref>J.Combin.Theory Ser.A 128 (2014) 41-55</journal-ref><doi>10.1016/j.jcta.2014.07.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For positive integers $w$ and $k$, two vectors $A$ and $B$ from
$\mathbb{Z}^w$ are called $k$-crossing if there are two coordinates $i$ and $j$
such that $A[i]-B[i]\geq k$ and $B[j]-A[j]\geq k$. What is the maximum size of
a family of pairwise $1$-crossing and pairwise non-$k$-crossing vectors in
$\mathbb{Z}^w$? We state a conjecture that the answer is $k^{w-1}$. We prove
the conjecture for $w\leq 3$ and provide weaker upper bounds for $w\geq 4$.
Also, for all $k$ and $w$, we construct several quite different examples of
families of desired size $k^{w-1}$. This research is motivated by a natural
question concerning the width of the lattice of maximum antichains of a
partially ordered set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1825</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1825</id><created>2012-05-08</created><updated>2012-06-26</updated><authors><author><keyname>Champarnaud</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Jeanne</keyname><forenames>Hadrien</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author></authors><title>Derivatives of Approximate Regular Expressions</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aim is to construct a finite automaton recognizing the set of words that
are at a bounded distance from some word of a given regular language. We define
new regular operators, the similarity operators, based on a generalization of
the notion of distance and we introduce the family of regular expressions
extended to similarity operators, that we call AREs (Approximate Regular
Expressions). We set formulae to compute the Brzozowski derivatives and the
Antimirov derivatives of an ARE, which allows us to give a solution to the ARE
membership problem and to provide the construction of two recognizers for the
language denoted by an ARE. As far as we know, the family of approximative
regular expressions is introduced for the first time in this paper. Classical
approximate regular expression matching algorithms are approximate matching
algorithms on regular expressions. Our approach is rather to process an exact
matching on approximate regular expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1828</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1828</id><created>2012-05-08</created><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author></authors><title>The Natural Gradient by Analogy to Signal Whitening, and Recipes and
  Tricks for its Use</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The natural gradient allows for more efficient gradient descent by removing
dependencies and biases inherent in a function's parameterization. Several
papers present the topic thoroughly and precisely. It remains a very difficult
idea to get your head around however. The intent of this note is to provide
simple intuition for the natural gradient and its use. We review how an ill
conditioned parameter space can undermine learning, introduce the natural
gradient by analogy to the more widely understood concept of signal whitening,
and present tricks and specific prescriptions for applying the natural gradient
to learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1853</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1853</id><created>2012-05-08</created><authors><author><keyname>Iyer</keyname><forenames>K. B. Priya</forenames></author><author><keyname>Shanthi</keyname><forenames>V.</forenames></author></authors><title>Goal Directed Relative Skyline Queries in Time Dependent Road Networks</title><categories>cs.NI cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Wireless GIS technology is progressing rapidly in the area of mobile
communications. Location-based spatial queries are becoming an integral part of
many new mobile applications. The Skyline queries are latest apps under
Location-based services. In this paper we introduce Goal Directed Relative
Skyline queries on Time dependent (GD-RST) road networks. The algorithm uses
travel time as a metric in finding the data object by considering multiple
query points (multi-source skyline) relative to user location and in the user
direction of travelling. We design an efficient algorithm based on Filter
phase, Heap phase and Refine Skyline phases. At the end, we propose a dynamic
skyline caching (DSC) mechanism which helps to reduce the computation cost for
future skyline queries. The experimental evaluation reflects the performance of
GD-RST algorithm over the traditional branch and bound algorithm for skyline
queries in real road networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1859</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1859</id><created>2012-05-08</created><authors><author><keyname>S.</keyname><forenames>Bhavana</forenames></author><author><keyname>Sudha</keyname><forenames>K. L.</forenames></author></authors><title>Text Steganography using LSB insertion method along with Chaos Theory</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The art of information hiding has been around nearly as long as the need for
covert communication. Steganography, the concealing of information, arose early
on as an extremely useful method for covert information transmission.
Steganography is the art of hiding secret message within a larger image or
message such that the hidden message or an image is undetectable; this is in
contrast to cryptography, where the existence of the message itself is not
disguised, but the content is obscure. The goal of a steganographic method is
to minimize the visually apparent and statistical differences between the cover
data and a steganogram while maximizing the size of the payload. Current
digital image steganography presents the challenge of hiding message in a
digital image in a way that is robust to image manipulation and attack. This
paper explains about how a secret message can be hidden into an image using
least significant bit insertion method along with chaos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1860</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1860</id><created>2012-05-08</created><authors><author><keyname>Sharma</keyname><forenames>Mohandeep</forenames></author><author><keyname>Kumar</keyname><forenames>Dilip</forenames></author></authors><title>Wishbone bus Architecture - A Survey and Comparison</title><categories>cs.AR</categories><comments>18 pages</comments><journal-ref>International journal of VLSI Design &amp; Communication Systems
  (VLSICS) Vol.3, No.2 April 2012, 107-124</journal-ref><doi>10.5121/vlsic.2012.3210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of an on-chip interconnection architecture used for
communication between IP cores depends on the efficiency of its bus
architecture. Any bus architecture having advantages of faster bus clock speed,
extra data transfer cycle, improved bus width and throughput is highly
desirable for a low cost, reduced time-to-market and efficient System-on-Chip
(SoC). This paper presents a survey of WISHBONE bus architecture and its
comparison with three other on-chip bus architectures viz. Advanced Micro
controller Bus Architecture (AMBA) by ARM, CoreConnect by IBM and Avalon by
Altera. The WISHBONE Bus Architecture by Silicore Corporation appears to be
gaining an upper edge over the other three bus architecture types because of
its special performance parameters like the use of flexible arbitration scheme
and additional data transfer cycle (Read-Modify-Write cycle). Moreover, its IP
Cores are available free for use requiring neither any registration nor any
agreement or license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1866</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1866</id><created>2012-05-09</created><authors><author><keyname>Singh</keyname><forenames>Amandeep</forenames></author><author><keyname>Singh</keyname><forenames>Balwinder</forenames></author></authors><title>Microcontroller Based Testing of Digital IP-Core</title><categories>cs.AR</categories><comments>Microcontroller, FPGA, Testing, TMR, SOC</comments><doi>10.5121/vlsic.2012.3205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing core based System on Chip is a challenge for the test engineers. To
test the complete SOC at one time with maximum fault coverage, test engineers
prefer to test each IP-core separately. At speed testing using external testers
is more expensive because of gigahertz processor. The purpose of this paper is
to develop cost efficient and flexible test methodology for testing digital
IP-cores . The prominent feature of the approach is to use microcontroller to
test IP-core. The novel feature is that there is no need of test pattern
generator and output response analyzer as microcontroller performs the function
of both. This approach has various advantages such as at speed testing, low
cost, less area overhead and greater flexibility since most of the testing
process is based on software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1867</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1867</id><created>2012-05-09</created><authors><author><keyname>Batabyal</keyname><forenames>Suvadip</forenames></author><author><keyname>Bhaumik</keyname><forenames>Parama</forenames></author></authors><title>Improving Network Performance with Affinity based Mobility Model in
  Opportunistic Network</title><categories>cs.NI</categories><comments>IJWMN Journal, Opportunistic Network, 14 pages, 10 figures with
  Appendix</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 2, April 2012, 189-202</journal-ref><doi>10.5121/ijwmn.2012.4213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic network is a type of Delay Tolerant Network which is
characterized by intermittent connectivity amongst the nodes and communication
largely depends upon the mobility of the participating nodes. The network being
highly dynamic, traditional MANET protocols cannot be applied and the nodes
must adhere to store-carry-forward mechanism. Nodes do not have the information
about the network topology, number of participating nodes and the location of
the destination node. Hence, message transfer reliability largely depends upon
the mobility pattern of the nodes. In this paper we have tried to find the
impact of RWP (Random Waypoint) mobility on packet delivery ratio. We estimate
mobility factors like number of node encounters, contact duration(link time)
and inter-contact time which in turn depends upon parameters like playfield
area (total network area), number of nodes, node velocity, bit-rate and RF
range of the nodes. We also propose a restricted form of RWP mobility model,
called the affinity based mobility model. The network scenario consists of a
source and a destination node that are located at two extreme corners of the
square playfield (to keep a maximum distance between them) and exchange data
packets with the aid of mobile 'helper' nodes. The source node and the
destination node are static. The mobile nodes only help in relaying the
message. We prove how affinity based mobility model helps in augmenting the
network reliability thereby increasing the message delivery ratio and reduce
message delivery latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1871</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1871</id><created>2012-05-09</created><authors><author><keyname>Alipour</keyname><forenames>Mehdi</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa E.</forenames></author><author><keyname>baghini</keyname><forenames>Hesamodin shojaei</forenames></author></authors><title>Design Space Exploration to Find the Optimum Cache and Register File
  Size for Embedded Applications</title><categories>cs.AR</categories><comments>The 2011 International Conference on Embedded Systems and
  Applications, las vegas, nevada, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the future, embedded processors must process more computation-intensive
network applications and internet traffic and packet-processing tasks become
heavier and sophisticated. Since the processor performance is severely related
to the average memory access delay and also the number of processor registers
affects the performance, cache and register file are two major parts in
designing embedded processor architecture. Although increasing cache and
register file size leads to performance improvement in embedded applications
and packet-processing tasks in high traffic networks with too much packets, the
increased area, power consumption and memory hierarchy delay are the overheads
of these techniques. Therefore, implementing these components in the optimum
size is of significant interest in the design of embedded processors. This
paper explores the effect of cache and register file size on the processor
performance to calculate the optimum size of these components for embedded
applications. Experimental results show that although having bigger cache and
register file is one of the performance improvement approaches in embedded
processors, however, by increasing the size of these parameters over a
threshold level, performance improvement is saturated and then, decreased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1877</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1877</id><created>2012-05-09</created><authors><author><keyname>B&#xed;lka</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>Structured Grammars are Effective</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Top-down parsing has received much attention recently. Parsing expression
grammars (PEG) allows construction of linear time parsers using packrat
algorithm. These techniques however suffer from problem of prefix hiding. We
use alternative formalism of relativized regular expressions REGREG for which
top-down backtracking parser runs in linear time. This formalism allows to
construct fast parsers with modest memory requirements for practical grammars.
We show that our formalism is equivalent to PEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1882</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1882</id><created>2012-05-09</created><authors><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>Jos&#xe9; Miguel</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author><author><keyname>Ventura</keyname><forenames>Inmaculada</forenames></author></authors><title>The 1-Center and 1-Highway problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a variation of the 1-center problem in which, in addition to a
single supply facility, we are allowed to locate a highway. This highway
increases the transportation speed between any demand point and the facility.
That is, given a set $S$ of points and $v&gt;1$, we are interested in locating the
facility point $f$ and the highway $h$ that minimize the expression $\max_{p\in
S}d_{h}(p,f)$, where $d_h$ is the time needed to travel between $p$ and $f$. We
consider two types of highways ({\em freeways} and {\em turnpikes}) and study
the cases in which the highway's length is fixed by the user (or can be
modified to further decrease the transportation time).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1885</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1885</id><created>2012-05-09</created><authors><author><keyname>Huang</keyname><forenames>Yongming</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Yang</keyname><forenames>Luxi</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Distributed Multicell Beamforming Design Approaching Pareto Boundary
  with Max-Min Fairness</title><categories>cs.IT math.IT</categories><comments>8 figures. To Appear in IEEE Trans. Wireless Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses coordinated downlink beamforming optimization in
multicell time-division duplex (TDD) systems where a small number of parameters
are exchanged between cells but with no data sharing. With the goal to reach
the point on the Pareto boundary with max-min rate fairness, we first develop a
two-step centralized optimization algorithm to design the joint beamforming
vectors. This algorithm can achieve a further sum-rate improvement over the
max-min optimal performance, and is shown to guarantee max-min Pareto
optimality for scenarios with two base stations (BSs) each serving a single
user. To realize a distributed solution with limited intercell communication,
we then propose an iterative algorithm by exploiting an approximate
uplink-downlink duality, in which only a small number of positive scalars are
shared between cells in each iteration. Simulation results show that the
proposed distributed solution achieves a fairness rate performance close to the
centralized algorithm while it has a better sum-rate performance, and
demonstrates a better tradeoff between sum-rate and fairness than the Nash
Bargaining solution especially at high signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1886</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1886</id><created>2012-05-09</created><authors><author><keyname>Makwana</keyname><forenames>Ishit</forenames></author><author><keyname>Sheth</keyname><forenames>Vitrag</forenames></author></authors><title>A low power high bandwidth four quadrant analog multiplier in 32 nm
  CNFET technology</title><categories>cs.ET</categories><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.3, No.2, April 2012, 73-83</journal-ref><doi>10.5121/vlsic.2012.3207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carbon Nanotube Field Effect Transistor (CNFET) is a promising new technology
that overcomes several limitations of traditional silicon integrated circuit
technology. In recent years, the potential of CNFET for analog circuit
applications has been explored. This paper proposes a novel four quadrant
analog multiplier design using CNFETs. The simulation based on 32nm CNFET
technology shows that the proposed multiplier has very low harmonic distortion
(&lt;0.45%), large input range ({\pm}400mV), large bandwidth (~50GHz) and low
power consumption (~247{\mu}W), while operating at a supply voltage of
{\pm}0.9V.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1900</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1900</id><created>2012-05-09</created><authors><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Rescigno</keyname><forenames>Adele A.</forenames></author></authors><title>Strong Conflict-Free Coloring of Intervals</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the k-strong conflict-free coloring of a set of points on a line
with respect to a family of intervals: Each point on the line must be assigned
a color so that the coloring has to be conflict-free, in the sense that in
every interval I there are at least k colors each appearing exactly once in I.
In this paper, we present a polynomial algorithm for the general problem; the
algorithm has an approximation factor 5-2/k when k\geq2 and approximation
factor 2 for k=1. In the special case the family contains all the possible
intervals on the given set of points, we show that a 2 approximation algorithm
exists, for any k\geq1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1904</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1904</id><created>2012-05-09</created><authors><author><keyname>Alrawashdeh</keyname><forenames>Thamer A.</forenames></author><author><keyname>Muhairat</keyname><forenames>Mohammad I.</forenames></author><author><keyname>Alqatawnah</keyname><forenames>Sokyna M.</forenames></author></authors><title>Factors affecting acceptance of web-based training system: Using
  extended UTAUT and structural equation modeling</title><categories>cs.OH</categories><comments>this article includes 3 tales and 2 figures, International Journal of
  Computer Science, Engineering and Information Technology (IJCSEIT), Vol.2,
  No.2, April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancement in information system leads organizations to apply e-learning
system to train their employees in order to enhance its performance. In this
respect, applying web based training will enable the organization to train
their employees quickly, efficiently and effectively anywhere at any time. This
research aims to extend Unified Theory of Acceptance and Use Technology (UTAUT)
using some factors such flexibility of web based training system, system
interactivity and system enjoyment, in order to explain the employees'
intention to use web based training system. A total of 290 employees have
participated in this study. The findings of the study revealed that performance
expectancy, facilitating conditions, social influence and system flexibility
have direct effect on the employees' intention to use web based training
system, while effort expectancy, system enjoyment and system interactivity have
indirect effect on employees' intention to use the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1923</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1923</id><created>2012-05-09</created><authors><author><keyname>Kharya</keyname><forenames>Shweta</forenames></author></authors><title>Using data mining techniques for diagnosis and prognosis of cancer
  disease</title><categories>cs.DB</categories><comments>12 pages, 4 figures, 4 tables, IJCSEIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast cancer is one of the leading cancers for women in developed countries
including India. It is the second most common cause of cancer death in women.
The high incidence of breast cancer in women has increased significantly in the
last years. In this paper we have discussed various data mining approaches that
have been utilized for breast cancer diagnosis and prognosis. Breast Cancer
Diagnosis is distinguishing of benign from malignant breast lumps and Breast
Cancer Prognosis predicts when Breast Cancer is to recur in patients that have
had their cancers excised. This study paper summarizes various review and
technical articles on breast cancer diagnosis and prognosis also we focus on
current research being carried out using the data mining techniques to enhance
the breast cancer diagnosis and prognosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1924</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1924</id><created>2012-05-09</created><updated>2012-10-05</updated><authors><author><keyname>Chakaravarthy</keyname><forenames>Venkatesan T.</forenames></author><author><keyname>Roy</keyname><forenames>Sambuddha</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author></authors><title>Distributed Algorithms for Scheduling on Line and Tree Networks</title><categories>cs.DS cs.DC</categories><comments>Accepted to PODC 2012, full version</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have a set of processors (or agents) and a set of graph networks defined
over some vertex set. Each processor can access a subset of the graph networks.
Each processor has a demand specified as a pair of vertices $&lt;u, v&gt;$, along
with a profit; the processor wishes to send data between $u$ and $v$. Towards
that goal, the processor needs to select a graph network accessible to it and a
path connecting $u$ and $v$ within the selected network. The processor requires
exclusive access to the chosen path, in order to route the data. Thus, the
processors are competing for routes/channels. A feasible solution selects a
subset of demands and schedules each selected demand on a graph network
accessible to the processor owning the demand; the solution also specifies the
paths to use for this purpose. The requirement is that for any two demands
scheduled on the same graph network, their chosen paths must be edge disjoint.
The goal is to output a solution having the maximum aggregate profit. Prior
work has addressed the above problem in a distibuted setting for the special
case where all the graph networks are simply paths (i.e, line-networks).
Distributed constant factor approximation algorithms are known for this case.
  The main contributions of this paper are twofold. First we design a
distributed constant factor approximation algorithm for the more general case
of tree-networks. The core component of our algorithm is a tree-decomposition
technique, which may be of independent interest. Secondly, for the case of
line-networks, we improve the known approximation guarantees by a factor of 5.
Our algorithms can also handle the capacitated scenario, wherein the demands
and edges have bandwidth requirements and capacities, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1925</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1925</id><created>2012-05-09</created><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author><author><keyname>Culpepper</keyname><forenames>Benjamin J.</forenames></author></authors><title>Hamiltonian Annealed Importance Sampling for partition function
  estimation</title><categories>cs.LG physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an extension to annealed importance sampling that uses
Hamiltonian dynamics to rapidly estimate normalization constants. We
demonstrate this method by computing log likelihoods in directed and undirected
probabilistic image models. We compare the performance of linear generative
models with both Gaussian and Laplace priors, product of experts models with
Laplace and Student's t experts, the mc-RBM, and a bilinear generative model.
We provide code to compare additional models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1928</identifier>
 <datestamp>2012-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1928</id><created>2012-05-09</created><updated>2012-07-17</updated><authors><author><keyname>Dinuzzo</keyname><forenames>Francesco</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>The representer theorem for Hilbert spaces: a necessary and sufficient
  condition</title><categories>math.FA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of regularization functionals is said to admit a linear representer
theorem if every member of the family admits minimizers that lie in a fixed
finite dimensional subspace. A recent characterization states that a general
class of regularization functionals with differentiable regularizer admits a
linear representer theorem if and only if the regularization term is a
non-decreasing function of the norm. In this report, we improve over such
result by replacing the differentiability assumption with lower semi-continuity
and deriving a proof that is independent of the dimensionality of the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1938</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1938</id><created>2012-05-09</created><authors><author><keyname>Ramya</keyname><forenames>C.</forenames></author><author><keyname>Kavitha</keyname><forenames>G.</forenames></author><author><keyname>Shreedhara</keyname><forenames>K. S.</forenames></author></authors><title>Dynamic Grouping of Web Users Based on Their Web Access Patterns using
  ART1 Neural Network Clustering Algorithm</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose ART1 neural network clustering algorithm to group
users according to their Web access patterns. We compare the quality of
clustering of our ART1 based clustering technique with that of the K-Means and
SOM clustering algorithms in terms of inter-cluster and intra-cluster
distances. The results show the average inter-cluster distance of ART1 is high
compared to K-Means and SOM when there are fewer clusters. As the number of
clusters increases, average inter-cluster distance of ART1 is low compared to
K-Means and SOM which indicates the high quality of clusters formed by our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1939</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1939</id><created>2012-05-09</created><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author></authors><title>Hamiltonian Monte Carlo with Reduced Momentum Flips</title><categories>physics.data-an cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hamiltonian Monte Carlo (or hybrid Monte Carlo) with partial momentum
refreshment explores the state space more slowly than it otherwise would due to
the momentum reversals which occur on proposal rejection. These cause
trajectories to double back on themselves, leading to random walk behavior on
timescales longer than the typical rejection time, and leading to slower
mixing. I present a technique by which the number of momentum reversals can be
reduced. This is accomplished by maintaining the net exchange of probability
between states with opposite momenta, but reducing the rate of exchange in both
directions such that it is 0 in one direction. An experiment illustrates these
reduced momentum flips accelerating mixing for a particular distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1960</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1960</id><created>2012-05-09</created><updated>2012-06-05</updated><authors><author><keyname>Grolmusz</keyname><forenames>Vince</forenames></author></authors><title>A Note on the PageRank of Undirected Graphs</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PageRank is a widely used scoring function of networks in general and of
the World Wide Web graph in particular. The PageRank is defined for directed
graphs, but in some special cases applications for undirected graphs occur. In
the literature it is widely noted that the PageRank for undirected graphs are
proportional to the degrees of the vertices of the graph. We prove that
statement for a particular personalization vector in the definition of the
PageRank, and we also show that in general, the PageRank of an undirected graph
is not exactly proportional to the degree distribution of the graph: our main
theorem gives an upper and a lower bound to the L_1 norm of the difference of
the PageRank and the degree distribution vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1975</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1975</id><created>2012-05-09</created><authors><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Godard</keyname><forenames>Emmanuel</forenames></author><author><keyname>Santoro</keyname><forenames>Nicola</forenames></author><author><keyname>Yamashita</keyname><forenames>Masafumi</forenames></author></authors><title>Expressivity of Time-Varying Graphs and the Power of Waiting in Dynamic
  Networks</title><categories>cs.DC cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In infrastructure-less highly dynamic networks, computing and performing even
basic tasks (such as routing and broadcasting) is a very challenging activity
due to the fact that connectivity does not necessarily hold, and the network
may actually be disconnected at every time instant. Clearly the task of
designing protocols for these networks is less difficult if the environment
allows waiting (i.e., it provides the nodes with store-carry-forward-like
mechanisms such as local buffering) than if waiting is not feasible. No
quantitative corroborations of this fact exist (e.g., no answer to the
question: how much easier?). In this paper, we consider these qualitative
questions about dynamic networks, modeled as time-varying (or evolving) graphs,
where edges exist only at some times.
  We examine the difficulty of the environment in terms of the expressivity of
the corresponding time-varying graph; that is in terms of the language
generated by the feasible journeys in the graph. We prove that the set of
languages $L_{nowait}$ when no waiting is allowed contains all computable
languages. On the other end, using algebraic properties of quasi-orders, we
prove that $L_{wait}$ is just the family of regular languages. In other words,
we prove that, when waiting is no longer forbidden, the power of the accepting
automaton (difficulty of the environment) drops drastically from being as
powerful as a Turing machine, to becoming that of a Finite-State machine. This
(perhaps surprisingly large) gap is a measure of the computational power of
waiting.
  We also study bounded waiting; that is when waiting is allowed at a node only
for at most $d$ time units. We prove the negative result that $L_{wait[d]} =
L_{nowait}$; that is, the expressivity decreases only if the waiting is finite
but unpredictable (i.e., under the control of the protocol designer and not of
the environment).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1986</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1986</id><created>2012-05-09</created><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author><author><keyname>Parveen</keyname><forenames>Rafat</forenames></author></authors><title>Evolutionary algorithms in genetic regulatory networks model</title><categories>cs.CE q-bio.MN</categories><comments>10 pages, 3 figures and 1 table</comments><msc-class>http://bipublication.com/files/JABAR-V3I1-2012-06.pdf</msc-class><journal-ref>Journal of Advanced Bioinformatics Applications and Research
  3(1):271-280, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic Regulatory Networks (GRNs) plays a vital role in the understanding of
complex biological processes. Modeling GRNs is significantly important in order
to reveal fundamental cellular processes, examine gene functions and
understanding their complex relationships. Understanding the interactions
between genes gives rise to develop better method for drug discovery and
diagnosis of the disease since many diseases are characterized by abnormal
behaviour of the genes. In this paper we have reviewed various evolutionary
algorithms-based approach for modeling GRNs and discussed various opportunities
and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1988</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1988</id><created>2012-05-09</created><authors><author><keyname>Zeng</keyname><forenames>Shuqing</forenames></author></authors><title>Fast Optimal Joint Tracking-Registration for Multi-Sensor Systems</title><categories>cs.RO</categories><journal-ref>IEEE T. Instrumentation and Measurement 60(10): 3461-3470 (2011)</journal-ref><doi>10.1109/TIM.2011.2134990</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sensor fusion of multiple sources plays an important role in vehicular
systems to achieve refined target position and velocity estimates. In this
article, we address the general registration problem, which is a key module for
a fusion system to accurately correct systematic errors of sensors. A fast
maximum a posteriori (FMAP) algorithm for joint registration-tracking (JRT) is
presented. The algorithm uses a recursive two-step optimization that involves
orthogonal factorization to ensure numerically stability. Statistical
efficiency analysis based on Cram\`{e}r-Rao lower bound theory is presented to
show asymptotical optimality of FMAP. Also, Givens rotation is used to derive a
fast implementation with complexity O(n) with $n$ the number of tracked
targets. Simulations and experiments are presented to demonstrate the promise
and effectiveness of FMAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.1997</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.1997</id><created>2012-05-09</created><updated>2012-10-28</updated><authors><author><keyname>McDaid</keyname><forenames>Aaron F.</forenames></author><author><keyname>Murphy</keyname><forenames>Brendan Thomas</forenames></author><author><keyname>Friel</keyname><forenames>Nial</forenames></author><author><keyname>Hurley</keyname><forenames>Neil J.</forenames></author></authors><title>Model-based clustering in networks with Stochastic Community Finding</title><categories>stat.CO cs.SI physics.soc-ph</categories><comments>Presented at COMPSTAT 2012 http://www.compstat2012.org</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the model-based clustering of networks, blockmodelling may be used to
identify roles in the network. We identify a special case of the Stochastic
Block Model (SBM) where we constrain the cluster-cluster interactions such that
the density inside the clusters of nodes is expected to be greater than the
density between clusters. This corresponds to the intuition behind
community-finding methods, where nodes tend to clustered together if they link
to each other. We call this model Stochastic Community Finding (SCF) and
present an efficient MCMC algorithm which can cluster the nodes, given the
network. The algorithm is evaluated on synthetic data and is applied to a
social network of interactions at a karate club and at a monastery,
demonstrating how the SCF finds the 'ground truth' clustering where sometimes
the SBM does not. The SCF is only one possible form of constraint or
specialization that may be applied to the SBM. In a more supervised context, it
may be appropriate to use other specializations to guide the SBM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2005</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2005</id><created>2012-05-09</created><updated>2012-08-10</updated><authors><author><keyname>Weiland</keyname><forenames>Michele</forenames></author><author><keyname>Mitchell</keyname><forenames>Lawrence</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard</forenames></author><author><keyname>Kramer</keyname><forenames>Stephan</forenames></author><author><keyname>Parsons</keyname><forenames>Mark</forenames></author><author><keyname>Southern</keyname><forenames>James</forenames></author></authors><title>Mixed-mode implementation of PETSc for scalable linear algebra on
  multi-core processors</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With multi-core processors a ubiquitous building block of modern
supercomputers, it is now past time to enable applications to embrace these
developments in processor design. To achieve exascale performance, applications
will need ways of exploiting the new levels of parallelism that are exposed in
modern high-performance computers. A typical approach to this is to use
shared-memory programming techniques to best exploit multi-core nodes along
with inter-node message passing. In this paper, we describe the addition of
OpenMP threaded functionality to the PETSc library. We highlight some issues
that hinder good performance of threaded applications on modern processors and
describe how to negate them. The OpenMP branch of PETSc was benchmarked using
matrices extracted from Fluidity, a CFD application code, which uses the
library as its linear solver engine. The overall performance of the mixed-mode
implementation is shown to be superior to that of the pure-MPI version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2007</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2007</id><created>2012-01-10</created><authors><author><keyname>Barnawi</keyname><forenames>Ahmed</forenames></author><author><keyname>Akkari</keyname><forenames>Nadine</forenames></author><author><keyname>Emran</keyname><forenames>Muhammad</forenames></author><author><keyname>Khan</keyname><forenames>Asif Irshad</forenames></author></authors><title>Deploying SIP-based Mobile Exam Application onto Next Generation Network
  testbed</title><categories>cs.NI</categories><comments>6 Pages, Electronics, Communications and Photonics Conference
  (SIECPC), 2011 Saudi International, Riyadh, KSA, 24-26 April 2011</comments><journal-ref>IEEE Xplore, E-ISBN: 978-1-4577-0067-5 ,April 2011</journal-ref><doi>10.1109/SIECPC.2011.5876936</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, mobile operators are faced with enormous challenges.
Of such challenges, evolved user demands on personalized applications.
Telecommunications industry as well as research community have paid enormous
attention to Next Generation Networks (NGN) to address this challenge. NGN is
perceived as a sophisticated platform where both application developers and
mobile operators cooperate to develop user applications with enhanced quality
of experience. The objective of this paper is twofold: first we present an
introduction to state-of-the-art NGN testbed to be developed at KAU, and second
we provide initial analysis for deploying a mobile application on top of the
testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2026</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2026</id><created>2012-05-09</created><updated>2012-08-10</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Fernandez</keyname><forenames>Nelson</forenames></author></authors><title>Complexity and Information: Measuring Emergence, Self-organization, and
  Homeostasis at Multiple Scales</title><categories>cs.IT math.IT nlin.AO nlin.CG</categories><comments>42 pages, 11 figures, 2 tables</comments><report-no>C3 2012.03</report-no><msc-class>94A15 (Primary) 94A17, 68Q15, 68Q80 (Secondary)</msc-class><acm-class>H.1.1; F.1.3; F.1.1</acm-class><journal-ref>Complexity 18(2):29-44. 2012</journal-ref><doi>10.1002/cplx.21424</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concepts used in the scientific study of complex systems have become so
widespread that their use and abuse has led to ambiguity and confusion in their
meaning. In this paper we use information theory to provide abstract and
concise measures of complexity, emergence, self-organization, and homeostasis.
The purpose is to clarify the meaning of these concepts with the aid of the
proposed formal measures. In a simplified version of the measures (focusing on
the information produced by a system), emergence becomes the opposite of
self-organization, while complexity represents their balance. Homeostasis can
be seen as a measure of the stability of the system. We use computational
experiments on random Boolean networks and elementary cellular automata to
illustrate our measures at multiple scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2031</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2031</id><created>2012-05-09</created><authors><author><keyname>Sreejini</keyname><forenames>K. S.</forenames></author><author><keyname>Lijiya</keyname><forenames>A.</forenames></author><author><keyname>Govindan</keyname><forenames>V. K.</forenames></author></authors><title>M-FISH Karyotyping - A New Approach Based on Watershed Transform</title><categories>cs.CV</categories><comments>13 pages,7 figures</comments><doi>10.5121/ijcseit.2012.2210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Karyotyping is a process in which chromosomes in a dividing cell are properly
stained, identified and displayed in a standard format, which helps geneticist
to study and diagnose genetic factors behind various genetic diseases and for
studying cancer. M-FISH (Multiplex Fluorescent In-Situ Hybridization) provides
color karyotyping. In this paper, an automated method for M-FISH chromosome
segmentation based on watershed transform followed by naive Bayes
classification of each region using the features, mean and standard deviation,
is presented. Also, a post processing step is added to re-classify the small
chromosome segments to the neighboring larger segment for reducing the chances
of misclassification. The approach provided improved accuracy when compared to
the pixel-by-pixel approach. The approach was tested on 40 images from the
dataset and achieved an accuracy of 84.21 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2040</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2040</id><created>2012-05-09</created><updated>2014-01-09</updated><authors><author><keyname>Eisenberg-Nagy</keyname><forenames>Marianna</forenames></author><author><keyname>Laurent</keyname><forenames>Monique</forenames></author><author><keyname>Varvitsiotis</keyname><forenames>Antonios</forenames></author></authors><title>Forbidden minor characterizations for low-rank optimal solutions to
  semidefinite programs over the elliptope</title><categories>math.CO cs.DM math.OC</categories><comments>33 pages, 8 Figures. In its second version, the paper has been
  modified to accommodate the suggestions of the referees. Furthermore, the
  title has been changed since we feel that the new title reflects more
  accurately the content and the main results of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new geometric graph parameter $\egd(G)$, defined as the smallest
integer $r\ge 1$ for which any partial symmetric matrix which is completable to
a correlation matrix and whose entries are specified at the positions of the
edges of $G$, can be completed to a matrix in the convex hull of correlation
matrices of $\rank $ at most $r$. This graph parameter is motivated by its
relevance to the problem of finding low rank solutions to semidefinite programs
over the elliptope, and also by its relevance to the bounded rank Grothendieck
constant. Indeed, $\egd(G)\le r$ if and only if the rank-$r$ Grothendieck
constant of $G$ is equal to 1. We show that the parameter $\egd(G)$ is minor
monotone, we identify several classes of forbidden minors for $\egd(G)\le r$
and we give the full characterization for the case $r=2$. We also show an upper
bound for $\egd(G)$ in terms of a new tree-width-like parameter $\sla(G)$,
defined as the smallest $r$ for which $G$ is a minor of the strong product of a
tree and $K_r$. We show that, for any 2-connected graph $G\ne K_{3,3}$ on at
least 6 nodes, $\egd(G)\le 2$ if and only if $\sla(G)\le 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2046</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2046</id><created>2012-05-09</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Multiset Estimates and Combinatorial Synthesis</title><categories>cs.SY cs.AI math.OC</categories><comments>30 pages, 24 figures, 10 tables</comments><msc-class>68T20, 06A06, 68T30, 68T35, 90B40, 90C27, 90C29, 90C39, 90C59</msc-class><acm-class>J.6; I.2.8; E.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses an approach to ordinal assessment of alternatives based
on assignment of elements into an ordinal scale. Basic versions of the
assessment problems are formulated while taking into account the number of
levels at a basic ordinal scale [1,2,...,l] and the number of assigned elements
(e.g., 1,2,3). The obtained estimates are multisets (or bags) (cardinality of
the multiset equals a constant). Scale-posets for the examined assessment
problems are presented. 'Interval multiset estimates' are suggested. Further,
operations over multiset estimates are examined: (a) integration of multiset
estimates, (b) proximity for multiset estimates, (c) comparison of multiset
estimates, (d) aggregation of multiset estimates, and (e) alignment of multiset
estimates. Combinatorial synthesis based on morphological approach is examined
including the modified version of the approach with multiset estimates of
design alternatives. Knapsack-like problems with multiset estimates are briefly
described as well. The assessment approach, multiset-estimates, and
corresponding combinatorial problems are illustrated by numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2048</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2048</id><created>2012-05-09</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Unfolding Prismatoids as Convex Patches: Counterexamples and Positive
  Results</title><categories>cs.CG</categories><comments>This paper was prepared for but never submitted to CCCG'12. 12
  two-column pages. 27 figures</comments><msc-class>52B10</msc-class><acm-class>F.2.2; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the unsolved problem of unfolding prismatoids in a new context,
viewing a &quot;topless prismatoid&quot; as a convex patch---a polyhedral subset of the
surface of a convex polyhedron homeomorphic to a disk. We show that several
natural strategies for unfolding a prismatoid can fail, but obtain a positive
result for &quot;petal unfolding&quot; topless prismatoids. We also show that the natural
extension to a convex patch consisting of a face of a polyhedron and all its
incident faces, does not always have a nonoverlapping petal unfolding. However,
we obtain a positive result by excluding the problematical patches. This then
leads a positive result for restricted prismatoids. Finally, we suggest suggest
studying the unfolding of convex patches in general, and offer some possible
lines of investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2051</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2051</id><created>2012-05-09</created><updated>2013-12-21</updated><authors><author><keyname>Hella</keyname><forenames>Lauri</forenames></author><author><keyname>J&#xe4;rvisalo</keyname><forenames>Matti</forenames></author><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author><author><keyname>Laurinharju</keyname><forenames>Juhana</forenames></author><author><keyname>Lempi&#xe4;inen</keyname><forenames>Tuomo</forenames></author><author><keyname>Luosto</keyname><forenames>Kerkko</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>Weak Models of Distributed Computing, with Connections to Modal Logic</title><categories>cs.DC cs.LO</categories><comments>1 + 40 pages, 9 figures</comments><doi>10.1007/s00446-013-0202-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a classification of weak models of distributed computing.
We focus on deterministic distributed algorithms, and study models of computing
that are weaker versions of the widely-studied port-numbering model. In the
port-numbering model, a node of degree d receives messages through d input
ports and sends messages through d output ports, both numbered with 1,2,...,d.
In this work, VVc is the class of all graph problems that can be solved in the
standard port-numbering model. We study the following subclasses of VVc:
  VV: Input port i and output port i are not necessarily connected to the same
neighbour.
  MV: Input ports are not numbered; algorithms receive a multiset of messages.
  SV: Input ports are not numbered; algorithms receive a set of messages.
  VB: Output ports are not numbered; algorithms send the same message to all
output ports.
  MB: Combination of MV and VB.
  SB: Combination of SV and VB.
  Now we have many trivial containment relations, such as SB \subseteq MB
\subseteq VB \subseteq VV \subseteq VVc, but it is not obvious if, for example,
either of VB \subseteq SV or SV \subseteq VB should hold. Nevertheless, it
turns out that we can identify a linear order on these classes. We prove that
SB \subsetneq MB = VB \subsetneq SV = MV = VV \subsetneq VVc. The same holds
for the constant-time versions of these classes.
  We also show that the constant-time variants of these classes can be
characterised by a corresponding modal logic. Hence the linear order identified
in this work has direct implications in the study of the expressibility of
modal logic. Conversely, one can use tools from modal logic to study these
classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2053</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2053</id><created>2012-05-09</created><authors><author><keyname>Sanghoi</keyname><forenames>Pavani</forenames></author><author><keyname>Kansal</keyname><forenames>Lavish</forenames></author></authors><title>Analysis of WiMAX Physical Layer Using Spatial Multiplexing</title><categories>cs.OH</categories><comments>14 pages, 12 figures, 1 table; International Journal of Wireless &amp;
  Mobile Networks (IJWMN) Vol. 4, No. 2, April 2012</comments><doi>10.5120/6263-8414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadband Wireless Access (BWA) has emerged as a promising solution for
providing last mile internet access technology to provide high speed internet
access to the users in the residential as well as in the small and medium sized
enterprise sectors. IEEE 802.16e is one of the most promising and attractive
candidate among the emerging technologies for broadband wireless access. The
emergence of WiMAX protocol has attracted various interests from almost all the
fields of wireless communications. MIMO systems which are created according to
the IEEE 802.16-2005 standard (WiMAX) under different fading channels can be
implemented to get the benefits of both the MIMO and WiMAX technologies. In
this paper analysis of higher level of modulations (i.e. M-PSK and M-QAM for
different values of M) with different code rates and on WiMAX-MIMO system is
presented for Rayleigh channel by focusing on spatial multiplexing MIMO
technique. Signal-to Noise Ratio (SNR) vs Bit Error Rate (BER) analysis has
been done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2056</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2056</id><created>2012-05-09</created><authors><author><keyname>Rossi</keyname><forenames>Ryan</forenames></author><author><keyname>Gallagher</keyname><forenames>Brian</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author><author><keyname>Henderson</keyname><forenames>Keith</forenames></author></authors><title>Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of real-world networks are dynamic and extremely large (e.g.,
Internet Traffic, Twitter, Facebook, ...). To understand the structural
behavior of nodes in these large dynamic networks, it may be necessary to model
the dynamics of behavioral roles representing the main connectivity patterns
over time. In this paper, we propose a dynamic behavioral mixed-membership
model (DBMM) that captures the roles of nodes in the graph and how they evolve
over time. Unlike other node-centric models, our model is scalable for
analyzing large dynamic networks. In addition, DBMM is flexible,
parameter-free, has no functional form or parameterization, and is
interpretable (identifies explainable patterns). The performance results
indicate our approach can be applied to very large networks while the
experimental results show that our model uncovers interesting patterns
underlying the dynamics of these networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2074</identifier>
 <datestamp>2013-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2074</id><created>2012-05-09</created><updated>2013-11-15</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Racz</keyname><forenames>Miklos Z.</forenames></author></authors><title>A Smooth Transition from Powerlessness to Absolute Power</title><categories>cs.GT math.PR</categories><comments>22 pages; v2 contains minor changes and corrections; v3 contains
  minor changes after comments of reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the phase transition of the coalitional manipulation problem for
generalized scoring rules. Previously it has been shown that, under some
conditions on the distribution of votes, if the number of manipulators is
$o(\sqrt{n})$, where $n$ is the number of voters, then the probability that a
random profile is manipulable by the coalition goes to zero as the number of
voters goes to infinity, whereas if the number of manipulators is
$\omega(\sqrt{n})$, then the probability that a random profile is manipulable
goes to one. Here we consider the critical window, where a coalition has size
$c\sqrt{n}$, and we show that as $c$ goes from zero to infinity, the limiting
probability that a random profile is manipulable goes from zero to one in a
smooth fashion, i.e., there is a smooth phase transition between the two
regimes. This result analytically validates recent empirical results, and
suggests that deciding the coalitional manipulation problem may be of limited
computational hardness in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2077</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2077</id><created>2012-05-09</created><authors><author><keyname>Aly</keyname><forenames>Salah A.</forenames></author><author><keyname>Salim</keyname><forenames>Mohamed</forenames></author></authors><title>Data Dissemination And Collection Algorithms For Collaborative Sensor
  Networks Using Dynamic Cluster Heads</title><categories>cs.NI cs.DS cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop novel data dissemination and collection algorithms for Wireless
Sensor Networks (WSNs) in which we consider $n$ sensor nodes distributed
randomly in a certain field to measure a physical phenomena. Such sensors have
limited energy, shortage coverage range, bandwidth and memory constraints. We
desire to disseminate nodes' data throughout the network such that a base
station will be able to collect the sensed data by querying a small number of
nodes. We propose two data dissemination and collection algorithms (DCA's) to
solve this problem. Data dissemination is achieved through dynamical selection
of some nodes. The selected nodes will be changed after a time slot $t$ and may
be repeated after a period $T$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2081</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2081</id><created>2012-05-09</created><updated>2013-11-04</updated><authors><author><keyname>Tillmann</keyname><forenames>Andreas M.</forenames></author><author><keyname>Pfetsch</keyname><forenames>Marc E.</forenames></author></authors><title>The Computational Complexity of the Restricted Isometry Property, the
  Nullspace Property, and Related Concepts in Compressed Sensing</title><categories>math.OC cs.IT math.IT</categories><comments>13 pages; accepted for publication in IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the computational complexity of conditions which
guarantee that the NP-hard problem of finding the sparsest solution to an
underdetermined linear system can be solved by efficient algorithms. In the
literature, several such conditions have been introduced. The most well-known
ones are the mutual coherence, the restricted isometry property (RIP), and the
nullspace property (NSP). While evaluating the mutual coherence of a given
matrix is easy, it has been suspected for some time that evaluating RIP and NSP
is computationally intractable in general. We confirm these conjectures by
showing that for a given matrix A and positive integer k, computing the best
constants for which the RIP or NSP hold is, in general, NP-hard. These results
are based on the fact that determining the spark of a matrix is NP-hard, which
is also established in this paper. Furthermore, we also give several complexity
statements about problems related to the above concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2107</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2107</id><created>2012-05-09</created><updated>2012-09-25</updated><authors><author><keyname>Petschow</keyname><forenames>Matthias</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>High-Performance Solvers for Dense Hermitian Eigenproblems</title><categories>cs.MS cs.NA</categories><report-no>AICES-2011/09-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new collection of solvers - subsequently called EleMRRR - for
large-scale dense Hermitian eigenproblems. EleMRRR solves various types of
problems: generalized, standard, and tridiagonal eigenproblems. Among these,
the last is of particular importance as it is a solver on its own right, as
well as the computational kernel for the first two; we present a fast and
scalable tridiagonal solver based on the Algorithm of Multiple Relatively
Robust Representations - referred to as PMRRR. Like the other EleMRRR solvers,
PMRRR is part of the freely available Elemental library, and is designed to
fully support both message-passing (MPI) and multithreading parallelism (SMP).
As a result, the solvers can equally be used in pure MPI or in hybrid MPI-SMP
fashion. We conducted a thorough performance study of EleMRRR and ScaLAPACK's
solvers on two supercomputers. Such a study, performed with up to 8,192 cores,
provides precise guidelines to assemble the fastest solver within the ScaLAPACK
framework; it also indicates that EleMRRR outperforms even the fastest solvers
built from ScaLAPACK's components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2114</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2114</id><created>2012-05-09</created><updated>2013-01-09</updated><authors><author><keyname>Velden</keyname><forenames>Theresa</forenames></author><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author></authors><title>The Extraction of Community Structures from Publication Networks to
  Support Ethnographic Observations of Field Differences in Scientific
  Communication</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>Accepted for publication in JASIST</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The scientific community of researchers in a research specialty is an
important unit of analysis for understanding the field specific shaping of
scientific communication practices. These scientific communities are, however,
a challenging unit of analysis to capture and compare because they overlap,
have fuzzy boundaries, and evolve over time. We describe a network analytic
approach that reveals the complexities of these communities through examination
of their publication networks in combination with insights from ethnographic
field studies. We suggest that the structures revealed indicate overlapping
sub- communities within a research specialty and we provide evidence that they
differ in disciplinary orientation and research practices. By mapping the
community structures of scientific fields we aim to increase confidence about
the domain of validity of ethnographic observations as well as of collaborative
patterns extracted from publication networks thereby enabling the systematic
study of field differences. The network analytic methods presented include
methods to optimize the delineation of a bibliographic data set in order to
adequately represent a research specialty, and methods to extract community
structures from this data. We demonstrate the application of these methods in a
case study of two research specialties in the physical and chemical sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2117</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2117</id><created>2012-05-09</created><updated>2015-07-28</updated><authors><author><keyname>Luttik</keyname><forenames>Bas</forenames></author></authors><title>Unique Parallel Decomposition in Branching and Weak Bisimulation
  Semantics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the property of unique parallel decomposition modulo branching
and weak bisimilarity. First, we show that infinite behaviours may fail to have
parallel decompositions at all. Then, we prove that totally normed behaviours
always have parallel decompositions, but that these are not necessarily unique.
Finally, we establish that weakly bounded behaviours have unique parallel
decompositions. We derive the latter result from a general theorem about unique
decompositions in partial commutative monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2118</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2118</id><created>2012-05-09</created><updated>2014-10-20</updated><authors><author><keyname>Polak</keyname><forenames>Adam C.</forenames></author><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis L.</forenames></author></authors><title>Performance Bounds for Grouped Incoherent Measurements in Compressive
  Sensing</title><categories>cs.IT math.IT</categories><comments>Revised for publication. 21 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) allows for acquisition of sparse signals at sampling
rates significantly lower than the Nyquist rate required for bandlimited
signals. Recovery guarantees for CS are generally derived based on the
assumption that measurement projections are selected independently at random.
However, for many practical signal acquisition applications, including medical
imaging and remote sensing, this assumption is violated as the projections must
be taken in groups. In this paper, we consider such applications and derive
requirements on the number of measurements needed for successful recovery of
signals when groups of dependent projections are taken at random. We find a
penalty factor on the number of required measurements with respect to the
standard CS scheme that employs conventional independent measurement selection
and evaluate the accuracy of the predicted penalty through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2129</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2129</id><created>2012-05-09</created><updated>2013-09-27</updated><authors><author><keyname>Nguyen</keyname><forenames>Vinh Phu</forenames></author><author><keyname>Bordas</keyname><forenames>St&#xe9;phane P. A.</forenames></author><author><keyname>Rabczuk</keyname><forenames>Timon</forenames></author></authors><title>Isogeometric analysis: an overview and computer implementation aspects</title><categories>cs.NA cs.MS math.NA</categories><doi>10.1016/j.matcom.2015.05.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isogeometric analysis (IGA) represents a recently developed technology in
computational mechanics that offers the possibility of integrating methods for
analysis and Computer Aided Design (CAD) into a single, unified process. The
implications to practical engineering design scenarios are profound, since the
time taken from design to analysis is greatly reduced, leading to dramatic
gains in efficiency. The tight coupling of CAD and analysis within IGA requires
knowledge from both fields and it is one of the goals of the present paper to
outline much of the commonly used notation. In this manuscript, through a clear
and simple Matlab implementation, we present an introduction to IGA applied to
the Finite Element (FE) method and related computer implementation aspects.
Furthermore, implemen- tation of the extended IGA which incorporates enrichment
functions through the partition of unity method (PUM) is also presented, where
several examples for both two-dimensional and three-dimensional fracture are
illustrated. The open source Matlab code which accompanies the present paper
can be applied to one, two and three-dimensional problems for linear
elasticity, linear elastic fracture mechanics, structural mechanics
(beams/plates/shells including large displacements and rotations) and Poisson
problems with or without enrichment. The Bezier extraction concept that allows
FE analysis to be performed efficiently on T-spline geometries is also
incorporated. The article includes a summary of recent trends and developments
within the field of IGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2141</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2141</id><created>2012-05-09</created><authors><author><keyname>Sun</keyname><forenames>Huanhuan</forenames></author><author><keyname>Zhang</keyname><forenames>Taotao</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Separating the Wheat from the Chaff: Sensing Wireless Microphones in
  TVWS</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE DySPAN 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes our attempts to establish a systematic approach that
overcomes a key difficulty in sensing wireless microphone signals, namely, the
inability for most existing detection methods to effectively distinguish
between a wireless microphone signal and a sinusoidal continuous wave (CW).
Such an inability has led to an excessively high false alarm rate and thus
severely limited the utility of sensing-based cognitive transmission in the TV
white space (TVWS) spectrum. Having recognized the root of the difficulty, we
propose two potential solutions. The first solution focuses on the periodogram
as an estimate of the power spectral density (PSD), utilizing the property that
a CW has a line spectral component while a wireless microphone signal has a
slightly dispersed PSD. In that approach, we formulate the resulting decision
model as an one-sided test for Gaussian vectors, based on Kullback-Leibler
distance type of decision statistics. The second solution goes beyond the PSD
and looks into the spectral correlation function (SCF), proposing an augmented
SCF that is capable of revealing more features in the cycle frequency domain
compared with the conventional SCF. Thus the augmented SCF exhibits the key
difference between CW and wireless microphone signals. Both simulation results
and experimental validation results indicate that the two proposed solutions
are promising for sensing wireless microphones in TVWS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2151</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2151</id><created>2012-05-09</created><authors><author><keyname>Mirzal</keyname><forenames>Andri</forenames></author></authors><title>A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix
  Factorization with Automatic Regularization Parameters Determination</title><categories>cs.LG</categories><comments>Preliminary result without experimental result</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a converged algorithm for Tikhonov regularized nonnegative matrix
factorization (NMF). We specially choose this regularization because it is
known that Tikhonov regularized least square (LS) is the more preferable form
in solving linear inverse problems than the conventional LS. Because an NMF
problem can be decomposed into LS subproblems, it can be expected that Tikhonov
regularized NMF will be the more appropriate approach in solving NMF problems.
The algorithm is derived using additive update rules which have been shown to
have convergence guarantee. We equip the algorithm with a mechanism to
automatically determine the regularization parameters based on the L-curve, a
well-known concept in the inverse problems community, but is rather unknown in
the NMF research. The introduction of this algorithm thus solves two inherent
problems in Tikhonov regularized NMF algorithm research, i.e., convergence
guarantee and regularization parameters determination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2152</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2152</id><created>2012-05-09</created><authors><author><keyname>Hameed</keyname><forenames>Ali</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Roughly Weighted Hierarchical Simple Games</title><categories>math.CO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical simple games - both disjunctive and conjunctive - are natural
generalizations of simple majority games. They take their origin in the theory
of secret sharing. Another important generalization of simple majority games
with origin in economics and politics are weighted and roughly weighted
majority games. In this paper we characterize roughly weighted hierarchical
games identifying where the two approaches coincide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2153</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2153</id><created>2012-05-10</created><authors><author><keyname>Paul</keyname><forenames>Rourab</forenames></author><author><keyname>Saha</keyname><forenames>Sangeet</forenames></author><author><keyname>Sau</keyname><forenames>Suman</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>Design and implementation of real time AES-128 on real time operating
  system for multiple FPGA communication</title><categories>cs.AR</categories><comments>6 pages, IEMCON 12, Kolkata</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security is the most important part in data communication system, where more
randomization in secret keys increases the security as well as complexity of
the cryptography algorithms. As a result in recent dates these algorithms are
compensating with enormous memory spaces and large execution time on hardware
platform. Field programmable gate arrays (FPGAs), provide one of the major
alternative in hardware platform scenario due to its reconfiguration nature,
low price and marketing speed. In FPGA based embedded system we can use
embedded processor to execute particular algorithm with the inclusion of a real
time operating System (RTOS), where threads may reduce resource utilization and
time consumption. A process in the runtime is separated in different smaller
tasks which are executed by the scheduler to meet the real time dead line using
RTOS. In this paper we demonstrate the design and implementation of a 128-bit
Advanced Encryption Standard (AES) both symmetric key encryption and decryption
algorithm by developing suitable hardware and software design on Xilinx
Spartan- 3E (XC3S500E-FG320) device using an Xilkernel RTOS, the implementation
has been tested successfully The system is optimized in terms of execution
speed and hardware utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2164</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2164</id><created>2012-05-10</created><authors><author><keyname>Kumar</keyname><forenames>Ankit</forenames></author><author><keyname>Patnaik</keyname><forenames>Tushar</forenames></author><author><keyname>Verma</keyname><forenames>Vivek Kr</forenames></author></authors><title>Discrimination of English to other Indian languages (Kannada and Hindi)
  for OCR system</title><categories>cs.CV</categories><comments>9 Pages, 5 Figure, 5 Tables, International Journal of Computer
  Science, Engineering and Applications (IJCSEA) Vol.2, No.2, April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  India is a multilingual multi-script country. In every state of India there
are two languages one is state local language and the other is English. For
example in Andhra Pradesh, a state in India, the document may contain text
words in English and Telugu script. For Optical Character Recognition (OCR) of
such a bilingual document, it is necessary to identify the script before
feeding the text words to the OCRs of individual scripts. In this paper, we are
introducing a simple and efficient technique of script identification for
Kannada, English and Hindi text words of a printed document. The proposed
approach is based on the horizontal and vertical projection profile for the
discrimination of the three scripts. The feature extraction is done based on
the horizontal projection profile of each text words. We analysed 700 different
words of Kannada, English and Hindi in order to extract the discrimination
features and for the development of knowledge base. We use the horizontal
projection profile of each text word and based on the horizontal projection
profile we extract the appropriate features. The proposed system is tested on
100 different document images containing more than 1000 text words of each
script and a classification rate of 98.25%, 99.25% and 98.87% is achieved for
Kannada, English and Hindi respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2170</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2170</id><created>2012-05-10</created><authors><author><keyname>Feinerman</keyname><forenames>Ofer</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Korman</keyname><forenames>Amos</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames><affiliation>CSE</affiliation></author><author><keyname>Sereni</keyname><forenames>Jean-S&#xe9;bastien</forenames><affiliation>LIAFA</affiliation></author></authors><title>Collaborative Search on the Plane without Communication</title><categories>cs.DC cs.DM</categories><comments>To appear in the Proceedings of the 31st Annual ACM SIGACT-SIGOPS
  Symposium on Principles of Distributed Computing (PODC'12)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use distributed computing tools to provide a new perspective on the
behavior of cooperative biological ensembles. We introduce the Ants Nearby
Treasure Search (ANTS) problem, a generalization of the classical cow-path
problem, which is relevant for collective foraging in animal groups. In the
ANTS problem, k identical (probabilistic) agents, initially placed at some
central location, collectively search for a treasure in the two-dimensional
plane. The treasure is placed at a target location by an adversary and the goal
is to find it as fast as possible as a function of both k and D, where D is the
distance between the central location and the target. This is biologically
motivated by cooperative, central place foraging, such as performed by ants
around their nest. In this type of search there is a strong preference to
locate nearby food sources before those that are further away. We focus on
trying to find what can be achieved if communication is limited or altogether
absent. Indeed, to avoid overlaps agents must be highly dispersed making
communication difficult. Furthermore, if the agents do not commence the search
in synchrony, then even initial communication is problematic. This holds, in
particular, with respect to the question of whether the agents can communicate
and conclude their total number, k. It turns out that the knowledge of k by the
individual agents is crucial for performance. Indeed, it is a straightforward
observation that the time required for finding the treasure is \Omega(D+
D^2/k), and we show in this paper that this bound can be matched if the agents
have knowledge of k up to some constant approximation. We present a tight bound
for the competitive penalty that must be paid, in the running time, if the
agents have no information about k. Specifically, this bound is slightly more
than logarithmic in the number of agents. In addition, we give a lower bound
for the setting in which the agents are given some estimation of k. Informally,
our results imply that the agents can potentially perform well without any
knowledge of their total number k, however, to further improve, they must use
non trivial information regarding k. Finally, we propose a uniform algorithm
that is both efficient and extremely simple, suggesting its relevance for
actual biological scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2171</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2171</id><created>2012-05-10</created><updated>2015-07-15</updated><authors><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Preux</keyname><forenames>Philippe</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>A Generalized Kernel Approach to Structured Output Learning</title><categories>stat.ML cs.LG</categories><comments>in International Conference on Machine Learning (ICML), Jun 2013,
  Atlanta, United States. 2013</comments><proxy>ccsd</proxy><report-no>RR-7956</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of structured output learning from a regression
perspective. We first provide a general formulation of the kernel dependency
estimation (KDE) problem using operator-valued kernels. We show that some of
the existing formulations of this problem are special cases of our framework.
We then propose a covariance-based operator-valued kernel that allows us to
take into account the structure of the kernel feature space. This kernel
operates on the output space and encodes the interactions between the outputs
without any reference to the input space. To address this issue, we introduce a
variant of our KDE method based on the conditional covariance operator that in
addition to the correlation between the outputs takes into account the effects
of the input variables. Finally, we evaluate the performance of our KDE
approach using both covariance and conditional covariance kernels on two
structured output problems, and compare it to the state-of-the-art kernel-based
structured output regression methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2172</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2172</id><created>2012-05-10</created><updated>2012-10-05</updated><authors><author><keyname>Mahrsi</keyname><forenames>Mohamed Khalil El</forenames><affiliation>LTCI</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Modularity-Based Clustering for Network-Constrained Trajectories</title><categories>stat.ML cs.LG physics.data-an</categories><comments>20-th European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN 2012), Bruges : Belgium (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel clustering approach for moving object trajectories that
are constrained by an underlying road network. The approach builds a similarity
graph based on these trajectories then uses modularity-optimization hiearchical
graph clustering to regroup trajectories with similar profiles. Our
experimental study shows the superiority of the proposed approach over classic
hierarchical clustering and gives a brief insight to visualization of the
clustering results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2174</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2174</id><created>2012-05-10</created><updated>2012-12-05</updated><authors><author><keyname>Fominykh</keyname><forenames>F&#xf6;dor</forenames></author><author><keyname>Martyugin</keyname><forenames>Pavel</forenames></author><author><keyname>Volkov</keyname><forenames>Mikhail</forenames></author></authors><title>P(l)aying for Synchronization</title><categories>cs.FL cs.CC</categories><comments>Version 1 (by F\&quot;odor Fominykh and Mikhail Volkov): 12 pages, 5
  figures, close to the version published in the Proceedings of the 17th
  International Conference on Implementation and Application of Automata (LNCS
  7381). Version 2: 19 pages, 7 figures, one of the problems left open in
  Version 1 solved, submitted</comments><msc-class>68Q45, 68Q25</msc-class><acm-class>F.2.2</acm-class><journal-ref>Int. J. Found. Comput. Sci. 24 (2013), 765-780</journal-ref><doi>10.1142/S0129054113400170</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two topics are presented: synchronization games and synchronization costs. In
a synchronization game on a deterministic finite automaton, there are two
players, Alice and Bob, whose moves alternate. Alice wants to synchronize the
given automaton, while Bob aims to make her task as hard as possible. We answer
a few natural questions related to such games. Speaking about synchronization
costs, we consider deterministic automata in which each transition has a
certain price. The problem is whether or not a given automaton can be
synchronized within a given budget. We determine the complexity of this
problem. We also formulate a few open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2177</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2177</id><created>2012-05-10</created><authors><author><keyname>C&#xe1;ceres</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Hernando</keyname><forenames>Carmen</forenames></author><author><keyname>Mora</keyname><forenames>Merc&#xe8;</forenames></author><author><keyname>Pelayo</keyname><forenames>Ignacio M.</forenames></author><author><keyname>Puertas</keyname><forenames>Mar&#xed;a Luz</forenames></author></authors><title>Locating dominating codes: Bounds and extremal cardinalities</title><categories>math.CO cs.IT math.IT</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, two types of codes such that they both dominate and locate the
vertices of a graph are studied. Those codes might be sets of detectors in a
network or processors controlling a system whose set of responses should
determine a malfunctioning processor or an intruder. Here, we present our more
significant contributions on \lambda-codes and \eta-codes concerning concerning
bounds, extremal values and realization theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2187</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2187</id><created>2012-05-10</created><updated>2013-11-14</updated><authors><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Model checking quantum Markov chains</title><categories>quant-ph cs.LO</categories><comments>Journal version</comments><journal-ref>Journal of Computer and System Sciences 2013, 79, 1181-1198</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the security of quantum cryptography is provable based on the
principles of quantum mechanics, it can be compromised by the flaws in the
design of quantum protocols and the noise in their physical implementations.
So, it is indispensable to develop techniques of verifying and debugging
quantum cryptographic systems. Model-checking has proved to be effective in the
verification of classical cryptographic protocols, but an essential difficulty
arises when it is applied to quantum systems: the state space of a quantum
system is always a continuum even when its dimension is finite. To overcome
this difficulty, we introduce a novel notion of quantum Markov chain, specially
suited to model quantum cryptographic protocols, in which quantum effects are
entirely encoded into super-operators labelling transitions, leaving the
location information (nodes) being classical. Then we define a quantum
extension of probabilistic computation tree logic (PCTL) and develop a
model-checking algorithm for quantum Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2200</identifier>
 <datestamp>2012-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2200</id><created>2012-05-10</created><authors><author><keyname>Choy</keyname><forenames>Murphy</forenames></author><author><keyname>Cheong</keyname><forenames>Michelle</forenames></author></authors><title>A Greedy Double Swap Heuristic for Nurse Scheduling</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key challenges of nurse scheduling problem (NSP) is the number of
constraints placed on preparing the timetable, both from the regulatory
requirements as well as the patients' demand for the appropriate nursing care
specialists. In addition, the preferences of the nursing staffs related to
their work schedules add another dimension of complexity. Most solutions
proposed for solving nurse scheduling involve the use of mathematical
programming and generally considers only the hard constraints. However, the
psychological needs of the nurses are ignored and this resulted in subsequent
interventions by the nursing staffs to remedy any deficiency and often results
in last minute changes to the schedule. In this paper, we present a staff
preference optimization framework which is solved with a greedy double swap
heuristic. The heuristic yields good performance in speed at solving the
problem. The heuristic is simple and we will demonstrate its performance by
implementing it on open source spreadsheet software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2234</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2234</id><created>2012-05-10</created><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Approximation Algorithms for Semi-random Graph Partitioning Problems</title><categories>cs.DS cs.CC</categories><comments>To appear at the 44th ACM Symposium on Theory of Computing (STOC
  2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and study a new semi-random model for graph
partitioning problems. We believe that it captures many properties of
real--world instances. The model is more flexible than the semi-random model of
Feige and Kilian and planted random model of Bui, Chaudhuri, Leighton and
Sipser.
  We develop a general framework for solving semi-random instances and apply it
to several problems of interest. We present constant factor bi-criteria
approximation algorithms for semi-random instances of the Balanced Cut,
Multicut, Min Uncut, Sparsest Cut and Small Set Expansion problems. We also
show how to almost recover the optimal solution if the instance satisfies an
additional expanding condition. Our algorithms work in a wider range of
parameters than most algorithms for previously studied random and semi-random
models.
  Additionally, we study a new planted algebraic expander model and develop
constant factor bi-criteria approximation algorithms for graph partitioning
problems in this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2251</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2251</id><created>2012-05-10</created><authors><author><keyname>Krawczyk</keyname><forenames>M. J.</forenames></author><author><keyname>Kulakowski</keyname><forenames>K.</forenames></author></authors><title>Combinatorial aspect of fashion</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulations are performed according to the Axelrod model of culture
dissemination, with modified mechanism of repulsion. Previously, repulsion was
considered by Radillo-Diaz et al (Phys. Rev. E 80 (2009) 066107) as dependent
on a predefined threshold. Here the probabilities of attraction and repulsion
are calculated from the number of cells in the same states. We also investigate
the influence of some homogeneity, introduced to the initial state. As the
result of the probabilistic definition of repulsion, the ordered state
vanishes. A small cluster of a few percent of population is retained only if in
the initial state a set of agents is prepared in the same state. We conclude
that the modelled imitation is successful only with respect to agents, and not
only their features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2263</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2263</id><created>2012-01-18</created><authors><author><keyname>Padma</keyname><forenames>S.</forenames></author><author><keyname>Seshasaayee</keyname><forenames>Ananthi</forenames></author></authors><title>Maximum Spanning Tree Model on Personalized Web Based Collaborative
  Learning in Web 3.0</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Web 3.0 is an evolving extension of the current web environme bnt.
Information in web 3.0 can be collaborated and communicated when queried. Web
3.0 architecture provides an excellent learning experience to the students. Web
3.0 is 3D, media centric and semantic. Web based learning has been on high in
recent days. Web 3.0 has intelligent agents as tutors to collect and
disseminate the answers to the queries by the students. Completely Interactive
learner's query determine the customization of the intelligent tutor. This
paper analyses the Web 3.0 learning environment attributes. A Maximum spanning
tree model for the personalized web based collaborative learning is designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2265</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2265</id><created>2012-05-08</created><updated>2012-10-04</updated><authors><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Efficient Constrained Regret Minimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning constitutes a mathematical and compelling framework to
analyze sequential decision making problems in adversarial environments. The
learner repeatedly chooses an action, the environment responds with an outcome,
and then the learner receives a reward for the played action. The goal of the
learner is to maximize his total reward. However, there are situations in
which, in addition to maximizing the cumulative reward, there are some
additional constraints on the sequence of decisions that must be satisfied on
average by the learner. In this paper we study an extension to the online
learning where the learner aims to maximize the total reward given that some
additional constraints need to be satisfied. By leveraging on the theory of
Lagrangian method in constrained optimization, we propose Lagrangian
exponentially weighted average (LEWA) algorithm, which is a primal-dual variant
of the well known exponentially weighted average algorithm, to efficiently
solve constrained online decision making problems. Using novel theoretical
analysis, we establish the regret and the violation of the constraint bounds in
full information and bandit feedback models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2269</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2269</id><created>2012-05-09</created><authors><author><keyname>Sengar</keyname><forenames>Suverna</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Partha Pratim</forenames></author></authors><title>Performance improvement in OFDM system by PAPR reduction</title><categories>cs.NI</categories><comments>13 pages, 8 figures, 1 Table, Signal &amp; Image Processing : An
  International Journal (SIPIJ) Vol.3, No.2, April 2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Orthogonal Frequency Division Multiplexing (OFDM) is an efficient method of
data transmission for high speed communication systems. However, the main
drawback of OFDM system is the high Peak to Average Power Ratio (PAPR) of the
transmitted signals. OFDM consist of large number of independent subcarriers,
as a result of which the amplitude of such a signal can have high peak values.
Coding, phase rotation and clipping are among many PAPR reduction schemes that
have been proposed to overcome this problem. Here two different PAPR reduction
methods e.g. partial transmit sequence (PTS) and selective mapping (SLM) are
used to reduce PAPR. Significant reduction in PAPR has been achieved using
these techniques. The performances of the two methods are then compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2282</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2282</id><created>2012-05-10</created><authors><author><keyname>Durut</keyname><forenames>Matthieu</forenames><affiliation>LTCI</affiliation></author><author><keyname>Patra</keyname><forenames>Beno&#xee;t</forenames><affiliation>LSTA</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>A Discussion on Parallelization Schemes for Stochastic Vector
  Quantization Algorithms</title><categories>stat.ML cs.DC cs.LG</categories><proxy>ccsd</proxy><journal-ref>20-th European Symposium on Artificial Neural Networks,
  Computational Intelligence and Machine Learning (ESANN 2012), Bruges :
  Belgium (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies parallelization schemes for stochastic Vector Quantization
algorithms in order to obtain time speed-ups using distributed resources. We
show that the most intuitive parallelization scheme does not lead to better
performances than the sequential algorithm. Another distributed scheme is
therefore introduced which obtains the expected speed-ups. Then, it is improved
to fit implementation on distributed architectures where communications are
slow and inter-machines synchronization too costly. The schemes are tested with
simulated distributed architectures and, for the last one, with Microsoft
Windows Azure platform obtaining speed-ups up to 32 Virtual Machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2285</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2285</id><created>2012-05-10</created><updated>2014-01-26</updated><authors><author><keyname>Yu</keyname><forenames>Lan</forenames></author><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author></authors><title>Complex-Demand Knapsack Problems and Incentives in AC Power Systems</title><categories>cs.DS</categories><comments>Appears in: Proceedings of the 12th International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2013)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider AC electrical systems where each electrical device has a power
demand expressed as a complex number, and there is a limit on the magnitude of
total power supply. Motivated by this scenario, we introduce the complex-demand
knapsack problem (C-KP), a new variation of the traditional knapsack problem,
where each item is associated with a demand as a complex number, rather than a
real number often interpreted as weight or size of the item. While keeping the
same goal as to maximize the sum of values of the selected items, we put the
capacity limit on the magnitude of the sum of satisfied demands. For C-KP, we
prove its inapproximability by FPTAS (unless P = NP), as well as presenting a
(1/2-epsilon)-approximation algorithm. Furthermore, we investigate the selfish
multi-agent setting where each agent is in charge of one item, and an agent may
misreport the demand and value of his item for his own interest. We show a
simple way to adapt our approximation algorithm to be monotone, which is
sufficient for the existence of incentive compatible payments such that no
agent has an incentive to misreport. Our results shed insight on the design of
multi-agent systems for smart grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2292</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2292</id><created>2012-05-10</created><authors><author><keyname>Stavrakas</keyname><forenames>Yannis</forenames></author><author><keyname>Papastefanatos</keyname><forenames>George</forenames></author><author><keyname>Dalamagas</keyname><forenames>Theodore</forenames></author><author><keyname>Christophides</keyname><forenames>Vassilis</forenames></author></authors><title>Diachronic Linked Data: Towards Long-Term Preservation of Structured
  Interrelated Information</title><categories>cs.DB cs.DL</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Linked Data Paradigm is one of the most promising technologies for
publishing, sharing, and connecting data on the Web, and offers a new way for
data integration and interoperability. However, the proliferation of
distributed, inter-connected sources of information and services on the Web
poses significant new challenges for managing consistently a huge number of
large datasets and their interdependencies. In this paper we focus on the key
problem of preserving evolving structured interlinked data. We argue that a
number of issues that hinder applications and users are related to the temporal
aspect that is intrinsic in linked data. We present a number of real use cases
to motivate our approach, we discuss the problems that occur, and propose a
direction for a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2310</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2310</id><created>2012-05-10</created><updated>2013-10-13</updated><authors><author><keyname>De Felice</keyname><forenames>Clelia</forenames></author></authors><title>A note on the factorization conjecture</title><categories>cs.FL</categories><journal-ref>Acta Informatica, vol. 50, Issue 7, p. 381-402, 2013</journal-ref><doi>10.1007/s00236-013-0187-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give partial results on the factorization conjecture on codes proposed by
Schutzenberger. We consider finite maximal codes C over the alphabet A = {a, b}
with C \cap a^* = a^p, for a prime number p. Let P, S in Z &lt;A&gt;, with S = S_0 +
S_1, supp(S_0) \subset a^* and supp(S_1) \subset a^*b supp(S_0). We prove that
if (P,S) is a factorization for C then (P,S) is positive, that is P,S have
coefficients 0,1, and we characterize the structure of these codes. As a
consequence, we prove that if C is a finite maximal code such that each word in
C has at most 4 occurrences of b's and a^p is in C, then each factorization for
C is a positive factorization. We also discuss the structure of these codes.
The obtained results show once again relations between (positive)
factorizations and factorizations of cyclic groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2318</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2318</id><created>2012-05-10</created><updated>2012-06-03</updated><authors><author><keyname>Roy</keyname><forenames>Soumen</forenames></author></authors><title>Systems biology beyond degree, hubs and scale-free networks: the case
  for multiple metrics in complex networks</title><categories>q-bio.QM cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>To appear in Systems and Synthetic Biology (Springer)</comments><journal-ref>Systems and Synthetic Biology (2012) 6:31-34</journal-ref><doi>10.1007/s11693-012-9094-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling and topological analysis of networks in biological and other complex
systems, must venture beyond the limited consideration of very few network
metrics like degree, betweenness or assortativity. A proper identification of
informative and redundant entities from many different metrics, using recently
demonstrated techniques, is essential. A holistic comparison of networks and
growth models is best achieved only with the use of such methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2320</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2320</id><created>2012-05-10</created><authors><author><keyname>Dalamagas</keyname><forenames>Theodore</forenames></author><author><keyname>Bikakis</keyname><forenames>Nikos</forenames></author><author><keyname>Papastefanatos</keyname><forenames>George</forenames></author><author><keyname>Stavrakas</keyname><forenames>Yannis</forenames></author><author><keyname>Hatzigeorgiou</keyname><forenames>Artemis G.</forenames></author></authors><title>Publishing Life Science Data as Linked Open Data: the Case Study of
  miRBase</title><categories>cs.DB</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (arXiv:1204.3726)</comments><report-no>WOD/2012/NANTES/7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our Linked Open Data (LOD) infrastructures for genomic
and experimental data related to microRNA biomolecules. Legacy data from two
well-known microRNA databases with experimental data and observations, as well
as change and version information about microRNA entities, are fused and
exported as LOD. Our LOD server assists biologists to explore biological
entities and their evolution, and provides a SPARQL endpoint for applications
and services to query historical miRNA data and track changes, their causes and
effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2334</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2334</id><created>2012-05-10</created><updated>2012-05-29</updated><authors><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author><author><keyname>Zhang</keyname><forenames>Yong</forenames></author></authors><title>Sparse Approximation via Penalty Decomposition Methods</title><categories>cs.LG math.OC stat.CO stat.ML</categories><comments>31 pages, 3 figures and 9 tables. arXiv admin note: substantial text
  overlap with arXiv:1008.5372</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider sparse approximation problems, that is, general
$l_0$ minimization problems with the $l_0$-&quot;norm&quot; of a vector being a part of
constraints or objective function. In particular, we first study the
first-order optimality conditions for these problems. We then propose penalty
decomposition (PD) methods for solving them in which a sequence of penalty
subproblems are solved by a block coordinate descent (BCD) method. Under some
suitable assumptions, we establish that any accumulation point of the sequence
generated by the PD methods satisfies the first-order optimality conditions of
the problems. Furthermore, for the problems in which the $l_0$ part is the only
nonconvex part, we show that such an accumulation point is a local minimizer of
the problems. In addition, we show that any accumulation point of the sequence
generated by the BCD method is a saddle point of the penalty subproblem.
Moreover, for the problems in which the $l_0$ part is the only nonconvex part,
we establish that such an accumulation point is a local minimizer of the
penalty subproblem. Finally, we test the performance of our PD methods by
applying them to sparse logistic regression, sparse inverse covariance
selection, and compressed sensing problems. The computational results
demonstrate that our methods generally outperform the existing methods in terms
of solution quality and/or speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2340</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2340</id><created>2012-05-10</created><authors><author><keyname>Thakur</keyname><forenames>Manoj Rameshchandra</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Multi-Dimensional approach towards Intrusion Detection System</title><categories>cs.CR</categories><comments>8 pages, 3 Figures, 4 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we suggest a multi-dimensional approach towards intrusion
detection. Network and system usage parameters like source and destination IP
addresses; source and destination ports; incoming and outgoing network traffic
data rate and number of CPU cycles per request are divided into multiple
dimensions. Rather than analyzing raw bytes of data corresponding to the values
of the network parameters, a mature function is inferred during the training
phase for each dimension. This mature function takes a dimension value as an
input and returns a value that represents the level of abnormality in the
system usage with respect to that dimension. This mature function is referred
to as Individual Anomaly Indicator. Individual Anomaly Indicators recorded for
each of the dimensions are then used to generate a Global Anomaly Indicator, a
function with n variables (n is the number of dimensions) that provides the
Global Anomaly Factor, an indicator of anomaly in the system usage based on all
the dimensions considered together. The Global Anomaly Indicator inferred
during the training phase is then used to detect anomaly in the network traffic
during the detection phase. Network traffic data encountered during the
detection phase is fed back to the system to improve the maturity of the
Individual Anomaly Indicators and hence the Global Anomaly Indicator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2345</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2345</id><created>2012-05-10</created><authors><author><keyname>Zawbaa</keyname><forenames>Hossam</forenames></author><author><keyname>Aly</keyname><forenames>Salah A.</forenames></author></authors><title>Hajj and Umrah Event Recognition Datasets</title><categories>cs.CV cs.CY</categories><comments>4 pages, 18 figures with 33 images</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, new Hajj and Umrah Event Recognition datasets (HUER) are
presented. The demonstrated datasets are based on videos and images taken
during 2011-2012 Hajj and Umrah seasons. HUER is the first collection of
datasets covering the six types of Hajj and Umrah ritual events (rotating in
Tawaf around Kabaa, performing Sa'y between Safa and Marwa, standing on the
mount of Arafat, staying overnight in Muzdalifah, staying two or three days in
Mina, and throwing Jamarat). The HUER datasets also contain video and image
databases for nine types of human actions during Hajj and Umrah (walking,
drinking from Zamzam water, sleeping, smiling, eating, praying, sitting,
shaving hairs and ablutions, reading the holy Quran and making duaa). The
spatial resolutions are 1280 x 720 pixels for images and 640 x 480 pixels for
videos and have lengths of 20 seconds in average with 30 frame per second
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2350</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2350</id><created>2012-05-10</created><authors><author><keyname>Medjiah</keyname><forenames>Samir</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Krief</keyname><forenames>Francine</forenames><affiliation>LaBRI</affiliation></author></authors><title>AGEM: Adaptive Greedy-Compass Energy-aware Multipath Routing Protocol
  for WMSNs</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>7th IEEE Consumer Communications and Networking Conference (CCNC),
  2010, Las Vegas : United States (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an Adaptive Greedy-compass Energy-aware Multipath
protocol (AGEM), a novel routing protocol for wireless multimedia sensors
networks (WMSNs). AGEM uses sensors node positions to make packet forwarding
decisions. These decisions are made online, at each forwarding node, in such a
way that there is no need for global network topology knowledge and
maintenance. AGEM routing protocol performs load-balancing to minimize energy
consumption among nodes using twofold policy: (1) smart greedy forwarding,
based on adaptive compass and (2) walking back forwarding to avoid holes.
Performance evaluations of AGEM compared to GPSR (Greedy Perimeter Stateless
Routing) show that AGEM can: (a) maximize the network lifetime, (b) guarantee
quality of service for video stream transmission, and (c) scale better on
densely deployed wireless sensors network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2352</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2352</id><created>2012-05-10</created><authors><author><keyname>Medjiah</keyname><forenames>Samir</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author></authors><title>Orion Routing Protocol for Delay-Tolerant Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>IEEE International Conference on Communications (ICC), 2011, Kyoto
  : Japan (2011)</journal-ref><doi>10.1109/icc.2011.5963362</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of efficient routing in delay tolerant
network. We propose a new routing protocol dubbed as ORION. In ORION, only a
single copy of a data packet is kept in the network and transmitted, contact by
contact, towards the destination. The aim of the ORION routing protocol is
twofold: on one hand, it enhances the delivery ratio in networks where an
end-to-end path does not necessarily exist, and on the other hand, it minimizes
the routing delay and the network overhead to achieve better performance. In
ORION, nodes are aware of their neighborhood by the mean of actual and
statistical estimation of new contacts. ORION makes use of autoregressive
moving average (ARMA) stochastic processes for best contact prediction and
geographical coordinates for optimal greedy data packet forwarding. Simulation
results have demonstrated that ORION outperforms other existing DTN routing
protocols such as PRoPHET in terms of end-to-end delay, packet delivery ratio,
hop count and first packet arrival.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2355</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2355</id><created>2012-05-10</created><authors><author><keyname>Medjiah</keyname><forenames>Samir</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Krief</keyname><forenames>Francine</forenames><affiliation>LaBRI</affiliation></author></authors><title>GEAMS: a Greedy Energy-Aware Multipath Stream-based Routing Protocol for
  WMSNs</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>IEEE 2nd Global Information Infrastructure Symposium (GIIS), 2009,
  Hammamet : Tunisia (2009)</journal-ref><doi>10.1109/GIIS.2009.5307078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because sensor nodes operate on power limited batteries, sensor
functionalities have to be designed carefully. In particular, designing
energy-efficient packet forwarding is important to maximize the lifetime of the
network and to minimize the power usage at each node. This paper presents a
Geographic Energy-Aware Multipath Stream-based (GEAMS) routing protocol for
WMSNs. GEAMS routing decisions are made online, at each forwarding node in such
a way that there is no need to global topology knowledge and maintenance. GEAMS
routing protocol performs load-balancing to minimize energy consumption among
nodes using twofold policy: (1) smart greedy forwarding and (2) walking back
forwarding. Performances evaluations of GEAMS show that it can maximize the
network lifetime and guarantee quality of service for video stream transmission
in WMSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2357</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2357</id><created>2012-05-10</created><authors><author><keyname>Medjiah</keyname><forenames>Samir</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Asgari</keyname><forenames>Abolghasem Hamid</forenames><affiliation>TRT UK</affiliation></author></authors><title>Streaming multimedia over WMSNs: an online multipath routing protocol</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>International Journal of Sensor Networks 11, 1 (2012) 10-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing is a challenge to Wireless Multimedia Sensor Networks (WMSNs) for
supporting multimedia applications due to nodes' energy constraints and
computational capabilities, and the ways sensor nodes obtain forwarding
information. In this paper, we propose an online multipath routing protocol
that uses nodes' positions to make forwarding decisions at each hop. Real-time
decisions are made without any need to have the entire network topology
knowledge. The protocol achieves load-balancing and minimises nodes' energy
consumption by utilizing: (a) smart greedy forwarding scheme for selecting next
hop, and (b) walking back forwarding scheme to bypass network holes.
Performance comparisons of the proposed protocol (schemes) are made with TPGF
and GPSR. The results show that our schemes: (a) maximise the overall network
lifespan by not draining energy from some specific nodes, (b) provide QoS
delivery for video streams by using best nodes along the route, and (c) scale
better in high density WMSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2367</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2367</id><created>2012-05-10</created><authors><author><keyname>Jackson</keyname><forenames>Adrian</forenames></author><author><keyname>Agathokleous</keyname><forenames>Orestis</forenames></author></authors><title>Dynamic Loop Parallelisation</title><categories>cs.PL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regions of nested loops are a common feature of High Performance Computing
(HPC) codes. In shared memory programming models, such as OpenMP, these
structure are the most common source of parallelism. Parallelising these
structures requires the programmers to make a static decision on how
parallelism should be applied. However, depending on the parameters of the
problem and the nature of the code, static decisions on which loop to
parallelise may not be optimial, especially as they do not enable the
exploitation of any runtime characteristics of the execution including changes
to the iterations of the loops to be parallelised.
  We have developed a system that allows a code to make a dynamic choice, at
runtime, of what parallelism is applied to nested loops. Our method for
providing dynamic decisions on which loop to parallelise significantly
outperforms the standard methods for acheiving this through OpenMP (using if
clauses).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2382</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2382</id><created>2012-05-10</created><updated>2015-02-05</updated><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>&#xd6;ztekin</keyname><forenames>Ilke</forenames></author><author><keyname>&#xd6;ztekin</keyname><forenames>Uygar</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author></authors><title>Mesh Learning for Classifying Cognitive Processes</title><categories>cs.NE cs.AI cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A relatively recent advance in cognitive neuroscience has been multi-voxel
pattern analysis (MVPA), which enables researchers to decode brain states
and/or the type of information represented in the brain during a cognitive
operation. MVPA methods utilize machine learning algorithms to distinguish
among types of information or cognitive states represented in the brain, based
on distributed patterns of neural activity. In the current investigation, we
propose a new approach for representation of neural data for pattern analysis,
namely a Mesh Learning Model. In this approach, at each time instant, a star
mesh is formed around each voxel, such that the voxel corresponding to the
center node is surrounded by its p-nearest neighbors. The arc weights of each
mesh are estimated from the voxel intensity values by least squares method. The
estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),
are then used to train a classifier, such as Neural Networks, k-Nearest
Neighbor, Na\&quot;ive Bayes and Support Vector Machines. The proposed Mesh Model
was tested on neuroimaging data acquired via functional magnetic resonance
imaging (fMRI) during a recognition memory experiment using categorized word
lists, employing a previously established experimental paradigm (\&quot;Oztekin &amp;
Badre, 2011). Results suggest that the proposed Mesh Learning approach can
provide an effective algorithm for pattern analysis of brain activity during
cognitive processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2424</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2424</id><created>2012-05-10</created><authors><author><keyname>Zhou</keyname><forenames>Ping</forenames></author><author><keyname>Zhong</keyname><forenames>Yongfeng</forenames></author></authors><title>The citation-based indicator and combined impact indicator - New options
  for measuring impact</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metrics based on percentile ranks (PRs) for measuring scholarly impact
involves complex treatment because of various defects such as overvaluing or
devaluing an object caused by percentile ranking schemes, ignoring precise
citation variation among those ranked next to each other, and inconsistency
caused by additional papers or citations. These defects are especially obvious
in a small-sized dataset. To avoid the complicated treatment of PRs based
metrics, we propose two new indicators - the citation-based indicator (CBI) and
the combined impact indicator (CII). Document types of publications are taken
into account. With the two indicators, one would no more be bothered by complex
issues encountered by PRs based indicators. For a small-sized dataset with less
than 100 papers, special calculation is no more needed. The CBI is based solely
on citation counts and the CII measures the integrate contributions of
publications and citations. Both virtual and empirical data are used so as to
compare the effect of related indicators. The CII and the PRs based indicator
I3 are highly correlated but the former reflects citation impact more and the
latter relates more to publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2425</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2425</id><created>2012-05-10</created><authors><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Pathak</keyname><forenames>Vinayak</forenames></author></authors><title>Flip Distance Between Two Triangulations of a Point-Set is NP-complete</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two triangulations of a convex polygon, computing the minimum number of
flips required to transform one to the other is a long-standing open problem.
It is not known whether the problem is in P or NP-complete. We prove that two
natural generalizations of the problem are NP-complete, namely computing the
minimum number of flips between two triangulations of (1) a polygon with holes;
(2) a set of points in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2428</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2428</id><created>2012-05-11</created><authors><author><keyname>Leduc-Primeau</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Hemati</keyname><forenames>Saied</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Relaxed Half-Stochastic Belief Propagation</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check codes are attractive for high throughput
applications because of their low decoding complexity per bit, but also because
all the codeword bits can be decoded in parallel. However, achieving this in a
circuit implementation is complicated by the number of wires required to
exchange messages between processing nodes. Decoding algorithms that exchange
binary messages are interesting for fully-parallel implementations because they
can reduce the number and the length of the wires, and increase logic density.
This paper introduces the Relaxed Half-Stochastic (RHS) decoding algorithm, a
binary message belief propagation (BP) algorithm that achieves a coding gain
comparable to the best known BP algorithms that use real-valued messages. We
derive the RHS algorithm by starting from the well-known Sum-Product algorithm,
and then derive a low-complexity version suitable for circuit implementation.
We present extensive simulation results on two standardized codes having
different rates and constructions, including low bit error rate results. These
simulations show that RHS can be an advantageous replacement for the existing
state-of-the-art decoding algorithms when targeting fully-parallel
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2432</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2432</id><created>2012-05-11</created><authors><author><keyname>Chauhan</keyname><forenames>Kamal Kumar</forenames></author><author><keyname>Sanger</keyname><forenames>Amit Kumar Singh</forenames></author></authors><title>Securing Mobile Ad hoc Networks:Key Management and Routing</title><categories>cs.CR</categories><comments>11 pages,(65-75)</comments><journal-ref>International Journal on AdHoc Networking Systems (IJANS) Vol. 2,
  No. 2, April 2012</journal-ref><doi>10.5121/ijans.2012.2207</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Secure communication between two nodes in a network depends on reliable key
management systems that generate and distribute keys between communicating
nodes and a secure routing protocol that establishes a route between them. But
due to lack of central server and infrastructure in Mobile Ad hoc Networks
(MANETs), this is major problem to manage the keys in the network. Dynamically
changes in network's topology causes weak trust relationship among the nodes in
the network. In MANETs a mobile node operates as not only end terminal but also
as an intermediate router. Therefore, a multi-hop scenario occurs for
communication in MANETs; where there may be one or more malicious nodes in
between source and destination. A routing protocol is said to be secure that
detects the detrimental effects of malicious node(s in the path from source to
destination). In this paper, we proposed a key management scheme and a secure
routing protocol that secures on demand routing protocol such as DSR and AODV.
We assume that MANETs is divided into groups having a group leader in each
group. Group leader has responsibility of key management in its group. Proposed
key management scheme is a decentralized scheme that does not require any
Trusted Third Party (TTP) for key management. In proposed key management
system, both a new node and group leader authenticates each other mutually
before joining the network. While proposed secure routing protocol allows both
communicating parties as well as intermediate nodes to authenticate other nodes
and maintains message integrity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2450</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2450</id><created>2012-05-11</created><authors><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author><author><keyname>Lu</keyname><forenames>Wu-Sheng</forenames></author></authors><title>MIMO Relaying Broadcast Channels with Linear Precoding and Quantized
  Channel State Information Feedback</title><categories>cs.IT math.IT</categories><comments>13pages appeared in IEEE Transactions on Signal Processing</comments><msc-class>94A05</msc-class><journal-ref>W. Xu, X. Dong, and W.-S. Lu &quot;MIMO relaying broadcast channels
  with linear precoding and quantized channel state information feedback,&quot; IEEE
  Trans. Sig. Proc., vol. 58, no. 10, pp. 5233-5245, Oct. 2010</journal-ref><doi>10.1109/TSP.2010.2056687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-antenna relaying has emerged as a promising technology to enhance the
system performance in cellular networks. However, when precoding techniques are
utilized to obtain multi-antenna gains, the system generally requires channel
state information (CSI) at the transmitters. We consider a linear precoding
scheme in a MIMO relaying broadcast channel with quantized CSI feedback from
both two-hop links. With this scheme, each remote user feeds back its quantized
CSI to the relay, and the relay sends back the quantized precoding information
to the base station (BS). An upper bound on the rate loss due to quantized
channel knowledge is first characterized. Then, in order to maintain the rate
loss within a predetermined gap for growing SNRs, a strategy of scaling
quantization quality of both two-hop links is proposed. It is revealed that the
numbers of feedback bits of both links should scale linearly with the transmit
power at the relay, while only the bit number of feedback from the relay to the
BS needs to grow with the increasing transmit power at the BS. Numerical
results are provided to verify the proposed strategy for feedback quality
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2465</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2465</id><created>2012-05-11</created><authors><author><keyname>Eberius</keyname><forenames>Julian</forenames></author><author><keyname>Braunschweig</keyname><forenames>Katrin</forenames></author><author><keyname>Thiele</keyname><forenames>Maik</forenames></author><author><keyname>Lehner</keyname><forenames>Wolfgang</forenames></author></authors><title>Identifying And Weighting Integration Hypotheses On Open Data Platforms</title><categories>cs.DB</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/11</report-no><acm-class>J.3; H.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open data platforms such as data.gov or opendata.socrata. com provide a huge
amount of valuable information. Their free-for-all nature, the lack of
publishing standards and the multitude of domains and authors represented on
these platforms lead to new integration and standardization problems. At the
same time, crowd-based data integration techniques are emerging as new way of
dealing with these problems. However, these methods still require input in form
of specific questions or tasks that can be passed to the crowd. This paper
discusses integration problems on Open Data Platforms, and proposes a method
for identifying and ranking integration hypotheses in this context. We will
evaluate our findings by conducting a comprehensive evaluation using on one of
the largest Open Data platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2467</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2467</id><created>2012-05-11</created><authors><author><keyname>Mutschke</keyname><forenames>Peter</forenames></author><author><keyname>Thamm</keyname><forenames>Mark</forenames></author></authors><title>Linking Social Networking Sites to Scholarly Information Portals by
  ScholarLib</title><categories>cs.DL cs.IR cs.SI</categories><comments>5 pages, ACM Web Science 2012</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks usually provide no or limited way to access scholarly
information provided by Digital Libraries (DLs) in order to share and discuss
scholarly content with other online community members. The paper addresses the
potentials of Social Networking sites (SNSs) for science and proposes initial
use cases as well as a basic bi-directional model called ScholarLib for linking
SNSs to scholarly DLs. The major aim of ScholarLib is to make scholarly
information provided by DLs accessible at SNSs, and vice versa, to enhance
retrieval quality at DL side by social information provided by SNSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2476</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2476</id><created>2012-05-11</created><authors><author><keyname>Otjacques</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Stefas</keyname><forenames>Micka&#xeb;l</forenames></author><author><keyname>Cornil</keyname><forenames>Ma&#xeb;l</forenames></author><author><keyname>Feltz</keyname><forenames>Fernand</forenames></author></authors><title>Open Data Visualization: Keeping Traces of the Exploration Process</title><categories>cs.HC</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/1</report-no><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a system to support the visual exploration of Open Data.
During his/her interactive experience with the graphics, the user can easily
store the current complete state of the visualization application (called a
viewpoint). Next, he/she can compose sequences of these viewpoints (called
scenarios) that can easily be reloaded. This feature allows to keep traces of a
former exploration process, which can be useful in single user (to support
investigation carried out in multiple sessions) as well as in collaborative
setting (to share points of interest identified in the data set).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2483</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2483</id><created>2012-05-11</created><authors><author><keyname>Chang</keyname><forenames>Maw-Shang</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Ching-Hao</forenames></author></authors><title>Edge-clique graphs of cocktail parties have unbounded rankwidth</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an attempt to find a polynomial-time algorithm for the edge-clique cover
problem on cographs we tried to prove that the edge-clique graphs of cographs
have bounded rankwidth. However, this is not the case. In this note we show
that the edge-clique graphs of cocktail party graphs have unbounded rankwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2492</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2492</id><created>2012-05-11</created><updated>2012-06-01</updated><authors><author><keyname>Atkey</keyname><forenames>Robert</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Johann</keyname><forenames>Patricia</forenames><affiliation>University of Strathclyde</affiliation></author><author><keyname>Ghani</keyname><forenames>Neil</forenames><affiliation>University of Strathclyde</affiliation></author></authors><title>Refining Inductive Types</title><categories>cs.LO cs.PL</categories><proxy>LMCS</proxy><acm-class>D.3.3, F.3.3, D.3.1, F.3.2, F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 4,
  2012) lmcs:957</journal-ref><doi>10.2168/LMCS-8(2:9)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependently typed programming languages allow sophisticated properties of
data to be expressed within the type system. Of particular use in dependently
typed programming are indexed types that refine data by computationally useful
information. For example, the N-indexed type of vectors refines lists by their
lengths. Other data types may be refined in similar ways, but programmers must
produce purpose-specific refinements on an ad hoc basis, developers must
anticipate which refinements to include in libraries, and implementations must
often store redundant information about data and their refinements. In this
paper we show how to generically derive inductive characterisations of
refinements of inductive types, and argue that these characterisations can
alleviate some of the aforementioned difficulties associated with ad hoc
refinements. Our characterisations also ensure that standard techniques for
programming with and reasoning about inductive types are applicable to
refinements, and that refinements can themselves be further refined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2509</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2509</id><created>2012-05-11</created><authors><author><keyname>Jackson</keyname><forenames>Adrian</forenames></author><author><keyname>Hein</keyname><forenames>Joachim</forenames></author><author><keyname>Roach</keyname><forenames>C. M.</forenames></author></authors><title>Optimising Performance Through Unbalanced Decompositions</title><categories>cs.DC physics.plasm-ph</categories><journal-ref>IEEE TPDS 2014</journal-ref><doi>10.1109/TPDS.2014.2351826</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GS2 is an initial value gyrokinetic simulation code developed to study
low-frequency turbulence in magnetized plasma. It is parallelised using MPI
with the simulation domain decomposed across tasks. The optimal domain
decomposition is non-trivial, and complicated by the different requirements of
the linear and non-linear parts of the calculations. GS2 users currently choose
a data layout, and are guided towards processor count that are efficient for
linear calculations. These choices can, however, lead to data decompositions
that are relatively inefficient for the non-linear calculations. We have
analysed the performance impact of the data decompositions on the non-linear
calculation and associated communications. This has helped us to optimise the
decomposition algorithm by using unbalanced data layouts for the non-linear
calculations whilst maintaining the existing decompositions for the linear
calculations, which has completely eliminated communications for parts of the
non-linear simulation and improved performance by up to 15% for a
representative simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2519</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2519</id><created>2012-05-11</created><updated>2012-06-15</updated><authors><author><keyname>Kop</keyname><forenames>Cynthia</forenames><affiliation>VU University Amsterdam</affiliation></author><author><keyname>van Raamsdonk</keyname><forenames>Femke</forenames><affiliation>VU University Amsterdam</affiliation></author></authors><title>Dynamic Dependency Pairs for Algebraic Functional Systems</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F4.1,F4.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 19,
  2012) lmcs:668</journal-ref><doi>10.2168/LMCS-8(2:10)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the higher-order termination method of dynamic dependency pairs to
Algebraic Functional Systems (AFSs). In this setting, simply typed lambda-terms
with algebraic reduction and separate {\beta}-steps are considered. For
left-linear AFSs, the method is shown to be complete. For so-called local AFSs
we define a variation of usable rules and an extension of argument filterings.
All these techniques have been implemented in the higher-order termination tool
WANDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2535</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2535</id><created>2012-05-11</created><updated>2013-09-22</updated><authors><author><keyname>Aboulker</keyname><forenames>Pierre</forenames></author><author><keyname>Charbit</keyname><forenames>Pierre</forenames></author><author><keyname>Trotignon</keyname><forenames>Nicolas</forenames></author><author><keyname>Vuskovic</keyname><forenames>Kristina</forenames></author></authors><title>Vertex elimination orderings for hereditary graph classes</title><categories>cs.DM math.CO</categories><msc-class>05C75</msc-class><journal-ref>Discrete Mathematics 338:825-834, 2015</journal-ref><doi>10.1016/j.disc.2014.12.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general method to prove the existence and compute efficiently
elimination orderings in graphs. Our method relies on several tools that were
known before, but that were not put together so far: the algorithm LexBFS due
to Rose, Tarjan and Lueker, one of its properties discovered by Berry and
Bordat, and a local decomposition property of graphs discovered by Maffray,
Trotignon and Vu\vskovi\'c. We use this method to prove the existence of
elimination orderings in several classes of graphs, and to compute them in
linear time. Some of the classes have already been studied, namely
even-hole-free graphs, square-theta-free Berge graphs, universally signable
graphs and wheel-free graphs. Some other classes are new. It turns out that all
the classes that we study in this paper can be defined by excluding some of the
so-called Truemper configurations. For several classes of graphs, we obtain
directly bounds on the chromatic number, or fast algorithms for the maximum
clique problem or the coloring problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2541</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2541</id><created>2012-05-11</created><authors><author><keyname>Wang</keyname><forenames>Changzhong</forenames></author><author><keyname>Sun</keyname><forenames>Baiqing</forenames></author><author><keyname>Hu</keyname><forenames>Qinhua</forenames></author></authors><title>An improved approach to attribute reduction with covering rough sets</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute reduction is viewed as an important preprocessing step for pattern
recognition and data mining. Most of researches are focused on attribute
reduction by using rough sets. Recently, Tsang et al. discussed attribute
reduction with covering rough sets in the paper [E. C.C. Tsang, D. Chen, Daniel
S. Yeung, Approximations and reducts with covering generalized rough sets,
Computers and Mathematics with Applications 56 (2008) 279-289], where an
approach based on discernibility matrix was presented to compute all attribute
reducts. In this paper, we provide an improved approach by constructing simpler
discernibility matrix with covering rough sets, and then proceed to improve
some characterizations of attribute reduction provided by Tsang et al. It is
proved that the improved discernible matrix is equivalent to the old one, but
the computational complexity of discernible matrix is greatly reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2546</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2546</id><created>2012-05-11</created><authors><author><keyname>Smith</keyname><forenames>James William</forenames></author><author><keyname>Khajeh-Hosseini</keyname><forenames>Ali</forenames></author><author><keyname>Ward</keyname><forenames>Jonathan Stuart</forenames></author><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author></authors><title>CloudMonitor: Profiling Power Usage</title><categories>cs.DC</categories><comments>2 page submission to appear in IEEE Cloud 2012 Work In Progress Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cloud Computing platforms the addition of hardware monitoring devices to
gather power usage data can be impractical or uneconomical due to the large
number of machines to be metered. CloudMonitor, a monitoring tool that can
generate power models for software-based power estimation, can provide insights
to the energy costs of deployments without additional hardware. Accurate power
usage data leads to the possibility of Cloud providers creating a separate
tariff for power and therefore incentivizing software developers to create
energy-efficient applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2548</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2548</id><created>2012-05-11</created><updated>2014-04-10</updated><authors><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Outerplanar graph drawings with few slopes</title><categories>cs.CG cs.DM math.CO</categories><comments>Major revision of the whole paper</comments><msc-class>05C62, 68R10</msc-class><journal-ref>Comput.Geom. 47 (2014) 614-624</journal-ref><doi>10.1016/j.comgeo.2014.01.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider straight-line outerplanar drawings of outerplanar graphs in which
a small number of distinct edge slopes are used, that is, the segments
representing edges are parallel to a small number of directions. We prove that
$\Delta-1$ edge slopes suffice for every outerplanar graph with maximum degree
$\Delta\ge 4$. This improves on the previous bound of $O(\Delta^5)$, which was
shown for planar partial 3-trees, a superclass of outerplanar graphs. The bound
is tight: for every $\Delta\ge 4$ there is an outerplanar graph with maximum
degree $\Delta$ that requires at least $\Delta-1$ distinct edge slopes in an
outerplanar straight-line drawing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2554</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2554</id><created>2012-05-11</created><authors><author><keyname>Barbero</keyname><forenames>Fausto</forenames></author></authors><title>On existential declarations of independence in IF Logic</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the behaviour of declarations of independence between existential
quantifiers in quantifier prefixes of IF sentences; we give a syntactical
criterion for deciding whether a sentence beginning with such prefix exists
such that its truth values may be affected by removal of the declaration of
independence. We extend the result also to equilibrium semantics values for
undetermined IF sentences.
  The main theorem allows us to describe the behaviour of various particular
classes of quantifier prefixes, and to prove as a remarkable corollary that all
existential IF sentences are equivalent to first-order sentences.
  As a further consequence, we prove that the fragment of IF sentences with
knowledge memory has only first-order expressive power (up to truth
equivalence).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2555</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2555</id><created>2012-05-11</created><updated>2012-05-15</updated><authors><author><keyname>Coletta</keyname><forenames>R.</forenames></author><author><keyname>Castanier</keyname><forenames>E.</forenames></author><author><keyname>Valduriez</keyname><forenames>P.</forenames></author><author><keyname>Frisch</keyname><forenames>C.</forenames></author><author><keyname>Ngo</keyname><forenames>D.</forenames></author><author><keyname>Bellahsene</keyname><forenames>Z.</forenames></author></authors><title>Public Data Integration with WebSmatch</title><categories>cs.DL</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/9</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating open data sources can yield high value information but raises
major problems in terms of metadata extraction, data source integration and
visualization of integrated data. In this paper, we describe WebSmatch, a
flexible environment for Web data integration, based on a real, end-to-end data
integration scenario over public data from Data Publica. WebSmatch supports the
full process of importing, refining and integrating data sources and uses third
party tools for high quality visualization. We use a typical scenario of public
data integration which involves problems not solved by currents tools: poorly
structured input data sources (XLS files) and rich visualization of integrated
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2583</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2583</id><created>2012-05-11</created><updated>2012-12-16</updated><authors><author><keyname>Cui</keyname><forenames>Ai-xiang</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-ke</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Hui</keyname><forenames>Pak Ming</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author></authors><title>Emergence of scale-free close-knit friendship structure in online social
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>48 pages, 34 figures</comments><doi>10.1371/journal.pone.0050702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the structural properties of online social networks have attracted
much attention, the properties of the close-knit friendship structures remain
an important question. Here, we mainly focus on how these mesoscale structures
are affected by the local and global structural properties. Analyzing the data
of four large-scale online social networks reveals several common structural
properties. It is found that not only the local structures given by the
indegree, outdegree, and reciprocal degree distributions follow a similar
scaling behavior, the mesoscale structures represented by the distributions of
close-knit friendship structures also exhibit a similar scaling law. The degree
correlation is very weak over a wide range of the degrees. We propose a simple
directed network model that captures the observed properties. The model
incorporates two mechanisms: reciprocation and preferential attachment. Through
rate equation analysis of our model, the local-scale and mesoscale structural
properties are derived. In the local-scale, the same scaling behavior of
indegree and outdegree distributions stems from indegree and outdegree of nodes
both growing as the same function of the introduction time, and the reciprocal
degree distribution also shows the same power-law due to the linear
relationship between the reciprocal degree and in/outdegree of nodes. In the
mesoscale, the distributions of four closed triples representing close-knit
friendship structures are found to exhibit identical power-laws, a behavior
attributed to the negligible degree correlations. Intriguingly, all the
power-law exponents of the distributions in the local-scale and mesoscale
depend only on one global parameter -- the mean in/outdegree, while both the
mean in/outdegree and the reciprocity together determine the ratio of the
reciprocal degree of a node to its in/outdegree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2584</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2584</id><created>2012-05-11</created><updated>2012-09-12</updated><authors><author><keyname>Phan</keyname><forenames>Anh Huy</forenames></author><author><keyname>Tichavsk&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC</title><categories>cs.NA cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP)
decomposition can handle the challenges of collinearity of factors and
different magnitudes of factors; nevertheless, for factorization of an $N$-D
tensor of size $I_1\times I_N$ with rank $R$, the algorithm is computationally
demanding due to construction of large approximate Hessian of size $(RT \times
RT)$ and its inversion where $T = \sum_n I_n$. In this paper, we propose a fast
implementation of the dGN algorithm which is based on novel expressions of the
inverse approximate Hessian in block form. The new implementation has lower
computational complexity, besides computation of the gradient (this part is
common to both methods), requiring the inversion of a matrix of size
$NR^2\times NR^2$, which is much smaller than the whole approximate Hessian, if
$T \gg NR$. In addition, the implementation has lower memory requirements,
because neither the Hessian nor its inverse never need to be stored in their
entirety. A variant of the algorithm working with complex valued data is
proposed as well. Complexity and performance of the proposed algorithm is
compared with those of dGN and ALS with line search on examples of difficult
benchmark tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2590</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2590</id><created>2012-05-11</created><updated>2014-07-08</updated><authors><author><keyname>Rosnes</keyname><forenames>Eirik</forenames></author><author><keyname>Ambroze</keyname><forenames>Marcel A.</forenames></author><author><keyname>Tomlinson</keyname><forenames>Martin</forenames></author></authors><title>On the Minimum/Stopping Distance of Array Low-Density Parity-Check Codes</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Inf. Theory. The material in this paper was
  presented in part at the 2014 IEEE International Symposium on Information
  Theory, Honolulu, HI, June/July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the minimum/stopping distance of array low-density
parity-check (LDPC) codes. An array LDPC code is a quasi-cyclic LDPC code
specified by two integers q and m, where q is an odd prime and m &lt;= q. In the
literature, the minimum/stopping distance of these codes (denoted by d(q,m) and
h(q,m), respectively) has been thoroughly studied for m &lt;= 5. Both exact
results, for small values of q and m, and general (i.e., independent of q)
bounds have been established. For m=6, the best known minimum distance upper
bound, derived by Mittelholzer (IEEE Int. Symp. Inf. Theory, Jun./Jul. 2002),
is d(q,6) &lt;= 32. In this work, we derive an improved upper bound of d(q,6) &lt;=
20 and a new upper bound d(q,7) &lt;= 24 by using the concept of a template
support matrix of a codeword/stopping set. The bounds are tight with high
probability in the sense that we have not been able to find codewords of
strictly lower weight for several values of q using a minimum distance
probabilistic algorithm. Finally, we provide new specific minimum/stopping
distance results for m &lt;= 7 and low-to-moderate values of q &lt;= 79.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2596</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2596</id><created>2012-05-11</created><updated>2014-08-28</updated><authors><author><keyname>Cozman</keyname><forenames>Fabio</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2011</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2597</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2597</id><created>2012-05-11</created><updated>2014-08-28</updated><authors><author><keyname>Grunwald</keyname><forenames>Peter</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter</forenames></author></authors><title>Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2010</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2599</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2599</id><created>2012-05-09</created><authors><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Hyvarinen</keyname><forenames>Aapo</forenames></author></authors><title>On the Identifiability of the Post-Nonlinear Causal Model</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-647-655</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By taking into account the nonlinear effect of the cause, the inner noise
effect, and the measurement distortion effect in the observed variables, the
post-nonlinear (PNL) causal model has demonstrated its excellent performance in
distinguishing the cause from effect. However, its identifiability has not been
properly addressed, and how to apply it in the case of more than two variables
is also a problem. In this paper, we conduct a systematic investigation on its
identifiability in the two-variable case. We show that this model is
identifiable in most cases; by enumerating all possible situations in which the
model is not identifiable, we provide sufficient conditions for its
identifiability. Simulations are given to support the theoretical results.
Moreover, in the case of more than two variables, we show that the whole causal
structure can be found by applying the PNL causal model to each structure in
the Markov equivalent class and testing if the disturbance is independent of
the direct causes for each variable. In this way the exhaustive search over all
possible causal structures is avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2600</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2600</id><created>2012-05-09</created><authors><author><keyname>Zadeh</keyname><forenames>Reza Bosagh</forenames></author><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author></authors><title>A Uniqueness Theorem for Clustering</title><categories>cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-639-646</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the widespread use of Clustering, there is distressingly little
general theory of clustering available. Questions like &quot;What distinguishes a
clustering of data from other data partitioning?&quot;, &quot;Are there any principles
governing all clustering paradigms?&quot;, &quot;How should a user choose an appropriate
clustering algorithm for a particular task?&quot;, etc. are almost completely
unanswered by the existing body of clustering literature. We consider an
axiomatic approach to the theory of Clustering. We adopt the framework of
Kleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, we
sidestep his impossibility result and arrive at a consistent set of axioms. We
suggest to extend these axioms, aiming to provide an axiomatic taxonomy of
clustering paradigms. Such a taxonomy should provide users some guidance
concerning the choice of the appropriate clustering paradigm for a given task.
The main result of this paper is a set of abstract properties that characterize
the Single-Linkage clustering function. This characterization result provides
new insight into the properties of desired data groupings that make
Single-Linkage the appropriate choice. We conclude by considering a taxonomy of
clustering functions based on abstract properties that each satisfies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2601</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2601</id><created>2012-05-09</created><authors><author><keyname>Yuan</keyname><forenames>Changhe</forenames></author><author><keyname>Liu</keyname><forenames>Xiaolu</forenames></author><author><keyname>Lu</keyname><forenames>Tsai-Ching</forenames></author><author><keyname>Lim</keyname><forenames>Heejin</forenames></author></authors><title>Most Relevant Explanation: Properties, Algorithms, and Evaluations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-631-638</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most Relevant Explanation (MRE) is a method for finding multivariate
explanations for given evidence in Bayesian networks [12]. This paper studies
the theoretical properties of MRE and develops an algorithm for finding
multiple top MRE solutions. Our study shows that MRE relies on an implicit soft
relevance measure in automatically identifying the most relevant target
variables and pruning less relevant variables from an explanation. The soft
measure also enables MRE to capture the intuitive phenomenon of explaining away
encoded in Bayesian networks. Furthermore, our study shows that the solution
space of MRE has a special lattice structure which yields interesting dominance
relations among the solutions. A K-MRE algorithm based on these dominance
relations is developed for generating a set of top solutions that are more
representative. Our empirical results show that MRE methods are promising
approaches for explanation in Bayesian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2602</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2602</id><created>2012-05-09</created><authors><author><keyname>Yu</keyname><forenames>Jin</forenames></author><author><keyname>Vishwanatan</keyname><forenames>S. V. N.</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author></authors><title>The Entire Quantile Path of a Risk-Agnostic SVM Classifier</title><categories>cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-623-630</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X =
x) &gt;= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has
been shown that Support Vector Machines (SVMs) in the limit are quantile
classifiers with t = 1/2 . In this paper, we show that by using asymmetric cost
of misclassification SVMs can be appropriately extended to recover, in the
limit, the quantile binary classifier for any t. We then present a principled
algorithm to solve the extended SVM classifier for all values of t
simultaneously. This has two implications: First, one can recover the entire
conditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build
a risk-agnostic SVM classifier where the cost of misclassification need not be
known apriori. Preliminary numerical experiments show the effectiveness of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2603</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2603</id><created>2012-05-09</created><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Chi</keyname><forenames>Yun</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>A Bayesian Framework for Community Detection Integrating Content and
  Link</title><categories>cs.SI cs.AI physics.soc-ph</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-615-622</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of community detection in networked data
that combines link and content analysis. Most existing work combines link and
content information by a generative model. There are two major shortcomings
with the existing approaches. First, they assume that the probability of
creating a link between two nodes is determined only by the community
memberships of the nodes; however other factors (e.g. popularity) could also
affect the link pattern. Second, they use generative models to model the
content of individual nodes, whereas these generative models are vulnerable to
the content attributes that are irrelevant to communities. We propose a
Bayesian framework for combining link and content information for community
detection that explicitly addresses these shortcomings. A new link model is
presented that introduces a random variable to capture the node popularity when
deciding the link between two nodes; a discriminative model is used to
determine the community membership of a node by its content. An approximate
inference algorithm is presented for efficient Bayesian inference. Our
empirical study shows that the proposed framework outperforms several
state-of-theart approaches in combining link and content information for
community detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2604</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2604</id><created>2012-05-09</created><authors><author><keyname>Wingate</keyname><forenames>David</forenames></author><author><keyname>Goodman</keyname><forenames>Noah</forenames></author><author><keyname>Roy</keyname><forenames>Daniel</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua</forenames></author></authors><title>The Infinite Latent Events Model</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-607-614</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Infinite Latent Events Model, a nonparametric hierarchical
Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with
binary state representations and noisy-OR-like transitions. The distribution
can be used to learn structure in discrete timeseries data by simultaneously
inferring a set of latent events, which events fired at each timestep, and how
those events are causally linked. We illustrate the model on a sound
factorization task, a network topology identification task, and a video game
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2605</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2605</id><created>2012-05-09</created><authors><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Herding Dynamic Weights for Partially Observed Random Field Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-599-606</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the parameters of a (potentially partially observable) random field
model is intractable in general. Instead of focussing on a single optimal
parameter value we propose to treat parameters as dynamical quantities. We
introduce an algorithm to generate complex dynamics for parameters and (both
visible and hidden) state vectors. We show that under certain conditions
averages computed over trajectories of the proposed dynamical system converge
to averages computed over the data. Our &quot;herding dynamics&quot; does not require
expensive operations such as exponentiation and is fully deterministic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2606</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2606</id><created>2012-05-09</created><authors><author><keyname>Walsh</keyname><forenames>Thomas J.</forenames></author><author><keyname>Szita</keyname><forenames>Istvan</forenames></author><author><keyname>Diuk</keyname><forenames>Carlos</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author></authors><title>Exploring compact reinforcement-learning representations with linear
  regression</title><categories>cs.LG cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-591-598</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new algorithm for online linear regression whose
efficiency guarantees satisfy the requirements of the KWIK (Knows What It
Knows) framework. The algorithm improves on the complexity bounds of the
current state-of-the-art procedure in this setting. We explore several
applications of this algorithm for learning compact reinforcement-learning
representations. We show that KWIK linear regression can be used to learn the
reward function of a factored MDP and the probabilities of action outcomes in
Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to
be efficiently learnable in the RL setting before. We also combine KWIK linear
regression with other KWIK learners to learn larger portions of these models,
including experiments on learning factored MDP transition and reward functions
together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2607</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2607</id><created>2012-05-09</created><authors><author><keyname>Vorobeychik</keyname><forenames>Yevgeniy</forenames></author></authors><title>Simulation-Based Game Theoretic Analysis of Keyword Auctions with
  Low-Dimensional Bidding Strategies</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-583-590</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform a simulation-based analysis of keyword auctions modeled as
one-shot games of incomplete information to study a series of mechanism design
questions. Our first question addresses the degree to which incentive
compatibility fails in generalized second-price (GSP) auctions. Our results
suggest that sincere bidding in GSP auctions is a strikingly poor strategy and
a poor predictor of equilibrium outcomes. We next show that the rank-by-revenue
mechanism is welfare optimal, corroborating past results. Finally, we analyze
profit as a function of auction mechanism under a series of alternative
settings. Our conclusions coincide with those of Lahaie and Pennock [2007] when
values and quality scores are strongly positively correlated: in such a case,
rank-by-bid rules are clearly superior. We diverge, however, in showing that
auctions that put little weight on quality scores almost universally dominate
the pure rank-by-revenue scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2608</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2608</id><created>2012-05-09</created><authors><author><keyname>Vigorito</keyname><forenames>Christopher M.</forenames></author></authors><title>Temporal-Difference Networks for Dynamical Systems with Continuous
  Observations and Actions</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-575-582</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal-difference (TD) networks are a class of predictive state
representations that use well-established TD methods to learn models of
partially observable dynamical systems. Previous research with TD networks has
dealt only with dynamical systems with finite sets of observations and actions.
We present an algorithm for learning TD network representations of dynamical
systems with continuous observations and actions. Our results show that the
algorithm is capable of learning accurate and robust models of several noisy
continuous dynamical systems. The algorithm presented here is the first fully
incremental method for learning a predictive representation of a continuous
dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2609</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2609</id><created>2012-05-09</created><authors><author><keyname>Verma</keyname><forenames>Nakul</forenames></author><author><keyname>Kpotufe</keyname><forenames>Samory</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Which Spatial Partition Trees are Adaptive to Intrinsic Dimension?</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-565-574</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent theory work has found that a special type of spatial partition tree -
called a random projection tree - is adaptive to the intrinsic dimension of the
data from which it is built. Here we examine this same question, with a
combination of theory and experiments, for a broader class of trees that
includes k-d trees, dyadic trees, and PCA trees. Our motivation is to get a
feel for (i) the kind of intrinsic low dimensional structure that can be
empirically verified, (ii) the extent to which a spatial partition can exploit
such structure, and (iii) the implications for standard statistical tasks such
as regression, vector quantization, and nearest neighbor search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2610</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2610</id><created>2012-05-09</created><authors><author><keyname>Vembu</keyname><forenames>Shankar</forenames></author><author><keyname>Gartner</keyname><forenames>Thomas</forenames></author><author><keyname>Boley</keyname><forenames>Mario</forenames></author></authors><title>Probabilistic Structured Predictors</title><categories>cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009). arXiv admin note: substantial text
  overlap with arXiv:0912.4473</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-557-564</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider MAP estimators for structured prediction with exponential family
models. In particular, we concentrate on the case that efficient algorithms for
uniform sampling from the output space exist. We show that under this
assumption (i) exact computation of the partition function remains a hard
problem, and (ii) the partition function and the gradient of the log partition
function can be approximated efficiently. Our main result is an approximation
scheme for the partition function based on Markov Chain Monte Carlo theory. We
also show that the efficient uniform sampling assumption holds in several
application settings that are of importance in machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2611</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2611</id><created>2012-05-09</created><authors><author><keyname>Truyen</keyname><forenames>Tran The</forenames></author><author><keyname>Phung</keyname><forenames>Dinh Q.</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Ordinal Boltzmann Machines for Collaborative Filtering</title><categories>cs.IR cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-548-556</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering is an effective recommendation technique wherein the
preference of an individual can potentially be predicted based on preferences
of other members. Early algorithms often relied on the strong locality in the
preference data, that is, it is enough to predict preference of a user on a
particular item based on a small subset of other users with similar tastes or
of other items with similar properties. More recently, dimensionality reduction
techniques have proved to be equally competitive, and these are based on the
co-occurrence patterns rather than locality. This paper explores and extends a
probabilistic model known as Boltzmann Machine for collaborative filtering
tasks. It seamlessly integrates both the similarity and co-occurrence in a
principled manner. In particular, we study parameterisation options to deal
with the ordinal nature of the preferences, and propose a joint modelling of
both the user-based and item-based processes. Experiments on moderate and
large-scale movie recommendation show that our framework rivals existing
well-known methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2612</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2612</id><created>2012-05-09</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author><author><keyname>He</keyname><forenames>Ru</forenames></author></authors><title>Computing Posterior Probabilities of Structural Features in Bayesian
  Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-538-547</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning Bayesian network structures from data.
Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that can
compute the exact marginal posterior probability of a subnetwork, e.g., a
single edge, in O(n2n) time and the posterior probabilities for all n(n-1)
potential edges in O(n2n) total time, assuming that the number of parents per
node or the indegree is bounded by a constant. One main drawback of their
algorithms is the requirement of a special structure prior that is non uniform
and does not respect Markov equivalence. In this paper, we develop an algorithm
that can compute the exact posterior probability of a subnetwork in O(3n) time
and the posterior probabilities for all n(n-1) potential edges in O(n3n) total
time. Our algorithm also assumes a bounded indegree but allows general
structure priors. We demonstrate the applicability of the algorithm on several
data sets with up to 20 variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2613</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2613</id><created>2012-05-09</created><authors><author><keyname>Thimm</keyname><forenames>Matthias</forenames></author></authors><title>Measuring Inconsistency in Probabilistic Knowledge Bases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-530-537</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops an inconsistency measure on conditional probabilistic
knowledge bases. The measure is based on fundamental principles for
inconsistency measures and thus provides a solid theoretical framework for the
treatment of inconsistencies in probabilistic expert systems. We illustrate its
usefulness and immediate application on several examples and present some
formal results. Building on this measure we use the Shapley value-a well-known
solution for coalition games-to define a sophisticated indicator that is not
only able to measure inconsistencies but to reveal the causes of
inconsistencies in the knowledge base. Altogether these tools guide the
knowledge engineer in his aim to restore consistency and therefore enable him
to build a consistent and usable knowledge base that can be employed in
probabilistic expert systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2614</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2614</id><created>2012-05-09</created><authors><author><keyname>Taylor</keyname><forenames>Graham W</forenames></author><author><keyname>Hinton</keyname><forenames>Geoffrey E.</forenames></author></authors><title>Products of Hidden Markov Models: It Takes N&gt;1 to Tango</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-522-529</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Products of Hidden Markov Models(PoHMMs) are an interesting class of
generative models which have received little attention since their
introduction. This maybe in part due to their more computationally expensive
gradient-based learning algorithm,and the intractability of computing the log
likelihood of sequences under the model. In this paper, we demonstrate how the
partition function can be estimated reliably via Annealed Importance Sampling.
We perform experiments using contrastive divergence learning on rainfall data
and data captured from pairs of people dancing. Our results suggest that
advances in learning and evaluation for undirected graphical models and recent
increases in available computing power make PoHMMs worth considering for
complex time-series modeling tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2615</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2615</id><created>2012-05-09</created><authors><author><keyname>Shpitser</keyname><forenames>Ilya</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Effects of Treatment on the Treated: Identification and Generalization</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-514-521</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications of causal analysis call for assessing, retrospectively, the
effect of withholding an action that has in fact been implemented. This
counterfactual quantity, sometimes called &quot;effect of treatment on the treated,&quot;
(ETT) have been used to to evaluate educational programs, critic public
policies, and justify individual decision making. In this paper we explore the
conditions under which ETT can be estimated from (i.e., identified in)
experimental and/or observational studies. We show that, when the action
invokes a singleton variable, the conditions for ETT identification have simple
characterizations in terms of causal diagrams. We further give a graphical
characterization of the conditions under which the effects of multiple
treatments on the treated can be identified, as well as ways in which the ETT
estimand can be constructed from both interventional and observational
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2616</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2616</id><created>2012-05-09</created><authors><author><keyname>Sen</keyname><forenames>Prithviraj</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Bisimulation-based Approximate Lifted Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-496-505</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a great deal of recent interest in methods for performing
lifted inference; however, most of this work assumes that the first-order model
is given as input to the system. Here, we describe lifted inference algorithms
that determine symmetries and automatically lift the probabilistic model to
speedup inference. In particular, we describe approximate lifted inference
techniques that allow the user to trade off inference accuracy for
computational efficiency by using a handful of tunable parameters, while
keeping the error bounded. Our algorithms are closely related to the
graph-theoretic concept of bisimulation. We report experiments on both
synthetic and real data to show that in the presence of symmetries, run-times
for inference can be improved significantly, with approximate lifted inference
providing orders of magnitude speedup over ground inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2617</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2617</id><created>2012-05-09</created><authors><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author></authors><title>Modeling Discrete Interventional Data using Directed Cyclic Graphical
  Models</title><categories>stat.ML cs.LG stat.ME</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-487-495</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline a representation for discrete multivariate distributions in terms
of interventional potential functions that are globally normalized. This
representation can be used to model the effects of interventions, and the
independence properties encoded in this model can be represented as a directed
graph that allows cycles. In addition to discussing inference and sampling with
this representation, we give an exponential family parametrization that allows
parameter estimation to be stated as a convex optimization problem; we also
give a convex relaxation of the task of simultaneous parameter and structure
learning using group l1-regularization. The model is evaluated on simulated
data and intracellular flow cytometry data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2618</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2618</id><created>2012-05-09</created><authors><author><keyname>Rendle</keyname><forenames>Steffen</forenames></author><author><keyname>Freudenthaler</keyname><forenames>Christoph</forenames></author><author><keyname>Gantner</keyname><forenames>Zeno</forenames></author><author><keyname>Schmidt-Thieme</keyname><forenames>Lars</forenames></author></authors><title>BPR: Bayesian Personalized Ranking from Implicit Feedback</title><categories>cs.IR cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-452-461</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Item recommendation is the task of predicting a personalized ranking on a set
of items (e.g. websites, movies, products). In this paper, we investigate the
most common scenario with implicit feedback (e.g. clicks, purchases). There are
many methods for item recommendation from implicit feedback like matrix
factorization (MF) or adaptive knearest-neighbor (kNN). Even though these
methods are designed for the item prediction task of personalized ranking, none
of them is directly optimized for ranking. In this paper we present a generic
optimization criterion BPR-Opt for personalized ranking that is the maximum
posterior estimator derived from a Bayesian analysis of the problem. We also
provide a generic learning algorithm for optimizing models with respect to
BPR-Opt. The learning method is based on stochastic gradient descent with
bootstrap sampling. We show how to apply our method to two state-of-the-art
recommender models: matrix factorization and adaptive kNN. Our experiments
indicate that for the task of personalized ranking our optimization method
outperforms the standard learning techniques for MF and kNN. The results show
the importance of optimizing models for the right criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2619</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2619</id><created>2012-05-09</created><authors><author><keyname>Regan</keyname><forenames>Kevin</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Regret-based Reward Elicitation for Markov Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-444-451</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The specification of aMarkov decision process (MDP) can be difficult. Reward
function specification is especially problematic; in practice, it is often
cognitively complex and time-consuming for users to precisely specify rewards.
This work casts the problem of specifying rewards as one of preference
elicitation and aims to minimize the degree of precision with which a reward
function must be specified while still allowing optimal or near-optimal
policies to be produced. We first discuss how robust policies can be computed
for MDPs given only partial reward information using the minimax regret
criterion. We then demonstrate how regret can be reduced by efficiently
eliciting reward information using bound queries, using regret-reduction as a
means for choosing suitable queries. Empirical results demonstrate that
regret-based reward elicitation offers an effective way to produce near-optimal
policies without resorting to the precise specification of the entire reward
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2620</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2620</id><created>2012-05-09</created><authors><author><keyname>Parviainen</keyname><forenames>Pekka</forenames></author><author><keyname>Koivisto</keyname><forenames>Mikko</forenames></author></authors><title>Exact Structure Discovery in Bayesian Networks with Less Space</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-436-443</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fastest known exact algorithms for scorebased structure discovery in
Bayesian networks on n nodes run in time and space 2nnO(1). The usage of these
algorithms is limited to networks on at most around 25 nodes mainly due to the
space requirement. Here, we study space-time tradeoffs for finding an optimal
network structure. When little space is available, we apply the Gurevich-Shelah
recurrence-originally proposed for the Hamiltonian path problem-and obtain time
22n-snO(1) in space 2snO(1) for any s = n/2, n/4, n/8, . . .; we assume the
indegree of each node is bounded by a constant. For the more practical setting
with moderate amounts of space, we present a novel scheme. It yields running
time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, . . ., n/2; these
bounds hold as long as the indegrees are at most 0.238n. Furthermore, the
latter scheme allows easy and efficient parallelization beyond previous
algorithms. We also explore empirically the potential of the presented
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2621</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2621</id><created>2012-05-09</created><authors><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author></authors><title>Logical Inference Algorithms and Matrix Representations for
  Probabilistic Conditional Independence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-428-435</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logical inference algorithms for conditional independence (CI) statements
have important applications from testing consistency during knowledge
elicitation to constraintbased structure learning of graphical models. We prove
that the implication problem for CI statements is decidable, given that the
size of the domains of the random variables is known and fixed. We will present
an approximate logical inference algorithm which combines a falsification and a
novel validation algorithm. The validation algorithm represents each set of CI
statements as a sparse 0-1 matrix A and validates instances of the implication
problem by solving specific linear programs with constraint matrix A. We will
show experimentally that the algorithm is both effective and efficient in
validating and falsifying instances of the probabilistic CI implication
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2622</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2622</id><created>2012-05-09</created><authors><author><keyname>Mostafavi</keyname><forenames>Sara</forenames></author><author><keyname>Morris</keyname><forenames>Quaid</forenames></author></authors><title>Using the Gene Ontology Hierarchy when Predicting Gene Function</title><categories>cs.LG cs.CE stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-419-427</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of multilabel classification when the labels are related through
a hierarchical categorization scheme occurs in many application domains such as
computational biology. For example, this problem arises naturally when trying
to automatically assign gene function using a controlled vocabularies like Gene
Ontology. However, most existing approaches for predicting gene functions solve
independent classification problems to predict genes that are involved in a
given function category, independently of the rest. Here, we propose two simple
methods for incorporating information about the hierarchical nature of the
categorization scheme. In the first method, we use information about a gene's
previous annotation to set an initial prior on its label. In a second approach,
we extend a graph-based semi-supervised learning algorithm for predicting gene
function in a hierarchy. We show that we can efficiently solve this problem by
solving a linear system of equations. We compare these approaches with a
previous label reconciliation-based approach. Results show that using the
hierarchy information directly, compared to using reconciliation methods,
improves gene function prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2623</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2623</id><created>2012-05-09</created><authors><author><keyname>Minka</keyname><forenames>Thomas P.</forenames><affiliation>Alan</affiliation></author><author><keyname>Xiang</keyname><forenames>Rongjing</forenames><affiliation>Alan</affiliation></author><author><keyname>Yuan</keyname><affiliation>Alan</affiliation></author><author><keyname>Qi</keyname></author></authors><title>Virtual Vector Machine for Bayesian Online Classification</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-411-418</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a typical online learning scenario, a learner is required to process a
large data stream using a small memory buffer. Such a requirement is usually in
conflict with a learner's primary pursuit of prediction accuracy. To address
this dilemma, we introduce a novel Bayesian online classi cation algorithm,
called the Virtual Vector Machine. The virtual vector machine allows you to
smoothly trade-off prediction accuracy with memory size. The virtual vector
machine summarizes the information contained in the preceding data stream by a
Gaussian distribution over the classi cation weights plus a constant number of
virtual data points. The virtual data points are designed to add extra
non-Gaussian information about the classi cation weights. To maintain the
constant number of virtual points, the virtual vector machine adds the current
real data point into the virtual point set, merges two most similar virtual
points into a new virtual point or deletes a virtual point that is far from the
decision boundary. The information lost in this process is absorbed into the
Gaussian distribution. The extra information provided by the virtual points
leads to improved predictive accuracy over previous online classification
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2624</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2624</id><created>2012-05-09</created><authors><author><keyname>Meshi</keyname><forenames>Ofer</forenames></author><author><keyname>Jaimovich</keyname><forenames>Ariel</forenames></author><author><keyname>Globerson</keyname><forenames>Amir</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>Convexifying the Bethe Free Energy</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-402-410</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of loopy belief propagation (LBP) revitalized the
application of graphical models in many domains. Many recent works present
improvements on the basic LBP algorithm in an attempt to overcome convergence
and local optima problems. Notable among these are convexified free energy
approximations that lead to inference procedures with provable convergence and
quality properties. However, empirically LBP still outperforms most of its
convex variants in a variety of settings, as we also demonstrate here.
Motivated by this fact we seek convexified free energies that directly
approximate the Bethe free energy. We show that the proposed approximations
compare favorably with state-of-the art convex free energy approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2625</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2625</id><created>2012-05-09</created><authors><author><keyname>Meltzer</keyname><forenames>Talya</forenames></author><author><keyname>Globerson</keyname><forenames>Amir</forenames></author><author><keyname>Weiss</keyname><forenames>Yair</forenames></author></authors><title>Convergent message passing algorithms - a unifying view</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-393-401</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Message-passing algorithms have emerged as powerful techniques for
approximate inference in graphical models. When these algorithms converge, they
can be shown to find local (or sometimes even global) optima of variational
formulations to the inference problem. But many of the most popular algorithms
are not guaranteed to converge. This has lead to recent interest in convergent
message-passing algorithms. In this paper, we present a unified view of
convergent message-passing algorithms. We present a simple derivation of an
abstract algorithm, tree-consistency bound optimization (TCBO) that is provably
convergent in both its sum and max product forms. We then show that many of the
existing convergent algorithms are instances of our TCBO algorithm, and obtain
novel convergent algorithms &quot;for free&quot; by exchanging maximizations and
summations in existing algorithms. In particular, we show that Wainwright's
non-convergent sum-product algorithm for tree based variational bounds, is
actually convergent with the right update order for the case where trees are
monotonic chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2626</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2626</id><created>2012-05-09</created><authors><author><keyname>Marlin</keyname><forenames>Benjamin</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author></authors><title>Group Sparse Priors for Covariance Estimation</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-383-392</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it has become popular to learn sparse Gaussian graphical models
(GGMs) by imposing l1 or group l1,2 penalties on the elements of the precision
matrix. Thispenalized likelihood approach results in a tractable convex
optimization problem. In this paper, we reinterpret these results as performing
MAP estimation under a novel prior which we call the group l1 and l1,2
positivedefinite matrix distributions. This enables us to build a hierarchical
model in which the l1 regularization terms vary depending on which group the
entries are assigned to, which in turn allows us to learn block structured
sparse GGMs with unknown group assignments. Exact inference in this
hierarchical model is intractable, due to the need to compute the normalization
constant of these matrix distributions. However, we derive upper bounds on the
partition functions, which lets us use fast variational inference (optimizing a
lower bound on the joint posterior). We show that on two real world data sets
(motion capture and financial data), our method which infers the block
structure outperforms a method that uses a fixed block structure, which in turn
outperforms baseline methods that ignore block structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2627</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2627</id><created>2012-05-09</created><authors><author><keyname>Mao</keyname><forenames>Yi</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author></authors><title>Domain Knowledge Uncertainty and Probabilistic Parameter Constraints</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-375-382</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating domain knowledge into the modeling process is an effective way
to improve learning accuracy. However, as it is provided by humans, domain
knowledge can only be specified with some degree of uncertainty. We propose to
explicitly model such uncertainty through probabilistic constraints over the
parameter space. In contrast to hard parameter constraints, our approach is
effective also when the domain knowledge is inaccurate and generally results in
superior modeling accuracy. We focus on generative and conditional modeling
where the parameters are assigned a Dirichlet or Gaussian prior and demonstrate
the framework with experiments on both synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2628</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2628</id><created>2012-05-09</created><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author></authors><title>Multiple Source Adaptation and the Renyi Divergence</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-367-374</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel theoretical study of the general problem of
multiple source adaptation using the notion of Renyi divergence. Our results
build on our previous work [12], but significantly broaden the scope of that
work in several directions. We extend previous multiple source loss guarantees
based on distribution weighted combinations to arbitrary target distributions
P, not necessarily mixtures of the source distributions, analyze both known and
unknown target distribution cases, and prove a lower bound. We further extend
our bounds to deal with the case where the learner receives an approximate
distribution for each source instead of the exact one, and show that similar
loss guarantees can be achieved depending on the divergence between the
approximate and true distributions. We also analyze the case where the labeling
functions of the source domains are somewhat different. Finally, we report the
results of experiments with both an artificial data set and a sentiment
analysis task, showing the performance benefits of the distribution weighted
combinations and the quality of our bounds based on the Renyi divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2629</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2629</id><created>2012-05-09</created><authors><author><keyname>Lyu</keyname><forenames>Siwei</forenames></author></authors><title>Interpretation and Generalization of Score Matching</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-359-366</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score matching is a recently developed parameter learning method that is
particularly effective to complicated high dimensional density models with
intractable partition functions. In this paper, we study two issues that have
not been completely resolved for score matching. First, we provide a formal
link between maximum likelihood and score matching. Our analysis shows that
score matching finds model parameters that are more robust with noisy training
data. Second, we develop a generalization of score matching. Based on this
generalization, we further demonstrate an extension of score matching to models
of discrete data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2630</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2630</id><created>2012-05-09</created><authors><author><keyname>Lubin</keyname><forenames>Benjamin</forenames></author><author><keyname>Parkes</keyname><forenames>David C.</forenames></author></authors><title>Quantifying the Strategyproofness of Mechanisms via Metrics on Payoff
  Distributions</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-349-358</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strategyproof mechanisms provide robust equilibrium with minimal assumptions
about knowledge and rationality but can be unachievable in combination with
other desirable properties such as budget-balance, stability against deviations
by coalitions, and computational tractability. In the search for
maximally-strategyproof mechanisms that simultaneously satisfy other desirable
properties, we introduce a new metric to quantify the strategyproofness of a
mechanism, based on comparing the payoff distribution, given truthful reports,
against that of a strategyproof &quot;reference&quot; mechanism that solves a problem
relaxation. Focusing on combinatorial exchanges, we demonstrate that the metric
is informative about the eventual equilibrium, where simple regretbased metrics
are not, and can be used for online selection of an effective mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2631</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2631</id><created>2012-05-09</created><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Ji</keyname><forenames>Shuiwang</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Multi-Task Feature Learning Via Efficient l2,1-Norm Minimization</title><categories>cs.LG cs.CV stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-339-348</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of joint feature selection across a group of related tasks has
applications in many areas including biomedical informatics and computer
vision. We consider the l2,1-norm regularized regression model for joint
feature selection from multiple tasks, which can be derived in the
probabilistic framework by assuming a suitable prior from the exponential
family. One appealing feature of the l2,1-norm regularization is that it
encourages multiple predictors to share similar sparsity patterns. However, the
resulting optimization problem is challenging to solve due to the
non-smoothness of the l2,1-norm regularization. In this paper, we propose to
accelerate the computation by reformulating it as two equivalent smooth convex
optimization problems which are then solved via the Nesterov's method-an
optimal first-order black-box method for smooth convex optimization. A key
building block in solving the reformulations is the Euclidean projection. We
show that the Euclidean projection for the first reformulation can be
analytically computed, while the Euclidean projection for the second one can be
computed in linear time. Empirical evaluations on several data sets verify the
efficiency of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2632</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2632</id><created>2012-05-09</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Improving Compressed Counting</title><categories>cs.DS cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-329-338</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Counting (CC) [22] was recently proposed for estimating the ath
frequency moments of data streams, where 0 &lt; a &lt;= 2. CC can be used for
estimating Shannon entropy, which can be approximated by certain functions of
the ath frequency moments as a -&gt; 1. Monitoring Shannon entropy for anomaly
detection (e.g., DDoS attacks) in large networks is an important task. This
paper presents a new algorithm for improving CC. The improvement is most
substantial when a -&gt; 1--. For example, when a = 0:99, the new algorithm
reduces the estimation variance roughly by 100-fold. This new algorithm would
make CC considerably more practical for estimating Shannon entropy.
Furthermore, the new algorithm is statistically optimal when a = 0.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2633</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2633</id><created>2012-05-09</created><authors><author><keyname>Kumar</keyname><forenames>M. Pawan</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-313-320</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of obtaining the maximum a posteriori estimate of
discrete pairwise random fields with arbitrary unary potentials and semimetric
pairwise potentials. For this problem, we propose an accurate hierarchical move
making strategy where each move is computed efficiently by solving an st-MINCUT
problem. Unlike previous move making approaches, e.g. the widely used
a-expansion algorithm, our method obtains the guarantees of the standard linear
programming (LP) relaxation for the important special case of metric labeling.
Unlike the existing LP relaxation solvers, e.g. interior-point algorithms or
tree-reweighted message passing, our method is significantly faster as it uses
only the efficient st-MINCUT algorithm in its design. Using both synthetic and
real data experiments, we show that our technique outperforms several commonly
used algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2634</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2634</id><created>2012-05-09</created><authors><author><keyname>Kleinberg</keyname><forenames>Samantha</forenames></author><author><keyname>Mishra</keyname><forenames>Bud</forenames></author></authors><title>The Temporal Logic of Causal Structures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-303-312</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational analysis of time-course data with an underlying causal
structure is needed in a variety of domains, including neural spike trains,
stock price movements, and gene expression levels. However, it can be
challenging to determine from just the numerical time course data alone what is
coordinating the visible processes, to separate the underlying prima facie
causes into genuine and spurious causes and to do so with a feasible
computational complexity. For this purpose, we have been developing a novel
algorithm based on a framework that combines notions of causality in philosophy
with algorithmic approaches built on model checking and statistical techniques
for multiple hypotheses testing. The causal relationships are described in
terms of temporal logic formulae, reframing the inference problem in terms of
model checking. The logic used, PCTL, allows description of both the time
between cause and effect and the probability of this relationship being
observed. We show that equipped with these causal formulae with their
associated probabilities we may compute the average impact a cause makes to its
effect and then discover statistically significant causes through the concepts
of multiple hypothesis testing (treating each causal relationship as a
hypothesis), and false discovery control. By exploring a well-chosen family of
potentially all significant hypotheses with reasonably minimal description
length, it is possible to tame the algorithm's computational complexity while
exploring the nearly complete search-space of all prima facie causes. We have
tested these ideas in a number of domains and illustrate them here with two
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2635</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2635</id><created>2012-05-09</created><authors><author><keyname>Kisynski</keyname><forenames>Jacek</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Constraint Processing in Lifted Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-293-302</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First-order probabilistic models combine representational power of
first-order logic with graphical models. There is an ongoing effort to design
lifted inference algorithms for first-order probabilistic models. We analyze
lifted inference from the perspective of constraint processing and, through
this viewpoint, we analyze and compare existing approaches and expose their
advantages and limitations. Our theoretical results show that the wrong choice
of constraint processing method can lead to exponential increase in
computational complexity. Our empirical tests confirm the importance of
constraint processing in lifted inference. This is the first theoretical and
empirical study of constraint processing in lifted inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2636</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2636</id><created>2012-05-09</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames></author></authors><title>Monolingual Probabilistic Programming Using Generalized Coroutines</title><categories>cs.PL cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-285-292</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic programming languages and modeling toolkits are two modular
ways to build and reuse stochastic models and inference procedures. Combining
strengths of both, we express models and inference as generalized coroutines in
the same general-purpose language. We use existing facilities of the language,
such as rich libraries, optimizing compilers, and types, to develop concise,
declarative, and realistic models with competitive performance on exact and
approximate inference. In particular, a wide range of models can be expressed
using memoization. Because deterministic parts of models run at full speed,
custom inference procedures are trivial to incorporate, and inference
procedures can reason about themselves without interpretive overhead. Within
this framework, we introduce a new, general algorithm for importance sampling
with look-ahead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2637</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2637</id><created>2012-05-09</created><authors><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Ahmadi</keyname><forenames>Babak</forenames></author><author><keyname>Natarajan</keyname><forenames>Sriraam</forenames></author></authors><title>Counting Belief Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-277-284</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major benefit of graphical models is that most knowledge is captured in the
model structure. Many models, however, produce inference problems with a lot of
symmetries not reflected in the graphical structure and hence not exploitable
by efficient inference techniques such as belief propagation (BP). In this
paper, we present a new and simple BP algorithm, called counting BP, that
exploits such additional symmetries. Starting from a given factor graph,
counting BP first constructs a compressed factor graph of clusternodes and
clusterfactors, corresponding to sets of nodes and factors that are
indistinguishable given the evidence. Then it runs a modified BP algorithm on
the compressed graph that is equivalent to running BP on the original factor
graph. Our experiments show that counting BP is applicable to a variety of
important AI tasks such as (dynamic) relational models and boolean model
counting, and that significant efficiency gains are obtainable, often by orders
of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2638</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2638</id><created>2012-05-09</created><authors><author><keyname>Jiang</keyname><forenames>Albert Xin</forenames></author><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Temporal Action-Graph Games: A New Representation for Dynamic Games</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-268-276</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce temporal action graph games (TAGGs), a novel
graphical representation of imperfect-information extensive form games. We show
that when a game involves anonymity or context-specific utility independencies,
its encoding as a TAGG can be much more compact than its direct encoding as a
multiagent influence diagram (MAID).We also show that TAGGs can be understood
as indirect MAID encodings in which many deterministic chance nodes are
introduced. We provide an algorithm for computing with TAGGs, and show both
theoretically and empirically that our approach improves significantly on the
previous state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2639</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2639</id><created>2012-05-09</created><authors><author><keyname>Jebara</keyname><forenames>Tony S.</forenames></author></authors><title>MAP Estimation, Message Passing, and Perfect Graphs</title><categories>cs.AI cs.DM cs.DS</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-258-267</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficiently finding the maximum a posteriori (MAP) configuration of a
graphical model is an important problem which is often implemented using
message passing algorithms. The optimality of such algorithms is only well
established for singly-connected graphs and other limited settings. This
article extends the set of graphs where MAP estimation is in P and where
message passing recovers the exact solution to so-called perfect graphs. This
result leverages recent progress in defining perfect graphs (the strong perfect
graph theorem), linear programming relaxations of MAP estimation and recent
convergent message passing schemes. The article converts graphical models into
nand Markov random fields which are straightforward to relax into linear
programs. Therein, integrality can be established in general by testing for
graph perfection. This perfection test is performed efficiently using a
polynomial time algorithm. Alternatively, known decomposition tools from
perfect graph theory may be used to prove perfection for certain families of
graphs. Thus, a general graph framework is provided for determining when MAP
estimation in any graphical model is in P, has an integral linear programming
relaxation and is exactly recoverable by message passing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2640</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2640</id><created>2012-05-09</created><authors><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Peters</keyname><forenames>Jonas</forenames></author><author><keyname>Mooij</keyname><forenames>Joris</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Identifying confounders using additive noise models</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-249-257</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for inferring the existence of a latent common cause
('confounder') of two observed random variables. The method assumes that the
two effects of the confounder are (possibly nonlinear) functions of the
confounder plus independent, additive noise. We discuss under which conditions
the model is identifiable (up to an arbitrary reparameterization of the
confounder) from the joint distribution of the effects. We state and prove a
theoretical result that provides evidence for the conjecture that the model is
generically identifiable under suitable technical conditions. In addition, we
propose a practical method to estimate the confounder from a finite i.i.d.
sample of the effects and illustrate that the method works well on both
simulated and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2641</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2641</id><created>2012-05-09</created><authors><author><keyname>Hoyer</keyname><forenames>Patrik O.</forenames></author><author><keyname>Hyttinen</keyname><forenames>Antti</forenames></author></authors><title>Bayesian Discovery of Linear Acyclic Causal Models</title><categories>stat.ML cs.LG stat.ME</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-240-248</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods for automated discovery of causal relationships from
non-interventional data have received much attention recently. A widely used
and well understood model family is given by linear acyclic causal models
(recursive structural equation models). For Gaussian data both constraint-based
methods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalence
class) and Bayesian score-based methods (Geiger and Heckerman, 1994) (which
assign relative scores to the equivalence classes) are available. On the
contrary, all current methods able to utilize non-Gaussianity in the data
(Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph or
a single equivalence class, and so are fundamentally unable to express the
degree of certainty attached to that output. In this paper we develop a
Bayesian score-based approach able to take advantage of non-Gaussianity when
estimating linear acyclic causal models, and we empirically demonstrate that,
at least on very modest size networks, its accuracy is as good as or better
than existing methods. We provide a complete code package (in R) which
implements all algorithms and performs all of the analysis provided in the
paper, and hope that this will further the application of these methods to
solving causal inference problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2642</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2642</id><created>2012-05-09</created><authors><author><keyname>Hooper</keyname><forenames>Peter</forenames></author><author><keyname>Abbasi-Yadkori</keyname><forenames>Yasin</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author><author><keyname>Hoehn</keyname><forenames>Bret</forenames></author></authors><title>Improved Mean and Variance Approximations for Belief Net Responses via
  Network Doubling</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-232-239</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian belief network models a joint distribution with an directed
acyclic graph representing dependencies among variables and network parameters
characterizing conditional distributions. The parameters are viewed as random
variables to quantify uncertainty about their values. Belief nets are used to
compute responses to queries; i.e., conditional probabilities of interest. A
query is a function of the parameters, hence a random variable. Van Allen et
al. (2001, 2008) showed how to quantify uncertainty about a query via a delta
method approximation of its variance. We develop more accurate approximations
for both query mean and variance. The key idea is to extend the query mean
approximation to a &quot;doubled network&quot; involving two independent replicates. Our
method assumes complete data and can be applied to discrete, continuous, and
hybrid networks (provided discrete variables have only discrete parents). We
analyze several improvements, and provide empirical studies to demonstrate
their effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2643</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2643</id><created>2012-05-09</created><authors><author><keyname>Hoffman</keyname><forenames>Matthias</forenames></author><author><keyname>Kueck</keyname><forenames>Hendrik</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author></authors><title>New inference strategies for solving Markov Decision Processes using
  reversible jump MCMC</title><categories>cs.LG cs.SY math.OC stat.CO stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-223-231</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we build on previous work which uses inferences techniques, in
particular Markov Chain Monte Carlo (MCMC) methods, to solve parameterized
control problems. We propose a number of modifications in order to make this
approach more practical in general, higher-dimensional spaces. We first
introduce a new target distribution which is able to incorporate more reward
information from sampled trajectories. We also show how to break strong
correlations between the policy parameters and sampled trajectories in order to
sample more freely. Finally, we show how to incorporate these techniques in a
principled manner to obtain estimates of the optimal policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2644</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2644</id><created>2012-05-09</created><authors><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author><author><keyname>Hong</keyname><forenames>Sue Ann</forenames></author><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author></authors><title>First-Order Mixed Integer Linear Programming</title><categories>cs.LO cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-213-222</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed integer linear programming (MILP) is a powerful representation often
used to formulate decision-making problems under uncertainty. However, it lacks
a natural mechanism to reason about objects, classes of objects, and relations.
First-order logic (FOL), on the other hand, excels at reasoning about classes
of objects, but lacks a rich representation of uncertainty. While representing
propositional logic in MILP has been extensively explored, no theory exists yet
for fully combining FOL with MILP. We propose a new representation, called
first-order programming or FOP, which subsumes both FOL and MILP. We establish
formal methods for reasoning about first order programs, including a sound and
complete lifted inference procedure for integer first order programs. Since FOP
can offer exponential savings in representation and proof size compared to FOL,
and since representations and proofs are never significantly longer in FOP than
in FOL, we anticipate that inference in FOP will be more tractable than
inference in FOL for corresponding problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2645</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2645</id><created>2012-05-09</created><authors><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Low</keyname><forenames>Yucheng</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos E.</forenames></author><author><keyname>O'Hallaron</keyname><forenames>David</forenames></author></authors><title>Distributed Parallel Inference on Large Factor Graphs</title><categories>cs.AI cs.DC</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-203-212</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As computer clusters become more common and the size of the problems
encountered in the field of AI grows, there is an increasing demand for
efficient parallel inference algorithms. We consider the problem of parallel
inference on large factor graphs in the distributed memory setting of computer
clusters. We develop a new efficient parallel inference algorithm, DBRSplash,
which incorporates over-segmented graph partitioning, belief residual
scheduling, and uniform work Splash operations. We empirically evaluate the
DBRSplash algorithm on a 120 processor cluster and demonstrate linear to
super-linear performance gains on large factor graph models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2646</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2646</id><created>2012-05-09</created><authors><author><keyname>Ganchev</keyname><forenames>Kuzman</forenames></author><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Nevmyvaka</keyname><forenames>Yuriy</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>Censored Exploration and the Dark Pool Problem</title><categories>cs.LG cs.GT</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-185-194</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze a natural algorithm for multi-venue exploration from
censored data, which is motivated by the Dark Pool Problem of modern
quantitative finance. We prove that our algorithm converges in polynomial time
to a near-optimal allocation policy; prior results for similar problems in
stochastic inventory control guaranteed only asymptotic convergence and
examined variants in which each venue could be treated independently. Our
analysis bears a strong resemblance to that of efficient exploration/
exploitation schemes in the reinforcement learning literature. We describe an
extensive experimental evaluation of our algorithm on the Dark Pool Problem
using real trading data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2647</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2647</id><created>2012-05-09</created><authors><author><keyname>Fritz</keyname><forenames>Christian</forenames></author><author><keyname>McIlraith</keyname><forenames>Sheila</forenames></author></authors><title>Generating Optimal Plans in Highly-Dynamic Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-177-184</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating optimal plans in highly dynamic environments is challenging. Plans
are predicated on an assumed initial state, but this state can change
unexpectedly during plan generation, potentially invalidating the planning
effort. In this paper we make three contributions: (1) We propose a novel
algorithm for generating optimal plans in settings where frequent, unexpected
events interfere with planning. It is able to quickly distinguish relevant from
irrelevant state changes, and to update the existing planning search tree if
necessary. (2) We argue for a new criterion for evaluating plan adaptation
techniques: the relative running time compared to the &quot;size&quot; of changes. This
is significant since during recovery more changes may occur that need to be
recovered from subsequently, and in order for this process of repeated recovery
to terminate, recovery time has to converge. (3) We show empirically that our
approach can converge and find optimal plans in environments that would
ordinarily defy planning due to their high dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2648</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2648</id><created>2012-05-09</created><authors><author><keyname>Fan</keyname><forenames>Yu</forenames></author><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author></authors><title>Learning Continuous-Time Social Network Dynamics</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-161-168</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that a number of sociology models for social network dynamics
can be viewed as continuous time Bayesian networks (CTBNs). A sampling-based
approximate inference method for CTBNs can be used as the basis of an
expectation-maximization procedure that achieves better accuracy in estimating
the parameters of the model than the standard method of moments
algorithmfromthe sociology literature. We extend the existing social network
models to allow for indirect and asynchronous observations of the links. A
Markov chain Monte Carlo sampling algorithm for this new model permits
estimation and inference. We provide results on both a synthetic network (for
verification) and real social network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2649</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2649</id><created>2012-05-09</created><authors><author><keyname>Dudik</keyname><forenames>Miroslav</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author></authors><title>A Sampling-Based Approach to Computing Equilibria in Succinct
  Extensive-Form Games</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-151-160</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central task of artificial intelligence is the design of artificial agents
that act towards specified goals in partially observed environments. Since such
environments frequently include interaction over time with other agents with
their own goals, reasoning about such interaction relies on sequential
game-theoretic models such as extensive-form games or some of their succinct
representations such as multi-agent influence diagrams. The current algorithms
for calculating equilibria either work with inefficient representations,
possibly doubly exponential inthe number of time steps, or place strong
assumptions on the game structure. In this paper,we propose a sampling-based
approach, which calculates extensive-form correlated equilibria with small
representations without placing such strong assumptions. Thus, it is practical
in situations where the previous approaches would fail. In addition, our
algorithm allows control over characteristics of the target equilibrium, e.g.,
we can ask for an equilibrium with high social welfare. Our approach is based
on a multiplicativeweight update algorithm analogous to AdaBoost, and Markov
chain Monte Carlo sampling. We prove convergence guarantees and explore the
utility of our approach on several moderately sized multi-player games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2650</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2650</id><created>2012-05-09</created><authors><author><keyname>Doshi-Velez</keyname><forenames>Finale</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Correlated Non-Parametric Latent Feature Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-143-150</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are often interested in explaining data through a set of hidden factors or
features. When the number of hidden features is unknown, the Indian Buffet
Process (IBP) is a nonparametric latent feature model that does not bound the
number of active features in dataset. However, the IBP assumes that all latent
features are uncorrelated, making it inadequate for many realworld problems. We
introduce a framework for correlated nonparametric feature models, generalising
the IBP. We use this framework to generate several specific models and
demonstrate applications on realworld datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2651</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2651</id><created>2012-05-09</created><authors><author><keyname>Crowley</keyname><forenames>Mark</forenames></author><author><keyname>Nelson</keyname><forenames>John</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal
  Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-126-134</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a challenging real-world planning problem where actions must be
taken at each location in a spatial area at each point in time. We use forestry
planning as the motivating application. In Large Scale Spatial-Temporal (LSST)
planning problems, the state and action spaces are defined as the
cross-products of many local state and action spaces spread over a large
spatial area such as a city or forest. These problems possess state
uncertainty, have complex utility functions involving spatial constraints and
we generally must rely on simulations rather than an explicit transition model.
We define LSST problems as reinforcement learning problems and present a
solution using policy gradients. We compare two different policy formulations:
an explicit policy that identifies each location in space and the action to
take there; and an abstract policy that defines the proportion of actions to
take across all locations in space. We show that the abstract policy is more
robust and achieves higher rewards with far fewer parameters than the
elementary policy. This abstract policy is also a better fit to the properties
that practitioners in LSST problem domains require for such methods to be
widely useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2652</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2652</id><created>2012-05-09</created><authors><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author><author><keyname>Polastro</keyname><forenames>Rodrigo Bellizia</forenames></author></authors><title>Complexity Analysis and Variational Inference for Interpretation-based
  Probabilistic Description Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-117-125</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents complexity analysis and variational methods for inference
in probabilistic description logics featuring Boolean operators,
quantification, qualified number restrictions, nominals, inverse roles and role
hierarchies. Inference is shown to be PEXP-complete, and variational methods
are designed so as to exploit logical inference whenever possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2653</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2653</id><created>2012-05-09</created><authors><author><keyname>Cortes</keyname><forenames>Corinna</forenames></author><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author></authors><title>L2 Regularization for Learning Kernels</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-109-116</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The choice of the kernel is critical to the success of many learning
algorithms but it is typically left to the user. Instead, the training data can
be used to learn the kernel by selecting it out of a given family, such as that
of non-negative linear combinations of p base kernels, constrained by a trace
or L1 regularization. This paper studies the problem of learning kernels with
the same family of kernels but with an L2 regularization instead, and for
regression problems. We analyze the problem of learning kernels with ridge
regression. We derive the form of the solution of the optimization problem and
give an efficient iterative algorithm for computing that solution. We present a
novel theoretical analysis of the problem based on stability and give learning
bounds for orthogonal kernels that contain only an additive term O(pp/m) when
compared to the standard kernel ridge regression stability bound. We also
report the results of experiments indicating that L1 regularization can lead to
modest improvements for a small number of kernels, but to performance
degradations in larger-scale cases. In contrast, L2 regularization never
degrades performance and in fact achieves significant improvements with a large
number of kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2654</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2654</id><created>2012-05-09</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author></authors><title>Prediction Markets, Mechanism Design, and Cooperative Game Theory</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-101-108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction markets are designed to elicit information from multiple agents in
order to predict (obtain probabilities for) future events. A good prediction
market incentivizes agents to reveal their information truthfully; such
incentive compatibility considerations are commonly studied in mechanism
design. While this relation between prediction markets and mechanism design is
well understood at a high level, the models used in prediction markets tend to
be somewhat different from those used in mechanism design. This paper considers
a model for prediction markets that fits more straightforwardly into the
mechanism design framework. We consider a number of mechanisms within this
model, all based on proper scoring rules. We discuss basic properties of these
mechanisms, such as incentive compatibility. We also draw connections between
some of these mechanisms and cooperative game theory. Finally, we speculate how
one might build a practical prediction market based on some of these ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2655</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2655</id><created>2012-05-09</created><authors><author><keyname>Cohn</keyname><forenames>Ido</forenames></author><author><keyname>El-Hay</keyname><forenames>Tal</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Kupferman</keyname><forenames>Raz</forenames></author></authors><title>Mean Field Variational Approximation for Continuous-Time Bayesian
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-91-100</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous-time Bayesian networks is a natural structured representation
language for multicomponent stochastic processes that evolve continuously over
time. Despite the compact representation, inference in such models is
intractable even in relatively simple structured networks. Here we introduce a
mean field variational approximation in which we use a product of inhomogeneous
Markov processes to approximate a distribution over trajectories. This
variational approach leads to a globally consistent distribution, which can be
efficiently queried. Additionally, it provides a lower bound on the probability
of observations, thus making it attractive for learning tasks. We provide the
theoretical foundations for the approximation, an efficient implementation that
exploits the wide range of highly optimized ordinary differential equations
(ODE) solvers, experimentally explore characterizations of processes for which
this approximation is suitable, and show applications to a large-scale
realworld inference problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2656</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2656</id><created>2012-05-09</created><authors><author><keyname>Bradley</keyname><forenames>David M.</forenames></author><author><keyname>Bagnell</keyname><forenames>J Andrew</forenames></author></authors><title>Convex Coding</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-83-90</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by recent work on convex formulations of clustering (Lashkari &amp;
Golland, 2008; Nowozin &amp; Bakir, 2008) we investigate a new formulation of the
Sparse Coding Problem (Olshausen &amp; Field, 1997). In sparse coding we attempt to
simultaneously represent a sequence of data-vectors sparsely (i.e. sparse
approximation (Tropp et al., 2006)) in terms of a 'code' defined by a set of
basis elements, while also finding a code that enables such an approximation.
As existing alternating optimization procedures for sparse coding are
theoretically prone to severe local minima problems, we propose a convex
relaxation of the sparse coding problem and derive a boosting-style algorithm,
that (Nowozin &amp; Bakir, 2008) serves as a convex 'master problem' which calls a
(potentially non-convex) sub-problem to identify the next code element to add.
Finally, we demonstrate the properties of our boosted coding algorithm on an
image denoising task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2657</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2657</id><created>2012-05-09</created><authors><author><keyname>Boyd-Graber</keyname><forenames>Jordan</forenames></author><author><keyname>Blei</keyname><forenames>David</forenames></author></authors><title>Multilingual Topic Models for Unaligned Text</title><categories>cs.CL cs.IR cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-75-82</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the multilingual topic model for unaligned text (MuTo), a
probabilistic model of text that is designed to analyze corpora composed of
documents in two languages. From these documents, MuTo uses stochastic EM to
simultaneously discover both a matching between the languages and multilingual
latent topics. We demonstrate that MuTo is able to find shared topics on
real-world multilingual corpora, successfully pairing related documents across
languages. MuTo provides a new framework for creating multilingual topic models
without needing carefully curated parallel corpora and allows applications
built using the topic model formalism to be applied to a much wider class of
corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2658</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2658</id><created>2012-05-09</created><authors><author><keyname>Bouchard-Cote</keyname><forenames>Alexandre</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Optimization of Structured Mean Field Objectives</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-67-74</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In intractable, undirected graphical models, an intuitive way of creating
structured mean field approximations is to select an acyclic tractable
subgraph. We show that the hardness of computing the objective function and
gradient of the mean field objective qualitatively depends on a simple graph
property. If the tractable subgraph has this property- we call such subgraphs
v-acyclic-a very fast block coordinate ascent algorithm is possible. If not,
optimization is harder, but we show a new algorithm based on the construction
of an auxiliary exponential family that can be used to make inference possible
in this case as well. We discuss the advantages and disadvantages of each
regime and compare the algorithms empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2659</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2659</id><created>2012-05-09</created><authors><author><keyname>Bonet</keyname><forenames>Blai</forenames></author></authors><title>Deterministic POMDPs Revisited</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-59-66</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a subclass of POMDPs, called Deterministic POMDPs, that is
characterized by deterministic actions and observations. These models do not
provide the same generality of POMDPs yet they capture a number of interesting
and challenging problems, and permit more efficient algorithms. Indeed, some of
the recent work in planning is built around such assumptions mainly by the
quest of amenable models more expressive than the classical deterministic
models. We provide results about the fundamental properties of Deterministic
POMDPs, their relation with AND/OR search problems and algorithms, and their
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2660</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2660</id><created>2012-05-09</created><authors><author><keyname>Bellare</keyname><forenames>Kedar</forenames></author><author><keyname>Druck</keyname><forenames>Gregory</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Alternating Projections for Learning with Expectation Constraints</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-43-50</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an objective function for learning with unlabeled data that
utilizes auxiliary expectation constraints. We optimize this objective function
using a procedure that alternates between information and moment projections.
Our method provides an alternate interpretation of the posterior regularization
framework (Graca et al., 2008), maintains uncertainty during optimization
unlike constraint-driven learning (Chang et al., 2007), and is more efficient
than generalized expectation criteria (Mann &amp; McCallum, 2008). Applications of
this framework include minimally supervised learning, semisupervised learning,
and learning with constraints that are more expressive than the underlying
model. In experiments, we demonstrate comparable accuracy to generalized
expectation criteria for minimally supervised learning, and use expressive
structural constraints to guide semi-supervised learning, providing a 3%-6%
improvement over stateof-the-art constraint-driven learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2661</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2661</id><created>2012-05-09</created><authors><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>REGAL: A Regularization based Algorithm for Reinforcement Learning in
  Weakly Communicating MDPs</title><categories>cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-35-42</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an algorithm that achieves the optimal regret rate in an unknown
weakly communicating Markov Decision Process (MDP). The algorithm proceeds in
episodes where, in each episode, it picks a policy using regularization based
on the span of the optimal bias vector. For an MDP with S states and A actions
whose optimal bias vector has span bounded by H, we show a regret bound of
~O(HSpAT). We also relate the span to various diameter-like quantities
associated with the MDP, demonstrating how our results improve on previous
regret bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2662</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2662</id><created>2012-05-09</created><authors><author><keyname>Asuncion</keyname><forenames>Arthur</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Smyth</keyname><forenames>Padhraic</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>On Smoothing and Inference for Topic Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-27-34</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet analysis, or topic modeling, is a flexible latent variable
framework for modeling high-dimensional sparse count data. Various learning
algorithms have been developed in recent years, including collapsed Gibbs
sampling, variational inference, and maximum a posteriori estimation, and this
variety motivates the need for careful empirical comparisons. In this paper, we
highlight the close connections between these approaches. We find that the main
differences are attributable to the amount of smoothing applied to the counts.
When the hyperparameters are optimized, the differences in performance among
the algorithms diminish significantly. The ability of these algorithms to
achieve solutions of comparable accuracy gives us the freedom to select
computationally efficient approaches. Using the insights gained from this
comparative study, we show how accurate topic models can be learned in several
seconds on text corpora with thousands of documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2663</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2663</id><created>2012-05-11</created><authors><author><keyname>Penatti</keyname><forenames>Otavio A. B.</forenames></author><author><keyname>Valle</keyname><forenames>Eduardo</forenames></author><author><keyname>Torres</keyname><forenames>Ricardo da S.</forenames></author></authors><title>Are visual dictionaries generalizable?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mid-level features based on visual dictionaries are today a cornerstone of
systems for classification and retrieval of images. Those state-of-the-art
representations depend crucially on the choice of a codebook (visual
dictionary), which is usually derived from the dataset. In general-purpose,
dynamic image collections (e.g., the Web), one cannot have the entire
collection in order to extract a representative dictionary. However, based on
the hypothesis that the dictionary reflects only the diversity of low-level
appearances and does not capture semantics, we argue that a dictionary based on
a small subset of the data, or even on an entirely different dataset, is able
to produce a good representation, provided that the chosen images span a
diverse enough portion of the low-level feature space. Our experiments confirm
that hypothesis, opening the opportunity to greatly alleviate the burden in
generating the codebook, and confirming the feasibility of employing visual
dictionaries in large-scale dynamic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2664</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2664</id><created>2012-05-09</created><authors><author><keyname>Asmuth</keyname><forenames>John</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Nouri</keyname><forenames>Ali</forenames></author><author><keyname>Wingate</keyname><forenames>David</forenames></author></authors><title>A Bayesian Sampling Approach to Exploration in Reinforcement Learning</title><categories>cs.LG</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-19-26</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a modular approach to reinforcement learning that uses a Bayesian
representation of the uncertainty over models. The approach, BOSS (Best of
Sampled Set), drives exploration by sampling multiple models from the posterior
and selecting actions optimistically. It extends previous work by providing a
rule for deciding when to resample and how to combine the models. We show that
our algorithm achieves nearoptimal reward with high probability with a sample
complexity that is low relative to the speed at which the posterior
distribution converges during learning. We demonstrate that BOSS performs quite
favorably compared to state-of-the-art reinforcement-learning approaches and
illustrate its flexibility by pairing it with a non-parametric model that
generalizes across states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2665</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2665</id><created>2012-05-09</created><authors><author><keyname>Andrade</keyname><forenames>Daniel</forenames></author><author><keyname>Sick</keyname><forenames>Bernhard</forenames></author></authors><title>Lower Bound Bayesian Networks - An Efficient Inference of Lower Bounds
  on Probability Distributions in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty
  in Artificial Intelligence (UAI2009)</comments><proxy>auai</proxy><report-no>UAI-P-2009-PG-10-18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method to propagate lower bounds on conditional probability
distributions in conventional Bayesian networks. Our method guarantees to
provide outer approximations of the exact lower bounds. A key advantage is that
we can use any available algorithms and tools for Bayesian networks in order to
represent and infer lower bounds. This new method yields results that are
provable exact for trees with binary variables, and results which are
competitive to existing approximations in credal networks for all other network
structures. Our method is not limited to a specific kind of network structure.
Basically, it is also not restricted to a specific kind of inference, but we
restrict our analysis to prognostic inference in this article. The
computational complexity is superior to that of other existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2670</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2670</id><created>2012-02-08</created><authors><author><keyname>Kose</keyname><forenames>Utku</forenames></author><author><keyname>Deperlioglu</keyname><forenames>Omer</forenames></author></authors><title>Intelligent learning environments within blended learning for ensuring
  effective C programming course</title><categories>cs.CY cs.PL</categories><comments>20 pages, 7 figures, 5 tables</comments><doi>10.5121/ijaia.2012.3109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a blended learning implementation and experience
supported with intelligent learning environments included in a learning
management system (LMS) called @KU-UZEM. The blended learning model is realized
as a combination of face to face education and e-learning. The intelligent
learning environments consist of two applications named CTutor, ITest. In
addition to standard e-learning tools, students can use CTutor to resolve C
programming exercises. CTutor is a problem-solving environment, which diagnoses
students' knowledge level but also gives feedbacks and tips to help them to
understand the course subject, overcome their misconceptions and reinforce
learnt concepts. ITest provides an assessment environment in which students can
take quizzes that were prepared according to their learning levels. The
realized model was used for two terms in the &quot;C Programming&quot; course given at
Afyon Kocatepe University. A survey was conducted at the end of the course to
find out to what extent the students were accepting the blended learning model
supported with @KU-UZEM and to discover students' attitude towards intelligent
learning environments. Additionally, an experiment formed with an experimental
group who took an active part in the realized model and a control group who
only took the face to face education was performed during the first term of the
course. According to the results, students were satisfied with intelligent
learning environments and the realized learning model. Furthermore, the use of
intelligent learning environments improved the students' knowledge about C
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2678</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2678</id><created>2012-02-08</created><authors><author><keyname>Sharma</keyname><forenames>Manish</forenames></author><author><keyname>Singh</keyname><forenames>Gurpadam</forenames></author></authors><title>Evaluation of Proactive, Reactive and Hybrid Ad hoc Routing Protocol for
  various Battery models in VANET using Qualnet</title><categories>cs.PF cs.NI</categories><comments>5 Pages, 5 Figures. arXiv admin note: substantial text overlap with
  arXiv:1202.1720</comments><journal-ref>International Journal of Smart Sensors and Ad Hoc Networks
  (IJSSAN) ISSN No. 2248-9738 Vol.1(2) 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In VANET high speed is the real characteristics which leads frequent
breakdown, interference etc. In this paper we studied various Ad hoc routing
protocols, Reactive, Proactive &amp; Hybrid, taking into consideration various
VANET parameters like speed, altitude etc in real traffic scenario and
evaluated them for various battery models for energy conservation.. The AODV
and DYMO (Reactive), OLSR (Proactive) and ZRP (hybrid) protocols are compared
for battery models Duracell AA(MX- 1500),Duracell AAA(MN-2400),Duracell
AAA(MX-2400), Duracell C-MN(MN-1400),Panasonic AA standard using Qualnet as a
Simulation tool. Since Energy conservation is main focus area nowadays. Hence
performance of the protocols with various battery models counts and helps to
make a right selection. Varying parameters of VANET shows that in the real
traffic scenarios proactive protocol performs more efficiently for energy
conservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2681</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2681</id><created>2012-05-11</created><updated>2012-05-14</updated><authors><author><keyname>Graves</keyname><forenames>Eric</forenames></author><author><keyname>Wong</keyname><forenames>Tan</forenames></author></authors><title>Detectability of Symbol Manipulation by an Amplify-and-Forward Relay</title><categories>cs.IT math.IT</categories><comments>25 pages, 9 figures. Math</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of detecting a potential malicious relay node
by a source node that relies on the relay to forward information to other
nodes. The channel model of two source nodes simultaneously sending symbols to
a relay is considered. The relay is contracted to forward the symbols that it
receives back to the sources in the amplify-and-forward manner. However there
is a chance that the relay may send altered symbols back to the sources. Each
source attempts to individually detect such malicious acts of the relay by
comparing the empirical distribution of the symbols that it receives from the
relay conditioned on its own transmitted symbols with known stochastic
characteristics of the channel. It is shown that maliciousness of the relay can
be asymptotically detected with sufficient channel observations if and only if
the channel satisfies a non-manipulable condition, which can be easily checked.
As a result, the non-manipulable condition provides us a clear-cut criterion to
determine the detectability of the aforementioned class of symbol manipulation
attacks potentially conducted by the relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2686</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2686</id><created>2012-05-11</created><updated>2013-01-12</updated><authors><author><keyname>Miller</keyname><forenames>Jeffrey W.</forenames></author></authors><title>Reduced Criteria for Degree Sequences</title><categories>math.CO cs.DM</categories><journal-ref>Discrete Mathematics, Volume 313, Issue 4, 28 February 2013, Pages
  550-562</journal-ref><doi>10.1016/j.disc.2012.11.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many types of graphs, criteria have been discovered that give necessary
and sufficient conditions for an integer sequence to be the degree sequence of
such a graph. These criteria tend to take the form of a set of inequalities,
and in the case of the Erd\H{o}s-Gallai criterion (for simple undirected
graphs) and the Gale-Ryser criterion (for bipartite graphs), it has been shown
that the number of inequalities that must be checked can be reduced
significantly. We show that similar reductions hold for the corresponding
criteria for many other types of graphs, including bipartite r-multigraphs,
bipartite graphs with structural edges, directed graphs, r-multigraphs, and
tournaments. We also prove a reduction for imbalance sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2691</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2691</id><created>2012-05-11</created><updated>2012-05-15</updated><authors><author><keyname>Assaf</keyname><forenames>Ahmad</forenames></author><author><keyname>Louw</keyname><forenames>Eldad</forenames></author><author><keyname>Senart</keyname><forenames>Aline</forenames></author><author><keyname>Follenfant</keyname><forenames>Corentin</forenames></author><author><keyname>Troncy</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Trastour</keyname><forenames>David</forenames></author></authors><title>Improving Schema Matching with Linked Data</title><categories>cs.DB</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With today's public data sets containing billions of data items, more and
more companies are looking to integrate external data with their traditional
enterprise data to improve business intelligence analysis. These distributed
data sources however exhibit heterogeneous data formats and terminologies and
may contain noisy data. In this paper, we present a novel framework that
enables business users to semi-automatically perform data integration on
potentially noisy tabular data. This framework offers an extension to Google
Refine with novel schema matching algorithms leveraging Freebase rich types.
First experiments show that using Linked Data to map cell values with instances
and column headers with types improves significantly the quality of the
matching results and therefore should lead to more informed decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2711</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2711</id><created>2012-02-09</created><authors><author><keyname>Barik</keyname><forenames>Nikhilesh</forenames></author><author><keyname>Karforma</keyname><forenames>Sunil</forenames></author></authors><title>Risks and remedies in e-learning system</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most effective applications of Information and Communication
Technology (ICT) is the emergence of E-Learning. Considering the importance and
need of E-Learning, recent years have seen a drastic change of learning
methodologies in Higher Education. Undoubtedly, the three main entities of
E-Learning system can be considered as Student, Teacher &amp; Controlling Authority
and there will be different level, but a good E-Learning system needs total
integrity among all entities in every level. Apart from integrity enforcement,
security enforcement in the whole system is the other crucial way to organize
the it. As internet is the backbone of the entire system which is inherently
insecure, during transaction of message in E-Learning system, hackers attack by
utilising different loopholes of technology. So different security measures are
required to be imposed on the system. In this paper, emphasis is given on
different risks called e-risks and their remedies called e-remedies to build
trust in the minds of all participants of E-Learning system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2726</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2726</id><created>2012-05-11</created><authors><author><keyname>Leoni</keyname><forenames>David</forenames></author></authors><title>Non-Interactive Differential Privacy: a Survey</title><categories>cs.DB</categories><comments>Presented at the First International Workshop On Open Data, WOD-2012
  (http://arxiv.org/abs/1204.3726)</comments><report-no>WOD/2012/NANTES/12</report-no><acm-class>H.4.m; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenData movement around the globe is demanding more access to information
which lies locked in public or private servers. As recently reported by a
McKinsey publication, this data has significant economic value, yet its release
has potential to blatantly conflict with people privacy. Recent UK government
inquires have shown concern from various parties about publication of
anonymized databases, as there is concrete possibility of user identification
by means of linkage attacks. Differential privacy stands out as a model that
provides strong formal guarantees about the anonymity of the participants in a
sanitized database. Only recent results demonstrated its applicability on
real-life datasets, though. This paper covers such breakthrough discoveries, by
reviewing applications of differential privacy for non-interactive publication
of anonymized real-life datasets. Theory, utility and a data-aware comparison
are discussed on a variety of principles and concrete applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2736</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2736</id><created>2012-05-11</created><updated>2012-09-10</updated><authors><author><keyname>Hodas</keyname><forenames>Nathan Oken</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>How Visibility and Divided Attention Constrain Social Contagion</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>In ASE/IEEE International Conference on Social Computing
  (SocialCom-2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How far and how fast does information spread in social media? Researchers
have recently examined a number of factors that affect information diffusion in
online social networks, including: the novelty of information, users' activity
levels, who they pay attention to, and how they respond to friends'
recommendations. Using URLs as markers of information, we carry out a detailed
study of retweeting, the primary mechanism by which information spreads on the
Twitter follower graph. Our empirical study examines how users respond to an
incoming stimulus, i.e., a tweet (message) from a friend, and reveals that
%retweeting behavior is constrained by a few simple principles. the &quot;principle
of least effort&quot; combined with limited attention plays a dominant role in
retweeting behavior. Specifically, we observe that users retweet information
when it is most visible, such as when it near the top of their Twitter stream.
Moreover, our measurements quantify how a user's limited attention is divided
among incoming tweets, providing novel evidence that highly connected
individuals are less likely to propagate an arbitrary tweet. Our study
indicates that the finite ability to process incoming information constrains
social contagion, and we conclude that rapid decay of visibility is the primary
barrier to information propagation online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2738</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2738</id><created>2012-01-14</created><authors><author><keyname>Bedi</keyname><forenames>Rajeev</forenames></author><author><keyname>Marwaha</keyname><forenames>Mohit</forenames></author><author><keyname>Singh</keyname><forenames>Tajinder</forenames></author><author><keyname>Singh</keyname><forenames>Harwinder</forenames></author><author><keyname>Singh</keyname><forenames>Amritpal</forenames></author></authors><title>Analysis of Different Privacy Preserving Cloud Storage Frameworks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy Security of data in Cloud Storage is one of the main issues. Many
Frameworks and Technologies are used to preserve data security in cloud
storage. [1] Proposes a framework which includes the design of data
organization structure, the generation and management of keys, the treatment of
change of user's access right and dynamic operations of data, and the
interaction between participants. It also design an interactive protocol and an
extirpation-based key derivation algorithm, which are combined with lazy
revocation, it uses multi-tree structure and symmetric encryption to form a
privacy-preserving, efficient framework for cloud storage. [2] Proposes a
framework which design a privacy-preserving cloud storage framework in which he
designed an interaction protocol among participants, use key derivation
algorithm to generate and manage keys, use both symmetric and asymmetric
encryption to hide the sensitive data of users, and apply Bloom filter for
cipher text retrieval. A system based on this framework is realized. This paper
analyzes both the frameworks in terms of the feasibility of the frameworks,
running overhead of the system and the privacy security of the frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2740</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2740</id><created>2012-05-11</created><authors><author><keyname>Gupte</keyname><forenames>Mangesh</forenames></author><author><keyname>Krushevskaja</keyname><forenames>Darja</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author></authors><title>Analyses of Cardinal Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study cardinal auctions for selling multiple copies of a good, in which
bidders specify not only their bid or how much they are ready to pay for the
good, but also a cardinality constraint on the number of copies that will be
sold via the auction. We perform first known Price of Anarchy type analyses
with detailed comparison of the classical Vickrey-Clarke-Groves (VCG) auction
and one based on minimum pay property (MPP) which is similar to Generalized
Second Price auction commonly used in sponsored search. Without cardinality
constraints, MPP has the same efficiency (total value to bidders) and at least
as much revenue (total income to the auctioneer) as VCG; this also holds for
certain other generalizations of MPP (e.g., prefix constrained auctions, as we
show here). In contrast, our main results are that, with cardinality
constraints, (a) equilibrium efficiency of MPP is 1/2 of that of VCG and this
factor is tight, and (b) in equilibrium MPP may collect as little as 1/2 the
revenue of VCG. These aspects arise because in presence of cardinality
constraints, more strategies are available to bidders in MPP, including bidding
above their value, and this makes analyses nontrivial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2748</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2748</id><created>2012-05-12</created><authors><author><keyname>Chroboczek</keyname><forenames>Juliusz</forenames><affiliation>PPS</affiliation></author><author><keyname>Lebresne</keyname><forenames>Sylvain</forenames><affiliation>PPS</affiliation></author></authors><title>Juppix: a Linux Live-CD for Undergraduate Students</title><categories>cs.OH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Juppix is a Linux Live-CD with a comfortable programming environment for the
Java, C and O'Caml programming languages that has been distributed to hundreds
of undergaduate students at the University of Paris 7 over the last few years.
We describe the lessons we learnt while compiling and distributing Juppix, and
outline our future plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2761</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2761</id><created>2012-05-12</created><authors><author><keyname>Pereszl&#xe9;nyi</keyname><forenames>Attila</forenames></author></authors><title>Multi-Prover Quantum Merlin-Arthur Proof Systems with Small Gap</title><categories>quant-ph cs.CC</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies multiple-proof quantum Merlin-Arthur (QMA) proof systems
in the setting when the completeness-soundness gap is small. Small means that
we only lower-bound the gap with an inverse-exponential function of the input
length, or with an even smaller function. Using the protocol of Blier and Tapp
[arXiv:0709.0738], we show that in this case the proof system has the same
expressive power as non-deterministic exponential time (NEXP). Since
single-proof QMA proof systems, with the same bound on the gap, have expressive
power at most exponential time (EXP), we get a separation between single and
multi-prover proof systems in the 'small-gap setting', under the assumption
that EXP is not equal to NEXP. This implies, among others, the nonexistence of
certain operators called disentanglers (defined by Aaronson et al.
[arXiv:0804.0802]), with good approximation parameters.
  We also show that in this setting the proof system has the same expressive
power if we restrict the verifier to be able to perform only Bell-measurements,
i.e., using a BellQMA verifier. This is not known to hold in the usual setting,
when the gap is bounded by an inverse-polynomial function of the input length.
To show this we use the protocol of Chen and Drucker [arXiv:1011.0716]. The
only caveat here is that we need at least a linear amount of proofs to achieve
the power of NEXP, while in the previous setting two proofs were enough.
  We also study the case when the proof-lengths are only logarithmic in the
input length and observe that in some cases the expressive power decreases.
However, we show that it doesn't decrease further if we make the proof lengths
to be even shorter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2762</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2762</id><created>2012-05-12</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>Efficient Packet Forwarding in Mesh Network</title><categories>cs.NI</categories><comments>4 pages, 6 figures</comments><journal-ref>International Journal of Interactive Mobile Technologies (iJIM)
  Issn: 18657923 Year: 2012 Volume: 6 Issue: 2 pages/rec.No: 47-50</journal-ref><doi>10.3991/ijim.v6i2.1991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Mesh Network (WMN) is a multi hop low cost, with easy maintenance
robust network providing reliable service coverage. WMNs consist of mesh
routers and mesh clients. In this architecture, while static mesh routers form
the wireless backbone, mesh clients access the network through mesh routers as
well as directly meshing with each other. Different from traditional wireless
networks, WMN is dynamically self-organized and self-configured. In other
words, the nodes in the mesh network automatically establish and maintain
network connectivity. Over the years researchers have worked, to reduce the
redundancy in broadcasting packet in the mesh network in the wireless domain
for providing reliable service coverage, the source node deserves to broadcast
or flood the control packets. The redundant control packet consumes the
bandwidth of the wireless medium and significantly reduces the average
throughput and consequently reduces the overall system performance. In this
paper I study the optimization problem in Wireless Mesh Networks. We have
proposed a novel approach to reduce the broadcast redundant packet in the
wireless mesh network. Also we have shown, a novel procedure to forward the
control packet to the destination nodes and efficiently minimize the
transmitted control packet in the wireless mesh cloud, that covers the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2766</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2766</id><created>2012-05-12</created><updated>2012-07-05</updated><authors><author><keyname>Ferreira</keyname><forenames>Rui</forenames></author><author><keyname>Grossi</keyname><forenames>Roberto</forenames></author><author><keyname>Marino</keyname><forenames>Andrea</forenames></author><author><keyname>Pisanti</keyname><forenames>Nadia</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author></authors><title>Optimal Listing of Cycles and st-Paths in Undirected Graphs</title><categories>cs.DS</categories><comments>12 Pages, 7 Page Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first optimal algorithm for the classical problem of listing
all the cycles in an undirected graph. We exploit their properties so that the
total cost is the time taken to read the input graph plus the time to list the
output, namely, the edges in each of the cycles. The algorithm uses a reduction
to the problem of listing all the paths from a vertex s to a vertex t which we
also solve optimally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2797</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2797</id><created>2012-05-12</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Perwej</keyname><forenames>Asif</forenames></author></authors><title>Forecasting of Indian Rupee (INR) / US Dollar (USD) Currency Exchange
  Rate Using Artificial Neural Network</title><categories>cs.NE</categories><comments>12 Pages, 3 Figures, ISSN:2230-9616(Online);2231-0088(Print)</comments><journal-ref>International Journal of Computer Science, Engineering and
  Applications (IJCSEA), April 2012, Volume 2, Number 2, Pages 41-52</journal-ref><doi>10.5121/ijcsea.2012.2204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large part of the workforce, and growing every day, is originally from
India. India one of the second largest populations in the world, they have a
lot to offer in terms of jobs. The sheer number of IT workers makes them a
formidable travelling force as well, easily picking up employment in English
speaking countries. The beginning of the economic crises since 2008 September,
many Indians have return homeland, and this has had a substantial impression on
the Indian Rupee (INR) as liken to the US Dollar (USD). We are using
numerational knowledge based techniques for forecasting has been proved highly
successful in present time. The purpose of this paper is to examine the effects
of several important neural network factors on model fitting and forecasting
the behaviours. In this paper, Artificial Neural Network has successfully been
used for exchange rate forecasting. This paper examines the effects of the
number of inputs and hidden nodes and the size of the training sample on the
in-sample and out-of-sample performance. The Indian Rupee (INR) / US Dollar
(USD) is used for detailed examinations. The number of input nodes has a
greater impact on performance than the number of hidden nodes, while a large
number of observations do reduce forecast errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2798</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2798</id><created>2012-05-12</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Parwej</keyname><forenames>Firoj</forenames></author></authors><title>Perceptual Evaluation Of Playout Buffer Algorithm For Enhancing
  Perceived Quality Of Voice Transmission Over Ip Network</title><categories>cs.NI</categories><comments>19 pages, 17 Figures, ISSN:1839-5678</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT), April 2012 , Volume 2, Number 2, Pages 1-19</journal-ref><doi>10.5121/ijmnct.2012.2201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over Internet Protocol (VoIP) is a technology that allows you to make
voice calls using a broadband Internet connection instead of a regular (or
analog) phone line. Voice over Internet Protocol (VoIP) has led human speech to
a new level, where conversation across continents can be much cheaper &amp; faster.
However, as IP networks are not designed for real-time applications, the
network impairments such as packet loss, jitter and delay have a severe impact
on speech quality. The playout buffer at the receiver side is used to
compensate jitter at a trade-off of delay and loss. We found the
characteristics of delay and loss are dependent on IP network and sudden
variable delay (spike) often performs both regular and irregular
characteristics. Different playout buffer algorithms can have different impacts
on the achievement speech quality. It is important to design a playout buffer
algorithm which can help achieve an optimum speech quality. In this paper, we
investigate to the understanding how network impairments and existing adaptive
buffer algorithms affect the speech quality and further to design a modified
buffer algorithm to obtain an optimized voice quality. We conduct experiments
to existing algorithms and compared their performance under different network
conditions with high and low network delay variations. Preliminary results show
that the new algorithm can enhance the perceived speech quality in most network
conditions and it is more efficient and suitable for real buffer mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2800</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2800</id><created>2012-05-12</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Parwej</keyname><forenames>Firoj</forenames></author><author><keyname>Perwej</keyname><forenames>Asif</forenames></author></authors><title>An Adaptive Watermarking Technique for the copyright of digital images
  and Digital Image Protection</title><categories>cs.MM</categories><comments>18 Pages, 11 Figures, ISSN:0975-5578(Online); 0975-5934 (Print).
  arXiv admin note: text overlap with arXiv:1003.4084 by other authors without
  attribution</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications
  (IJMA),April 2012, Volume 4, Number 2, Pages 21-38</journal-ref><doi>10.5121/ijma.2012.4202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet as a whole does not use secure links, thus information in
transit may be vulnerable to interruption as well. The important of reducing a
chance of the information being detected during the transmission is being an
issue in the real world now days. The Digital watermarking method provides for
the quick and inexpensive distribution of digital information over the
Internet. This method provides new ways of ensuring the sufficient protection
of copyright holders in the intellectual property dispersion process. The
property of digital watermarking images allows insertion of additional data in
the image without altering the value of the image.In this paper investigate the
following relevant concepts and terminology, history of watermarks and the
properties of a watermarking system and applications. We are proposing edge
detection using Gabor Filters. In this paper we are proposed least significant
bit (LSB) substitution method to encrypt the message in the watermark image
file. The benefits of the LSB are its simplicity to embed the bits of the
message directly into the LSB plane of cover-image and many techniques using
these methods. The LSB does not result in a human perceptible difference
because the amplitude of the change is little therefore the human eye the
resulting stego image will look identical to the cover image and this allows
high perceptual transparency of the LSB. The spatial domain technique LSB
substitution it would be able to use a pseudo-random number generator to
determine the pixels to be used for embedding based on a given key. We are
using DCT transform watermark algorithms based on robustness. The watermarking
robustness have been calculated by the Peak Signal to Noise Ratio (PSNR) and
Normalized cross correlation (NC) is used to quantify by the similarity between
the real watermark and after extracting watermark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2807</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2807</id><created>2012-05-12</created><updated>2015-01-29</updated><authors><author><keyname>Maitra</keyname><forenames>Arpita</forenames></author><author><keyname>Paul</keyname><forenames>Goutam</forenames></author></authors><title>Eavesdropping in Semiquantum Key Distribution Protocol</title><categories>quant-ph cs.CR</categories><comments>11 pages, 2 Figures</comments><msc-class>81P94</msc-class><journal-ref>Information Processing Letters, pages 418-422, vol. 113, issue 12,
  June 30, 2013</journal-ref><doi>10.1016/j.ipl.2013.03.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In semiquantum key-distribution (Boyer et al.) Alice has the same capability
as in BB84 protocol, but Bob can measure and prepare qubits only in
$\{|0\rangle, |1\rangle\}$ basis and reflect any other qubit. We study an
eavesdropping strategy on this scheme that listens to the channel in both the
directions. With the same level of disturbance induced in the channel, Eve can
extract more information using our two-way strategy than what can be obtained
by the direct application of one-way eavesdropping in BB84.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2821</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2821</id><created>2012-05-12</created><authors><author><keyname>Florindo</keyname><forenames>J. B.</forenames></author><author><keyname>Bruno</keyname><forenames>O. M.</forenames></author></authors><title>Texture Analysis And Characterization Using Probability Fractal
  Descriptors</title><categories>physics.data-an cs.CV</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A gray-level image texture descriptors based on fractal dimension estimation
is proposed in this work. The proposed method estimates the fractal dimension
using probability (Voss) method. The descriptors are computed applying a
multiscale transform to the fractal dimension curves of the texture image. The
proposed texture descriptor method is evaluated in a classification task of
well known benchmark texture datasets. The results show the great performance
of the proposed method as a tool for texture images analysis and
characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2822</identifier>
 <datestamp>2012-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2822</id><created>2012-05-12</created><updated>2012-07-24</updated><authors><author><keyname>Qiu</keyname><forenames>Tian</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Chen</keyname><forenames>Guang</forenames></author></authors><title>Promotional effect on cold start problem and diversity in a data
  characteristic based recommendation method</title><categories>cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pure methods generally perform excellently in either recommendation accuracy
or diversity, whereas hybrid methods generally outperform pure cases in both
recommendation accuracy and diversity, but encounter the dilemma of optimal
hybridization parameter selection for different recommendation focuses. In this
article, based on a user-item bipartite network, we propose a data
characteristic based algorithm, by relating the hybridization parameter to the
data characteristic. Different from previous hybrid methods, the present
algorithm adaptively assign the optimal parameter specifically for each
individual items according to the correlation between the algorithm and the
item degrees. Compared with a highly accurate pure method, and a hybrid method
which is outstanding in both the recommendation accuracy and the diversity, our
method shows a remarkably promotional effect on the long-standing challenging
problem of the cold start, as well as the recommendation diversity, while
simultaneously keeps a high overall recommendation accuracy. Even compared with
an improved hybrid method which is highly efficient on the cold start problem,
the proposed method not only further improves the recommendation accuracy of
the cold items, but also enhances the recommendation diversity. Our work might
provide a promising way to better solving the personal recommendation from the
perspective of relating algorithms with dataset properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2825</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2825</id><created>2012-05-12</created><updated>2012-07-26</updated><authors><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Ingroup favoritism and intergroup cooperation under indirect reciprocity
  based on group reputation</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>3 figures, 4 tables</comments><journal-ref>Journal of Theoretical Biology, 311, 8-18 (2012)</journal-ref><doi>10.1016/j.jtbi.2012.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indirect reciprocity in which players cooperate with unacquainted other
players having good reputations is a mechanism for cooperation in relatively
large populations subjected to social dilemma situations. When the population
has group structure, as is often found in social networks, players in
experiments are considered to show behavior that deviates from existing
theoretical models of indirect reciprocity. First, players often show ingroup
favoritism (i.e., cooperation only within the group) rather than full
cooperation (i.e., cooperation within and across groups), even though the
latter is Pareto efficient. Second, in general, humans approximate outgroup
members' personal characteristics, presumably including the reputation used for
indirect reciprocity, by a single value attached to the group. Humans use such
a stereotypic approximation, a phenomenon known as outgroup homogeneity in
social psychology. I propose a model of indirect reciprocity in populations
with group structure to examine the possibility of ingroup favoritism and full
cooperation. In accordance with outgroup homogeneity, I assume that players
approximate outgroup members' personal reputations by a single reputation value
attached to the group. I show that ingroup favoritism and full cooperation are
stable under different social norms (i.e., rules for assigning reputations)
such that they do not coexist in a single model. If players are forced to
consistently use the same social norm for assessing different types of
interactions (i.e., ingroup versus outgroup interactions), only full
cooperation survives. The discovered mechanism is distinct from any form of
group selection. The results also suggest potential methods for reducing
ingroup bias to shift the equilibrium from ingroup favoritism to full
cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2828</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2828</id><created>2012-05-13</created><authors><author><keyname>Chiu</keyname><forenames>Eddy</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Cellular Multi-User Two-Way MIMO AF Relaying via Signal Space Alignment:
  Minimum Weighted SINR Maximization</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2012.2201151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider linear MIMO transceiver design for a cellular
two-way amplify-and-forward relaying system consisting of a single
multi-antenna base station, a single multi-antenna relay station, and multiple
multi-antenna mobile stations (MSs). Due to the two-way transmission, the MSs
could suffer from tremendous multi-user interference. We apply an interference
management model exploiting signal space alignment and propose a transceiver
design algorithm, which allows for alleviating the loss in spectral efficiency
due to half-duplex operation and providing flexible performance optimization
accounting for each user's quality of service priorities. Numerical comparisons
to conventional two-way relaying schemes based on bidirectional channel
inversion and spatial division multiple access-only processing show that the
proposed scheme achieves superior error rate and average data rate performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2833</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2833</id><created>2012-05-13</created><updated>2012-11-16</updated><authors><author><keyname>Ye</keyname><forenames>Qiaoyang</forenames></author><author><keyname>Rong</keyname><forenames>Beiyu</forenames></author><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Al-Shalash</keyname><forenames>Mazin</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>User Association for Load Balancing in Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For small cell technology to significantly increase the capacity of
tower-based cellular networks, mobile users will need to be actively pushed
onto the more lightly loaded tiers (corresponding to, e.g., pico and
femtocells), even if they offer a lower instantaneous SINR than the macrocell
base station (BS). Optimizing a function of the long-term rates for each user
requires (in general) a massive utility maximization problem over all the SINRs
and BS loads. On the other hand, an actual implementation will likely resort to
a simple biasing approach where a BS in tier j is treated as having its SINR
multiplied by a factor A_j&gt;=1, which makes it appear more attractive than the
heavily-loaded macrocell. This paper bridges the gap between these approaches
through several physical relaxations of the network-wide optimal association
problem, whose solution is NP hard. We provide a low-complexity distributed
algorithm that converges to a near-optimal solution with a theoretical
performance guarantee, and we observe that simple per-tier biasing loses
surprisingly little, if the bias values A_j are chosen carefully. Numerical
results show a large (3.5x) throughput gain for cell-edge users and a 2x rate
gain for median users relative to a max received power association.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2841</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2841</id><created>2012-05-13</created><authors><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Ducobu</keyname><forenames>Marc</forenames></author><author><keyname>Gauwin</keyname><forenames>Olivier</forenames></author></authors><title>Visibly pushdown automata on trees: universality and u-universality</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automaton is universal if it accepts every possible input. We study the
notion of u-universality, which asserts that the automaton accepts every input
starting with u. Universality and u-universality are both EXPTIME-hard for
non-deterministic tree automata. We propose efficient antichain-based
techniques to address these problems for visibly pushdown automata operating on
trees. One of our approaches yields algorithms for the universality and
u-universality of hedge automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2842</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2842</id><created>2012-05-13</created><updated>2013-02-20</updated><authors><author><keyname>Erd\Hos</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Kir&#xe1;ly</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author></authors><title>On the swap-distances of different realizations of a graphical degree
  sequence</title><categories>math.CO cs.DM</categories><comments>to be published</comments><msc-class>05C07, 05C85, 05C20, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the first graph theoretical problems which got serious attention
(already in the fifties of the last century) was to decide whether a given
integer sequence is equal to the degree sequence of a simple graph (or it is
{\em graphical} for short). One method to solve this problem is the greedy
algorithm of Havel and Hakimi, which is based on the {\em swap} operation.
Another, closely related question is to find a sequence of swap operations to
transform one graphical realization into another one of the same degree
sequence. This latter problem got particular emphases in connection of fast
mixing Markov chain approaches to sample uniformly all possible realizations of
a given degree sequence. (This becomes a matter of interest in connection of --
among others -- the study of large social networks.) Earlier there were only
crude upper bounds on the shortest possible length of such swap sequences
between two realizations. In this paper we develop formulae (Gallai-type
identities) for these {\em swap-distance}s of any two realizations of simple
undirected or directed degree sequences. These identities improves considerably
the known upper bounds on the swap-distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2850</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2850</id><created>2012-05-13</created><authors><author><keyname>Shakya</keyname><forenames>Indu L.</forenames></author><author><keyname>Ali</keyname><forenames>Falah H.</forenames></author></authors><title>Spectral Efficiency of Multiple Access Fading Channels with Adaptive
  Interference Cancellation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable estimation of users' channels and data in rapidly time varying
fading environments is a very challenging task of multiuser detection (MUD)
techniques that promise impressive capacity gains for interference limited
systems such as non-orthogonal CDMA and spatial multiplexing MIMO based LTE.
This paper analyzes relative channel estimation error performances of
conventional single user and multiuser receivers for an uplink of DS-CDMA and
shows their impact on output signal to interference and noise ratio (SINR)
performances. Mean squared error (MSE) of channel estimation and achievable
spectral efficiencies of these receivers obtained from the output SINR
calculations are then compared with that achieved with new adaptive
interference canceling receivers. It is shown that the adaptive receivers using
successive (SIC) and parallel interference cancellation (PIC) methods offer
much improved channel estimation and SINR performances, and hence significant
increase in achievable sum date rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2856</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2856</id><created>2012-05-13</created><authors><author><keyname>Zaker</keyname><forenames>Manouchehr</forenames></author></authors><title>Generalized degeneracy, dynamic monopolies and maximum degenerate
  subgraphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is said to be a $k$-degenerate graph if any subgraph of $G$
contains a vertex of degree at most $k$. Let $\kappa$ be any non-negative
function on the vertex set of $G$. We first define a $\kappa$-degenerate graph.
Next we give an efficient algorithm to determine whether a graph is
$\kappa$-degenerate. We revisit the concept of dynamic monopolies in graphs.
The latter notion is used in formulation and analysis of spread of influence
such as disease or opinion in social networks. We consider dynamic monopolies
with (not necessarily positive) but integral threshold assignments. We obtain a
sufficient and necessary relationship between dynamic monopolies and
generalized degeneracy. As applications of the previous results we consider the
problem of determining the maximum size of $\kappa$-degenerate (or
$k$-degenerate) induced subgraphs in any graph. We obtain some upper and lower
bounds for the maximum size of any $\kappa$-degenerate induced subgraph in
general and regular graphs. All of our bounds are constructive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2857</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2857</id><created>2012-05-13</created><authors><author><keyname>Zhu</keyname><forenames>Ping</forenames></author><author><keyname>Wen</keyname><forenames>Qiaoyan</forenames></author></authors><title>Operations on soft sets revisited</title><categories>cs.AI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soft sets, as a mathematical tool for dealing with uncertainty, have recently
gained considerable attention, including some successful applications in
information processing, decision, demand analysis, and forecasting. To
construct new soft sets from given soft sets, some operations on soft sets have
been proposed. Unfortunately, such operations cannot keep all classical
set-theoretic laws true for soft sets. In this paper, we redefine the
intersection, complement, and difference of soft sets and investigate the
algebraic properties of these operations along with a known union operation. We
find that the new operation system on soft sets inherits all basic properties
of operations on classical sets, which justifies our definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2874</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2874</id><created>2012-05-13</created><updated>2012-06-30</updated><authors><author><keyname>Avner</keyname><forenames>Orly</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Decoupling Exploration and Exploitation in Multi-Armed Bandits</title><categories>cs.LG</categories><comments>Full version of the paper presented at ICML 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-armed bandit problem where the decision maker can explore
and exploit different arms at every round. The exploited arm adds to the
decision maker's cumulative reward (without necessarily observing the reward)
while the explored arm reveals its value. We devise algorithms for this setup
and show that the dependence on the number of arms, k, can be much better than
the standard square root of k dependence, depending on the behavior of the
arms' reward sequences. For the important case of piecewise stationary
stochastic bandits, we show a significant improvement over existing algorithms.
Our algorithms are based on a non-uniform sampling policy, which we show is
essential to the success of any algorithm in the adversarial setup. Finally, we
show some simulation results on an ultra-wide band channel selection inspired
setting indicating the applicability of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2876</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2876</id><created>2012-05-13</created><authors><author><keyname>Goli</keyname><forenames>Ali</forenames></author><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Universal Bounds on the Scaling Behavior of Polar Codes</title><categories>cs.IT math.IT</categories><comments>To be presented at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of determining the trade-off between the rate and the
block-length of polar codes for a given block error probability when we use the
successive cancellation decoder. We take the sum of the Bhattacharyya
parameters as a proxy for the block error probability, and show that there
exists a universal parameter $\mu$ such that for any binary memoryless
symmetric channel $W$ with capacity $I(W)$, reliable communication requires
rates that satisfy $R&lt; I(W)-\alpha N^{-\frac{1}{\mu}}$, where $\alpha$ is a
positive constant and $N$ is the block-length. We provide lower bounds on
$\mu$, namely $\mu \geq 3.553$, and we conjecture that indeed $\mu=3.627$, the
parameter for the binary erasure channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2877</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2877</id><created>2012-05-13</created><authors><author><keyname>Colomer-de-Simon</keyname><forenames>Pol</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author></authors><title>Clustering of random scale-free networks</title><categories>cond-mat.dis-nn cs.SI physics.soc-ph</categories><doi>10.1103/PhysRevE.86.026120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the finite size dependence of the clustering coefficient of
scale-free random graphs generated by the configuration model with degree
distribution exponent $2&lt;\gamma&lt;3$. Degree heterogeneity increases the presence
of triangles in the network up to levels that compare to those found in many
real networks even for extremely large nets. We also find that for values of
$\gamma \approx 2$, clustering is virtually size independent and, at the same
time, becomes a {\it de facto} non self-averaging topological property. This
implies that a single instance network is not representative of the ensemble
even for very large network sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2880</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2880</id><created>2012-05-13</created><authors><author><keyname>Cong</keyname><forenames>Gao</forenames></author><author><keyname>Lu</keyname><forenames>Hua</forenames></author><author><keyname>Ooi</keyname><forenames>Beng Chin</forenames></author><author><keyname>Zhang</keyname><forenames>Dongxiang</forenames></author><author><keyname>Zhang</keyname><forenames>Meihui</forenames></author></authors><title>Efficient Spatial Keyword Search in Trajectory Databases</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing amount of trajectory data is being annotated with text
descriptions to better capture the semantics associated with locations. The
fusion of spatial locations and text descriptions in trajectories engenders a
new type of top-$k$ queries that take into account both aspects. Each
trajectory in consideration consists of a sequence of geo-spatial locations
associated with text descriptions. Given a user location $\lambda$ and a
keyword set $\psi$, a top-$k$ query returns $k$ trajectories whose text
descriptions cover the keywords $\psi$ and that have the shortest match
distance. To the best of our knowledge, previous research on querying
trajectory databases has focused on trajectory data without any text
description, and no existing work has studied such kind of top-$k$ queries on
trajectories. This paper proposes one novel method for efficiently computing
top-$k$ trajectories. The method is developed based on a new hybrid index,
cell-keyword conscious B$^+$-tree, denoted by \cellbtree, which enables us to
exploit both text relevance and location proximity to facilitate efficient and
effective query processing. The results of our extensive empirical studies with
an implementation of the proposed algorithms on BerkeleyDB demonstrate that our
proposed methods are capable of achieving excellent performance and good
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2888</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2888</id><created>2012-05-13</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Comparative Study on the Performance of Permutation Algorithms</title><categories>cs.DS</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</comments><journal-ref>Journal of Computer Science &amp; Research (JCSCR), Vol.1, No.1,
  pp.7-19, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutation is the different arrangements that can be made with a given
number of things taking some or all of them at a time. The notation P(n,r) is
used to denote the number of permutations of n things taken r at a time.
Permutation is used in various fields such as mathematics, group theory,
statistics, and computing, to solve several combinatorial problems such as the
job assignment problem and the traveling salesman problem. In effect,
permutation algorithms have been studied and experimented for many years now.
Bottom-Up, Lexicography, and Johnson-Trotter are three of the most popular
permutation algorithms that emerged during the past decades. In this paper, we
are implementing three of the most eminent permutation algorithms, they are
respectively: Bottom-Up, Lexicography, and Johnson-Trotter algorithms. The
implementation of each algorithm will be carried out using two different
approaches: brute-force and divide and conquer. The algorithms codes will be
tested using a computer simulation tool to measure and evaluate the execution
time between the different implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2889</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2889</id><created>2012-05-13</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Comparative Study on the Performance of the Top DBMS Systems</title><categories>cs.DB cs.PF</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org</comments><journal-ref>Journal of Computer Science &amp; Research (JCSCR), Vol.1, No.1,
  pp.20-31, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database management systems are today's most reliable mean to organize data
into collections that can be searched and updated. However, many DBMS systems
are available on the market each having their pros and cons in terms of
reliability, usability, security, and performance. This paper presents a
comparative study on the performance of the top DBMS systems. They are mainly
MS SQL Server 2008, Oracle 11g, IBM DB2, MySQL 5.5, and MS Access 2010. The
testing is aimed at executing different SQL queries with different level of
complexities over the different five DBMSs under test. This would pave the way
to build a head-to-head comparative evaluation that shows the average execution
time, memory usage, and CPU utilization of each DBMS after completion of the
test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2891</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2891</id><created>2012-05-13</created><authors><author><keyname>AbdulNabi</keyname><forenames>Sk.</forenames></author><author><keyname>Premchand</keyname><forenames>P.</forenames></author></authors><title>Effective performance of information retrieval on web by using web
  crawling</title><categories>cs.IR</categories><comments>International Journal of Web &amp; Semantic Technology (IJWesT) Vol.3,
  No.2, April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  World Wide Web consists of more than 50 billion pages online. It is highly
dynamic i.e. the web continuously introduces new capabilities and attracts many
people. Due to this explosion in size, the effective information retrieval
system or search engine can be used to access the information. In this paper we
have proposed the EPOW (Effective Performance of WebCrawler) architecture. It
is a software agent whose main objective is to minimize the overload of a user
locating needed information. We have designed the web crawler by considering
the parallelization policy. Since our EPOW crawler has a highly optimized
system it can download a large number of pages per second while being robust
against crashes. We have also proposed to use the data structure concepts for
implementation of scheduler &amp; circular Queue to improve the performance of our
web crawler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2909</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2909</id><created>2012-05-13</created><updated>2012-09-24</updated><authors><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author><author><keyname>Bornholdt</keyname><forenames>Stefan</forenames></author></authors><title>Evolution of robust network topologies: Emergence of central backbones</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.comp-ph</categories><comments>5 pages + 3 pages of supplemental material</comments><journal-ref>Phys. Rev. Lett. 109, 118703 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.118703</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We model the robustness against random failure or intentional attack of
networks with arbitrary large-scale structure. We construct a block-based model
which incorporates --- in a general fashion --- both connectivity and
interdependence links, as well as arbitrary degree distributions and block
correlations. By optimizing the percolation properties of this general class of
networks, we identify a simple core-periphery structure as the topology most
robust against random failure. In such networks, a distinct and small &quot;core&quot; of
nodes with higher degree is responsible for most of the connectivity,
functioning as a central &quot;backbone&quot; of the system. This centralized topology
remains the optimal structure when other constraints are imposed, such as a
given fraction of interdependence links and fixed degree distributions. This
distinguishes simple centralized topologies as the most likely to emerge, when
robustness against failure is the dominant evolutionary force.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2923</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2923</id><created>2012-05-13</created><authors><author><keyname>Fountoulakis</keyname><forenames>Nikolaos</forenames></author></authors><title>On the evolution of random graphs on spaces of negative curvature</title><categories>math.CO cs.DM math.PR</categories><comments>25 pages, 1 figure</comments><msc-class>05C80, 60B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study a family of random geometric graphs on hyperbolic
spaces. In this setting, N points are chosen randomly on a hyperbolic space and
any two of them are joined by an edge with probability that depends on their
hyperbolic distance, independently of every other pair. In particular, when the
positions of the points have been fixed, the distribution over the set of
graphs on these points is the Boltzmann distribution, where the Hamiltonian is
given by the sum of weighted indicator functions for each pair of points, with
the weight being proportional to a real parameter \beta&gt;0 (interpreted as the
inverse temperature) as well as to the hyperbolic distance between the
corresponding points. This class of random graphs was introduced by Krioukov et
al. We provide a rigorous analysis of aspects of this model and its dependence
on the parameter \beta, verifying some of their observations. We show that a
phase transition occurs around \beta =1. More specifically, we show that when
\beta &gt; 1 the degree of a typical vertex is bounded in probability (in fact it
follows a distribution which for large values exhibits a power-law tail whose
exponent depends only on the curvature of the space), whereas for \beta &lt;1 the
degree is a random variable whose expected value grows polynomially in N. When
\beta = 1, we establish logarithmic growth.
  For the case \beta &gt; 1, we establish a connection with a class of
inhomogeneous random graphs known as the Chung-Lu model. Assume that we use the
Poincar\'e disc representation of a hyperbolic space. If we condition on the
distance of each one of the points from the origin, then the probability that
two given points are adjacent is expressed through the kernel of this
inhomogeneous random graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2926</identifier>
 <datestamp>2013-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2926</id><created>2012-05-13</created><updated>2013-09-25</updated><authors><author><keyname>Harvey</keyname><forenames>David</forenames></author></authors><title>Faster arithmetic for number-theoretic transforms</title><categories>cs.SC cs.DS</categories><comments>9 pages, a few minor changes and reorganisation, to appear in JSC</comments><msc-class>68W30</msc-class><acm-class>F.2.1</acm-class><doi>10.1016/j.jsc.2013.09.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to improve the efficiency of the computation of fast Fourier
transforms over F_p where p is a word-sized prime. Our main technique is
optimisation of the basic arithmetic, in effect decreasing the total number of
reductions modulo p, by making use of a redundant representation for integers
modulo p. We give performance results showing a significant improvement over
Shoup's NTL library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2927</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2927</id><created>2012-05-13</created><authors><author><keyname>D'Alberto</keyname><forenames>Paolo</forenames></author></authors><title>A Heterogeneous Accelerated Matrix Multiplication: OpenCL + APU + GPU+
  Fast Matrix Multiply</title><categories>cs.MS</categories><comments>15 pages, 6 Figure, Fusion AMD Fusion Developer Summit 2012</comments><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As users and developers, we are witnessing the opening of a new computing
scenario: the introduction of hybrid processors into a single die, such as an
accelerated processing unit (APU) processor, and the plug-and-play of
additional graphics processing units (GPUs) onto a single motherboard. These
APU processors provide multiple symmetric cores with their memory hierarchies
and an integrated GPU. Moreover, these processors are designed to work with
external GPUs that can push the peak performance towards the TeraFLOPS
boundary. We present a case study for the development of dense Matrix
Multiplication (MM) codes for matrix sizes up to 19K\times19K, thus using all
of the above computational engines, and an achievable peak performance of 200
GFLOPS for, literally, a made- at-home built. We present the results of our
experience, the quirks, the pitfalls, the achieved performance, and the
achievable peak performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2930</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2930</id><created>2012-05-13</created><authors><author><keyname>Lin</keyname><forenames>Yue</forenames></author><author><keyname>Cai</keyname><forenames>Deng</forenames></author><author><keyname>Li</keyname><forenames>Cheng</forenames></author></authors><title>Density Sensitive Hashing</title><categories>cs.IR cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nearest neighbors search is a fundamental problem in various research fields
like machine learning, data mining and pattern recognition. Recently,
hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to
be effective for scalable high dimensional nearest neighbors search. Many
hashing algorithms found their theoretic root in random projection. Since these
algorithms generate the hash tables (projections) randomly, a large number of
hash tables (i.e., long codewords) are required in order to achieve both high
precision and recall. To address this limitation, we propose a novel hashing
algorithm called {\em Density Sensitive Hashing} (DSH) in this paper. DSH can
be regarded as an extension of LSH. By exploring the geometric structure of the
data, DSH avoids the purely random projections selection and uses those
projective functions which best agree with the distribution of the data.
Extensive experimental results on real-world data sets have shown that the
proposed method achieves better performance compared to the state-of-the-art
hashing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2931</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2931</id><created>2012-05-13</created><updated>2012-06-25</updated><authors><author><keyname>Bridges</keyname><forenames>Douglas S</forenames><affiliation>University of Canterbury</affiliation></author></authors><title>Precompact Apartness Spaces</title><categories>cs.LO math.GN</categories><proxy>LMCS</proxy><acm-class>math.GN</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 25,
  2012) lmcs:1052</journal-ref><doi>10.2168/LMCS-8(2:15)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a notion of precompactness, and study some of its properties, in
the context of apartness spaces whose apartness structure is not necessarily
induced by any uniform one. The presentation lies entirely with a Bishop-style
constructive framework, and is a contribution to the ongoing development of the
constructive theories of apartness and uniformity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2934</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2934</id><created>2012-05-13</created><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>Inapproximability After Uniqueness Phase Transition in Two-Spin Systems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-state spin system is specified by a 2 x 2 matrix
  A = {A_{0,0} A_{0,1}, A_{1,0} A_{1,1}} = {\beta 1, 1 \gamma} where \beta,
\gamma \ge 0. Given an input graph G=(V,E), the partition function Z_A(G) of a
system is defined as
  Z_A(G) = \sum_{\sigma: V -&gt; {0,1}} \prod_{(u,v) \in E} A_{\sigma(u),
\sigma(v)}
  We prove inapproximability results for the partition function in the region
specified by the non-uniqueness condition from phase transition for the Gibbs
measure. More specifically, assuming NP \ne RP, for any fixed \beta, \gamma in
the unit square, there is no randomized polynomial-time algorithm that
approximates Z_A(G) for d-regular graphs G with relative error \epsilon =
10^{-4}, if d = \Omega(\Delta(\beta,\gamma)), where \Delta(\beta,\gamma) &gt;
1/(1-\beta\gamma) is the uniqueness threshold. Up to a constant factor, this
hardness result confirms the conjecture that the uniqueness phase transition
coincides with the transition from computational tractability to intractability
for Z_A(G). We also show a matching inapproximability result for a region of
parameters \beta, \gamma outside the unit square, and all our results
generalize to partition functions with an external field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2952</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2952</id><created>2012-05-14</created><updated>2013-05-27</updated><authors><author><keyname>Bechon</keyname><forenames>Patrick</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>Synchronization and quorum sensing in a swarm of humanoid robots</title><categories>nlin.AO cs.RO</categories><comments>5 pages, 7 figures. For associated video :
  http://www.youtube.com/watch?v=emFM8xaQkK4 ; v2 : Minor editiorial
  corrections ; v3 : Corrected typo in hyperlink</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of inexpensive simple humanoid robots, new classes of robotic
questions can be considered experimentally. One of these is collective behavior
of groups of humanoid robots, and in particular robot synchronization and
swarming. The goal of this work is to robustly synchronize a group of humanoid
robots, and to demonstrate the approach experimentally on a choreography of 8
robots. We aim to be robust to network latencies, and to allow robots to join
or leave the group at any time (for example a fallen robot should be able to
stand up to rejoin the choreography). Contraction theory is used to allow each
robot in the group to synchronize to a common virtual oscillator, and quorum
sensing strategies are exploited to fit within the available bandwidth. The
humanoids used are Nao's, developed by Aldebaran Robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2958</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2958</id><created>2012-05-14</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Konig</keyname><forenames>Arnd Christian</forenames></author></authors><title>b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning
  and Using GPUs for Fast Preprocessing with Simple Hash Functions</title><categories>cs.IR cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study several critical issues which must be tackled before
one can apply b-bit minwise hashing to the volumes of data often used
industrial applications, especially in the context of search.
  1. (b-bit) Minwise hashing requires an expensive preprocessing step that
computes k (e.g., 500) minimal values after applying the corresponding
permutations for each data vector. We developed a parallelization scheme using
GPUs and observed that the preprocessing time can be reduced by a factor of
20-80 and becomes substantially smaller than the data loading time.
  2. One major advantage of b-bit minwise hashing is that it can substantially
reduce the amount of memory required for batch learning. However, as online
algorithms become increasingly popular for large-scale learning in the context
of search, it is not clear if b-bit minwise yields significant improvements for
them. This paper demonstrates that $b$-bit minwise hashing provides an
effective data size/dimension reduction scheme and hence it can dramatically
reduce the data loading time for each epoch of the online training process.
This is significant because online learning often requires many (e.g., 10 to
100) epochs to reach a sufficient accuracy.
  3. Another critical issue is that for very large data sets it becomes
impossible to store a (fully) random permutation matrix, due to its space
requirements. Our paper is the first study to demonstrate that $b$-bit minwise
hashing implemented using simple hash functions, e.g., the 2-universal (2U) and
4-universal (4U) hash families, can produce very similar learning results as
using fully random permutations. Experiments on datasets of up to 200GB are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.2996</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.2996</id><created>2012-05-14</created><updated>2016-03-04</updated><authors><author><keyname>Ghosh</keyname><forenames>Mrinalkanti</forenames></author><author><keyname>Nandakumar</keyname><forenames>Satyadev</forenames></author></authors><title>Predictive Complexity and Generalized Entropy Rate of Stationary Ergodic
  Processes</title><categories>cs.IT math.IT</categories><comments>Only updated metadata, In Proceedings of the 23rd international
  conference on Algorithmic Learning Theory (ALT'12), Nader H. Bshouty, Gilles
  Stoltz, Nicolas Vayatis, and Thomas Zeugmann (Eds.). Springer-Verlag, Berlin,
  Heidelberg, 365-379., 2012</comments><doi>10.1007/978-3-642-34106-9_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the online prediction framework, we use generalized entropy of to study
the loss rate of predictors when outcomes are drawn according to stationary
ergodic distributions over the binary alphabet. We show that the notion of
generalized entropy of a regular game \cite{KVV04} is well-defined for
stationary ergodic distributions. In proving this, we obtain new game-theoretic
proofs of some classical information theoretic inequalities. Using Birkhoff's
ergodic theorem and convergence properties of conditional distributions, we
prove that a classical Shannon-McMillan-Breiman theorem holds for a restricted
class of regular games, when no computational constraints are imposed on the
prediction strategies.
  If a game is mixable, then there is an optimal aggregating strategy which
loses at most an additive constant when compared to any other lower
semicomputable strategy. The loss incurred by this algorithm on an infinite
sequence of outcomes is called its predictive complexity. We use our version of
Shannon-McMillan-Breiman theorem to prove that when a restriced regular game
has a predictive complexity, the predictive complexity converges to the
generalized entropy of the game almost everywhere with respect to the
stationary ergodic distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3015</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3015</id><created>2012-05-14</created><updated>2015-05-19</updated><authors><author><keyname>Ostroumova</keyname><forenames>Liudmila</forenames></author><author><keyname>Ryabchenko</keyname><forenames>Alexander</forenames></author><author><keyname>Samosvat</keyname><forenames>Egor</forenames></author></authors><title>Generalized preferential attachment: tunable power-law degree
  distribution and clustering coefficient</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a wide class of preferential attachment models of random graphs,
generalizing previous approaches. Graphs described by these models obey the
power-law degree distribution, with the exponent that can be controlled in the
models. Moreover, clustering coefficient of these graphs can also be
controlled. We propose a concrete flexible model from our class and provide an
efficient algorithm for generating graphs in this model. All our theoretical
results are demonstrated in practice on examples of graphs obtained using this
algorithm. Moreover, observations of generated graphs lead to future questions
and hypotheses not yet justified by theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3020</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3020</id><created>2012-05-14</created><updated>2013-02-26</updated><authors><author><keyname>Kang</keyname><forenames>Jaewook</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Bayesian Hypothesis Test for Sparse Support Recovery using Belief
  Propagation</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new support recovery algorithm from noisy
measurements called Bayesian hypothesis test via belief propagation (BHT-BP).
BHT-BP focuses on sparse support recovery rather than sparse signal estimation.
The key idea behind BHT-BP is to detect the support set of a sparse vector
using hypothesis test where the posterior densities used in the test are
obtained by aid of belief propagation (BP). Since BP provides precise posterior
information using the noise statistic, BHT-BP can recover the support with
robustness against the measurement noise. In addition, BHT-BP has low
computational cost compared to the other algorithms by the use of BP. We show
the support recovery performance of BHT-BP on the parameters (N; M; K; SNR) and
compare the performance of BHT-BP to OMP and Lasso via numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3021</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3021</id><created>2012-05-14</created><authors><author><keyname>Hussein</keyname><forenames>Sari Haj</forenames></author></authors><title>The Hush Cryptosystem</title><categories>cs.CR</categories><comments>7 pages. 5 figures. Appeared in the 2nd International Conference on
  Security of Information and Networks (SIN 2009), North Cyprus, Turkey;
  Proceedings of the 2nd International Conference on Security of Information
  and Networks (SIN 2009), North Cyprus, Turkey</comments><acm-class>E.3</acm-class><doi>10.1145/1626195.1626224</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a new cryptosystem we call &quot;The Hush Cryptosystem&quot;
for hiding encrypted data in innocent Arabic sentences. The main purpose of
this cryptosystem is to fool observer-supporting software into thinking that
the encrypted data is not encrypted at all. We employ a modified Word
Substitution Method known as the Grammatical Substitution Method in our
cryptosystem. We also make use of Hidden Markov Models. We test our
cryptosystem using a computer program written in the Java Programming Language.
Finally, we test the output of our cryptosystem using statistical tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3025</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3025</id><created>2012-05-14</created><updated>2013-11-20</updated><authors><author><keyname>Gewaltig</keyname><forenames>Marc-Oliver</forenames></author><author><keyname>Cannon</keyname><forenames>Robert</forenames></author></authors><title>Current practice in software development for computational neuroscience
  and how to improve it</title><categories>q-bio.NC cs.CY cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost all research work in computational neuroscience involves software. As
researchers try to understand ever more complex systems, there is a continual
need for software with new capabilities. Because of the wide range of questions
being investigated, new software is often developed rapidly by individuals or
small groups. In these cases, it can be hard to demonstrate that the software
gives the right results. Software developers are often open about the code they
produce and willing to share it, but there is little appreciation among
potential users of the great diversity of software development practices and
end results, and how this affects the suitability of software tools for use in
research projects. To help clarify these issues, we have reviewed a range of
software tools and asked how the culture and practice of software development
affects their validity and trustworthiness. We identified four key questions
that can be used to categorize software projects and correlate them with the
type of product that results. The first question addresses what is being
produced. The other three concern why, how, and by whom the work is done. The
answers to these questions show strong correlations with the nature of the
software being produced, and its suitability for particular purposes. Based on
our findings, we suggest ways in which current software development practice in
computational neuroscience can be improved and propose checklists to help
developers, reviewers and scientists to assess the quality whether particular
pieces of software are ready for use in research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3031</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3031</id><created>2012-05-14</created><authors><author><keyname>Lande</keyname><forenames>D. V.</forenames></author><author><keyname>Kalinovskiy</keyname><forenames>Ya. A.</forenames></author><author><keyname>Boyarinova</keyname><forenames>Yu. E.</forenames></author></authors><title>The model of information retrieval based on the theory of hypercomplex
  numerical systems</title><categories>cs.IR</categories><comments>4 pages</comments><msc-class>68P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provided a description of a new model of information retrieval,
which is an extension of vector-space model and is based on the principles of
the theory of hypercomplex numerical systems. The model allows to some extent
realize the idea of fuzzy search and allows you to apply in practice the model
of information retrieval practical developments in the field of hypercomplex
numerical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3047</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3047</id><created>2012-01-24</created><authors><author><keyname>Sambandan</keyname><forenames>Sanjiv</forenames></author></authors><title>Automating Open Fault Correction in Integrated Circuits via Field
  Induced Diffusion Limited Aggregation</title><categories>cs.ET cond-mat.soft</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform studies on electric field induced diffusion limited aggregation of
conductive particles dispersed in a non conductive medium. The bridges formed
across gaps between electrodes with an electric field in between due to the
aggregation mechanism provides a means to automate the correction of open
interconnect faults for integrated circuit application. We derive an expression
for the bridging time and describe the mechanics of bridge formation with the
above application in mind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3050</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3050</id><created>2012-05-14</created><authors><author><keyname>Curien</keyname><forenames>Pierre-Louis</forenames><affiliation>PPS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Operads, clones, and distributive laws</title><categories>math.CT cs.LO math.KT</categories><comments>Operads and Universal Algebra, China (2010)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how non-symmetric operads (or multicategories), symmetric operads,
and clones, arise from three suitable monads on Cat, each extending to a
(pseudo-)monad on the bicategory of categories and profunctors. We also explain
how other previous categorical analyses of operads (via Day's tensor products,
or via analytical functors) fit with the profunctor approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3054</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3054</id><created>2012-05-14</created><updated>2012-05-18</updated><authors><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Gabillon</keyname><forenames>Victor</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Geist</keyname><forenames>Matthieu</forenames><affiliation>UMI2958</affiliation></author></authors><title>Approximate Modified Policy Iteration</title><categories>cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that
contains the two celebrated policy and value iteration methods. Despite its
generality, MPI has not been thoroughly studied, especially its approximation
form which is used when the state and/or action spaces are large or infinite.
In this paper, we propose three implementations of approximate MPI (AMPI) that
are extensions of well-known approximate DP algorithms: fitted-value iteration,
fitted-Q iteration, and classification-based policy iteration. We provide error
propagation analyses that unify those for approximate policy and value
iteration. On the last classification-based implementation, we develop a
finite-sample analysis that shows that MPI's main parameter allows to control
the balance between the estimation error of the classifier and the overall
value function approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3058</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3058</id><created>2012-02-08</created><authors><author><keyname>Yazicioglu</keyname><forenames>Ahmet Yasin</forenames></author><author><keyname>Abbas</keyname><forenames>Waseem</forenames></author><author><keyname>Egerstedt</keyname><forenames>Magnus</forenames></author></authors><title>A Tight Lower Bound on the Controllability of Networks with Multiple
  Leaders</title><categories>cs.SY math.OC</categories><doi>10.1109/CDC.2012.6426844</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the controllability of networked systems with static
network topologies using tools from algebraic graph theory. Each agent in the
network acts in a decentralized fashion by updating its state in accordance
with a nearest-neighbor averaging rule, known as the consensus dynamics. In
order to control the system, external control inputs are injected into the so
called leader nodes, and the influence is propagated throughout the network.
Our main result is a tight topological lower bound on the rank of the
controllability matrix for such systems with arbitrary network topologies and
possibly multiple leaders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3062</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3062</id><created>2012-02-08</created><authors><author><keyname>Singhal</keyname><forenames>Priyank</forenames></author><author><keyname>Raul</keyname><forenames>Nataasha</forenames></author></authors><title>Malware Detection Module using Machine Learning Algorithms to Assist in
  Centralized Security in Enterprise Networks</title><categories>cs.CR cs.LG</categories><comments>6 pages</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.4, No.1, January 2012</journal-ref><doi>10.5121/ijnsa.2012.4106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malicious software is abundant in a world of innumerable computer users, who
are constantly faced with these threats from various sources like the internet,
local networks and portable drives. Malware is potentially low to high risk and
can cause systems to function incorrectly, steal data and even crash. Malware
may be executable or system library files in the form of viruses, worms,
Trojans, all aimed at breaching the security of the system and compromising
user privacy. Typically, anti-virus software is based on a signature definition
system which keeps updating from the internet and thus keeping track of known
viruses. While this may be sufficient for home-users, a security risk from a
new virus could threaten an entire enterprise network. This paper proposes a
new and more sophisticated antivirus engine that can not only scan files, but
also build knowledge and detect files as potential viruses. This is done by
extracting system API calls made by various normal and harmful executable, and
using machine learning algorithms to classify and hence, rank files on a scale
of security risk. While such a system is processor heavy, it is very effective
when used centrally to protect an enterprise network which maybe more prone to
such threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3068</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3068</id><created>2012-05-14</created><authors><author><keyname>Trapp</keyname><forenames>Sebastian</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Schiller</keyname><forenames>Jochen</forenames></author></authors><title>Bridge the Gap: Measuring and Analyzing Technical Data for Social Trust
  between Smartphones</title><categories>cs.NI cs.HC cs.SI</categories><acm-class>C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobiles are nowadays the most relevant communication devices in terms of
quantity and flexibility. Like in most MANETs ad-hoc communication between two
mobile phones requires mutual trust between the devices. A new way of
establishing this trust conducts social trust from technically measurable data
(e.g., interaction logs). To explore the relation between social and technical
trust, we conduct a large-scale survey with more than 217 Android users and
analyze their anonymized call and message logs. We show that a reliable a
priori trust value for a mobile system can be derived from common social
communication metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3077</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3077</id><created>2012-05-14</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Pierrakos</keyname><forenames>George</forenames></author><author><keyname>Singer</keyname><forenames>Yaron</forenames></author></authors><title>Efficiency-Revenue Trade-offs in Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When agents with independent priors bid for a single item, Myerson's optimal
auction maximizes expected revenue, whereas Vickrey's second-price auction
optimizes social welfare. We address the natural question of trade-offs between
the two criteria, that is, auctions that optimize, say, revenue under the
constraint that the welfare is above a given level. If one allows for
randomized mechanisms, it is easy to see that there are polynomial-time
mechanisms that achieve any point in the trade-off (the Pareto curve) between
revenue and welfare. We investigate whether one can achieve the same guarantees
using deterministic mechanisms. We provide a negative answer to this question
by showing that this is a (weakly) NP-hard problem. On the positive side, we
provide polynomial-time deterministic mechanisms that approximate with
arbitrary precision any point of the trade-off between these two fundamental
objectives for the case of two bidders, even when the valuations are correlated
arbitrarily. The major problem left open by our work is whether there is such
an algorithm for three or more bidders with independent valuation
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3109</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3109</id><created>2012-05-14</created><updated>2013-12-18</updated><authors><author><keyname>Guez</keyname><forenames>Arthur</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Dayan</keyname><forenames>Peter</forenames></author></authors><title>Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based
  Search</title><categories>cs.LG cs.AI stat.ML</categories><comments>14 pages, 7 figures, includes supplementary material. Advances in
  Neural Information Processing Systems (NIPS) 2012</comments><journal-ref>(2012) Advances in Neural Information Processing Systems 25, pages
  1034-1042</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian model-based reinforcement learning is a formally elegant approach to
learning optimal behaviour under model uncertainty, trading off exploration and
exploitation in an ideal way. Unfortunately, finding the resulting
Bayes-optimal policies is notoriously taxing, since the search space becomes
enormous. In this paper we introduce a tractable, sample-based method for
approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our
approach outperformed prior Bayesian model-based RL algorithms by a significant
margin on several well-known benchmark problems -- because it avoids expensive
applications of Bayes rule within the search tree by lazily sampling models
from the current beliefs. We illustrate the advantages of our approach by
showing it working in an infinite state space domain which is qualitatively out
of reach of almost all previous work in Bayesian exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3125</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3125</id><created>2012-02-09</created><authors><author><keyname>Khan</keyname><forenames>Md Nazmul Islam</forenames></author><author><keyname>Ahmed</keyname><forenames>Rashed</forenames></author><author><keyname>Aziz</keyname><forenames>Md. Tariq</forenames></author></authors><title>A Survey of TCP Reno, New Reno and Sack Over Mobile Ad-Hoc Network</title><categories>cs.NI</categories><comments>8 Figures, MANET</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS),
  Volume 3(1) ISSN : 0976 - 9757 (2012) [Online]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission Control Protocol (TCP) is often preferred to be implemented at
the transport layer of a Mobile Ad-hoc Network (MANET) because of its wide
range of applications, which enjoys the advantage of reliable data transmission
in the Internet. However, because of some unique characteristics of MANET, TCP
cannot offer reliable services while using e-mail, internet search and file
transmission in such a network. The research investigates how well the
different versions of TCP respond to various performance differentials when
subjected to different network stresses and topology changes, aside from
identifying the most efficient and robust TCP version(s) for different MANET
scenarios. Among several TCP variants, three types are considered important for
the analysis, namely TCP Reno, TCP New Reno and TCP Selective Acknowledgment
(SACK). In most cases, the TCP performance is found in our study to decrease
when the node size and mobility rate is increased in the network. There is,
however, exception to this. As our simulation results demonstrate, the
increases in the node velocity sometimes help the TCP to attain a better
performance. The study also reveals that out of the three variants, TCP SACK
can adapt relatively well to the changing network sizes while TCP Reno performs
most robustly in the presence of different mobility rates within MANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3137</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3137</id><created>2012-05-14</created><updated>2012-08-18</updated><authors><author><keyname>Singh</keyname><forenames>Saurabh</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author><author><keyname>Efros</keyname><forenames>Alexei A.</forenames></author></authors><title>Unsupervised Discovery of Mid-Level Discriminative Patches</title><categories>cs.CV cs.AI cs.LG</categories><journal-ref>European Conference on Computer Vision, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to discover a set of discriminative patches which
can serve as a fully unsupervised mid-level visual representation. The desired
patches need to satisfy two requirements: 1) to be representative, they need to
occur frequently enough in the visual world; 2) to be discriminative, they need
to be different enough from the rest of the visual world. The patches could
correspond to parts, objects, &quot;visual phrases&quot;, etc. but are not restricted to
be any one of them. We pose this as an unsupervised discriminative clustering
problem on a huge dataset of image patches. We use an iterative procedure which
alternates between clustering and training discriminative classifiers, while
applying careful cross-validation at each step to prevent overfitting. The
paper experimentally demonstrates the effectiveness of discriminative patches
as an unsupervised mid-level visual representation, suggesting that it could be
used in place of visual words for many tasks. Furthermore, discriminative
patches can also be used in a supervised regime, such as scene classification,
where they demonstrate state-of-the-art performance on the MIT Indoor-67
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3180</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3180</id><created>2012-05-14</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Villacorta</keyname><forenames>Pablo J.</forenames></author></authors><title>Community-Quality-Based Player Ranking in Collaborative Games with no
  Explicit Objectives</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Player ranking can be used to determine the quality of the contributions of a
player to a collaborative community. However, collaborative games with no
explicit objectives do not support player ranking, as there is no metric to
measure the quality of player contributions. An implicit objective of such
communities is not being disruptive towards other players. In this paper, we
propose a parameterizable approach for real-time player ranking in
collaborative games with no explicit objectives. Our method computes a ranking
by applying a simple heuristic community quality function. We also demonstrate
the capabilities of our approach by applying several parameterizations of it to
a case study and comparing the obtained results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3181</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3181</id><created>2012-05-14</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Wang</keyname><forenames>Tengyao</forenames></author><author><keyname>Viswanathan</keyname><forenames>Nitin</forenames></author></authors><title>Multiple Identifications in Multi-Armed Bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of identifying the top $m$ arms in a multi-armed bandit
game. Our proposed solution relies on a new algorithm based on successive
rejects of the seemingly bad arms, and successive accepts of the good ones.
This algorithmic contribution allows to tackle other multiple identifications
settings that were previously out of reach. In particular we show that this
idea of successive accepts and rejects applies to the multi-bandit best arm
identification problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3183</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3183</id><created>2012-05-14</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cortijo</keyname><forenames>Francisco J.</forenames></author></authors><title>A Model-Driven Probabilistic Parser Generator</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing probabilistic scanners and parsers impose hard constraints on the
way lexical and syntactic ambiguities can be resolved. Furthermore, traditional
grammar-based parsing tools are limited in the mechanisms they allow for taking
context into account. In this paper, we propose a model-driven tool that allows
for statistical language models with arbitrary probability estimators. Our work
on model-driven probabilistic parsing is built on top of ModelCC, a model-based
parser generator, and enables the probabilistic interpretation and resolution
of anaphoric, cataphoric, and recursive references in the disambiguation of
abstract syntax graphs. In order to prove the expression power of ModelCC, we
describe the design of a general-purpose natural language parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3188</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3188</id><created>2012-05-14</created><updated>2012-10-17</updated><authors><author><keyname>Huang</keyname><forenames>Xuqing</forenames></author><author><keyname>Shao</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author><author><keyname>Buldyrev</keyname><forenames>Sergey V.</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>The robustness of interdependent clustered networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 6 figures</comments><journal-ref>EPL, 101 (2013) 18002</journal-ref><doi>10.1209/0295-5075/101/18002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently found that cascading failures can cause the abrupt breakdown
of a system of interdependent networks. Using the percolation method developed
for single clustered networks by Newman [Phys. Rev. Lett. {\bf 103}, 058701
(2009)], we develop an analytical method for studying how clustering within the
networks of a system of interdependent networks affects the system's
robustness. We find that clustering significantly increases the vulnerability
of the system, which is represented by the increased value of the percolation
threshold $p_c$ in interdependent networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3193</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3193</id><created>2012-05-14</created><authors><author><keyname>Lee</keyname><forenames>Joonseok</forenames></author><author><keyname>Sun</keyname><forenames>Mingxuan</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author></authors><title>A Comparative Study of Collaborative Filtering Algorithms</title><categories>cs.IR stat.ML</categories><comments>27 pages, 12 figures</comments><acm-class>I.2.6; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering is a rapidly advancing research area. Every year
several new techniques are proposed and yet it is not clear which of the
techniques work best and under what conditions. In this paper we conduct a
study comparing several collaborative filtering techniques -- both classic and
recent state-of-the-art -- in a variety of experimental contexts. Specifically,
we report conclusions controlling for number of items, number of users,
sparsity level, performance criteria, and computational complexity. Our
conclusions identify what algorithms work well and in what conditions, and
contribute to both industrial deployment collaborative filtering algorithms and
to the research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3205</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3205</id><created>2012-05-14</created><authors><author><keyname>Kim</keyname><forenames>Seungyeon</forenames></author><author><keyname>Dillon</keyname><forenames>Joshua V.</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author></authors><title>Cumulative Revision Map</title><categories>cs.HC cs.DL</categories><comments>17 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike static documents, version-controlled documents are edited by one or
more authors over a certain period of time. Examples include large scale
computer code, papers authored by a team of scientists, and online discussion
boards. Such collaborative revision process makes traditional document modeling
and visualization techniques inappropriate. In this paper we propose a new
visualization technique for version-controlled documents that reveals
interesting authoring patterns in papers, computer code and Wikipedia articles.
The revealed authoring patterns are useful for the readers, participants in the
authoring process, and supervisors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3208</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3208</id><created>2012-05-14</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Agaian</keyname><forenames>Sos</forenames></author><author><keyname>Noonan</keyname><forenames>Joseph P.</forenames></author></authors><title>A New Family of Generalized 3D Cat Maps</title><categories>nlin.CD cs.CR</categories><comments>A paper draft sent to IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the 1990s chaotic cat maps are widely used in data encryption, for
their very complicated dynamics within a simple model and desired
characteristics related to requirements of cryptography. The number of cat map
parameters and the map period length after discretization are two major
concerns in many applications for security reasons. In this paper, we propose a
new family of 36 distinctive 3D cat maps with different spatial configurations
taking existing 3D cat maps [1]-[4] as special cases. Our analysis and
comparisons show that this new 3D cat maps family has more independent map
parameters and much longer averaged period lengths than existing 3D cat maps.
The presented cat map family can be extended to higher dimensional cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3212</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3212</id><created>2012-05-14</created><authors><author><keyname>Zhao</keyname><forenames>Siqi</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Wickramasuriya</keyname><forenames>Jehan</forenames></author><author><keyname>Vasudevan</keyname><forenames>Venu</forenames></author><author><keyname>LiKamWa</keyname><forenames>Robert</forenames></author><author><keyname>Rahmati</keyname><forenames>Ahmad</forenames></author></authors><title>SportSense: Real-Time Detection of NFL Game Events from Twitter</title><categories>cs.SI physics.soc-ph</categories><report-no>Technical Report TR0511-2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report our experience in building a working system, SportSense
(http://www.sportsense.us), which exploits Twitter users as human sensors of
the physical world to detect events in real-time. Using the US National
Football League (NFL) games as a case study, we report in-depth measurement
studies of the delay and post rate of tweets, and their dependence on other
properties. We subsequently develop a novel event detection method based on
these findings, and demonstrate that it can effectively and accurately extract
game events using open access Twitter data. SportSense has been evolving during
the 2010-11 and 2011-12 NFL seasons and is able to recognize NFL game big plays
in 30 to 90 seconds with 98% true positive, and 9% false positive rates. Using
a smart electronic TV program guide, we show that SportSense can utilize human
sensors to empower novel services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3225</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3225</id><created>2012-05-14</created><authors><author><keyname>Parvaresh</keyname><forenames>Farzad</forenames></author><author><keyname>Etkin</keyname><forenames>Raul</forenames></author></authors><title>Using Superposition Codebooks and Partial Decode and Forward in Low SNR
  Parallel Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new communication scheme for Gaussian parallel relay networks based on
superposition coding and partial decoding at the relays is presented. Some
specific examples are proposed in which two codebook layers are superimposed.
The first level codebook is constructed with symbols from a binary or ternary
alphabet while the second level codebook is composed of codewords chosen with
Gaussian symbols. The new communication scheme is a generalization of
decode-and-forward, amplify-and-forward, and bursty-amplify-and-forward. The
asymptotic low SNR regime is studied using achievable rates and minimum
energy-per-bit as performance metrics. It is shown that the new scheme
outperforms all previously known schemes for some channels and parameter
ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3231</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3231</id><created>2012-05-14</created><authors><author><keyname>T</keyname><forenames>Rekha Sunny</forenames></author><author><keyname>Thampi</keyname><forenames>Sabu M.</forenames></author></authors><title>Survey on Distributed Data Mining in P2P Networks</title><categories>cs.DB</categories><comments>23 pages 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exponential increase of availability of digital data and the necessity to
process it in business and scientific fields has literally forced upon us the
need to analyze and mine useful knowledge from it. Traditionally data mining
has used a data warehousing model of gathering all data into a central site,
and then running an algorithm upon that data. Such a centralized approach is
fundamentally inappropriate due to many reasons like huge amount of data,
infeasibility to centralize data stored at multiple sites, bandwidth limitation
and privacy concerns. To solve these problems, Distributed Data Mining (DDM)
has emerged as a hot research area. Careful attention in the usage of
distributed resources of data, computing, communication, and human factors in a
near optimal fashion are paid by distributed data mining. DDM is gaining
attention in peer-to-peer (P2P) systems which are emerging as a choice of
solution for applications such as file sharing, collaborative movie and song
scoring, electronic commerce, and surveillance using sensor networks. The main
intension of this draft paper is to provide an overview of DDM and P2P Data
Mining. The paper discusses the need for DDM, taxonomy of DDM architectures,
various DDM approaches, DDM related works in P2P systems and issues and
challenges in P2P data mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3245</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3245</id><created>2012-05-14</created><authors><author><keyname>Marcelino</keyname><forenames>Jose</forenames></author><author><keyname>Kaiser</keyname><forenames>Marcus</forenames></author></authors><title>Critical paths in a metapopulation model of H1N1: Efficiently delaying
  influenza spreading through flight cancellation</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>The network of connections between the top 500 airports is available
  under the resources link on our website http://www.biological-networks.org.
  Flight connections for the top 500 airports, based on total passenger volume,
  worldwide. The existence of flight connections between airports is based on
  flights within one year from 1 July 2007 to 30 June 2008</comments><journal-ref>PLoS Currents Influenza 2012 Apr 23</journal-ref><doi>10.1371/4f8c9a2e1fca8</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Disease spreading through human travel networks has been a topic of great
interest in recent years, as witnessed during outbreaks of influenza A (H1N1)
or SARS pandemics. One way to stop spreading over the airline network are
travel restrictions for major airports or network hubs based on the total
number of passengers of an airport. Here, we test alternative strategies using
edge removal, cancelling targeted flight connections rather than restricting
traffic for network hubs, for controlling spreading over the airline network.
We employ a SEIR metapopulation model that takes into account the population of
cities, simulates infection within cities and across the network of the top 500
airports, and tests different flight cancellation methods for limiting the
course of infection. The time required to spread an infection globally, as
simulated by a stochastic global spreading model was used to rank the candidate
control strategies. The model includes both local spreading dynamics at the
level of populations and long-range connectivity obtained from real global
airline travel data. Simulated spreading in this network showed that spreading
infected 37% less individuals after cancelling a quarter of flight connections
between cities, as selected by betweenness centrality. The alternative strategy
of closing down whole airports causing the same number of cancelled connections
only reduced infections by 18%. In conclusion, selecting highly ranked single
connections between cities for cancellation was more effective, resulting in
fewer individuals infected with influenza, compared to shutting down whole
airports. It is also a more efficient strategy, affecting fewer passengers
while producing the same reduction in infections.
  The network of connections between the top 500 airports is available under
the resources link on our website http://www.biological-networks.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3247</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3247</id><created>2012-05-14</created><authors><author><keyname>Sanaei</keyname><forenames>Zohreh</forenames></author><author><keyname>Abolfazli</keyname><forenames>Saeid</forenames></author><author><keyname>Gani</keyname><forenames>Abdullah</forenames></author><author><keyname>Khokhar</keyname><forenames>Rashid Hafeez</forenames></author></authors><title>Tripod of Requirements in Horizontal Heterogeneous Mobile Cloud
  Computing</title><categories>cs.DC</categories><journal-ref>Z. Sanaei, S. Abolfazli, A. Gani, and R. H. Khokhar, &quot;Tripod of
  requirements in horizontal heterogeneous mobile cloud computing,&quot; in Proc.1st
  Int'l Conf. Computing, Information Systems, and Communications, 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent trend of mobile computing is emerging toward executing
resource-intensive applications in mobile devices regardless of underlying
resource restrictions (e.g. limited processor and energy) that necessitate
imminent technologies. Prosperity of cloud computing in stationary computers
breeds Mobile Cloud Computing (MCC) technology that aims to augment computing
and storage capabilities of mobile devices besides conserving energy. However,
MCC is more heterogeneous and unreliable (due to wireless connectivity) compare
to cloud computing. Problems like variations in OS, data fragmentation, and
security and privacy discourage and decelerate implementation and pervasiveness
of MCC. In this paper, we describe MCC as a horizontal heterogeneous ecosystem
and identify thirteen critical metrics and approaches that influence on
mobile-cloud solutions and success of MCC. We divide them into three major
classes, namely ubiquity, trust, and energy efficiency and devise a tripod of
requirements in MCC. Our proposed tripod shows that success of MCC is
achievable by reducing mobility challenges (e.g. seamless connectivity,
fragmentation), increasing trust, and enhancing energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3252</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3252</id><created>2012-05-15</created><authors><author><keyname>Xu</keyname><forenames>Xiaozhong</forenames></author><author><keyname>Alay</keyname><forenames>Ozgu</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra</forenames></author></authors><title>Two-way Wireless Video Communication using Randomized Cooperation,
  Network Coding and Packet Level FEC</title><categories>cs.IT cs.NI math.IT</categories><comments>ICC 2012 workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way real-time video communication in wireless networks requires high
bandwidth, low delay and error resiliency. This paper addresses these demands
by proposing a system with the integration of Network Coding (NC), user
cooperation using Randomized Distributed Space-time Coding (R-DSTC) and packet
level Forward Error Correction (FEC) under a one-way delay constraint.
Simulation results show that the proposed scheme significantly outperforms both
conventional direct transmission as well as R-DSTC based two-way cooperative
transmission, and is most effective when the distance between the users is
large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3269</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3269</id><created>2012-05-15</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Characterization and Moment Stability Analysis of Quasilinear Quantum
  Stochastic Systems with Quadratic Coupling to External Fields</title><categories>quant-ph cs.SY math.DS math.OC math.PR</categories><comments>14 pages, submitted to CDC 2012 on 7 March 2012</comments><msc-class>81S22, 81S25, 93B25, 93B28, 93E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with open quantum systems whose Heisenberg dynamics
are described by quantum stochastic differential equations driven by external
boson fields. The system-field coupling operators are assumed to be quadratic
polynomials of the system observables, with the latter satisfying canonical
commutation relations. In combination with a cubic system Hamiltonian, this
leads to a class of quasilinear quantum stochastic systems which retain
algebraic closedness in the evolution of mixed moments of the observables.
Although such a system is nonlinear and its quantum state is no longer
Gaussian, the dynamics of the moments of any order are amenable to exact
analysis, including the computation of their steady-state values. In
particular, a generalized criterion is developed for quadratic stability of the
quasilinear systems. The results of the paper are applicable to the generation
of non-Gaussian quantum states with manageable moments and an optimal design of
linear quantum controllers for quasilinear quantum plants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3272</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3272</id><created>2012-05-15</created><authors><author><keyname>Patel</keyname><forenames>Aaqib</forenames></author><author><keyname>Khan</keyname><forenames>Md. Zafar Ali</forenames></author><author><keyname>Merchant</keyname><forenames>S. N.</forenames></author><author><keyname>Desai</keyname><forenames>U. B.</forenames></author></authors><title>Capacity and Spectral Efficiency of Interference Avoiding Cognitive
  Radio with Imperfect Detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a model in which the unlicensed or the Secondary
User (SU) equipped with a Cognitive Radio (CR) (together referred to as CR)
interweaves its transmission with that of the licensed or the Primary User
(PU). In this model, when the CR detects the PU to be (i) busy it does not
transmit and; (ii) PU to be idle it transmits. Two situations based on CR's
detection of PU are considered, where the CR detects PU (i) perfectly -
referred to as the &quot;ideal case&quot; and; (ii) imperfectly - referred to as &quot;non
ideal case&quot;. For both the cases we bring out the rate region, sum capacity of
PU and CR and spectral efficiency factor - the ratio of sum capacity of PU and
CR to the capacity of PU without CR. We consider the Rayleigh fading channel to
provide insight to our results. For the ideal case we study the effect of PU
occupancy on spectral efficiency factor. For the non ideal case, in addition to
the effect of occupancy, we study the effect of false alarm and missed
detection on the rate region and spectral efficiency factor. We characterize
the set of values of false alarm and missed detection probabilities for which
the system benefits, in the form of admissible regions. We show that false
alarm has a more profound effect on the spectral efficiency factor than missed
detection. We also show that when PU occupancy is small, the effects of both
false alarm and missed detection decrease. Finally, for the standard detection
techniques viz. energy detection, matched filter and magnitude squared
coherence, we show that that the matched filter performs best followed by
magnitude squared coherence followed by energy detection with respect to
spectral efficiency factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3277</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3277</id><created>2012-05-15</created><authors><author><keyname>lin</keyname><forenames>Cen</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Cross-Layer Optimization of Two-Way Relaying for Statistical QoS
  Guarantees</title><categories>cs.IT math.IT</categories><comments>14 pages, 9 figures, accepted for publication</comments><journal-ref>IEEE Journal on Selected Areas in Communications, Special Issue on
  Theories and Methods for Advanced Wireless Relays, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way relaying promises considerable improvements on spectral efficiency in
wireless relay networks. While most existing works focus on physical layer
approaches to exploit its capacity gain, the benefits of two-way relaying on
upper layers are much less investigated. In this paper, we study the
cross-layer design and optimization for delay quality-of-service (QoS)
provisioning in two-way relay systems. Our goal is to find the optimal
transmission policy to maximize the weighted sum throughput of the two users in
the physical layer while guaranteeing the individual statistical delay-QoS
requirement for each user in the datalink layer. This statistical delay-QoS
requirement is characterized by the QoS exponent. By integrating the concept of
effective capacity, the cross-layer optimization problem is equivalent to a
weighted sum effective capacity maximization problem. We derive the jointly
optimal power and rate adaptation policies for both three-phase and two-phase
two-way relay protocols. Numerical results show that the proposed adaptive
transmission policies can efficiently provide QoS guarantees and improve the
performance. In addition, the throughput gain obtained by the considered
three-phase and two-phase protocols over direct transmission is significant
when the delay-QoS requirements are loose, but the gain diminishes at tight
delay requirements. It is also found that, in the two-phase protocol, the relay
node should be placed closer to the source with more stringent delay
requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3286</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3286</id><created>2012-05-15</created><authors><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>On Linear Coherent Estimation with Spatial Collaboration</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures. This is a manuscript under preparation and will
  be submitted to IEEE Transactions on Signal processing. Part of this work has
  been accepted for publication in Proceedings of ISIT-2012, IEEE International
  Symposium of Information Theory, July 1--6, 2012, Cambridge, MA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a power-constrained sensor network, consisting of multiple sensor
nodes and a fusion center (FC), that is deployed for the purpose of estimating
a common random parameter of interest. In contrast to the distributed
framework, the sensor nodes are allowed to update their individual observations
by (linearly) combining observations from neighboring nodes. The updated
observations are communicated to the FC using an analog amplify-and-forward
modulation scheme and through a coherent multiple access channel. The optimal
collaborative strategy is obtained by minimizing the cumulative transmission
power subject to a maximum distortion constraint. For the distributed scenario
(i.e., with no observation sharing), the solution reduces to the
power-allocation problem considered by [Xiao, TSP08]. Collaboration among
neighbors significantly improves power efficiency of the network in the low
local-SNR regime, as demonstrated through an insightful example and numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3305</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3305</id><created>2012-05-15</created><authors><author><keyname>Zayani</keyname><forenames>Mohamed-Haykel</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Zeghlache</keyname><forenames>Djamal</forenames></author></authors><title>A Joint Model for IEEE 802.15.4 Physical and Medium Access Control
  Layers</title><categories>cs.NI</categories><comments>Published in the proceeding of the 7th International Wireless
  Communications and Mobile Computing Conference (IWCMC), Istanbul, Turkey,
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies have tried to evaluate wireless networks and especially the IEEE
802.15.4 standard. Hence, several papers have aimed to describe the
functionalities of the physical (PHY) and medium access control (MAC) layers.
They have highlighted some characteristics with experimental results and/or
have attempted to reproduce them using theoretical models. In this paper, we
use the first way to better understand IEEE 802.15.4 standard. Indeed, we
provide a comprehensive model, able more faithfully to mimic the
functionalities of this standard at the PHY and MAC layers. We propose a
combination of two relevant models for the two layers. The PHY layer behavior
is reproduced by a mathematical framework, which is based on radio and channel
models, in order to quantify link reliability. On the other hand, the MAC layer
is mimed by an enhanced Markov chain. The results show the pertinence of our
approach compared to the model based on a Markov chain for IEEE 802.15.4 MAC
layer. This contribution allows us fully and more precisely to estimate the
network performance with different network sizes, as well as different metrics
such as node reliability and delay. Our contribution enables us to catch
possible failures at both layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3310</identifier>
 <datestamp>2012-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3310</id><created>2012-05-15</created><updated>2012-07-30</updated><authors><author><keyname>Hall</keyname><forenames>Joanne L.</forenames></author><author><keyname>Rao</keyname><forenames>Asha</forenames></author><author><keyname>Donovan</keyname><forenames>Diane</forenames></author></authors><title>Planar Difference Functions</title><categories>math.CO cs.IT math.IT quant-ph</categories><comments>Accepted to ISIT2012. v2 added a reference and adjusted margins</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1980 Alltop produced a family of cubic phase sequences that nearly meet
the Welch bound for maximum non-peak correlation magnitude. This family of
sequences were shown by Wooters and Fields to be useful for quantum state
tomography. Alltop's construction used a function that is not planar, but whose
difference function is planar. In this paper we show that Alltop type functions
cannot exist in fields of characteristic 3 and that for a known class of planar
functions, $x^3$ is the only Alltop type function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3316</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3316</id><created>2012-05-15</created><authors><author><keyname>Terbeh</keyname><forenames>Naim</forenames></author><author><keyname>Zrigui</keyname><forenames>Mounir</forenames></author></authors><title>Arabic Language Learning Assisted by Computer, based on Automatic Speech
  Recognition</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work consists of creating a system of the Computer Assisted Language
Learning (CALL) based on a system of Automatic Speech Recognition (ASR) for the
Arabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.
To this work, we have constructed a corpus of six hours of speech recordings
with a number of nine speakers. we find in the robustness to noise a grounds
for the choice of the HMM approach [2]. the results achieved are encouraging
since our corpus is made by only nine speakers, but they are always reasons
that open the door for other improvement works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3317</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3317</id><created>2012-05-15</created><authors><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Chiani</keyname><forenames>Marco</forenames></author></authors><title>Spatially-Coupled Random Access on Graphs</title><categories>cs.IT math.IT</categories><comments>To be presented at IEEE ISIT 2012, Boston</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the effect of spatial coupling applied to the
recently-proposed coded slotted ALOHA (CSA) random access protocol. Thanks to
the bridge between the graphical model describing the iterative interference
cancelation process of CSA over the random access frame and the erasure
recovery process of low-density parity-check (LDPC) codes over the binary
erasure channel (BEC), we propose an access protocol which is inspired by the
convolutional LDPC code construction. The proposed protocol exploits the
terminations of its graphical model to achieve the spatial coupling effect,
attaining performance close to the theoretical limits of CSA. As for the
convolutional LDPC code case, large iterative decoding thresholds are obtained
by simply increasing the density of the graph. We show that the threshold
saturation effect takes place by defining a suitable counterpart of the
maximum-a-posteriori decoding threshold of spatially-coupled LDPC code
ensembles. In the asymptotic setting, the proposed scheme allows sustaining a
traffic close to 1 [packets/slot].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3319</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3319</id><created>2012-05-15</created><authors><author><keyname>Chaudhuri</keyname><forenames>Sruti Gan</forenames></author></authors><title>Design and implementation of a differentiated service based qos model
  for real-time interactive traffic on constrained bandwidth ip networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis work, a QoS model for real-time interactive traffic on a real
network with constrained bandwidth and real-time traffic has been proposed. The
model supports tight guarantees of QoS to real-time interactive traffic without
over provisioning of bandwidth. A dynamic scheduling model which is adaptive to
input data rate of traffic has been proposed. In this model, A Differentiated
Service (DiffServ) based approach is proposed for QoS provisioning. The packets
are classified and distributed among finite number of queues with limited
buffer based on different priorities and total available bandwidth. The model
proposes a mechanism to derive the weighted service rates and queue length
distribution so as to meet the requirement of low packet loss and delay for
real time interactive traffic in the QoS engineered network. An adaptive
queuing strategy is proposed so that minimum bandwidth in used for real time
traffic. This ensures maximizing availability to best effort traffic. The model
assumes constrained bandwidth without having to over provision the network
resources and thus keeping the cost low. A modified version suitable for
testing on a real network is also presented. Experimental verification of these
in a test bed network in a laboratory as well as on a real network has been
carried out. The results of the QoS provisioning model for different sources of
real-time traffic such as video conferencing equipment, robotic surveillance
camera has also been shown. The thesis also introduces a real-time Variable Bit
Rate (VBR) traffic tuning parameter for controlling the service of VBR traffic
to give better and fair performance to rest of the traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3321</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3321</id><created>2012-05-15</created><updated>2012-12-28</updated><authors><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>Tree Projections and Structural Decomposition Methods: The Power of
  Local Consistency and Larger Islands of Tractability</title><categories>cs.DB</categories><acm-class>H.2.4; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating conjunctive queries and solving constraint satisfaction problems
are fundamental problems in database theory and artificial intelligence,
respectively. These problems are NP-hard, so that several research efforts have
been made in the literature for identifying tractable classes, known as islands
of tractability, as well as for devising clever heuristics for solving
efficiently real-world instances. Many heuristic approaches are based on
enforcing on the given instance a property called local consistency, where (in
database terms) each tuple in every query atom matches at least one tuple in
every other query atom. Interestingly, it turns out that, for many well-known
classes of queries, such as for the acyclic queries, enforcing local
consistency is even sufficient to solve the given instance correctly. However,
the precise power of such a procedure was unclear, but for some very restricted
cases. The paper provides full answers to the long-standing questions about the
precise power of algorithms based on enforcing local consistency. The classes
of instances where enforcing local consistency turns out to be a correct
query-answering procedure are however not efficiently recognizable. In fact,
the paper finally focuses on certain subclasses defined in terms of the novel
notion of greedy tree projections. These latter classes are shown to be
efficiently recognizable and strictly larger than most islands of tractability
known so far, both in the general case of tree projections and for specific
structural decomposition methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3322</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3322</id><created>2012-05-15</created><authors><author><keyname>Zayani</keyname><forenames>Mohamed-Haykel</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Zeghlache</keyname><forenames>Djamal</forenames></author></authors><title>Improving Link Prediction in Intermittently Connected Wireless Networks
  by Considering Link and Proximity Stabilities</title><categories>cs.NI</categories><comments>Published in the proceedings of the 13th IEEE International Symposium
  on a World of Wireless, Mobile and Multimedia Networks (WoWMoM), San
  Francisco, United States, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several works have outlined the fact that the mobility in intermittently
connected wireless networks is strongly governed by human behaviors as they are
basically human-centered. It has been shown that the users' moves can be
correlated and that the social ties shared by the users highly impact their
mobility patterns and hence the network structure. Tracking these correlations
and measuring the strength of social ties have led us to propose an efficient
distributed tensor-based link prediction technique. In fact, we are convinced
that the feedback provided by such a prediction mechanism can enhance
communication protocols such as opportunistic routing protocols. In this paper,
we aim to bring out that measuring the stabilities of the link and the
proximity at two hops can improve the efficiency of the proposed link
prediction technique. To quantify these two parameters, we propose an entropy
estimator in order to measure the two stability aspects over successive time
periods. Then, we join these entropy estimations to the tensor-based link
prediction framework by designing new prediction metrics. To assess the
contribution of these entropy estimations in the enhancement of tensor-based
link prediction efficiency, we perform prediction on two real traces. Our
simulation results show that by exploiting the information corresponding to the
link stability and/or to the proximity stability, the performance of the
tensor-based link prediction technique is improved. Moreover, the results
attest that our proposal's ability to outperform other well-known prediction
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3327</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3327</id><created>2012-05-15</created><authors><author><keyname>Zayani</keyname><forenames>Mohamed-Haykel</forenames></author><author><keyname>Zeghlache</keyname><forenames>Djamal</forenames></author></authors><title>Cooperation Enforcement for Packet Forwarding Optimization in Multi-hop
  Ad-hoc Networks</title><categories>cs.NI</categories><comments>Published in the proceedings of the IEEE Wireless Communications and
  Networking Conference (WCNC 2012), Paris, France, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ad-hoc networks are independent of any infrastructure. The nodes are
autonomous and make their own decisions. They also have limited energy
resources. Thus, a node tends to behave selfishly when it is asked to forward
the packets of other nodes. Indeed, it would rather choose to reject a
forwarding request in order to save its energy. To overcome this problem, the
nodes need to be motivated to cooperate. To this end, we propose a
self-learning repeated game framework to enforce cooperation between the nodes
of a network. This framework is inspired by the concept of &quot;The Weakest Link&quot;
TV game. Each node has a utility function whose value depends on its
cooperation in forwarding packets on a route as well as the cooperation of all
the nodes that form this same route. The more these nodes cooperate the higher
is their utility value. This would establish a cooperative spirit within the
nodes of the networks. All the nodes will then more or less equally participate
to the forwarding tasks which would then eventually guarantee a more efficient
packets forwarding from sources to respective destinations. Simulations are run
and the results show that the proposed framework efficiently enforces nodes to
cooperate and outperforms two other self-learning repeated game frameworks
which we are interested in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3328</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3328</id><created>2012-05-15</created><authors><author><keyname>Zayani</keyname><forenames>Mohamed-Haykel</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Slama</keyname><forenames>Ines</forenames></author><author><keyname>Zeghlache</keyname><forenames>Djamal</forenames></author></authors><title>Tracking Topology Dynamicity for Link Prediction in Intermittently
  Connected Wireless Networks</title><categories>cs.NI</categories><comments>Published in the proceedings of the 8th International Wireless
  Communications and Mobile Computing Conference (IWCMC), Limassol, Cyprus,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through several studies, it has been highlighted that mobility patterns in
mobile networks are driven by human behaviors. This effect has been
particularly observed in intermittently connected networks like DTN (Delay
Tolerant Networks). Given that common social intentions generate similar human
behavior, it is relevant to exploit this knowledge in the network protocols
design, e.g. to identify the closeness degree between two nodes. In this paper,
we propose a temporal link prediction technique for DTN which quantifies the
behavior similarity between each pair of nodes and makes use of it to predict
future links. We attest that the tensor-based technique is effective for
temporal link prediction applied to the intermittently connected networks. The
validity of this method is proved when the prediction is made in a distributed
way (i.e. with local information) and its performance is compared to well-known
link prediction metrics proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3336</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3336</id><created>2012-05-15</created><authors><author><keyname>Tall&#xf3;n-Ballesteros</keyname><forenames>A. J.</forenames></author><author><keyname>Guti&#xe9;rrez-Pe&#xf1;a</keyname><forenames>P. A.</forenames></author><author><keyname>Herv&#xe1;s-Mart&#xed;nez</keyname><forenames>C.</forenames></author></authors><title>Distribution of the search of evolutionary product unit neural networks
  for classification</title><categories>cs.NE cs.AI cs.CV</categories><comments>8 pages, 2 figures, in Proc. IADIS International Conference Applied
  Computing 2007 (AC 2007), ISBN 978-972-8924-30-0, pp. 266-273, Spain. Note:
  &quot;This is a reprint from a paper published in the Proceedings of the IADIS
  International Conference Applied Comupting 2007, http://www.iadis.org&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the distributed processing in the search for an optimum
classification model using evolutionary product unit neural networks. For this
distributed search we used a cluster of computers. Our objective is to obtain a
more efficient design than those net architectures which do not use a
distributed process and which thus result in simpler designs. In order to get
the best classification models we use evolutionary algorithms to train and
design neural networks, which require a very time consuming computation. The
reasons behind the need for this distribution are various. It is complicated to
train this type of nets because of the difficulty entailed in determining their
architecture due to the complex error surface. On the other hand, the use of
evolutionary algorithms involves running a great number of tests with different
seeds and parameters, thus resulting in a high computational cost
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3352</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3352</id><created>2012-05-15</created><authors><author><keyname>Peres</keyname><forenames>Lucas R.</forenames></author><author><keyname>Fontanari</keyname><forenames>Jose F.</forenames></author></authors><title>Revisiting the effect of external fields in Axelrod's model of social
  dynamics</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. E 86, 031131 (2012)</journal-ref><doi>10.1103/PhysRevE.86.031131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of the effects of spatially uniform fields on the steady-state
properties of Axelrod's model has yielded plenty of controversial results. Here
we re-examine the impact of this type of field for a selection of parameters
such that the field-free steady state of the model is heterogeneous or
multicultural. Analyses of both one and two-dimensional versions of Axelrod's
model indicate that, contrary to previous claims in the literature, the steady
state remains heterogeneous regardless of the value of the field strength.
Turning on the field leads to a discontinuous decrease on the number of
cultural domains, which we argue is due to the instability of zero-field
heterogeneous absorbing configurations. We find, however, that spatially
nonuniform fields that implement a consensus rule among the neighborhood of the
agents enforces homogenization. Although the overall effects of the fields are
essentially the same irrespective of the dimensionality of the model, we argue
that the dimensionality has a significant impact on the stability of the
field-free homogeneous steady state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3371</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3371</id><created>2012-05-15</created><authors><author><keyname>Luque</keyname><forenames>Jean-Gabriel</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author><author><keyname>Nicart</keyname><forenames>Florent</forenames></author></authors><title>Some Combinatorial Operators in Language Theory</title><categories>cs.FL cs.DM math.CO</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multitildes are regular operators that were introduced by Caron et al. in
order to increase the number of Glushkov automata. In this paper, we study the
family of the multitilde operators from an algebraic point of view using the
notion of operad. This leads to a combinatorial description of already known
results as well as new results on compositions, actions and enumerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3378</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3378</id><created>2012-05-15</created><authors><author><keyname>Kaspi</keyname><forenames>Yonatan</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>On Real-Time and Causal Secure Source Coding</title><categories>cs.IT math.IT</categories><comments>5 pages. Will be presented at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two source coding problems with secrecy constraints. In the
first problem we consider real--time fully secure transmission of a memoryless
source. We show that although classical variable--rate coding is not an option
since the lengths of the codewords leak information on the source, the key rate
can be as low as the average Huffman codeword length of the source. In the
second problem we consider causal source coding with a fidelity criterion and
side information at the decoder and the eavesdropper. We show that when the
eavesdropper has degraded side information, it is optimal to first use a causal
rate distortion code and then encrypt its output with a key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3380</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3380</id><created>2012-02-11</created><authors><author><keyname>Bakman</keyname><forenames>Yefim</forenames></author></authors><title>Unfair items detection in educational measurement</title><categories>cs.AI</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurement professionals cannot come to an agreement on the definition of
the term 'item fairness'. In this paper a continuous measure of item unfairness
is proposed. The more the unfairness measure deviates from zero, the less fair
the item is. If the measure exceeds the cutoff value, the item is identified as
definitely unfair. The new approach can identify unfair items that would not be
identified with conventional procedures. The results are in accord with
experts' judgments on the item qualities. Since no assumptions about scores
distributions and/or correlations are assumed, the method is applicable to any
educational test. Its performance is illustrated through application to scores
of a real test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3394</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3394</id><created>2012-02-07</created><authors><author><keyname>Aida</keyname><forenames>Zaier</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>A Full Performance Analysis of Channel Estimation Methods for Time
  Varying OFDM Systems</title><categories>cs.OH cs.NI</categories><comments>20 pages, 11 figures</comments><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT), Vol.1, No.2 (2011)</journal-ref><doi>10.5121/ijmnct.2011.1201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have evaluated various methods of time-frequency-selective
fading channels estimation in OFDM system and some of them improved under time
varying conditions. So, these different techniques will be studied through
different algorithms and for different schemes of modulations (16 QAM, BPSK,
QPSK, ...). Channel estimation gathers different schemes and algorithms, some
of them are dedicated for slowly time varying (such as block type arrangement
insertion, Bayesian Cramer-Rao Bound, Kalman estimator, Subspace estimator,
...) whereas the others concern highly time varying channels (comb type
insertion, ...). There are others methods that are just suitable for stationary
channels like blind or semi blind estimators. For this aim, diverse algorithms
were used for these schemes such as Least Squares estimator LS, Least Minimum
Squares LMS, Minimum Mean-Square-Error MMSE, Linear Minimum Mean-Square-Error
LMMSE, Maximum Likelihood ML, ... to refine estimators shown previously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3397</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3397</id><created>2012-05-15</created><authors><author><keyname>Calinescu</keyname><forenames>Gruia</forenames></author></authors><title>1.85 Approximation for Min-Power Strong Connectivity</title><categories>cs.DS</categories><comments>Same algorithm, with the analysis improved and simplified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed simple graph G=(V,E) and a nonnegative-valued cost function
the power of a vertex u in a directed spanning subgraph H is given by the
maximum cost of an arcs of H exiting u. The power of H is the sum of the power
of its vertices.
  Power Assignment seeks to minimize the power of H while H satisfies some
connectivity constraint. In this paper, we assume E is bidirected (for every
directed edge e in E, the opposite edge exists and has the same cost), while H
is required to be strongly connected. This is the original power assignment
problem introduced by Chen and Huang in 1989, who proved that bidirected
minimum spanning tree has approximation ratio at most 2 (this is tight). In
Approx 2010, we introduced a Greedy approximation algorithm and claimed a ratio
of 1.992. Here we improve the analysis to 1.85.
  The proof also shows that a natural linear programming relaxation, introduced
by us in 2012, has the same 1.85 integrality gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3402</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3402</id><created>2012-05-15</created><authors><author><keyname>Gast</keyname><forenames>Mikael</forenames></author><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author></authors><title>Efficient Parallel Computation of Nearest Neighbor Interchange Distances</title><categories>cs.DS cs.DC q-bio.PE</categories><comments>17 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nni-distance is a well-known distance measure for phylogenetic trees. We
construct an efficient parallel approximation algorithm for the nni-distance in
the CRCW-PRAM model running in O(log n) time on O(n) processors. Given two
phylogenetic trees T1 and T2 on the same set of taxa and with the same
multi-set of edge-weights, the algorithm constructs a sequence of
nni-operations of weight at most O(log n) \cdot opt, where opt denotes the
minimum weight of a sequence of nni-operations transforming T1 into T2 . This
algorithm is based on the sequential approximation algorithm for the
nni-distance given by DasGupta et al. (2000). Furthermore, we show that the
problem of identifying so called good edge-pairs between two weighted
phylogenies can be computed in O(log n) time on O(n log n) processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3407</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3407</id><created>2012-05-15</created><updated>2012-05-22</updated><authors><author><keyname>Koenig</keyname><forenames>Robert</forenames></author><author><keyname>Smith</keyname><forenames>Graeme</forenames></author></authors><title>Limits on classical communication from quantum entropy power
  inequalities</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v2: slightly rewritten, fixed typos, updated references, fixed
  figures. High-level description of proof with emphasis on applications to
  classical capacity. 10 pages, 4 figures</comments><journal-ref>Nature Photonics 7, 142-146 (2013)</journal-ref><doi>10.1038/nphoton.2012.342</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost all modern communication systems rely on electromagnetic fields as a
means of information transmission, and finding the capacities of these systems
is a problem of significant practical importance. The Additive White Gaussian
Noise (AWGN) channel is often a good approximate description of such systems,
and its capacity is given by a simple formula. However, when quantum effects
are important, estimating the capacity becomes difficult: a lower bound is
known, but a similar upper bound is missing. We present strong new upper bounds
for the classical capacity of quantum additive noise channels, including
quantum analogues of the AWGN channel. Our main technical tool is a quantum
entropy power inequality that controls the entropy production as two quantum
signals combine at a beam splitter. Its proof involves a new connection between
entropy production rates and a quantum Fisher information, and uses a quantum
diffusion that smooths arbitrary states towards gaussians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3409</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3409</id><created>2012-05-15</created><updated>2014-02-20</updated><authors><author><keyname>Koenig</keyname><forenames>Robert</forenames></author><author><keyname>Smith</keyname><forenames>Graeme</forenames></author></authors><title>The entropy power inequality for quantum systems</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>Mathematical exposition with detailed proofs. 14 pages. v2: updated
  to match published version</comments><journal-ref>IEEE Trans. Inf. Th., vol. 60, no. 3, p. 1536-1548 (2014)</journal-ref><doi>10.1109/TIT.2014.2298436</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When two independent analog signals, X and Y are added together giving Z=X+Y,
the entropy of Z, H(Z), is not a simple function of the entropies H(X) and
H(Y), but rather depends on the details of X and Y's distributions.
Nevertheless, the entropy power inequality (EPI), which states that exp [2H(Z)]
\geq exp[2H(X)] + exp[2H(Y)], gives a very tight restriction on the entropy of
Z. This inequality has found many applications in information theory and
statistics. The quantum analogue of adding two random variables is the
combination of two independent bosonic modes at a beam splitter. The purpose of
this work is to give a detailed outline of the proof of two separate
generalizations of the entropy power inequality to the quantum regime. Our
proofs are similar in spirit to standard classical proofs of the EPI, but some
new quantities and ideas are needed in the quantum setting. Specifically, we
find a new quantum de Bruijin identity relating entropy production under
diffusion to a divergence-based quantum Fisher information. Furthermore, this
Fisher information exhibits certain convexity properties in the context of beam
splitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3414</identifier>
 <datestamp>2013-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3414</id><created>2012-05-15</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Chowdhury</keyname><forenames>Muhammad F. I.</forenames></author><author><keyname>Lebreton</keyname><forenames>Romain</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author></authors><title>Power Series Solutions of Singular (q)-Differential Equations</title><categories>cs.SC</categories><comments>Final version to appear in proceedings ISSAC 2012</comments><journal-ref>Proceedings ISSAC'12, pages 107-114, 2012</journal-ref><doi>10.1145/2442829.2442847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide algorithms computing power series solutions of a large class of
differential or $q$-differential equations or systems. Their number of
arithmetic operations grows linearly with the precision, up to logarithmic
terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3423</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3423</id><created>2012-05-15</created><authors><author><keyname>Werner</keyname><forenames>Elisabeth M.</forenames></author></authors><title>f-Divergence for convex bodies</title><categories>math.FA cs.IT math.IT</categories><msc-class>52A20, 53A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce f-divergence, a concept from information theory and statistics,
for convex bodies in R^n. We prove that f-divergences are SL(n) invariant
valuations and we establish an affine isoperimetric inequality for these
quantities. We show that generalized affine surface area and in particular the
L_p affine surface area from the L_p Brunn Minkowski theory are special cases
of f-divergences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3426</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3426</id><created>2012-05-15</created><authors><author><keyname>Kim</keyname><forenames>Kyoung-Dae</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Bounded epsilon-Reach Set Computation of a Class of Deterministic and
  Transversal Linear Hybrid Automata</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a special class of hybrid automata, called Deterministic and
Transversal Linear Hybrid Automata (DTLHA), whose continuous dynamics in each
location are linear time-invariant (LTI) with a constant input, and for which
every discrete transition up to a given bounded time is deterministic and,
importantly, transversal. For such a DTLHA starting from an initial state, we
show that it is possible to compute an approximation of the reach set of a
DTLHA over a finite time interval that is arbitrarily close to the exact reach
set, called a bounded epsilon-reach set, through sampling and polyhedral
over-approximation of sampled states. We propose an algorithm and an attendant
architecture for the overall bounded epsilon-reach set computation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3441</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3441</id><created>2012-02-20</created><authors><author><keyname>Giot</keyname><forenames>Romain</forenames><affiliation>GREYC</affiliation></author><author><keyname>Rosenberger</keyname><forenames>Christophe</forenames><affiliation>GREYC</affiliation></author></authors><title>Genetic Programming for Multibiometrics</title><categories>cs.NE cs.CR cs.LG</categories><proxy>ccsd</proxy><journal-ref>Expert Systems with Applications 39, 2 1837-1847 (2012)</journal-ref><doi>10.1016/j.eswa.2011.08.066</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric systems suffer from some drawbacks: a biometric system can provide
in general good performances except with some individuals as its performance
depends highly on the quality of the capture. One solution to solve some of
these problems is to use multibiometrics where different biometric systems are
combined together (multiple captures of the same biometric modality, multiple
feature extraction algorithms, multiple biometric modalities...). In this
paper, we are interested in score level fusion functions application (i.e., we
use a multibiometric authentication scheme which accept or deny the claimant
for using an application). In the state of the art, the weighted sum of scores
(which is a linear classifier) and the use of an SVM (which is a non linear
classifier) provided by different biometric systems provide one of the best
performances. We present a new method based on the use of genetic programming
giving similar or better performances (depending on the complexity of the
database). We derive a score fusion function by assembling some classical
primitives functions (+, *, -, ...). We have validated the proposed method on
three significant biometric benchmark datasets from the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3445</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3445</id><created>2012-05-15</created><authors><author><keyname>Giraldo</keyname><forenames>Leidy Marcela</forenames></author><author><keyname>Villegas</keyname><forenames>Edward Yesid</forenames></author></authors><title>Optical Encryption with Jigsaw Transform using Matlab</title><categories>physics.comp-ph cs.CR physics.optics</categories><comments>4 pages, 11 figures, conference</comments><msc-class>78-06</msc-class><journal-ref>Proceedings of the 2009 International Conference on Security and
  Management, SAM 2009, 162-165</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article will describe an optical encryption technical of images which it
is proposed in an analogical and digital way. The development of the technical
to a digital level, it is made to implementing algorithms (routines) in MATLAB.
We will propose a functional diagram to the described analogical development
from which designated the optical systems associated with each functional
block. Level of security that the jigsaw algorithms provide applied on an
image, which has been decomposed into its bit-planes, is significantly better
if they are applied on an image that has not been previously decomposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3474</identifier>
 <datestamp>2012-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3474</id><created>2012-05-15</created><authors><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author></authors><title>Degrees-of-Freedom Region of the MISO Broadcast Channel with General
  Mixed-CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the setting of the two-user broadcast channel, recent work by Maddah-Ali
and Tse has shown that knowledge of prior channel state information at the
transmitter (CSIT) can be useful, even in the absence of any knowledge of
current CSIT. Very recent work by Kobayashi et al., Yang et al., and Gou and
Jafar, extended this to the case where, instead of no current CSIT knowledge,
the transmitter has partial knowledge, and where under a symmetry assumption,
the quality of this knowledge is identical for the different users' channels.
  Motivated by the fact that in multiuser settings, the quality of CSIT
feedback may vary across different links, we here generalize the above results
to the natural setting where the current CSIT quality varies for different
users' channels. For this setting we derive the optimal degrees-of-freedom
(DoF) region, and provide novel multi-phase broadcast schemes that achieve this
optimal region. Finally this generalization incorporates and generalizes the
corresponding result in Maleki et al. which considered the broadcast channel
with one user having perfect CSIT and the other only having prior CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3504</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3504</id><created>2012-05-15</created><authors><author><keyname>Ma</keyname><forenames>Zhanshan Sam</forenames></author></authors><title>A Note on Extending Taylor's Power Law for Characterizing Human
  Microbial Communities: Inspiration from Comparative Studies on the
  Distribution Patterns of Insects and Galaxies, and as a Case Study for
  Medical Ecology</title><categories>cs.CE q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many natural patterns, such as the distributions of blood particles in a
blood sample, proteins on cell surfaces, biological populations in their
habitat, galaxies in the universe, the sequence of human genes, and the fitness
in evolutionary computing, have been found to follow power law. Taylor's power
law (Taylor 1961: Nature, 189:732-) is well recognized as one of the
fundamental models in population ecology. A fundamental property of biological
populations, which Taylor's power law reveals, is the near universal
heterogeneity of population abundance distribution in habitat. Obviously, the
heterogeneity also exists at the community level, where not only the
distributions of population abundances but also the proportions of the species
composition in the community are often heterogeneous. Nevertheless, existing
community diversity indexes such as Shannon index and Simpson index can only
measure &quot;local&quot; or &quot;static&quot; diversity in the sense that they are computed for
each habitat at a specific time point, and the indexes alone do not reflect the
diversity changes. In this note, I propose to extend the application scope of
Taylor's power law to the studies of human microbial communities, specifically,
the community heterogeneity at both population and community levels. I further
suggested that population dispersion models such as Taylor (1980: Nature, 286,
53-), which is known to generate population distribution patterns consistent
with the power law, should also be very useful for analyzing the distribution
patterns of human microbes within the human body. Overall, I hope that the
approach to human microbial community with the power law offers an example that
ecological theories can play an important role in the emerging medical ecology,
which aims at studying the ecology of human microbiome and its implications to
human diseases and health, as well as in personalized medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3506</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3506</id><created>2012-05-15</created><authors><author><keyname>Phipps</keyname><forenames>Eric</forenames></author><author><keyname>Pawlowski</keyname><forenames>Roger</forenames></author></authors><title>Efficient Expression Templates for Operator Overloading-based Automatic
  Differentiation</title><categories>cs.MS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expression templates are a well-known set of techniques for improving the
efficiency of operator overloading-based forward mode automatic differentiation
schemes in the C++ programming language by translating the differentiation from
individual operators to whole expressions. However standard expression template
approaches result in a large amount of duplicate computation, particularly for
large expression trees, degrading their performance. In this paper we describe
several techniques for improving the efficiency of expression templates and
their implementation in the automatic differentiation package Sacado. We
demonstrate their improved efficiency through test functions as well as their
application to differentiation of a large-scale fluid dynamics simulation code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3518</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3518</id><created>2012-05-15</created><updated>2012-10-20</updated><authors><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Lower Bounds for Adaptive Sparse Recovery</title><categories>cs.DS</categories><comments>19 pages; appearing at SODA 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We give lower bounds for the problem of stable sparse recovery from
/adaptive/ linear measurements. In this problem, one would like to estimate a
vector $x \in \R^n$ from $m$ linear measurements $A_1x,..., A_mx$. One may
choose each vector $A_i$ based on $A_1x,..., A_{i-1}x$, and must output $x*$
satisfying |x* - x|_p \leq (1 + \epsilon) \min_{k\text{-sparse} x'} |x - x'|_p
with probability at least $1-\delta&gt;2/3$, for some $p \in \{1,2\}$. For $p=2$,
it was recently shown that this is possible with $m = O(\frac{1}{\epsilon}k
\log \log (n/k))$, while nonadaptively it requires $\Theta(\frac{1}{\epsilon}k
\log (n/k))$. It is also known that even adaptively, it takes $m =
\Omega(k/\epsilon)$ for $p = 2$. For $p = 1$, there is a non-adaptive upper
bound of $\tilde{O}(\frac{1}{\sqrt{\epsilon}} k\log n)$. We show:
  * For $p=2$, $m = \Omega(\log \log n)$. This is tight for $k = O(1)$ and
constant $\epsilon$, and shows that the $\log \log n$ dependence is correct.
  * If the measurement vectors are chosen in $R$ &quot;rounds&quot;, then $m = \Omega(R
\log^{1/R} n)$. For constant $\epsilon$, this matches the previously known
upper bound up to an O(1) factor in $R$.
  * For $p=1$, $m = \Omega(k/(\sqrt{\epsilon} \cdot \log k/\epsilon))$. This
shows that adaptivity cannot improve more than logarithmic factors, providing
the analog of the $m = \Omega(k/\epsilon)$ bound for $p = 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3534</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3534</id><created>2012-05-15</created><authors><author><keyname>Gopala</keyname><forenames>Parikshit</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Reingold</keyname><forenames>Omer</forenames></author></authors><title>DNF Sparsification and a Faster Deterministic Counting Algorithm</title><categories>cs.CC</categories><comments>To appear in the IEEE Conference on Computational Complexity, 2012</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a DNF formula on n variables, the two natural size measures are the
number of terms or size s(f), and the maximum width of a term w(f). It is
folklore that short DNF formulas can be made narrow. We prove a converse,
showing that narrow formulas can be sparsified. More precisely, any width w DNF
irrespective of its size can be $\epsilon$-approximated by a width $w$ DNF with
at most $(w\log(1/\epsilon))^{O(w)}$ terms.
  We combine our sparsification result with the work of Luby and Velikovic to
give a faster deterministic algorithm for approximately counting the number of
satisfying solutions to a DNF. Given a formula on n variables with poly(n)
terms, we give a deterministic $n^{\tilde{O}(\log \log(n))}$ time algorithm
that computes an additive $\epsilon$ approximation to the fraction of
satisfying assignments of f for $\epsilon = 1/\poly(\log n)$. The previous best
result due to Luby and Velickovic from nearly two decades ago had a run-time of
$n^{\exp(O(\sqrt{\log \log n}))}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3549</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3549</id><created>2012-05-15</created><updated>2012-05-16</updated><authors><author><keyname>Hirai</keyname><forenames>So</forenames></author><author><keyname>Yamanishi</keyname><forenames>Kenji</forenames></author></authors><title>Normalized Maximum Likelihood Coding for Exponential Family with Its
  Applications to Optimal Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are concerned with the issue of how to calculate the normalized maximum
likelihood (NML) code-length. There is a problem that the normalization term of
the NML code-length may diverge when it is continuous and unbounded and a
straightforward computation of it is highly expensive when the data domain is
finite . In previous works it has been investigated how to calculate the NML
code-length for specific types of distributions. We first propose a general
method for computing the NML code-length for the exponential family. Then we
specifically focus on Gaussian mixture model (GMM), and propose a new efficient
method for computing the NML to them. We develop it by generalizing Rissanen's
re-normalizing technique. Then we apply this method to the clustering issue, in
which a clustering structure is modeled using a GMM, and the main task is to
estimate the optimal number of clusters on the basis of the NML code-length. We
demonstrate using artificial data sets the superiority of the NML-based
clustering over other criteria such as AIC, BIC in terms of the data size
required for high accuracy rate to be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3554</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3554</id><created>2012-05-16</created><authors><author><keyname>Mahmoody</keyname><forenames>Mohammad</forenames></author><author><keyname>Maji</keyname><forenames>Hemanta K.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Manoj</forenames></author></authors><title>Limits of Random Oracles in Secure Computation</title><categories>cs.CR cs.CC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The seminal result of Impagliazzo and Rudich (STOC 1989) gave a black-box
separation between one-way functions and public-key encryption: informally, a
public-key encryption scheme cannot be constructed using one-way functions as
the sole source of computational hardness. In addition, this implied a
black-box separation between one-way functions and protocols for certain Secure
Function Evaluation (SFE) functionalities (in particular, Oblivious Transfer).
Surprisingly, however, {\em since then there has been no further progress in
separating one-way functions and SFE functionalities} (though several other
black-box separation results were shown). In this work, we present the complete
picture for deterministic 2-party SFE functionalities. We show that one-way
functions are black-box separated from {\em all such SFE functionalities},
except the ones which have unconditionally secure protocols (and hence do not
rely on any computational hardness), when secure computation against
semi-honest adversaries is considered. In the case of security against active
adversaries, a black-box one-way function is indeed useful for SFE, but we show
that it is useful only as much as access to an ideal commitment functionality
is useful.
  Technically, our main result establishes the limitations of random oracles
for secure computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3566</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3566</id><created>2012-05-16</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Risk-sensitive Dissipativity of Linear Quantum Stochastic Systems under
  Lur'e Type Perturbations of Hamiltonians</title><categories>quant-ph cs.SY math.DS math.OC math.PR</categories><comments>14 pages, submitted to AUCC 2012 on 7 May 2012</comments><msc-class>81Q15, 81Q93, 81S05, 81S22, 81S25, 93B28, 93C10, 93D09, 93E03, 93E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a stochastic dissipativity theory using
quadratic-exponential storage functions for open quantum systems with
canonically commuting dynamic variables governed by quantum stochastic
differential equations. The system is linearly coupled to external boson fields
and has a quadratic Hamiltonian which is perturbed by nonquadratic functions of
linear combinations of system variables. Such perturbations are similar to
those in the classical Lur'e systems and make the quantum dynamics nonlinear.
We study their effect on the quantum expectation of the exponential of a
positive definite quadratic form of the system variables. This allows
conditions to be established for the risk-sensitive stochastic storage function
of the quantum system to remain bounded, thus securing boundedness for the
moments of system variables of arbitrary order. These results employ a
noncommutative analogue of the Doleans-Dade exponential and a multivariate
partial differential version of the Gronwall-Bellman lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3569</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3569</id><created>2012-05-16</created><authors><author><keyname>van Schijndel</keyname><forenames>A. W. M.</forenames></author></authors><title>The Simulation and Mapping of Building Performance Indicators based on
  European Weather Stations</title><categories>physics.comp-ph cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the climate change debate, a lot of research and maps of external
climate parameters are available. However, maps of indoor climate performance
parameters are still lacking. This paper presents a methodology for obtaining
maps of performances of similar buildings that are virtually spread over whole
Europe. The produced maps are useful for analyzing regional climate influence
on building performance indicators such as energy use and indoor climate. This
is shown using the Bestest building as a reference benchmark. An important
application of the mapping tool is the visualization of potential building
measures over the EU. Also the performances of single building components can
be simulated and mapped. It is concluded that the presented method is efficient
as it takes less than 15 minutes to simulate and produce the maps on a
2.6GHz/4GB computer. Moreover, the approach is applicable for any type of
building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3570</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3570</id><created>2012-05-16</created><authors><author><keyname>Cherian</keyname><forenames>Mary</forenames></author><author><keyname>Nair</keyname><forenames>T. R Gopalakrishnan</forenames></author></authors><title>A QoS-Aware Routing Protocol for Real-time Applications in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>10 pages. arXiv admin note: text overlap with arXiv:1001.5339 by
  other authors</comments><journal-ref>Innovative Systems Design and Engineering, ISSN 2222-1727
  (Paper),ISSN 2222-2871 (Online), Vol 2, No 7, 2011 pp. 84-93</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a quality of service aware routing protocol which provides
low latency for high priority packets. Packets are differentiated based on
their priority by applying queuing theory. Low priority packets are transferred
through less energy paths. The sensor nodes interact with the pivot nodes which
in turn communicate with the sink node. This protocol can be applied in
monitoring context aware physical environments for critical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3576</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3576</id><created>2012-05-16</created><updated>2013-01-31</updated><authors><author><keyname>Bartel</keyname><forenames>Alexandre</forenames><affiliation>SnT</affiliation></author><author><keyname>Klein</keyname><forenames>Jacques</forenames><affiliation>SnT</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>SnT</affiliation></author></authors><title>Dexpler: Converting Android Dalvik Bytecode to Jimple for Static
  Analysis with Soot</title><categories>cs.SE</categories><comments>ACM SIGPLAN International Workshop on the State Of the Art in Java
  Program Analysis(SOAP 2012), Beijing : China (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Dexpler, a software package which converts Dalvik
bytecode to Jimple. Dexpler is built on top of Dedexer and Soot. As Jimple is
Soot's main internal rep- resentation of code, the Dalvik bytecode can be
manipu- lated with any Jimple based tool, for instance for performing point-to
or flow analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3588</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3588</id><created>2012-05-16</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Uncertainties and Ambiguities in Percentiles and how to Avoid Them</title><categories>stat.OT cs.DL physics.soc-ph</categories><comments>6 pages, 2 figures, submitted to Journal of the American Society for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed fractional scoring scheme is used to attribute
publications to percentile rank classes. It is shown that in this way
uncertainties and ambiguities in the evaluation of percentile ranks do not
occur. Using the fractional scoring the total score of all papers exactly
reproduces the theoretical value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3594</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3594</id><created>2012-05-16</created><authors><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Peleg</keyname><forenames>Yitzhak</forenames></author></authors><title>A Mean-field Approach for an Intercarrier Interference Canceller for
  OFDM</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT</categories><comments>7pages, 3figures</comments><doi>10.1088/1742-5468/2012/07/P07006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The similarity of the mathematical description of random-field spin systems
to orthogonal frequency-division multiplexing (OFDM) scheme for wireless
communication is exploited in an intercarrier-interference (ICI) canceller used
in the demodulation of OFDM. The translational symmetry in the Fourier domain
generically concentrates the major contribution of ICI from each subcarrier in
the subcarrier's neighborhood. This observation in conjunction with mean field
approach leads to a development of an ICI canceller whose necessary cost of
computation scales linearly with respect to the number of subcarriers. It is
also shown that the dynamics of the mean-field canceller are well captured by a
discrete map of a single macroscopic variable, without taking the spatial and
time correlations of estimated variables into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3597</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3597</id><created>2012-05-16</created><authors><author><keyname>Suslo</keyname><forenames>Tomasz</forenames></author></authors><title>The Correct Classic Generalized Least-Squares Estimator of an Unknown
  Constant Mean of Randon Field</title><categories>cs.NA</categories><comments>3 figures, 5 pages, attached source code and input data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the paper is to derive for the negative correlation function with
a time parameter an asymptotic disjunction of the numerical generalized
least-squares estimator of an unknown constant mean of random field in fact the
correct classic generalized least-squares estimator of an unknown constant mean
of the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3605</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3605</id><created>2012-05-16</created><authors><author><keyname>Grandoni</keyname><forenames>Fabrizio</forenames></author></authors><title>On Min-Power Steiner Tree</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical (min-cost) Steiner tree problem, we are given an
edge-weighted undirected graph and a set of terminal nodes. The goal is to
compute a min-cost tree S which spans all terminals. In this paper we consider
the min-power version of the problem, which is better suited for wireless
applications. Here, the goal is to minimize the total power consumption of
nodes, where the power of a node v is the maximum cost of any edge of S
incident to v. Intuitively, nodes are antennas (part of which are terminals
that we need to connect) and edge costs define the power to connect their
endpoints via bidirectional links (so as to support protocols with ack
messages). Differently from its min-cost counterpart, min-power Steiner tree is
NP-hard even in the spanning tree case, i.e. when all nodes are terminals.
Since the power of any tree is within once and twice its cost, computing a rho
\leq ln(4)+eps [Byrka et al.'10] approximate min-cost Steiner tree provides a
2rho&lt;2.78 approximation for the problem. For min-power spanning tree the same
approach provides a 2 approximation, which was improved to 5/3+eps with a
non-trivial approach in [Althaus et al.'06]. Here we present an improved
approximation algorithm for min-power Steiner tree. Our result is based on two
main ingredients. We prove the first decomposition theorem for min-power
Steiner tree, in the spirit of analogous structural results for min-cost
Steiner tree and min-power spanning tree. Based on this theorem, we define a
proper LP relaxation, that we exploit within the iterative randomized rounding
framework in [Byrka et al.'10]. A careful analysis provides a 3ln
4-9/4+eps&lt;1.91 approximation factor. The same approach gives an improved
1.5+eps approximation for min-power spanning tree as well, matching the
approximation factor in [Nutov and Yaroshevitch'09] for the special case of
min-power spanning tree with edge weights in {0,1}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3612</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3612</id><created>2012-05-16</created><updated>2012-06-19</updated><authors><author><keyname>Pous</keyname><forenames>Damien</forenames><affiliation>CNRS</affiliation></author></authors><title>Untyping Typed Algebras and Colouring Cyclic Linear Logic</title><categories>cs.LO</categories><comments>21p</comments><proxy>LMCS</proxy><acm-class>F.4.1, F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 20,
  2012) lmcs:718</journal-ref><doi>10.2168/LMCS-8(2:13)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove &quot;untyping&quot; theorems: in some typed theories (semirings, Kleene
algebras, residuated lattices, involutive residuated lattices), typed equations
can be derived from the underlying untyped equations. As a consequence, the
corresponding untyped decision procedures can be extended for free to the typed
settings. Some of these theorems are obtained via a detour through fragments of
cyclic linear logic, and give rise to a substantial optimisation of standard
proof search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3630</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3630</id><created>2012-05-16</created><updated>2012-11-05</updated><authors><author><keyname>Michoel</keyname><forenames>Tom</forenames></author><author><keyname>Nachtergaele</keyname><forenames>Bruno</forenames></author></authors><title>Alignment and integration of complex networks by hypergraph-based
  spectral clustering</title><categories>physics.soc-ph cs.SI q-bio.MN q-bio.QM</categories><comments>16 pages, 5 figures; revised version with minor corrections and
  figures printed in two-column format for better readability; algorithm
  implementation and supplementary information available at Google code at
  http://schype.googlecode.com</comments><journal-ref>Phys. Rev. E 86, 056111 (2012)</journal-ref><doi>10.1103/PhysRevE.86.056111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks possess a rich, multi-scale structure reflecting the
dynamical and functional organization of the systems they model. Often there is
a need to analyze multiple networks simultaneously, to model a system by more
than one type of interaction or to go beyond simple pairwise interactions, but
currently there is a lack of theoretical and computational methods to address
these problems. Here we introduce a framework for clustering and community
detection in such systems using hypergraph representations. Our main result is
a generalization of the Perron-Frobenius theorem from which we derive spectral
clustering algorithms for directed and undirected hypergraphs. We illustrate
our approach with applications for local and global alignment of
protein-protein interaction networks between multiple species, for tripartite
community detection in folksonomies, and for detecting clusters of overlapping
regulatory pathways in directed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3642</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3642</id><created>2012-05-16</created><authors><author><keyname>Monga</keyname><forenames>Ana</forenames></author><author><keyname>Singh</keyname><forenames>Balwinder</forenames></author></authors><title>Finite State Machine based Vending Machine Controller with Auto-Billing
  Features</title><categories>cs.OH</categories><journal-ref>International Journal on VLSI and Communication System,April 2012
  pp 19-28</journal-ref><doi>10.5121/vlsic.2012.3202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, Vending Machines are well known among Japan, Malaysia and
Singapore. The quantity of machines in these countries is on the top worldwide.
This is due to the modern lifestyles which require fast food processing with
high quality. This paper describes the designing of multi select machine using
Finite State Machine Model with Auto-Billing Features. Finite State Machine
(FSM) modelling is the most crucial part in developing proposed model as this
reduces the hardware. In this paper the process of four state (user Selection,
Waiting for money insertion, product delivery and servicing) has been modelled
using MEALY Machine Model. The proposed model is tested using Spartan 3
development board and its performance is compared with CMOS based machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3643</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3643</id><created>2012-05-16</created><authors><author><keyname>Datta</keyname><forenames>Samik</forenames></author><author><keyname>Majumder</keyname><forenames>Anirban</forenames></author><author><keyname>Naidu</keyname><forenames>KVM</forenames></author></authors><title>Capacitated Team Formation Problem on Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a team formation problem, one is required to find a group of users that
can match the requirements of a collaborative task. Example of such
collaborative tasks abound, ranging from software product development to
various participatory sensing tasks in knowledge creation. Due to the nature of
the task, team members are often required to work on a co-operative basis.
Previous studies have indicated that co-operation becomes effective in presence
of social connections. Therefore, effective team selection requires the team
members to be socially close as well as a division of the task among team
members so that no user is overloaded by the assignment. In this work, we
investigate how such teams can be formed on a social network.
  Since our team formation problems are proven to be NP-hard, we design
efficient approximate algorithms for finding near optimum teams with provable
guarantees. As traditional data-sets from on-line social networks (e.g.
Twitter, Facebook etc) typically do not contain instances of large scale
collaboration, we have crawled millions of software repositories spanning a
period of four years and hundreds of thousands of developers from GitHub, a
popular open-source social coding network. We perform large scale experiments
on this data-set to evaluate the accuracy and efficiency of our algorithms.
Experimental results suggest that our algorithms achieve significant
improvement in finding effective teams, as compared to naive strategies and
scale well with the size of the data. Finally, we provide a validation of our
techniques by comparing with existing software teams in GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3647</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3647</id><created>2012-05-16</created><authors><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>Irene</forenames></author><author><keyname>Pellikaan</keyname><forenames>Ruud</forenames></author></authors><title>Error-correcting pairs for a public-key cryptosystem</title><categories>cs.IT math.IT</categories><msc-class>11T71, 94A60, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code-based cryptography is an interesting alternative to classic
number-theory PKC since it is conjectured to be secure against quantum computer
attacks. Many families of codes have been proposed for these cryptosystems, one
of the main requirements is having high performance t-bounded decoding
algorithms which in the case of having high an error-correcting pair is
achieved. In this article the class of codes with a t-ECP is proposed for the
McEliece cryptosystem. The hardness of retrieving the t-ECP for a given code is
considered. As a first step distinguishers of several subclasses are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3655</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3655</id><created>2012-05-16</created><updated>2012-11-19</updated><authors><author><keyname>Furones</keyname><forenames>Asia</forenames></author></authors><title>P versus UP</title><categories>cs.CC</categories><comments>Administratively withdrawn due to policy violations</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Admin note: withdrawn by arXiv admin because of the use of a pseudonym, in
violation of arXiv policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3663</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3663</id><created>2012-02-15</created><authors><author><keyname>Fichte</keyname><forenames>Johannes Klaus</forenames></author></authors><title>The Good, the Bad, and the Odd: Cycles in Answer-Set Programs</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backdoors of answer-set programs are sets of atoms that represent clever
reasoning shortcuts through the search space. Assignments to backdoor atoms
reduce the given program to several programs that belong to a tractable target
class. Previous research has considered target classes based on notions of
acyclicity where various types of cycles (good and bad cycles) are excluded
from graph representations of programs. We generalize the target classes by
taking the parity of the number of negative edges on bad cycles into account
and consider backdoors for such classes. We establish new hardness results and
non-uniform polynomial-time tractability relative to directed or undirected
cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3668</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3668</id><created>2012-05-16</created><authors><author><keyname>Alessandro</keyname><forenames>Cristiano</forenames></author><author><keyname>Carbajal</keyname><forenames>Juan Pablo</forenames></author><author><keyname>d'Avella</keyname><forenames>Andrea</forenames></author></authors><title>Synthesis and Adaptation of Effective Motor Synergies for the Solution
  of Reaching Tasks</title><categories>cs.RO cs.SY nlin.AO physics.comp-ph</categories><comments>conference paper</comments><doi>10.1007/978-3-642-33093-3_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking inspiration from the hypothesis of muscle synergies, we propose a
method to generate open loop controllers for an agent solving point-to-point
reaching tasks. The controller output is defined as a linear combination of a
small set of predefined actuations, termed synergies. The method can be
interpreted from a developmental perspective, since it allows the agent to
autonomously synthesize and adapt an effective set of synergies to new
behavioral needs. This scheme greatly reduces the dimensionality of the control
problem, while keeping a good performance level. The framework is evaluated in
a planar kinematic chain, and the quality of the solutions is quantified in
several scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3669</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3669</id><created>2012-05-16</created><updated>2014-01-08</updated><authors><author><keyname>Bubenik</keyname><forenames>Peter</forenames></author><author><keyname>Scott</keyname><forenames>Jonathan A.</forenames></author></authors><title>Categorification of persistent homology</title><categories>math.AT cs.CG math.CT</categories><comments>27 pages, v3: minor changes, to appear in Discrete &amp; Computational
  Geometry</comments><msc-class>55N99, 68W30, 18A25, 18E10, 54E35</msc-class><journal-ref>Discrete Comput. Geom. 51 (2014) 600-627</journal-ref><doi>10.1007/s00454-014-9573-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We redevelop persistent homology (topological persistence) from a categorical
point of view. The main objects of study are diagrams, indexed by the poset of
real numbers, in some target category. The set of such diagrams has an
interleaving distance, which we show generalizes the previously-studied
bottleneck distance. To illustrate the utility of this approach, we greatly
generalize previous stability results for persistence, extended persistence,
and kernel, image and cokernel persistence. We give a natural construction of a
category of interleavings of these diagrams, and show that if the target
category is abelian, so is this category of interleavings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3676</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3676</id><created>2012-02-22</created><updated>2012-06-20</updated><authors><author><keyname>LeBlanc</keyname><forenames>Heath J.</forenames></author><author><keyname>Zhang</keyname><forenames>Haotian</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author><author><keyname>Koutsoukos</keyname><forenames>Xenofon</forenames></author></authors><title>Consensus of Multi-Agent Networks in the Presence of Adversaries Using
  Only Local Information</title><categories>cs.DC cs.SY</categories><comments>This report contains the proofs of the results presented at HiCoNS
  2012</comments><acm-class>C.2.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of resilient consensus in the presence of
misbehaving nodes. Although it is typical to assume knowledge of at least some
nonlocal information when studying secure and fault-tolerant consensus
algorithms, this assumption is not suitable for large-scale dynamic networks.
To remedy this, we emphasize the use of local strategies to deal with
resilience to security breaches. We study a consensus protocol that uses only
local information and we consider worst-case security breaches, where the
compromised nodes have full knowledge of the network and the intentions of the
other nodes. We provide necessary and sufficient conditions for the normal
nodes to reach consensus despite the influence of the malicious nodes under
different threat assumptions. These conditions are stated in terms of a novel
graph-theoretic property referred to as network robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3720</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3720</id><created>2012-05-16</created><updated>2012-08-27</updated><authors><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author></authors><title>A k-shell decomposition method for weighted networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 6 figures</comments><journal-ref>New J. Phys. 14 083030 (2012)</journal-ref><doi>10.1088/1367-2630/14/8/083030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalized method for calculating the k-shell structure of
weighted networks. The method takes into account both the weight and the degree
of a network, in such a way that in the absence of weights we resume the shell
structure obtained by the classic k-shell decomposition. In the presence of
weights, we show that the method is able to partition the network in a more
refined way, without the need of any arbitrary threshold on the weight values.
Furthermore, by simulating spreading processes using the
susceptible-infectious-recovered model in four different weighted real-world
networks, we show that the weighted k-shell decomposition method ranks the
nodes more accurately, by placing nodes with higher spreading potential into
shells closer to the core. In addition, we demonstrate our new method on a real
economic network and show that the core calculated using the weighted k-shell
method is more meaningful from an economic perspective when compared with the
unweighted one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3727</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3727</id><created>2012-05-16</created><authors><author><keyname>Hervier</keyname><forenames>Thibault</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>Goulette</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Accurate 3D maps from depth images and motion sensors via nonlinear
  Kalman filtering</title><categories>cs.RO</categories><comments>Submitted to IROS 2012. 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the use of depth images as localisation sensors for
3D map building. The localisation information is derived from the 3D data
thanks to the ICP (Iterative Closest Point) algorithm. The covariance of the
ICP, and thus of the localization error, is analysed, and described by a Fisher
Information Matrix. It is advocated this error can be much reduced if the data
is fused with measurements from other motion sensors, or even with prior
knowledge on the motion. The data fusion is performed by a recently introduced
specific extended Kalman filter, the so-called Invariant EKF, and is directly
based on the estimated covariance of the ICP. The resulting filter is very
natural, and is proved to possess strong properties. Experiments with a Kinect
sensor and a three-axis gyroscope prove clear improvement in the accuracy of
the localization, and thus in the accuracy of the built 3D map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3728</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3728</id><created>2012-05-16</created><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>Parameterized Domination in Circle Graphs</title><categories>cs.DS cs.CC cs.DM</categories><msc-class>05C85, 05C10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A circle graph is the intersection graph of a set of chords in a circle. Keil
[Discrete Applied Mathematics, 42(1):51-63, 1993] proved that Dominating Set,
Connected Dominating Set, and Total Dominating Set are NP-complete in circle
graphs. To the best of our knowledge, nothing was known about the parameterized
complexity of these problems in circle graphs. In this paper we prove the
following results, which contribute in this direction:
  - Dominating Set, Independent Dominating Set, Connected Dominating Set, Total
Dominating Set, and Acyclic Dominating Set are W[1]-hard in circle graphs,
parameterized by the size of the solution.
  - Whereas both Connected Dominating Set and Acyclic Dominating Set are
W[1]-hard in circle graphs, it turns out that Connected Acyclic Dominating Set
is polynomial-time solvable in circle graphs.
  - If T is a given tree, deciding whether a circle graph has a dominating set
isomorphic to T is NP-complete when T is in the input, and FPT when
parameterized by |V(T)|. We prove that the FPT algorithm is subexponential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3736</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3736</id><created>2012-05-16</created><updated>2012-09-04</updated><authors><author><keyname>Arnon-Friedman</keyname><forenames>Rotem</forenames></author><author><keyname>H&#xe4;nggi</keyname><forenames>Esther</forenames></author><author><keyname>Ta-Shma</keyname><forenames>Amnon</forenames></author></authors><title>Towards the Impossibility of Non-Signalling Privacy Amplification from
  Time-Like Ordering Constraints</title><categories>quant-ph cs.CR</categories><comments>23 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years there was a growing interest in proving the security of
cryptographic protocols, such as key distribution protocols, from the sole
assumption that the systems of Alice and Bob cannot signal to each other. This
can be achieved by making sure that Alice and Bob perform their measurements in
a space-like separated way (and therefore signalling is impossible according to
the non-signalling postulate of relativity theory) or even by shielding their
apparatus. Unfortunately, it was proven in [E. Haenggi, R. Renner, and S. Wolf.
The impossibility of non-signaling privacy amplification] that, no matter what
hash function we use, privacy amplification is impossible if we only impose
non-signalling conditions between Alice and Bob and not within their systems.
In this letter we reduce the gap between the assumptions of Haenggi et al. and
the physical relevant assumptions, from an experimental point of view, which
say that the systems can only signal forward in time within the systems of
Alice and Bob. We consider a set of assumptions which is very close to the
conditions above and prove that the impossibility result of Haenggi et al.
still holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3752</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3752</id><created>2012-03-02</created><authors><author><keyname>Herrera</keyname><forenames>Roberto H.</forenames></author><author><keyname>van der Baan</keyname><forenames>Mirko</forenames></author></authors><title>Revisiting Homomorphic Wavelet Estimation and Phase Unwrapping</title><categories>cs.IT math.IT physics.geo-ph</categories><comments>4 pages, Conference: Recovery - 2011 CSPG CSEG CWLS Convention</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surface-consistent deconvolution is a standard processing technique in land
data to uniformize the wavelet across all sources and receivers. The required
wavelet estimation step is generally done in the homomorphic domain since this
is a convenient way to separate the phase and the amplitude spectrum in a
linear fashion. Unfortunately all surface-consistent deconvolutions make a
minimum-phase assumption which is likely to be sub-optimal. Recent developments
in statistical wavelet estimation demonstrate that nonminimum wavelets can be
estimated directly from seismic data, thus offering promise to create a
nonminimum phase surface-consistent deconvolution approach. Unfortunately the
major impediment turns out to be phase unwrapping. In this paper we review
several existing phase unwrapping techniques and discuss their advantages and
inconveniences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3753</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3753</id><created>2012-02-26</created><authors><author><keyname>Herrera</keyname><forenames>Roberto H.</forenames></author><author><keyname>Moreno</keyname><forenames>Eduardo</forenames></author><author><keyname>Calas</keyname><forenames>H&#xe9;ctor</forenames></author><author><keyname>Orozco</keyname><forenames>Rub&#xe9;n</forenames></author></authors><title>Blind Deconvolution of Ultrasonic Signals Using High-Order Spectral
  Analysis and Wavelets</title><categories>cs.IT math.IT</categories><comments>8 pages, CIARP 2005, LNCS 3773</comments><journal-ref>LNCS 3773: 663 -- 670 (2005)</journal-ref><doi>10.1007/11578079_69</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defect detection by ultrasonic method is limited by the pulse width.
Resolution can be improved through a deconvolution process with a priori
information of the pulse or by its estimation. In this paper a regularization
of the Wiener filter using wavelet shrinkage is presented for the estimation of
the reflectivity function. The final result shows an improved signal to noise
ratio with better axial resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3754</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3754</id><created>2012-02-23</created><authors><author><keyname>Marikkannan</keyname><forenames>Sangeetha</forenames></author></authors><title>Scheduling and allocation algorithm for an elliptic filter</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new evolutionary algorithm for scheduling and allocation algorithm is
developed for an elliptic filter. The elliptic filter is scheduled and
allocated in the proposed work which is then compared with the different
scheduling algorithms like As Soon As Possible algorithm, As Late As Possible
algorithm, Mobility Based Shift algorithm, FDLS, FDS and MOGS. In this paper
execution time and resource utilization is calculated using different
scheduling algorithm for an Elliptic Filter and reported that proposed
Scheduling and Allocation increases the speed of operation by reducing the
control step. The proposed work to analyse the magnitude, phase and noise
responses for different scheduling algorithm in an elliptic filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3756</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3756</id><created>2012-05-16</created><updated>2012-08-27</updated><authors><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Dupuis</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Achieving the Capacity of any DMC using only Polar Codes</title><categories>cs.IT math.IT</categories><comments>9 pages, 7 figures</comments><doi>10.1109/ITW.2012.6404638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a channel coding scheme to achieve the capacity of any discrete
memoryless channel based solely on the techniques of polar coding. In
particular, we show how source polarization and randomness extraction via
polarization can be employed to &quot;shape&quot; uniformly-distributed i.i.d. random
variables into approximate i.i.d. random variables distributed ac- cording to
the capacity-achieving distribution. We then combine this shaper with a variant
of polar channel coding, constructed by the duality with source coding, to
achieve the channel capacity. Our scheme inherits the low complexity encoder
and decoder of polar coding. It differs conceptually from Gallager's method for
achieving capacity, and we discuss the advantages and disadvantages of the two
schemes. An application to the AWGN channel is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3757</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3757</id><created>2012-05-16</created><updated>2012-11-22</updated><authors><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author></authors><title>A reduced integer programming model for the ferry scheduling problem</title><categories>cs.DM math.OC</categories><comments>To appear in Public Transport</comments><journal-ref>Public Transport 4(3), 2013, 151-163</journal-ref><doi>10.1007/s12469-012-0058-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an integer programming model for the ferry scheduling problem,
improving existing models in various ways. In particular, our model has reduced
size in terms of the number of variables and constraints compared to existing
models by a factor of approximately O(n), where n being the number of ports.
The model also handles efficiently load/unload time constraints, crew
scheduling and passenger transfers. Experiments using real world data produced
high quality solutions in 12 hours using CPLEX 12.4 with a performance
guarantee of within 15% of optimality, on average. This establishes that using
a general purpose integer programming solver is a viable alternative in solving
the ferry scheduling problem of moderate size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3758</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3758</id><created>2012-05-16</created><authors><author><keyname>Garoche</keyname><forenames>Pierre-Lo&#xef;c</forenames></author><author><keyname>Kahsai</keyname><forenames>Temesghen</forenames></author><author><keyname>Tinelli</keyname><forenames>Cesare</forenames></author></authors><title>Invariant stream generators using automatic abstract transformers based
  on a decidable logic</title><categories>cs.LO cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of formal analysis tools on models or source code often requires the
availability of auxiliary invariants about the studied system. Abstract
interpretation is currently one of the best approaches to discover useful
invariants, especially numerical ones. However, its application is limited by
two orthogonal issues: (i) developing an abstract interpretation is often
non-trivial; each transfer function of the system has to be represented at the
abstract level, depending on the abstract domain used; (ii) with precise but
costly abstract domains, the information computed by the abstract interpreter
can be used only once a post fix point has been reached; something that may
take a long time for very large system analysis or with delayed widening to
improve precision.
  This paper proposes a new, completely automatic, method to build abstract
interpreters. One of its nice features is that its produced interpreters can
provide sound invariants of the analyzed system before reaching the end of the
post fix point computation, and so act as on-the-fly invariant generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3766</identifier>
 <datestamp>2012-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3766</id><created>2012-05-16</created><authors><author><keyname>Chang</keyname><forenames>Jason</forenames></author><author><keyname>Fisher</keyname><forenames>John W.</forenames><suffix>III</suffix></author></authors><title>Efficient Topology-Controlled Sampling of Implicit Shapes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling from distributions of implicitly defined shapes enables analysis of
various energy functionals used for image segmentation. Recent work describes a
computationally efficient Metropolis-Hastings method for accomplishing this
task. Here, we extend that framework so that samples are accepted at every
iteration of the sampler, achieving an order of magnitude speed up in
convergence. Additionally, we show how to incorporate topological constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3767</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3767</id><created>2012-05-16</created><updated>2014-11-03</updated><authors><author><keyname>V'yugin</keyname><forenames>Vladimir</forenames></author><author><keyname>Trunov</keyname><forenames>Vladimir</forenames></author></authors><title>Universal Algorithm for Online Trading Based on the Method of
  Calibration</title><categories>cs.LG q-fin.PM</categories><comments>32 pages. arXiv admin note: substantial text overlap with
  arXiv:1105.4272</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a universal algorithm for online trading in Stock Market which
performs asymptotically at least as good as any stationary trading strategy
that computes the investment at each step using a fixed function of the side
information that belongs to a given RKHS (Reproducing Kernel Hilbert Space).
Using a universal kernel, we extend this result for any continuous stationary
strategy. In this learning process, a trader rationally chooses his gambles
using predictions made by a randomized well-calibrated algorithm. Our strategy
is based on Dawid's notion of calibration with more general checking rules and
on some modification of Kakade and Foster's randomized rounding algorithm for
computing the well-calibrated forecasts. We combine the method of randomized
calibration with Vovk's method of defensive forecasting in RKHS. Unlike the
statistical theory, no stochastic assumptions are made about the stock prices.
Our empirical results on historical markets provide strong evidence that this
type of technical trading can &quot;beat the market&quot; if transaction costs are
ignored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3776</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3776</id><created>2012-05-16</created><authors><author><keyname>Aholt</keyname><forenames>Chris</forenames></author><author><keyname>Oeding</keyname><forenames>Luke</forenames></author></authors><title>The ideal of the trifocal variety</title><categories>math.AG cs.CV</categories><doi>10.1090/S0025-5718-2014-02842-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques from representation theory, symbolic computational algebra, and
numerical algebraic geometry are used to find the minimal generators of the
ideal of the trifocal variety. An effective test for determining whether a
given tensor is a trifocal tensor is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3797</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3797</id><created>2012-05-16</created><authors><author><keyname>Ferner</keyname><forenames>Ulric J.</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author></authors><title>Toward Sustainable Networking: Storage Area Networks with Network Coding</title><categories>cs.DC</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript provides a model to characterize the energy savings of
network coded storage (NCS) in storage area networks (SANs). We consider
blocking probability of drives as our measure of performance. A mapping
technique to analyze SANs as independent M/G/K/K queues is presented, and
blocking probabilities for uncoded storage schemes and NCS are derived and
compared. We show that coding operates differently than the amalgamation of
file chunks and energy savings are shown to scale well with striping number. We
illustrate that for enterprise-level SANs energy savings of 20-50% can be
realized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3803</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3803</id><created>2012-05-16</created><updated>2012-06-03</updated><authors><author><keyname>Lewitzka</keyname><forenames>Steffen</forenames></author></authors><title>Necessity as justified truth</title><categories>cs.LO</categories><comments>36 pages; corrected typos from the first version; content and results
  remain completely unchanged</comments><msc-class>03B45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a logic for the reasoning about necessity and justifications which
is independent from relational semantics. We choose the concept of
justification -- coming from a class of &quot;Justification Logics&quot; (Artemov 2008,
Fitting 2009) -- as the primitive notion on which the concept of necessity is
based. Our axiomatization extends Suszko's non-Fregean logic SCI (Brown, Suszko
1972) by basic axioms from Justification Logic, axioms for quantification over
propositions and over justifications, and some further principles. The core
axiom is: $\varphi$ is necessarily true iff there is a justification for
$\varphi$. That is, necessity is first-order definable by means of
justifications. Instead of defining purely algebraic models in the style of
(Brown, Suszko 1972) we extend the semantics investigated in (Lewitzka 2012) by
some algebraic structure for dealing with justifications and prove soundness
and completeness of our deductive system. Moreover, we are able to restore the
modal logic principle of Necessitation if we add the axiom schema
$\square\varphi \to \square\square\varphi$ and a rule of Axiom Necessitation to
our system. As a main result, we show that the modal logics S4 and S5 can be
captured by our semantics if we impose the corresponding modal logic principles
as additional semantic constraints. This will follow from proof-theoretic
considerations and from our completeness theorems. For the system S4 we present
also a purely model-theoretic proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3809</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3809</id><created>2012-05-16</created><authors><author><keyname>Catalyurek</keyname><forenames>Umit</forenames></author><author><keyname>Feo</keyname><forenames>John</forenames></author><author><keyname>Gebremedhin</keyname><forenames>Assefaw</forenames></author><author><keyname>Halappanavar</keyname><forenames>Mahantesh</forenames></author><author><keyname>Pothen</keyname><forenames>Alex</forenames></author></authors><title>Graph Coloring Algorithms for Muti-core and Massively Multithreaded
  Architectures</title><categories>cs.DC</categories><comments>25 pages, 11 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the interplay between architectures and algorithm design in the
context of shared-memory platforms and a specific graph problem of central
importance in scientific and high-performance computing, distance-1 graph
coloring. We introduce two different kinds of multithreaded heuristic
algorithms for the stated, NP-hard, problem. The first algorithm relies on
speculation and iteration, and is suitable for any shared-memory system. The
second algorithm uses dataflow principles, and is targeted at the
non-conventional, massively multithreaded Cray XMT system. We study the
performance of the algorithms on the Cray XMT and two multi-core systems, Sun
Niagara 2 and Intel Nehalem. Together, the three systems represent a spectrum
of multithreading capabilities and memory structure. As testbed, we use
synthetically generated large-scale graphs carefully chosen to cover a wide
range of input types. The results show that the algorithms have scalable
runtime performance and use nearly the same number of colors as the underlying
serial algorithm, which in turn is effective in practice. The study provides
insight into the design of high performance algorithms for irregular problems
on many-core architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3813</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3813</id><created>2012-05-16</created><updated>2012-11-29</updated><authors><author><keyname>Apon</keyname><forenames>Daniel</forenames></author><author><keyname>Gasarch</keyname><forenames>William</forenames></author><author><keyname>Lawler</keyname><forenames>Kevin</forenames></author></authors><title>An NP-Complete Problem in Grid Coloring</title><categories>cs.CC</categories><comments>25 pages</comments><msc-class>68Q17, 03F20</msc-class><acm-class>F.2.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A c-coloring of G(n,m)=n x m is a mapping of G(n,m) into {1,...,c} such that
no four corners forming a rectangle have the same color. In 2009 a challenge
was proposed via the internet to find a 4-coloring of G(17,17). This attracted
considerable attention from the popular mathematics community. A coloring was
produced; however, finding it proved to be difficult. The question arises: is
the problem of grid coloring is difficult in general? We present three results
that support this conjecture, (1) an NP completeness result, (2) a lower bound
on Tree-resolution, (3) a lower bound on Tree-CP proofs. Note that items (2)
and (3) yield statements from Ramsey Theory which are of size polynomial in
their parameters and require exponential size in various proof systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3816</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3816</id><created>2012-05-16</created><updated>2013-03-24</updated><authors><author><keyname>Kita</keyname><forenames>Nanao</forenames></author></authors><title>A Partially Ordered Structure and a Generalization of the Canonical
  Partition for General Graphs with Perfect Matchings</title><categories>cs.DM math.CO</categories><comments>21 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with structures of general graphs with perfect
matchings. We first reveal a partially ordered structure among
factor-components of general graphs with perfect matchings. Our second result
is a generalization of Kotzig's canonical partition to a decomposition of
general graphs with perfect matchings. It contains a short proof for the
theorem of the canonical partition. These results give decompositions which are
canonical, that is, unique to given graphs. We also show that there are
correlations between these two and that these can be computed in polynomial
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3830</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3830</id><created>2012-05-16</created><updated>2014-04-01</updated><authors><author><keyname>Lucas</keyname><forenames>Andrew</forenames></author><author><keyname>Stalzer</keyname><forenames>Mark</forenames></author><author><keyname>Feo</keyname><forenames>John</forenames></author></authors><title>Parallel implementation of fast randomized algorithms for the
  decomposition of low rank matrices</title><categories>cs.DC</categories><comments>9 pages, 2 figures, 5 tables. v2: extended version. this is a
  preprint of a published paper - see published version for definitive version</comments><journal-ref>Parallel Processing Letters 24, 1450004 (2014)</journal-ref><doi>10.1142/S0129626414500042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the parallel performance of randomized interpolative decomposition
by decomposing low rank complex-valued Gaussian random matrices up to 64 GB. We
chose a Cray XMT supercomputer as it provides an almost ideal PRAM model
permitting quick investigation of parallel algorithms without obfuscation from
hardware idiosyncrasies. We obtain that on non-square matrices performance
becomes very good, with overall runtime over 70 times faster on 128 processors.
We also verify that numerically discovered error bounds still hold on matrices
nearly two orders of magnitude larger than those previously tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3831</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3831</id><created>2012-05-16</created><authors><author><keyname>Kuhn</keyname><forenames>Nicolas</forenames></author><author><keyname>Lochin</keyname><forenames>Emmanuel</forenames></author><author><keyname>Lacan</keyname><forenames>Jerome</forenames></author><author><keyname>Boreli</keyname><forenames>Roksana</forenames></author><author><keyname>Bes</keyname><forenames>Caroline</forenames></author><author><keyname>Clarac</keyname><forenames>Laurence</forenames></author></authors><title>Enabling Realistic Cross-Layer Analysis based on Satellite Physical
  Layer Traces</title><categories>cs.NI</categories><comments>6 pages, 5 figures and 1 table. Submitted at PIMRC 2012</comments><msc-class>68M99</msc-class><acm-class>C.2.5; D.1.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a solution to evaluate the performance of transport protocols as a
function of link layer reliability schemes (i.e. ARQ, FEC and Hybrid ARQ)
applied to satellite physical layer traces. As modelling such traces is complex
and may require approximations, the use of real traces will minimise the
potential for erroneous performance evaluations resulting from imperfect
models. Our Trace Manager Tool (TMT) produces the corresponding link layer
output, which is then used within the ns-2 network simulator via the
additionally developed ns-2 interface module. We first present the analytical
models for the link layer with bursty erasure packets and for the link layer
reliability mechanisms with bursty erasures. Then, we present details of the
TMT tool and our validation methodology, demonstrating that the selected
performance metrics (recovery delay and throughput efficiency) exhibit a good
match between the theoretical results and those obtained with TMT. Finally, we
present results showing the impact of different link layer reliability
mechanisms on the performance of TCP Cubic transport layer protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3832</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3832</id><created>2012-05-16</created><updated>2012-09-07</updated><authors><author><keyname>Taylor</keyname><forenames>Dane</forenames></author><author><keyname>Larremore</keyname><forenames>Daniel B.</forenames></author></authors><title>Social Climber attachment in forming networks produces phase transition
  in a measure of connectivity</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>6 pages, 4 figures</comments><journal-ref>Phys. Rev. E 86, 031140 (2012)</journal-ref><doi>10.1103/PhysRevE.86.031140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formation and fragmentation of networks is typically studied using
percolation theory, but most previous research has been restricted to studying
a phase transition in cluster size, examining the emergence of a giant
component. This approach does not study the effects of evolving network
structure on dynamics that occur at the nodes, such as the synchronization of
oscillators and the spread of information, epidemics, and neuronal excitations.
We introduce and analyze new link-formation rules, called Social Climber (SC)
attachment, that may be combined with arbitrary percolation models to produce a
previously unstudied phase transition using the largest eigenvalue of the
network adjacency matrix as the order parameter. This eigenvalue is significant
in the analyses of many network-coupled dynamical systems in which it measures
the quality of global coupling and is hence a natural measure of connectivity.
We highlight the important self-organized properties of SC attachment and
discuss implications for controlling dynamics on networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3846</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3846</id><created>2012-05-17</created><authors><author><keyname>Mehani</keyname><forenames>Olivier</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Rakotoarivelo</keyname><forenames>Thierry</forenames></author></authors><title>A Method for the Characterisation of Observer Effects and its
  Application to OML</title><categories>cs.PF cs.NI</categories><comments>24 pages, submitted to ACM MSWIM 2012</comments><report-no>NICTA technical report number 5895</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In all measurement campaigns, one needs to assert that the instrumentation
tools do not significantly impact the system being monitored. This is critical
to future claims based on the collected data and is sometimes overseen in
experimental studies. We propose a method to evaluate the potential &quot;observer
effect&quot; of an instrumentation system, and apply it to the OMF Measurement
Library (OML). OML allows the instrumentation of almost any software to collect
any type of measurements. As it is increasingly being used in networking
research, it is important to characterise possible biases it may introduce in
the collected metrics. Thus, we study its effect on multiple types of reports
from various applications commonly used in wireless research. To this end, we
designed experiments comparing OML-instrumented software with their original
flavours. Our analyses of the results from these experiments show that, with an
appropriate reporting setup, OML has no significant impact on the instrumented
applications, and may even improve some of their performances in specifics
cases. We discuss our methodology and the implication of using OML, and provide
guidelines on instrumenting off-the-shelf software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3853</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3853</id><created>2012-05-17</created><authors><author><keyname>Schieler</keyname><forenames>Curt</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Secrecy Is Cheap if the Adversary Must Reconstruct</title><categories>cs.IT cs.CR math.IT</categories><comments>ISIT 2012, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A secret key can be used to conceal information from an eavesdropper during
communication, as in Shannon's cipher system. Most theoretical guarantees of
secrecy require the secret key space to grow exponentially with the length of
communication. Here we show that when an eavesdropper attempts to reconstruct
an information sequence, as posed in the literature by Yamamoto, very little
secret key is required to effect unconditionally maximal distortion;
specifically, we only need the secret key space to increase unboundedly,
growing arbitrarily slowly with the blocklength. As a corollary, even with a
secret key of constant size we can still cause the adversary arbitrarily close
to maximal distortion, regardless of the length of the information sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3856</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3856</id><created>2012-05-17</created><updated>2012-12-07</updated><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Mohanlal</keyname><forenames>Manish</forenames></author><author><keyname>Wilson</keyname><forenames>Christo</forenames></author><author><keyname>Wang</keyname><forenames>Xiao</forenames></author><author><keyname>Metzger</keyname><forenames>Miriam</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Social Turing Tests: Crowdsourcing Sybil Detection</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As popular tools for spreading spam and malware, Sybils (or fake accounts)
pose a serious threat to online communities such as Online Social Networks
(OSNs). Today, sophisticated attackers are creating realistic Sybils that
effectively befriend legitimate users, rendering most automated Sybil detection
techniques ineffective. In this paper, we explore the feasibility of a
crowdsourced Sybil detection system for OSNs. We conduct a large user study on
the ability of humans to detect today's Sybil accounts, using a large corpus of
ground-truth Sybil accounts from the Facebook and Renren networks. We analyze
detection accuracy by both &quot;experts&quot; and &quot;turkers&quot; under a variety of
conditions, and find that while turkers vary significantly in their
effectiveness, experts consistently produce near-optimal results. We use these
results to drive the design of a multi-tier crowdsourcing Sybil detection
system. Using our user study data, we show that this system is scalable, and
can be highly effective either as a standalone system or as a complementary
technique to current tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3863</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3863</id><created>2012-05-17</created><updated>2012-05-18</updated><authors><author><keyname>Hajizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>Three-Receiver Broadcast Channels with Side Information</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, This paper was accepted for presentation at IEEE
  ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-receiver broadcast channel (BC) is of interest due to its information
theoretical differences with two receiver one. In this paper, we derive
achievable rate regions for two classes of 3-receiver BC with side information
available at the transmitter, Multilevel BC and 3-receiver less noisy BC, by
using superposition coding, Gel'fand-Pinsker binning scheme and Nair-El Gamal
indirect decoding. Our rate region for multilevel BC subsumes the Steinberg
rate region for 2-receiver degraded BC with side information as its special
case. We also find the capacity region of 3-receiver less noisy BC when side
information is available both at the transmitter and at the receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3932</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3932</id><created>2012-05-17</created><updated>2013-01-31</updated><authors><author><keyname>Obregon</keyname><forenames>Evanny</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>On the Feasibility of Indoor Broadband Secondary Access to 960-1215 MHz
  Aeronautical Spectrum</title><categories>cs.NI</categories><comments>19 pages, 7 figures, 1 tables, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the feasibility of indoor broadband service
provisioning using secondary spectrum access to the 960-1215 MHz band,
primarily allocated to the distance measuring equipment (DME) system for
aeronautical navigation. We propose a practical secondary sharing scheme
customized to the characteristics of the DME. Since the primary system performs
a safety-of-life functionality, protection from harmful interference becomes
extremely critical. The proposed scheme controls aggregate interference by
imposing an individual interference threshold on the secondary users. We
examine the feasibility of large scale secondary access in terms of the
transmission probability of the secondary users that keeps the probability of
harmful interference below a given limit. Uncertainties in the estimation of
propagation loss and DME location affect the feasibility of the secondary
access. Numerical results show that large number of secondary users are able to
operate in adjacent DME channels without harming the primary system even with
limited accuracy in the estimation of the propagation loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3940</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3940</id><created>2012-05-17</created><updated>2015-09-30</updated><authors><author><keyname>Jacobs</keyname><forenames>Bart</forenames><affiliation>Institute for Computing and Information Sciences</affiliation></author></authors><title>New Directions in Categorical Logic, for Classical, Probabilistic and
  Quantum Logic</title><categories>math.LO cs.LO quant-ph</categories><proxy>LMCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitionistic logic, in which the double negation law not-not-P = P fails,
is dominant in categorical logic, notably in topos theory. This paper follows a
different direction in which double negation does hold. The algebraic notions
of effect algebra/module that emerged in theoretical physics form the
cornerstone. It is shown that under mild conditions on a category, its maps of
the form X -&gt; 1+1 carry such effect module structure, and can be used as
predicates. Predicates are identified in many different situations, and capture
for instance ordinary subsets, fuzzy predicates in a probabilistic setting,
idempotents in a ring, and effects (positive elements below the unit) in a
C*-algebra or Hilbert space. In quantum foundations the duality between states
and effects plays an important role. It appears here in the form of an
adjunction, where we use maps 1 -&gt; X as states. For such a state s and a
predicate p, the validity probability s |= p is defined, as an abstract Born
rule. It captures many forms of (Boolean or probabilistic) validity known from
the literature. Measurement from quantum mechanics is formalised categorically
in terms of `instruments', using L\&quot;uders rule in the quantum case. These
instruments are special maps associated with predicates (more generally, with
tests), which perform the act of measurement and may have a side-effect that
disturbs the system under observation. This abstract description of
side-effects is one of the main achievements of the current approach. It is
shown that in the special case of C*-algebras, side-effect appear exclusively
in the non-commutative case. Also, these instruments are used for test
operators in a dynamic logic that can be used for reasoning about quantum
programs/protocols. The paper describes four successive assumptions, towards a
categorical axiomatisation of quantitative logic for probabilistic and quantum
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3952</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3952</id><created>2012-05-17</created><authors><author><keyname>Pawlowski</keyname><forenames>Roger P.</forenames></author><author><keyname>Phipps</keyname><forenames>Eric T.</forenames></author><author><keyname>Salinger</keyname><forenames>Andrew G.</forenames></author><author><keyname>Owen</keyname><forenames>Steven J.</forenames></author><author><keyname>Siefert</keyname><forenames>Christopher M.</forenames></author><author><keyname>Staten</keyname><forenames>Matthew L.</forenames></author></authors><title>Automating embedded analysis capabilities and managing software
  complexity in multiphysics simulation part II: application to partial
  differential equations</title><categories>cs.MS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A template-based generic programming approach was presented in a previous
paper that separates the development effort of programming a physical model
from that of computing additional quantities, such as derivatives, needed for
embedded analysis algorithms. In this paper, we describe the implementation
details for using the template-based generic programming approach for
simulation and analysis of partial differential equations (PDEs). We detail
several of the hurdles that we have encountered, and some of the software
infrastructure developed to overcome them. We end with a demonstration where we
present shape optimization and uncertainty quantification results for a 3D PDE
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3959</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3959</id><created>2012-05-17</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Haq</keyname><forenames>Dr. Kashiful</forenames></author><author><keyname>Jaleel</keyname><forenames>Uruj</forenames></author><author><keyname>Saxena</keyname><forenames>Sharad</forenames></author></authors><title>Some drastic improvements found in the analysis of routing protocol for
  the Bluetooth technology using scatternet</title><categories>cs.NI</categories><comments>10 pages, 8 Figures, ISSN Print 1994-4608, ISSN Online 1992-8424</comments><journal-ref>Special Issue(CCITA-2010), Ubiquitous Computing and Communication
  Journal (UbiCC),www.ubicc.org, September 2010 , Volume CCITA-2010 , Pages
  Number 86-95</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bluetooth is a promising wireless technology that enables portable devices to
form short-range wireless ad hoc networks. Unlike wireless LAN, the
communication of Bluetooth devices follow a strict master slave relationship,
that is, it is not possible for a slave device to directly communicate with
another slave device even though they are within the radio coverage of each
other. For inter piconet communication, a scatternet has to be formed, in which
some Bluetooth devices have to act as bridge nodes between piconets. The
Scatternet formed have following properties in which they are connected i.e
every Bluetooth device can be reached from every other device, Piconet size is
limited to eight nodes [1]. The author of this research paper have studied
different type of routing protocol and have made efforts to improve throughput
and reduce packet loss due to failure in the routing loop and increased
mobility and improve the cohesive network structure, resolve the change
topology conflicts [2], and a successful &amp; efficient transfer of packet from
source to destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3964</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3964</id><created>2012-05-17</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Ashish</forenames></author></authors><title>Machine Recognition of Hand Written Characters using Neural Networks</title><categories>cs.AI</categories><comments>4 pages, 1 Figure, ISSN:0975 - 8887</comments><journal-ref>International Journal of Computer Applications (IJCA) ,January
  2011 Volume 14, Number 2, Pages 6-9</journal-ref><doi>10.5120/1819-2380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even today in Twenty First Century Handwritten communication has its own
stand and most of the times, in daily life it is globally using as means of
communication and recording the information like to be shared with others.
Challenges in handwritten characters recognition wholly lie in the variation
and distortion of handwritten characters, since different people may use
different style of handwriting, and direction to draw the same shape of the
characters of their known script. This paper demonstrates the nature of
handwritten characters, conversion of handwritten data into electronic data,
and the neural network approach to make machine capable of recognizing hand
written characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3966</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3966</id><created>2012-05-17</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Ashish</forenames></author></authors><title>Neural Networks for Handwritten English Alphabet Recognition</title><categories>cs.AI cs.CV</categories><comments>5 pages, 3 Figure, ISSN:0975 - 8887</comments><journal-ref>International Journal of Computer Applications(IJCA), April 2011,
  Volume 20, Number 7, Pages 1-5</journal-ref><doi>10.5120/2449-2824</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates the use of neural networks for developing a system
that can recognize hand-written English alphabets. In this system, each English
alphabet is represented by binary values that are used as input to a simple
feature extraction system, whose output is fed to our neural network system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3981</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3981</id><created>2012-05-17</created><updated>2014-07-28</updated><authors><author><keyname>Frasconi</keyname><forenames>Paolo</forenames></author><author><keyname>Costa</keyname><forenames>Fabrizio</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author><author><keyname>De Grave</keyname><forenames>Kurt</forenames></author></authors><title>kLog: A Language for Logical and Relational Learning with Kernels</title><categories>cs.AI cs.LG cs.PL</categories><acm-class>I.2.6; I.2.3; D.3.2</acm-class><doi>10.1016/j.artint.2014.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce kLog, a novel approach to statistical relational learning.
Unlike standard approaches, kLog does not represent a probability distribution
directly. It is rather a language to perform kernel-based learning on
expressive logical and relational representations. kLog allows users to specify
learning problems declaratively. It builds on simple but powerful concepts:
learning from interpretations, entity/relationship data modeling, logic
programming, and deductive databases. Access by the kernel to the rich
representation is mediated by a technique we call graphicalization: the
relational representation is first transformed into a graph --- in particular,
a grounded entity/relationship diagram. Subsequently, a choice of graph kernel
defines the feature space. kLog supports mixed numerical and symbolic data, as
well as background knowledge in the form of Prolog or Datalog programs as in
inductive logic programming systems. The kLog framework can be applied to
tackle the same range of tasks that has made statistical relational learning so
popular, including classification, regression, multitask learning, and
collective classification. We also report about empirical comparisons, showing
that kLog can be either more accurate, or much faster at the same level of
accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at
http://klog.dinfo.unifi.it along with tutorials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3982</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3982</id><created>2012-05-17</created><authors><author><keyname>Aumann</keyname><forenames>Yonatan</forenames></author><author><keyname>Dombb</keyname><forenames>Yair</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author></authors><title>Computing Socially-Efficient Cake Divisions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting in which a single divisible good (&quot;cake&quot;) needs to be
divided between n players, each with a possibly di?fferent valuation function
over pieces of the cake. For this setting, we address the problem of ?finding
divisions that maximize the social welfare, focusing on divisions where each
player needs to get one contiguous piece of the cake. We show that for both the
utilitarian and the egalitarian social welfare functions it is NP-hard to find
the optimal division. For the utilitarian welfare, we provide a constant factor
approximation algorithm, and prove that no FPTAS is possible unless P=NP. For
egalitarian welfare, we prove that it is NP-hard to approximate the optimum to
any factor smaller than 2. For the case where the number of players is small,
we provide an FPT (fixed parameter tractable) FPTAS for both the utilitarian
and the egalitarian welfare objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3986</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3986</id><created>2012-05-17</created><authors><author><keyname>Al-Mushayt</keyname><forenames>Omar S.</forenames></author><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author><author><keyname>Haq</keyname><forenames>Kashiful</forenames></author></authors><title>Electronic-government in Saudi Arabia: A positive revolution in the
  peninsula</title><categories>cs.CY</categories><comments>12 Pages, 1 Figures, ISSN Print: (0974-7273),ISSN Online:(0975-3761)</comments><journal-ref>International Transactions in Applied Sciences (ITAS), January
  2009, Volume 1 , Number 1, Pages 87-98</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The informatization practice of countries all over the world has shown that
the level of a government's informatization is one main factor that can affect
its international competitive power. At present, e-government construction is
regarded as one of the most important tasks for the national economy and
society upliftment and informatization in Saudi Arabia. Unlike the traditional
governments, an e-government takes on a new look with its framework and
operation mode more suitable for the contemporary era. In fact, it is a basic
national strategy to promote Saudi Arabia's informatization by means of
e-government construction. This talk firstly introduces the basic concepts and
relevant viewpoints of egovernment, then reviews the development process of
e-government in Saudi Arabia, and describes the current states, development
strategies of e-government in Saudi Arabia. And also review e-government
maturity models and synthesize them e-government maturity models are
investigated, in which the authors have proposed the Delloite's six-stage
model, Layne and Lee four-stage model and Accenture five-stage model. So, the
main e-government maturity stages are: online presence, interaction,
transaction, transformation and digital democracy. After that, according to
many references, the main technologies which are used in each stage are
summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3993</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3993</id><created>2012-05-17</created><updated>2012-08-28</updated><authors><author><keyname>Tu</keyname><forenames>Sheng-Yuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Diffusion Strategies Outperform Consensus Strategies for Distributed
  Estimation over Adaptive Networks</title><categories>cs.IT cs.SI math.IT</categories><comments>37 pages, 7 figures, To appear in IEEE Transactions on Signal
  Processing, 2012</comments><doi>10.1109/TSP.2012.2217338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive networks consist of a collection of nodes with adaptation and
learning abilities. The nodes interact with each other on a local level and
diffuse information across the network to solve estimation and inference tasks
in a distributed manner. In this work, we compare the mean-square performance
of two main strategies for distributed estimation over networks: consensus
strategies and diffusion strategies. The analysis in the paper confirms that
under constant step-sizes, diffusion strategies allow information to diffuse
more thoroughly through the network and this property has a favorable effect on
the evolution of the network: diffusion networks are shown to converge faster
and reach lower mean-square deviation than consensus networks, and their
mean-square stability is insensitive to the choice of the combination weights.
In contrast, and surprisingly, it is shown that consensus networks can become
unstable even if all the individual nodes are stable and able to solve the
estimation task on their own. When this occurs, cooperation over the network
leads to a catastrophic failure of the estimation task. This phenomenon does
not occur for diffusion networks: we show that stability of the individual
nodes always ensures stability of the diffusion network irrespective of the
combination topology. Simulation results support the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3997</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3997</id><created>2012-05-17</created><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author></authors><title>Free Energy and the Generalized Optimality Equations for Sequential
  Decision Making</title><categories>stat.ML cs.AI cs.GT cs.SY</categories><comments>10 pages, 2 figures</comments><journal-ref>European Workshop on Reinforcement Learning 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The free energy functional has recently been proposed as a variational
principle for bounded rational decision-making, since it instantiates a natural
trade-off between utility gains and information processing costs that can be
axiomatically derived. Here we apply the free energy principle to general
decision trees that include both adversarial and stochastic environments. We
derive generalized sequential optimality equations that not only include the
Bellman optimality equations as a limit case, but also lead to well-known
decision-rules such as Expectimax, Minimax and Expectiminimax. We show how
these decision-rules can be derived from a single free energy principle that
assigns a resource parameter to each node in the decision tree. These resource
parameters express a concrete computational cost that can be measured as the
amount of samples that are needed from the distribution that belongs to each
node. The free energy principle therefore provides the normative basis for
generalized optimality equations that account for both adversarial and
stochastic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3998</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3998</id><created>2012-05-17</created><authors><author><keyname>Buranapanichkit</keyname><forenames>Dujdow</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Distributed Time-Frequency Division Multiple Access Protocol For
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>4 pages, IEEE Wireless Communications Letters, to appear in 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that biology-inspired self-maintaining algorithms in
wireless sensor nodes achieve near optimum time division multiple access (TDMA)
characteristics in a decentralized manner and with very low complexity. We
extend such distributed TDMA approaches to multiple channels (frequencies).
This is achieved by extending the concept of collaborative reactive listening
in order to balance the number of nodes in all available channels. We prove the
stability of the new protocol and estimate the delay until the balanced system
state is reached. Our approach is benchmarked against single-channel
distributed TDMA and channel hopping approaches using TinyOS imote2 wireless
sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.3999</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.3999</id><created>2012-05-17</created><authors><author><keyname>Jin</keyname><forenames>Qiyu</forenames></author><author><keyname>Grama</keyname><forenames>Ion</forenames></author><author><keyname>Liu</keyname><forenames>Quansheng</forenames></author></authors><title>Optimal Weights Mixed Filter for Removing Mixture of Gaussian and
  Impulse Noises</title><categories>cs.CV</categories><comments>9 pages, 3 figures and 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the character of Gaussian, we modify the Rank-Ordered Absolute
Differences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussian
and impulse noises (ROADG). It will be more effective to detect impulse noise
when the impulse is mixed with Gaussian noise. Combining rightly the ROADG with
Optimal Weights Filter (OWF), we obtain a new method to deal with the mixed
noise, called Optimal Weights Mixed Filter (OWMF). The simulation results show
that the method is effective to remove the mixed noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4011</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4011</id><created>2012-05-17</created><authors><author><keyname>Herzberg</keyname><forenames>Amir</forenames></author><author><keyname>Shulman</keyname><forenames>Haya</forenames></author></authors><title>Fragmentation Considered Poisonous</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present practical poisoning and name-server block- ing attacks on standard
DNS resolvers, by off-path, spoofing adversaries. Our attacks exploit large DNS
responses that cause IP fragmentation; such long re- sponses are increasingly
common, mainly due to the use of DNSSEC. In common scenarios, where DNSSEC is
partially or incorrectly deployed, our poisoning attacks allow 'com- plete'
domain hijacking. When DNSSEC is fully de- ployed, attacker can force use of
fake name server; we show exploits of this allowing off-path traffic analy- sis
and covert channel. When using NSEC3 opt-out, attacker can also create fake
subdomains, circumvent- ing same origin restrictions. Our attacks circumvent
resolver-side defenses, e.g., port randomisation, IP ran- domisation and query
randomisation. The (new) name server (NS) blocking attacks force re- solver to
use specific name server. This attack allows Degradation of Service,
traffic-analysis and covert chan- nel, and also facilitates DNS poisoning. We
validated the attacks using standard resolver soft- ware and standard DNS name
servers and zones, e.g., org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4013</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4013</id><created>2012-05-17</created><authors><author><keyname>Zhao</keyname><forenames>Xiaohan</forenames></author><author><keyname>Sala</keyname><forenames>Alessandra</forenames></author><author><keyname>Wilson</keyname><forenames>Christo</forenames></author><author><keyname>Wang</keyname><forenames>Xiao</forenames></author><author><keyname>Gaito</keyname><forenames>Sabrina</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>Multi-scale Dynamics in a Massive Online Social Network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data confidentiality policies at major social network providers have severely
limited researchers' access to large-scale datasets. The biggest impact has
been on the study of network dynamics, where researchers have studied citation
graphs and content-sharing networks, but few have analyzed detailed dynamics in
the massive social networks that dominate the web today. In this paper, we
present results of analyzing detailed dynamics in the Renren social network,
covering a period of 2 years when the network grew from 1 user to 19 million
users and 199 million edges. Rather than validate a single model of network
dynamics, we analyze dynamics at different granularities (user-, community- and
network- wide) to determine how much, if any, users are influenced by dynamics
processes at different scales. We observe in- dependent predictable processes
at each level, and find that while the growth of communities has moderate and
sustained impact on users, significant events such as network merge events have
a strong but short-lived impact that is quickly dominated by the continuous
arrival of new users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4049</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4049</id><created>2012-05-17</created><authors><author><keyname>Aguilar</keyname><forenames>Teck</forenames></author><author><keyname>Syue</keyname><forenames>Syue-Ju</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Wang</keyname><forenames>Chin-Liang</forenames></author></authors><title>CoopGeo: A Beaconless Geographic Cross-Layer Protocol for Cooperative
  Wireless Ad Hoc Networks</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Wireless Communications (2011)</journal-ref><doi>10.1109/TWC.2011.060711.100480</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative relaying has been proposed as a promising transmission technique
that effectively creates spatial diversity through the cooperation among
spatially distributed nodes. However, to achieve efficient communications while
gaining full benefits from cooperation, more interactions at higher protocol
layers, particularly the MAC (Medium Access Control) and network layers, are
vitally required. This is ignored in most existing articles that mainly focus
on physical (PHY)-layer relaying techniques. In this paper, we propose a novel
cross-layer framework involving two levels of joint design---a MAC-network
cross-layer design for forwarder selection (or termed routing) and a MAC-PHY
for relay selection---over symbol-wise varying channels. Based on location
knowledge and contention processes, the proposed cross-layer protocol, CoopGeo,
aims at providing an efficient, distributed approach to select next hops and
optimal relays along a communication path. Simulation results demonstrate that
CoopGeo not only operates properly with varying densities of nodes, but
performs significantly better than the existing protocol BOSS in terms of
packet error rate, transmission error probability, and saturated throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4067</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4067</id><created>2012-05-17</created><updated>2013-03-23</updated><authors><author><keyname>Torezzan</keyname><forenames>Cristiano</forenames></author><author><keyname>Strapasson</keyname><forenames>Jo&#xe3;o E.</forenames></author><author><keyname>Costa</keyname><forenames>Sueli I. R.</forenames></author><author><keyname>Siqueira</keyname><forenames>Rogerio M.</forenames></author></authors><title>Optimum Commutative Group Codes</title><categories>cs.IT math.GR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for finding an optimum $n$-dimensional commutative group code of a
given order $M$ is presented. The approach explores the structure of lattices
related to these codes and provides a significant reduction in the number of
non-isometric cases to be analyzed. The classical factorization of matrices
into Hermite and Smith normal forms and also basis reduction of lattices are
used to characterize isometric commutative group codes. Several examples of
optimum commutative group codes are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4070</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4070</id><created>2012-05-17</created><authors><author><keyname>Zhang</keyname><forenames>Kai</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author><author><keyname>Zhao</keyname><forenames>Shancheng</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoyi</forenames></author></authors><title>A New Ensemble of Rate-Compatible LDPC Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we presented three approaches to improve the design of Kite
codes (newly proposed rateless codes), resulting in an ensemble of
rate-compatible
  LDPC codes with code rates varying &quot;continuously&quot; from 0.1 to 0.9 for
additive white Gaussian noise (AWGN) channels. The new ensemble rate-compatible
LDPC codes can be constructed conveniently with an empirical formula.
Simulation results show that, when applied to incremental redundancy hybrid
automatic repeat request (IR-HARQ) system, the constructed codes (with higher
order modulation) perform well in a wide range of signal-to-noise-ratios
(SNRs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4080</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4080</id><created>2012-05-18</created><updated>2013-04-17</updated><authors><author><keyname>Ziniel</keyname><forenames>Justin</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Dynamic Compressive Sensing of Time-Varying Signals via Approximate
  Message Passing</title><categories>cs.IT math.IT</categories><comments>32 pages, 7 figures</comments><doi>10.1109/TSP.2013.2273196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the dynamic compressive sensing (CS) problem of recovering
sparse, correlated, time-varying signals from sub-Nyquist, non-adaptive, linear
measurements is explored from a Bayesian perspective. While there has been a
handful of previously proposed Bayesian dynamic CS algorithms in the
literature, the ability to perform inference on high-dimensional problems in a
computationally efficient manner remains elusive. In response, we propose a
probabilistic dynamic CS signal model that captures both amplitude and support
correlation structure, and describe an approximate message passing algorithm
that performs soft signal estimation and support detection with a computational
complexity that is linear in all problem dimensions. The algorithm, DCS-AMP,
can perform either causal filtering or non-causal smoothing, and is capable of
learning model parameters adaptively from the data through an
expectation-maximization learning procedure. We provide numerical evidence that
DCS-AMP performs within 3 dB of oracle bounds on synthetic data under a variety
of operating conditions. We further describe the result of applying DCS-AMP to
two real dynamic CS datasets, as well as a frequency estimation task, to
bolster our claim that DCS-AMP is capable of offering state-of-the-art
performance and speed on real-world high-dimensional problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4104</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4104</id><created>2012-05-18</created><authors><author><keyname>Abraham</keyname><forenames>Ittai</forenames></author><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Combinatorial Auctions with Restricted Complements</title><categories>cs.GT</categories><comments>This work will appear in the 13th ACM conference on Electronic
  Commerce (EC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complements between goods - where one good takes on added value in the
presence of another - have been a thorn in the side of algorithmic mechanism
designers. On the one hand, complements are common in the standard motivating
applications for combinatorial auctions, like spectrum license auctions. On the
other, welfare maximization in the presence of complements is notoriously
difficult, and this intractability has stymied theoretical progress in the
area. For example, there are no known positive results for combinatorial
auctions in which bidder valuations are multi-parameter and
non-complement-free, other than the relatively weak results known for general
valuations.
  To make inroads on the problem of combinatorial auction design in the
presence of complements, we propose a model for valuations with complements
that is parameterized by the &quot;size&quot; of the complements. A valuation in our
model is represented succinctly by a weighted hypergraph, where the size of the
hyper-edges corresponds to degree of complementarity. Our model permits a
variety of computationally efficient queries, and non-trivial
welfare-maximization algorithms and mechanisms.
  We design the following polynomial-time approximation algorithms and truthful
mechanisms for welfare maximization with bidders with hypergraph valuations.
  1- For bidders whose valuations correspond to subgraphs of a known graph that
is planar (or more generally, excludes a fixed minor), we give a truthful and
(1+epsilon)-approximate mechanism.
  2- We give a polynomial-time, r-approximation algorithm for welfare
maximization with hypergraph-r valuations. Our algorithm randomly rounds a
compact linear programming relaxation of the problem.
  3- We design a different approximation algorithm and use it to give a
polynomial-time, truthful-in-expectation mechanism that has an approximation
factor of O(log^r m).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4124</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4124</id><created>2012-05-18</created><authors><author><keyname>Schridde</keyname><forenames>Christian</forenames></author></authors><title>The permanent, graph gadgets and counting solutions for certain types of
  planar formulas</title><categories>cs.CC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we build on the idea of Valiant \cite{Val79a} and
Ben-Dor/Halevi \cite{Ben93}, that is, to count the number of satisfying
solutions of a boolean formula via computing the permanent of a specially
constructed matrix. We show that the Desnanot-Jacobi identity ($\dji$) prevents
Valiant's original approach to achieve a parsimonious reduction to the
permanent over a field of characteristic two. As the next step, since the
computation of the permanent is $#\classP$-complete, we make use of the
equality of the permanent and the number of perfect matchings in an unweighted
graph's bipartite double cover. Whenever this bipartite double cover (BDC) is
planar, the number of perfect matchings can be counted in polynomial time using
Kasteleyn's algorithm \cite{Kas67}. To enforce planarity of the BDC, we replace
Valiant's original gadgets with new gadgets and describe what properties these
gadgets must have. We show that the property of \textit{circular planarity}
plays a crucial role to find the correct gadgets for a counting problem. To
circumvent the $\dji$-barrier, we switch over to fields
$\mathbb{Z}/p\mathbb{Z}$, for a prime $p &gt; 2$.
  With this approach we are able to count the number of solutions for
$\forestdreisat$ formulas in randomized polynomial time. Finally, we present a
conjecture that states which kind of generalized gadgets can not be found,
since otherwise one could prove $\classRP = \classNP$. The conjecture
establishes a relationship between the determinants of the minors of a graph
$\grG$'s adjacency matrix and the \textit{circular planar} structure of
$\grG$'s BDC regarding a given set of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4126</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4126</id><created>2012-05-18</created><updated>2014-03-31</updated><authors><author><keyname>Nguyen</keyname><forenames>Van Minh</forenames></author></authors><title>Some Properties of Large Excursions of a Stationary Gaussian Process</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work investigates two properties of level crossings of a
stationary Gaussian process $X(t)$ with autocorrelation function $R_X(\tau)$.
We show firstly that if $R_X(\tau)$ admits finite second and fourth derivatives
at the origin, the length of up-excursions above a large negative level
$-\gamma$ is asymptotically exponential as $-\gamma \to -\infty$. Secondly,
assuming that $R_X(\tau)$ admits a finite second derivative at the origin and
some defined properties, we derive the mean number of crossings as well as the
length of successive excursions above two subsequent large levels. The
asymptotic results are shown to be effective even for moderate values of
crossing level. An application of the developed results is proposed to derive
the probability of successive excursions above adjacent levels during a time
window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4133</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4133</id><created>2012-05-18</created><updated>2013-02-20</updated><authors><author><keyname>Yaghoobi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Nam</keyname><forenames>Sangnam</forenames></author><author><keyname>Gribonval</keyname><forenames>Remi</forenames></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author></authors><title>Constrained Overcomplete Analysis Operator Learning for Cosparse Signal
  Modelling</title><categories>math.NA cs.LG</categories><comments>29 pages, 13 figures, accepted to be published in TSP</comments><doi>10.1109/TSP.2013.2250968</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a low-dimensional signal model from a
collection of training samples. The mainstream approach would be to learn an
overcomplete dictionary to provide good approximations of the training samples
using sparse synthesis coefficients. This famous sparse model has a less well
known counterpart, in analysis form, called the cosparse analysis model. In
this new model, signals are characterised by their parsimony in a transformed
domain using an overcomplete (linear) analysis operator. We propose to learn an
analysis operator from a training corpus using a constrained optimisation
framework based on L1 optimisation. The reason for introducing a constraint in
the optimisation framework is to exclude trivial solutions. Although there is
no final answer here for which constraint is the most relevant constraint, we
investigate some conventional constraints in the model adaptation field and use
the uniformly normalised tight frame (UNTF) for this purpose. We then derive a
practical learning algorithm, based on projected subgradients and
Douglas-Rachford splitting technique, and demonstrate its ability to robustly
recover a ground truth analysis operator, when provided with a clean training
set, of sufficient size. We also find an analysis operator for images, using
some noisy cosparse signals, which is indeed a more realistic experiment. As
the derived optimisation problem is not a convex program, we often find a local
minimum using such variational methods. Some local optimality conditions are
derived for two different settings, providing preliminary theoretical support
for the well-posedness of the learning problem under appropriate conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4135</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4135</id><created>2012-05-18</created><updated>2012-06-21</updated><authors><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Duffy</keyname><forenames>Ken R.</forenames></author></authors><title>Guesswork, large deviations and Shannon entropy</title><categories>cs.IT math.IT</categories><msc-class>94A17</msc-class><journal-ref>IEEE Transactions on Information Theory, 59 (2), 796-802 2013</journal-ref><doi>10.1109/TIT.2012.2219036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How hard is it guess a password? Massey showed that that the Shannon entropy
of the distribution from which the password is selected is a lower bound on the
expected number of guesses, but one which is not tight in general. In a series
of subsequent papers under ever less restrictive stochastic assumptions, an
asymptotic relationship as password length grows between scaled moments of the
guesswork and specific R\'{e}nyi entropy was identified.
  Here we show that, when appropriately scaled, as the password length grows
the logarithm of the guesswork satisfies a Large Deviation Principle (LDP),
providing direct estimates of the guesswork distribution when passwords are
long. The rate function governing the LDP possess a specific, restrictive form
that encapsulates underlying structure in the nature of guesswork. Returning to
Massey's original observation, a corollary to the LDP shows that expectation of
the logarithm of the guesswork is the specific Shannon entropy of the password
selection process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4138</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4138</id><created>2012-05-18</created><authors><author><keyname>Hienert</keyname><forenames>Daniel</forenames></author><author><keyname>Luciano</keyname><forenames>Francesco</forenames></author></authors><title>Extraction of Historical Events from Wikipedia</title><categories>cs.IR cs.DL</categories><comments>To be published in Proceedings of Knowledge Discovery and Data Mining
  Meets Linked Open Data (Know@LOD) Workshop at ESWC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The DBpedia project extracts structured information from Wikipedia and makes
it available on the web. Information is gathered mainly with the help of
infoboxes that contain structured information of the Wikipedia article. A lot
of information is only contained in the article body and is not yet included in
DBpedia. In this paper we focus on the extraction of historical events from
Wikipedia articles that are available for about 2,500 years for different
languages. We have extracted about 121,000 events with more than 325,000 links
to DBpedia entities and provide access to this data via a Web API, SPARQL
endpoint, Linked Data Interface and in a timeline application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4139</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4139</id><created>2012-05-18</created><authors><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>Park</keyname><forenames>Hosung</forenames></author><author><keyname>Hong</keyname><forenames>Seokbeom</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Chung</keyname><forenames>Habong</forenames></author></authors><title>Fast Correlation Computation Method for Matching Pursuit Algorithms in
  Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been many matching pursuit algorithms (MPAs) which handle the
sparse signal recovery problem a.k.a. compressed sensing (CS). In the MPAs, the
correlation computation step has a dominant computational complexity. In this
letter, we propose a new fast correlation computation method when we use some
classes of partial unitary matrices as the sensing matrix. Those partial
unitary matrices include partial Fourier matrices and partial Hadamard matrices
which are popular sensing matrices. The proposed correlation computation method
can be applied to almost all MPAs without causing any degradation of their
recovery performance. And, for most practical parameters, the proposed method
can reduce the computational complexity of the MPAs substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4144</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4144</id><created>2012-05-18</created><updated>2016-01-24</updated><authors><author><keyname>Delgosha</keyname><forenames>Payam</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Information Theoretic cutting of a cake</title><categories>cs.IT cs.GT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cutting a cake is a metaphor for the problem of dividing a resource (cake)
among several agents. The problem becomes non-trivial when the agents have
different valuations for different parts of the cake (i.e. one agent may like
chocolate while the other may like cream). A fair division of the cake is one
that takes into account the individual valuations of agents and partitions the
cake based on some fairness criterion. Fair division may be accomplished in a
distributed or centralized way. Due to its natural and practical appeal, it has
been a subject of study in economics. To best of our knowledge the role of
partial information in fair division has not been studied so far from an
information theoretic perspective. In this paper we study two important
algorithms in fair division, namely &quot;divide and choose&quot; and &quot;adjusted winner&quot;
for the case of two agents. We quantify the benefit of negotiation in the
divide and choose algorithm, and its use in tricking the adjusted winner
algorithm. Also we analyze the role of implicit information transmission
through actions for the repeated divide and choose problem by finding a
trembling hand perfect equilibrium for an specific setup. Lastly we consider a
centralized algorithm for maximizing the overall welfare of the agents under
the Nash collective utility function (CUF). This corresponds to a clustering
problem of the type traditionally studied in data mining and machine learning.
Drawing a conceptual link between this problem and the portfolio selection
problem in stock markets, we prove an upper bound on the increase of the Nash
CUF for a clustering refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4148</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4148</id><created>2012-05-18</created><authors><author><keyname>Singh</keyname><forenames>Abhay Kumar</forenames></author><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author></authors><title>On cyclic codes over the ring $Z_p + uZ_p + ... + u^{k-1}Z_p$</title><categories>cs.IT math.IT math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study cyclic codes over the ring $ \Z_p + u\Z_p +...+
u^{k-1}\Z_p $, where $u^k =0$. We find a set of generator for these codes. We
also study the rank, the dual and the Hamming distance of these codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4159</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4159</id><created>2012-05-18</created><updated>2012-05-25</updated><authors><author><keyname>Chen</keyname><forenames>Changyou</forenames></author><author><keyname>Buntine</keyname><forenames>Wray</forenames></author><author><keyname>Ding</keyname><forenames>Nan</forenames></author></authors><title>Theory of Dependent Hierarchical Normalized Random Measures</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents theory for Normalized Random Measures (NRMs), Normalized
Generalized Gammas (NGGs), a particular kind of NRM, and Dependent Hierarchical
NRMs which allow networks of dependent NRMs to be analysed. These have been
used, for instance, for time-dependent topic modelling. In this paper, we first
introduce some mathematical background of completely random measures (CRMs) and
their construction from Poisson processes, and then introduce NRMs and NGGs.
Slice sampling is also introduced for posterior inference. The dependency
operators in Poisson processes and for the corresponding CRMs and NRMs is then
introduced and Posterior inference for the NGG presented. Finally, we give
dependency and composition results when applying these operators to NRMs so
they can be used in a network with hierarchical and dependent relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4168</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4168</id><created>2012-05-18</created><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Goela</keyname><forenames>Naveen</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Approximate Feedback Capacity of the Gaussian Multicast Channel</title><categories>cs.IT math.IT</categories><comments>Extended version of a conference paper that appears in ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the capacity region to within log{2(M-1)} bits/s/Hz for the
M-transmitter K-receiver Gaussian multicast channel with feedback where each
receiver wishes to decode every message from the M transmitters. Extending
Cover-Leung's achievable scheme intended for (M,K)=(2,1), we show that this
generalized scheme achieves the cutset-based outer bound within log{2(M-1)}
bits per transmitter for all channel parameters. In contrast to the capacity in
the non-feedback case, the feedback capacity improves upon the naive
intersection of the feedback capacities of K individual multiple access
channels. We find that feedback provides unbounded multiplicative gain at high
signal-to-noise ratios as was shown in the Gaussian interference channel. To
complement the results, we establish the exact feedback capacity of the
Avestimehr-Diggavi-Tse (ADT) deterministic model, from which we make the
observation that feedback can also be beneficial for function computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4174</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4174</id><created>2012-05-18</created><updated>2013-08-29</updated><authors><author><keyname>Hauser</keyname><forenames>Alain</forenames></author><author><keyname>B&#xfc;hlmann</keyname><forenames>Peter</forenames></author></authors><title>Two Optimal Strategies for Active Learning of Causal Models from
  Interventional Data</title><categories>stat.ME cs.DM</categories><journal-ref>International Journal of Approximate Reasoning, Volume 55, Issue
  4, June 2014, Pages 926-939</journal-ref><doi>10.1016/j.ijar.2013.11.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From observational data alone, a causal DAG is only identifiable up to Markov
equivalence. Interventional data generally improves identifiability; however,
the gain of an intervention strongly depends on the intervention target, that
is, the intervened variables. We present active learning (that is, optimal
experimental design) strategies calculating optimal interventions for two
different learning goals. The first one is a greedy approach using
single-vertex interventions that maximizes the number of edges that can be
oriented after each intervention. The second one yields in polynomial time a
minimum set of targets of arbitrary size that guarantees full identifiability.
This second approach proves a conjecture of Eberhardt (2008) indicating the
number of unbounded intervention targets which is sufficient and in the worst
case necessary for full identifiability. In a simulation study, we compare our
two active learning approaches to random interventions and an existing
approach, and analyze the influence of estimation errors on the overall
performance of active learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4194</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4194</id><created>2012-05-18</created><updated>2014-08-10</updated><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Decision Taking as a Service</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision taking can be performed as a service to other parties and it is
amenable to outtasking rather than to outsourcing. Outtasking decision taking
is compatible with selfsourcing of decision making activities carried out in
preparation of decision taking. Decision taking as a service (DTaaS) is viewed
as an instance of so-called decision casting. Preconditions for service casting
are examined, and compliance of decision taking with these preconditions is
confirmed. Potential advantages and disadvantages of using decision taking as a
service are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4208</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4208</id><created>2012-05-18</created><updated>2012-10-10</updated><authors><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Vukobratovic</keyname><forenames>Dejan</forenames></author></authors><title>Frameless ALOHA Protocol for Wireless Networks</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel distributed random access scheme for wireless networks
based on slotted ALOHA, motivated by the analogies between successive
interference cancellation and iterative belief-propagation decoding on erasure
channels. The proposed scheme assumes that each user independently accesses the
wireless link in each slot with a predefined probability, resulting in a
distribution of user transmissions over slots. The operation bears analogy with
rateless codes, both in terms of probability distributions as well as to the
fact that the ALOHA frame becomes fluid and adapted to the current contention
process. Our aim is to optimize the slot access probability in order to achieve
rateless-like distributions, focusing both on the maximization of the
resolution probability of user transmissions and the throughput of the scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4212</identifier>
 <datestamp>2012-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4212</id><created>2012-05-17</created><authors><author><keyname>Ivan</keyname><forenames>Mihai</forenames></author><author><keyname>Ivan</keyname><forenames>Gheorghe</forenames></author></authors><title>Sample programs in C++ for matrix computations in max plus algebra</title><categories>cs.MS math.RA</categories><comments>29 pages, no figures</comments><msc-class>15A80, 68-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this paper is to propose five programs in C++ for matrix
computations and solving recurrent equations systems with entries in max plus
algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4213</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4213</id><created>2012-05-18</created><updated>2012-06-27</updated><authors><author><keyname>Shivaswamy</keyname><forenames>Pannaga</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Online Structured Prediction via Coactive Learning</title><categories>cs.LG cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Coactive Learning as a model of interaction between a learning
system and a human user, where both have the common goal of providing results
of maximum utility to the user. At each step, the system (e.g. search engine)
receives a context (e.g. query) and predicts an object (e.g. ranking). The user
responds by correcting the system if necessary, providing a slightly improved
-- but not necessarily optimal -- object as feedback. We argue that such
feedback can often be inferred from observable user behavior, for example, from
clicks in web-search. Evaluating predictions by their cardinal utility to the
user, we propose efficient learning algorithms that have ${\cal
O}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithm
never observes cardinal utility values as in conventional online learning. We
demonstrate the applicability of our model and learning algorithms on a movie
recommendation task, as well as ranking for web-search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4217</identifier>
 <datestamp>2012-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4217</id><created>2012-05-18</created><updated>2012-07-19</updated><authors><author><keyname>Kaufmann</keyname><forenames>Emilie</forenames></author><author><keyname>Korda</keyname><forenames>Nathaniel</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Thompson Sampling: An Asymptotically Optimal Finite Time Analysis</title><categories>stat.ML cs.LG</categories><comments>15 pages, 2 figures, submitted to ALT (Algorithmic Learning Theory)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of the optimality of Thompson Sampling for solving the
stochastic multi-armed bandit problem had been open since 1933. In this paper
we answer it positively for the case of Bernoulli rewards by providing the
first finite-time analysis that matches the asymptotic rate given in the Lai
and Robbins lower bound for the cumulative regret. The proof is accompanied by
a numerical comparison with other optimal policies, experiments that have been
lacking in the literature until now for the Bernoulli case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4220</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4220</id><created>2012-05-18</created><updated>2013-05-05</updated><authors><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Diffusion Adaptation over Networks</title><categories>cs.MA cs.LG</categories><comments>114 pages, 16 figures, 9 tables, to appear in E-Reference Signal
  Processing, R. Chellapa and S. Theodoridis, Eds., Elsevier, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive networks are well-suited to perform decentralized information
processing and optimization tasks and to model various types of self-organized
and complex behavior encountered in nature. Adaptive networks consist of a
collection of agents with processing and learning abilities. The agents are
linked together through a connection topology, and they cooperate with each
other through local interactions to solve distributed optimization, estimation,
and inference problems in real-time. The continuous diffusion of information
across the network enables agents to adapt their performance in relation to
streaming data and network conditions; it also results in improved adaptation
and learning performance relative to non-cooperative agents. This article
provides an overview of diffusion strategies for adaptation and learning over
networks. The article is divided into several sections: 1. Motivation; 2.
Mean-Square-Error Estimation; 3. Distributed Optimization via Diffusion
Strategies; 4. Adaptive Diffusion Strategies; 5. Performance of
Steepest-Descent Diffusion Strategies; 6. Performance of Adaptive Diffusion
Strategies; 7. Comparing the Performance of Cooperative Strategies; 8.
Selecting the Combination Weights; 9. Diffusion with Noisy Information
Exchanges; 10. Extensions and Further Considerations; Appendix A: Properties of
Kronecker Products; Appendix B: Graph Laplacian and Network Connectivity;
Appendix C: Stochastic Matrices; Appendix D: Block Maximum Norm; Appendix E:
Comparison with Consensus Strategies; References.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4233</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4233</id><created>2012-05-18</created><authors><author><keyname>Li</keyname><forenames>Yao</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Spasojevic</keyname><forenames>Predrag</forenames></author></authors><title>Three Schemes for Wireless Coded Broadcast to Heterogeneous Users</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted by Elsevier PhyCom NetCod Special Issue</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We study and compare three coded schemes for single-server wireless broadcast
of multiple description coded content to heterogeneous users. The users (sink
nodes) demand different number of descriptions over links with different packet
loss rates. The three coded schemes are based on the LT codes, growth codes,
and randomized chunked codes. The schemes are compared on the basis of the
total number of transmissions required to deliver the demands of all users,
which we refer to as the server (source) delivery time. We design the degree
distributions of LT codes by solving suitably defined linear optimization
problems, and numerically characterize the achievable delivery time for
different coding schemes. We find that including a systematic phase (uncoded
transmission) is significantly beneficial for scenarios with low demands, and
that coding is necessary for efficiently delivering high demands. Different
demand and error rate scenarios may require very different coding schemes.
Growth codes and chunked codes do not perform as well as optimized LT codes in
the heterogeneous communication scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4234</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4234</id><created>2012-05-19</created><updated>2012-05-25</updated><authors><author><keyname>Lande</keyname><forenames>D. V.</forenames></author></authors><title>Visualization of features of a series of measurements with
  one-dimensional cellular structure</title><categories>cs.LG</categories><comments>4 pages, russian language</comments><msc-class>68R</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the method of visualization of periodic constituents and
instability areas in series of measurements, being based on the algorithm of
smoothing out and concept of one-dimensional cellular automata. A method can be
used at the analysis of temporal series, related to the volumes of thematic
publications in web-space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4251</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4251</id><created>2012-05-18</created><updated>2012-05-25</updated><authors><author><keyname>Nosek</keyname><forenames>Brian A.</forenames></author><author><keyname>Spies</keyname><forenames>Jeffrey R.</forenames></author><author><keyname>Motyl</keyname><forenames>Matt</forenames></author></authors><title>Scientific Utopia: II. Restructuring incentives and practices to promote
  truth over publishability</title><categories>physics.soc-ph cs.DL</categories><comments>41 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An academic scientist's professional success depends on publishing.
Publishing norms emphasize novel, positive results. As such, disciplinary
incentives encourage design, analysis, and reporting decisions that elicit
positive results and ignore negative results. Prior reports demonstrate how
these incentives inflate the rate of false effects in published science. When
incentives favor novelty over replication, false results persist in the
literature unchallenged, reducing efficiency in knowledge accumulation.
Previous suggestions to address this problem are unlikely to be effective. For
example, a journal of negative results publishes otherwise unpublishable
reports. This enshrines the low status of the journal and its content. The
persistence of false findings can be meliorated with strategies that make the
fundamental but abstract accuracy motive - getting it right - competitive with
the more tangible and concrete incentive - getting it published. We develop
strategies for improving scientific practices and knowledge accumulation that
account for ordinary human motivations and self-serving biases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4261</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4261</id><created>2012-05-18</created><authors><author><keyname>Belkacem</keyname><forenames>Kouninef</forenames></author><author><keyname>Mohamed</keyname><forenames>Bouzerita</forenames></author></authors><title>Deployment of software components: Application to Wireless System</title><categories>cs.SE</categories><msc-class>68Nxx</msc-class><acm-class>D.2.10; D.2.11</acm-class><journal-ref>ARPN Journal of Systems and Software Volume 1 No. 3, JUNE 2011
  ISSN 2222-9833</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The wide variety of wireless devices brings to design mobile applications as
a collection of interchangeable software components adapted to the deployment
environment of the software. To ensure the proper functioning of the software
assembly and make a real enforcement in case of failures, the introduction of
concepts, models and tools necessary for the administration of these components
is crucial. This article proposes a method for deploying components in wireless
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4265</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4265</id><created>2012-05-18</created><updated>2014-03-31</updated><authors><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author><author><keyname>Koch</keyname><forenames>Christof</forenames></author></authors><title>Quantifying synergistic mutual information</title><categories>cs.IT math.IT q-bio.QM</categories><comments>15 pages; 12 page appendix. Lots of figures. Guided Self
  Organization: Inception. Ed: Mikhail Prokopenko. (2014); ISBN
  978-3-642-53734-9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantifying cooperation or synergy among random variables in predicting a
single target random variable is an important problem in many complex systems.
We review three prior information-theoretic measures of synergy and introduce a
novel synergy measure defined as the difference between the whole and the union
of its parts. We apply all four measures against a suite of binary circuits to
demonstrate that our measure alone quantifies the intuitive concept of synergy
across all examples. We show that for our measure of synergy that independent
predictors can have positive redundant information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4266</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4266</id><created>2012-05-18</created><authors><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Chernoff Bounds for Analysis of Rate-Compatible Sphere-Packing with
  Numerous Transmissions</title><categories>cs.IT math.IT</categories><comments>Submitted to 2012 Information Theory Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results by Chen et al. and Polyanskiy et al. explore using feedback to
approach capacity with short blocklengths. This paper explores Chernoff
bounding techniques to extend the rate-compatible sphere-packing (RCSP)
analysis proposed by Chen et al. to scenarios involving numerous
retransmissions and different step sizes in each incremental retransmission.
Williamson et al. employ exact RCSP computations for up to six transmissions.
However, exact RCSP computation with more than six retransmissions becomes
unwieldy because of joint error probabilities involving numerous chi-squared
distributions. This paper explores Chernoff approaches for upper and lower
bounds to provide support for computations involving more than six
transmissions.
  We present two versions of upper and lower bounds for the two-transmission
case. One of the versions is extended to the general case of $m$ transmissions
where $m \geq 1$. Computing the general bounds requires minimization of
exponential functions with the auxiliary parameters, but is less complex and
more stable than multiple rounds of numerical integration. These bounds also
provide a good estimate of the expected throughput and expected latency, which
are useful for optimization purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4279</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4279</id><created>2012-05-18</created><authors><author><keyname>Dey</keyname><forenames>Somdip</forenames></author></authors><title>SD-AREE: A New Modified Caesar Cipher Cryptographic Method Along with
  Bit-Manipulation to Exclude Repetition from a Message to be Encrypted</title><categories>cs.CR</categories><comments>The paper (manuscript) consist of 5 pages, in which there are 10
  figures and 1 table for results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the author presents a new cryptographic technique to exclude
the repetitive terms in a message, when it is to be encrypted, so that it
becomes almost impossible for a person to retrieve or predict the original
message from the encrypted message. In modern world, cryptography hackers try
to break a code or cryptographic algorithm or try to retrieve the key, which is
needed to encrypt a message, by analyzing the insertion or presence of
repetitive bits / characters (bytes) in the message and encrypted message to
find out the encryption algorithm or the key used for it. So it is must for a
good encryption method to exclude the repetitive terms such that no trace of
repetitions can be tracked down. For this reason we apply SD-AREE cryptographic
method to exclude repetitive terms from a message, which is to be encrypted. In
SD-AREE method the repetitive bits / characters are removed and there is no
trace of any repetition in the message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4295</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4295</id><created>2012-05-19</created><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author></authors><title>Efficient Methods for Unsupervised Learning of Probabilistic Models</title><categories>cs.LG cs.AI cs.IT cs.NE math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4298</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4298</id><created>2012-05-19</created><authors><author><keyname>Goldberg</keyname><forenames>Yoav</forenames></author></authors><title>Task-specific Word-Clustering for Part-of-Speech Tagging</title><categories>cs.CL</categories><comments>Rejected from ACL 2012 Short Papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the use of cluster features became ubiquitous in core NLP tasks, most
cluster features in NLP are based on distributional similarity. We propose a
new type of clustering criteria, specific to the task of part-of-speech
tagging. Instead of distributional similarity, these clusters are based on the
beha vior of a baseline tagger when applied to a large corpus. These cluster
features provide similar gains in accuracy to those achieved by
distributional-similarity derived clusters. Using both types of cluster
features together further improve tagging accuracies. We show that the method
is effective for both the in-domain and out-of-domain scenarios for English,
and for French, German and Italian. The effect is larger for out-of-domain
text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4304</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4304</id><created>2012-05-19</created><authors><author><keyname>Chopin</keyname><forenames>Nicolas</forenames><affiliation>ENSAE, CREST</affiliation></author><author><keyname>Gelman</keyname><forenames>Andrew</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Mengersen</keyname><forenames>Kerrie L.</forenames><affiliation>Queensland University of Technology</affiliation></author><author><keyname>Robert</keyname><forenames>Christian P.</forenames><affiliation>Universite Paris-Dauphine, IUF, and CREST</affiliation></author></authors><title>In praise of the referee</title><categories>stat.OT cs.DL physics.soc-ph</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a lively debate in many fields, including statistics and
related applied fields such as psychology and biomedical research, on possible
reforms of the scholarly publishing system. Currently, referees contribute so
much to improve scientific papers, both directly through constructive criticism
and indirectly through the threat of rejection. We discuss ways in which new
approaches to journal publication could continue to make use of the valuable
efforts of peer reviewers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4316</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4316</id><created>2012-05-19</created><authors><author><keyname>Caminati</keyname><forenames>Marco B.</forenames></author></authors><title>A simplified framework for first-order languages and its formalization
  in Mizar</title><categories>math.LO cs.LO</categories><comments>Ph.D. thesis, defended on January, 20th, 2012</comments><msc-class>03C07, 97N80, 03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A strictly formal, set-theoretical treatment of classical first-order logic
is given. Since this is done with the goal of a concrete Mizar formalization of
basic results (Lindenbaum lemma; Henkin, satisfiability, completeness and
Lowenheim-Skolem theorems) in mind, it turns into a systematic pursue of
simplification: we give up the notions of free occurrence, of derivation tree,
and study what inference rules are strictly needed to prove the mentioned
results. Afterwards, we discuss details of the actual Mizar implementation, and
give general techniques developed therein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4318</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4318</id><created>2012-05-19</created><authors><author><keyname>Ageyev</keyname><forenames>Dmytro</forenames></author><author><keyname>Abdalla</keyname><forenames>Haidara</forenames></author><author><keyname>Starkova</keyname><forenames>O.</forenames></author></authors><title>Mpls Overlay Network Sinthesis Method With Multilayer Graph Usage</title><categories>cs.NI</categories><journal-ref>Proceeding of 22nd International Crimean Conference Microwave and
  Telecommunication Technology (CriMiCo), 2012 pp. 385 - 386</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  l. Introduction Modern network are multi-layer by their structure. This
requires the development of new mathematical models, which would allow to
adequately describe the existing physical and logical connections between the
elements on its different levels, and to effectively solve the problems of
design. The paper formulates the problem of synthesis of structure of MPLS
network layered with the transport SDH network or WDM and suggests the method
of its solving. The solving is based on the application of mathematical model
of multi-layer graph. II, III, IV. Main Part During the planning of MPLS
networks it is necessary to determine the topology of both the networks: that
of transport one and that of MPLS one. This means that one needs to determine:
- what nodes of the transport network should support the MPLS functionality; -
in what way the LSR nodes should be connected via the transport network; - what
should be the bandwidth between LSR links. According to the general method for
solving the problem of synthesis of multiservice telecommunication systems with
the usage of multi-layer graphs, we have to synthesize the initial redundant .
The solving of the task above can be reduced to the finding of the multi-layer
minimum weight subgraph that provides the transfer of information flows with
the consideration of the requirements to the structure of the multilayer graph
[5] and the flows on its edges [6] observed at the applying of constraints to
the bandwidth of the edges of the multi-layer graph. V. Conclusion The paper
reduces the problem of design of multiservice telecommunication system with the
transferred flows to the problem of finding a multilayer minimum weight
subgraph with the consideration of constraints to the graph edges bandwidth. It
is shown that application of given method provide to reduce MPLS network cost
to 10 - 16 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4324</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4324</id><created>2012-05-19</created><updated>2012-07-18</updated><authors><author><keyname>Mac Carron</keyname><forenames>P&#xe1;draig</forenames></author><author><keyname>Kenna</keyname><forenames>Ralph</forenames></author></authors><title>Universal Properties of Mythological Networks</title><categories>physics.soc-ph cs.CL cs.SI</categories><comments>6 pages, 3 figures, 2 tables. Updated to incorporate corrections from
  EPL acceptance process</comments><journal-ref>EPL 99 (2012) 28002</journal-ref><doi>10.1209/0295-5075/99/28002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As in statistical physics, the concept of universality plays an important,
albeit qualitative, role in the field of comparative mythology. Here we apply
statistical mechanical tools to analyse the networks underlying three iconic
mythological narratives with a view to identifying common and distinguishing
quantitative features. Of the three narratives, an Anglo-Saxon and a Greek text
are mostly believed by antiquarians to be partly historically based while the
third, an Irish epic, is often considered to be fictional. Here we show that
network analysis is able to discriminate real from imaginary social networks
and place mythological narratives on the spectrum between them. Moreover, the
perceived artificiality of the Irish narrative can be traced back to anomalous
features associated with six characters. Considering these as amalgams of
several entities or proxies, renders the plausibility of the Irish text
comparable to the others from a network-theoretic point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4328</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4328</id><created>2012-05-19</created><authors><author><keyname>Lozano</keyname><forenames>George A.</forenames></author><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author><author><keyname>Gingras</keyname><forenames>Yves</forenames></author></authors><title>The weakening relationship between the Impact Factor and papers'
  citations in the digital age</title><categories>cs.DL physics.soc-ph</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Historically, papers have been physically bound to the journal in which they
were published but in the electronic age papers are available individually, no
longer tied to their respective journals. Hence, papers now can be read and
cited based on their own merits, independently of the journal's physical
availability, reputation, or Impact Factor. We compare the strength of the
relationship between journals' Impact Factors and the actual citations received
by their respective papers from 1902 to 2009. Throughout most of the 20th
century, papers' citation rates were increasingly linked to their respective
journals' Impact Factors. However, since 1990, the advent of the digital age,
the strength of the relation between Impact Factors and paper citations has
been decreasing. This decrease began sooner in physics, a field that was
quicker to make the transition into the electronic domain. Furthermore, since
1990, the proportion of highly cited papers coming from highly cited journals
has been decreasing, and accordingly, the proportion of highly cited papers not
coming from highly cited journals has also been increasing. Should this pattern
continue, it might bring an end to the use of the Impact Factor as a way to
evaluate the quality of journals, papers and researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4332</identifier>
 <datestamp>2012-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4332</id><created>2012-05-19</created><updated>2012-05-31</updated><authors><author><keyname>Wei</keyname><forenames>Yi-Peng</forenames></author><author><keyname>Lin</keyname><forenames>Shih-Chun</forenames></author><author><keyname>Lin</keyname><forenames>Yu-Hsiu</forenames></author><author><keyname>Su</keyname><forenames>Hsuan-Jung</forenames></author></authors><title>Graph-based Code Design for Quadratic-Gaussian Wyner-Ziv Problem with
  Arbitrary Side Information</title><categories>cs.IT math.IT</categories><comments>To appear, IEEE International Symposium on Information Theory (ISIT)
  Jul, 2012, corrected reference [13]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wyner-Ziv coding (WZC) is a compression technique using decoder side
information, which is unknown at the encoder, to help the reconstruction. In
this paper, we propose and implement a new WZC structure, called residual WZC,
for the quadratic-Gaussian Wyner-Ziv problem where side information can be
arbitrarily distributed. In our two-stage residual WZC, the source is quantized
twice and the input of the second stage is the quantization error (residue) of
the first stage. The codebook of the first stage quantizer must be
simultaneously good for source and channel coding, since it also acts as a
channel code at the decoder. Stemming from the non-ideal quantization at the
encoder, a problem of channel decoding beyond capacity is identified and solved
when we design the practical decoder. Moreover,by using the modified reinforced
belief-propagation quantization algorithm, the low-density parity check code
(LDPC), whose edge degree is optimized for channel coding, also performs well
as a source code. We then implement the residual WZC by an LDPC and a low
density generator matrix code (LDGM). The simulation results show that our
practical construction approaches the Wyner-Ziv bound. Compared with previous
works, our construction can offer more design lexibility in terms of
distribution of side information and practical code rate selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4336</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4336</id><created>2012-05-19</created><updated>2012-05-23</updated><authors><author><keyname>Thangavel</keyname><forenames>K.</forenames></author><author><keyname>Roselin</keyname><forenames>R.</forenames></author></authors><title>Fuzzy - Rough Feature Selection With {\Pi}- Membership Function For
  Mammogram Classification</title><categories>cs.CV</categories><comments>Due to Crucial Error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast cancer is the second leading cause for death among women and it is
diagnosed with the help of mammograms. Oncologists are miserably failed in
identifying the micro calcification at the early stage with the help of the
mammogram visually. In order to improve the performance of the breast cancer
screening, most of the researchers have proposed Computer Aided Diagnosis using
image processing. In this study mammograms are preprocessed and features are
extracted, then the abnormality is identified through the classification. If
all the extracted features are used, most of the cases are misidentified. Hence
feature selection procedure is sought. In this paper, Fuzzy-Rough feature
selection with {\pi} membership function is proposed. The selected features are
used to classify the abnormalities with help of Ant-Miner and Weka tools. The
experimental analysis shows that the proposed method improves the mammograms
classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4338</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4338</id><created>2012-05-19</created><authors><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Results on the Fundamental Gain of Memory-Assisted Universal Source
  Coding</title><categories>cs.IT math.IT</categories><comments>2012 IEEE International Symposium on Information Theory (ISIT '2012),
  Boston, MA, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require data processing to be performed on individual
pieces of data which are of finite sizes, e.g., files in cloud storage units
and packets in data networks. However, traditional universal compression
solutions would not perform well over the finite-length sequences. Recently, we
proposed a framework called memory-assisted universal compression that holds a
significant promise for reducing the amount of redundant data from the
finite-length sequences. The proposed compression scheme is based on the
observation that it is possible to learn source statistics (by memorizing
previous sequences from the source) at some intermediate entities and then
leverage the memorized context to reduce redundancy of the universal
compression of finite-length sequences. We first present the fundamental gain
of the proposed memory-assisted universal source coding over conventional
universal compression (without memorization) for a single parametric source.
Then, we extend and investigate the benefits of the memory-assisted universal
source coding when the data sequences are generated by a compound source which
is a mixture of parametric sources. We further develop a clustering technique
within the memory-assisted compression framework to better utilize the memory
by classifying the observed data sequences from a mixture of parametric
sources. Finally, we demonstrate through computer simulations that the proposed
joint memorization and clustering technique can achieve up to 6-fold
improvement over the traditional universal compression technique when a mixture
of non-binary Markov sources is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4343</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4343</id><created>2012-05-19</created><updated>2012-08-25</updated><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Medina</keyname><forenames>Andres Munoz</forenames></author></authors><title>New Analysis and Algorithm for Learning with Drifting Distributions</title><categories>cs.LG stat.ML</categories><comments>15 pages, 2 figures to be published in volume 7568 of the Lecture
  Notes in Computer Science series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new analysis of the problem of learning with drifting
distributions in the batch setting using the notion of discrepancy. We prove
learning bounds based on the Rademacher complexity of the hypothesis set and
the discrepancy of distributions both for a drifting PAC scenario and a
tracking scenario. Our bounds are always tighter and in some cases
substantially improve upon previous ones based on the $L_1$ distance. We also
present a generalization of the standard on-line to batch conversion to the
drifting scenario in terms of the discrepancy and arbitrary convex combinations
of hypotheses. We introduce a new algorithm exploiting these learning
guarantees, which we show can be formulated as a simple QP. Finally, we report
the results of preliminary experiments demonstrating the benefits of this
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4349</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4349</id><created>2012-05-19</created><authors><author><keyname>Goschin</keyname><forenames>Sergiu</forenames></author></authors><title>From Exact Learning to Computing Boolean Functions and Back Again</title><categories>cs.LG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the paper is to relate complexity measures associated with the
evaluation of Boolean functions (certificate complexity, decision tree
complexity) and learning dimensions used to characterize exact learning
(teaching dimension, extended teaching dimension). The high level motivation is
to discover non-trivial relations between exact learning of an unknown concept
and testing whether an unknown concept is part of a concept class or not.
Concretely, the goal is to provide lower and upper bounds of complexity
measures for one problem type in terms of the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4353</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4353</id><created>2012-05-19</created><authors><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Wu</keyname><forenames>Yuhua</forenames></author><author><keyname>L&#xf3;pez-P&#xe9;rez</keyname><forenames>David</forenames></author><author><keyname>Tao</keyname><forenames>Xiaofeng</forenames></author></authors><title>On Providing Downlink Services in Collocated Spectrum-Sharing Macro and
  Femto Networks</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Wireless Communications, December 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Femtocells have been considered by the wireless industry as a cost-effective
solution not only to improve indoor service providing, but also to unload
traffic from already overburdened macro networks. Due to spectrum availability
and network infrastructure considerations, a macro network may have to share
spectrum with overlaid femtocells. In spectrum-sharing macro and femto
networks, inter-cell interference caused by different transmission powers of
macrocell base stations (MBS) and femtocell access points (FAP), in conjunction
with potentially densely deployed femtocells, may create dead spots where
reliable services cannot be guaranteed to either macro or femto users. In this
paper, based on a thorough analysis of downlink (DL) outage probabilities (OP)
of collocated spectrum-sharing orthogonal frequency division multiple access
(OFDMA) based macro and femto networks, we devise a decentralized strategy for
an FAP to self-regulate its transmission power level and usage of radio
resources depending on its distance from the closest MBS. Simulation results
show that the derived closed-form lower bounds of DL OPs are tight, and the
proposed decentralized femtocell self-regulation strategy is able to guarantee
reliable DL services in targeted macro and femto service areas while providing
superior spatial reuse, for even a large number of spectrum-sharing femtocells
deployed per cell site.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4361</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4361</id><created>2012-05-19</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Fahad</forenames></author><author><keyname>Beg</keyname><forenames>Saira</forenames></author></authors><title>Transference &amp; Retrieval of Pulse-code modulation Audio over Short
  Messaging Service</title><categories>cs.NI cs.MM cs.SD</categories><comments>3,3, International Journal of Computer Applications (0975 -
  8887),Volume 32- No.10, October 2011</comments><journal-ref>International Journal of Computer Applications ,Volume 32-- No.10,
  October 2011</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper presents the method of transferring PCM (Pulse-Code Modulation)
based audio messages through SMS (Short Message Service) over GSM (Global
System for Mobile Communications) network. As SMS is text based service, and
could not send voice. Our method enables voice transferring through SMS, by
converting PCM audio into characters. Than Huffman coding compression technique
is applied in order to reduce numbers of characters which will latterly set as
payload text of SMS. Testing the said method we develop an application using
J2me platform
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4363</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4363</id><created>2012-05-19</created><authors><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Mori&#x161;</keyname><forenames>Ondrej</forenames></author></authors><title>Detours in Scope-Based Route Planning</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a dynamic scenario of the static route planning problem in road
networks. Particularly, we put accent on the most practical dynamic case -
increased edge weights (up to infinity). We show how to enhance the scope-based
route planning approach presented at ESA'11 to intuitively by-pass closures by
detours. Three variants of a detour &quot;admissibility&quot; are presented - from a
simple one with straightforward implementation through its enhanced version to
a full and very complex variant variant which always returns an optimal detour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4377</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4377</id><created>2012-05-19</created><updated>2013-01-29</updated><authors><author><keyname>Trapeznikov</keyname><forenames>Kirill</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author><author><keyname>Castanon</keyname><forenames>David</forenames></author></authors><title>Multi-Stage Classifier Design</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many classification systems, sensing modalities have different acquisition
costs. It is often {\it unnecessary} to use every modality to classify a
majority of examples. We study a multi-stage system in a prediction time cost
reduction setting, where the full data is available for training, but for a
test example, measurements in a new modality can be acquired at each stage for
an additional cost. We seek decision rules to reduce the average measurement
acquisition cost. We formulate an empirical risk minimization problem (ERM) for
a multi-stage reject classifier, wherein the stage $k$ classifier either
classifies a sample using only the measurements acquired so far or rejects it
to the next stage where more attributes can be acquired for a cost. To solve
the ERM problem, we show that the optimal reject classifier at each stage is a
combination of two binary classifiers, one biased towards positive examples and
the other biased towards negative examples. We use this parameterization to
construct stage-by-stage global surrogate risk, develop an iterative algorithm
in the boosting framework and present convergence and generalization results.
We test our work on synthetic, medical and explosives detection datasets. Our
results demonstrate that substantial cost reduction without a significant
sacrifice in accuracy is achievable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4378</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4378</id><created>2012-05-19</created><updated>2012-06-18</updated><authors><author><keyname>Zhu</keyname><forenames>Yin</forenames></author><author><keyname>Zheng</keyname><forenames>Yu</forenames></author><author><keyname>Zhang</keyname><forenames>Liuhang</forenames></author><author><keyname>Santani</keyname><forenames>Darshan</forenames></author><author><keyname>Xie</keyname><forenames>Xing</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author></authors><title>Inferring Taxi Status Using GPS Trajectories</title><categories>cs.AI cs.DB</categories><msc-class>68T20</msc-class><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we infer the statuses of a taxi, consisting of occupied,
non-occupied and parked, in terms of its GPS trajectory. The status information
can enable urban computing for improving a city's transportation systems and
land use planning. In our solution, we first identify and extract a set of
effective features incorporating the knowledge of a single trajectory,
historical trajectories and geographic data like road network. Second, a
parking status detection algorithm is devised to find parking places (from a
given trajectory), dividing a trajectory into segments (i.e.,
sub-trajectories). Third, we propose a two-phase inference model to learn the
status (occupied or non-occupied) of each point from a taxi segment. This model
first uses the identified features to train a local probabilistic classifier
and then carries out a Hidden Semi-Markov Model (HSMM) for globally considering
long term travel patterns. We evaluated our method with a large-scale
real-world trajectory dataset generated by 600 taxis, showing the advantages of
our method over baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4384</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4384</id><created>2012-05-20</created><updated>2014-01-20</updated><authors><author><keyname>Papadopoulos</keyname><forenames>Fragkiskos</forenames></author><author><keyname>Psomas</keyname><forenames>Constantinos</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Network Mapping by Replaying Hyperbolic Growth</title><categories>cs.SI cond-mat.stat-mech cs.NI physics.soc-ph</categories><comments>14 pages, to appear in ToN</comments><journal-ref>IEEE/ACM Transactions on Networking (ToN), Vol. 23, Issue 1, pp.
  198-211, 2015</journal-ref><doi>10.1109/TNET.2013.2294052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have shown a promising progress in understanding geometric
underpinnings behind the structure, function, and dynamics of many complex
networks in nature and society. However these promises cannot be readily
fulfilled and lead to important practical applications, without a simple,
reliable, and fast network mapping method to infer the latent geometric
coordinates of nodes in a real network. Here we present HyperMap, a simple
method to map a given real network to its hyperbolic space. The method utilizes
a recent geometric theory of complex networks modeled as random geometric
graphs in hyperbolic spaces. The method replays the network's geometric growth,
estimating at each time step the hyperbolic coordinates of new nodes in a
growing network by maximizing the likelihood of the network snapshot in the
model. We apply HyperMap to the AS Internet, and find that: 1) the method
produces meaningful results, identifying soft communities of ASs belonging to
the same geographic region; 2) the method has a remarkable predictive power:
using the resulting map, we can predict missing links in the Internet with high
precision, outperforming popular existing methods; and 3) the resulting map is
highly navigable, meaning that a vast majority of greedy geometric routing
paths are successful and low-stretch. Even though the method is not without
limitations, and is open for improvement, it occupies a unique attractive
position in the space of trade-offs between simplicity, accuracy, and
computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4385</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4385</id><created>2012-05-20</created><updated>2014-02-21</updated><authors><author><keyname>Bahrami</keyname><forenames>Mehdi</forenames></author><author><keyname>Bahrami</keyname><forenames>Mohammad</forenames></author></authors><title>An overview to Software Architecture in Intrusion Detection System</title><categories>cs.SE cs.CR cs.DC cs.NI</categories><comments>8 Pages, International Journal of Soft Computing and Software
  Engineering [JSCSE]. arXiv admin note: text overlap with arXiv:1101.0241 by
  other authors</comments><acm-class>D.2.11</acm-class><journal-ref>International Journal of Soft Computing and Software Engineering
  [JSCSE], Vol. 1, No. 1, pp. 1-8, 2011</journal-ref><doi>10.7321/jscse.v1.n1.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today by growing network systems, security is a key feature of each network
infrastructure. Network Intrusion Detection Systems (IDS) provide defense model
for all security threats which are harmful to any network. The IDS could detect
and block attack-related network traffic. The network control is a complex
model. Implementation of an IDS could make delay in the network. Several
software-based network intrusion detection systems are developed. However, the
model has a problem with high speed traffic. This paper reviews of many type of
software architecture in intrusion detection systems and describes the design
and implementation of a high-performance network intrusion detection system
that combines the use of software-based network intrusion detection sensors and
a network processor board. The network processor which is a hardware-based
model could acts as a customized load balancing splitter. This model cooperates
with a set of modified content-based network intrusion detection sensors rather
than IDS in processing network traffic and controls the high-speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4387</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4387</id><created>2012-05-20</created><authors><author><keyname>Goldberg</keyname><forenames>Yoav</forenames></author><author><keyname>Elhadad</keyname><forenames>Michael</forenames></author></authors><title>Precision-biased Parsing and High-Quality Parse Selection</title><categories>cs.CL</categories><comments>Rejected from EMNLP 2012 (among others)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce precision-biased parsing: a parsing task which favors precision
over recall by allowing the parser to abstain from decisions deemed uncertain.
We focus on dependency-parsing and present an ensemble method which is capable
of assigning parents to 84% of the text tokens while being over 96% accurate on
these tokens. We use the precision-biased parsing task to solve the related
high-quality parse-selection task: finding a subset of high-quality (accurate)
trees in a large collection of parsed text. We present a method for choosing
over a third of the input trees while keeping unlabeled dependency parsing
accuracy of 97% on these trees. We also present a method which is not based on
an ensemble but rather on directly predicting the risk associated with
individual parser decisions. In addition to its efficiency, this method
demonstrates that a parsing system can provide reasonable estimates of
confidence in its predictions without relying on ensembles or aggregate corpus
counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4389</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4389</id><created>2012-05-20</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Minimum Mean-Squared Error Iterative Successive Parallel Arbitrated
  Decision Feedback Detectors for DS-CDMA Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose minimum mean squared error (MMSE) iterative
successive parallel arbitrated decision feedback (DF) receivers for direct
sequence code division multiple access (DS-CDMA) systems. We describe the MMSE
design criterion for DF multiuser detectors along with successive, parallel and
iterative interference cancellation structures. A novel efficient DF structure
that employs successive cancellation with parallel arbitrated branches and a
near-optimal low complexity user ordering algorithm are presented. The proposed
DF receiver structure and the ordering algorithm are then combined with
iterative cascaded DF stages for mitigating the deleterious effects of error
propagation for convolutionally encoded systems with both Viterbi and turbo
decoding as well as for uncoded schemes. We mathematically study the relations
between the MMSE achieved by the analyzed DF structures, including the novel
scheme, with imperfect and perfect feedback. Simulation results for an uplink
scenario assess the new iterative DF detectors against linear receivers and
evaluate the effects of error propagation of the new cancellation methods
against existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4390</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4390</id><created>2012-05-20</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Reduced-Rank Adaptive Filtering Based on Joint Iterative Optimization of
  Adaptive Filters</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a novel adaptive reduced-rank filtering scheme based on
joint iterative optimization of adaptive filters. The novel scheme consists of
a joint iterative optimization of a bank of full-rank adaptive filters that
forms the projection matrix and an adaptive reduced-rank filter that operates
at the output of the bank of filters. We describe minimum mean-squared error
(MMSE) expressions for the design of the projection matrix and the reduced-rank
filter and low-complexity normalized least-mean squares (NLMS) adaptive
algorithms for its efficient implementation. Simulations for an interference
suppression application show that the proposed scheme outperforms in
convergence and tracking the state-ofthe- art reduced-rank schemes at
significantly lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4391</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4391</id><created>2012-05-20</created><authors><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Wang</keyname><forenames>L.</forenames></author><author><keyname>Fa</keyname><forenames>R.</forenames></author></authors><title>Adaptive Reduced-Rank LCMV Beamforming Algorithms Based on Joint
  Iterative Optimization of Filters: Design and Analysis</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents reduced-rank linearly constrained minimum variance (LCMV)
beamforming algorithms based on joint iterative optimization of filters. The
proposed reduced-rank scheme is based on a constrained joint iterative
optimization of filters according to the minimum variance criterion. The
proposed optimization procedure adjusts the parameters of a projection matrix
and an adaptive reducedrank filter that operates at the output of the bank of
filters. We describe LCMV expressions for the design of the projection matrix
and the reduced-rank filter. We then describe stochastic gradient and develop
recursive least-squares adaptive algorithms for their efficient implementation
along with automatic rank selection techniques. An analysis of the stability
and the convergence properties of the proposed algorithms is presented and
semi-analytical expressions are derived for predicting their mean squared error
(MSE) performance. Simulations for a beamforming application show that the
proposed scheme and algorithms outperform in convergence and tracking the
existing full-rank and reduced-rank algorithms while requiring comparable
complexity
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4397</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4397</id><created>2012-05-20</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>On Murty-Simon Conjecture</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is diameter two edge-critical if its diameter is two and the deletion
of any edge increases the diameter. Murty and Simon conjectured that the number
of edges in a diameter two edge-critical graph on $n$ vertices is at most
$\lfloor \frac{n^{2}}{4} \rfloor$ and the extremal graph is the complete
bipartite graph $K_{\lfloor \frac{n}{2} \rfloor, \lceil \frac{n}{2} \rceil}$.
In the series papers [8-10], the Murty-Simon Conjecture stated by Haynes et al
is not the original conjecture, indeed, it is only for the diameter two
edge-critical graphs of even order. Haynes et al proved the conjecture for the
graphs whose complements have diameter three but only with even vertices. In
this paper, we prove the Murty-Simon Conjecture for the graphs whose
complements have diameter three, not only with even vertices but also odd ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4418</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4418</id><created>2012-05-20</created><authors><author><keyname>Baccini</keyname><forenames>Alberto</forenames></author><author><keyname>Barabesi</keyname><forenames>Lucio</forenames></author><author><keyname>Marcheselli</keyname><forenames>Marzia</forenames></author><author><keyname>Pratelli</keyname><forenames>Luca</forenames></author></authors><title>Statistical inference on the h-index with an application to
  top-scientist performance</title><categories>stat.AP cs.DL physics.soc-ph</categories><comments>14 pages, 3 tables</comments><journal-ref>Journal of Informetrics, Volume 6, Issue 4, October 2012, Pages
  721 - 728</journal-ref><doi>10.1016/j.joi.2012.07.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the huge amount of literature on h-index, few papers have been
devoted to the statistical analysis of h-index when a probabilistic
distribution is assumed for citation counts. The present contribution relies on
showing the available inferential techniques, by providing the details for
proper point and set estimation of the theoretical h-index. Moreover, some
issues on simultaneous inference - aimed to produce suitable scholar
comparisons - are carried out. Finally, the analysis of the citation dataset
for the Nobel Laureates (in the last five years) and for the Fields medallists
(from 2002 onward) is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4423</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4423</id><created>2012-05-20</created><updated>2012-07-26</updated><authors><author><keyname>de Reyna</keyname><forenames>Juan Arias</forenames></author><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author><author><keyname>van de Lune</keyname><forenames>Jan</forenames></author></authors><title>On the sign of the real part of the Riemann zeta-function</title><categories>math.NT cs.NA stat.CO</categories><comments>22 pages, 3 tables. To appear in Proceedings of the International
  Number Theory Conference in Memory of Alf van der Poorten (Newcastle,
  Australia, 2011)</comments><msc-class>11M06 (Primary) 65-04, 65B99, 65E99, 65T40 (Secondary)</msc-class><acm-class>F.2.1</acm-class><journal-ref>Springer Proceedings in Mathematics and Statistics, vol. 43, 2013,
  75-97</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the distribution of $\arg\zeta(\sigma+it)$ on fixed lines $\sigma
&gt; \frac12$, and in particular the density \[d(\sigma) = \lim_{T \rightarrow
+\infty}
  \frac{1}{2T}
  |\{t \in [-T,+T]: |\arg\zeta(\sigma+it)| &gt; \pi/2\}|\,,\] and the closely
related density \[d_{-}(\sigma) = \lim_{T \rightarrow +\infty}
  \frac{1}{2T}
  |\{t \in [-T,+T]: \Re\zeta(\sigma+it) &lt; 0\}|\,.\] Using classical results of
Bohr and Jessen, we obtain an explicit expression for the characteristic
function $\psi_\sigma(x)$ associated with $\arg\zeta(\sigma+it)$. We give
explicit expressions for $d(\sigma)$ and $d_{-}(\sigma)$ in terms of
$\psi_\sigma(x)$. Finally, we give a practical algorithm for evaluating these
expressions to obtain accurate numerical values of $d(\sigma)$ and
$d_{-}(\sigma)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4431</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4431</id><created>2012-05-20</created><updated>2013-03-15</updated><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Paulo</keyname><forenames>Damon</forenames></author></authors><title>Large Social Networks can be Targeted for Viral Marketing with Small
  Seed Sets</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In a &quot;tipping&quot; model, each node in a social network, representing an
individual, adopts a behavior if a certain number of his incoming neighbors
previously held that property. A key problem for viral marketers is to
determine an initial &quot;seed&quot; set in a network such that if given a property then
the entire network adopts the behavior. Here we introduce a method for quickly
finding seed sets that scales to very large networks. Our approach finds a set
of nodes that guarantees spreading to the entire network under the tipping
model. After experimentally evaluating 31 real-world networks, we found that
our approach often finds such sets that are several orders of magnitude smaller
than the population size. Our approach also scales well - on a Friendster
social network consisting of 5.6 million nodes and 28 million edges we found a
seed sets in under 3.6 hours. We also find that highly clustered local
neighborhoods and dense network-wide community structure together suppress the
ability of a trend to spread under the tipping model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4450</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4450</id><created>2012-05-20</created><updated>2012-09-19</updated><authors><author><keyname>Ye</keyname><forenames>Chengxi</forenames></author><author><keyname>Lin</keyname><forenames>Yuxu</forenames></author><author><keyname>Song</keyname><forenames>Mingli</forenames></author><author><keyname>Chen</keyname><forenames>Chun</forenames></author><author><keyname>Jacobs</keyname><forenames>David W.</forenames></author></authors><title>Spectral Graph Cut from a Filtering Point of View</title><categories>cs.CV</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze spectral graph theory based image segmentation algorithms and show
there is a natural connection with edge preserving filtering. Based on this
connection we show that the normalized cut algorithm is equivalent to repeated
application of bilateral filtering. Then, using this interpretation we present
and implement a fast normalized cut algorithm. Experiments show that our
implementation can solve the original optimization problem with a 10x-100x
speedup. Furthermore, we show this connection makes possible a new model for
segmentation called conditioned normalized cut that easily incorporates image
patches in color and demonstrate how this problem can be solved with edge
preserving filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4454</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4454</id><created>2012-05-20</created><authors><author><keyname>Zhong</keyname><forenames>Peng</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Combined Decode-Forward and Layered Noisy Network Coding Schemes for
  Relay Channels</title><categories>cs.IT math.IT</categories><comments>To appear in International Symposium on Information Theory (ISIT)
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two coding schemes combining decode-forward (DF) and noisy network
coding (NNC) with different flavors. The first is a combined DF-NNC scheme for
the one-way relay channel which includes both DF and NNC as special cases by
performing rate splitting, partial block Markov encoding and NNC. The second
combines two different DF strategies and layered NNC for the two-way relay
channel. One DF strategy performs coherent block Markov encoding at the source
at the cost of power splitting at the relay, the other performs independent
source and relay encoding but with full relay power, and layered NNC allows a
different compression rate for each destination. Analysis and simulation show
that both proposed schemes supersede each individual scheme and take full
advantage of both DF and NNC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4457</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4457</id><created>2012-05-20</created><authors><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Thakur</keyname><forenames>Manoj Rameshchandra</forenames></author></authors><title>A Hybrid Approach Towards Intrusion Detection Based on Artificial Immune
  System and Soft Computing</title><categories>cs.CR</categories><comments>11 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of works in the field of intrusion detection have been based on
Artificial Immune System and Soft Computing. Artificial Immune System based
approaches attempt to leverage the adaptability, error tolerance, self-
monitoring and distributed nature of Human Immune Systems. Whereas Soft
Computing based approaches are instrumental in developing fuzzy rule based
systems for detecting intrusions. They are computationally intensive and apply
machine learning (both supervised and unsupervised) techniques to detect
intrusions in a given system. A combination of these two approaches could
provide significant advantages for intrusion detection. In this paper we
attempt to leverage the adaptability of Artificial Immune System and the
computation intensive nature of Soft Computing to develop a system that can
effectively detect intrusions in a given network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4458</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4458</id><created>2012-05-20</created><updated>2012-06-17</updated><authors><author><keyname>Bonnet</keyname><forenames>R&#xe9;mi</forenames><affiliation>LSV, ENS Cachan and CNRS, France</affiliation></author><author><keyname>FInkel</keyname><forenames>Alain</forenames><affiliation>LSV, ENS Cachan and CNRS, France</affiliation></author><author><keyname>Leroux</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>LaBRI, Universit&#xe9; de Bordeaux and CNRS, France</affiliation></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames><affiliation>LSV, ENS Cachan, France and LaBRI, Universit&#xe9; de Bordeaux and CNRS, France</affiliation></author></authors><title>Model Checking Vector Addition Systems with one zero-test</title><categories>cs.DM cs.FL</categories><proxy>LMCS</proxy><acm-class>F.1.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (June 19,
  2012) lmcs:870</journal-ref><doi>10.2168/LMCS-8(2:11)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a variation of the Karp-Miller algorithm to compute, in a forward
manner, a finite representation of the cover (i.e., the downward closure of the
reachability set) of a vector addition system with one zero-test. This
algorithm yields decision procedures for several problems for these systems,
open until now, such as place-boundedness or LTL model-checking. The proof
techniques to handle the zero-test are based on two new notions of cover: the
refined and the filtered cover. The refined cover is a hybrid between the
reachability set and the classical cover. It inherits properties of the
reachability set: equality of two refined covers is undecidable, even for usual
Vector Addition Systems (with no zero-test), but the refined cover of a Vector
Addition System is a recursive set. The second notion of cover, called the
filtered cover, is the central tool of our algorithms. It inherits properties
of the classical cover, and in particular, one can effectively compute a finite
representation of this set, even for Vector Addition Systems with one
zero-test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4463</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4463</id><created>2012-05-20</created><updated>2012-12-29</updated><authors><author><keyname>Aly</keyname><forenames>Salah A.</forenames></author></authors><title>Pilgrims Face Recognition Dataset -- HUFRD</title><categories>cs.CV cs.CY</categories><comments>5 pages, 13 images, 1 table of a new HUFRD work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we define a new pilgrims face recognition dataset, called HUFRD
dataset. The new developed dataset presents various pilgrims' images taken from
outside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrah
seasons. Such dataset will be used to test our developed facial recognition and
detection algorithms, as well as assess in the missing and found recognition
system \cite{crowdsensing}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4467</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4467</id><created>2012-05-20</created><updated>2012-05-23</updated><authors><author><keyname>Ciulla</keyname><forenames>Fabio</forenames></author><author><keyname>Mocanu</keyname><forenames>Delia</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Beating the news using Social Media: the case study of American Idol</title><categories>physics.soc-ph cs.HC cs.SI</categories><comments>6 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a contribution to the debate on the predictability of social
events using big data analytics. We focus on the elimination of contestants in
the American Idol TV shows as an example of a well defined electoral phenomenon
that each week draws millions of votes in the USA. We provide evidence that
Twitter activity during the time span defined by the TV show airing and the
voting period following it, correlates with the contestants ranking and allows
the anticipation of the voting outcome. Furthermore, the fraction of Tweets
that contain geolocation information allows us to map the fanbase of each
contestant, both within the US and abroad, showing that strong regional
polarizations occur. Although American Idol voting is just a minimal and
simplified version of complex societal phenomena such as political elections,
this work shows that the volume of information available in online systems
permits the real time gathering of quantitative indicators anticipating the
future unfolding of opinion formation events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4471</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4471</id><created>2012-05-20</created><authors><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Jin</keyname><forenames>Yuzhe</forenames></author></authors><title>Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector
  Correlation</title><categories>cs.IT cs.LG math.IT stat.ME stat.ML</categories><comments>Invited review paper of 2012 International Conference on Signal
  Processing and Communications (SPCOM 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work discusses the problem of sparse signal recovery when there is
correlation among the values of non-zero entries. We examine intra-vector
correlation in the context of the block sparse model and inter-vector
correlation in the context of the multiple measurement vector model, as well as
their combination. Algorithms based on the sparse Bayesian learning are
presented and the benefits of incorporating correlation at the algorithm level
are discussed. The impact of correlation on the limits of support recovery is
also discussed highlighting the different impact intra-vector and inter-vector
correlations have on such limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4476</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4476</id><created>2012-05-20</created><updated>2013-02-22</updated><authors><author><keyname>Akdemir</keyname><forenames>Deniz</forenames></author><author><keyname>Heslot</keyname><forenames>Nicolas</forenames></author></authors><title>Soft Rule Ensembles for Statistical Learning</title><categories>stat.ML cs.LG stat.AP</categories><comments>arXiv admin note: text overlap with arXiv:1112.3699</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article supervised learning problems are solved using soft rule
ensembles. We first review the importance sampling learning ensembles (ISLE)
approach that is useful for generating hard rules. The soft rules are then
obtained with logistic regression from the corresponding hard rules. In order
to deal with the perfect separation problem related to the logistic regression,
Firth's bias corrected likelihood is used. Various examples and simulation
results show that soft rule ensembles can improve predictive performance over
hard rule ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4477</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4477</id><created>2012-05-20</created><authors><author><keyname>Patnaik</keyname><forenames>Debprakash</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Laxman</keyname><forenames>Srivatsan</forenames></author><author><keyname>Chandramouli</keyname><forenames>Badrish</forenames></author></authors><title>Streaming Algorithms for Pattern Discovery over Dynamically Changing
  Event Sequences</title><categories>cs.LG cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering frequent episodes over event sequences is an important data
mining task. In many applications, events constituting the data sequence arrive
as a stream, at furious rates, and recent trends (or frequent episodes) can
change and drift due to the dynamical nature of the underlying event generation
process. The ability to detect and track such the changing sets of frequent
episodes can be valuable in many application scenarios. Current methods for
frequent episode discovery are typically multipass algorithms, making them
unsuitable in the streaming context. In this paper, we propose a new streaming
algorithm for discovering frequent episodes over a window of recent events in
the stream. Our algorithm processes events as they arrive, one batch at a time,
while discovering the top frequent episodes over a window consisting of several
batches in the immediate past. We derive approximation guarantees for our
algorithm under the condition that frequent episodes are approximately
well-separated from infrequent ones in every batch of the window. We present
extensive experimental evaluations of our algorithm on both real and synthetic
data. We also present comparisons with baselines and adaptations of streaming
algorithms from itemset mining literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4481</identifier>
 <datestamp>2012-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4481</id><created>2012-05-20</created><updated>2012-10-01</updated><authors><author><keyname>Ouyang</keyname><forenames>Hua</forenames></author><author><keyname>Gray</keyname><forenames>Alexander</forenames></author></authors><title>Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by
  Exploiting Structure</title><categories>cs.LG stat.CO stat.ML</categories><comments>Full length version of ICML'12 with all proofs. In this version, a
  bug in proving Theorem 6 is fixed. We'd like to thank Dr. Francesco Orabona
  for pointing it out</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the stochastic minimization of nonsmooth convex loss
functions, a central problem in machine learning. We propose a novel algorithm
called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which
exploits the structure of common nonsmooth loss functions to achieve optimal
convergence rates for a class of problems including SVMs. It is the first
stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing
nonsmooth loss functions (with strong convexity). The fast rates are confirmed
by empirical comparisons, in which ANSGD significantly outperforms previous
subgradient descent algorithms including SGD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4484</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4484</id><created>2012-05-21</created><updated>2014-11-16</updated><authors><author><keyname>Barak</keyname><forenames>Boaz</forenames></author><author><keyname>Brand&#xe3;o</keyname><forenames>Fernando G. S. L.</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author><author><keyname>Kelner</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Hypercontractivity, Sum-of-Squares Proofs, and their Applications</title><categories>cs.CC cs.DS quant-ph</categories><comments>v1: 52 pages. v2: 53 pages, fixed small bugs in proofs of section 6
  (on UG integrality gaps) and section 7 (on 2-&gt;4 norm of random matrices).
  Added comments about real-vs-complex random matrices and about the
  k-extendable vs k-extendable &amp; PPT hierarchies. v3: fixed mistakes in random
  matrix section. The result now holds only for matrices with random entries
  instead of random columns</comments><journal-ref>Proc. STOC 2012, pp. 307--326</journal-ref><doi>10.1145/2213977.2214006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of approximating the 2-&gt;q norm of
linear operators (defined as ||A||_{2-&gt;q} = sup_v ||Av||_q/||v||_2), as well as
connections between this question and issues arising in quantum information
theory and the study of Khot's Unique Games Conjecture (UGC). We show the
following:
  1. For any constant even integer q&gt;=4, a graph $G$ is a &quot;small-set expander&quot;
if and only if the projector into the span of the top eigenvectors of G's
adjacency matrix has bounded 2-&gt;q norm. As a corollary, a good approximation to
the 2-&gt;q norm will refute the Small-Set Expansion Conjecture--a close variant
of the UGC. We also show that such a good approximation can be obtained in
exp(n^(2/q)) time, thus obtaining a different proof of the known subexponential
algorithm for Small Set Expansion.
  2. Constant rounds of the &quot;Sum of Squares&quot; semidefinite programing hierarchy
certify an upper bound on the 2-&gt;4 norm of the projector to low-degree
polynomials over the Boolean cube, as well certify the unsatisfiability of the
&quot;noisy cube&quot; and &quot;short code&quot; based instances of Unique Games considered by
prior works. This improves on the previous upper bound of exp(poly log n)
rounds (for the &quot;short code&quot;), as well as separates the &quot;Sum of
Squares&quot;/&quot;Lasserre&quot; hierarchy from weaker hierarchies that were known to
require omega(1) rounds.
  3. We show reductions between computing the 2-&gt;4 norm and computing the
injective tensor norm of a tensor, a problem with connections to quantum
information theory. Three corollaries are: (i) the 2-&gt;4 norm is NP-hard to
approximate to precision inverse-polynomial in the dimension, (ii) the 2-&gt;4
norm does not have a good approximation (in the sense above) unless 3-SAT can
be solved in time exp(sqrt(n) polylog(n)), and (iii) known algorithms for the
quantum separability problem imply a non-trivial additive approximation for the
2-&gt;4 norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4487</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4487</id><created>2012-05-21</created><authors><author><keyname>Rajesh</keyname><forenames>V.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijaya</forenames></author></authors><title>CDMA Based Interconnect Mechanism for SOPC</title><categories>cs.ET</categories><comments>IJCSI Vol.9 Issue 2, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Network-on-chip (NoC) designs consisting of large pack of Intellectual
Property (IP) blocks (cores) on the same silicon die is becoming technically
possible nowadays. But, the communication between the IP Cores is the main
issue in recent years. This paper presents the design of a Code Division
Multiple Access (CDMA) based wrapper interconnect as a component of System on
programmable chip (SOPC) builder to communicate between IP cores. In the
proposal, only bus lines that carry address and data signals are CDMA coded.
CDMA technology has better data integrity, channel continuity, channel
isolation, and also mainly it reduces the no.of lines in the bus architecture
for transmitting the data from master to slave.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4489</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4489</id><created>2012-05-21</created><authors><author><keyname>Mohanty</keyname><forenames>Saraju P.</forenames></author></authors><title>ISWAR: An Imaging System with Watermarking and Attack Resilience</title><categories>cs.CR cs.DL cs.MM</categories><acm-class>H.2.7; H.3.7; K.4.4; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of internet technology, easy transfer of digital
multimedia is feasible. However, this kind of convenience with which authorized
users can access information, turns out to be a mixed blessing due to
information piracy. The emerging field of Digital Rights Management (DRM)
systems addresses issues related to the intellectual property rights of digital
content. In this paper, an object-oriented (OO) DRM system, called &quot;Imaging
System with Watermarking and Attack Resilience&quot; (ISWAR), is presented that
generates and authenticates color images with embedded mechanisms for
protection against infringement of ownership rights as well as security
attacks. In addition to the methods, in the object-oriented sense, for
performing traditional encryption and decryption, the system implements methods
for visible and invisible watermarking. This paper presents one visible and one
invisible watermarking algorithm that have been integrated in the system. The
qualitative and quantitative results obtained for these two watermarking
algorithms with several benchmark images indicate that high-quality watermarked
images are produced by the algorithms. With the help of experimental results it
is demonstrated that the presented invisible watermarking techniques are
resilient to the well known benchmark attacks and hence a fail-safe method for
providing constant protection to ownership rights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4545</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4545</id><created>2012-05-21</created><authors><author><keyname>Feinerman</keyname><forenames>Ofer</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author></authors><title>Memory Lower Bounds for Randomized Collaborative Search and Applications
  to Biology</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Initial knowledge regarding group size can be crucial for collective
performance. We study this relation in the context of the {\em Ants Nearby
Treasure Search (ANTS)} problem \cite{FKLS}, which models natural cooperative
foraging behavior such as that performed by ants around their nest. In this
problem, $k$ (probabilistic) agents, initially placed at some central location,
collectively search for a treasure on the two-dimensional grid. The treasure is
placed at a target location by an adversary and the goal is to find it as fast
as possible as a function of both $k$ and $D$, where $D$ is the (unknown)
distance between the central location and the target. It is easy to see that
$T=\Omega(D+D^2/k)$ time units are necessary for finding the treasure.
Recently, it has been established that $O(T)$ time is sufficient if the agents
know their total number $k$ (or a constant approximation of it), and enough
memory bits are available at their disposal \cite{FKLS}. In this paper, we
establish lower bounds on the agent memory size required for achieving certain
running time performances. To the best our knowledge, these bounds are the
first non-trivial lower bounds for the memory size of probabilistic searchers.
For example, for every given positive constant $\epsilon$, terminating the
search by time $O(\log^{1-\epsilon} k \cdot T)$ requires agents to use
$\Omega(\log\log k)$ memory bits. Such distributed computing bounds may provide
a novel, strong tool for the investigation of complex biological systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4546</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4546</id><created>2012-05-21</created><authors><author><keyname>Kim</keyname><forenames>Myunghwan</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Latent Multi-group Membership Graph Model</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>10 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the Latent Multi-group Membership Graph (LMMG) model, a model of
networks with rich node feature structure. In the LMMG model, each node belongs
to multiple groups and each latent group models the occurrence of links as well
as the node feature structure. The LMMG can be used to summarize the network
structure, to predict links between the nodes, and to predict missing features
of a node. We derive efficient inference and learning algorithms and evaluate
the predictive performance of the LMMG on several social and document network
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4550</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4550</id><created>2012-05-21</created><updated>2013-02-07</updated><authors><author><keyname>Etesi</keyname><forenames>Gabor</forenames></author></authors><title>A proof of the Geroch-Horowitz-Penrose formulation of the strong cosmic
  censor conjecture motivated by computability theory</title><categories>gr-qc cs.CC</categories><comments>16pp, LaTeX, no figures. Final published version</comments><journal-ref>Int. Journ. Theor. Phys. 52(3), 946-960 (2013)</journal-ref><doi>10.1007/s10773-012-1407-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a proof of a mathematical version of the strong
cosmic censor conjecture attributed to Geroch-Horowitz and Penrose but
formulated explicitly by Wald. The proof is based on the existence of
future-inextendible causal curves in causal pasts of events on the future
Cauchy horizon in a non-globally hyperbolic space-time. By examining explicit
non-globally hyperbolic space-times we find that in case of several physically
relevant solutions these future-inextendible curves have in fact infinite
length. This way we recognize a close relationship between asymptotically flat
or anti-de Sitter, physically relevant extendible space-times and the so-called
Malament-Hogarth space-times which play a central role in recent investigations
in the theory of &quot;gravitational computers&quot;. This motivates us to exhibit a more
sharp, more geometric formulation of the strong cosmic censor conjecture,
namely &quot;all physically relevant, asymptotically flat or anti-de Sitter but
non-globally hyperbolic space-times are Malament-Hogarth ones&quot;.
  Our observations may indicate a natural but hidden connection between the
strong cosmic censorship scenario and the Church-Turing thesis revealing an
unexpected conceptual depth beneath both conjectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4551</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4551</id><created>2012-05-21</created><authors><author><keyname>Aubel</keyname><forenames>C&#xe9;line</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Pope</keyname><forenames>Graeme</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Sparse Signal Separation in Redundant Dictionaries</title><categories>cs.IT math.IT</categories><comments>Proc. of IEEE International Symposium on Information Theory (ISIT),
  Boston, MA, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate a unified framework for the separation of signals that are
sparse in &quot;morphologically&quot; different redundant dictionaries. This formulation
incorporates the so-called &quot;analysis&quot; and &quot;synthesis&quot; approaches as special
cases and contains novel hybrid setups. We find corresponding coherence-based
recovery guarantees for an l1-norm based separation algorithm. Our results
recover those reported in Studer and Baraniuk, ACHA, submitted, for the
synthesis setting, provide new recovery guarantees for the analysis setting,
and form a basis for comparing performance in the analysis and synthesis
settings. As an aside our findings complement the D-RIP recovery results
reported in Cand\`es et al., ACHA, 2011, for the &quot;analysis&quot; signal recovery
problem: minimize_x ||{\Psi}x||_1 subject to ||y - Ax||_2 \leq {\epsilon}, by
delivering corresponding coherence-based recovery results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4564</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4564</id><created>2012-05-21</created><authors><author><keyname>Chiesa</keyname><forenames>Marco</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Erlebach</keyname><forenames>Thomas</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author></authors><title>Computational Complexity of Traffic Hijacking under BGP and S-BGP</title><categories>cs.NI cs.GT</categories><comments>17 pages with 6 figures</comments><acm-class>C.2.2; F.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harmful Internet hijacking incidents put in evidence how fragile the Border
Gateway Protocol (BGP) is, which is used to exchange routing information
between Autonomous Systems (ASes). As proved by recent research contributions,
even S-BGP, the secure variant of BGP that is being deployed, is not fully able
to blunt traffic attraction attacks. Given a traffic flow between two ASes, we
study how difficult it is for a malicious AS to devise a strategy for hijacking
or intercepting that flow. We show that this problem marks a sharp difference
between BGP and S-BGP. Namely, while it is solvable, under reasonable
assumptions, in polynomial time for the type of attacks that are usually
performed in BGP, it is NP-hard for S-BGP. Our study has several by-products.
E.g., we solve a problem left open in the literature, stating when performing a
hijacking in S-BGP is equivalent to performing an interception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4572</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4572</id><created>2012-05-21</created><authors><author><keyname>Liu</keyname><forenames>Jing</forenames></author><author><keyname>Qiao</keyname><forenames>Fei</forenames></author><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>A Novel Video Compression Approach Based on Underdetermined Blind Source
  Separation</title><categories>cs.MM</categories><comments>4 pages with 4 figures and 1 table</comments><acm-class>H.5.1; H.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a new video compression approach based on underdetermined
blind source separation. Underdetermined blind source separation, which can be
used to efficiently enhance the video compression ratio, is combined with
various off-the-shelf codecs in this paper. Combining with MPEG-2, video
compression ratio could be improved slightly more than 33%. As for combing with
H.264, 4X~12X more compression ratio could be achieved with acceptable PSNR,
according to different kinds of video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4576</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4576</id><created>2012-05-21</created><authors><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author><author><keyname>Sinha</keyname><forenames>Makrand</forenames></author></authors><title>Constructing a Pseudorandom Generator Requires an Almost Linear Number
  of Calls</title><categories>cs.CR cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a black-box construction of a pseudorandom generator from a
one-way function needs to make Omega(n/log(n)) calls to the underlying one-way
function. The bound even holds if the one-way function is guaranteed to be
regular. In this case it matches the best known construction due to Goldreich,
Krawczyk, and Luby (SIAM J. Comp. 22, 1993), which uses O(n/log(n)) calls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4583</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4583</id><created>2012-05-21</created><authors><author><keyname>Pope</keyname><forenames>Graeme</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Sparse Signal Recovery in Hilbert Spaces</title><categories>cs.IT math.IT</categories><comments>Proc. of IEEE International Symposium on Information Theory (ISIT),
  Boston, MA, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports an effort to consolidate numerous coherence-based sparse
signal recovery results available in the literature. We present a single theory
that applies to general Hilbert spaces with the sparsity of a signal defined as
the number of (possibly infinite-dimensional) subspaces participating in the
signal's representation. Our general results recover uncertainty relations and
coherence-based recovery thresholds for sparse signals, block-sparse signals,
multi-band signals, signals in shift-invariant spaces, and signals in finite
unions of (possibly infinite-dimensional) subspaces. Moreover, we improve upon
and generalize several of the existing results and, in many cases, we find
shortened and simplified proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4605</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4605</id><created>2012-05-21</created><authors><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>No Sublogarithmic-time Approximation Scheme for Bipartite Vertex Cover</title><categories>cs.DC cs.CC cs.DS</categories><comments>11 pages, 5 figures</comments><doi>10.1007/s00446-013-0194-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K\&quot;onig's theorem states that on bipartite graphs the size of a maximum
matching equals the size of a minimum vertex cover. It is known from prior work
that for every \epsilon &gt; 0 there exists a constant-time distributed algorithm
that finds a (1+\epsilon)-approximation of a maximum matching on 2-coloured
graphs of bounded degree. In this work, we show---somewhat surprisingly---that
no sublogarithmic-time approximation scheme exists for the dual problem: there
is a constant \delta &gt; 0 so that no randomised distributed algorithm with
running time o(\log n) can find a (1+\delta)-approximation of a minimum vertex
cover on 2-coloured graphs of maximum degree 3. In fact, a simple application
of the Linial--Saks (1993) decomposition demonstrates that this lower bound is
tight.
  Our lower-bound construction is simple and, to some extent, independent of
previous techniques. Along the way we prove that a certain cut minimisation
problem, which might be of independent interest, is hard to approximate locally
on expander graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4611</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4611</id><created>2012-05-21</created><updated>2012-10-08</updated><authors><author><keyname>Goude</keyname><forenames>Anders</forenames></author><author><keyname>Engblom</keyname><forenames>Stefan</forenames></author></authors><title>Adaptive fast multipole methods on the GPU</title><categories>cs.DC cs.DS</categories><comments>Software available at http://user.it.uu.se/~stefane/freeware.html</comments><msc-class>65Y05, 65Y20</msc-class><journal-ref>J. Supercomput. 63(3): 897--918 (2013)</journal-ref><doi>10.1007/s11227-012-0836-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a highly general implementation of fast multipole methods on
graphics processing units (GPUs). Our two-dimensional double precision code
features an asymmetric type of adaptive space discretization leading to a
particularly elegant and flexible implementation. All steps of the multipole
algorithm are efficiently performed on the GPU, including the initial phase
which assembles the topological information of the input data. Through careful
timing experiments we investigate the effects of the various peculiarities of
the GPU architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4626</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4626</id><created>2012-05-21</created><authors><author><keyname>Sodhi</keyname><forenames>Balwinder</forenames></author><author><keyname>Prabhakar</keyname><forenames>T. V.</forenames></author></authors><title>Examining the Impact of Platform Properties on Quality Attributes</title><categories>cs.SE</categories><acm-class>D.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine and bring out the architecturally significant characteristics of
various virtualization and cloud oriented platforms. The impact of such
characteristics on the ability of guest applications to achieve various quality
attributes (QA) has also been determined by examining existing body of
architecture knowledge. We observe from our findings that efficiency, resource
elasticity and security are among the most impacted QAs, and virtualization
platforms exhibit the maximum impact on various QAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4634</identifier>
 <datestamp>2012-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4634</id><created>2012-05-21</created><updated>2012-05-24</updated><authors><author><keyname>Gao</keyname><forenames>T.</forenames></author><author><keyname>Eldridge</keyname><forenames>P. S.</forenames></author><author><keyname>Liew</keyname><forenames>T. C. H.</forenames></author><author><keyname>Tsintzos</keyname><forenames>S. I.</forenames></author><author><keyname>Stavrinidis</keyname><forenames>G.</forenames></author><author><keyname>Deligeorgis</keyname><forenames>G.</forenames></author><author><keyname>Hatzopoulos</keyname><forenames>Z.</forenames></author><author><keyname>Savvidis</keyname><forenames>P. G.</forenames></author></authors><title>Polariton Condensate Transistor Switch</title><categories>cond-mat.mtrl-sci cs.ET</categories><doi>10.1103/PhysRevB.85.235102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polariton condensate transistor switch is realized through optical
excitation of a microcavity ridge with two beams. The ballistically ejected
polaritons from a condensate formed at the source are gated using the 20 times
weaker second beam to switch on and off the flux of polaritons. In the absence
of the gate beam the small built-in detuning creates potential landscape in
which ejected polaritons are channelled toward the end of the ridge where they
condense. The low loss photon-like propagation combined with strong
nonlinearities associated with their excitonic component makes polariton based
transistors particularly attractive for the implementation of all-optical
integrated circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4639</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4639</id><created>2012-05-21</created><authors><author><keyname>Ilhem</keyname><forenames>Kilani</forenames></author><author><keyname>Dalel</keyname><forenames>Jabri</forenames></author><author><keyname>Saloua</keyname><forenames>Bel Hadj Ali</forenames></author><author><keyname>Naceur</keyname><forenames>Abdelkrim Mohamed</forenames></author></authors><title>Observer Design for Takagi-Sugeno Descriptor System with Lipschitz
  Constraints</title><categories>cs.SY</categories><comments>13 pages,5 figures; International Journal of Instrumentation and
  Control Systems (IJICS) Vol.2, No.2, April 2012</comments><doi>10.5121/ijics.2012.2202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the design problem of observers for nonlinear
descriptor systems described by Takagi-Sugeno (TS) system; Depending on the
available knowledge on the premise variables two cases are considered. First a
TS descriptor system with measurables premises variables are proposed. Second,
an observer design which satisfying the Lipschitz condition is proposed when
the premises variables are unmeasurables. The convergence of the state
estimation error is studied using the Lyapunov theory and the stability
conditions are given in terms of Linear Matrix Inequalities (LMIs). Examples
are included to illustrate those methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4641</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4641</id><created>2012-05-21</created><authors><author><keyname>Karimian</keyname><forenames>Yasser</forenames></author><author><keyname>Ziapour</keyname><forenames>Saeideh</forenames></author><author><keyname>Attari</keyname><forenames>Mahmoud Ahmadian</forenames></author></authors><title>Parity Check Matrix Recognition from Noisy Codewords</title><categories>cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study recovering parity check relations for an unknown code from
intercepted bitstream received from Binary Symmetric Channel in this paper. An
iterative column elimination algorithm is introduced which attempts to
eliminate parity bits in codewords of noisy data. This algorithm is very
practical due to low complexity and use of XOR operator. Since, the
computational complexity is low, searching for the length of code and
synchronization is possible. Furthermore, the Hamming weight of the parity
check words are only used in threshold computation and unlike other algorithms,
they have negligible effect in the proposed algorithm. Eventually, experimental
results are presented and estimations for the maximum noise level allowed for
recovering the words of the parity check matrix are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4653</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4653</id><created>2012-03-06</created><authors><author><keyname>Paszkiewicz</keyname><forenames>Zbigniew</forenames></author><author><keyname>Cellary</keyname><forenames>Wojciech</forenames></author></authors><title>Computer Supported Collaborative Processes in Virtual Organizations</title><categories>cs.HC</categories><journal-ref>Advances in Global Management Development, Challenges and
  opportunities of global business in the new millenium: comtemporary issues
  and future trends, Volume XX, p. 85-94, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In global economy, turbulent organization environment strongly influences
organization's operation. Organizations must constantly adapt to changing
circumstances and search for new possibilities of gaining competitive
advantage. To face this challenge, small organizations base their operation on
collaboration within Virtual Organizations (VOs). VO operation is based on
collaborative processes. Due to dynamism and required flexibility of
collaborative processes, existing business information systems are insufficient
to efficiently support them. In this paper a novel method for supporting
collaborative processes based on process mining techniques is proposed. The
method allows activity patterns in various instances of collaborative processes
to be identified and used for recommendation of activities. This provides an
opportunity for better computer support of collaborative processes leading to
more efficient and effective realization of business goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4655</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4655</id><created>2012-05-21</created><authors><author><keyname>Caroprese</keyname><forenames>Luciano</forenames></author><author><keyname>Trubitsyna</keyname><forenames>Irina</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author><author><keyname>Zumpano</keyname><forenames>Ester</forenames></author></authors><title>The View-Update Problem for Indefinite Databases</title><categories>cs.DB cs.AI</categories><comments>24 pages (includes proofs)</comments><acm-class>I.2.4; I.2.3; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces and studies a declarative framework for updating views
over indefinite databases. An indefinite database is a database with null
values that are represented, following the standard database approach, by a
single null constant. The paper formalizes views over such databases as
indefinite deductive databases, and defines for them several classes of
database repairs that realize view-update requests. Most notable is the class
of constrained repairs. Constrained repairs change the database &quot;minimally&quot; and
avoid making arbitrary commitments. They narrow down the space of alternative
ways to fulfill the view-update request to those that are grounded, in a
certain strong sense, in the database, the view and the view-update request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4656</identifier>
 <datestamp>2012-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4656</id><created>2012-05-21</created><updated>2012-07-24</updated><authors><author><keyname>Gr&#xfc;new&#xe4;lder</keyname><forenames>Steffen</forenames></author><author><keyname>Lever</keyname><forenames>Guy</forenames></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author><author><keyname>Patterson</keyname><forenames>Sam</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Pontil</keyname><forenames>Massimilano</forenames></author></authors><title>Conditional mean embeddings as regressors - supplementary</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS)
embeddings of conditional distributions and vector-valued regressors. This
connection introduces a natural regularized loss function which the RKHS
embeddings minimise, providing an intuitive understanding of the embeddings and
a justification for their use. Furthermore, the equivalence allows the
application of vector-valued regression methods and results to the problem of
learning conditional distributions. Using this link we derive a sparse version
of the embedding by considering alternative formulations. Further, by applying
convergence results for vector-valued regression to the embedding problem we
derive minimax convergence rates which are O(\log(n)/n) -- compared to current
state of the art rates of O(n^{-1/4}) -- and are valid under milder and more
intuitive assumptions. These minimax upper rates coincide with lower rates up
to a logarithmic factor, showing that the embedding method achieves nearly
optimal rates. We study our sparse embedding algorithm in a reinforcement
learning task where the algorithm shows significant improvement in sparsity
over an incomplete Cholesky decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4667</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4667</id><created>2012-05-21</created><authors><author><keyname>Akopov</keyname><forenames>Z.</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Amerio</keyname><forenames>Silvia</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Asner</keyname><forenames>David</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Avetisyan</keyname><forenames>Eduard</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Barring</keyname><forenames>Olof</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Beacham</keyname><forenames>James</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Bellis</keyname><forenames>Matthew</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Bernardi</keyname><forenames>Gregorio</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Bethke</keyname><forenames>Siegfried</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Boehnlein</keyname><forenames>Amber</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Brooks</keyname><forenames>Travis</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Browder</keyname><forenames>Thomas</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Brun</keyname><forenames>Rene</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Cartaro</keyname><forenames>Concetta</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Cattaneo</keyname><forenames>Marco</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Chen</keyname><forenames>Gang</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Corney</keyname><forenames>David</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Cranmer</keyname><forenames>Kyle</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Culbertson</keyname><forenames>Ray</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Dallmeier-Tiessen</keyname><forenames>Sunje</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Denisov</keyname><forenames>Dmitri</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Diaconu</keyname><forenames>Cristinel</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Dodonov</keyname><forenames>Vitaliy</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Doyle</keyname><forenames>Tony</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Dubois-Felsmann</keyname><forenames>Gregory</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Ernst</keyname><forenames>Michael</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Gasthuber</keyname><forenames>Martin</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Geiser</keyname><forenames>Achim</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Gianotti</keyname><forenames>Fabiola</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Giubellino</keyname><forenames>Paolo</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Golutvin</keyname><forenames>Andrey</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Gordon</keyname><forenames>John</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Guelzow</keyname><forenames>Volker</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Hara</keyname><forenames>Takanori</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Hayashii</keyname><forenames>Hisaki</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Heiss</keyname><forenames>Andreas</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Hemmer</keyname><forenames>Frederic</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Hernandez</keyname><forenames>Fabio</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Heyes</keyname><forenames>Graham</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Holzner</keyname><forenames>Andre</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Igo-Kemenes</keyname><forenames>Peter</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Iijima</keyname><forenames>Toru</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Incandela</keyname><forenames>Joe</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Jones</keyname><forenames>Roger</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Kemp</keyname><forenames>Yves</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>van Dam</keyname><forenames>Kerstin Kleese</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Knobloch</keyname><forenames>Juergen</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Kreincik</keyname><forenames>David</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Lassila-Perini</keyname><forenames>Kati</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Diberder</keyname><forenames>Francois Le</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Levonian</keyname><forenames>Sergey</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Levy</keyname><forenames>Aharon</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Li</keyname><forenames>Qizhong</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Lobodzinski</keyname><forenames>Bogdan</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Maggi</keyname><forenames>Marcello</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Malka</keyname><forenames>Janusz</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Mele</keyname><forenames>Salvatore</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Mount</keyname><forenames>Richard</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Neal</keyname><forenames>Homer</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Olsson</keyname><forenames>Jan</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Ozerov</keyname><forenames>Dmitri</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Piilonen</keyname><forenames>Leo</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Punzi</keyname><forenames>Giovanni</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Regimbal</keyname><forenames>Kevin</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Riley</keyname><forenames>Daniel</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Roney</keyname><forenames>Michael</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Roser</keyname><forenames>Robert</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Ruf</keyname><forenames>Thomas</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Sakai</keyname><forenames>Yoshihide</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Sasaki</keyname><forenames>Takashi</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Schnell</keyname><forenames>Gunar</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Schroeder</keyname><forenames>Matthias</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Schutz</keyname><forenames>Yves</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Shiers</keyname><forenames>Jamie</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Smith</keyname><forenames>Tim</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Snider</keyname><forenames>Rick</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>South</keyname><forenames>David M.</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Denis</keyname><forenames>Rick St.</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Steder</keyname><forenames>Michael</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Van Wezel</keyname><forenames>Jos</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Varnes</keyname><forenames>Erich</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Votava</keyname><forenames>Margaret</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Wang</keyname><forenames>Yifang</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Weygand</keyname><forenames>Dennis</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>White</keyname><forenames>Vicky</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Wichmann</keyname><forenames>Katarzyna</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Wolbers</keyname><forenames>Stephen</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Yamauchi</keyname><forenames>Masanori</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>Yavin</keyname><forenames>Itay</forenames><affiliation>DPHEP Study Group</affiliation></author><author><keyname>von der Schmitt</keyname><forenames>Hans</forenames><affiliation>DPHEP Study Group</affiliation></author></authors><title>Status Report of the DPHEP Study Group: Towards a Global Effort for
  Sustainable Data Preservation in High Energy Physics</title><categories>hep-ex cs.DL</categories><report-no>DPHEP-2012-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data from high-energy physics (HEP) experiments are collected with
significant financial and human effort and are mostly unique. An
inter-experimental study group on HEP data preservation and long-term analysis
was convened as a panel of the International Committee for Future Accelerators
(ICFA). The group was formed by large collider-based experiments and
investigated the technical and organisational aspects of HEP data preservation.
An intermediate report was released in November 2009 addressing the general
issues of data preservation in HEP. This paper includes and extends the
intermediate report. It provides an analysis of the research case for data
preservation and a detailed description of the various projects at experiment,
laboratory and international levels. In addition, the paper provides a concrete
proposal for an international organisation in charge of the data management and
policies in high-energy physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4672</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4672</id><created>2012-05-17</created><authors><author><keyname>Elloumi</keyname><forenames>Yaroub</forenames></author><author><keyname>Akil</keyname><forenames>Mohamed</forenames></author><author><keyname>Bedoui</keyname><forenames>Mohamed Hedi</forenames></author></authors><title>Timing and Code Size Optimization on Achieving Full Parallelism in
  Uniform Nested Loops</title><categories>cs.PL</categories><comments>10 pages, 16 figures</comments><msc-class>68M20</msc-class><acm-class>B.2.1; D.4.7</acm-class><journal-ref>Journal of Computing, Volume 3, Issue 7, July 2011, 68-77</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multidimensional Retiming is one of the most important optimization
techniques to improve timing parameters of nested loops. It consists in
exploring the iterative and recursive structures of loops to redistribute
computation nodes on cycle periods, and thus to achieve full parallelism.
However, this technique introduces a large overhead in a loop generation due to
the loop transformation. The provided solutions are generally characterized by
an important cycle number and a great code size. It represents the most
limiting factors while implementing them in embedded systems. In this paper, we
present a new Multidimensional Retiming technique, called &quot;Optimal
Multidimensional Retiming&quot; (OMDR). It reveals the timing and data dependency
characteristics of nodes, to minimize the overhead. The experimental results
show that the average improvement on the execution time of the nested loops by
our technique is 19.31% compared to the experiments provided by an existent
Multidimensional Retiming Technique. The average code size is reduced by 43.53%
compared to previous experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4673</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4673</id><created>2012-05-21</created><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>Minimum Complexity Pursuit: Stability Analysis</title><categories>cs.IT math.IT</categories><comments>5 pages, To be presented at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A host of problems involve the recovery of structured signals from a
dimensionality reduced representation such as a random projection; examples
include sparse signals (compressive sensing) and low-rank matrices (matrix
completion). Given the wide range of different recovery algorithms developed to
date, it is natural to ask whether there exist &quot;universal&quot; algorithms for
recovering &quot;structured&quot; signals from their linear projections. We recently
answered this question in the affirmative in the noise-free setting. In this
paper, we extend our results to the case of noisy measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4674</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4674</id><created>2012-05-21</created><authors><author><keyname>Elishco</keyname><forenames>Ohad</forenames></author><author><keyname>Permuter</keyname><forenames>Haim</forenames></author></authors><title>Capacity and coding for the Ising Channel with Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ising channel, which was introduced in 1990, is a channel with memory
that models Inter-Symbol interference. In this paper we consider the Ising
channel with feedback and find the capacity of the channel together with a
capacity-achieving coding scheme. To calculate the channel capacity, an
equivalent dynamic programming (DP) problem is formulated and solved. Using the
DP solution, we establish that the feedback capacity is the expression
$C=(\frac{2H_b(a)}{3+a})\approx 0.575522$ where $a$ is a particular root of a
fourth-degree polynomial and $H_b(x)$ denotes the binary entropy function.
Simultaneously, $a=\arg \max_{0\leq x \leq 1} (\frac{2H_b(x)}{3+x})$. Finally,
a simple, error-free, capacity-achieving coding scheme is provided together
with outlining a strong connection between the DP results and the coding
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4681</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4681</id><created>2012-05-21</created><updated>2013-07-26</updated><authors><author><keyname>Knockel</keyname><forenames>Jeffrey</forenames></author><author><keyname>Saad</keyname><forenames>George</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author></authors><title>Self-Healing Algorithms of Byzantine Faults</title><categories>cs.CR</categories><comments>17 pages, 5 figures and submitted to SSS 2013, Paper #13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen significant interest in designing networks that are
self-healing in the sense that they can automatically recover from adversarial
attacks. Previous work shows that it is possible for a network to automatically
recover, even when an adversary repeatedly deletes nodes in the network.
However, there have not yet been any algorithms that self-heal in the case
where an adversary takes over nodes in the network. In this paper, we address
this gap. In particular, we describe a communication network over n nodes that
ensures the following properties, even when an adversary controls up to t &lt;=
(1/8 - \epsilon)n nodes, for any non-negative \epsilon. First, the network
provides a point-to-point communication with bandwidth and latency costs that
are asymptotically optimal. Second, the expected total number of message
corruptions is O(t(log* n)^2) before the adversarially controlled nodes are
effectively quarantined so that they cause no more corruptions. Empirical
results show that our algorithm can reduce the bandwidth cost by up to a factor
of 70.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4683</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4683</id><created>2012-05-21</created><updated>2013-07-08</updated><authors><author><keyname>Szell</keyname><forenames>Michael</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>How women organize social networks different from men</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 3 figures</comments><journal-ref>Scientific Reports 3, 1214 (2013)</journal-ref><doi>10.1038/srep01214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superpositions of social networks, such as communication, friendship, or
trade networks, are called multiplex networks, forming the structural backbone
of human societies. Novel datasets now allow quantification and exploration of
multiplex networks. Here we study gender-specific differences of a multiplex
network from a complete behavioral dataset of an online-game society of about
300,000 players. On the individual level females perform better economically
and are less risk-taking than males. Males reciprocate friendship requests from
females faster than vice versa and hesitate to reciprocate hostile actions of
females. On the network level females have more communication partners, who are
less connected than partners of males. We find a strong homophily effect for
females and higher clustering coefficients of females in trade and attack
networks. Cooperative links between males are under-represented, reflecting
competition for resources among males. These results confirm quantitatively
that females and males manage their social networks in substantially different
ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4691</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4691</id><created>2012-05-21</created><updated>2012-05-22</updated><authors><author><keyname>Breuvart</keyname><forenames>Flavien</forenames><affiliation>PPS</affiliation></author></authors><title>On the discriminating power of tests in resource lambda-calculus</title><categories>cs.LO cs.PL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its discovery, differential linear logic (DLL) inspired numerous
domains. In denotational semantics, categorical models of DLL are now commune,
and the simplest one is Rel, the category of sets and relations. In proof
theory this naturally gave birth to differential proof nets that are full and
complete for DLL. In turn, these tools can naturally be translated to their
intuitionistic counterpart. By taking the co-Kleisly category associated to the
! comonad, Rel becomes MRel, a model of the \Lcalcul that contains a notion of
differentiation. Proof nets can be used naturally to extend the \Lcalcul into
the lambda calculus with resources, a calculus that contains notions of
linearity and differentiations. Of course MRel is a model of the \Lcalcul with
resources, and it has been proved adequate, but is it fully abstract? That was
a strong conjecture of Bucciarelli, Carraro, Ehrhard and Manzonetto. However,
in this paper we exhibit a counter-example. Moreover, to give more intuition on
the essence of the counter-example and to look for more generality, we will use
an extension of the resource \Lcalcul also introduced by Bucciarelli et al for
which $\Minf$ is fully abstract, the tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4697</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4697</id><created>2012-05-21</created><updated>2016-01-12</updated><authors><author><keyname>Karwa</keyname><forenames>Vishesh</forenames></author><author><keyname>Slavkovi&#x107;</keyname><forenames>Aleksandra</forenames></author></authors><title>Inference using noisy degrees: Differentially private $\beta$-model and
  synthetic graphs</title><categories>stat.ME cs.DS</categories><comments>Published at http://dx.doi.org/10.1214/15-AOS1358 in the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1358</report-no><journal-ref>Annals of Statistics 2016, Vol. 44, No. 1, 87-112</journal-ref><doi>10.1214/15-AOS1358</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\beta$-model of random graphs is an exponential family model with the
degree sequence as a sufficient statistic. In this paper, we contribute three
key results. First, we characterize conditions that lead to a quadratic time
algorithm to check for the existence of MLE of the $\beta$-model, and show that
the MLE never exists for the degree partition $\beta$-model. Second, motivated
by privacy problems with network data, we derive a differentially private
estimator of the parameters of $\beta$-model, and show it is consistent and
asymptotically normally distributed - it achieves the same rate of convergence
as the nonprivate estimator. We present an efficient algorithm for the private
estimator that can be used to release synthetic graphs. Our techniques can also
be used to release degree distributions and degree partitions accurately and
privately, and to perform inference from noisy degrees arising from contexts
other than privacy. We evaluate the proposed estimator on real graphs and
compare it with a current algorithm for releasing degree distributions and find
that it does significantly better. Finally, our paper addresses shortcomings of
current approaches to a fundamental problem of how to perform valid statistical
inference from data released by privacy mechanisms, and lays a foundational
groundwork on how to achieve optimal and private statistical inference in a
principled manner by modeling the privacy mechanism; these principles should be
applicable to a class of models beyond the $\beta$-model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4698</identifier>
 <datestamp>2013-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4698</id><created>2012-05-21</created><updated>2013-02-07</updated><authors><author><keyname>Panagiotakopoulos</keyname><forenames>Constantinos</forenames></author><author><keyname>Tsampouka</keyname><forenames>Petroula</forenames></author></authors><title>The Role of Weight Shrinking in Large Margin Perceptron Learning</title><categories>cs.LG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce into the classical perceptron algorithm with margin a mechanism
that shrinks the current weight vector as a first step of the update. If the
shrinking factor is constant the resulting algorithm may be regarded as a
margin-error-driven version of NORMA with constant learning rate. In this case
we show that the allowed strength of shrinking depends on the value of the
maximum margin. We also consider variable shrinking factors for which there is
no such dependence. In both cases we obtain new generalizations of the
perceptron with margin able to provably attain in a finite number of steps any
desirable approximation of the maximal margin hyperplane. The new approximate
maximum margin classifiers appear experimentally to be very competitive in
2-norm soft margin tasks involving linear kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4699</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4699</id><created>2012-05-21</created><authors><author><keyname>Minora</keyname><forenames>Leonardo</forenames><affiliation>DIMAp, IRISA</affiliation></author><author><keyname>Buisson</keyname><forenames>J&#xe9;r&#xe9;my</forenames><affiliation>IRISA</affiliation></author><author><keyname>Oquendo</keyname><forenames>Flavio</forenames><affiliation>IRISA</affiliation></author><author><keyname>Batista</keyname><forenames>Thais</forenames><affiliation>DIMAp</affiliation></author></authors><title>Issues of Architectural Description Languages for Handling Dynamic
  Reconfiguration</title><categories>cs.SE</categories><comments>6\`eme Conf\'erence francophone sur les architectures logicielles
  (CAL'2012), Montpellier : France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic reconfiguration is the action of modifying a software system at
runtime. Several works have been using architectural specification as the basis
for dynamic reconfiguration. Indeed ADLs (architecture description languages)
let architects describe the elements that could be reconfigured as well as the
set of constraints to which the system must conform during reconfiguration. In
this work, we investigate the ADL literature in order to illustrate how
reconfiguration is supported in four well-known ADLs: pi-ADL, ACME, C2SADL and
Dynamic Wright. From this review, we conclude that none of these ADLs: (i)
addresses the issue of consistently reconfiguring both instances and types;
(ii) takes into account the behaviour of architectural elements during
reconfiguration; and (iii) provides support for assessing reconfiguration,
e.g., verifying the transition against properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4738</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4738</id><created>2012-05-21</created><authors><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author></authors><title>Triangulating the Square and Squaring the Triangle: Quadtrees and
  Delaunay Triangulations are Equivalent</title><categories>cs.CG</categories><comments>37 pages, 13 figures, full version of a paper that appeared in SODA
  2011</comments><journal-ref>SIAM Journal on Computing (SICOMP), 41(4), 2012, pp. 941-974</journal-ref><doi>10.1137/110825698</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that Delaunay triangulations and compressed quadtrees are equivalent
structures. More precisely, we give two algorithms: the first computes a
compressed quadtree for a planar point set, given the Delaunay triangulation;
the second finds the Delaunay triangulation, given a compressed quadtree. Both
algorithms run in deterministic linear time on a pointer machine. Our work
builds on and extends previous results by Krznaric and Levcopolous and Buchin
and Mulzer. Our main tool for the second algorithm is the well-separated pair
decomposition(WSPD), a structure that has been used previously to find
Euclidean minimum spanning trees in higher dimensions (Eppstein). We show that
knowing the WSPD (and a quadtree) suffices to compute a planar Euclidean
minimum spanning tree (EMST) in linear time. With the EMST at hand, we can find
the Delaunay triangulation in linear time.
  As a corollary, we obtain deterministic versions of many previous algorithms
related to Delaunay triangulations, such as splitting planar Delaunay
triangulations, preprocessing imprecise points for faster Delaunay computation,
and transdichotomous Delaunay triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4776</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4776</id><created>2012-05-21</created><authors><author><keyname>Icke</keyname><forenames>Ilknur</forenames></author><author><keyname>Rosenberg</keyname><forenames>Andrew</forenames></author></authors><title>Visual and semantic interpretability of projections of high dimensional
  data for classification tasks</title><categories>cs.HC cs.LG</categories><comments>Longer version of the VAST 2011 poster.
  http://dx.doi.org/10.1109/VAST.2011.6102474</comments><doi>10.1109/VAST.2011.6102474</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of visual quality measures have been introduced in visual analytics
literature in order to automatically select the best views of high dimensional
data from a large number of candidate data projections. These methods generally
concentrate on the interpretability of the visualization and pay little
attention to the interpretability of the projection axes. In this paper, we
argue that interpretability of the visualizations and the feature
transformation functions are both crucial for visual exploration of high
dimensional labeled data. We present a two-part user study to examine these two
related but orthogonal aspects of interpretability. We first study how humans
judge the quality of 2D scatterplots of various datasets with varying number of
classes and provide comparisons with ten automated measures, including a number
of visual quality measures and related measures from various machine learning
fields. We then investigate how the user perception on interpretability of
mathematical expressions relate to various automated measures of complexity
that can be used to characterize data projection functions. We conclude with a
discussion of how automated measures of visual and semantic interpretability of
data projections can be used together for exploratory analysis in
classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4778</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4778</id><created>2012-05-21</created><updated>2012-09-02</updated><authors><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>Vahlenkamp</keyname><forenames>Markus</forenames></author></authors><title>Backscatter from the Data Plane --- Threats to Stability and Security in
  Information-Centric Networking</title><categories>cs.NI cs.CR</categories><comments>15 pages</comments><acm-class>C.2.1; C.2.2; C.2.6</acm-class><journal-ref>Computer Networks, Vol. 57, No. 16, pp. 3192-3206, Elsevier, Nov.
  2013</journal-ref><doi>10.1016/j.comnet.2013.07.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-centric networking proposals attract much attention in the
ongoing search for a future communication paradigm of the Internet. Replacing
the host-to-host connectivity by a data-oriented publish/subscribe service
eases content distribution and authentication by concept, while eliminating
threats from unwanted traffic at an end host as are common in today's Internet.
However, current approaches to content routing heavily rely on data-driven
protocol events and thereby introduce a strong coupling of the control to the
data plane in the underlying routing infrastructure. In this paper, threats to
the stability and security of the content distribution system are analyzed in
theory and practical experiments. We derive relations between state resources
and the performance of routers and demonstrate how this coupling can be misused
in practice. We discuss new attack vectors present in its current state of
development, as well as possibilities and limitations to mitigate them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4781</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4781</id><created>2012-05-21</created><authors><author><keyname>Bandemer</keyname><forenames>Bernd</forenames></author></authors><title>An Achievable Rate Region for Three-Pair Interference Channels with
  Noise</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures; abbreviated version to be presented at IEEE
  International Symposium on Information Theory (ISIT 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An achievable rate region for certain noisy three-user-pair interference
channels is proposed. The channel class under consideration generalizes the
three-pair deterministic interference channel (3-DIC) in the same way as the
Telatar-Tse noisy two-pair interference channel generalizes the El Gamal-Costa
injective channel. Specifically, arbitrary noise is introduced that acts on the
combined interference signal before it affects the desired signal. This class
of channels includes the Gaussian case.
  The rate region includes the best-known inner bound on the 3-DIC capacity
region, dominates treating interference as noise, and subsumes the
Han-Kobayashi region for the two-pair case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4785</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4785</id><created>2012-05-21</created><updated>2012-11-28</updated><authors><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Tan</keyname><forenames>Peng Hui</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Energy-Efficient Relaying over Multiple Slots with Causal CSI</title><categories>cs.IT math.IT</categories><comments>final version for IEEE Journal on Selected Areas in Communications,
  Special Issue on Theories and Methods for Advanced Wireless Relays</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many communication scenarios, such as in cellular systems, the energy cost
is substantial and should be conserved, yet there is a growing need to support
many real-time applications that require timely data delivery. To model such a
scenario, in this paper we consider the problem of minimizing the expected sum
energy of delivering a message of a given size from a source to a destination
subject to a deadline constraint. A relay is present and can assist after it
has decoded the message. Causal channel state information (CSI), in the form of
present and past SNRs of all links, is available for determining the optimal
power allocation for the source and relay. We obtain the optimal power
allocation policy by dynamic programming and explore its structure. We also
obtain conditions for which the minimum expected sum energy is bounded given a
general channel distribution. In particular, we show that for Rayleigh and
Rician fading channels, relaying is necessary for the minimum expected sum
energy to be bounded. This illustrates the fundamental advantage of relaying
from the perspective of energy efficient communications when only causal CSI is
available. Numerical results are obtained which show the reduction in the
expected sum energy under different communication scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4788</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4788</id><created>2012-05-21</created><authors><author><keyname>Platzer</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Dynamic Logics of Dynamical Systems</title><categories>cs.LO math.DS math.LO</categories><msc-class>03B70, 03B45, 03F03, 68Q60, 34A38, 68M14, 34C45, 37H10, 60H10, 03D78</msc-class><acm-class>F.3.1; F.4.1; D.2.4; C.1.m; G.1.4; C.2.4; D.4.7; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey dynamic logics for specifying and verifying properties of dynamical
systems, including hybrid systems, distributed hybrid systems, and stochastic
hybrid systems. A dynamic logic is a first-order modal logic with a pair of
parametrized modal operators for each dynamical system to express necessary or
possible properties of their transition behavior. Due to their full basis of
first-order modal logic operators, dynamic logics can express a rich variety of
system properties, including safety, controllability, reactivity, liveness, and
quantified parametrized properties, even about relations between multiple
dynamical systems. In this survey, we focus on some of the representatives of
the family of differential dynamic logics, which share the ability to express
properties of dynamical systems having continuous dynamics described by various
forms of differential equations.
  We explain the dynamical system models, dynamic logics of dynamical systems,
their semantics, their axiomatizations, and proof calculi for proving logical
formulas about these dynamical systems. We study differential invariants, i.e.,
induction principles for differential equations. We survey theoretical results,
including soundness and completeness and deductive power. Differential dynamic
logics have been implemented in automatic and interactive theorem provers and
have been used successfully to verify safety-critical applications in
automotive, aviation, railway, robotics, and analogue electrical circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4802</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4802</id><created>2012-05-22</created><authors><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Straubing</keyname><forenames>Howard</forenames></author></authors><title>An effective characterization of the alternation hierarchy in
  two-variable logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the languages in the individual levels of the quantifier
alternation hierarchy of first-order logic with two variables by identities.
This implies decidability of the individual levels. More generally we show that
the two-sided semidirect product of a decidable variety with the variety J is
decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4808</identifier>
 <datestamp>2012-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4808</id><created>2012-05-22</created><updated>2012-10-02</updated><authors><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Sato</keyname><forenames>Nobuo</forenames></author><author><keyname>Yano</keyname><forenames>Kazuo</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Importance of individual events in temporal networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>36 pages, 13 figures, 2 tables</comments><journal-ref>New J. Phys. 14, 093003 (2012)</journal-ref><doi>10.1088/1367-2630/14/9/093003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Records of time-stamped social interactions between pairs of individuals
(e.g., face-to-face conversations, e-mail exchanges, and phone calls)
constitute a so-called temporal network. A remarkable difference between
temporal networks and conventional static networks is that time-stamped events
rather than links are the unit elements generating the collective behavior of
nodes. We propose an importance measure for single interaction events. By
generalizing the concept of the advance of event proposed by [Kossinets G,
Kleinberg J, and Watts D J (2008) Proceeding of the 14th ACM SIGKDD
International conference on knowledge discovery and data mining, p 435], we
propose that an event is central when it carries new information about others
to the two nodes involved in the event. We find that the proposed measure
properly quantifies the importance of events in connecting nodes along
time-ordered paths. Because of strong heterogeneity in the importance of events
present in real data, a small fraction of highly important events is necessary
and sufficient to sustain the connectivity of temporal networks. Nevertheless,
in contrast to the behavior of scale-free networks against link removal, this
property mainly results from bursty activity patterns and not heterogeneous
degree distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4809</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4809</id><created>2012-05-22</created><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Iterative Approximate Byzantine Consensus under a Generalized Fault
  Model</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1203.1888</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a generalized fault model that can be used to
represent a wide range of failure scenarios, including correlated failures and
non-uniform node reliabilities. This fault model is general in the sense that
fault models studied in prior related work, such as f -total and f -local
models, are special cases of the generalized fault model. Under the generalized
fault model, we explore iterative approximate Byzantine consensus (IABC)
algorithms in arbitrary directed networks. We prove a necessary and sufficient
condition for the existence of IABC algorithms. The use of the generalized
fault model helps to gain a better understanding of IABC algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4810</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4810</id><created>2012-05-22</created><updated>2012-07-06</updated><authors><author><keyname>Moldovan</keyname><forenames>Teodor Mihai</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Safe Exploration in Markov Decision Processes</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In environments with uncertain dynamics exploration is necessary to learn how
to perform well. Existing reinforcement learning algorithms provide strong
exploration guarantees, but they tend to rely on an ergodicity assumption. The
essence of ergodicity is that any state is eventually reachable from any other
state by following a suitable policy. This assumption allows for exploration
algorithms that operate by simply favoring states that have rarely been visited
before. For most physical systems this assumption is impractical as the systems
would break before any reasonable exploration has taken place, i.e., most
physical systems don't satisfy the ergodicity assumption. In this paper we
address the need for safe exploration methods in Markov decision processes. We
first propose a general formulation of safety through ergodicity. We show that
imposing safety by restricting attention to the resulting set of guaranteed
safe policies is NP-hard. We then present an efficient algorithm for guaranteed
safe, but potentially suboptimal, exploration. At the core is an optimization
formulation in which the constraints restrict attention to a subset of the
guaranteed safe policies and the objective favors exploration policies. Our
framework is compatible with the majority of previously proposed exploration
methods, which rely on an exploration bonus. Our experiments, which include a
Martian terrain exploration problem, show that our method is able to explore
better than classical exploration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4813</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4813</id><created>2012-05-22</created><authors><author><keyname>Sivadasan</keyname><forenames>Praveen</forenames></author><author><keyname>Lal</keyname><forenames>P. Sojan</forenames></author></authors><title>Securing SQLJ Source Codes from Business Logic Disclosure by Data Hiding
  Obfuscation</title><categories>cs.CR cs.DB cs.DC</categories><comments>4 pages,3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information security is protecting information from unauthorized access, use,
disclosure, disruption, modification, perusal and destruction. CAIN model
suggest maintaining the Confidentiality, Authenticity, Integrity and
Non-repudiation (CAIN) of information. Oracle 8i, 9i and 11g Databases support
SQLJ framework allowing embedding of SQL statements in Java Programs and
providing programmer friendly means to access the Oracle database. As cloud
computing technology is becoming popular, SQLJ is considered as a flexible and
user friendly language for developing distributed applications in grid
architectures. SQLJ source codes are translated to java byte codes and
decompilation is generation of source codes from intermediate byte codes. The
intermediate SQLJ application byte codes are open to decompilation, allowing a
malicious reader to forcefully decompile it for understanding confidential
business logic or data from the codes. To the best of our knowledge, strong and
cost effective techniques exist for Oracle Database security, but still data
security techniques are lacking for client side applications, giving
possibility for revelation of confidential business data. Data obfuscation is
hiding the data in codes and we suggest enhancing the data security in SQLJ
source codes by data hiding, to mitigate disclosure of confidential business
data, especially integers in distributed applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4821</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4821</id><created>2012-05-22</created><authors><author><keyname>Dougherty</keyname><forenames>Randall</forenames></author><author><keyname>Lutz</keyname><forenames>Jack</forenames></author><author><keyname>Mauldin</keyname><forenames>R. Daniel</forenames></author><author><keyname>Teutsch</keyname><forenames>Jason</forenames></author></authors><title>Translating the Cantor set by a random</title><categories>cs.CC</categories><msc-class>68Q30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the constructive dimension of points in random translates of the
Cantor set. The Cantor set &quot;cancels randomness&quot; in the sense that some of its
members, when added to Martin-Lof random reals, identify a point with lower
constructive dimension than the random itself. In particular, we find the
Hausdorff dimension of the set of points in a Cantor set translate with a given
constructive dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4829</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4829</id><created>2012-05-22</created><authors><author><keyname>Dey</keyname><forenames>Somdip</forenames></author></authors><title>SD-EQR: A New Technique To Use QR CodesTM in Cryptography</title><categories>cs.CR</categories><comments>6 Pages and 3 figures, International Journal of Information
  Technology &amp; Computer Science (IJITCS), May/June 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the author present a new technique of using QR Codes (commonly
known as 'Quick Respond Codes') in the field of Cryptography. QR Codes are
mainly used to convey or store messages because they have higher or large
storage capacity than any other normal conventional 'barcodes'. In this paper
the primary focus will be on storing messages in encrypted format with a
password and send it to the required destination hiding in a QR Code, without
being tracked or decrypted properly by any hacker or spyware. Since QR Codes
have fast response time and have large storage capacity, QR Codes can be used
perfectly to send encrypted data (messages) to the receiver. This method will
be suitable in any business house, government sectors, communication network to
send their encrypted messages faster to the destination. Or a person can even
use this method to keep his important documents, like passport number, pan-card
id, social security number, perfectly secured with him all the time, without
the information getting leaked to outside world. The new method is achieved by
entering the message along with a password. This password will generate a
secret code, which will be added to each digit or alphabet in the numbers or
text entered in the message (which is needed to be encrypted) and generate the
first phase of encryption. That newly generated encrypted message will again be
encrypted using various other methods to generate the final encrypted message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4831</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4831</id><created>2012-05-22</created><authors><author><keyname>Sebastian</keyname><forenames>Bino</forenames><suffix>V</suffix></author><author><keyname>Unnikrishnan</keyname><forenames>A.</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Kannan</forenames></author></authors><title>Gray Level Co-Occurrence Matrices: Generalisation and Some New Features</title><categories>cs.CV</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniques
used for image texture analysis. In this paper we defined a new feature called
trace extracted from the GLCM and its implications in texture analysis are
discussed in the context of Content Based Image Retrieval (CBIR). The
theoretical extension of GLCM to n-dimensional gray scale images are also
discussed. The results indicate that trace features outperform Haralick
features when applied to CBIR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4832</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4832</id><created>2012-05-22</created><authors><author><keyname>Dey</keyname><forenames>Somdip</forenames></author></authors><title>SD-REE: A Cryptographic Method to Exclude Repetition from a Message</title><categories>cs.CR</categories><comments>7 Pages,1 table for Results, 1 table for Cryptanalysis and 6 figures</comments><journal-ref>Proceedings of The International Conference on Informatics and
  Applications (ICIA2012), Malaysia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the author presents a new cryptographic technique, SD-REE, to
exclude the repetitive terms in a message, when it is to be encrypted, so that
it becomes almost impossible for a person to retrieve or predict the original
message from the encrypted message. In modern world, cryptography hackers try
to break a code or cryptographic algorithm [1,2] or retrieve the key, used for
encryption, by inserting repetitive bytes / characters in the message and
encrypt the message or by analyzing repetitions in the encrypted message, to
find out the encryption algorithm or retrieve the key used for the encryption.
But in SD-REE method the repetitive bytes / characters are removed and there is
no trace of any repetition in the message, which was encrypted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4839</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4839</id><created>2012-05-22</created><updated>2013-06-20</updated><authors><author><keyname>Degris</keyname><forenames>Thomas</forenames></author><author><keyname>White</keyname><forenames>Martha</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>Off-Policy Actor-Critic</title><categories>cs.LG</categories><comments>Full version of the paper, appendix and errata included; Proceedings
  of the 2012 International Conference on Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first actor-critic algorithm for off-policy
reinforcement learning. Our algorithm is online and incremental, and its
per-time-step complexity scales linearly with the number of learned weights.
Previous work on actor-critic algorithms is limited to the on-policy setting
and does not take advantage of the recent advances in off-policy gradient
temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable
a target policy to be learned while following and obtaining data from another
(behavior) policy. For many problems, however, actor-critic methods are more
practical than action value methods (like Greedy-GQ) because they explicitly
represent the policy; consequently, the policy can be stochastic and utilize a
large action space. In this paper, we illustrate how to practically combine the
generality and learning potential of off-policy learning with the flexibility
in action selection given by actor-critic methods. We derive an incremental,
linear time and space complexity algorithm that includes eligibility traces,
prove convergence under assumptions similar to previous off-policy algorithms,
and empirically show better or comparable performance to existing algorithms on
standard reinforcement-learning benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4856</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4856</id><created>2012-05-22</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Gupta</keyname><forenames>Piyush</forenames></author></authors><title>Bounds on Minimum Number of Anchors for Iterative Localization and its
  Connections to Bootstrap Percolation</title><categories>cs.IT cs.NI math.IT math.PR</categories><comments>Invited paper SPCOM 2012, Indian Institute of Science, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterated localization is considered where each node of a network needs to get
localized (find its location on 2-D plane), when initially only a subset of
nodes have their location information. The iterated localization process
proceeds as follows. Starting with a subset of nodes that have their location
information, possibly using global positioning system (GPS) devices, any other
node gets localized if it has three or more localized nodes in its radio range.
The newly localized nodes are included in the subset of nodes that have their
location information for the next iteration. This process is allowed to
continue, until no new node can be localized. The problem is to find the
minimum size of the initially localized subset to start with so that the whole
network is localized with high probability. There are intimate connections
between iterated localization and bootstrap percolation, that is well studied
in statistical physics. Using results known in bootstrap percolation, we find a
sufficient condition on the size of the initially localized subset that
guarantees the localization of all nodes in the network with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4874</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4874</id><created>2012-05-22</created><authors><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>Perfect Secrecy Systems Immune to Spoofing Attacks</title><categories>cs.CR cs.IT math.IT</categories><comments>10 pages (double-column); to appear in &quot;International Journal of
  Information Security&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel perfect secrecy systems that provide immunity to spoofing
attacks under equiprobable source probability distributions. On the theoretical
side, relying on an existence result for $t$-designs by Teirlinck, our
construction method constructively generates systems that can reach an
arbitrary high level of security. On the practical side, we obtain, via cyclic
difference families, very efficient constructions of new optimal systems that
are onefold secure against spoofing. Moreover, we construct, by means of
$t$-designs for large values of $t$, the first near-optimal systems that are 5-
and 6-fold secure as well as further systems with a feasible number of keys
that are 7-fold secure against spoofing. We apply our results furthermore to a
recently extended authentication model, where the opponent has access to a
verification oracle. We obtain this way novel perfect secrecy systems with
immunity to spoofing in the verification oracle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4875</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4875</id><created>2012-05-22</created><updated>2013-11-11</updated><authors><author><keyname>Horak</keyname><forenames>Peter</forenames></author><author><keyname>Grosek</keyname><forenames>Otokar</forenames></author></authors><title>A New Approach Towards the Golomb-Welch Conjecture</title><categories>cs.IT math.IT</categories><msc-class>05B45, 94B05, 52C22, 20K30</msc-class><doi>10.1016/j.ejc.2013.10.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Golomb-Welch conjecture deals with the existence of perfect $e$% -error
correcting Lee codes of word length $n,$ $PL(n,e)$ codes. Although there are
many papers on the topic, the conjecture is still far from being solved. In
this paper we initiate the study of an invariant connected to abelian groups
that enables us to reformulate the conjecture, and then to prove the
non-existence of linear PL(n,2) codes for $n\leq 12$. Using this new approach
we also construct the first quasi-perfect Lee codes for dimension $n=3,$ and
show that, for fixed $n$, there are only finitely many such codes over $Z^n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4876</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4876</id><created>2012-05-22</created><authors><author><keyname>Behboodi</keyname><forenames>Arash</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>Selective Coding Strategy for Unicast Composite Networks</title><categories>cs.IT math.IT</categories><comments>To appear in International Symposium on Information Theory (ISIT)
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a composite unicast relay network where the channel statistic is
randomly drawn from a set of conditional distributions indexed by a random
variable, which is assumed to be unknown at the source, fully known at the
destination and only partly known at the relays. Commonly, the coding strategy
at each relay is fixed regardless of its channel measurement. A novel coding
for unicast composite networks with multiple relays is introduced. This enables
the relays to select dynamically --based on its channel measurement-- the best
coding scheme between compress-and-forward (CF) and decode-and-forward (DF). As
a part of the main result, a generalization of Noisy Network Coding is shown
for the case of unicast general networks where the relays are divided between
those using DF and CF coding. Furthermore, the relays using DF scheme can
exploit the help of those based on CF scheme via offset coding. It is
demonstrated via numerical results that this novel coding, referred to as
Selective Coding Strategy (SCS), outperforms conventional coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4883</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4883</id><created>2012-05-22</created><authors><author><keyname>Liao</keyname><forenames>Gang</forenames></author><author><keyname>Luo</keyname><forenames>Lian</forenames></author><author><keyname>Liu</keyname><forenames>Lei</forenames></author></authors><title>Hybrid Parallel Bidirectional Sieve based on SMP Cluster</title><categories>cs.DC</categories><comments>11 pages,7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this article, hybrid parallel bidirectional sieve method is implemented by
SMP Cluster, the individual computational units joined together by the
communication network, are usually shared-memory systems with one or more
multicore processor. To high-efficiency optimization, we propose average divide
data into nodes, generating double-ended queues (deque) for sieve method that
are able to exploit dual-cores simultaneously start sifting out primes from the
head and tail.And each node create a FIFO queue as dynamic data buffer to ache
temporary data from another nodes send to. The approach obtains huge speedup
and efficiency on SMP Cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4889</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4889</id><created>2012-05-22</created><authors><author><keyname>Brihaye</keyname><forenames>Thomas</forenames></author><author><keyname>Bruy&#xe8;re</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>De Pril</keyname><forenames>Julie</forenames></author></authors><title>On Equilibria in Quantitative Games with Reachability/Safety Objectives</title><categories>cs.GT</categories><comments>Full version of the CSR 2010 proceedings paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study turn-based quantitative multiplayer non zero-sum
games played on finite graphs with both reachability and safety objectives. In
this framework a player with a reachability objective aims at reaching his own
goal as soon as possible, whereas a player with a safety objective aims at
avoiding his bad set or, if impossible, delaying its visit as long as possible.
We prove the existence of Nash equilibria with finite memory in quantitative
multiplayer reachability/safety games. Moreover, we prove the existence of
finite-memory secure equilibria for quantitative two-player reachability games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4891</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4891</id><created>2012-05-22</created><authors><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author></authors><title>Clustering is difficult only when it does not matter</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous papers ask how difficult it is to cluster data. We suggest that the
more relevant and interesting question is how difficult it is to cluster data
sets {\em that can be clustered well}. More generally, despite the ubiquity and
the great importance of clustering, we still do not have a satisfactory
mathematical theory of clustering. In order to properly understand clustering,
it is clearly necessary to develop a solid theoretical basis for the area. For
example, from the perspective of computational complexity theory the clustering
problem seems very hard. Numerous papers introduce various criteria and
numerical measures to quantify the quality of a given clustering. The resulting
conclusions are pessimistic, since it is computationally difficult to find an
optimal clustering of a given data set, if we go by any of these popular
criteria. In contrast, the practitioners' perspective is much more optimistic.
Our explanation for this disparity of opinions is that complexity theory
concentrates on the worst case, whereas in reality we only care for data sets
that can be clustered well.
  We introduce a theoretical framework of clustering in metric spaces that
revolves around a notion of &quot;good clustering&quot;. We show that if a good
clustering exists, then in many cases it can be efficiently found. Our
conclusion is that contrary to popular belief, clustering should not be
considered a hard task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4893</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4893</id><created>2012-05-22</created><authors><author><keyname>Bilu</keyname><forenames>Yonatan</forenames></author><author><keyname>Daniely</keyname><forenames>Amit</forenames></author><author><keyname>Linial</keyname><forenames>Nati</forenames></author><author><keyname>Saks</keyname><forenames>Michael</forenames></author></authors><title>On the practically interesting instances of MAXCUT</title><categories>cs.CC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of a computational problem is traditionally quantified based
on the hardness of its worst case. This approach has many advantages and has
led to a deep and beautiful theory. However, from the practical perspective,
this leaves much to be desired. In application areas, practically interesting
instances very often occupy just a tiny part of an algorithm's space of
instances, and the vast majority of instances are simply irrelevant. Addressing
these issues is a major challenge for theoretical computer science which may
make theory more relevant to the practice of computer science.
  Following Bilu and Linial, we apply this perspective to MAXCUT, viewed as a
clustering problem. Using a variety of techniques, we investigate practically
interesting instances of this problem. Specifically, we show how to solve in
polynomial time distinguished, metric, expanding and dense instances of MAXCUT
under mild stability assumptions. In particular, $(1+\epsilon)$-stability
(which is optimal) suffices for metric and dense MAXCUT. We also show how to
solve in polynomial time $\Omega(\sqrt{n})$-stable instances of MAXCUT,
substantially improving the best previously known result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4894</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4894</id><created>2012-05-22</created><authors><author><keyname>Bozzo</keyname><forenames>Enrico</forenames></author><author><keyname>Franceschet</keyname><forenames>Massimo</forenames></author></authors><title>Effective and efficient approximations of the generalized inverse of the
  graph Laplacian matrix with an application to current-flow betweenness
  centrality</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise methods for finding approximations of the generalized inverse of
the graph Laplacian matrix, which arises in many graph-theoretic applications.
Finding this matrix in its entirety involves solving a matrix inversion
problem, which is resource demanding in terms of consumed time and memory and
hence impractical whenever the graph is relatively large. Our approximations
use only few eigenpairs of the Laplacian matrix and are parametric with respect
to this number, so that the user can compromise between effectiveness and
efficiency of the approximated solution. We apply the devised approximations to
the problem of computing current-flow betweenness centrality on a graph.
However, given the generality of the Laplacian matrix, many other applications
can be sought. We experimentally demonstrate that the approximations are
effective already with a constant number of eigenpairs. These few eigenpairs
can be stored with a linear amount of memory in the number of nodes of the
graph and, in the realistic case of sparse networks, they can be efficiently
computed using one of the many methods for retrieving few eigenpairs of sparse
matrices that abound in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4900</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4900</id><created>2012-05-22</created><authors><author><keyname>Sudarsanan</keyname><forenames>Adethya</forenames></author></authors><title>CloudPass - a passport system based on Cloud Computing and Near Field
  Communication</title><categories>cs.OH</categories><comments>Presented at Cloud2012 - International Workshop on Cloud Computing,
  New Delhi, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication has penetrated into all fields of technology,
especially in mobility, where wireless transactions are gaining importance with
improvements in standards like 3G and 4G. There are many technologies that
support the wireless forms of interactions between devices. One among them is
NFC - Near Field Communication. In addition to NFC, other external technologies
like Quick Response (QR) Codes assist in establishing interactions among
participating devices. In this paper, we examine an approach that will involve
standards and technologies like NFC, QR Codes and Cloud Infrastructure to
design a mobile application which will perform desired functionalities. Cloud
Storage is used as a reservoir to store the artifacts used by the application.
Development and testing of the application is initially carried out on
emulators or simulators followed by testing on real handsets/devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4905</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4905</id><created>2012-05-22</created><authors><author><keyname>Sharma</keyname><forenames>Preeti</forenames></author><author><keyname>Saluja</keyname><forenames>Monika</forenames></author><author><keyname>Saluja</keyname><forenames>Krishan Kumar</forenames></author></authors><title>Detection techniques of selective forwarding attacks in wireless sensor
  networks: a survey</title><categories>cs.CR</categories><comments>6 Pages</comments><doi>10.5120/7718-1082</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The wireless sensor network has become a hot research area due its wide range
of application in military and civilian domain, but as it uses wireless media
for communication these are easily prone to security attacks. There are number
of attacks on wireless sensor networks like black hole attack, sink hole
attack, Sybil attack, selective forwarding attacks etc. in this paper we will
concentrate on selective forwarding attacks In selective forwarding attacks,
malicious nodes behave like normal nodes and selectively drop packets. The
selection of dropping nodes may be random. Identifying such attacks is very
difficult and sometimes impossible. In this paper we have listed up some
detection techniques, which have been proposed by different researcher in
recent years, there we also have tabular representation of qualitative analysis
of detection techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4928</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4928</id><created>2012-05-22</created><authors><author><keyname>Arlt</keyname><forenames>Stephan</forenames></author><author><keyname>Banerjee</keyname><forenames>Ishan</forenames></author><author><keyname>Bertolini</keyname><forenames>Cristiano</forenames></author><author><keyname>Memon</keyname><forenames>Atif M.</forenames></author><author><keyname>Sch&#xe4;f</keyname><forenames>Martin</forenames></author></authors><title>Grey-box GUI Testing: Efficient Generation of Event Sequences</title><categories>cs.SE</categories><comments>11 pages</comments><msc-class>68N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical user interfaces (GUIs), due to their event driven nature, present a
potentially unbounded space of all possible ways to interact with software.
During testing it becomes necessary to effectively sample this space. In this
paper we develop algorithms that sample the GUI's input space by only
generating sequences that (1) are allowed by the GUI's structure, and (2) chain
together only those events that have data dependencies between their event
handlers. We create a new abstraction, called an event-dependency graph (EDG)
of the GUI, that captures data dependencies between event handler code. We
develop a mapping between EDGs and an existing black-box user-level model of
the GUI's workflow, called an event-flow graph (EFG). We have implemented
automated EDG construction in a tool that analyzes the bytecode of each event
handler. We evaluate our &quot;grey-box&quot; approach using four open-source
applications and compare it with the current state-of-the-art EFG approach. Our
results show that using the EDG reduces the number of test cases while still
achieving at least the same coverage. Furthermore, we were able to detect 2 new
bugs in the subject applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4933</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4933</id><created>2012-05-22</created><authors><author><keyname>Walk</keyname><forenames>Philipp</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>Compressed Sensing on the Image of Bilinear Maps</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, Proc. of IEEE International Symposium on
  Information Theory (ISIT), Boston, MA, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several communication models, the dispersive part of a communication
channel is described by a bilinear operation $T$ between the possible sets of
input signals and channel parameters. The received channel output has then to
be identified from the image $T(X,Y)$ of the input signal difference sets $X$
and the channel state sets $Y$. The main goal in this contribution is to
characterize the compressibility of $T(X,Y)$ with respect to an ambient
dimension $N$. In this paper we show that a restricted norm multiplicativity of
$T$ on all canonical subspaces $X$ and $Y$ with dimension $S$ resp. $F$ is
sufficient for the reconstruction of output signals with an overwhelming
probability from $\mathcal{O}((S+F)\log N)$ random sub-Gaussian measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4944</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4944</id><created>2012-05-22</created><authors><author><keyname>Shivhare</keyname><forenames>Shiv Naresh</forenames></author><author><keyname>Khethawat</keyname><forenames>Saritha</forenames></author></authors><title>Emotion Detection from Text</title><categories>cs.HC</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotion can be expressed in many ways that can be seen such as facial
expression and gestures, speech and by written text. Emotion Detection in text
documents is essentially a content - based classification problem involving
concepts from the domains of Natural Language Processing as well as Machine
Learning. In this paper emotion recognition based on textual data and the
techniques used in emotion detection are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4951</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4951</id><created>2012-05-22</created><updated>2012-05-31</updated><authors><author><keyname>Zhang</keyname><forenames>Yufeng</forenames></author><author><keyname>Chen</keyname><forenames>Zhenbang</forenames></author><author><keyname>Wang</keyname><forenames>Ji</forenames></author></authors><title>Speculative Symbolic Execution</title><categories>cs.SE</categories><comments>14 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Symbolic execution is an effective path oriented and constraint based program
analysis technique. Recently, there is a significant development in the
research and application of symbolic execution. However, symbolic execution
still suffers from the scalability problem in practice, especially when applied
to large-scale or very complex programs. In this paper, we propose a new
fashion of symbolic execution, named Speculative Symbolic Execution (SSE), to
speed up symbolic execution by reducing the invocation times of constraint
solver. In SSE, when encountering a branch statement, the search procedure may
speculatively explore the branch without regard to the feasibility. Constraint
solver is invoked only when the speculated branches are accumulated to a
specified number. In addition, we present a key optimization technique that
enhances SSE greatly. We have implemented SSE and the optimization technique on
Symbolic Pathfinder (SPF). Experimental results on six programs show that, our
method can reduce the invocation times of constraint solver by 21% to 49% (with
an average of 30%), and save the search time from 23.6% to 43.6% (with an
average of 30%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4959</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4959</id><created>2012-03-01</created><authors><author><keyname>Yu</keyname><forenames>Hong</forenames></author></authors><title>A Multiple Access Protocol for Multimedia Transmission over Wireless
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops and evaluates the performance of an advanced multiple
access protocol for transmission of full complement of multimedia signals
consisting of various combinations of voice, video, data, text and images over
wireless networks. The protocol is called Advanced Multiple Access Protocol for
Multimedia Transmission (AMAPMT) and is to be used in the Data Link Layer of
the protocol stack. The principle of operation of the protocol is presented in
a number of logical flow charts. The protocol grants permission to transmit to
a source on the basis of a priority scheme that takes into account a
time-to-live (TTL) parameter of all the transactions, selectable priorities
assigned to all the sources and relevant channel state information (CSI) in
this order. Performance of the protocol is evaluated in terms of quality of
service parameters like packet loss ratio (PLR), mean packet transfer delay
(MPTD) and throughput. Using a simulation model based on an OPNET simulation
software package does the evaluation. Under various traffic loads with constant
distributions with various mean arrival rates and transaction sizes results
obtained show that the performance is improved when this priority scheme is
used than when it is not used. The results for AMAPMT are compared with that of
the best currently available multiple access protocol called Adaptive Request
Channel Multiple Access (ARCMA). AMAPMT protocol out performs ARCMA protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4967</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4967</id><created>2012-05-22</created><authors><author><keyname>Lashgar</keyname><forenames>Ahmad</forenames></author><author><keyname>Baniasadi</keyname><forenames>Amirali</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author></authors><title>Investigating Warp Size Impact in GPUs</title><categories>cs.AR</categories><comments>7 pages, 7 figures, 2 tables, Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are a number of design decisions that impact a GPU's performance. Among
such decisions deciding the right warp size can deeply influence the rest of
the design. Small warps reduce the performance penalty associated with branch
divergence at the expense of a reduction in memory coalescing. Large warps
enhance memory coalescing significantly but also increase branch divergence.
This leaves designers with two choices: use a small warps and invest in finding
new solutions to enhance coalescing or use large warps and address branch
divergence employing effective control-flow solutions. In this work our goal is
to investigate the answer to this question. We analyze warp size impact on
memory coalescing and branch divergence. We use our findings to study two
machines: a GPU using small warps but equipped with excellent memory coalescing
(SW+) and a GPU using large warps but employing an MIMD engine immune from
control-flow costs (LW+). Our evaluations show that building
coalescing-enhanced small warp GPUs is a better approach compared to pursuing a
control-flow enhanced large warp GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4968</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4968</id><created>2012-05-22</created><authors><author><keyname>Pande</keyname><forenames>Akshara</forenames></author><author><keyname>Pant</keyname><forenames>Vivekanand</forenames></author><author><keyname>Nigam</keyname><forenames>S.</forenames></author></authors><title>SubGraD- An Approach for Subgraph Detection</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach of graph matching is introduced in this paper, which
efficiently solves the problem of graph isomorphism and subgraph isomorphism.
In this paper we are introducing a new approach called SubGraD, for query graph
detection in source graph. Firstly consider the model graph (query graph) and
make the possible sets called model sets starting from the chosen initial node
or starter. Similarly, for the source graph (reference graph), all the possible
sets called reference sets could be made. Our aim is to make the reference set
on the basis of the model set. If it is possible to make the reference set,
then it is said that query graph has been detected in the source graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4971</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4971</id><created>2012-05-22</created><authors><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Data Gathering in Networks of Bacteria Colonies: Collective Sensing and
  Relaying Using Molecular Communication</title><categories>cs.IT math.IT q-bio.MN</categories><comments>appeared in 2012 IEEE INFOCOM WORKSHOPS (NetSciCom '2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prospect of new biological and industrial applications that require
communication in micro-scale, encourages research on the design of
bio-compatible communication networks using networking primitives already
available in nature. One of the most promising candidates for constructing such
networks is to adapt and engineer specific types of bacteria that are capable
of sensing, actuation, and above all, communication with each other. In this
paper, we describe a new architecture for networks of bacteria to form a data
collecting network, as in traditional sensor networks. The key to this
architecture is the fact that the node in the network itself is a bacterial
colony; as an individual bacterium (biological agent) is a tiny unreliable
element with limited capabilities. We describe such a network under two
different scenarios. We study the data gathering (sensing and multihop
communication) scenario as in sensor networks followed by the consensus problem
in a multi-node network. We will explain as to how the bacteria in the colony
collectively orchestrate their actions as a node to perform sensing and
relaying tasks that would not be possible (at least reliably) by an individual
bacterium. Each single bacterium in the colony forms a belief by sensing
external parameter (e.g., a molecular signal from another node) from the medium
and shares its belief with other bacteria in the colony. Then, after some
interactions, all the bacteria in the colony form a common belief and act as a
single node. We will model the reception process of each individual bacteria
and will study its impact on the overall functionality of a node. We will
present results on the reliability of the multihop communication for data
gathering scenario as well as the speed of convergence in the consensus
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4973</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4973</id><created>2012-05-22</created><updated>2012-06-26</updated><authors><author><keyname>Edalat</keyname><forenames>Abbas</forenames></author><author><keyname>Ghoroghi</keyname><forenames>Ali</forenames></author><author><keyname>Sakellariou</keyname><forenames>Georgios</forenames></author></authors><title>Multi-games and a double game extension of the Prisoner's Dilemma</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new class of games, called Multi-Games (MG), in which a given
number of players play a fixed number of basic games simultaneously. In each
round of the MG, each player will have a specific set of weights, one for each
basic game, which add up to one and represent the fraction of the player's
investment in each basic game. The total payoff for each player is then the
convex combination, with the corresponding weights, of the payoffs it obtains
in the basic games. The basic games in a MG can be regarded as different
environments for the players. When the players' weights for the different games
in MG are private information or types with given conditional probability
distributions, we obtain a particular class of Bayesian games. We show that for
the class of so-called completely pure regular Double Game (DG) with finite
sets of types, the Nash equilibria (NE) of the basic games can be used to
compute a Bayesian Nash equilibrium of the DG in linear time with respect to
the number of types of the players. We study a DG for the Prisoner's Dilemma
(PD) by extending the PD with a second so-called Social Game (SG), generalising
the notion of altruistic extension of a game in which players have different
altruistic levels (or social coefficients). We study two different examples of
Bayesian games in this context in which the social coefficients have a finite
set of values and each player only knows the probability distribution of the
opponent's social coefficient. In the first case we have a completely pure
regular DG for which we deduce a Bayesian NE. Finally, we use the second
example to compare various strategies in a round-robin tournament of the DG for
PD, in which the players can change their social coefficients incrementally
from one round to the next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4977</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4977</id><created>2012-05-22</created><updated>2014-04-02</updated><authors><author><keyname>Mink</keyname><forenames>Alan</forenames></author><author><keyname>Nakassis</keyname><forenames>Anastase</forenames></author></authors><title>LDPC for QKD Reconciliation</title><categories>cs.CR quant-ph</categories><comments>9 pages with Pseudo code and matrix correction performance</comments><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present the Low Density Parity Check (LDPC) forward error correction
algorithm adapted for the Quantum Key Distribution (QKD) protocol in a form
readily applied by developers. A sparse parity check matrix is required for the
LDPC algorithm and we suggest using some that have been defined by the IEEE and
ETSI standards organizations for use in various communication protocols. We
evaluate the QKD performance of these various parity check matrices as a
function of the quantum bit error rate. We also discuss the computational
precision required for this LPDC algorithm. As QKD evolves towards deployment,
complete algorithm descriptions and performance analysis, as we present, will
be required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4983</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4983</id><created>2012-05-22</created><authors><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Collective Sensing-Capacity of Bacteria Populations</title><categories>cs.IT math.IT</categories><comments>Published in International Symposium on Information Theory 2012 (ISIT
  '2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of biological networks using bacteria as the basic elements of the
network is initially motivated by a phenomenon called quorum sensing. Through
quorum sensing, each bacterium performs sensing the medium and communicating it
to others via molecular communication. As a result, bacteria can orchestrate
and act collectively and perform tasks impossible otherwise. In this paper, we
consider a population of bacteria as a single node in a network. In our version
of biological communication networks, such a node would communicate with one
another via molecular signals. As a first step toward such networks, this paper
focuses on the study of the transfer of information to the population (i.e.,
the node) by stimulating it with a concentration of special type of a molecules
signal. These molecules trigger a chain of processes inside each bacteria that
results in a final output in the form of light or fluorescence. Each stage in
the process adds noise to the signal carried to the next stage. Our objective
is to measure (compute) the maximum amount of information that we can transfer
to the node. This can be viewed as the collective sensing capacity of the node.
The molecular concentration, which carries the information, is the input to the
node, which should be estimated by observing the produced light as the output
of the node (i.e., the entire population of bacteria forming the node). We
focus on the noise caused by the random process of trapping molecules at the
receptors as well as the variation of outputs of different bacteria in the
node. The capacity variation with the number of bacteria in the node and the
number of receptors per bacteria is obtained. Finally, we investigated the
collective sensing capability of the node when a specific form of molecular
signaling concentration is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4984</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4984</id><created>2012-03-09</created><authors><author><keyname>Sreedhar</keyname><forenames>K.</forenames></author><author><keyname>Sreenivasulu</keyname><forenames>Y.</forenames></author></authors><title>Determination of RF source power in WPSN using modulated backscattering</title><categories>cs.OH</categories><comments>10 pages; International Journal on Soft Computing (IJSC) Vol.3, No.1
  (2012). arXiv admin note: text overlap with arXiv:1001.5339 by other authors</comments><doi>10.5121/ijsc.2012.3104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wireless sensor network (WSN) is a wireless network consisting of spatially
distributed autonomous devices using sensors to cooperatively monitor physical
or environmental conditions, such as temperature, sound, vibration, pressure,
motion or pollutants, at different locations. During RF transmission energy
consumed by critically energy-constrained sensor nodes in a WSN is related to
the life time system, but the life time of the system is inversely proportional
to the energy consumed by sensor nodes. In that regard, modulated
backscattering (MB) is a promising design choice, in which sensor nodes send
their data just by switching their antenna impedance and reflecting the
incident signal coming from an RF source. Hence wireless passive sensor
networks (WPSN) designed to operate using MB do not have the lifetime
constraints. In this we are going to investigate the system analytically. To
obtain interference-free communication connectivity with the WPSN nodes number
of RF sources is determined and analyzed in terms of output power and the
transmission frequency of RF sources, network size, RF source and WPSN node
characteristics. The results of this paper reveal that communication coverage
and RF Source Power can be practically maintained in WPSN through careful
selection of design parameters
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4988</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4988</id><created>2012-05-22</created><authors><author><keyname>Einolghozati</keyname><forenames>Arash</forenames></author><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Capacity of Diffusion-based Molecular Communication with Ligand
  Receptors</title><categories>cs.IT math.IT</categories><comments>Published in Information Theory Workshop 2011 (ITW '2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A diffusion-based molecular communication system has two major components:
the diffusion in the medium, and the ligand-reception. Information bits,
encoded in the time variations of the concentration of molecules, are conveyed
to the receiver front through the molecular diffusion in the medium. The
receiver, in turn, measures the concentration of the molecules in its vicinity
in order to retrieve the information. This is done via ligand-reception
process. In this paper, we develop models to study the constraints imposed by
the concentration sensing at the receiver side and derive the maximum rate by
which a ligand-receiver can receive information. Therefore, the overall
capacity of the diffusion channel with the ligand receptors can be obtained by
combining the results presented in this paper with our previous work on the
achievable information rate of molecular communication over the diffusion
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.4996</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.4996</id><created>2012-03-08</created><authors><author><keyname>Kabir</keyname><forenames>M. Hasnat</forenames></author><author><keyname>Ullah</keyname><forenames>Shaikh Enayet</forenames></author><author><keyname>Zaman</keyname><forenames>Mustari</forenames></author><author><keyname>Rashed</keyname><forenames>Md. Golam</forenames></author></authors><title>Ber analysis of iterative turbo encoded miso wireless communication
  system under implementation of q-ostbc scheme</title><categories>cs.IT math.IT</categories><comments>International Journal of Information Technology Convergence and
  Services (IJITCS), Vol.2, No.1, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a comprehensive study has been made to evaluate the
performance of a MISO wireless communication system. The 4-by-1 spatially
multiplexed Turbo encoded system under investigation incorporates
Quasi-orthogonal space-time block coding (Q-STBC) and ML signal detection
schemes under QPSK, QAM, 16PSK and 16QAM digital modulations. The simulation
results elucidate that a significant improvement of system performance is
achieved in QAM modulation. The results are also indicative of noticeable
system performance enhancement with increasing number of iterations in Turbo
encoding/decoding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5003</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5003</id><created>2012-05-22</created><authors><author><keyname>Datta</keyname><forenames>Ajoy K.</forenames><affiliation>MIS</affiliation></author><author><keyname>Lamani</keyname><forenames>Anissa</forenames><affiliation>MIS</affiliation></author><author><keyname>Larmore</keyname><forenames>Lawrence L.</forenames><affiliation>LIP6</affiliation></author><author><keyname>Petit</keyname><forenames>Franck</forenames><affiliation>LIP6</affiliation></author></authors><title>Ring Exploration with Oblivious Myopic Robots</title><categories>cs.MA cs.DC</categories><comments>(2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploration problem in the discrete universe, using identical oblivious
asynchronous robots without direct communication, has been well investigated.
These robots have sensors that allow them to see their environment and move
accordingly. However, the previous work on this problem assume that robots have
an unlimited visibility, that is, they can see the position of all the other
robots. In this paper, we consider deterministic exploration in an anonymous,
unoriented ring using asynchronous, oblivious, and myopic robots. By myopic, we
mean that the robots have only a limited visibility. We study the computational
limits imposed by such robots and we show that under some conditions the
exploration problem can still be solved. We study the cases where the robots
visibility is limited to 1, 2, and 3 neighboring nodes, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5004</identifier>
 <datestamp>2012-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5004</id><created>2012-05-22</created><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Systematic DFT Frames: Principle and Eigenvalues Structure</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 table, to appear in ISIT2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a host of recent applications requiring some amount of
redundancy, frames are becoming a standard tool in the signal processing
toolbox. In this paper, we study a specific class of frames, known as discrete
Fourier transform (DFT) codes, and introduce the notion of systematic frames
for this class. This is encouraged by application of systematic DFT codes in
distributed source coding using DFT codes, a new application for frames.
Studying their extreme eigenvalues, we show that, unlike DFT frames, systematic
DFT frames are not necessarily tight. Then, we come up with conditions for
which these frames can be tight. In either case, the best and worst systematic
frames are established from reconstruction error point of view. Eigenvalues of
DFT frames, and their subframes, play a pivotal role in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1205.5012</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1205.5012</id><created>2012-05-22</created><updated>2013-07-03</updated><authors><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Hastie</keyname><forenames>Trevor J.</forenames></author></authors><title>Learning Mixed Graphical Models</title><categories>stat.ML cs.CV cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning the structure of a pairwise graphical
model over continuous and discrete variables. We present a new pairwise model
for graphical models with both continuous and discrete variables that is
amenable to structure learning. In previous work, authors have considered
structure learning of Gaussian graphical models and structure learning of
discrete models. Our approach is a natural generalization of these two lines of
work to the mixed case. The penalization scheme involves a novel symmetric use
of the group-lasso norm and follows naturally from a particular parametrization
of the model.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="31000" completeListSize="102538">1122234|32001</resumptionToken>
</ListRecords>
</OAI-PMH>
